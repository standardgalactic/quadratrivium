WEBVTT

00:00.000 --> 00:08.080
I want to introduce our first keynote speaker this morning who's going to talk to us about

00:08.080 --> 00:16.720
data in the age of AI. David Weinberger is Harvard's Brookman Klein Center for Internet

00:16.720 --> 00:23.600
and Society. He's participated in with that group for many years. He's an author of quite a few books,

00:24.480 --> 00:31.440
Everyday Chaos, Everything is Miscellaneous, Too Big to Know, and many years ago the Clutrain

00:31.440 --> 00:40.640
Manifesto. But he's also a columnist for KM World, so he keeps his eye on what's happening

00:40.640 --> 00:48.800
in the KM World. He has spoken at KM World a number of times on many different topics,

00:48.800 --> 00:53.280
but he hasn't for a while, so we're very happy to have him with us this year.

00:54.000 --> 00:57.520
And please welcome David Weinberger.

01:04.560 --> 01:14.800
Thank you so much. I guess I can take this off. It's wonderful to be back in this community

01:15.440 --> 01:21.600
and amazing and wonderful to have participants from 38 countries, and I would now like to welcome

01:21.600 --> 01:28.160
all of them in each of their 25 different languages, so just bear with me. Good morning,

01:28.160 --> 01:32.800
which is the one I've really mastered, and I'll move on from there. Sorry, I'm an American.

01:33.680 --> 01:40.240
So I want to talk about data in the age of AI, and I should warn you that this is a topic,

01:40.320 --> 01:45.120
it's part of a larger topic that I've been trying to work through for the past few years,

01:45.120 --> 01:47.440
and you'll see I have not fully worked it through.

01:50.560 --> 01:54.400
Whoops, sorry, I've got to get the right clicker. The right clicker.

01:56.400 --> 02:03.840
I found the right clicker. I'm not looking for your approval, but still. Thank you. Thank you very

02:03.840 --> 02:09.120
much. So I'm also going to include metadata, because metadata is the type of data,

02:10.960 --> 02:15.920
and it's really important from the beginning to me that you understand this is not a technical

02:15.920 --> 02:20.240
talk, it's not a talk about the technicalities of data, it's an attempt to try to follow

02:21.840 --> 02:29.280
how we think about data, not as data scientists, which I absolutely am not, sorry, humanities major,

02:29.280 --> 02:37.600
and how that might be affecting how we view our world. So don't be alarmed at all the technical

02:37.600 --> 02:44.320
errors, and I didn't fill in everything and the like. Sorry, enough disclaimers? So good. So this

02:44.320 --> 02:52.240
is Martin Heidegger, a German philosopher who died in 1976. In 1954, he wrote an essay called

02:52.240 --> 02:57.520
The Question Concerning Technology, except it was in German, and in it he says in a typical

02:57.520 --> 03:06.240
Heideggerian way, technology discloses being. Heidegger was a terrible, terrible writer. He was

03:06.240 --> 03:13.440
also, please do not look him up in Wikipedia. He was a horrible, horrible person, and I feel

03:13.440 --> 03:18.000
ashamed even introducing him. I did do my doctoral dissertation on him, but that was a long time

03:18.000 --> 03:24.480
ago, a very long time ago. Anyway, so my understanding, my way of understanding this is that our

03:24.480 --> 03:34.000
engagement with technology casts a light on the world. We see our world through what technology

03:34.000 --> 03:42.320
shows us of it, but it also casts a shadow, which is very prominent in how AI, which is certainly

03:42.320 --> 03:48.800
a two-sided technology when it comes to good and evil. It's certainly true of AI. So the sort of

03:48.800 --> 03:53.040
thing that I think he's talking about, but I don't really care because I want to talk about it, and

03:53.040 --> 04:00.480
it's not a Heidegger lecture, is that say in the 17th and 18th century, where the pinnacle technology

04:00.480 --> 04:08.880
was watches, which were amazing creations, incredibly complex, handmade, just mind-blowingly,

04:08.880 --> 04:12.720
they were the chat GPT of the time. It was unbelievable that these things could work,

04:13.440 --> 04:18.320
but because they were the pinnacle technology, the dominant technology, we began to see the

04:18.320 --> 04:24.480
universe through that lens, so to speak. So the entire universe, we started talking about the

04:24.480 --> 04:28.960
clockwork universe. The universe itself started to look like a clockwork, which meant that there

04:28.960 --> 04:37.040
were very simple rules that govern it and govern it perfectly, beautifully. This was aided and abetted

04:37.040 --> 04:44.160
by Newton's laws, which were the mechanisms, the mechanics of the universe. So that's an example

04:44.240 --> 04:50.560
of how we interpret our world through our tech. And so there's, I think, a big question that asked,

04:51.200 --> 04:57.520
which is, okay, so how are we going to interpret our world in the age of AI? And I'm picking a subset

04:57.520 --> 05:07.120
of it, which is in light of data. So I'm going to begin with a really quick overview of starting

05:07.120 --> 05:14.160
with mainframes working up to the internet and then AI, about how data has affected the sort of

05:14.160 --> 05:20.720
public idea of data has affected our view of how the world works. So starting with mainframes,

05:22.000 --> 05:30.160
mainframes, in the age of mainframes, data was really, really scarce, and it was a resource.

05:30.160 --> 05:34.320
It had to be carefully managed. And just as with the punch cards of the day,

05:35.280 --> 05:43.360
data was structured and it was a reduction. A typical human resources record was really

05:43.360 --> 05:47.600
sparse. There was hardly any information in it because of the capacity of the computers.

05:47.600 --> 05:52.160
And it was, of course, completely regular. Everybody's record was structured the same way.

05:55.280 --> 06:01.520
I don't think it's a coincidence that one of the large cultural divides of the time was between the

06:01.520 --> 06:09.840
IBM rep in the blue suit, absolute symbol of conformity with the rebel, the symbol of rebellion

06:09.840 --> 06:17.680
here. And so it looked like our culture was divided into two parts. In the 1960s, this blue suit guy

06:17.680 --> 06:23.280
remained, but the angry beaten it got replaced by a stoned and overly happy hippie. But it was the

06:23.280 --> 06:33.520
same sort of tension. And so it was the tension between conformity, individual control and spontaneity

06:33.520 --> 06:39.520
and reduction of information so we could manage it and part of control and wild excess often for

06:39.520 --> 06:45.200
its own sake. As I mentioned oddly to somebody just a few minutes ago, this does not come up very

06:45.200 --> 06:53.360
often, I was at the original Woodstock. So I'm talking from experience here. Oh, my mastering a

06:53.360 --> 06:58.320
remote doesn't count for anything, but Woodstock gets applause? What sort of group are you?

07:01.200 --> 07:09.120
So age of computers of PCs, as we all know, I think, what drove, well, those of us who lived

07:09.200 --> 07:15.840
through the era can confirm that what drove the adoption of PCs was spreadsheets, killer app,

07:15.840 --> 07:21.520
absolute killer app drove the hardware and spreadsheets present data in a matrix. Of

07:21.520 --> 07:29.280
course, I'm you're all familiar with spreadsheets. There's a model of in this case how the business

07:29.280 --> 07:37.280
works and the relationship among the pieces of that model. But all the way back in 1984, Stephen

07:37.280 --> 07:46.000
Levy, who's a remarkably good writer about tech, he's still writing very a lot, always interesting.

07:46.000 --> 07:55.440
1984, Levy said that what was actually interesting and important about the adoption of spreadsheets,

07:55.440 --> 08:01.600
reason people were adopting them so enthusiastically wasn't because they modeled business, it was

08:01.600 --> 08:06.720
because you could play with that model. It was the what if factor. And I think he correctly says,

08:06.800 --> 08:13.280
in 1984, did I mention it was 1984 when he said this? He said that it's already this ability to

08:13.280 --> 08:17.680
alter the model to play with it was already changing organizational structures. They're

08:17.680 --> 08:25.440
incredibly early and insightful. So I think there's a tremendous amount of truth in this.

08:25.440 --> 08:31.680
And so I'm going to pretty arbitrarily say that if you want to talk about data, the change in data

08:31.680 --> 08:38.480
in the era of PCs, it goes from this controlled minimize thing to still pretty minimal amount

08:38.480 --> 08:44.480
of data that it handled. But we want to play with it. And so data becomes detached, it's not

08:44.480 --> 08:48.880
just a read out of the world. It's a read out of the world that you can you can mess around with,

08:48.880 --> 08:55.040
you can try other worlds, you can try other forms of your business. And this liberates data from

08:55.040 --> 09:02.640
feeling like a direct read out of the world that data are facts that we can just read out and they

09:02.640 --> 09:11.200
are the reality. So then there's a sort of a micro era of big data, where one way of thinking about

09:11.200 --> 09:18.000
this is the data becomes a source of surprises. And the canonical example of this is the discovery

09:18.000 --> 09:23.120
that people who buy diapers also often buy beer. There's a correlation.

09:26.640 --> 09:36.960
That's our granddaughter. I know. Anyway, she doesn't drink beer. I see where I went wrong

09:36.960 --> 09:44.960
in this slide. We are not giving her beer. She doesn't even like beer. I mean, she likes it,

09:44.960 --> 09:54.160
but she's not crazy about it. Oh, Poppy, I'm sorry. Anyway, so this actually, it doesn't

09:54.160 --> 09:59.040
matter. It's a good example, but it's not a real example. It was actually a correlation that was

09:59.040 --> 10:03.760
discovered in 1992, which is generally before the era of big data. And it was not discovered through

10:03.760 --> 10:08.960
deep data analysis. Somebody noticed that people buy a lot of diapers and they did a sequel and

10:08.960 --> 10:13.920
ask you L query to see what is correlated with that. It's beer. That turns out to be

10:13.920 --> 10:21.120
an unlikely in reality, not really a helpful correlation. Although there is some debate

10:21.120 --> 10:28.080
about it. Okay. So data in the age of the internet, the next two sections are going to get a little

10:28.080 --> 10:34.240
bit longer. Sorry. So internet and AI. So in the age of the internet, data gets really confusing

10:35.200 --> 10:41.280
because it's one sense. It shows up as a type of smog that we omit, which is an unfortunate image,

10:41.280 --> 10:47.440
I guess. But as we're browsing and doing all the other internet things and corporations are

10:47.440 --> 10:52.080
gathering all that data and compiling it and manipulating it and using it against us,

10:53.760 --> 10:59.760
it was the overwhelming amount of data was so much that it actually seemed at times not to

10:59.840 --> 11:05.840
clarify things but to make where to even start unless you were a professional data analyst

11:05.840 --> 11:10.800
like at the platforms. So that's, I'm going to give you a few different ways, I think,

11:10.800 --> 11:16.560
of characterizing data in the age of the internet. At the same time that it felt like smog,

11:17.280 --> 11:23.360
so much and dangerous, the internet is all about links, which is sort of up from the data level.

11:23.360 --> 11:28.160
It's more at the information level, if you want, where links are just about the opposite. I mean,

11:28.160 --> 11:34.560
links are obviously and deeply human and form a structure that we can navigate at will and

11:34.560 --> 11:42.640
notice how things are connected. Very different from the smoggy idea of data. And so I think

11:43.760 --> 11:51.760
we've gotten a similar sort of polarity in which there's the smoggy. We all, I think,

11:51.840 --> 11:59.680
maybe not. Most of us are concerned, let's say, about the use of the data that we are

11:59.680 --> 12:06.480
surrendering unknowingly. Again, we click yes in order to, you know, that we've surrendered

12:06.480 --> 12:11.040
unwillingly. We don't know exactly what's being captured, but we know that it's being used to

12:11.040 --> 12:18.400
influence our decisions by people who don't have our interests usually as their interests. And on

12:18.400 --> 12:25.040
the other hand, links, which is 100%, individuals who are able to control and connect with others

12:26.160 --> 12:30.240
in an intensely social way, and both those things go on at the same time. And I think a lot of us

12:30.240 --> 12:39.680
have this sort of divided understanding of data in the age of AI. But we also saw, if only because

12:39.680 --> 12:45.040
of the gigantic amount of data that suddenly was there and seemed useful, we started unstructuring

12:45.040 --> 12:51.520
our databases. We did this in all sorts of ways, a semantic web with linked open data from Tim Berners-Lee,

12:51.520 --> 13:01.760
but also on structured databases like MongoDB and in graphs and data lakes and JSON,

13:01.760 --> 13:09.120
this enormous unstructuring of databases, which has enabled a lot. Okay, so I want to take,

13:09.600 --> 13:17.680
it's not really a detour, but I don't care. It's a detour. It's not about metadata in the

13:17.680 --> 13:22.480
age of the internet. It's not a detour because it actually reinforces the same point. So

13:24.160 --> 13:30.400
as you know, have you all tried a search engine? Because they're really amazing. If you haven't

13:30.400 --> 13:34.880
tried a search engine, oh, I'm telling you, you really should. There's like a guluan, a bing. So

13:34.880 --> 13:40.880
then you know that if you don't know who wrote Moby Dick, you can ask, it'll tell you it's Melville.

13:40.880 --> 13:45.440
If you don't know what Melville wrote, you could ask, it'll tell you Moby Dick and some other stuff.

13:45.440 --> 13:49.360
If you don't know either of those things, you can ask it about the content of Moby Dick.

13:50.640 --> 13:54.640
And you can even misspell the content. Call me Ishmael. Where is that? It's in Moby Dick

13:54.640 --> 14:00.160
by Herman Melville. So each of these questions contain a piece of metadata that linked to data.

14:00.720 --> 14:12.240
And that destructures metadata, which used to be a label of fix to things. And so it turns out,

14:12.240 --> 14:17.600
as we have discovered, that the difference between metadata and data is only functional,

14:17.600 --> 14:23.280
only operational. It's how you're using it. Because so that metadata is what you know. Well,

14:23.280 --> 14:28.160
you know it was by Herman Melville, but you don't know what the book is. And data is what you're

14:28.160 --> 14:33.280
looking for. Oh, it's Moby Dick. That's the only difference between them. So these, which is an

14:33.280 --> 14:40.560
incredibly powerful and liberating thing when everything can be metadata for something else.

14:40.560 --> 14:46.400
And then in turn be the thing, the data that some other metadata is looking for. So this is why it

14:46.400 --> 14:53.280
wasn't actually a detour. It's a, we're seeing the unstructuring of metadata, just as we are

14:53.280 --> 15:01.120
seeing, have seen already the unstructuring of data. So now let's talk about AI, where again,

15:01.120 --> 15:07.760
I'm going to have a few ways of characterizing it in a phrase, a bumper sticker. So the first is

15:09.040 --> 15:16.320
data is generative in the age of AI, right? And we all, we all know this because we know

15:17.040 --> 15:25.280
that in the old days before AI, a spreadsheet or any other sort of program generates data for sure.

15:26.480 --> 15:32.880
But it generates it because humans have constructed the model. Whereas with AI, as we all know,

15:32.880 --> 15:38.640
when I say AI, I really mean machine learning. I hope that's, I should have said that, but now I have.

15:39.200 --> 15:47.360
With AI, data creates the model, which is insane and seemed completely implausible until, you

15:47.360 --> 15:53.920
know, about 15 years ago, we found out, oh, yeah, that works. We can get more accurate classification

15:55.520 --> 15:59.840
of objects in photos that way, even though it makes no sense. We're not going to tell

15:59.840 --> 16:03.920
it anything at all about what we know about objects and how you recognize them. We'll just

16:04.000 --> 16:12.320
give it data. It's insane, but it works. So data creates the model, the model then generates

16:13.680 --> 16:18.800
new data, but it is remarkable that now data is generative this way.

16:21.520 --> 16:26.560
Although I've already mentioned chat GPT once and thus have fulfilled my legal requirement,

16:26.560 --> 16:33.600
I'm going to mention it again. So I asked it, this is about metadata. So I asked it the other day,

16:34.320 --> 16:39.200
Dante's Inferno has three levels. Are there any other, give me five other artworks that have three,

16:40.160 --> 16:45.200
that's, you know, a trio of things. And it did a good job. It's unbelievable. It's amazing.

16:46.160 --> 16:50.960
It's amazing because we don't have a sense of scale. Humans can't, you know, at least I can't

16:50.960 --> 16:54.800
think it's scale. So I'm amazed and surprised by what scale can do. But it is, of course,

16:54.800 --> 17:01.680
incredible. It gave me five. It gave, they're good. They gave pretty good explanations of why,

17:01.680 --> 17:09.600
which I hadn't even asked for. And so if we think about this in terms of metadata, the metadata in

17:09.600 --> 17:16.160
my query was artworks that show something in three parts. And the data that it fetched was

17:16.160 --> 17:23.600
that text that you just saw. And what is, I think, amazing about this is the metadata now is generating

17:23.600 --> 17:32.000
data. The metadata generated that content. It's not something I've seen before. And if we have,

17:32.000 --> 17:36.480
I'd be really interested in some other field or some other way. I'd really like to know about it.

17:37.360 --> 17:44.080
This is, this is mind blowing. We have metadata that will generate its own data. And generally,

17:44.720 --> 17:50.480
it's good. We can't, it doesn't know when the data isn't good. Basically, as you know,

17:50.480 --> 17:56.400
basically every, not basically, everything that chat AI says is an hallucination. It's just that

17:56.400 --> 18:02.560
most of those hallucinations are true hallucinations. It doesn't know that the ones that it's making up

18:02.560 --> 18:08.000
and we often have trouble telling. So that's a terrible problem. So let me give you an example.

18:10.080 --> 18:16.560
So in January 2022, researchers at the University of Leeds and some other institutions published a

18:16.560 --> 18:24.720
paper that said we built a model from retinal scans and some really basic medical information,

18:24.720 --> 18:31.520
like age, weight and the like, really a very small set of it. Does anybody know about this?

18:33.440 --> 18:40.480
It's sort of mind blowing. Because it works. I'm sorry. Let me be more precise. What works is

18:41.280 --> 18:48.160
the AI is able to predict with some degree of accuracy the likelihood of an individual

18:48.160 --> 18:59.200
having a heart attack, myocardial infarction, based upon the retinal scan. Nobody, data scientists,

19:00.000 --> 19:06.480
AI people, doctors of all stripes have tried to figure out what about those images, presumably,

19:06.480 --> 19:13.280
but who knows? Presumably, it's the veins, but we don't know. We can't figure out how it's doing it.

19:14.240 --> 19:19.920
It is, at the moment, inexplicable. And I know that there are actually bunches of people here

19:19.920 --> 19:27.520
working on making machine learning less inexplicable at dinner with Beth Truden and a bunch of other

19:27.520 --> 19:32.800
people at that table last night where this was a lively topic of conversation and Beth's company

19:32.800 --> 19:40.560
has a way of keeping the sources, the citations and sources of the knowledge

19:41.280 --> 19:47.920
with the output and is generating it from a more fact-based and reliable set of information.

19:47.920 --> 19:54.240
Is that approximately right? Okay. It's approximately right. There's tons and tons of work in all

19:54.240 --> 20:04.480
areas to try to make AI less inexplicable. But as it stands, let me put it like this,

20:04.480 --> 20:10.560
that inexplicability is one of the two original sins of AI, by which I mean,

20:12.400 --> 20:20.000
and I know the sentence doesn't actually make sense, but it may make sense. You'll be the judge.

20:20.000 --> 20:25.680
So left on its own, AI would tend towards inexplicability. There are interventions and

20:25.680 --> 20:30.480
structurings of all kinds that we might be able to do to prevent that, but it doesn't care if we

20:30.480 --> 20:36.320
understand that has not been its mission. It's this mission has been to give accurate predictions

20:36.320 --> 20:41.360
based upon data guided by a ton of human decisions about what we meet, what we're looking for and

20:41.360 --> 20:51.680
what we will accept. So inexplicability is pretty common so far in AI models. Beth, how much trouble

20:51.680 --> 20:57.920
am I in? You'll tell me afterwards. Okay. So in this regard, I think a second formulation of data

20:57.920 --> 21:09.360
in the age of AI would be to say that, okay, we'll do it by hand. I have forgotten how to use my

21:09.360 --> 21:23.360
clicker. A few moments, it was great. Okay. That AI, data in the era of AI is a source of secrets,

21:23.360 --> 21:28.160
like, oh, there's secret information in a retina that can let us see what's going on with the left

21:28.160 --> 21:34.160
ventricle, which is an indicator of heart health. Didn't know it. It's there, but it's secret. But

21:34.240 --> 21:42.400
actually, I would think I would prefer the formulation that data in the era of AI is a

21:42.400 --> 21:46.800
keeper of mysteries, because secrets, once you know them, generally, you know why you know them,

21:46.800 --> 21:53.280
how you know them, why they're true and all that. When mysteries, you know, but it remains a mystery

21:53.360 --> 22:01.040
how it happened. And so far, that seems to be at least some of what AI does.

22:02.160 --> 22:07.040
So let's for the moment, we're going to just overstate and say AI tends to be inexplicable

22:08.320 --> 22:16.000
at the moment. And we can argue later, we won't. And then we can play the five-year-old game.

22:16.000 --> 22:21.200
Now, the baby you saw, we do have a five-year-old grandchild as well. So that little drunken baby

22:21.200 --> 22:26.480
that you saw would not be doing this, but she will be in a few years. So you can ask why. Just

22:26.480 --> 22:31.360
keep asking why. So why isn't it inexplicable? And the answer is because the model is just too

22:31.360 --> 22:35.840
complex. Okay, why? Why is the model too complex? Because there are too many factors, there are

22:35.840 --> 22:40.960
too many variables, there's too much data that's connecting to too many others. You know, ChatGPT

22:40.960 --> 22:48.960
has 175 billion parameters, which are weights, weighting the relationship, the importance of the

22:49.680 --> 22:57.200
relationship of words, 175 billion. So if you want to know why it chose one word over another,

22:57.200 --> 23:02.960
why it called a house you're looking at luxurious rather than upscale, you're never going to know,

23:02.960 --> 23:06.320
at least at this point, you're not going to know, there's just too much going on there.

23:07.120 --> 23:13.360
So, okay, well, why are there so many factors connecting to too many others? Well, that's a

23:13.360 --> 23:22.960
good question. Because actually, that's how the world works. The world is really complex. I mean,

23:22.960 --> 23:29.840
the universe is the single most complex thing in the universe. And it is really, really complex.

23:29.840 --> 23:34.960
And the reason that these complex models work is that they are capturing something about the

23:34.960 --> 23:40.960
complexity of the world that we live in. They go wrong in all sorts of ways, they're dangerous and

23:41.040 --> 23:46.640
how they go wrong. But when they work, they're capturing something mysterious about the world

23:46.640 --> 23:52.400
we live in, which is there's so much stuff and everything affects everything else all the time

23:52.400 --> 24:02.240
forever, everywhere, everything, everything. It's simultaneously. It's not like a clock. If only

24:02.320 --> 24:11.520
it was like a clock, but it's not. It's intensely beyond imagination complex in its interrelationships.

24:12.560 --> 24:18.400
Okay, so why didn't we notice this before? And lots of people have, I think we all have,

24:18.960 --> 24:26.400
it's not news that the universe is complex, but it doesn't register. And I think it's

24:26.400 --> 24:30.880
for two reasons. So, I mean, the sort of complexity I have in mind is, and this is a relatively simple

24:30.880 --> 24:39.040
case, what determined why those people were going to be in the crosswalk with you this morning,

24:39.040 --> 24:46.480
exactly where they are. There's no hope of figuring that out. It's way, way too complex. It's too

24:46.480 --> 24:54.160
complex for any one person. So why haven't we done anything? Why haven't we come to grips with this?

24:54.160 --> 24:58.080
Why isn't this the baseline of all of our thought and thinking? Well, I think for one thing,

24:58.080 --> 25:02.880
it's because we couldn't do anything with that sort of information. And so we just ignored it.

25:03.680 --> 25:08.800
Generally, we said, well, no, that's an accident. It's just, oh, who knows? It's chance. It's

25:08.800 --> 25:14.560
coincidence. We have a whole vocabulary for dismissing complex, the results of complex

25:14.560 --> 25:21.040
interactions is not worth our attention because we couldn't do anything with it. And second of all,

25:21.040 --> 25:27.360
because it didn't fit well with our old human models. So I'm using our here as the West.

25:28.320 --> 25:35.200
I just have to limit my domain because it's basically the only thing I know enough about

25:35.200 --> 25:43.760
to be talking about, any legs to stand on. So in the West, this view does not fit very well

25:43.760 --> 25:50.080
because for thousands of years, if we take the Greek ancient Greece as the origins of

25:50.080 --> 25:56.960
Western civilization, which is controversial, but traditional to do that. Back then, the idea was

25:56.960 --> 26:00.560
there's all this mess in the world that seems chaotic, but underneath it, there are laws and

26:00.560 --> 26:04.720
the laws are simple and understandable. There are laws, there are rules, there's universals,

26:05.280 --> 26:11.200
there are principles, there are overall, there are generalizations. That's what we hang on to

26:11.200 --> 26:19.120
because we can use those. So in the West, traditionally, we have viewed generalizations

26:19.120 --> 26:25.760
of various forms. They explain what's explained, which is the particulars. This is a chart of why

26:25.760 --> 26:34.080
we have preferred the general because they are generalizations reduce information. They're simpler

26:34.640 --> 26:38.640
and it just so happens that the laws of the universe happen to be simple enough for humans

26:38.640 --> 26:44.320
to understand what a coincidence, but they simplify something that's very complex, the realm of

26:46.000 --> 26:52.400
particulars. We have thought that these laws are eternal. They've always held. They explained

26:52.400 --> 26:56.080
everything going back to the Big Bang, even before we knew about a Big Bang.

26:57.440 --> 27:02.560
And ultimately, they are the truth. In our tradition, the Western tradition, we have

27:02.560 --> 27:07.920
looked up into the skies for the eternal truths. In the case of the ancient Greeks, more or less

27:07.920 --> 27:15.280
literally, we still do it. We valorize the eternal over the particulars, which are sort of just,

27:15.280 --> 27:20.080
they're over like that. They change all the time. There's no real abiding truth in them.

27:20.400 --> 27:28.880
But I think that we are entering an age with the age of AI. For me, I think the most important

27:28.880 --> 27:35.840
sort of change in how we view things is that we are getting a more particularized view. We're

27:35.840 --> 27:45.040
taking particulars more seriously. We are letting them have voice with models themselves. I should

27:45.280 --> 27:50.800
pay attention to my slides. The models themselves, machine learning models, being literally

27:50.800 --> 27:55.760
generalizations. But they're generalizations that are made up of patterns that have been derived

27:55.760 --> 28:00.000
from particulars. Those patterns can be so small in particular that there are billions of them

28:00.000 --> 28:04.560
that get sorted through and generalized in various ways. But they stay true to letting

28:04.560 --> 28:12.240
the particulars speak. And this is why machine learning can make, can sort animal photos better

28:12.240 --> 28:20.160
than handwritten code has been able to. Okay, so if we are going to get more used to the idea of

28:20.160 --> 28:26.800
particulars as being real and important and in some ways determinative and having their own voice,

28:27.440 --> 28:34.800
then how will that happen? And I think here it may be because of the light that is cast

28:35.760 --> 28:43.040
by, by the shadows that are cast by AI. It's our fear. So I want to give two examples.

28:43.040 --> 28:47.920
One is a common sort of fear gets expressed variously, but we don't know how it works,

28:47.920 --> 28:53.760
which can be genuinely scary and important to recognize, right? But it, there's two words at

28:53.760 --> 28:58.880
the end of the sentence that are really crucial. So when you hear, we don't know how it works,

28:58.880 --> 29:07.360
we also hear it works, which is amazing. So this fear may be moving us towards an embrace

29:07.360 --> 29:13.040
of the particular because we then hear when we ask, well, why don't we know? We're told

29:13.040 --> 29:18.960
because it's way too complex. Well, in hearing, we don't know how it works. We may be being led to

29:18.960 --> 29:24.640
believe, not on purpose, but being led to believe that it works because the world is also wildly,

29:24.640 --> 29:33.120
complexly particular. So I want to give you a slight example of what it might mean to understand

29:33.120 --> 29:40.400
something outside of the realm of AI in terms of particularity. So in this case, I picked a simple

29:40.400 --> 29:44.080
little topic, morality. I think I have like three slides and we'll be done with morality and that

29:44.080 --> 29:48.880
will be great. So typically in the West, traditionally in the West, we have had, we've resorted to

29:48.880 --> 29:55.920
ethical frameworks. So religious framework says, do what God commands you to do. Reason one says,

29:55.920 --> 30:02.400
do follow principles that are based in reason. And then the utilitarian one says, the framework says,

30:02.400 --> 30:06.160
do that which will bring the most happiness to the most people. Pretty rough, but you know what I

30:06.160 --> 30:10.480
mean. So it's a framework. It tells you why some things are good and some things are bad. And it

30:10.480 --> 30:16.000
tells you what to do in particular situations, except they don't. They don't work. I'm going to

30:16.080 --> 30:25.520
give you the world's briefest and simplest example, I think. So you're you, you have a friend, A,

30:25.520 --> 30:31.600
who is angry, tells you that they're angry at your mutual friend, B. But A says, but don't tell

30:31.600 --> 30:37.520
anybody about it. Shortly thereafter, your mutual friend C comes along and says, oh, I'm very excited.

30:37.520 --> 30:43.520
I'm putting together the seating plan for our wedding. And I'm going to put C here. I'm going

30:43.760 --> 30:47.360
I know A and B are such good friends. I want to put them next to each other. Do you tell

30:48.800 --> 30:54.240
do you tell C that that's a bad idea because of the fight? I don't know. And I'm going to say,

30:54.240 --> 30:58.720
neither do you, because we don't know the particulars. You're going to think about this,

30:58.720 --> 31:05.600
and you're going to think, well, yeah, how vindictive is A? Is A forgiving? Would A understand if I

31:05.600 --> 31:12.080
didn't explain why? How deep is the rift between A and B? How how upset is C going to be if there's

31:12.080 --> 31:17.920
some minor tension between two people at their wedding and so forth? Without that, you don't

31:17.920 --> 31:23.840
know what to do. And considering what to do morally, you have to think about the details. In fact,

31:23.840 --> 31:29.520
if weirdly, the next next week, you run into different set of friends who have exactly the

31:29.520 --> 31:33.360
same sort of formal thing that one's angry and don't tell and the rest of it, but it has

31:33.360 --> 31:37.600
their different people and it's not a wedding. It's whatever. It's a camping trip.

31:38.240 --> 31:44.800
What you did in the first case is not going to help you decide what to do in the second case.

31:44.800 --> 31:49.680
You've got to rethink the particulars all again. That's what you would do. You would say, well,

31:49.680 --> 31:55.440
I didn't, I violated my promise to A because A is forgiving, but C, oh, C would never forgive me

31:55.440 --> 32:01.920
if I did that, for example. It comes down to particulars. So the philosopher Martha Nussbaum,

32:01.920 --> 32:07.440
1999, in a book called Love's Knowledge, which is a serious philosophy work with one of the

32:07.440 --> 32:13.200
great titles and very apt title. Anyway, she talks about this and she says moral situations

32:13.200 --> 32:19.120
are not commensurable. You can't compare them. And that's because they are so particular.

32:19.120 --> 32:27.840
So the second example is of a fear that may be telling us inadvertently what the world under AI

32:27.840 --> 32:34.960
type of data is. So it's like what the light is that's being shown. So AI is biased. It is biased.

32:34.960 --> 32:44.800
It is the second of its original sins. Left on its own, AI will be biased. We have to guard against

32:44.800 --> 32:50.400
it. It's very hard to guard against it, as I'm sure everybody here knows, but we can ask, okay,

32:50.400 --> 32:55.840
well, why is it biased? We're told this and then it's explained to us why it's biased because

32:55.840 --> 33:03.040
data tends to reflect societal biases and the like. And what we hear from this is, oh,

33:03.040 --> 33:08.240
oh, we get to select the data. So data is human stuff. It's not a readout of the world. It's

33:08.240 --> 33:14.560
not the facts about the world. Facts aren't exactly facts either. Different topic. It's stuff that we

33:14.560 --> 33:21.280
decide, data is stuff that we read off of meters, meters that we have decided to plant. Where and

33:21.280 --> 33:25.840
why and to what degree of accuracy and what we do with the data are all human decisions.

33:26.960 --> 33:29.760
If I say data is human stuff, I don't mean it's

33:32.240 --> 33:37.520
unreliable, but I sort of mean it's unreliable that there's a human element of decision

33:37.520 --> 33:43.360
which includes unconscious biases and occasionally conscious, but generally the issue in AI is

33:43.360 --> 33:50.720
unconscious, unaware biases of what's going on, how what we're doing might be taken in any of the

33:51.280 --> 33:54.960
38 other countries other than the one that we happen to be in right now, for example.

33:56.480 --> 34:01.760
Data is human. It's human stuff, obviously. Okay, so how does that lead to anything? Well,

34:01.760 --> 34:07.840
it may be that in the discussion of bias, we get to the point of proxies because the first

34:07.840 --> 34:12.160
response is, well, let me just don't record if you worried about bias against women or whatever

34:12.160 --> 34:17.360
protected class or whatever you want to call it, then don't collect, don't have a column for women

34:17.360 --> 34:22.240
and then you have to explain, well, no, but there are proxies for women in this case.

34:24.320 --> 34:33.360
That's, proxies are really interesting, at least I think so, in part because it's proxies work by

34:34.960 --> 34:39.120
putting shape around the whole, the thing you're trying not to have affect your,

34:39.120 --> 34:43.120
it's like the missing piece in the jigsaw puzzle. You can see what that shape is and that can be

34:43.280 --> 34:48.240
part, that becomes in a sense part of the data. I know I'm not putting this technically correctly,

34:48.240 --> 34:53.200
but I'm trying to talk about how this will perhaps appear to people who are not technical.

34:54.640 --> 35:02.480
And there are only proxies because things are so interrelated. They're so interrelated that the

35:02.480 --> 35:12.560
absence of something can be the presence of something. That's maybe one way through this fear

35:12.560 --> 35:19.520
justifiable and terrible and correctly terrible fear of bias. You don't have to, I got myself

35:19.520 --> 35:25.360
started on it, I'm going to back away from it. That this, the explanation of it, the understanding

35:25.360 --> 35:31.600
of the first question, well, why is there bias? Why can't you just lead you to understand the deep,

35:32.160 --> 35:38.960
deep, multi-dimensional interrelationship of all of the data and all of the world?

35:39.520 --> 35:44.160
So if we ask what, the original question of this talk, which is,

35:46.400 --> 35:51.520
if tech casts light and shadow on the world, can we think about how our world is already beginning

35:51.520 --> 36:01.120
to look in the light of this change in data? So way too short what I'm about to say, I understand

36:01.120 --> 36:04.960
that, but it's just to give you a sense of it. So if you look at an enterprise in light of particulars,

36:05.920 --> 36:11.120
I think that we begin to see black swans, you all know black swans, you all know black swans,

36:11.120 --> 36:19.360
the unexpected things that happen, drop out of the sky and destroy a supplier's factory

36:19.360 --> 36:23.200
in your supply chain and suddenly your business is in great danger.

36:24.640 --> 36:32.320
Literal lightning struck. Yes, that's right, I think there are black swans, but look that through

36:32.320 --> 36:38.000
in this light, in the light of AI and data, everything's a black swan. That's the nature of

36:38.000 --> 36:46.640
particulars. You can't, it's not just the big events, everything that happens is a mashup of

36:46.640 --> 36:52.960
everything that happens all at once. So everything's a black swan and except some things are

36:52.960 --> 36:58.080
butterflies in the chaos theory sense in which other butterfly alights on a plant in Brazil and

36:58.080 --> 37:04.480
triggers a tornado in Kansas, right, the standard example. It seems pretty implausible, but the

37:04.480 --> 37:12.080
idea is very validated and sound, which is that a small event can create cascades by which it picks

37:12.080 --> 37:19.600
up energy and has surprising and important results. And by the way, it's really hard to go backwards

37:19.600 --> 37:25.120
from the tornado to pin it on that damn butterfly and sort of pin it to a wall for what it did.

37:25.840 --> 37:29.920
That would be, often that's what we're trying to do with AI to understand it.

37:31.440 --> 37:38.240
Okay, so more specifically, but still not very specific, this sort of thinking I would imagine

37:38.240 --> 37:43.920
should have important effects on really important business topics beyond pinning butterflies,

37:44.480 --> 37:48.880
including strategy, obviously, if we're living in this sort of chaotic world in which

37:48.880 --> 37:54.480
each particular affects every other throughout the universe basically, then strategic thinking

37:54.480 --> 38:00.560
takes on a different tone, especially in terms of it's committing people to long term strategies,

38:01.680 --> 38:08.560
design of products, managing people, all of these things, and more, it seems to me

38:09.680 --> 38:16.560
can get rethought in the light of particulars. And then moving from business to life, I know

38:16.560 --> 38:21.840
that business is part of life, but broader. So we talked, I talked about morality,

38:24.080 --> 38:29.680
excuse me, but exactly the same sorts of considerations that you have to look at

38:29.680 --> 38:35.440
the particulars I think holds for every decision that we make where we realize we're making a

38:35.440 --> 38:39.600
decision. So there are decisions I'm making about waving my hands now, which I'm not doing

38:39.600 --> 38:44.480
with any conscious awareness of, but decisions that we actually deliberate on even a little bit,

38:44.480 --> 38:48.560
whether it's what items to pick up from the lovely buffet for breakfast,

38:49.200 --> 38:54.400
too much more important things. Decision making is also, like morality,

38:55.680 --> 39:01.680
all in the particulars. I mean, the muffins looked really good, the croissants look good,

39:01.680 --> 39:06.160
but were they? I need to crinkle them first, and then I'm going to do sort of a carb,

39:06.800 --> 39:14.080
sort of a balance, and what did I have last night? It's particulars, all decision making, and,

39:14.080 --> 39:25.040
you know, love. We don't love our loved ones in general. I don't, that's a very weird thought.

39:25.040 --> 39:28.640
We love them for who they are, and who they are as particulars, and that

39:29.600 --> 39:40.720
year and a half old baby is extremely particular in both ways. So I think this line of thought,

39:40.800 --> 39:45.280
this is actually what I've been working on, and more or less consumes me,

39:46.480 --> 39:51.520
is how this changes our ideas about creativity, which is particularly relevant in the age of

39:51.520 --> 39:58.560
generative AI. I think it changes our ideas about free will, which I know is not something that we

39:58.560 --> 40:05.040
generally talk or even think about, but it is a background concept of considerable lineage in

40:05.040 --> 40:12.960
the West. And I think the model that we are seeing, the light that AI is casting on a role,

40:13.600 --> 40:20.400
should have us rethinking the age-old impossible argument about free will. Knowledge for sure.

40:21.360 --> 40:25.920
I hope I don't have to say anything more. I mean, what does it mean to know in an age where

40:25.920 --> 40:32.080
particulars dominate where we will continue to want to generalize? Of course, there's a

40:32.080 --> 40:37.600
rhythm here, but one in which the particulars have become more dominant and recognized than they

40:37.600 --> 40:45.440
have is, I think, a really important question. And I actually think that many of you here at

40:45.440 --> 40:51.840
KM World are managing that question, although not in the terms I'm trying to push on you.

40:53.600 --> 41:00.480
I'm going to guess that much of your life as KM people is, in fact, engaged very practically

41:00.480 --> 41:06.480
with this question. Mind and body, I'm going to guess, is not a burning, you know, the relation

41:06.480 --> 41:11.520
to mind and body. It's probably not a burning question for everyone, but it's, again, a key

41:12.480 --> 41:16.240
shaping and background thought in the Western culture. And even the nature of reality, which

41:16.240 --> 41:22.400
has become, to everyone's surprise, it's not a topic that has been on the top 10 list in philosophy

41:22.720 --> 41:29.760
for a while, but with the rise of simulations suddenly, thanks to AI, and the questions in

41:30.560 --> 41:37.920
many of the questions of simulations also have to do with how particulars show themselves,

41:38.800 --> 41:47.520
rather than about big generalizations. So, finally, data in light of AI.

41:48.080 --> 41:53.520
I think that we recognize it's a human artifact. It's something we make. It's

41:53.520 --> 41:58.480
something we participate in making anyway, by the decisions that we make, and then what we do with

41:58.480 --> 42:04.560
that data as well. We can't just accept it as we did in the 1950s, which was part of the weight

42:04.560 --> 42:10.960
that was on that poor, who was the guy in that, the actor on the left?

42:11.040 --> 42:11.680
Gregory Peck.

42:12.800 --> 42:13.200
Is that good?

42:14.640 --> 42:16.240
Exactly right. Gregory Peck, thank you.

42:19.520 --> 42:23.840
I forgot where I was going, but I now know it was Gregory Peck. Appreciate that.

42:28.720 --> 42:38.320
Anyway, so it's, Gregory Peck took data as the bedrock, that is, the IBM generation.

42:38.320 --> 42:42.960
Took data as a bedrock. Now we see how much of human decision making, how much of human

42:44.880 --> 42:53.120
assumptions plays in the creation of data, and how we use it in AI especially, directly,

42:54.000 --> 42:59.280
is the responsibility of humans who make decisions about it, even if they shirk those decisions.

42:59.280 --> 43:04.960
And thankfully, most people that I know who are doing AI take those decisions really seriously

43:04.960 --> 43:10.240
now, but it's still an enormous problem. Seeing data as a black box of relationships,

43:10.240 --> 43:18.960
from which we withdraw what we need as human artifacts. And I think the pithiest way I can put

43:18.960 --> 43:26.720
it is, if particulars are becoming more important to us outside of the world of AI, but in part

43:26.720 --> 43:34.240
because of AI, then I think we can think about data as particulars. But there are particulars

43:34.240 --> 43:37.840
that have been rendered machine learning. They gain something from that, I'm sorry,

43:37.840 --> 43:41.360
from machine readable. They gain something from that, but they also learn something.

43:41.360 --> 43:49.040
They gain some capabilities from that. So I think we're in a world in which

43:51.760 --> 43:57.760
particulars are rising. I'm not sure if it's obvious. I think this is a really important

43:57.760 --> 44:05.600
corrective to a Western focus on generalizations. It's because particulars are a reality.

44:06.960 --> 44:08.960
So, thank you very much.

