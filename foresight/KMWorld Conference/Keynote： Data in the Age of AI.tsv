start	end	text
0	8080	I want to introduce our first keynote speaker this morning who's going to talk to us about
8080	16720	data in the age of AI. David Weinberger is Harvard's Brookman Klein Center for Internet
16720	23600	and Society. He's participated in with that group for many years. He's an author of quite a few books,
24480	31440	Everyday Chaos, Everything is Miscellaneous, Too Big to Know, and many years ago the Clutrain
31440	40640	Manifesto. But he's also a columnist for KM World, so he keeps his eye on what's happening
40640	48800	in the KM World. He has spoken at KM World a number of times on many different topics,
48800	53280	but he hasn't for a while, so we're very happy to have him with us this year.
54000	57520	And please welcome David Weinberger.
64560	74800	Thank you so much. I guess I can take this off. It's wonderful to be back in this community
75440	81600	and amazing and wonderful to have participants from 38 countries, and I would now like to welcome
81600	88160	all of them in each of their 25 different languages, so just bear with me. Good morning,
88160	92800	which is the one I've really mastered, and I'll move on from there. Sorry, I'm an American.
93680	100240	So I want to talk about data in the age of AI, and I should warn you that this is a topic,
100320	105120	it's part of a larger topic that I've been trying to work through for the past few years,
105120	107440	and you'll see I have not fully worked it through.
110560	114400	Whoops, sorry, I've got to get the right clicker. The right clicker.
116400	123840	I found the right clicker. I'm not looking for your approval, but still. Thank you. Thank you very
123840	129120	much. So I'm also going to include metadata, because metadata is the type of data,
130960	135920	and it's really important from the beginning to me that you understand this is not a technical
135920	140240	talk, it's not a talk about the technicalities of data, it's an attempt to try to follow
141840	149280	how we think about data, not as data scientists, which I absolutely am not, sorry, humanities major,
149280	157600	and how that might be affecting how we view our world. So don't be alarmed at all the technical
157600	164320	errors, and I didn't fill in everything and the like. Sorry, enough disclaimers? So good. So this
164320	172240	is Martin Heidegger, a German philosopher who died in 1976. In 1954, he wrote an essay called
172240	177520	The Question Concerning Technology, except it was in German, and in it he says in a typical
177520	186240	Heideggerian way, technology discloses being. Heidegger was a terrible, terrible writer. He was
186240	193440	also, please do not look him up in Wikipedia. He was a horrible, horrible person, and I feel
193440	198000	ashamed even introducing him. I did do my doctoral dissertation on him, but that was a long time
198000	204480	ago, a very long time ago. Anyway, so my understanding, my way of understanding this is that our
204480	214000	engagement with technology casts a light on the world. We see our world through what technology
214000	222320	shows us of it, but it also casts a shadow, which is very prominent in how AI, which is certainly
222320	228800	a two-sided technology when it comes to good and evil. It's certainly true of AI. So the sort of
228800	233040	thing that I think he's talking about, but I don't really care because I want to talk about it, and
233040	240480	it's not a Heidegger lecture, is that say in the 17th and 18th century, where the pinnacle technology
240480	248880	was watches, which were amazing creations, incredibly complex, handmade, just mind-blowingly,
248880	252720	they were the chat GPT of the time. It was unbelievable that these things could work,
253440	258320	but because they were the pinnacle technology, the dominant technology, we began to see the
258320	264480	universe through that lens, so to speak. So the entire universe, we started talking about the
264480	268960	clockwork universe. The universe itself started to look like a clockwork, which meant that there
268960	277040	were very simple rules that govern it and govern it perfectly, beautifully. This was aided and abetted
277040	284160	by Newton's laws, which were the mechanisms, the mechanics of the universe. So that's an example
284240	290560	of how we interpret our world through our tech. And so there's, I think, a big question that asked,
291200	297520	which is, okay, so how are we going to interpret our world in the age of AI? And I'm picking a subset
297520	307120	of it, which is in light of data. So I'm going to begin with a really quick overview of starting
307120	314160	with mainframes working up to the internet and then AI, about how data has affected the sort of
314160	320720	public idea of data has affected our view of how the world works. So starting with mainframes,
322000	330160	mainframes, in the age of mainframes, data was really, really scarce, and it was a resource.
330160	334320	It had to be carefully managed. And just as with the punch cards of the day,
335280	343360	data was structured and it was a reduction. A typical human resources record was really
343360	347600	sparse. There was hardly any information in it because of the capacity of the computers.
347600	352160	And it was, of course, completely regular. Everybody's record was structured the same way.
355280	361520	I don't think it's a coincidence that one of the large cultural divides of the time was between the
361520	369840	IBM rep in the blue suit, absolute symbol of conformity with the rebel, the symbol of rebellion
369840	377680	here. And so it looked like our culture was divided into two parts. In the 1960s, this blue suit guy
377680	383280	remained, but the angry beaten it got replaced by a stoned and overly happy hippie. But it was the
383280	393520	same sort of tension. And so it was the tension between conformity, individual control and spontaneity
393520	399520	and reduction of information so we could manage it and part of control and wild excess often for
399520	405200	its own sake. As I mentioned oddly to somebody just a few minutes ago, this does not come up very
405200	413360	often, I was at the original Woodstock. So I'm talking from experience here. Oh, my mastering a
413360	418320	remote doesn't count for anything, but Woodstock gets applause? What sort of group are you?
421200	429120	So age of computers of PCs, as we all know, I think, what drove, well, those of us who lived
429200	435840	through the era can confirm that what drove the adoption of PCs was spreadsheets, killer app,
435840	441520	absolute killer app drove the hardware and spreadsheets present data in a matrix. Of
441520	449280	course, I'm you're all familiar with spreadsheets. There's a model of in this case how the business
449280	457280	works and the relationship among the pieces of that model. But all the way back in 1984, Stephen
457280	466000	Levy, who's a remarkably good writer about tech, he's still writing very a lot, always interesting.
466000	475440	1984, Levy said that what was actually interesting and important about the adoption of spreadsheets,
475440	481600	reason people were adopting them so enthusiastically wasn't because they modeled business, it was
481600	486720	because you could play with that model. It was the what if factor. And I think he correctly says,
486800	493280	in 1984, did I mention it was 1984 when he said this? He said that it's already this ability to
493280	497680	alter the model to play with it was already changing organizational structures. They're
497680	505440	incredibly early and insightful. So I think there's a tremendous amount of truth in this.
505440	511680	And so I'm going to pretty arbitrarily say that if you want to talk about data, the change in data
511680	518480	in the era of PCs, it goes from this controlled minimize thing to still pretty minimal amount
518480	524480	of data that it handled. But we want to play with it. And so data becomes detached, it's not
524480	528880	just a read out of the world. It's a read out of the world that you can you can mess around with,
528880	535040	you can try other worlds, you can try other forms of your business. And this liberates data from
535040	542640	feeling like a direct read out of the world that data are facts that we can just read out and they
542640	551200	are the reality. So then there's a sort of a micro era of big data, where one way of thinking about
551200	558000	this is the data becomes a source of surprises. And the canonical example of this is the discovery
558000	563120	that people who buy diapers also often buy beer. There's a correlation.
566640	576960	That's our granddaughter. I know. Anyway, she doesn't drink beer. I see where I went wrong
576960	584960	in this slide. We are not giving her beer. She doesn't even like beer. I mean, she likes it,
584960	594160	but she's not crazy about it. Oh, Poppy, I'm sorry. Anyway, so this actually, it doesn't
594160	599040	matter. It's a good example, but it's not a real example. It was actually a correlation that was
599040	603760	discovered in 1992, which is generally before the era of big data. And it was not discovered through
603760	608960	deep data analysis. Somebody noticed that people buy a lot of diapers and they did a sequel and
608960	613920	ask you L query to see what is correlated with that. It's beer. That turns out to be
613920	621120	an unlikely in reality, not really a helpful correlation. Although there is some debate
621120	628080	about it. Okay. So data in the age of the internet, the next two sections are going to get a little
628080	634240	bit longer. Sorry. So internet and AI. So in the age of the internet, data gets really confusing
635200	641280	because it's one sense. It shows up as a type of smog that we omit, which is an unfortunate image,
641280	647440	I guess. But as we're browsing and doing all the other internet things and corporations are
647440	652080	gathering all that data and compiling it and manipulating it and using it against us,
653760	659760	it was the overwhelming amount of data was so much that it actually seemed at times not to
659840	665840	clarify things but to make where to even start unless you were a professional data analyst
665840	670800	like at the platforms. So that's, I'm going to give you a few different ways, I think,
670800	676560	of characterizing data in the age of the internet. At the same time that it felt like smog,
677280	683360	so much and dangerous, the internet is all about links, which is sort of up from the data level.
683360	688160	It's more at the information level, if you want, where links are just about the opposite. I mean,
688160	694560	links are obviously and deeply human and form a structure that we can navigate at will and
694560	702640	notice how things are connected. Very different from the smoggy idea of data. And so I think
703760	711760	we've gotten a similar sort of polarity in which there's the smoggy. We all, I think,
711840	719680	maybe not. Most of us are concerned, let's say, about the use of the data that we are
719680	726480	surrendering unknowingly. Again, we click yes in order to, you know, that we've surrendered
726480	731040	unwillingly. We don't know exactly what's being captured, but we know that it's being used to
731040	738400	influence our decisions by people who don't have our interests usually as their interests. And on
738400	745040	the other hand, links, which is 100%, individuals who are able to control and connect with others
746160	750240	in an intensely social way, and both those things go on at the same time. And I think a lot of us
750240	759680	have this sort of divided understanding of data in the age of AI. But we also saw, if only because
759680	765040	of the gigantic amount of data that suddenly was there and seemed useful, we started unstructuring
765040	771520	our databases. We did this in all sorts of ways, a semantic web with linked open data from Tim Berners-Lee,
771520	781760	but also on structured databases like MongoDB and in graphs and data lakes and JSON,
781760	789120	this enormous unstructuring of databases, which has enabled a lot. Okay, so I want to take,
789600	797680	it's not really a detour, but I don't care. It's a detour. It's not about metadata in the
797680	802480	age of the internet. It's not a detour because it actually reinforces the same point. So
804160	810400	as you know, have you all tried a search engine? Because they're really amazing. If you haven't
810400	814880	tried a search engine, oh, I'm telling you, you really should. There's like a guluan, a bing. So
814880	820880	then you know that if you don't know who wrote Moby Dick, you can ask, it'll tell you it's Melville.
820880	825440	If you don't know what Melville wrote, you could ask, it'll tell you Moby Dick and some other stuff.
825440	829360	If you don't know either of those things, you can ask it about the content of Moby Dick.
830640	834640	And you can even misspell the content. Call me Ishmael. Where is that? It's in Moby Dick
834640	840160	by Herman Melville. So each of these questions contain a piece of metadata that linked to data.
840720	852240	And that destructures metadata, which used to be a label of fix to things. And so it turns out,
852240	857600	as we have discovered, that the difference between metadata and data is only functional,
857600	863280	only operational. It's how you're using it. Because so that metadata is what you know. Well,
863280	868160	you know it was by Herman Melville, but you don't know what the book is. And data is what you're
868160	873280	looking for. Oh, it's Moby Dick. That's the only difference between them. So these, which is an
873280	880560	incredibly powerful and liberating thing when everything can be metadata for something else.
880560	886400	And then in turn be the thing, the data that some other metadata is looking for. So this is why it
886400	893280	wasn't actually a detour. It's a, we're seeing the unstructuring of metadata, just as we are
893280	901120	seeing, have seen already the unstructuring of data. So now let's talk about AI, where again,
901120	907760	I'm going to have a few ways of characterizing it in a phrase, a bumper sticker. So the first is
909040	916320	data is generative in the age of AI, right? And we all, we all know this because we know
917040	925280	that in the old days before AI, a spreadsheet or any other sort of program generates data for sure.
926480	932880	But it generates it because humans have constructed the model. Whereas with AI, as we all know,
932880	938640	when I say AI, I really mean machine learning. I hope that's, I should have said that, but now I have.
939200	947360	With AI, data creates the model, which is insane and seemed completely implausible until, you
947360	953920	know, about 15 years ago, we found out, oh, yeah, that works. We can get more accurate classification
955520	959840	of objects in photos that way, even though it makes no sense. We're not going to tell
959840	963920	it anything at all about what we know about objects and how you recognize them. We'll just
964000	972320	give it data. It's insane, but it works. So data creates the model, the model then generates
973680	978800	new data, but it is remarkable that now data is generative this way.
981520	986560	Although I've already mentioned chat GPT once and thus have fulfilled my legal requirement,
986560	993600	I'm going to mention it again. So I asked it, this is about metadata. So I asked it the other day,
994320	999200	Dante's Inferno has three levels. Are there any other, give me five other artworks that have three,
1000160	1005200	that's, you know, a trio of things. And it did a good job. It's unbelievable. It's amazing.
1006160	1010960	It's amazing because we don't have a sense of scale. Humans can't, you know, at least I can't
1010960	1014800	think it's scale. So I'm amazed and surprised by what scale can do. But it is, of course,
1014800	1021680	incredible. It gave me five. It gave, they're good. They gave pretty good explanations of why,
1021680	1029600	which I hadn't even asked for. And so if we think about this in terms of metadata, the metadata in
1029600	1036160	my query was artworks that show something in three parts. And the data that it fetched was
1036160	1043600	that text that you just saw. And what is, I think, amazing about this is the metadata now is generating
1043600	1052000	data. The metadata generated that content. It's not something I've seen before. And if we have,
1052000	1056480	I'd be really interested in some other field or some other way. I'd really like to know about it.
1057360	1064080	This is, this is mind blowing. We have metadata that will generate its own data. And generally,
1064720	1070480	it's good. We can't, it doesn't know when the data isn't good. Basically, as you know,
1070480	1076400	basically every, not basically, everything that chat AI says is an hallucination. It's just that
1076400	1082560	most of those hallucinations are true hallucinations. It doesn't know that the ones that it's making up
1082560	1088000	and we often have trouble telling. So that's a terrible problem. So let me give you an example.
1090080	1096560	So in January 2022, researchers at the University of Leeds and some other institutions published a
1096560	1104720	paper that said we built a model from retinal scans and some really basic medical information,
1104720	1111520	like age, weight and the like, really a very small set of it. Does anybody know about this?
1113440	1120480	It's sort of mind blowing. Because it works. I'm sorry. Let me be more precise. What works is
1121280	1128160	the AI is able to predict with some degree of accuracy the likelihood of an individual
1128160	1139200	having a heart attack, myocardial infarction, based upon the retinal scan. Nobody, data scientists,
1140000	1146480	AI people, doctors of all stripes have tried to figure out what about those images, presumably,
1146480	1153280	but who knows? Presumably, it's the veins, but we don't know. We can't figure out how it's doing it.
1154240	1159920	It is, at the moment, inexplicable. And I know that there are actually bunches of people here
1159920	1167520	working on making machine learning less inexplicable at dinner with Beth Truden and a bunch of other
1167520	1172800	people at that table last night where this was a lively topic of conversation and Beth's company
1172800	1180560	has a way of keeping the sources, the citations and sources of the knowledge
1181280	1187920	with the output and is generating it from a more fact-based and reliable set of information.
1187920	1194240	Is that approximately right? Okay. It's approximately right. There's tons and tons of work in all
1194240	1204480	areas to try to make AI less inexplicable. But as it stands, let me put it like this,
1204480	1210560	that inexplicability is one of the two original sins of AI, by which I mean,
1212400	1220000	and I know the sentence doesn't actually make sense, but it may make sense. You'll be the judge.
1220000	1225680	So left on its own, AI would tend towards inexplicability. There are interventions and
1225680	1230480	structurings of all kinds that we might be able to do to prevent that, but it doesn't care if we
1230480	1236320	understand that has not been its mission. It's this mission has been to give accurate predictions
1236320	1241360	based upon data guided by a ton of human decisions about what we meet, what we're looking for and
1241360	1251680	what we will accept. So inexplicability is pretty common so far in AI models. Beth, how much trouble
1251680	1257920	am I in? You'll tell me afterwards. Okay. So in this regard, I think a second formulation of data
1257920	1269360	in the age of AI would be to say that, okay, we'll do it by hand. I have forgotten how to use my
1269360	1283360	clicker. A few moments, it was great. Okay. That AI, data in the era of AI is a source of secrets,
1283360	1288160	like, oh, there's secret information in a retina that can let us see what's going on with the left
1288160	1294160	ventricle, which is an indicator of heart health. Didn't know it. It's there, but it's secret. But
1294240	1302400	actually, I would think I would prefer the formulation that data in the era of AI is a
1302400	1306800	keeper of mysteries, because secrets, once you know them, generally, you know why you know them,
1306800	1313280	how you know them, why they're true and all that. When mysteries, you know, but it remains a mystery
1313360	1321040	how it happened. And so far, that seems to be at least some of what AI does.
1322160	1327040	So let's for the moment, we're going to just overstate and say AI tends to be inexplicable
1328320	1336000	at the moment. And we can argue later, we won't. And then we can play the five-year-old game.
1336000	1341200	Now, the baby you saw, we do have a five-year-old grandchild as well. So that little drunken baby
1341200	1346480	that you saw would not be doing this, but she will be in a few years. So you can ask why. Just
1346480	1351360	keep asking why. So why isn't it inexplicable? And the answer is because the model is just too
1351360	1355840	complex. Okay, why? Why is the model too complex? Because there are too many factors, there are
1355840	1360960	too many variables, there's too much data that's connecting to too many others. You know, ChatGPT
1360960	1368960	has 175 billion parameters, which are weights, weighting the relationship, the importance of the
1369680	1377200	relationship of words, 175 billion. So if you want to know why it chose one word over another,
1377200	1382960	why it called a house you're looking at luxurious rather than upscale, you're never going to know,
1382960	1386320	at least at this point, you're not going to know, there's just too much going on there.
1387120	1393360	So, okay, well, why are there so many factors connecting to too many others? Well, that's a
1393360	1402960	good question. Because actually, that's how the world works. The world is really complex. I mean,
1402960	1409840	the universe is the single most complex thing in the universe. And it is really, really complex.
1409840	1414960	And the reason that these complex models work is that they are capturing something about the
1414960	1420960	complexity of the world that we live in. They go wrong in all sorts of ways, they're dangerous and
1421040	1426640	how they go wrong. But when they work, they're capturing something mysterious about the world
1426640	1432400	we live in, which is there's so much stuff and everything affects everything else all the time
1432400	1442240	forever, everywhere, everything, everything. It's simultaneously. It's not like a clock. If only
1442320	1451520	it was like a clock, but it's not. It's intensely beyond imagination complex in its interrelationships.
1452560	1458400	Okay, so why didn't we notice this before? And lots of people have, I think we all have,
1458960	1466400	it's not news that the universe is complex, but it doesn't register. And I think it's
1466400	1470880	for two reasons. So, I mean, the sort of complexity I have in mind is, and this is a relatively simple
1470880	1479040	case, what determined why those people were going to be in the crosswalk with you this morning,
1479040	1486480	exactly where they are. There's no hope of figuring that out. It's way, way too complex. It's too
1486480	1494160	complex for any one person. So why haven't we done anything? Why haven't we come to grips with this?
1494160	1498080	Why isn't this the baseline of all of our thought and thinking? Well, I think for one thing,
1498080	1502880	it's because we couldn't do anything with that sort of information. And so we just ignored it.
1503680	1508800	Generally, we said, well, no, that's an accident. It's just, oh, who knows? It's chance. It's
1508800	1514560	coincidence. We have a whole vocabulary for dismissing complex, the results of complex
1514560	1521040	interactions is not worth our attention because we couldn't do anything with it. And second of all,
1521040	1527360	because it didn't fit well with our old human models. So I'm using our here as the West.
1528320	1535200	I just have to limit my domain because it's basically the only thing I know enough about
1535200	1543760	to be talking about, any legs to stand on. So in the West, this view does not fit very well
1543760	1550080	because for thousands of years, if we take the Greek ancient Greece as the origins of
1550080	1556960	Western civilization, which is controversial, but traditional to do that. Back then, the idea was
1556960	1560560	there's all this mess in the world that seems chaotic, but underneath it, there are laws and
1560560	1564720	the laws are simple and understandable. There are laws, there are rules, there's universals,
1565280	1571200	there are principles, there are overall, there are generalizations. That's what we hang on to
1571200	1579120	because we can use those. So in the West, traditionally, we have viewed generalizations
1579120	1585760	of various forms. They explain what's explained, which is the particulars. This is a chart of why
1585760	1594080	we have preferred the general because they are generalizations reduce information. They're simpler
1594640	1598640	and it just so happens that the laws of the universe happen to be simple enough for humans
1598640	1604320	to understand what a coincidence, but they simplify something that's very complex, the realm of
1606000	1612400	particulars. We have thought that these laws are eternal. They've always held. They explained
1612400	1616080	everything going back to the Big Bang, even before we knew about a Big Bang.
1617440	1622560	And ultimately, they are the truth. In our tradition, the Western tradition, we have
1622560	1627920	looked up into the skies for the eternal truths. In the case of the ancient Greeks, more or less
1627920	1635280	literally, we still do it. We valorize the eternal over the particulars, which are sort of just,
1635280	1640080	they're over like that. They change all the time. There's no real abiding truth in them.
1640400	1648880	But I think that we are entering an age with the age of AI. For me, I think the most important
1648880	1655840	sort of change in how we view things is that we are getting a more particularized view. We're
1655840	1665040	taking particulars more seriously. We are letting them have voice with models themselves. I should
1665280	1670800	pay attention to my slides. The models themselves, machine learning models, being literally
1670800	1675760	generalizations. But they're generalizations that are made up of patterns that have been derived
1675760	1680000	from particulars. Those patterns can be so small in particular that there are billions of them
1680000	1684560	that get sorted through and generalized in various ways. But they stay true to letting
1684560	1692240	the particulars speak. And this is why machine learning can make, can sort animal photos better
1692240	1700160	than handwritten code has been able to. Okay, so if we are going to get more used to the idea of
1700160	1706800	particulars as being real and important and in some ways determinative and having their own voice,
1707440	1714800	then how will that happen? And I think here it may be because of the light that is cast
1715760	1723040	by, by the shadows that are cast by AI. It's our fear. So I want to give two examples.
1723040	1727920	One is a common sort of fear gets expressed variously, but we don't know how it works,
1727920	1733760	which can be genuinely scary and important to recognize, right? But it, there's two words at
1733760	1738880	the end of the sentence that are really crucial. So when you hear, we don't know how it works,
1738880	1747360	we also hear it works, which is amazing. So this fear may be moving us towards an embrace
1747360	1753040	of the particular because we then hear when we ask, well, why don't we know? We're told
1753040	1758960	because it's way too complex. Well, in hearing, we don't know how it works. We may be being led to
1758960	1764640	believe, not on purpose, but being led to believe that it works because the world is also wildly,
1764640	1773120	complexly particular. So I want to give you a slight example of what it might mean to understand
1773120	1780400	something outside of the realm of AI in terms of particularity. So in this case, I picked a simple
1780400	1784080	little topic, morality. I think I have like three slides and we'll be done with morality and that
1784080	1788880	will be great. So typically in the West, traditionally in the West, we have had, we've resorted to
1788880	1795920	ethical frameworks. So religious framework says, do what God commands you to do. Reason one says,
1795920	1802400	do follow principles that are based in reason. And then the utilitarian one says, the framework says,
1802400	1806160	do that which will bring the most happiness to the most people. Pretty rough, but you know what I
1806160	1810480	mean. So it's a framework. It tells you why some things are good and some things are bad. And it
1810480	1816000	tells you what to do in particular situations, except they don't. They don't work. I'm going to
1816080	1825520	give you the world's briefest and simplest example, I think. So you're you, you have a friend, A,
1825520	1831600	who is angry, tells you that they're angry at your mutual friend, B. But A says, but don't tell
1831600	1837520	anybody about it. Shortly thereafter, your mutual friend C comes along and says, oh, I'm very excited.
1837520	1843520	I'm putting together the seating plan for our wedding. And I'm going to put C here. I'm going
1843760	1847360	I know A and B are such good friends. I want to put them next to each other. Do you tell
1848800	1854240	do you tell C that that's a bad idea because of the fight? I don't know. And I'm going to say,
1854240	1858720	neither do you, because we don't know the particulars. You're going to think about this,
1858720	1865600	and you're going to think, well, yeah, how vindictive is A? Is A forgiving? Would A understand if I
1865600	1872080	didn't explain why? How deep is the rift between A and B? How how upset is C going to be if there's
1872080	1877920	some minor tension between two people at their wedding and so forth? Without that, you don't
1877920	1883840	know what to do. And considering what to do morally, you have to think about the details. In fact,
1883840	1889520	if weirdly, the next next week, you run into different set of friends who have exactly the
1889520	1893360	same sort of formal thing that one's angry and don't tell and the rest of it, but it has
1893360	1897600	their different people and it's not a wedding. It's whatever. It's a camping trip.
1898240	1904800	What you did in the first case is not going to help you decide what to do in the second case.
1904800	1909680	You've got to rethink the particulars all again. That's what you would do. You would say, well,
1909680	1915440	I didn't, I violated my promise to A because A is forgiving, but C, oh, C would never forgive me
1915440	1921920	if I did that, for example. It comes down to particulars. So the philosopher Martha Nussbaum,
1921920	1927440	1999, in a book called Love's Knowledge, which is a serious philosophy work with one of the
1927440	1933200	great titles and very apt title. Anyway, she talks about this and she says moral situations
1933200	1939120	are not commensurable. You can't compare them. And that's because they are so particular.
1939120	1947840	So the second example is of a fear that may be telling us inadvertently what the world under AI
1947840	1954960	type of data is. So it's like what the light is that's being shown. So AI is biased. It is biased.
1954960	1964800	It is the second of its original sins. Left on its own, AI will be biased. We have to guard against
1964800	1970400	it. It's very hard to guard against it, as I'm sure everybody here knows, but we can ask, okay,
1970400	1975840	well, why is it biased? We're told this and then it's explained to us why it's biased because
1975840	1983040	data tends to reflect societal biases and the like. And what we hear from this is, oh,
1983040	1988240	oh, we get to select the data. So data is human stuff. It's not a readout of the world. It's
1988240	1994560	not the facts about the world. Facts aren't exactly facts either. Different topic. It's stuff that we
1994560	2001280	decide, data is stuff that we read off of meters, meters that we have decided to plant. Where and
2001280	2005840	why and to what degree of accuracy and what we do with the data are all human decisions.
2006960	2009760	If I say data is human stuff, I don't mean it's
2012240	2017520	unreliable, but I sort of mean it's unreliable that there's a human element of decision
2017520	2023360	which includes unconscious biases and occasionally conscious, but generally the issue in AI is
2023360	2030720	unconscious, unaware biases of what's going on, how what we're doing might be taken in any of the
2031280	2034960	38 other countries other than the one that we happen to be in right now, for example.
2036480	2041760	Data is human. It's human stuff, obviously. Okay, so how does that lead to anything? Well,
2041760	2047840	it may be that in the discussion of bias, we get to the point of proxies because the first
2047840	2052160	response is, well, let me just don't record if you worried about bias against women or whatever
2052160	2057360	protected class or whatever you want to call it, then don't collect, don't have a column for women
2057360	2062240	and then you have to explain, well, no, but there are proxies for women in this case.
2064320	2073360	That's, proxies are really interesting, at least I think so, in part because it's proxies work by
2074960	2079120	putting shape around the whole, the thing you're trying not to have affect your,
2079120	2083120	it's like the missing piece in the jigsaw puzzle. You can see what that shape is and that can be
2083280	2088240	part, that becomes in a sense part of the data. I know I'm not putting this technically correctly,
2088240	2093200	but I'm trying to talk about how this will perhaps appear to people who are not technical.
2094640	2102480	And there are only proxies because things are so interrelated. They're so interrelated that the
2102480	2112560	absence of something can be the presence of something. That's maybe one way through this fear
2112560	2119520	justifiable and terrible and correctly terrible fear of bias. You don't have to, I got myself
2119520	2125360	started on it, I'm going to back away from it. That this, the explanation of it, the understanding
2125360	2131600	of the first question, well, why is there bias? Why can't you just lead you to understand the deep,
2132160	2138960	deep, multi-dimensional interrelationship of all of the data and all of the world?
2139520	2144160	So if we ask what, the original question of this talk, which is,
2146400	2151520	if tech casts light and shadow on the world, can we think about how our world is already beginning
2151520	2161120	to look in the light of this change in data? So way too short what I'm about to say, I understand
2161120	2164960	that, but it's just to give you a sense of it. So if you look at an enterprise in light of particulars,
2165920	2171120	I think that we begin to see black swans, you all know black swans, you all know black swans,
2171120	2179360	the unexpected things that happen, drop out of the sky and destroy a supplier's factory
2179360	2183200	in your supply chain and suddenly your business is in great danger.
2184640	2192320	Literal lightning struck. Yes, that's right, I think there are black swans, but look that through
2192320	2198000	in this light, in the light of AI and data, everything's a black swan. That's the nature of
2198000	2206640	particulars. You can't, it's not just the big events, everything that happens is a mashup of
2206640	2212960	everything that happens all at once. So everything's a black swan and except some things are
2212960	2218080	butterflies in the chaos theory sense in which other butterfly alights on a plant in Brazil and
2218080	2224480	triggers a tornado in Kansas, right, the standard example. It seems pretty implausible, but the
2224480	2232080	idea is very validated and sound, which is that a small event can create cascades by which it picks
2232080	2239600	up energy and has surprising and important results. And by the way, it's really hard to go backwards
2239600	2245120	from the tornado to pin it on that damn butterfly and sort of pin it to a wall for what it did.
2245840	2249920	That would be, often that's what we're trying to do with AI to understand it.
2251440	2258240	Okay, so more specifically, but still not very specific, this sort of thinking I would imagine
2258240	2263920	should have important effects on really important business topics beyond pinning butterflies,
2264480	2268880	including strategy, obviously, if we're living in this sort of chaotic world in which
2268880	2274480	each particular affects every other throughout the universe basically, then strategic thinking
2274480	2280560	takes on a different tone, especially in terms of it's committing people to long term strategies,
2281680	2288560	design of products, managing people, all of these things, and more, it seems to me
2289680	2296560	can get rethought in the light of particulars. And then moving from business to life, I know
2296560	2301840	that business is part of life, but broader. So we talked, I talked about morality,
2304080	2309680	excuse me, but exactly the same sorts of considerations that you have to look at
2309680	2315440	the particulars I think holds for every decision that we make where we realize we're making a
2315440	2319600	decision. So there are decisions I'm making about waving my hands now, which I'm not doing
2319600	2324480	with any conscious awareness of, but decisions that we actually deliberate on even a little bit,
2324480	2328560	whether it's what items to pick up from the lovely buffet for breakfast,
2329200	2334400	too much more important things. Decision making is also, like morality,
2335680	2341680	all in the particulars. I mean, the muffins looked really good, the croissants look good,
2341680	2346160	but were they? I need to crinkle them first, and then I'm going to do sort of a carb,
2346800	2354080	sort of a balance, and what did I have last night? It's particulars, all decision making, and,
2354080	2365040	you know, love. We don't love our loved ones in general. I don't, that's a very weird thought.
2365040	2368640	We love them for who they are, and who they are as particulars, and that
2369600	2380720	year and a half old baby is extremely particular in both ways. So I think this line of thought,
2380800	2385280	this is actually what I've been working on, and more or less consumes me,
2386480	2391520	is how this changes our ideas about creativity, which is particularly relevant in the age of
2391520	2398560	generative AI. I think it changes our ideas about free will, which I know is not something that we
2398560	2405040	generally talk or even think about, but it is a background concept of considerable lineage in
2405040	2412960	the West. And I think the model that we are seeing, the light that AI is casting on a role,
2413600	2420400	should have us rethinking the age-old impossible argument about free will. Knowledge for sure.
2421360	2425920	I hope I don't have to say anything more. I mean, what does it mean to know in an age where
2425920	2432080	particulars dominate where we will continue to want to generalize? Of course, there's a
2432080	2437600	rhythm here, but one in which the particulars have become more dominant and recognized than they
2437600	2445440	have is, I think, a really important question. And I actually think that many of you here at
2445440	2451840	KM World are managing that question, although not in the terms I'm trying to push on you.
2453600	2460480	I'm going to guess that much of your life as KM people is, in fact, engaged very practically
2460480	2466480	with this question. Mind and body, I'm going to guess, is not a burning, you know, the relation
2466480	2471520	to mind and body. It's probably not a burning question for everyone, but it's, again, a key
2472480	2476240	shaping and background thought in the Western culture. And even the nature of reality, which
2476240	2482400	has become, to everyone's surprise, it's not a topic that has been on the top 10 list in philosophy
2482720	2489760	for a while, but with the rise of simulations suddenly, thanks to AI, and the questions in
2490560	2497920	many of the questions of simulations also have to do with how particulars show themselves,
2498800	2507520	rather than about big generalizations. So, finally, data in light of AI.
2508080	2513520	I think that we recognize it's a human artifact. It's something we make. It's
2513520	2518480	something we participate in making anyway, by the decisions that we make, and then what we do with
2518480	2524560	that data as well. We can't just accept it as we did in the 1950s, which was part of the weight
2524560	2530960	that was on that poor, who was the guy in that, the actor on the left?
2531040	2531680	Gregory Peck.
2532800	2533200	Is that good?
2534640	2536240	Exactly right. Gregory Peck, thank you.
2539520	2543840	I forgot where I was going, but I now know it was Gregory Peck. Appreciate that.
2548720	2558320	Anyway, so it's, Gregory Peck took data as the bedrock, that is, the IBM generation.
2558320	2562960	Took data as a bedrock. Now we see how much of human decision making, how much of human
2564880	2573120	assumptions plays in the creation of data, and how we use it in AI especially, directly,
2574000	2579280	is the responsibility of humans who make decisions about it, even if they shirk those decisions.
2579280	2584960	And thankfully, most people that I know who are doing AI take those decisions really seriously
2584960	2590240	now, but it's still an enormous problem. Seeing data as a black box of relationships,
2590240	2598960	from which we withdraw what we need as human artifacts. And I think the pithiest way I can put
2598960	2606720	it is, if particulars are becoming more important to us outside of the world of AI, but in part
2606720	2614240	because of AI, then I think we can think about data as particulars. But there are particulars
2614240	2617840	that have been rendered machine learning. They gain something from that, I'm sorry,
2617840	2621360	from machine readable. They gain something from that, but they also learn something.
2621360	2629040	They gain some capabilities from that. So I think we're in a world in which
2631760	2637760	particulars are rising. I'm not sure if it's obvious. I think this is a really important
2637760	2645600	corrective to a Western focus on generalizations. It's because particulars are a reality.
2646960	2648960	So, thank you very much.
