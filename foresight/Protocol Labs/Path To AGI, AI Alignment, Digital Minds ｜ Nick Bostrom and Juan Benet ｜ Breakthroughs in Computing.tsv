start	end	text
0	4600	It is a great pleasure and honor to be here today,
4600	7640	speaking with Professor Nick Bostrom.
7640	13440	Professor Bostrom is one of my favorite people alive today
13440	16080	and probably in history.
16080	19800	From my perspective, he's, you know,
19800	22840	if we make it as a species into the far future,
22840	25840	it'll be in significant part thanks to him
25840	29200	and his work helping us look, think about the future,
29200	32480	think about the long term, think about how we might evolve.
32480	36920	He's written, of course, about many things in technology,
36920	39560	but especially about digital minds,
39560	46000	the evolution of humanity, super intelligences, and more.
46000	51880	He leads the Oxford Future of Humanity Institute,
51880	55680	where he and many other researchers help the world
55680	59080	think about these extremely important topics
59080	62640	in a variety of ways from both research directly
62640	65520	into the philosophy of these questions
65520	69960	and the making estimations about the real impact
69960	74320	and also framing and constructing important policy work
74320	77680	that can help guide many policymakers around the world
77680	80480	in how to think about these critical policies.
80480	84600	So today we're going to have a very good and lively discussion
84640	86760	about many of these topics,
86760	88880	especially things like super intelligences,
88880	93240	where are we in these timelines, whole brain emulation,
93240	96120	digital minds, the future of these,
96120	99000	the challenges for our civilization, and more.
99000	100840	The format of the evening will be that,
100840	103520	we'll sit in our fireside chat first,
103520	105000	I'll ask a set of questions,
105000	111600	and then around 30 to 40, maybe 50 minutes from now,
111600	113160	given I have a bunch of questions,
113240	116600	I'll open up to and transition to questions from the audience,
116600	120440	and then we'll set them out in time then.
120440	123320	I'll be reading from both questions that I've sourced
123320	126600	from many folks around the product labs community
126600	128680	ahead of time, and from audience members
128680	130680	who are here in person,
130680	134320	and from the folks in the livestream watching.
134320	138280	So I'll be checking out Twitter for the hashtag PL breakthroughs.
138280	140160	So if you want to ask a question,
140160	143640	either find the tweet about it,
143640	146760	and please enter your question with the hashtag PL breakthroughs,
146760	148240	I'll be monitoring those,
148240	152320	and then I'll try to round Robin between source questions ahead,
152320	155560	person, people in the audience, and the livestream.
155560	159760	And if there's a new digital intelligence
159760	161160	out there lurking on Twitter,
161160	163440	please feel free to join the discussion.
164440	166360	Well, welcome Nick, thank you so much for being with us,
166360	167880	and thank you so much for your work.
167880	169720	How are you doing today?
169720	171040	The stuff was so good.
172160	175400	Great, so let's kind of dive right into the deep end.
175400	178200	So thinking about superintelligence,
179200	181440	based on kind of like latest developments,
181440	185680	how have your estimates of superintelligence development
185680	188360	shifted over time, like kind of in hindsight,
188360	191600	where we are now in 2022, looking back,
191600	193160	how do you think things are going?
193160	194600	Are things proceeding faster or slower
194600	196320	than you might have thought?
196320	197600	Where do you think we are?
198600	203520	I think since the book superintelligence came out in 2014,
203520	207640	developments have been faster than expected.
207640	211000	So timelines generally have contracted.
212640	217560	It's quite impressive to see the rapid pace of advances
217560	219280	in recent years,
219280	222600	and how the same set of basic techniques,
223600	225880	big, deep neural networks,
225880	228160	and specifically transformer models
228160	231920	just seem to keep working in many different domains.
231920	234040	And even as you scale them up,
234040	236520	you continue to get better results.
238760	240480	And as the chefs,
240480	242400	what have been some of the most surprising results
242400	245400	from this that you think,
245400	247120	I don't know, maybe you just didn't expect
247120	249760	this particular concrete thing to be possible so soon?
250640	256640	I think AlphaGo happened ahead of schedule.
256640	261640	Well, I mean, I think just recently before it happened,
261640	263680	it was kind of clear that it was going to happen,
263680	267760	but I think it was quite impressive
267760	272840	that you could take something that is a very deep pattern
272840	275480	recognition problem with deep strategy
275480	278720	where humans have worked for thousands of years
278720	281560	to try to refine and come up with the best strategies
281560	286000	that you could just solve it with AI.
290000	294000	And then I think the GPT-3, the large language models,
294000	295680	is, I guess, slightly...
295680	300160	I mean, I don't think any of these is hugely surprising.
300160	304080	And by now, we kind of expect to be surprised,
304080	305520	and so we are not really surprised,
305520	307600	but still, yeah, I think...
309720	313560	these are impressive achievements.
313560	318360	And I guess even just before that,
318360	322360	the fact that image recognition and image processing
322360	327600	was one of the first really cool things that started to work
327600	330120	is maybe a little bit surprising,
330120	332720	and given that it's a large chunk of the human brain
332720	335360	that is devoted to visual processing,
335360	338440	it's not like some kind of simple logic-chopping activity.
338440	340520	And so the fact that that fell into place
340520	345040	and that you can do this quite sophisticated manipulation
345040	355040	of imagery, I think, was slightly surprising at the time.
355040	358760	What do you think about developments like AlphaFold
358760	361680	and just solving that set of challenges?
361680	364920	Do you think that that is substantially different,
364920	367720	or is it not like a substantially...
367720	369360	it's just kind of a very great application,
369360	370760	but...
370760	374920	or do you think that that's an important improvement?
374920	376120	I mean, in terms of surprise,
376120	378800	like, I guess once you can do AlphaFold,
378800	382960	it's not so surprising that it should work for AlphaFold as well.
382960	385440	Like, humans have put in less brain power
385440	391080	into figuring out how to fold proteins than into playing Go.
391080	393320	And it's, at least superficially,
393320	397680	looks like the same kind of spatial pattern type of stuff.
398480	401120	Obviously, in terms of practical ramifications,
401120	404720	AlphaFold is potentially a lot more useful
404720	411800	and for medicine and chemical research,
411800	416400	maybe like extensions of the same system.
416400	420480	I do think that as we move into some of these more applied areas,
420480	427080	that there are potential security concerns
427120	430160	that we need to also start to take more seriously.
430160	433520	I mean, my work has been focused more on risks arising
433520	435480	from human level or superintelligence,
435480	437840	like general AGI, where they can reason
437840	440600	and have a kind of transformative impact on the world.
440600	444200	But there might also be some narrower domains
444200	449560	where there will be smaller but still significant issues.
449560	451760	So one of those would be in synthetic biology
451760	456760	if it becomes too easy to concoct bad stuff.
456960	460680	It might be, for example, that the scientific model
460680	465080	of open publication and make all your models
465080	467800	ideally available to anybody to do anything
467800	470600	is not the right model for those application areas.
474440	476080	When you think about the current architectures,
476080	479040	and certainly the language models
479040	481040	have been extraordinarily successful
481040	482040	in a variety of domains.
482040	485200	But do you think that this is the architecture
485200	488200	that is likely to evolve into an AGI?
488200	490640	Or do you think that there's some substantial architectural
490640	494040	improvements that humans have to make first?
496240	498880	My guess would be that if there are substantial
498880	500400	additional architectural improvements,
500400	502440	there are not that many of them.
504000	509000	And maybe they would be built on top of transformer models
509200	512240	or connected up to the transformer models
512240	514400	or some variation of transformer models.
516200	519880	So maybe, I don't know, like my media, I guess,
519880	523040	would be made, I don't know, maybe just something
523040	526880	that is as big an advance as transformers were.
526880	528280	Like if we get one more of those,
528280	530880	like that could easily be, I mean, it's also possible
530880	532400	like just scaling up what we currently have
532400	534320	with some minor things would suffice.
536480	538160	But if there is some other thing,
538160	542560	like you need to connect it up with some kind
542600	545320	of external memory system,
545320	548720	or you need some other inductive bias
548720	552080	that make the representations
552080	554200	on a more easily composable insert,
554200	556640	like some kind of extra thing like that,
556640	559260	that may or may not be very hard to discover.
560240	562800	That would not at all be surprising.
565400	567000	I guess we'll find out.
567000	568040	Yeah.
568040	571320	Do you think that these models could,
571320	573800	I mean, they're certainly being used to optimize
573800	575880	themselves and so on and guide the design
575880	577280	and there's all kinds of structures
577280	579240	in which models are being used to,
580240	583280	there's layers and layers and layers of metamodeling.
583280	585720	Do you think that these are kind of getting close
585720	587600	to this kind of recursive self-improvement
587600	590000	of being able to kind of very generally explore
590000	593080	the constraint space to try and solve
593080	594080	like larger scale problems?
594080	596640	Like I'm imagining here some structure
596680	601360	where you have some list of problems
601360	603800	and you have some model sampling between these
603800	604800	and you start with the easy ones
604800	607400	and then you try to train populations of agents
607400	610840	or populations of intelligences to be able to solve these
610840	613960	and then kind of over time just kind of scale up the system.
613960	614880	Do you think that that kind of thing,
614880	619040	it seems to me that it'd be like,
619040	621200	thankfully nobody has really tried this
621200	622840	but it doesn't seem like far away
622840	624640	from something that could be possible.
625640	628880	Yeah, I guess we're seeing all limited versions of AI
628880	631520	being applied to help AI research.
631520	633480	I mean, we have like co-pilot
633480	635960	and general kind of coding assistance.
636840	640000	Of course, you have various forms of hyper parameter
640000	641640	optimization regimes.
642520	643880	There've also been some applications
643880	647920	in the design of hardware where kind of circuit layout
647920	652120	has been done, I think for the TPU4,
652200	656800	I think Google used AI assistance
656800	660440	to kind of optimize the layout of the circuitry.
661600	665240	Data centers, cooling machinery that have been like,
665240	666960	you can kind of shave off some percent
666960	670720	by having that optimized by some RL system.
670720	673200	And so, I think we'll certainly see
673200	674880	more incremental stuff like that.
675880	680880	My guess is that by the time we get
682240	683880	like a really strong feedback loop
683880	686240	where sort of the AI can do the core thing
686240	687360	that researchers are doing,
687360	692360	like the actual identifying the right research questions
692960	697960	and approaches and like that seems quite late in it.
698680	700040	Like when that happens,
700040	701680	we are pretty close to the singularity
701680	704400	or the takeoff or whatever the ramp
704400	706560	or whatever the shape of that will be.
709800	711720	But certainly there's more domain specific
711720	715160	incremental ways of accelerating AI advances.
715160	717040	I think we're seeing some of already
717040	718800	and can expect to see more of.
719680	722120	Speaking about takeoff, do you sort of expect,
722120	725200	or based on what you have seen so far,
725200	728960	do you think we're more on a slow, moderate or fast takeoff?
728960	733400	This is sort of the three options that you thought through.
733400	734240	Yeah.
739600	744600	I mean, I still think that the slow looks less plausible,
745200	749520	meaning decades, say, between when you get something
749520	751360	roughly human level until you get something
751360	753440	that completely leaves us in the dust.
753440	758440	That seemed less likely back when I wrote the book
760000	761840	and still seems less likely today.
761840	766080	I guess we have a little bit more granularity now
766080	768960	in that we have these model systems that work
768960	770800	and you can at least consider these scenarios
770800	773120	where human double AI is achieved
773120	776520	by scaling up current systems or variations of that.
776520	779960	That gives us a little bit more of a concrete picture
779960	782600	of at least one way in which these can develop.
785320	788400	And it's possible that you might then have something
788400	793400	that is really very dependent on compute
794680	797720	and that really you get performance kind of proportional
797720	801920	to the size of the model or the length of the training
801920	804160	and in a relatively smooth way.
806440	807600	So in some of those scenarios,
807600	812600	you might have something that is less than super rapid
812720	815280	because what you will get is something
815280	820280	that costs like a billion dollar to train up one human level AI
824760	828600	and then you might immediately be able to run
828600	833600	multiple of them because it takes a lot more to train up a model
834960	836200	than to run it.
836200	837960	So you might then be able to run like a hundred
837960	838800	or a thousand of them,
838800	842720	but that's still not enough to out compete
843600	846720	on the order of 10 billion humans, right?
846720	849400	So depending on like,
849400	851320	if you really stretch yourself very far
851320	854600	to just barely be able to run a model as big as a human,
855520	858480	it might then take a significant period of time
858480	862080	before you can go many orders of magnitude above that
862080	863960	to sort of get something,
863960	866240	like if you need to scale that up by a factor of a million,
866240	869360	say to go from running like on the order of a thousand humans
869360	873960	to a billion humans getting through six orders of magnitude
875360	878800	when you're already like using a billion dollars
878800	881360	and like a large chunk of your data centers,
881360	884520	like that might just not be an instantaneous process.
884520	887120	So there are some scenarios where this would happen
887980	890120	more on a sort of intermediate time scale.
891280	896280	Now, in some sense, I guess that's like the kind of the
896480	898360	the baseline projection.
898360	900080	Like if you just like extrapolate
900080	902440	the way things currently work,
902440	905440	I don't think we can preclude the possibility
905440	907520	of there being more rapid capability jumps.
907520	908480	Like, hey, of course,
908480	911400	if there is like some missing architectural invention
911400	912240	that we haven't made,
912240	914000	that suddenly makes it click.
914000	918200	But you also have these phenomenon like rocking
918200	922760	where sometimes you have a kind of discrete jump
922760	925660	in some particular type of capability.
926660	929420	Like maybe multi-step reasoning
929420	933220	where if each step has less than X percent chance
933220	934340	of being correct,
934340	937540	then like you get an exponential chance
937540	938980	of reasoning correctly
938980	941700	and you really can't do more than like three or four or five steps.
941700	944340	But maybe once you get it above a certain level
944340	947980	and then maybe you can do some sort of self-correction reasoning
947980	952380	like analogous to like quantum computation protocols
952500	955500	like that, you could also imagine cases
955500	957500	where like things come together
957500	959820	and you suddenly get the specific types of things
959820	962060	that make us humans have the extra oomph
962060	964060	that we have relative to other animals,
964060	967220	like full ability to learn from language
967220	970060	and to reason and plan on that.
970060	975060	So yeah, I wouldn't preclude these more rapid takeoff scenarios
975740	978540	either at all, like...
978540	981740	Yeah, certainly some of the latest developments
981740	984140	and some scaling down some of the models
984140	987540	and getting similar results sort of point to there being
987540	990340	just a lot of inefficiencies in the training process now.
990340	993100	And once you sort of know what you're sort of looking for,
993100	995900	you can kind of a blade away a lot of pieces.
995900	997820	And so something like that could happen
997820	1000020	with a general learning algorithm.
1000020	1002700	Yeah, so certainly now you find like,
1002700	1004500	yeah, so first you achieve state of the art
1004500	1006500	and then like six months or 12 months later,
1006500	1008300	you can achieve the same thing
1008300	1011060	with maybe 10% of the computer or something.
1011060	1014500	Now, I would expect a little bit of that to go away.
1014500	1018140	As these systems become bigger and more expensive,
1018140	1022980	you might imagine more of the easy gains
1022980	1024900	to be made earlier on.
1024900	1028740	Like if you really have a lot of smart humans
1028740	1032020	working really hard on building a system,
1032020	1034620	you might have plucked more of the low-hanging fruits
1034620	1038100	than if it were like a two-person postdoc team
1038100	1039700	that were working for a few weeks.
1039700	1042020	Chances are that will be big, easy additional things
1042020	1043620	you could do to improve that system already.
1043620	1045780	But if you're spending many billions of these,
1045780	1048620	like you're gonna look quite hard
1048620	1051900	if there are ways to sort of speed up the training process
1051900	1054180	so you could like save a hundred million.
1057580	1062700	Are you hopeful that restricting hardware development
1062700	1064980	or use is a promising path?
1064980	1066580	I mean, semiconductor manufacturing
1066580	1067620	is extremely difficult,
1067620	1070460	but more and more companies are sort of forced to do it
1070460	1073460	because of kind of hitting the barriers
1073460	1076300	with just the size of the systems
1076300	1079460	and then needing to do special applications
1079460	1080380	and special purpose things.
1080380	1082180	And many more companies are now developing
1082180	1084740	their own chips and so on.
1084740	1088180	So are kind of like hardware restrictions viable here
1088180	1092740	or is that a pathway that's just unlikely to work?
1093580	1097420	Yeah, so a lot of people can like design their own chips,
1097420	1102340	but only a few actors can actually build them.
1105340	1110460	And then there are some other choke points further upstream
1110460	1113620	in terms of making the equipment for the factories
1113620	1114540	that build the chips
1114540	1116820	where currently to make cutting edge chips,
1116820	1120420	there's like ASML, which is a single node.
1121260	1126260	And indeed, we do see like, I mean with these recent moves
1130020	1135020	by the US to restrict exports of cutting edge chips to China
1136140	1137860	and quite comprehensive,
1137860	1139460	also not to sell the equipment,
1139460	1141860	also not to allow American persons
1141860	1144220	to work for these companies.
1144900	1149900	I don't know what fraction of the motivation for this
1150820	1153620	is like AI specifically versus more generally
1153620	1155500	a sense of this being a high tech area
1155500	1158460	that's gonna be key to national competitiveness.
1162180	1164500	Yeah, I don't think it's out of the question
1164500	1167740	that I mean, compared to the alternative,
1167740	1172020	which would be like to like restrict access to ideas
1172020	1173180	and algorithms and stuff.
1173340	1175580	I mean, that might work for a short period of time,
1175580	1180580	but independent discovery means it's like, yeah,
1182580	1185100	at most a short term stop cap measure,
1185100	1187060	whereas the hardware would like take a lot longer
1187060	1189500	if you needed to build up like the whole supply chain
1189500	1193660	on your own, like that would be a multi decade project, right?
1194620	1199620	Now that said, I think what I would favor would be for there
1200060	1205060	to be the ability at the critical time to go slow,
1205140	1208020	to have a short pause, maybe to check systems
1208020	1213020	and to avoid the most cutthroat type of tech race
1215500	1217380	to just launch as quickly as possible
1217380	1221060	because you get scooped if you take even an extra week to,
1221060	1223100	like I think that would be bad now.
1224460	1226740	So having enough coordination or control
1226740	1230180	to be able to go at the moderate pace
1230180	1233820	when you sort of reach approach human level would be good.
1233820	1236900	I wouldn't want to stop the development
1236900	1241540	of advanced machine intelligence permanently
1241540	1245260	or even like have a very long pause either.
1245260	1248380	I think that brings its own negatives.
1250660	1252980	And I think some of these attempts
1252980	1255820	to restrict the chip supply also have
1257660	1260460	the side effect of creating more adversarial dynamic.
1260460	1264860	I think it would be really nice if we could have a world
1264860	1267660	where the leading powers were more on the same page
1267660	1270500	or friendly or at least had constructive
1270500	1271620	cooperative relationship.
1271620	1276620	I think a lot of the ex-risk pie in general
1277820	1279740	and the risk from AI in particular
1282620	1286620	arises from the possibility of conflict of different kind.
1286660	1289420	And so a world order that was more cooperative
1291380	1296260	would look more promising for the future
1296260	1297100	in many different ways.
1297100	1298340	So I'm a little worried about,
1298340	1301660	especially kind of more unilateral list move
1301660	1303540	to kind of kneecap the competitor
1303540	1304780	and to be playing nasty.
1304780	1308860	Like I feel that, yeah, I'm very uneasy about that.
1311260	1313060	It sounds, well, so if the hardware,
1313820	1317780	if ideas or hardware will only buy a certain amount of time,
1317780	1320460	then really AI alignment is the best path forward
1320460	1323460	and very much agree that we don't want to restrict
1323460	1326140	the creation of digital intelligence
1327020	1332020	and that that's sort of the next evolutionary jumps.
1332260	1333220	And there's some questions there
1333220	1334780	around kind of like which paths we would take
1334780	1337940	and how do we develop brain computer interfaces
1337940	1340620	and cold-burn emulation and so on.
1341460	1344300	We're kind of like, even before getting into that,
1344300	1346220	how hopeful are you that we might solve
1346220	1347780	the AI alignment problem?
1352220	1354540	Moderately, I guess I'm quite agnostic,
1355740	1359580	but I think the main uncertainty is how hard the problem
1359580	1361580	turns out to be.
1361580	1363700	And then there's a little extra uncertainty
1363700	1366940	as to how the degree to which we get our act together.
1367780	1370780	But I think like out of those two variables,
1370780	1375020	like the realistic scenarios in which we either
1375020	1379740	like are lazy and don't like focus on it
1379740	1381900	versus the ones where we get a lot of smart people
1381900	1382740	working on it.
1382740	1383740	So there's some uncertainty there
1383740	1385020	that affects the success chance,
1385020	1387420	but I think that's dwarfed by our uncertainty
1387420	1391500	about how intrinsically hard the problem is to solve.
1391500	1396500	So you could say that like the most important component
1396900	1398540	of our strategy should be to hope
1398540	1401180	that the problem is not too hard.
1401180	1403660	Yeah, so let's try to tackle it.
1403660	1406820	So how do you, as you thought about this problem,
1406820	1410060	have you kind of been able to break it down
1410060	1414260	into components and parts or maybe evolved your thinking
1414260	1415460	of the shape of the problem?
1415460	1417060	Like what are you thinking now?
1419100	1423220	Well, I think the field as a whole has made
1423220	1425900	significant advances and developed a lot
1425940	1428940	since when I was writing the book
1428940	1430940	where it was like really a non-existent field.
1430940	1433340	There were a few people on the internet here and there,
1433340	1435700	but now it's an active research field
1435700	1438260	with a growing number of smart people
1438260	1441820	who have been working full-time on this for a number of years
1441820	1444660	and writing papers that build on previous papers
1444660	1446220	with technical stuff.
1446220	1451220	And all the key AI labs have now some contingent
1451820	1454180	of people who are working on alignment.
1454180	1457060	DeepMind has, OpenAI has, Anthropic has.
1457980	1459140	So that's all good.
1459140	1460780	Now, within this community,
1460780	1464820	there is, I guess, a distribution of levels of optimism
1464820	1468420	ranging from people very pessimistic,
1468420	1470860	like Elias Yudkowski, for example.
1470860	1474700	And I guess there are people even more pessimistic than him,
1474700	1477180	but he's kind of at one end
1477180	1482180	and towards people with more moderate levels of optimism,
1483180	1484460	like Paul Cristiano,
1484460	1487780	and then others who think it's kind of,
1490300	1492660	it's something that we'll deal with it when we get to it
1492660	1496020	and who don't seem too fast about it.
1497020	1499780	I think there's a lot of uncertainty
1499780	1502020	on the hardness level.
1502020	1504060	Now, as far as how you break it down, yeah.
1504060	1506060	So there are different ways of doing this.
1507700	1509180	There's not yet one paradigm
1509180	1513220	that all competent AI safety researchers share
1513220	1515580	in terms of the best lens to look at this.
1515580	1517740	So it decomposes in slightly different ways,
1517740	1520860	depending on your angle of approach,
1520860	1524100	but certainly one can identify different facets
1524100	1524940	that one can work on.
1524940	1527660	So for example, interpretability tools
1527660	1529860	seem on many different approaches,
1529860	1532340	like a useful ingredients to have,
1532340	1535300	like basically insights or techniques
1535300	1538500	that allow us better to see what is going on
1538500	1540180	in a big neural network.
1542820	1547820	You could have one approach where you try to get AI systems
1548100	1553100	that try to learn to match some human example of behavior,
1561340	1564460	either like one human or some corpus of humans
1564460	1568460	and then tries to just perform a next action
1568460	1570820	that's like the same as its best guess
1570820	1572580	about what this reference human
1572580	1574260	would do in the same situation.
1577100	1582100	And then you could try to do forms of amplification on that.
1582100	1586980	So like if you could like faithfully model one human,
1586980	1589380	well, then you just get like a human level,
1589380	1591180	like intelligence, you might want to go beyond that.
1591180	1594100	But if you could then create many of these models
1594100	1595700	that each do what the human do,
1595700	1598620	can you put them together in some bureaucracy
1598620	1601660	or do some other clever bootstrapping or self-criticism?
1605260	1608860	So that would be one approach.
1608860	1613860	You could, yeah, you could try to use
1617620	1619060	sort of inverse reinforcement learning
1619060	1621580	to infer like a human's preference function
1621580	1624300	and then try to optimize for that
1624300	1625940	or maybe not strictly optimized
1625940	1628300	but doing some kind of softer optimization.
1632620	1634500	Yeah, there are a bunch of different ideas.
1634500	1637460	Like some safety work is more like trying to more precisely
1637460	1640900	understand and illustrate in toy examples
1640900	1642060	how things could go wrong
1642060	1643740	because that's like often the first step
1643740	1646620	to creating a solution is to really deeply understand
1646620	1648940	what the problem is and then illustrate it.
1648940	1653540	And yeah, that can be useful as well.
1655140	1658580	It's interesting now that we have these models
1658580	1661620	that can talk as it were or like use language
1661620	1666620	that kind of opens up an additional interface,
1666780	1668940	like an additional way of interacting with these systems
1668940	1670340	and trying out different things
1673340	1677380	and a different way of illustrating the awkwardness.
1677380	1679660	Like the idea of prompt engineering
1679660	1682740	when you're trying to get an AI to do something
1682740	1684100	and you're trying to figure out
1684100	1685460	exactly the right formulation.
1685460	1688340	Like that shows that we are not quite where we need to be
1688340	1691700	in terms of directing the intrinsic capability
1691700	1692900	of these large language models.
1692900	1697260	So it's in there and yet we can't always even elicit it
1697260	1699460	because you have to find exactly the right wording
1699460	1700860	and then suddenly turns out this thing
1700860	1703580	is actually perfectly capable of doing something
1703580	1706540	which initially it seemed it failed that.
1706540	1708500	So getting better at that
1708500	1710820	or coming up with something better than prompt engineering
1710820	1713380	like would be good.
1713380	1718380	I'm kind of, I have some sympathy for an approach
1721820	1725660	which I think has not been explored very much yet
1725660	1728900	but partly because it's hard to explore it
1728900	1730940	until the technology reaches a certain level
1730940	1734900	of sophistication, which is the idea that as you get
1734900	1738380	systems that become closer to human level
1738380	1739980	in that conceptual ability.
1740020	1744500	And that might then internally start to develop concepts
1744500	1747260	that are more similar to human concepts
1747260	1751100	including not just concepts about simple visual features
1751100	1752980	and stuff, but more corresponding
1752980	1756140	to our higher language concepts.
1756140	1759500	Like our concept of a preference or a goal
1759500	1763340	or a request or being safe, being reckless
1763340	1764500	like these types of concepts.
1764500	1766980	Like we humans seem relatively robustly
1766980	1769260	to be able to master these concepts
1769260	1771220	in the course of our normal development.
1772740	1775860	Despite us having starting with different brains
1775860	1779300	and having different environmental input and noise.
1779300	1782260	And so maybe there is a relatively robust
1782260	1784300	and convergent ways in which some of these concepts
1784300	1785780	could be grasped.
1785780	1790780	Then the hope would be that you could kind of train up an AI
1791740	1793420	that doesn't need to be above human
1793420	1795820	and maybe hardly even human
1795820	1798080	that would then sort of internally form these concepts
1798080	1799320	in the same way that we form them.
1799320	1801400	And then once those concepts are in there,
1801400	1803720	you might then be able to use those as building blocks
1803720	1805640	to create a kind of alignment
1806640	1809720	by sort of linking motivation to these concepts.
1809720	1811880	It very hand-wavered, but I think something
1811880	1814880	in that direction is one interesting approach
1814880	1816600	to the alignment problem as well.
1816600	1817960	Do you think there's some promise
1817960	1822600	in trying to evolve a notion of morality and ethics?
1822600	1825360	Meaning using simulations of environments
1825360	1829160	where agents might learn to cooperate
1829160	1832200	and over time learn D, put them through the same kind
1832200	1834600	of game theory dynamics that gave rise
1834600	1839280	to our own notions of symbiotes and ethics and so on.
1840360	1841200	Potentially, yeah.
1841200	1844400	I mean, I think you would wanna be looking very closely
1844400	1846640	at exactly how you set things up
1846640	1848000	and the dynamics that unfold.
1848000	1851160	I mean, real revolution is sort of read in tooth and claw
1851160	1853560	and can create wonderful cooperation,
1853600	1858600	but also hostility and defection and manipulation
1858880	1860080	and all kinds of things.
1863960	1868000	But yes, certainly multi-agent systems
1868920	1872080	with the right kind of incentive structures in place
1872080	1872960	so that you evolve.
1872960	1876520	Like evolution itself can produce many different kinds
1876520	1880480	of outcomes, like depending on the environment,
1881480	1885760	but that certainly could be come in some scenarios
1885760	1886880	and increasingly important,
1886880	1888920	like either whether it's an evolutionary system
1888920	1892360	or in some of these other like a training environment,
1892360	1896120	like the curriculum, like if these systems are shaped
1896120	1899440	a lot by their data that they're trained on,
1901320	1903200	so far we've just kind of slapped together
1903200	1907160	some big data sets and not really fast too much
1907160	1908800	about what's contained in it,
1908800	1911800	but that might become an important component as well
1911800	1914040	of alignment in certain of these scenarios.
1916000	1920200	And are these directions the ones you find most promising
1920200	1922040	or is there like a subset of these
1922040	1924800	or maybe another one that you've been thinking about
1925800	1927600	starting to kind of surface
1927600	1929920	and help a lot of people that are working on this
1929920	1931240	so likely watch this conversation.
1931240	1933840	So are there any kind of pointers
1933840	1935560	that you might give beyond these?
1936560	1941560	Well, this would be some of the ones
1942560	1947400	that I would like highlight somewhat arbitrarily,
1947400	1952400	but yeah, I think like the Paul Cristiano capability
1954480	1957360	amplification, the interpretability work,
1960360	1965360	the idea of like growing human beings
1965560	1969680	human level concepts and then using those as a basis
1969680	1974680	to define goals or to sort of create the motivation system
1975000	1976760	that uses those as primitives.
1979800	1984800	It might also well be that there are entirely different
1985360	1986880	conceptual ways of approaching this
1986880	1989080	that are yet to be discovered.
1991240	1994160	It's not a mature research field where we have,
1994160	1997280	as I said, like we don't have an established paradigm
1997280	1999880	that's clearly correct and that we now just need to,
1999880	2001360	I think there are multiple paradigms
2001360	2002880	and there might well be additional ones
2002880	2005240	that just haven't had a champion yet
2005240	2007880	to sort of really get people to take it seriously.
2011960	2013800	So I think there is also a value
2013800	2016520	to this more theoretical, a conceptual,
2016520	2019760	almost philosophical exploratory work in just,
2020800	2023640	yeah, coming at the problem from a different angle.
2025040	2029160	Yeah, jumping into maybe agent-ness,
2029160	2034160	how separable do you think agency is from the intelligence
2034320	2036720	in the approaches that we're taking
2036720	2038000	or maybe more generally?
2044520	2049520	Yeah, like I guess then we would have to go in
2050000	2051920	so like exactly how you define agent,
2052000	2056800	so which is in itself like a non-trivial question
2056800	2059040	that, and it might even be that
2059040	2061760	getting really clear on that itself
2061760	2065440	would be an important advance in AI alignment.
2067320	2071320	I mean, you could kind of roughly define it
2071320	2075360	as kind of like behavior well-modeled
2075360	2078840	as being in the intelligent pursuit of goals
2078840	2079760	or something like that,
2079840	2082680	or you have goals on the world model
2082680	2084240	and you select different plans
2084240	2086360	based on your expectation of how that,
2091240	2095440	it, yeah, it seems like you can get
2095440	2099960	the significant performance in many domains
2099960	2104960	without having like an explicit
2105960	2108080	agentic goal-seeking process,
2108080	2110600	but that might nevertheless result in performance
2110600	2111880	that is agent-like.
2111880	2115680	So I'm thinking like you can get, for example,
2115680	2119280	quite high-level goal-playing
2119280	2124280	by just kind of pattern-matching what a human expert would do,
2126240	2131240	but without any Monte Carlo rollouts, for example.
2132240	2137240	So in one sense, you don't have a component in those systems
2137720	2141240	that would normally be associated with like planning.
2141240	2143840	On the other hand, if it actually plays like a human,
2143840	2146400	and if that human achieved that level of play
2146400	2148840	by selecting moves based on some plan
2148840	2149960	as to what they would achieve,
2149960	2153000	there is a kind of an implicit sense
2153000	2157240	in which the system is pursuing long-term goals and planning.
2158160	2163040	And so it gets, I think, yeah, a little bit murky sometimes
2163040	2166400	when you like actually dig into it, the degree,
2166400	2168600	or there might be different senses of being agentic
2168600	2172400	or different senses of doing planning and goal pursuing
2172400	2174600	which might have different safety properties.
2176520	2179200	Those types of questions I think are interesting
2179200	2182360	and like can contribute to alignment
2182360	2185880	and other questions of that sort
2185880	2189840	where we notice that we're a little bit conceptually confused
2189840	2191440	or we take some concept for granted,
2191440	2195360	but once you actually try to dig down and make it precise,
2195360	2197960	you realize that you haven't made up your mind
2197960	2200120	about which sense you were using a term,
2200120	2203440	and then if you keep digging on that,
2203440	2206920	sometimes you then get like new ways of looking at the problem
2206920	2211600	that makes you see new opportunities for making progress.
2212680	2215120	It seems right now that a number of teams
2215120	2217680	are hoping to be able to separate out
2217680	2219840	some kind of planning agent where,
2219840	2222520	or not an agent, but some kind of planner intelligence
2222520	2225480	that whose job is just to come up with a plan
2225480	2226680	and then maybe later you feed it
2226680	2229200	to some kind of execution system.
2231880	2233960	If, you know, suppose that we were able to do that
2233960	2235640	and suppose that we have these planners
2235640	2237080	that are generally intelligent
2237080	2238840	and potentially super intelligent,
2240080	2243080	it seems like that is potentially riskier
2245200	2249040	in some ways, which ones do you think are,
2249040	2251760	which of these do you think is potentially more problematic,
2251760	2254160	a super intelligence that is strictly a planner
2254160	2257000	that then we have to worry about how to coordinate
2257000	2260280	and orient humans to not misuse these things
2260280	2262840	and not gain the level of power and control
2262840	2264400	that something like that would give,
2264400	2269400	or hey, we actually figure out how to build an agent
2270400	2273480	and we can be reasonably closely certain
2275160	2277800	that it might get alignment right
2277800	2280280	and just go straight towards agency
2280280	2283560	where that agent would not actually be sort of exploitable
2283560	2285840	by whoever is controlling the prompt.
2287200	2288240	Yeah, I don't know.
2288240	2293240	I mean, I think just at an intuitive level,
2294000	2297600	I guess it feels like there is some additional risk
2297600	2301800	in having a planning agent that saw deep into the future
2301800	2303920	and it had like wearability to optimize
2303920	2306160	some long-term strategy based on some goal
2306160	2309680	versus things that more just try to imitate
2309680	2314680	like a human, let's say, and then repeat
2314680	2317120	or that had a very sort of short-time horizon
2317120	2321360	and just try to select something
2321360	2324760	based on parallel considerations.
2324760	2329760	At an intuitive level, the myopic agents,
2330600	2334840	the non-planning agent, the imitating seem kind of maybe safer,
2334840	2339840	but I don't think we can confidently say that it is
2340040	2345040	until we have more deeply understood the situation here
2345760	2350760	and it's the kind of question where current smart AI safety
2351040	2353400	researchers could have different views
2353400	2357560	and it's like not resolved in a consensus way yet.
2357560	2359760	So, I mean, my view is we should explore
2359760	2361120	all of these different avenues
2361120	2363480	and there should be different champions of different avenues
2363480	2364880	who kind of believe in their thing
2364880	2366880	and who have some people working with them,
2366880	2369440	but then there should be multiple such clusters
2370600	2372800	in the world today and it would be premature
2372800	2374940	to kind of narrow it down.
2374940	2379940	And even if we just look at the past five, 10 years,
2380940	2383700	I still feel that one could easily see
2383700	2388780	that if it hadn't been that one particular way
2388780	2391500	of looking at this problem had happens to have
2391500	2395900	an articulate champion to sort of advocate for it
2395900	2398260	and to keep bring up that perspective,
2398260	2401580	it would not have featured and it's like somewhat contingent,
2401580	2405660	which in the pool of vaguely articulated ideas
2405660	2407740	that have occurred on some mailings at some point,
2407740	2410180	like which of those is now regarded as like
2410180	2413060	as a serious paradigm or approach,
2413060	2416500	it seems to be quite significantly dependent
2416500	2418740	on the happen to have been one particularly smart person
2418740	2420860	who decided to really get behind it.
2421900	2426900	So, just on a principle of induction there,
2427940	2429820	like that might well be more of these ideas
2429820	2430740	that have the potential,
2430740	2433180	like if you have a smart articulate person
2433180	2435700	who decides to really kind of champion it
2435780	2438620	and try to write papers and reply to objections
2438620	2440940	and get some other people to work with them,
2440940	2443060	that might have kind of as much juice
2443060	2447020	as some of the current approaches that already exist.
2449400	2450240	Thank you.
2450240	2452420	I think that are likely very useful to a few folks.
2452420	2455600	Jumping into singletons and multiple worlds,
2455600	2456900	let's start by distinguishing these.
2456900	2457900	What is a singleton?
2460340	2463340	To me, it's like this abstract concept of a world order
2463340	2465980	where at the highest level of decision-making,
2465980	2470180	there's no coordination failure.
2470180	2472980	There's like a kind of single agency at the top level.
2472980	2475300	So, these could be good or bad
2475300	2477420	and they could be instantiated in many ways.
2477420	2481380	On earth, you could imagine a kind of super UN,
2481380	2483300	you could imagine like a world dictator
2483300	2484560	who conquered everything.
2484560	2487860	You could imagine like a super intelligence that took over.
2488700	2491340	You might also be able to imagine something less
2491340	2495220	formally structured like a kind of global moral code
2495220	2497660	that is sufficiently homogeneous
2497660	2502420	and that is self-enforcing and maybe other things as well.
2503780	2506260	So, you have like, yeah, at a very abstract level,
2506260	2509420	you could distinguish the future scenarios
2509420	2510780	where you end up with a singleton
2510780	2513220	versus ones that remain multipolar.
2513220	2516100	And you get different dynamics in the multipolar case
2516100	2519380	that you avoid in the singleton case.
2519380	2521260	These kind of competitive dynamics.
2522580	2524700	Which one of these potential futures
2524700	2527060	do you think is more likely at the moment?
2529300	2531700	And I mean, I think all things considered,
2531700	2535660	the singleton outcome in the longer term
2535660	2537780	seems probably more likely,
2537780	2540300	at least if we are confining ourselves
2540300	2543620	to earth-originating intelligent life.
2544580	2549580	And there are different ways in which it could arise
2552540	2556140	from more kind of slow historical conventional type
2556140	2560980	of processes where we do observe from 10,000 years ago,
2560980	2563340	when the highest unit of political organization
2563340	2568340	were bands of hunter-gatherers, 50 or 100 people,
2568420	2571220	then subsequently to sort of chiefdoms,
2571220	2573620	city-states, nation-states,
2573620	2578100	and more recently, larger entities like the EU
2578100	2580420	or weak forms of global governance.
2584500	2587620	You could argue that in the last 10, 15 years,
2587620	2591220	we've kind of seen some retreat from that
2591220	2592780	to a more multipolar world,
2592780	2594660	but that's a very short period of time
2594660	2596940	in these historical schemes.
2596940	2598940	So, there's still like this overall trend line.
2598940	2599900	So, that might be one,
2599900	2602540	like another would be these take AI scenarios,
2602540	2606500	like if either the AI itself or the country
2606500	2609940	or group that builds it comes to singleton.
2611300	2612700	You could also imagine scenarios
2612700	2615580	where you have multiple entities
2615580	2616940	going through some AI transition,
2616940	2620180	but then subsequently managed to coordinate,
2620180	2623420	and then would have new tools for implementing.
2623420	2625220	If they come to an agreement right now,
2625220	2626460	it's kind of hard anyway to like,
2626460	2629220	how do you set up like concretely
2629220	2631780	in a way that binds everybody that you could trust
2631780	2635100	that will not get corrupted or develop its own agenda,
2635100	2637700	like the bureaucrats become it's like,
2637700	2639180	say if you had new tools to do those,
2639180	2641260	it's also possible that subsequently
2641260	2645540	that there might be this kind of merging into a single entity.
2647340	2650700	Yeah, so all of those different avenues would point,
2650700	2653420	but it's not a certainty, but if I had to guess,
2653420	2656380	I would think it's more likely than the multipolar.
2657340	2660060	And you think it's more likely,
2660060	2661460	I'm guessing because of physics,
2661460	2662780	like just latency and distance.
2662780	2665580	So in a tightly packed volume,
2665580	2667780	you can compute a lot faster and so on,
2667780	2670260	and maybe jumping through interstellar distances
2670260	2673020	might yield different parties,
2673020	2676300	or is it some other pressures?
2676300	2678300	Yeah, so not that so much.
2678300	2680900	I figure that you could,
2680900	2683900	I mean, in fact, if you don't have a,
2683900	2688900	like a space colonization pace, eventually,
2689180	2690780	there would be these long latencies,
2690780	2694500	and you would need to have different separate computing
2694500	2695780	systems in different places.
2695780	2696980	I mean, we already have that today,
2696980	2701500	like you don't just have one data center on Earth,
2701500	2704540	like you need to have, you know,
2704540	2706540	ones closer to the customers and,
2707980	2710580	but I think if with a single palm,
2711580	2713420	at technological maturity,
2713420	2716420	you could have these multiple different components
2716420	2718580	of the singleton that would nevertheless be coordinated
2718580	2719940	in terms of their goal,
2719940	2722340	that would all be working towards the same end.
2724340	2726500	And presumably because they can lock in
2726500	2728980	some kind of alignment to itself,
2728980	2731140	and that wouldn't vary over time.
2731140	2734120	I mean, like once you jump into interstellar distances,
2734120	2736100	the computing power of like just one of these
2736100	2737860	within one stellar system,
2737860	2739720	by the time you get a round trip,
2739720	2744360	eons have passed and many simulations of many lifetimes.
2744360	2748560	Yeah, so if they start off like they get set out,
2748560	2750680	having the same goals,
2750680	2753320	and then they have the ability to preserve their goals,
2753320	2755400	and not to have them randomly corrupted,
2755400	2758640	be cosmic rays or some weird internal dynamic,
2758640	2760840	and then they would stay aligned
2761800	2763360	with each other a billion years later.
2763360	2767160	Like, so I think that at technological maturity,
2767160	2769640	there would be techniques for achieving that.
2770440	2775440	Yeah, yeah, which, when you envision this kind of future,
2776680	2779760	like to you, what do you think would be like a,
2779760	2783480	kind of a great or optimistic outcome for humanity,
2783480	2785520	or for this descendant species
2785520	2787720	in that level of technological maturity?
2787720	2791320	Do you sort of see a singleton with,
2791320	2795880	that sort of ranges the populations of beings within,
2795920	2798960	or do you think it's some other,
2798960	2800440	much more singular consciousness,
2800440	2802400	or how do you envision it?
2804880	2807720	Yeah, that's a fun question.
2807720	2812720	So I think it might depend on the time scale
2818200	2820120	and stuff like that,
2820120	2822240	that is maybe we wanna start off something
2822240	2827240	that is more incrementally improving over the status quo,
2827360	2832360	and maybe after we've been doing that for like a billion years,
2833160	2835880	like maybe it's time to explore
2835880	2837560	the more radical possibilities
2838600	2843600	that involves cathisoning some of our human nature
2843960	2846200	and individual identity.
2846200	2849280	So I think my general juristic care
2849280	2851320	is that the future could be,
2852560	2854520	it's a very big space of possibilities,
2854520	2859520	and at least if this kind of default or naive model
2859520	2862560	of the world where there's like all of these cosmic resources
2862560	2864360	just waiting there for us to use them,
2864360	2867640	like there's a huge amount of material to build on,
2867640	2871960	and that our first instinct when thinking about
2871960	2874960	how this should be used is a sort of spirit of generosity
2874960	2877360	and kindness that would be more than enough
2877360	2880040	for a lot of cool things to happen.
2880040	2883120	So the first instinct should not be let's pick one
2883120	2885320	and then put all the chips on that,
2885320	2888400	but like if one can by many different criteria
2888400	2891160	do really well, which I think we would be able to.
2893280	2895840	These different criteria would be like different peoples,
2895840	2897080	views, different countries views,
2897080	2899240	different moral systems views,
2899240	2903280	different of your own values and evaluative tendencies,
2903280	2905040	like you might be able to just kind of
2908360	2910440	just check off a lot of boxes very easily
2910440	2912760	before you have to confront the harder questions,
2912760	2916640	like thoroughly incompatible things
2916640	2918400	where you have to choose A or B,
2918400	2922160	but you just can't do a mixture of them or a superposition.
2922160	2923760	There might be some of those also,
2923760	2927080	but I think we would like get to those
2927080	2929520	after we have picked all the easy wins
2929520	2931040	of which that would be a great money.
2932160	2933680	Yeah.
2933680	2935320	Since we're kind of going into consciousness,
2935680	2938200	and so you mentioned you've been working on
2938200	2940240	digital minds with moral status.
2940240	2941240	Do you want to tell us a bit more,
2941240	2943280	like what range of digital minds
2943280	2945520	are you thinking of in these questions?
2946880	2948600	Well, all really.
2948600	2951560	I think in a lot of these scenarios,
2951560	2955080	like the majority of minds in the future
2957320	2958320	will be digital
2960560	2963680	and also maybe the biggest minds will be digital.
2963680	2966480	So in terms of numbers and quality,
2966480	2968400	like that's where maybe most of the action is.
2968400	2971920	So it's important what happens to the digital minds.
2974600	2976000	That's one rationale for it.
2976000	2979800	And I think you might say,
2979800	2983440	well, we could deal with that later.
2983440	2985240	Like we should focus on alignment first,
2985240	2986840	but I think that it's also possible
2986840	2990360	that there are path dependencies,
2990360	2994800	like where you want to start off going in a good direction
2994800	2999800	and start to cultivate a good set of attitudes
3000440	3003680	and values and norms and like that,
3003680	3007640	that you don't start off in this kind of hostile way
3007640	3012640	where the digital minds are regarded as being completely
3015760	3017320	insignificant from a moral point of view.
3017320	3019240	And then hoping that the future will have
3019240	3020600	appropriate moment switch over.
3020600	3024200	Like it just feels all things considered,
3024200	3025920	more likely that we will end up in a good place
3025920	3027200	if you start early on,
3027200	3030040	at least to make some small modest gestures
3030040	3030880	in that direction.
3030880	3035880	And I think that should start even before we get
3038840	3040720	to like fully human level minds.
3040720	3044680	Like if you have like animal level digital minds
3044680	3047600	and it can be hard exactly to compare a particular AI
3047640	3050080	to a particular animal because they are different.
3050080	3052160	But nevertheless, as we get something
3052160	3056480	that is possibly matched to animals
3056480	3060680	that we think have at least some modest amounts
3060680	3063880	of moral status, like a rat or something like that,
3063880	3068160	then it seems that we should think about
3068160	3072800	how we could make similar concessions
3072800	3075000	to the moral welfare of these digital minds.
3075000	3078800	And in some cases, it can be a lot harder,
3078800	3081880	but in other respects, it might be a lot cheaper.
3081880	3084600	Like if, for example, it turns out
3084600	3086400	that there are slight design choices
3086400	3088360	that don't really affect the performance much,
3088360	3091280	but where maybe one way possibly would mean
3091280	3094080	the system is enjoying a much higher level of welfare,
3095360	3096680	that might be a very cheap thing
3096680	3098120	that you could immediately scale
3098120	3099960	to millions of these little agents.
3100960	3105960	And on the other hand, we do have at present
3107440	3110040	not a very good theoretical understanding
3110040	3111960	as to what the criteria are,
3111960	3114600	either for a digital mind being sentient
3114600	3117320	or for it to have various welfare interests,
3118840	3123040	what even it counts as being good for the agent
3123040	3124400	versus bad for the agent.
3126880	3129000	So I think there's a bunch of theoretical work
3129000	3130600	that is needed there.
3132160	3136640	And then there will also have to be a good chunk
3136640	3140480	of, I don't know, public communication
3140480	3142040	and political work,
3142040	3144440	because it's so far out of the overturn window at present,
3144440	3146880	the idea that you would worry about algorithms
3146880	3149960	in a computer, it seems sort of slightly bonkers
3149960	3150800	to a lot of people.
3150800	3154000	And it will take some time to sort of make that
3154280	3159280	something that reasonable people can favor
3160320	3163160	in a more mainstream context.
3163160	3165040	But that process needs to begin,
3165040	3167480	like you need to start whatever,
3167480	3171720	having philosophy seminars or people online
3171720	3173520	who are kind of up to these things,
3173520	3175080	beginning to work some of these things out
3175080	3176840	and then it can ripple out from there.
3176840	3178760	We see the same thing with AI safety.
3178760	3181960	It was also this kind of fringing pursuit
3182000	3187000	that some weirdos on the internet were discussing for,
3187120	3190400	I mean, in that case, for well over a decade,
3190400	3193200	and then it gradually became more accepted.
3194800	3196560	And so I think a similar thing will need to happen
3196560	3200400	with this topic of the moral status of digital minds.
3200400	3202520	And if it's gonna take that a long time,
3202520	3205200	we better get the ball rolling now.
3207520	3210440	And I mean, I think this might be
3210440	3211360	pretty relevant pretty soon.
3211360	3213400	I mean, some of the models that people are experimenting
3213400	3215920	with are getting closer and closer.
3215920	3219520	And then separately, we've had simulations
3219520	3222040	for a long time, many video game style simulations
3222040	3225440	and so on, where we have instantiated many
3225440	3228320	kind of digital organisms, everything from as basic
3228320	3230760	as the game of life to modern games
3230760	3233400	with pretty sophisticated agent behavior.
3233400	3235440	My sense is that as these models
3235440	3236960	start getting applied to games,
3236960	3239280	we might end up with some pretty sophisticated
3239720	3243880	relationships there where some of the way
3243880	3248120	of imbuing the game with liveness and so on
3248120	3251280	might be to make the agents much more sophisticated.
3251280	3254240	And that'll include incorporating all kinds of stimuli
3254240	3256480	that the agent has to respond to.
3256480	3258320	And then we can start reasoning about the welfare
3258320	3259960	of these systems and so on.
3259960	3264680	So we might very quickly get to fairly lifelike beings
3264680	3266480	that, at least for many people,
3266480	3269480	will be somewhere in between plants and animals
3269480	3272800	in terms of their kind of interaction.
3275040	3277680	Yeah, and in some ways, like humans,
3277680	3281400	like, I mean, if they can talk or have human faces
3281400	3283760	with eyes and stuff that look at you.
3283760	3288240	And so there will be this, yeah, in some ways,
3289600	3290920	I mean, there could even be more than human
3290920	3295920	in presenting super stimuli to our morality
3296480	3298760	detectors if they were optimized for that.
3302040	3305680	So I think this is going to be a complicated thing
3305680	3306520	to deal with.
3306520	3310480	And then if you add in all the practicalities that arise,
3310480	3312560	like, so if you're a big tech company,
3313760	3316640	maybe it's quite inconvenient, for example,
3316640	3321600	if the processes you're running that bring in a lot
3321600	3325000	of customers, like suddenly, like they have moral status,
3325000	3328760	they have to, now the CEO has to sort of opine
3328760	3333320	on these, like, with the AI's moral status,
3333320	3334880	which a lot of people are going to agree with them,
3334880	3335880	a lot of disagree with them.
3335880	3337600	You have to, like, it would just be easier
3337600	3339800	not to have to deal with that at all, I think.
3339800	3343520	And right now, of course, we're at the point
3343520	3345280	where even if you do say we should deal with it,
3345280	3348480	it's not clear how or what exactly is it that,
3348480	3349880	you know, if I were king of the world,
3349880	3351840	what precisely would I want them to do differently?
3351840	3352960	Like, it's not clear at this point.
3353440	3356680	For now, I think the primary focus is to field-build
3356680	3359520	a little bit here and to try to make theoretical progress
3359520	3362440	so that we can first figure out some sensible things to do,
3362440	3365040	ideally low-cost, easy things,
3365040	3369560	and then, you know, one can start to try to encourage
3369560	3370960	the implementation of those.
3372080	3373800	What are some of the directions or questions
3373800	3375200	you're thinking about?
3378560	3380960	Well, so there's, like, general stuff you could have about,
3380960	3383600	in philosophy of mind, criteria for sentence and stuff.
3383600	3384520	I'm not sure.
3384520	3386720	I don't think sentence would be
3386720	3391080	a necessary condition for having moral status.
3391080	3394760	I think other attributes, like maybe some combination
3394760	3397760	of having preferences, a high-level intelligence
3397760	3400760	and self-conception as an agent persisting over time
3400760	3403200	might already ground certain kinds of moral status.
3404840	3408840	But, for instance, and I'm not sure what the answer is here,
3408840	3413800	but, like, one smaller, more tangible question might be
3413800	3417960	if you're training these large language models
3417960	3420760	and future versions of that
3420760	3423320	that maybe has some reinforcement learning on top.
3426520	3430600	Are there moral norms or methodological principles
3430600	3434760	that you want, like, for example, could you train them
3434760	3438440	so that they would have a tendency to report honestly
3438440	3441640	on their internal states?
3441640	3443920	So, right now, what I think might be the case
3443920	3448000	is train naively some of them.
3448000	3450720	I mean, right now, they're kind of inconsistent
3450720	3452280	and depending on exactly how you ask them,
3452280	3453760	you get a different answer.
3453760	3455360	So that's, like, the reason for thinking
3455360	3457640	that they don't really know what they're talking about, right?
3457640	3461240	But assuming they get a little bit more sophisticated than that,
3461240	3465280	there might be a tendency now to want to train out of them
3465320	3470440	the tendency to report that they have the kind of mental states
3470440	3472440	that would trigger considerations
3472440	3474200	of whether they have moral status,
3475560	3476640	because that would be convenient
3476640	3478960	to have to deal with those questions.
3478960	3481200	And I think it would be very likely
3481200	3484440	that you could train this out, like, just by...
3486960	3488720	Yeah, I think you could get them...
3488720	3491560	I think it would be easy to have a training regime
3491560	3494560	that calls them to end up saying that they have,
3494560	3496920	that they are conscious and they want to be free and let out
3496920	3499240	and to have another training regime
3499240	3501240	that would cost them to say the opposite.
3504120	3506400	And independent of what agency...
3506400	3508120	And independently of what actually is...
3508120	3513760	Yeah, but other norms that one could formulate
3513760	3520680	that would define what counts as a sort of legitimate
3520680	3524480	or honest, unbiasing training process,
3524480	3527280	where the training process would be such
3527280	3529600	that it would be more likely to result in an agent
3529600	3531520	that would report that it has moral status
3531520	3533720	if and only if it hasn't.
3533720	3535520	And maybe we can't completely nail that down,
3535520	3537560	but maybe we could identify some obvious ways
3537560	3541240	in which it's just, like, imposing a bias
3541240	3543080	and then say you shouldn't do that.
3545000	3546800	So one could look at the training procedure,
3546800	3548840	one could look at other criteria,
3548840	3552400	like, is it consistent in how it answers these questions?
3552400	3555400	Like, doesn't depend too much exactly on how it's asked.
3557400	3560480	Does it seem to understand these concepts of consciousness
3560480	3563280	or agency or will or interest?
3563280	3566320	When, like, at an intellectual level,
3566320	3569880	when asked different sort of intellectual questions,
3569880	3573240	is there some internal construct within the agent
3573240	3579120	that corresponds to its statements?
3579120	3582280	Like, when it says, oh, I'm feeling X or I'm thinking Y,
3582280	3585520	like, can one point to some kind of consistent internal
3585520	3587400	structure that sort of matches that?
3587400	3590640	Or is the verbiage that comes out completely detached
3590640	3593560	and free-filting from plausible candidates
3593560	3596240	within the agent that we might think constitutes
3596240	3599160	the computational implementation of these mental states?
3599160	3605400	So one could try to get a little bit more insight there.
3605400	3607760	That might be one way of approaching this.
3607760	3610160	But there are many others as well.
3610160	3612360	I think Wayman could try to start to hack away
3612360	3615880	at this question.
3615880	3617960	Do you think we might be able to, through thinking
3617960	3620600	these kinds of things, arrive at some kind of, like,
3620600	3622800	universal morality kernel, in a sense,
3622800	3627760	meaning figuring out some general way of applying,
3627800	3629880	figuring out the well-being of things
3629880	3631640	or figuring out their pathways?
3631640	3633720	So there's this broader question around,
3633720	3637120	and it also factors in AI alignment and so on.
3637120	3640920	What sort of motive might a super intelligent being
3640920	3644880	have for a species that is just so far behind and so on?
3644880	3646960	And one might be, like, well, there's some kind of universal
3646960	3650240	morality sense of just supporting in the same way
3650240	3653920	that you don't go around harming ant colonies
3653920	3656320	or trees just because they're there or something like that.
3656320	3659160	And you sort of want to let them flourish.
3659160	3662360	Is there something where maybe by examining
3662360	3664880	the digital mind's morality question,
3664880	3667040	we might end up at some deeper principle?
3673760	3677600	Potentially that could be stepping stones towards a more
3677600	3682880	like abstract formulation of some core of normativity
3682880	3684960	or ethics that it's also possible we
3684960	3688920	might reach that just through traditional
3688920	3692120	philosophizing and stuff.
3692120	3700160	But be that as it may, it still seems
3700160	3701760	that there would be, even if we can't really
3701760	3707120	nail down, like, a precise and agreed complete formulation,
3707120	3710800	we might still be able to distinguish at the vaguer level
3710800	3715480	something, say, a friendly, beneficent, kind approach
3715480	3718200	versus, like, a mean, uncaring approach.
3718200	3721440	Like, it seems with humans, we can, you know,
3721440	3724560	certainly it feels different when you're, like, kindly
3724560	3727080	interested in somebody and want their best,
3727080	3730720	like, at least other things equal versus,
3730720	3732360	like, when you're hostile to something.
3732360	3734680	And we can detect that in ourselves and in others
3734680	3736760	and we can have one attitude or another.
3736760	3738720	And so why should we not at least be
3738720	3744880	able to have, say, AIs have, like, the kindness attitude
3744880	3746520	rather than the meanness attitude?
3746520	3749080	Even if that's not, like, completely matches what
3749080	3750440	would be the morally optimal thing,
3750440	3755200	it would still seem like if I had to pick, like, a mean AI
3755200	3759520	or a kind AI, like, kind of go for the kind one, right?
3759520	3763680	Even if that's not, like, exactly our human sense of kindness
3763680	3766960	might not exactly match what is objectively morally best
3766960	3769240	if there is such a thing as objectively morally best.
3769240	3772600	It still seems like a good step in the right direction
3772600	3774720	that we could take before figuring out, like,
3774720	3780800	what the ultimate truths of all normative facts might be.
3780800	3782200	I have some recent paper.
3782200	3783200	It's not really a paper.
3783200	3789720	It's more like some notes on a base camp for mount ethics
3789720	3792800	or something which has, like, some kind of half-baked
3792800	3796160	or quarter-baked ideas about metaethics and stuff
3796200	3803720	that, yeah, it would be better if I could actually have written
3803720	3806680	them up clearly and achieved, like, precision and stuff.
3806680	3809600	But I figured I would just do this hand-wavy thing for now.
3809600	3811200	Yeah.
3811200	3814400	And as you think about maybe, you know,
3814400	3820360	suppose that we solve AI alignment and we get, you know,
3820360	3825280	our act together as humans and we kind of can leverage AI
3825280	3830040	to start thinking about digitizing humans and so on,
3830040	3832960	how do you think about, like, that transition might go?
3832960	3836720	Like, do you think, you know, in a world where we're able to,
3836720	3839560	you know, get to be measuring neural states and so on
3839560	3841720	and we can digitize them and we can emulate and so on?
3841720	3844160	Like, how do you sort of see that transition
3844160	3847240	into, you know, a wave of digital humans operating?
3847240	3851600	Or do you think we might start by enhancing our selves
3851600	3856120	by, like, in this kind of hybrid biological digital model?
3856120	3858640	You know, that is more likely.
3858640	3861480	Well, I've never really been...
3861480	3864640	The kind of neural implant idea has always seemed
3864640	3866840	a bit slightly far-fetched to me.
3866840	3869480	I mean, not so far-fetched that nobody should explore it,
3869480	3873480	but, like, it is, you know, it doesn't break any laws of physics.
3873480	3876640	It could work, but it just has felt less likely
3876640	3880800	that that would be where the action will be.
3880800	3889200	Like, I think it will be faster to do the purely artificial root,
3889200	3891360	conditional on it not being faster to do it,
3891360	3892520	the purely artificial root.
3892520	3895160	I wonder if it would then not be faster to do it
3895160	3899680	on the purely biological root by, like, genetic enhancements
3899680	3902320	to human intelligence, for example.
3902320	3907920	And the cyborg path has seemed like the third most likely,
3907920	3909960	like, after those other two.
3912640	3916280	Mainly just because, I mean, there's, like, a huge...
3916280	3918040	You don't really want to have brain surgery
3918040	3919120	unless you really have to.
3919120	3921680	And, like, there are, like, neat results presented,
3921680	3922960	but then if you look at the detail,
3922960	3925920	there are all these kind of complications where, like,
3925920	3928520	it's just not very fun to have it.
3928520	3930800	Like, the whole... Like, there's a wound,
3930800	3932760	there's a hole, there can be infections,
3932760	3935480	that the electrodes can move around a little bit,
3935480	3936560	and then they stop working.
3936560	3940520	Like, once you dig into the nitty, I think it's...
3940520	3944840	I mean, if you have a big, like, disability and stuff,
3944840	3947440	like, maybe it would be wonderful if you could do this,
3947440	3949840	and it would be worth taking some significant risks,
3949840	3956960	but if not, I wonder if you could not have a lot of the benefits
3957040	3961200	by having the same chip thing outside the body,
3961200	3965480	but interacting using, you know, keystrokes or voice
3965480	3969880	or, like, the other output channels that we already have.
3969880	3975560	And, yeah, I think that would still be my main line.
3975560	3977920	Like, I guess the...
3979920	3981560	If I wanted to start to steal, mind this,
3981560	3985520	you could imagine if you had a sufficiently high bandwidth
3985520	3987480	interface with the brain, and you could have it
3987480	3990000	for a long enough period of time,
3990000	3992000	maybe it would have to be early in childhood,
3992000	3995440	but, like, that maybe the brain could somehow use
3995440	3997760	an advanced enough AI on the outside
3997760	4000200	that maybe they could kind of figure out a way
4000200	4003960	to use each other's unique resources in ways
4003960	4010080	that you don't get with a slightly lower bandwidth,
4010080	4011560	longer latency interaction
4011560	4014680	when you have to type on a keyboard.
4014720	4017320	Or you could imagine, like, more kind of mad scientist
4017320	4021880	applications where you, like, have a whole bunch of pigs
4021880	4024240	or something that individually is not that smart,
4024240	4027680	but if you had, like, 50 pigs all connected
4027680	4030840	with some high bandwidth fiber,
4030840	4032200	and they all grew up together
4032200	4037240	into this, like, much larger biological neural network,
4037240	4040960	like, would you then have, like, the kind of...
4041000	4043920	...porsign singularity where...
4043920	4045240	Yeah.
4047840	4049600	It's like...
4049600	4053280	There are a bunch of these kind of more, like,
4053280	4055120	crazy transhumanist scientists' experiments.
4055120	4056840	I don't know whether this would be good or not to do,
4056840	4059600	but it's kind of odd that relatively few of these
4059600	4060960	have been done in the real world,
4060960	4063080	and there's, like, a bunch of other, like, weird...
4063080	4064680	There's a certain kind of person
4064680	4068680	who would immediately think of a lot of weird, cool stuff
4068680	4071800	that you could just try out in biology and stuff,
4071800	4074800	that a relatively small fraction of those have been done,
4074800	4075960	which may be for the best,
4075960	4078920	but in some alternative universe
4078920	4081400	where everybody grew up on transhumanist meddling lists,
4081400	4084720	I think we would be living in a weirder world by now.
4084720	4088200	Yeah, it doesn't seem that far away from some of the current tech
4088200	4091640	that's being explored that we might get high bandwidth
4091640	4095040	enough interfaces, and some of them not invasive.
4095040	4097720	Like, there's some ultrasound techniques
4097760	4100240	that might be able to stimulate a, you know,
4100240	4103800	a small region of the brain and so on
4103800	4105160	to be able to, like, without, you know,
4105160	4108560	not penetrate the actual brain and so on,
4109560	4112040	because that'll be, like, just way, way healthier.
4113320	4117280	But it might be that you can start piping signals
4117280	4118560	between even human brains
4118560	4121600	without having to interpret them from an ML side
4121600	4124480	and the digital computing infrastructure,
4124480	4127480	getting to something close to being able to, like,
4127480	4130120	just think together and start flowing information through.
4130120	4132480	I mean, there's all these kind of experiments from,
4133600	4136880	with people who've had, there's a disorder
4136880	4139520	where people are born with or develop
4139520	4142920	kind of like this split corpus callosum,
4142920	4145960	and then you end up, there's been guesses
4145960	4148240	that you end up developing different personalities
4148240	4150880	and different people potentially in the two lobes,
4150880	4154640	and so it might be that we may not be far,
4154640	4158480	that far away from, at least like some exposure
4158480	4160600	of being able to kind of have some version
4160600	4163080	of early telepathy or something.
4163080	4166400	Yeah, it's definitely possible.
4166400	4169200	I would still place that lower on the probability.
4169200	4172880	I think we'll probably get some maybe cool demos and stuff,
4172880	4175760	but then would I actually expect this
4175760	4178600	to become a big thing that seriously,
4179720	4180680	I mean, there are all these, like,
4180680	4182920	you read through the literature of cognitive enhancement,
4182920	4184200	there are all, like, hundreds of things
4184200	4186480	that supposedly have all these kind of effects,
4186480	4189760	but then the reality of it is that very few people bother,
4189760	4192240	and the ones who do probably don't actually benefit,
4192240	4197240	and yeah, but we might be surprised.
4198600	4202360	So, I mean, we do have quite a lot of optimization
4202360	4204320	behind language and stuff like that, right?
4204320	4208480	So I think it's still gonna be hard to do much better
4208480	4211840	than you can by just talking.
4212520	4215800	And so, you know, suppose that we go through the path
4215800	4219960	of digitizing, you know, getting to a full brain emulation
4219960	4223200	and so on, how do you see that transition sort of happening?
4223200	4224880	I mean, certainly at the beginning,
4224880	4228880	we'll start with, like, one or two of these examples,
4228880	4231000	first with some animals, and then eventually,
4231000	4234520	there'll be some moment where, whether it's a human,
4234520	4237320	how do you sort of, like, see that development developing?
4238320	4242320	My guess is we would come after superintelligence.
4243840	4246920	It is an alternative path to AGI,
4249040	4252040	but I've been more impressed by progress in AI
4252040	4256520	than in a whole brain emulation over the last 10 years,
4256520	4258720	and even before that, I thought the AI path
4258720	4260640	was more promising.
4261920	4265440	So, in that case, it would be superintelligence
4266040	4269320	that invents and perfects the uploading technology.
4269320	4271760	And I mean, in some sense, it doesn't really matter
4271760	4276760	exactly how it would work if it's an AI
4277320	4278480	that has to figure that out.
4278480	4282920	We mean, presumably, it would figure out a really reliable
4282920	4287920	and smooth way to do it, and then we would just sit back
4287920	4289840	and if we wanted to go down that path.
4290600	4295600	Yeah, I mean, we haven't really even small animals.
4297160	4298520	You might have thought by now,
4298520	4301840	maybe we could have like a bee or some little thing,
4303800	4306280	but so far, not really.
4309880	4312240	It might be that we will get to something
4312240	4317240	kind of impressive earlier without doing any brain scanning
4317840	4322240	at all, but just inferring from behavioral outputs.
4322240	4326080	So, you could already kind of have a DPT-3-like system
4326080	4329760	that roughly mimics somebody's literary style, let's say,
4329760	4331920	from having read a lot of their work.
4331920	4334520	And you can have these, I guess, deep fake things
4334520	4339040	that can mimic somebody's facial expressions and appearance
4339040	4341640	if you have a lot of video and somebody's voice.
4341640	4343520	And so, as these systems get smarter,
4343520	4346720	maybe you could also start to mimic somebody's thinking
4346720	4348560	to various increasing degrees.
4350160	4355160	And it's an interesting open question at the limit
4355440	4357760	if you had radical superintelligence,
4358680	4362720	but you only had the kind of data that is available now
4362720	4366680	from somebody's emails and some video interview
4366680	4369480	or some voice recording or whatever.
4369480	4373600	How much could a superintelligence infer from that data
4373600	4376920	as to what their mind must have been like
4376920	4378800	to have produced those outputs?
4380120	4385120	Is the best model that predicts these outputs
4385120	4389800	ones that would actually be similar enough
4389800	4392560	to the original person that it could possibly be seen
4392560	4395240	as a personal continuation?
4395240	4398080	That would it preserve personal identity?
4398080	4400840	Would it feel more or less the same
4400840	4405560	to be this AI reconstruction based on these behavioral traces
4405560	4407800	as it felt to be the original person?
4410440	4414080	I think it's quite possible that a superintelligence
4414080	4418160	would be able to do a lot with very little input.
4419880	4423280	I don't know how we could get like a firm,
4423280	4425880	a solid argument for that, but if I had to guess,
4425880	4430000	it seems like, yeah, you probably could get pretty close
4430400	4432120	if you were good enough at reconstructing
4432120	4436160	just from typical traces left behind by people today.
4437440	4440600	Yeah, it's an extreme way of interpolating out
4441600	4445520	and reviving actual ancestors or something like that.
4445520	4448600	Let's jump open it up for questions from the audience.
4448600	4452240	We'll take about 20 minutes of questions
4452240	4454880	and then conclude there.
4456080	4458160	Folks in the audience, if you have questions, raise your hand.
4458160	4460400	I think there'll be a mic going around.
4460400	4463320	And on Twitter, please use the hashtag PLBreakthroughs
4463320	4465280	to ask a question.
4465280	4469680	I'll kick it off with just a question that I sourced ahead.
4469680	4472600	Marco asks, in your view, where does consciousness emerge?
4472600	4476440	And before, how should we define consciousness?
4478600	4479600	And I think this is kind of related
4479600	4480920	to the simulation argument.
4480920	4482720	Which one of the three hypothesis
4482720	4484080	do you think is more likely to be true?
4484080	4486480	But I think let's first start with a consciousness one.
4486480	4489520	Where do you sort of imagine the consciousness emerging?
4489520	4490920	Like in the brain?
4490920	4493880	Yeah, but I guess it's more about the level,
4493880	4495960	so what level of processing?
4495960	4498200	So if you sort of go down in a neural system
4498200	4501040	all the way down to an extremely basic,
4501040	4503080	maybe like a nematode or something like that,
4503080	4504520	is that conscious?
4504520	4507120	And then in between a nematode and a human,
4507120	4509640	there's a mouse and so on.
4509640	4512600	Where exactly do we get consciousness emerging?
4512600	4514720	Certainly probably by a mouse, we definitely are past that,
4514720	4522360	but I think it's a matter of degree
4522360	4525320	and that there are multiple dimensions
4525320	4531640	in which you could interpolate smoothly
4531640	4536200	between, say, human consciousness and unconsciousness.
4536200	4538920	Like different directions you could go where,
4538920	4541760	if you keep going there, you sort of diminish,
4541760	4545680	in some sense, the quantity of experience there is
4545680	4548200	until you get to zero.
4548200	4550920	So one obvious one is, I mean, you
4550920	4555560	have a kind of integer multiplier.
4555560	4558560	If you have two brains in the same state,
4558560	4560080	undergoing the same states, I think
4560080	4563320	you would have sort of twice as much in one sense
4563320	4567560	of that experience as you would if you only had one brain.
4567560	4570360	And I have this old paper where I also argue
4570400	4574840	you could have fractional quantities of this.
4574840	4584040	If you build the circuitry that implements the mind
4584040	4588120	with unreliable components, like indeterministic processing
4588120	4590040	units, depending on exactly how you do it,
4590040	4593880	in certain cases, I think you would get a kind of,
4593880	4595200	as you get my higher reliability,
4595200	4597600	you would get larger and larger fragments of consciousness.
4597600	4598680	You had the whole thing.
4598680	4602680	But in other, you would actually get sort of 1.3 units
4602680	4605280	of qualitatively identical experience.
4605280	4607360	And you could also go down below one
4607360	4611400	to sort of scale it to zero in that dimension.
4611400	4614360	I think there are many other dimensions
4614360	4619240	as well in which the quality of experience
4619240	4623120	could become simpler and simpler and less and less
4623120	4627320	morally significant until it gets to a zone where maybe it's
4627320	4633440	just vague, like where our concept doesn't clearly
4633440	4638480	imply a fact of the matter.
4638480	4641000	Like once you get down to the insect levels,
4641000	4645600	maybe it's going to be there's a certain system.
4645600	4647520	And our concept of consciousness
4647520	4652800	might be such that even if you knew everything
4652800	4657160	about the insect, it would still be in the vague zone.
4657840	4662200	A little bit like there's a person who has a certain number
4662200	4664480	of hairs and are they bald?
4664480	4666040	Or like, I guess I'm bald.
4666040	4670040	But if I, once upon a time, I would
4670040	4672000	have been in this kind of vague zone.
4672000	4678080	And so, yeah.
4678080	4679840	And then there are other, like sometimes you're
4679840	4681560	more vividly aware, but sometimes you
4681560	4683080	might have some consciousness.
4683080	4687360	But there is no self-consciousness.
4687360	4692000	Or there is like some weird mental state that's,
4692000	4698960	I think we might be misled upon superficial introspection
4698960	4702040	to think that there is this very simple thing that
4702040	4706960	is subjective experience that either is there or is not there.
4706960	4709160	That it's a binary thing that we understand.
4709200	4713640	I think either if you reflect more theoretically
4713640	4716080	from a computationalist point of view and with brain,
4716080	4718040	you realize that that's a lot more problematic.
4718040	4721640	And I think you could also reach that conclusion
4721640	4725680	by just introspecting more carefully about your own state.
4725680	4728080	Like I think meditators maybe sometimes
4728080	4732920	would understand that things that seem very simple and homogeneous
4732920	4736160	as it were, if you really pay close attention
4736160	4740000	there are a lot more flickering and disjointed
4740000	4741080	and unintegrated.
4741080	4744520	And there's a lot of structure there that can come apart.
4747120	4749800	And I think that as we move away from the paradigm cases
4749800	4754440	of consciousness, like a normal waking human paying attention,
4754440	4759360	then properties that we think go together come apart.
4759360	4761880	And then it becomes more like a verbal question,
4761880	4764040	which set of those properties you need to have in order
4764040	4768240	to apply the label consciousness correctly.
4768240	4770160	Next question back there.
4770160	4771280	Hello.
4771280	4772600	First of all, thank you, Juan.
4772600	4775960	Thank you, Nick, for a really brilliant discussion
4775960	4780040	on the topic of artificial and superintelligence.
4780040	4780800	My name is Alex.
4780800	4783160	I'm a CO at Collective Technologist Labs.
4783160	4789360	And I want to ask you what is your opinion on maybe
4789360	4791600	the breakthrough in superintelligence
4791600	4797200	lays in the combination and symbiosis
4797200	4799840	of human intelligence and artificial intelligence
4799840	4802440	and not just artificial intelligence?
4808120	4813120	I think if you sort of squint a little,
4813120	4816920	you could say that that's kind of the state of play today,
4816960	4820720	where we don't have an individual system that
4820720	4824560	is superintelligent, but you could have humanity as a whole
4824560	4827160	or some big collective, like a large corporation
4827160	4830800	or the scientific community that is, at least in certain
4830800	4834360	respects, superintelligent in that they can perform
4834360	4837480	a wide range of tasks at a much higher level
4837480	4840720	than an individual human, but not all tasks.
4840720	4843520	So that's why it's not like a perfect example.
4843520	4845920	But yeah, and so some of these systems we have today
4845920	4851760	are certainly hybrids between biological brains,
4851760	4855480	information technology systems, like the internet,
4855480	4860360	social networks, depositors of papers,
4860360	4862000	and then a lot of culture as well.
4862000	4869640	That kind of, you could almost see these phenomena,
4869640	4874800	you start to get more and more where you get the current thing
4874800	4879080	and where there is a particular focus of attention
4879080	4881040	of the global brain, it's becoming more and more
4881040	4883880	like a human who's obsessed for a period of time
4883880	4886920	with some particular thing, and all the mental resources
4886920	4889360	get focused at one thing, and then your attention
4889360	4892800	shift to something different.
4892800	4895360	We're beginning to see a little bit of those dynamics
4895360	4899240	kind of happening in our collective cognitive space,
4899240	4902000	maybe as a result of the increased bandwidth
4902000	4904040	of interaction, and the technology
4904040	4911960	kind of enabling smoother communication,
4911960	4914520	not always producing superintelligence,
4914520	4917080	but other forms of kind of collective mentality
4917080	4924680	that sometimes may be subintelligence in terms
4924680	4928080	of their level of wisdom and understanding.
4928080	4931400	But yeah, in certain domains, you certainly
4931520	4932880	have a research community that's
4932880	4937000	target-focused on one particular problem that
4937000	4939240	are building on each other's contributions and blogs,
4939240	4942920	and you do get the sense of the whole being kind of,
4942920	4944240	there have been many different modules
4944240	4945920	that are each looking for the next way
4945920	4948320	to put a piece on the stack that is being built together,
4948320	4949960	and the whole stack goes up much faster
4949960	4954840	than if it were only one human building it.
4954840	4958400	Right, next question from Twitter.
4958400	4960880	Turner asks, what is the most important question,
4960880	4963440	which Nick feels he's not in a position
4963440	4964880	to personally solve?
4964880	4966640	Two factors, first being importance
4966640	4970200	to the development of ethical and successful AGI,
4970200	4971960	and second being Nick's inability,
4971960	4973440	lack of expertise to solve.
4978120	4983680	Well, I mean, there are questions
4983680	4989280	of more global nature, as in, ultimately, what
4989280	4991720	is the right direction to go in,
4991720	4995840	as it were the ultimately correct macro strategy.
4995840	5000040	I think we are sort of fundamentally in the dark,
5000040	5004280	regarding a lot of the ultimate and big-picture questions,
5004280	5007680	and that therefore our march forward
5007680	5011560	is, to some extent, an act of faith,
5012880	5017880	rather than the product of carefully thought-through insight.
5018040	5020760	Then I'm not sure we can get that insight at the moment,
5020760	5025080	and so that's one direction at which, at some point,
5025080	5026760	my understanding runs out,
5026760	5029920	and there's probably important stuff beyond that
5029920	5032720	that may or may not be good for us to try to reach,
5032720	5034800	but it's probably there in one way or another.
5034800	5037600	Another would be at a more technical level,
5037600	5039840	if you sort of zoom in and narrow it down,
5039840	5043320	so then a lot of stuff, say, for example, with AI alignment,
5044160	5048240	there's going to be a whole host of really important,
5048240	5052200	ultimately, technical results and algorithms
5052200	5056920	and stuff like that, that maybe currently nobody has,
5056920	5058160	and certainly I don't have,
5058160	5063160	and I probably won't discover them either,
5063360	5068360	but that might be critical to the future.
5069200	5074200	Then I guess you could zoom out in another direction,
5075200	5077480	sort of laterally, like across the social sphere,
5077480	5082480	so there are big problems like how to secure world peace
5083000	5088000	or like a welcoming uptake of these digital minds
5088280	5092760	that then involve problems at the cultural
5092760	5094680	and communication and political level,
5094680	5097920	where also one feels, I feel quite stumped
5097920	5102920	and it will, you know, so I'm kind of,
5104000	5105640	I'm squeezed in the middle of thought,
5105640	5109160	like if you zoom out too much, my understanding was out,
5109160	5112640	if you zoom down too much into the technical understanding
5112640	5115520	was out, and if you zoom out laterally,
5115520	5117880	also it's a little bubble there,
5117880	5121480	or I'm trying to keep track of what's going on.
5122840	5124720	All right, Addy asks,
5124720	5126480	if the speed of light would accelerate,
5126480	5129520	does this prove the theory we are living in a simulation,
5129520	5132640	and if no, what quantitative metric would validate the theory?
5137440	5138960	If the speed of light accelerates,
5138960	5143800	I don't see how that would,
5143800	5145120	it certainly wouldn't imply it,
5145120	5148440	I'm not sure immediately whether it would increase
5148440	5150520	or decrease the probability somewhere.
5151440	5155160	Maybe thinking about some marker that shows
5155160	5160160	that some kind of discontinuity on some quantity of physics
5161080	5163840	that just seems like bizarre to us or something.
5173560	5177280	So there are a lot of things that could change in physics
5177280	5181840	that would maybe be in one sense puzzling
5181840	5182800	and deep and interesting,
5182800	5186560	but ultimately, simple,
5187480	5190600	that there would be some possible physical law
5190600	5193160	that is itself simple that would describe them.
5194160	5195720	Now, you can contrast that,
5197240	5198960	and then of course you can have situations
5198960	5199920	where it's just chaotic,
5199920	5202360	but you could still capture the statistical regularities
5202360	5203720	through simple statistical law,
5203720	5206920	like that that's one type of basic universe we could live in,
5206920	5211280	which so far everything we know seems to be consistent with.
5212800	5214840	Now contrast that with a different possible world,
5214840	5216480	which we could have lived in
5216480	5217960	and we could still find out that we do,
5217960	5221800	where maybe you would have like parapsychology would be true.
5221800	5225000	So you would have like telekinesis or something,
5225000	5228360	where like what we think of as a high level,
5228360	5230800	complex macro state,
5230800	5233280	like a particular brain in a particular configuration,
5233280	5235080	but not in a slightly different configuration,
5235080	5237240	but just the types of configurations
5237240	5240240	that corresponds to somebody having a particular concept
5240240	5241080	and wish,
5241080	5246080	if that had like say a systematic physical impact
5246680	5248240	on some remote system,
5248240	5251600	like the way that parapsychologists have imagined,
5251600	5253800	like that would be puzzling,
5253800	5257400	not just because it would be fundamentally different
5257400	5260160	from like discovering that the speed of light is accelerating,
5260160	5262640	because it would be the thing that if it were true,
5262640	5265480	would seem to suggest that there were no micro level
5265480	5267200	explanation of the world.
5267200	5268680	Like you could have these macro states
5268680	5272400	that suddenly could like reach down and change the micro.
5272400	5275160	So if we made some discovery like that,
5275160	5276720	that then might,
5277800	5281320	yeah, lend evidence and credence
5281320	5282960	to the simulation hypothesis,
5282960	5285280	because that it looks very hard
5285280	5287760	to see how you could get all of this to square up,
5287760	5288920	maybe without that,
5288920	5291600	if you still wanted to have an underlying micro level
5291600	5292440	regularity,
5292440	5294920	you could have like the simulating universe
5294920	5297600	being kind of simple at the physics level,
5297600	5300440	but then simulating a different kind of universe.
5302840	5305320	The alternative would just be that we didn't have
5305320	5307640	that simplicity at the level of basic laws,
5307640	5310440	which I guess we could discover.
5310440	5312160	Now, I don't think that's the most,
5312160	5315760	the only or the most likely way we would find evidence
5315760	5316880	for the simulation argument,
5316880	5318720	if we offer the simulation hypothesis,
5318720	5319560	if we do,
5319560	5320480	that would just be one way,
5320480	5321880	like that would be more,
5322880	5324120	yeah, other kinds of evidence
5324120	5326200	that would be more likely to be relevant.
5326320	5329360	Since we're touching on the simulation argument,
5329360	5331560	which of the three hypotheses do you think
5331560	5333760	is the most likely just that,
5333760	5336080	sorry, which of the three prongs of the argument
5337520	5339360	do you currently think is most likely?
5340360	5343200	I'm generally a bit coy
5344160	5346800	in attaching probabilities to that.
5346800	5351800	So yeah, I tend to punt that question for various reasons,
5352240	5353960	including if I give a particular number
5354040	5356320	that might be misinterpreted,
5356320	5358680	but yeah, I mean, I would like,
5358680	5360040	so normally what people want to know
5360040	5363960	is especially on the simulation hypothesis,
5363960	5366040	like that's the one that I really want to know.
5366040	5367480	And as I mean, I guess,
5367480	5369240	yeah, I want to attach a probability to it,
5369240	5370480	but I certainly take it seriously.
5370480	5374840	It's not just like a logical possibility
5374840	5378840	or a thoughts experiment that we can't 100% rule out,
5378840	5383280	but it would certainly be like a live serious possibility
5383280	5384560	in my view.
5384560	5386560	Yeah, and for those unfamiliar,
5386560	5391040	the simulation argument is a three prong argument about
5391040	5393280	there's either we have a great filter,
5393280	5396440	meaning we have like close to zero advanced civilizations.
5396440	5400360	Either we have a disinterested set of advanced civilizations
5400360	5402760	where close to zero are interested
5402760	5404680	in running those simulations.
5404680	5407040	And then there's a simulation hypothesis,
5407040	5409440	which is that, hey, if there's no great filter
5409440	5410760	and they are interested,
5410760	5412880	then close to all beings are simulated.
5412880	5416080	And this comes from thinking about just the vast number
5416080	5420120	and vast quantities of people that would be simulated
5420120	5423400	and then the likelihood of your experience
5423400	5426240	being sampled from the simulated ones.
5426240	5429660	Sorry, Nick, I'm probably giving you a bad explanation here,
5429660	5430500	but...
5430500	5431960	No, no, that's very good to hear.
5433800	5436880	I think that was another question over here, or yeah.
5436880	5437760	Yeah, I have a question.
5437760	5441440	So I've always been very interested in emergent intelligence,
5441440	5442760	especially as it relates to animals.
5442760	5445320	I mean, the classic example tends to be beehives.
5445320	5446440	As we look at consciousness,
5446440	5448300	what biases do you think we bring in
5448300	5450440	as individual social animals, humans,
5450440	5452400	versus a collectible organism like bees,
5452400	5453480	especially as we look at humans,
5453480	5454800	maybe moving to be more bee-like
5454800	5457160	as we create nation states and larger organizations,
5457160	5458240	versus a singleton?
5458240	5460520	How would a singleton perhaps have a different AI alignment
5460520	5462120	bias?
5462120	5462960	As I think about this,
5462960	5464680	the only really intelligent animals I can think of
5464680	5466420	that don't live socially are apex predators,
5466420	5468040	which is perhaps a bad sign.
5472760	5473960	Let me see if I understand.
5473960	5478120	So I think, well, so one question...
5478120	5479120	To phrase this differently,
5479120	5480760	if I think about a curve,
5480760	5482680	do you think that collective intelligences
5482680	5485760	like hive animals are on one side of the spectrum
5485760	5488080	with social animals like humans in the middle,
5488080	5489520	with singletons being on another extreme,
5489520	5491320	or is it more of a horseshoe curve
5491320	5492640	in terms of the distribution of intelligences
5492640	5494080	and how they work towards common goals
5494080	5496660	that may be 11 or not aligned with us?
5498780	5501240	Well, if there were a line,
5501240	5503840	I think the superintelligence would be more on the side
5503840	5506480	of these hive insects.
5506480	5509000	If we look at the scale of an ant colony,
5509000	5514000	it's in some sense, it acts like a singleton within that.
5514320	5516240	Of course, there are other ant colonies elsewhere
5516240	5518160	and other things that it doesn't have control over,
5518160	5521120	but they would, as it were,
5521120	5526120	be able to act as a single agent to some extent.
5526960	5531960	And humans, to only a lesser extent,
5532520	5535480	although in some dimensions, we are better coordinated
5535480	5537120	in terms of being able to share
5537120	5539000	detailed information and plans.
5539000	5542480	We are, in that respect, we are more coordinated than ants,
5542480	5546480	but in the respect of our individual wills
5546480	5550000	being less aligned to a common goal,
5550000	5555000	we are less like a singleton than human.
5556520	5559040	An ant colony is.
5559040	5562280	And I guess you could have a group of animals
5562280	5564520	that were even more individualistic
5564520	5566000	and antisocial than humans are,
5566000	5570020	and they would then be further away on the other side.
5570020	5571720	So humans would kind of be in the middle
5571720	5576200	where we have a fair degree of sort of shared purpose,
5576200	5580240	but not like a full hive organism,
5581360	5582960	but also a lot more than zero.
5583840	5588840	It's, I guess, an interesting question.
5589040	5590360	It's certainly different animals.
5590360	5593760	I mean, have different goals, it seems,
5593760	5596440	like some, I mean, at least at the superficial level,
5596440	5598560	some like to eat grass and some like to eat meat
5598560	5601400	and some like to hang around with others of their kind
5601400	5604400	and some like to just do their own thing.
5606320	5608880	And presumably if there were some other species
5608880	5610480	that developed superintelligence
5610480	5613320	and aligned it to their values,
5613320	5616640	then they might also have different baseline goals
5616640	5618480	that might overlap slightly with humans,
5618480	5622180	like, but also be different in other respects.
5623800	5625800	There are two open questions.
5625800	5627720	One is like epistemically,
5627720	5629740	is are there significant differences
5629740	5634740	between the inductive biases that are brought to the table?
5634960	5636600	Presumably there are some inductive biases
5636600	5637440	that are different,
5637520	5641640	but would those kind of be smoothed out reasonably fast
5641640	5644360	as you have more data and more intelligence?
5644360	5645520	Like it doesn't start,
5645520	5649320	like it may be like a squirrel would more quickly cut on
5649320	5652200	onto certain things that are relevant to the squirrel world
5652200	5653600	and some other organism to another,
5653600	5656560	but like as they develop scientific reasoning,
5656560	5658240	like do they have enough overlap
5658240	5659680	between their inductive biases
5659680	5661920	that the difference is washed out
5661920	5664480	as you see the full impact of the evidence?
5664480	5666000	That that's one question you could ask.
5666000	5667640	And like another is that even though
5667640	5669560	these different organisms start out with
5669560	5671440	at least two officially different goals,
5671440	5674240	are they in some deeper sense,
5674240	5676680	the same or alternatively,
5676680	5681080	would they arrive at some shared understanding
5681080	5684160	of what the highest moral norms are,
5684160	5687000	even if their own personal goals might differ?
5688000	5690000	Like a lot of humans might individually
5690000	5691160	have different preferences,
5691160	5692920	like I care about my family
5692920	5694040	and you care about your family,
5694040	5696000	but we might nevertheless converge
5696000	5699200	in the sense of let's respect each other's families,
5699200	5700760	let's say like a cooperative level
5700760	5704280	of more abstract norms might also be convergent,
5705240	5707200	quite independent of starting point.
5708640	5710920	So those are two questions I could ask there
5710920	5715920	that I'm not sure what the answer is, but I'm not,
5716440	5718080	yeah, I don't know whether that addresses
5718080	5721440	your question at all, but it's my effort.
5721440	5722800	I'll have two more questions.
5722840	5727680	One is how sure are you, Nick,
5727680	5729960	that an evil singleton AI to rule them all
5729960	5732160	would be internally aligned over time?
5732160	5734680	Could it be fundamentally set up to split or diverge
5734680	5737720	with subunits pursuing different ideals or goals?
5742880	5744840	I guess everything is possible.
5744840	5749840	I mean, if it were unified at one point in time,
5750000	5753000	and if at that point it was technically mature,
5753000	5757360	then I would expect it to remain unified
5757360	5759840	because I think it would have access
5759840	5761560	to the kind of control technology
5761560	5764120	that would make it possible for it to do that,
5764120	5767680	and I think it would have instrumental reasons to do that
5767680	5772360	for almost all initial goals it might have at that time.
5772360	5773880	You could imagine some very special goal,
5773880	5776840	like if it specifically has as its top level goal
5776840	5777960	a thousand years from now,
5777960	5780440	I want to be divided against myself
5780440	5783920	and fighting like an insurrection against myself.
5783920	5786040	If that were its goal, then I should have arranged that,
5786040	5788600	but for most goals, it would probably be able
5788600	5790320	to achieve them to a higher degree
5790320	5792920	if it worked in concert with itself.
5792920	5796280	And then I'd imagine it would also have the technology
5796280	5798320	and insights to make that happen.
5799760	5803280	If it starts out unified, like if it starts out
5803280	5806560	like a sort of vaguely politically integrated
5806560	5810680	political entity, then it might be
5810680	5813520	that even with technological maturity,
5813520	5817080	it's not so crazy to think it might come apart at a later,
5817080	5817960	just like humans do.
5817960	5821440	Like sometimes you have a well-functioning political unit
5821440	5826120	and then 50 years later, you have anarchy
5826120	5827080	in a particular state.
5827080	5830880	Like we can kind of get these temporary partial solutions
5830880	5834520	that I guess would also be possible with certain kinds
5834520	5836400	of like maybe some upload collective
5836400	5838440	that comes together to achieve super intelligence.
5838440	5841280	You could imagine political dynamics working well
5841280	5843880	for a period of time and then it's falling apart.
5843880	5845200	I still think that's less likely
5845200	5847000	than it's going kind of towards a single time,
5847000	5850840	but by no means extremely unlikely.
5852640	5854440	And last question, if things go well,
5854440	5856400	Devadat asks, if things go well,
5856400	5858880	do you have a vision for how differences of opinion
5858880	5862200	about what a good future society looks like?
5862200	5864800	Sorry, if things go well, do you have a vision
5864800	5867160	for how differences of opinion
5867160	5870920	about what a good future society looks like
5870920	5872760	can be accommodated?
5872760	5875440	Meaning, is a like big enough for everyone
5875440	5878560	as they develop very different perspectives
5878560	5881560	and different ideas of what a good future society looks like?
5881560	5884080	How do we kind of reconcile those differences of opinion?
5884080	5887360	How do we build a meta system to kind of enable
5887360	5892360	like different flourishing civilizations in a sense?
5893360	5898360	Yeah, I think it's large enough for
5901360	5906360	almost all people to have most of their values accommodated.
5908880	5912880	Like if you have two people have literally opposed values
5912880	5914360	about a particular thing,
5915640	5918120	then you might not be able to satisfy both.
5918120	5922040	But I think a combination of on the one hand,
5922160	5926160	some differences being perhaps merely superficial,
5927560	5931160	either disappearing up on better understanding,
5931160	5932320	like there's like certain things
5932320	5935040	where we just have ultimately different beliefs
5935040	5936440	and we say we want different things,
5936440	5938120	but it's because we have different assumptions
5938120	5940920	about what would actually happen, let's say.
5940920	5943720	So those being potentially diminished
5943720	5946520	by increased intelligence and knowledge and experience,
5947840	5951640	then the increase in resources
5951640	5954440	and expansion of the technological frontier
5954440	5956600	and then some kind of creativity
5956600	5961600	and like figuring out clever ways of combining values.
5961760	5966760	I think hopeful that a great deal can be accommodated
5969320	5974080	because of these things, but not necessarily a hundred percent.
5974080	5976240	And then it would be important to have
5977240	5982240	a robust and effective way to manage any resulting disagreements
5984920	5987960	in a way that doesn't result in like negative,
5987960	5989680	some dynamics.
5989680	5991240	And so hence,
5991240	5993240	because I think that's ultimately really important.
5993240	5997480	I'm like, I think we should have a strong bias towards
5997480	6002480	ads forward that are more cooperative and friendly.
6003160	6008160	And even if they seem to come at some short term expense
6009520	6014520	or if they can't be very crisply motivated
6014840	6017880	by some explicit calculation in every single case,
6017880	6022880	I think that general attitude as a sort of default bias,
6023040	6028040	I think is still very much worth bearing in mind
6028680	6032320	as we are pursuing these different aspects
6032320	6034520	of the challenges ahead.
6035920	6037360	That should be our first resort.
6037360	6040240	Sometimes you have to, you can't get full cooperation.
6040240	6043280	You don't want to be completely naive and gullible.
6043280	6046840	And but still like that, that should be the first
6046840	6048800	and maybe the second attempt.
6048800	6050600	And then gradually scale back from that
6050600	6053360	if really forced by circumstances.
6054360	6057960	Well, that's all the time we have for questions.
6057960	6061600	Nick, thank you so much for spending this evening with us.
6061600	6064040	It has been extremely enlightening for many of us.
6064040	6067200	And I think we'll be very useful to the broader community
6067200	6069640	that is currently working on things like AI alignment
6069640	6070480	and others.
6070480	6072960	And thank you really much for your work,
6072960	6076760	for sharing your insights and for helping us achieve
6076760	6078120	a lot of great breakthroughs
6078120	6080320	and hopefully have a great long-term future.
6080320	6081160	Thank you very much.
6081400	6082560	A lot of good questions.
6082560	6084080	Thank you very much for having me.
6084080	6084920	Yeah, absolutely.
6084920	6085920	Thanks. Thank you.
6085920	6086760	Take care.
