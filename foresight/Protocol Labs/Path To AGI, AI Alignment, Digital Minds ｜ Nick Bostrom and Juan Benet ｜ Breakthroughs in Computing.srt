1
00:00:00,000 --> 00:00:04,600
It is a great pleasure and honor to be here today,

2
00:00:04,600 --> 00:00:07,640
speaking with Professor Nick Bostrom.

3
00:00:07,640 --> 00:00:13,440
Professor Bostrom is one of my favorite people alive today

4
00:00:13,440 --> 00:00:16,080
and probably in history.

5
00:00:16,080 --> 00:00:19,800
From my perspective, he's, you know,

6
00:00:19,800 --> 00:00:22,840
if we make it as a species into the far future,

7
00:00:22,840 --> 00:00:25,840
it'll be in significant part thanks to him

8
00:00:25,840 --> 00:00:29,200
and his work helping us look, think about the future,

9
00:00:29,200 --> 00:00:32,480
think about the long term, think about how we might evolve.

10
00:00:32,480 --> 00:00:36,920
He's written, of course, about many things in technology,

11
00:00:36,920 --> 00:00:39,560
but especially about digital minds,

12
00:00:39,560 --> 00:00:46,000
the evolution of humanity, super intelligences, and more.

13
00:00:46,000 --> 00:00:51,880
He leads the Oxford Future of Humanity Institute,

14
00:00:51,880 --> 00:00:55,680
where he and many other researchers help the world

15
00:00:55,680 --> 00:00:59,080
think about these extremely important topics

16
00:00:59,080 --> 00:01:02,640
in a variety of ways from both research directly

17
00:01:02,640 --> 00:01:05,520
into the philosophy of these questions

18
00:01:05,520 --> 00:01:09,960
and the making estimations about the real impact

19
00:01:09,960 --> 00:01:14,320
and also framing and constructing important policy work

20
00:01:14,320 --> 00:01:17,680
that can help guide many policymakers around the world

21
00:01:17,680 --> 00:01:20,480
in how to think about these critical policies.

22
00:01:20,480 --> 00:01:24,600
So today we're going to have a very good and lively discussion

23
00:01:24,640 --> 00:01:26,760
about many of these topics,

24
00:01:26,760 --> 00:01:28,880
especially things like super intelligences,

25
00:01:28,880 --> 00:01:33,240
where are we in these timelines, whole brain emulation,

26
00:01:33,240 --> 00:01:36,120
digital minds, the future of these,

27
00:01:36,120 --> 00:01:39,000
the challenges for our civilization, and more.

28
00:01:39,000 --> 00:01:40,840
The format of the evening will be that,

29
00:01:40,840 --> 00:01:43,520
we'll sit in our fireside chat first,

30
00:01:43,520 --> 00:01:45,000
I'll ask a set of questions,

31
00:01:45,000 --> 00:01:51,600
and then around 30 to 40, maybe 50 minutes from now,

32
00:01:51,600 --> 00:01:53,160
given I have a bunch of questions,

33
00:01:53,240 --> 00:01:56,600
I'll open up to and transition to questions from the audience,

34
00:01:56,600 --> 00:02:00,440
and then we'll set them out in time then.

35
00:02:00,440 --> 00:02:03,320
I'll be reading from both questions that I've sourced

36
00:02:03,320 --> 00:02:06,600
from many folks around the product labs community

37
00:02:06,600 --> 00:02:08,680
ahead of time, and from audience members

38
00:02:08,680 --> 00:02:10,680
who are here in person,

39
00:02:10,680 --> 00:02:14,320
and from the folks in the livestream watching.

40
00:02:14,320 --> 00:02:18,280
So I'll be checking out Twitter for the hashtag PL breakthroughs.

41
00:02:18,280 --> 00:02:20,160
So if you want to ask a question,

42
00:02:20,160 --> 00:02:23,640
either find the tweet about it,

43
00:02:23,640 --> 00:02:26,760
and please enter your question with the hashtag PL breakthroughs,

44
00:02:26,760 --> 00:02:28,240
I'll be monitoring those,

45
00:02:28,240 --> 00:02:32,320
and then I'll try to round Robin between source questions ahead,

46
00:02:32,320 --> 00:02:35,560
person, people in the audience, and the livestream.

47
00:02:35,560 --> 00:02:39,760
And if there's a new digital intelligence

48
00:02:39,760 --> 00:02:41,160
out there lurking on Twitter,

49
00:02:41,160 --> 00:02:43,440
please feel free to join the discussion.

50
00:02:44,440 --> 00:02:46,360
Well, welcome Nick, thank you so much for being with us,

51
00:02:46,360 --> 00:02:47,880
and thank you so much for your work.

52
00:02:47,880 --> 00:02:49,720
How are you doing today?

53
00:02:49,720 --> 00:02:51,040
The stuff was so good.

54
00:02:52,160 --> 00:02:55,400
Great, so let's kind of dive right into the deep end.

55
00:02:55,400 --> 00:02:58,200
So thinking about superintelligence,

56
00:02:59,200 --> 00:03:01,440
based on kind of like latest developments,

57
00:03:01,440 --> 00:03:05,680
how have your estimates of superintelligence development

58
00:03:05,680 --> 00:03:08,360
shifted over time, like kind of in hindsight,

59
00:03:08,360 --> 00:03:11,600
where we are now in 2022, looking back,

60
00:03:11,600 --> 00:03:13,160
how do you think things are going?

61
00:03:13,160 --> 00:03:14,600
Are things proceeding faster or slower

62
00:03:14,600 --> 00:03:16,320
than you might have thought?

63
00:03:16,320 --> 00:03:17,600
Where do you think we are?

64
00:03:18,600 --> 00:03:23,520
I think since the book superintelligence came out in 2014,

65
00:03:23,520 --> 00:03:27,640
developments have been faster than expected.

66
00:03:27,640 --> 00:03:31,000
So timelines generally have contracted.

67
00:03:32,640 --> 00:03:37,560
It's quite impressive to see the rapid pace of advances

68
00:03:37,560 --> 00:03:39,280
in recent years,

69
00:03:39,280 --> 00:03:42,600
and how the same set of basic techniques,

70
00:03:43,600 --> 00:03:45,880
big, deep neural networks,

71
00:03:45,880 --> 00:03:48,160
and specifically transformer models

72
00:03:48,160 --> 00:03:51,920
just seem to keep working in many different domains.

73
00:03:51,920 --> 00:03:54,040
And even as you scale them up,

74
00:03:54,040 --> 00:03:56,520
you continue to get better results.

75
00:03:58,760 --> 00:04:00,480
And as the chefs,

76
00:04:00,480 --> 00:04:02,400
what have been some of the most surprising results

77
00:04:02,400 --> 00:04:05,400
from this that you think,

78
00:04:05,400 --> 00:04:07,120
I don't know, maybe you just didn't expect

79
00:04:07,120 --> 00:04:09,760
this particular concrete thing to be possible so soon?

80
00:04:10,640 --> 00:04:16,640
I think AlphaGo happened ahead of schedule.

81
00:04:16,640 --> 00:04:21,640
Well, I mean, I think just recently before it happened,

82
00:04:21,640 --> 00:04:23,680
it was kind of clear that it was going to happen,

83
00:04:23,680 --> 00:04:27,760
but I think it was quite impressive

84
00:04:27,760 --> 00:04:32,840
that you could take something that is a very deep pattern

85
00:04:32,840 --> 00:04:35,480
recognition problem with deep strategy

86
00:04:35,480 --> 00:04:38,720
where humans have worked for thousands of years

87
00:04:38,720 --> 00:04:41,560
to try to refine and come up with the best strategies

88
00:04:41,560 --> 00:04:46,000
that you could just solve it with AI.

89
00:04:50,000 --> 00:04:54,000
And then I think the GPT-3, the large language models,

90
00:04:54,000 --> 00:04:55,680
is, I guess, slightly...

91
00:04:55,680 --> 00:05:00,160
I mean, I don't think any of these is hugely surprising.

92
00:05:00,160 --> 00:05:04,080
And by now, we kind of expect to be surprised,

93
00:05:04,080 --> 00:05:05,520
and so we are not really surprised,

94
00:05:05,520 --> 00:05:07,600
but still, yeah, I think...

95
00:05:09,720 --> 00:05:13,560
these are impressive achievements.

96
00:05:13,560 --> 00:05:18,360
And I guess even just before that,

97
00:05:18,360 --> 00:05:22,360
the fact that image recognition and image processing

98
00:05:22,360 --> 00:05:27,600
was one of the first really cool things that started to work

99
00:05:27,600 --> 00:05:30,120
is maybe a little bit surprising,

100
00:05:30,120 --> 00:05:32,720
and given that it's a large chunk of the human brain

101
00:05:32,720 --> 00:05:35,360
that is devoted to visual processing,

102
00:05:35,360 --> 00:05:38,440
it's not like some kind of simple logic-chopping activity.

103
00:05:38,440 --> 00:05:40,520
And so the fact that that fell into place

104
00:05:40,520 --> 00:05:45,040
and that you can do this quite sophisticated manipulation

105
00:05:45,040 --> 00:05:55,040
of imagery, I think, was slightly surprising at the time.

106
00:05:55,040 --> 00:05:58,760
What do you think about developments like AlphaFold

107
00:05:58,760 --> 00:06:01,680
and just solving that set of challenges?

108
00:06:01,680 --> 00:06:04,920
Do you think that that is substantially different,

109
00:06:04,920 --> 00:06:07,720
or is it not like a substantially...

110
00:06:07,720 --> 00:06:09,360
it's just kind of a very great application,

111
00:06:09,360 --> 00:06:10,760
but...

112
00:06:10,760 --> 00:06:14,920
or do you think that that's an important improvement?

113
00:06:14,920 --> 00:06:16,120
I mean, in terms of surprise,

114
00:06:16,120 --> 00:06:18,800
like, I guess once you can do AlphaFold,

115
00:06:18,800 --> 00:06:22,960
it's not so surprising that it should work for AlphaFold as well.

116
00:06:22,960 --> 00:06:25,440
Like, humans have put in less brain power

117
00:06:25,440 --> 00:06:31,080
into figuring out how to fold proteins than into playing Go.

118
00:06:31,080 --> 00:06:33,320
And it's, at least superficially,

119
00:06:33,320 --> 00:06:37,680
looks like the same kind of spatial pattern type of stuff.

120
00:06:38,480 --> 00:06:41,120
Obviously, in terms of practical ramifications,

121
00:06:41,120 --> 00:06:44,720
AlphaFold is potentially a lot more useful

122
00:06:44,720 --> 00:06:51,800
and for medicine and chemical research,

123
00:06:51,800 --> 00:06:56,400
maybe like extensions of the same system.

124
00:06:56,400 --> 00:07:00,480
I do think that as we move into some of these more applied areas,

125
00:07:00,480 --> 00:07:07,080
that there are potential security concerns

126
00:07:07,120 --> 00:07:10,160
that we need to also start to take more seriously.

127
00:07:10,160 --> 00:07:13,520
I mean, my work has been focused more on risks arising

128
00:07:13,520 --> 00:07:15,480
from human level or superintelligence,

129
00:07:15,480 --> 00:07:17,840
like general AGI, where they can reason

130
00:07:17,840 --> 00:07:20,600
and have a kind of transformative impact on the world.

131
00:07:20,600 --> 00:07:24,200
But there might also be some narrower domains

132
00:07:24,200 --> 00:07:29,560
where there will be smaller but still significant issues.

133
00:07:29,560 --> 00:07:31,760
So one of those would be in synthetic biology

134
00:07:31,760 --> 00:07:36,760
if it becomes too easy to concoct bad stuff.

135
00:07:36,960 --> 00:07:40,680
It might be, for example, that the scientific model

136
00:07:40,680 --> 00:07:45,080
of open publication and make all your models

137
00:07:45,080 --> 00:07:47,800
ideally available to anybody to do anything

138
00:07:47,800 --> 00:07:50,600
is not the right model for those application areas.

139
00:07:54,440 --> 00:07:56,080
When you think about the current architectures,

140
00:07:56,080 --> 00:07:59,040
and certainly the language models

141
00:07:59,040 --> 00:08:01,040
have been extraordinarily successful

142
00:08:01,040 --> 00:08:02,040
in a variety of domains.

143
00:08:02,040 --> 00:08:05,200
But do you think that this is the architecture

144
00:08:05,200 --> 00:08:08,200
that is likely to evolve into an AGI?

145
00:08:08,200 --> 00:08:10,640
Or do you think that there's some substantial architectural

146
00:08:10,640 --> 00:08:14,040
improvements that humans have to make first?

147
00:08:16,240 --> 00:08:18,880
My guess would be that if there are substantial

148
00:08:18,880 --> 00:08:20,400
additional architectural improvements,

149
00:08:20,400 --> 00:08:22,440
there are not that many of them.

150
00:08:24,000 --> 00:08:29,000
And maybe they would be built on top of transformer models

151
00:08:29,200 --> 00:08:32,240
or connected up to the transformer models

152
00:08:32,240 --> 00:08:34,400
or some variation of transformer models.

153
00:08:36,200 --> 00:08:39,880
So maybe, I don't know, like my media, I guess,

154
00:08:39,880 --> 00:08:43,040
would be made, I don't know, maybe just something

155
00:08:43,040 --> 00:08:46,880
that is as big an advance as transformers were.

156
00:08:46,880 --> 00:08:48,280
Like if we get one more of those,

157
00:08:48,280 --> 00:08:50,880
like that could easily be, I mean, it's also possible

158
00:08:50,880 --> 00:08:52,400
like just scaling up what we currently have

159
00:08:52,400 --> 00:08:54,320
with some minor things would suffice.

160
00:08:56,480 --> 00:08:58,160
But if there is some other thing,

161
00:08:58,160 --> 00:09:02,560
like you need to connect it up with some kind

162
00:09:02,600 --> 00:09:05,320
of external memory system,

163
00:09:05,320 --> 00:09:08,720
or you need some other inductive bias

164
00:09:08,720 --> 00:09:12,080
that make the representations

165
00:09:12,080 --> 00:09:14,200
on a more easily composable insert,

166
00:09:14,200 --> 00:09:16,640
like some kind of extra thing like that,

167
00:09:16,640 --> 00:09:19,260
that may or may not be very hard to discover.

168
00:09:20,240 --> 00:09:22,800
That would not at all be surprising.

169
00:09:25,400 --> 00:09:27,000
I guess we'll find out.

170
00:09:27,000 --> 00:09:28,040
Yeah.

171
00:09:28,040 --> 00:09:31,320
Do you think that these models could,

172
00:09:31,320 --> 00:09:33,800
I mean, they're certainly being used to optimize

173
00:09:33,800 --> 00:09:35,880
themselves and so on and guide the design

174
00:09:35,880 --> 00:09:37,280
and there's all kinds of structures

175
00:09:37,280 --> 00:09:39,240
in which models are being used to,

176
00:09:40,240 --> 00:09:43,280
there's layers and layers and layers of metamodeling.

177
00:09:43,280 --> 00:09:45,720
Do you think that these are kind of getting close

178
00:09:45,720 --> 00:09:47,600
to this kind of recursive self-improvement

179
00:09:47,600 --> 00:09:50,000
of being able to kind of very generally explore

180
00:09:50,000 --> 00:09:53,080
the constraint space to try and solve

181
00:09:53,080 --> 00:09:54,080
like larger scale problems?

182
00:09:54,080 --> 00:09:56,640
Like I'm imagining here some structure

183
00:09:56,680 --> 00:10:01,360
where you have some list of problems

184
00:10:01,360 --> 00:10:03,800
and you have some model sampling between these

185
00:10:03,800 --> 00:10:04,800
and you start with the easy ones

186
00:10:04,800 --> 00:10:07,400
and then you try to train populations of agents

187
00:10:07,400 --> 00:10:10,840
or populations of intelligences to be able to solve these

188
00:10:10,840 --> 00:10:13,960
and then kind of over time just kind of scale up the system.

189
00:10:13,960 --> 00:10:14,880
Do you think that that kind of thing,

190
00:10:14,880 --> 00:10:19,040
it seems to me that it'd be like,

191
00:10:19,040 --> 00:10:21,200
thankfully nobody has really tried this

192
00:10:21,200 --> 00:10:22,840
but it doesn't seem like far away

193
00:10:22,840 --> 00:10:24,640
from something that could be possible.

194
00:10:25,640 --> 00:10:28,880
Yeah, I guess we're seeing all limited versions of AI

195
00:10:28,880 --> 00:10:31,520
being applied to help AI research.

196
00:10:31,520 --> 00:10:33,480
I mean, we have like co-pilot

197
00:10:33,480 --> 00:10:35,960
and general kind of coding assistance.

198
00:10:36,840 --> 00:10:40,000
Of course, you have various forms of hyper parameter

199
00:10:40,000 --> 00:10:41,640
optimization regimes.

200
00:10:42,520 --> 00:10:43,880
There've also been some applications

201
00:10:43,880 --> 00:10:47,920
in the design of hardware where kind of circuit layout

202
00:10:47,920 --> 00:10:52,120
has been done, I think for the TPU4,

203
00:10:52,200 --> 00:10:56,800
I think Google used AI assistance

204
00:10:56,800 --> 00:11:00,440
to kind of optimize the layout of the circuitry.

205
00:11:01,600 --> 00:11:05,240
Data centers, cooling machinery that have been like,

206
00:11:05,240 --> 00:11:06,960
you can kind of shave off some percent

207
00:11:06,960 --> 00:11:10,720
by having that optimized by some RL system.

208
00:11:10,720 --> 00:11:13,200
And so, I think we'll certainly see

209
00:11:13,200 --> 00:11:14,880
more incremental stuff like that.

210
00:11:15,880 --> 00:11:20,880
My guess is that by the time we get

211
00:11:22,240 --> 00:11:23,880
like a really strong feedback loop

212
00:11:23,880 --> 00:11:26,240
where sort of the AI can do the core thing

213
00:11:26,240 --> 00:11:27,360
that researchers are doing,

214
00:11:27,360 --> 00:11:32,360
like the actual identifying the right research questions

215
00:11:32,960 --> 00:11:37,960
and approaches and like that seems quite late in it.

216
00:11:38,680 --> 00:11:40,040
Like when that happens,

217
00:11:40,040 --> 00:11:41,680
we are pretty close to the singularity

218
00:11:41,680 --> 00:11:44,400
or the takeoff or whatever the ramp

219
00:11:44,400 --> 00:11:46,560
or whatever the shape of that will be.

220
00:11:49,800 --> 00:11:51,720
But certainly there's more domain specific

221
00:11:51,720 --> 00:11:55,160
incremental ways of accelerating AI advances.

222
00:11:55,160 --> 00:11:57,040
I think we're seeing some of already

223
00:11:57,040 --> 00:11:58,800
and can expect to see more of.

224
00:11:59,680 --> 00:12:02,120
Speaking about takeoff, do you sort of expect,

225
00:12:02,120 --> 00:12:05,200
or based on what you have seen so far,

226
00:12:05,200 --> 00:12:08,960
do you think we're more on a slow, moderate or fast takeoff?

227
00:12:08,960 --> 00:12:13,400
This is sort of the three options that you thought through.

228
00:12:13,400 --> 00:12:14,240
Yeah.

229
00:12:19,600 --> 00:12:24,600
I mean, I still think that the slow looks less plausible,

230
00:12:25,200 --> 00:12:29,520
meaning decades, say, between when you get something

231
00:12:29,520 --> 00:12:31,360
roughly human level until you get something

232
00:12:31,360 --> 00:12:33,440
that completely leaves us in the dust.

233
00:12:33,440 --> 00:12:38,440
That seemed less likely back when I wrote the book

234
00:12:40,000 --> 00:12:41,840
and still seems less likely today.

235
00:12:41,840 --> 00:12:46,080
I guess we have a little bit more granularity now

236
00:12:46,080 --> 00:12:48,960
in that we have these model systems that work

237
00:12:48,960 --> 00:12:50,800
and you can at least consider these scenarios

238
00:12:50,800 --> 00:12:53,120
where human double AI is achieved

239
00:12:53,120 --> 00:12:56,520
by scaling up current systems or variations of that.

240
00:12:56,520 --> 00:12:59,960
That gives us a little bit more of a concrete picture

241
00:12:59,960 --> 00:13:02,600
of at least one way in which these can develop.

242
00:13:05,320 --> 00:13:08,400
And it's possible that you might then have something

243
00:13:08,400 --> 00:13:13,400
that is really very dependent on compute

244
00:13:14,680 --> 00:13:17,720
and that really you get performance kind of proportional

245
00:13:17,720 --> 00:13:21,920
to the size of the model or the length of the training

246
00:13:21,920 --> 00:13:24,160
and in a relatively smooth way.

247
00:13:26,440 --> 00:13:27,600
So in some of those scenarios,

248
00:13:27,600 --> 00:13:32,600
you might have something that is less than super rapid

249
00:13:32,720 --> 00:13:35,280
because what you will get is something

250
00:13:35,280 --> 00:13:40,280
that costs like a billion dollar to train up one human level AI

251
00:13:44,760 --> 00:13:48,600
and then you might immediately be able to run

252
00:13:48,600 --> 00:13:53,600
multiple of them because it takes a lot more to train up a model

253
00:13:54,960 --> 00:13:56,200
than to run it.

254
00:13:56,200 --> 00:13:57,960
So you might then be able to run like a hundred

255
00:13:57,960 --> 00:13:58,800
or a thousand of them,

256
00:13:58,800 --> 00:14:02,720
but that's still not enough to out compete

257
00:14:03,600 --> 00:14:06,720
on the order of 10 billion humans, right?

258
00:14:06,720 --> 00:14:09,400
So depending on like,

259
00:14:09,400 --> 00:14:11,320
if you really stretch yourself very far

260
00:14:11,320 --> 00:14:14,600
to just barely be able to run a model as big as a human,

261
00:14:15,520 --> 00:14:18,480
it might then take a significant period of time

262
00:14:18,480 --> 00:14:22,080
before you can go many orders of magnitude above that

263
00:14:22,080 --> 00:14:23,960
to sort of get something,

264
00:14:23,960 --> 00:14:26,240
like if you need to scale that up by a factor of a million,

265
00:14:26,240 --> 00:14:29,360
say to go from running like on the order of a thousand humans

266
00:14:29,360 --> 00:14:33,960
to a billion humans getting through six orders of magnitude

267
00:14:35,360 --> 00:14:38,800
when you're already like using a billion dollars

268
00:14:38,800 --> 00:14:41,360
and like a large chunk of your data centers,

269
00:14:41,360 --> 00:14:44,520
like that might just not be an instantaneous process.

270
00:14:44,520 --> 00:14:47,120
So there are some scenarios where this would happen

271
00:14:47,980 --> 00:14:50,120
more on a sort of intermediate time scale.

272
00:14:51,280 --> 00:14:56,280
Now, in some sense, I guess that's like the kind of the

273
00:14:56,480 --> 00:14:58,360
the baseline projection.

274
00:14:58,360 --> 00:15:00,080
Like if you just like extrapolate

275
00:15:00,080 --> 00:15:02,440
the way things currently work,

276
00:15:02,440 --> 00:15:05,440
I don't think we can preclude the possibility

277
00:15:05,440 --> 00:15:07,520
of there being more rapid capability jumps.

278
00:15:07,520 --> 00:15:08,480
Like, hey, of course,

279
00:15:08,480 --> 00:15:11,400
if there is like some missing architectural invention

280
00:15:11,400 --> 00:15:12,240
that we haven't made,

281
00:15:12,240 --> 00:15:14,000
that suddenly makes it click.

282
00:15:14,000 --> 00:15:18,200
But you also have these phenomenon like rocking

283
00:15:18,200 --> 00:15:22,760
where sometimes you have a kind of discrete jump

284
00:15:22,760 --> 00:15:25,660
in some particular type of capability.

285
00:15:26,660 --> 00:15:29,420
Like maybe multi-step reasoning

286
00:15:29,420 --> 00:15:33,220
where if each step has less than X percent chance

287
00:15:33,220 --> 00:15:34,340
of being correct,

288
00:15:34,340 --> 00:15:37,540
then like you get an exponential chance

289
00:15:37,540 --> 00:15:38,980
of reasoning correctly

290
00:15:38,980 --> 00:15:41,700
and you really can't do more than like three or four or five steps.

291
00:15:41,700 --> 00:15:44,340
But maybe once you get it above a certain level

292
00:15:44,340 --> 00:15:47,980
and then maybe you can do some sort of self-correction reasoning

293
00:15:47,980 --> 00:15:52,380
like analogous to like quantum computation protocols

294
00:15:52,500 --> 00:15:55,500
like that, you could also imagine cases

295
00:15:55,500 --> 00:15:57,500
where like things come together

296
00:15:57,500 --> 00:15:59,820
and you suddenly get the specific types of things

297
00:15:59,820 --> 00:16:02,060
that make us humans have the extra oomph

298
00:16:02,060 --> 00:16:04,060
that we have relative to other animals,

299
00:16:04,060 --> 00:16:07,220
like full ability to learn from language

300
00:16:07,220 --> 00:16:10,060
and to reason and plan on that.

301
00:16:10,060 --> 00:16:15,060
So yeah, I wouldn't preclude these more rapid takeoff scenarios

302
00:16:15,740 --> 00:16:18,540
either at all, like...

303
00:16:18,540 --> 00:16:21,740
Yeah, certainly some of the latest developments

304
00:16:21,740 --> 00:16:24,140
and some scaling down some of the models

305
00:16:24,140 --> 00:16:27,540
and getting similar results sort of point to there being

306
00:16:27,540 --> 00:16:30,340
just a lot of inefficiencies in the training process now.

307
00:16:30,340 --> 00:16:33,100
And once you sort of know what you're sort of looking for,

308
00:16:33,100 --> 00:16:35,900
you can kind of a blade away a lot of pieces.

309
00:16:35,900 --> 00:16:37,820
And so something like that could happen

310
00:16:37,820 --> 00:16:40,020
with a general learning algorithm.

311
00:16:40,020 --> 00:16:42,700
Yeah, so certainly now you find like,

312
00:16:42,700 --> 00:16:44,500
yeah, so first you achieve state of the art

313
00:16:44,500 --> 00:16:46,500
and then like six months or 12 months later,

314
00:16:46,500 --> 00:16:48,300
you can achieve the same thing

315
00:16:48,300 --> 00:16:51,060
with maybe 10% of the computer or something.

316
00:16:51,060 --> 00:16:54,500
Now, I would expect a little bit of that to go away.

317
00:16:54,500 --> 00:16:58,140
As these systems become bigger and more expensive,

318
00:16:58,140 --> 00:17:02,980
you might imagine more of the easy gains

319
00:17:02,980 --> 00:17:04,900
to be made earlier on.

320
00:17:04,900 --> 00:17:08,740
Like if you really have a lot of smart humans

321
00:17:08,740 --> 00:17:12,020
working really hard on building a system,

322
00:17:12,020 --> 00:17:14,620
you might have plucked more of the low-hanging fruits

323
00:17:14,620 --> 00:17:18,100
than if it were like a two-person postdoc team

324
00:17:18,100 --> 00:17:19,700
that were working for a few weeks.

325
00:17:19,700 --> 00:17:22,020
Chances are that will be big, easy additional things

326
00:17:22,020 --> 00:17:23,620
you could do to improve that system already.

327
00:17:23,620 --> 00:17:25,780
But if you're spending many billions of these,

328
00:17:25,780 --> 00:17:28,620
like you're gonna look quite hard

329
00:17:28,620 --> 00:17:31,900
if there are ways to sort of speed up the training process

330
00:17:31,900 --> 00:17:34,180
so you could like save a hundred million.

331
00:17:37,580 --> 00:17:42,700
Are you hopeful that restricting hardware development

332
00:17:42,700 --> 00:17:44,980
or use is a promising path?

333
00:17:44,980 --> 00:17:46,580
I mean, semiconductor manufacturing

334
00:17:46,580 --> 00:17:47,620
is extremely difficult,

335
00:17:47,620 --> 00:17:50,460
but more and more companies are sort of forced to do it

336
00:17:50,460 --> 00:17:53,460
because of kind of hitting the barriers

337
00:17:53,460 --> 00:17:56,300
with just the size of the systems

338
00:17:56,300 --> 00:17:59,460
and then needing to do special applications

339
00:17:59,460 --> 00:18:00,380
and special purpose things.

340
00:18:00,380 --> 00:18:02,180
And many more companies are now developing

341
00:18:02,180 --> 00:18:04,740
their own chips and so on.

342
00:18:04,740 --> 00:18:08,180
So are kind of like hardware restrictions viable here

343
00:18:08,180 --> 00:18:12,740
or is that a pathway that's just unlikely to work?

344
00:18:13,580 --> 00:18:17,420
Yeah, so a lot of people can like design their own chips,

345
00:18:17,420 --> 00:18:22,340
but only a few actors can actually build them.

346
00:18:25,340 --> 00:18:30,460
And then there are some other choke points further upstream

347
00:18:30,460 --> 00:18:33,620
in terms of making the equipment for the factories

348
00:18:33,620 --> 00:18:34,540
that build the chips

349
00:18:34,540 --> 00:18:36,820
where currently to make cutting edge chips,

350
00:18:36,820 --> 00:18:40,420
there's like ASML, which is a single node.

351
00:18:41,260 --> 00:18:46,260
And indeed, we do see like, I mean with these recent moves

352
00:18:50,020 --> 00:18:55,020
by the US to restrict exports of cutting edge chips to China

353
00:18:56,140 --> 00:18:57,860
and quite comprehensive,

354
00:18:57,860 --> 00:18:59,460
also not to sell the equipment,

355
00:18:59,460 --> 00:19:01,860
also not to allow American persons

356
00:19:01,860 --> 00:19:04,220
to work for these companies.

357
00:19:04,900 --> 00:19:09,900
I don't know what fraction of the motivation for this

358
00:19:10,820 --> 00:19:13,620
is like AI specifically versus more generally

359
00:19:13,620 --> 00:19:15,500
a sense of this being a high tech area

360
00:19:15,500 --> 00:19:18,460
that's gonna be key to national competitiveness.

361
00:19:22,180 --> 00:19:24,500
Yeah, I don't think it's out of the question

362
00:19:24,500 --> 00:19:27,740
that I mean, compared to the alternative,

363
00:19:27,740 --> 00:19:32,020
which would be like to like restrict access to ideas

364
00:19:32,020 --> 00:19:33,180
and algorithms and stuff.

365
00:19:33,340 --> 00:19:35,580
I mean, that might work for a short period of time,

366
00:19:35,580 --> 00:19:40,580
but independent discovery means it's like, yeah,

367
00:19:42,580 --> 00:19:45,100
at most a short term stop cap measure,

368
00:19:45,100 --> 00:19:47,060
whereas the hardware would like take a lot longer

369
00:19:47,060 --> 00:19:49,500
if you needed to build up like the whole supply chain

370
00:19:49,500 --> 00:19:53,660
on your own, like that would be a multi decade project, right?

371
00:19:54,620 --> 00:19:59,620
Now that said, I think what I would favor would be for there

372
00:20:00,060 --> 00:20:05,060
to be the ability at the critical time to go slow,

373
00:20:05,140 --> 00:20:08,020
to have a short pause, maybe to check systems

374
00:20:08,020 --> 00:20:13,020
and to avoid the most cutthroat type of tech race

375
00:20:15,500 --> 00:20:17,380
to just launch as quickly as possible

376
00:20:17,380 --> 00:20:21,060
because you get scooped if you take even an extra week to,

377
00:20:21,060 --> 00:20:23,100
like I think that would be bad now.

378
00:20:24,460 --> 00:20:26,740
So having enough coordination or control

379
00:20:26,740 --> 00:20:30,180
to be able to go at the moderate pace

380
00:20:30,180 --> 00:20:33,820
when you sort of reach approach human level would be good.

381
00:20:33,820 --> 00:20:36,900
I wouldn't want to stop the development

382
00:20:36,900 --> 00:20:41,540
of advanced machine intelligence permanently

383
00:20:41,540 --> 00:20:45,260
or even like have a very long pause either.

384
00:20:45,260 --> 00:20:48,380
I think that brings its own negatives.

385
00:20:50,660 --> 00:20:52,980
And I think some of these attempts

386
00:20:52,980 --> 00:20:55,820
to restrict the chip supply also have

387
00:20:57,660 --> 00:21:00,460
the side effect of creating more adversarial dynamic.

388
00:21:00,460 --> 00:21:04,860
I think it would be really nice if we could have a world

389
00:21:04,860 --> 00:21:07,660
where the leading powers were more on the same page

390
00:21:07,660 --> 00:21:10,500
or friendly or at least had constructive

391
00:21:10,500 --> 00:21:11,620
cooperative relationship.

392
00:21:11,620 --> 00:21:16,620
I think a lot of the ex-risk pie in general

393
00:21:17,820 --> 00:21:19,740
and the risk from AI in particular

394
00:21:22,620 --> 00:21:26,620
arises from the possibility of conflict of different kind.

395
00:21:26,660 --> 00:21:29,420
And so a world order that was more cooperative

396
00:21:31,380 --> 00:21:36,260
would look more promising for the future

397
00:21:36,260 --> 00:21:37,100
in many different ways.

398
00:21:37,100 --> 00:21:38,340
So I'm a little worried about,

399
00:21:38,340 --> 00:21:41,660
especially kind of more unilateral list move

400
00:21:41,660 --> 00:21:43,540
to kind of kneecap the competitor

401
00:21:43,540 --> 00:21:44,780
and to be playing nasty.

402
00:21:44,780 --> 00:21:48,860
Like I feel that, yeah, I'm very uneasy about that.

403
00:21:51,260 --> 00:21:53,060
It sounds, well, so if the hardware,

404
00:21:53,820 --> 00:21:57,780
if ideas or hardware will only buy a certain amount of time,

405
00:21:57,780 --> 00:22:00,460
then really AI alignment is the best path forward

406
00:22:00,460 --> 00:22:03,460
and very much agree that we don't want to restrict

407
00:22:03,460 --> 00:22:06,140
the creation of digital intelligence

408
00:22:07,020 --> 00:22:12,020
and that that's sort of the next evolutionary jumps.

409
00:22:12,260 --> 00:22:13,220
And there's some questions there

410
00:22:13,220 --> 00:22:14,780
around kind of like which paths we would take

411
00:22:14,780 --> 00:22:17,940
and how do we develop brain computer interfaces

412
00:22:17,940 --> 00:22:20,620
and cold-burn emulation and so on.

413
00:22:21,460 --> 00:22:24,300
We're kind of like, even before getting into that,

414
00:22:24,300 --> 00:22:26,220
how hopeful are you that we might solve

415
00:22:26,220 --> 00:22:27,780
the AI alignment problem?

416
00:22:32,220 --> 00:22:34,540
Moderately, I guess I'm quite agnostic,

417
00:22:35,740 --> 00:22:39,580
but I think the main uncertainty is how hard the problem

418
00:22:39,580 --> 00:22:41,580
turns out to be.

419
00:22:41,580 --> 00:22:43,700
And then there's a little extra uncertainty

420
00:22:43,700 --> 00:22:46,940
as to how the degree to which we get our act together.

421
00:22:47,780 --> 00:22:50,780
But I think like out of those two variables,

422
00:22:50,780 --> 00:22:55,020
like the realistic scenarios in which we either

423
00:22:55,020 --> 00:22:59,740
like are lazy and don't like focus on it

424
00:22:59,740 --> 00:23:01,900
versus the ones where we get a lot of smart people

425
00:23:01,900 --> 00:23:02,740
working on it.

426
00:23:02,740 --> 00:23:03,740
So there's some uncertainty there

427
00:23:03,740 --> 00:23:05,020
that affects the success chance,

428
00:23:05,020 --> 00:23:07,420
but I think that's dwarfed by our uncertainty

429
00:23:07,420 --> 00:23:11,500
about how intrinsically hard the problem is to solve.

430
00:23:11,500 --> 00:23:16,500
So you could say that like the most important component

431
00:23:16,900 --> 00:23:18,540
of our strategy should be to hope

432
00:23:18,540 --> 00:23:21,180
that the problem is not too hard.

433
00:23:21,180 --> 00:23:23,660
Yeah, so let's try to tackle it.

434
00:23:23,660 --> 00:23:26,820
So how do you, as you thought about this problem,

435
00:23:26,820 --> 00:23:30,060
have you kind of been able to break it down

436
00:23:30,060 --> 00:23:34,260
into components and parts or maybe evolved your thinking

437
00:23:34,260 --> 00:23:35,460
of the shape of the problem?

438
00:23:35,460 --> 00:23:37,060
Like what are you thinking now?

439
00:23:39,100 --> 00:23:43,220
Well, I think the field as a whole has made

440
00:23:43,220 --> 00:23:45,900
significant advances and developed a lot

441
00:23:45,940 --> 00:23:48,940
since when I was writing the book

442
00:23:48,940 --> 00:23:50,940
where it was like really a non-existent field.

443
00:23:50,940 --> 00:23:53,340
There were a few people on the internet here and there,

444
00:23:53,340 --> 00:23:55,700
but now it's an active research field

445
00:23:55,700 --> 00:23:58,260
with a growing number of smart people

446
00:23:58,260 --> 00:24:01,820
who have been working full-time on this for a number of years

447
00:24:01,820 --> 00:24:04,660
and writing papers that build on previous papers

448
00:24:04,660 --> 00:24:06,220
with technical stuff.

449
00:24:06,220 --> 00:24:11,220
And all the key AI labs have now some contingent

450
00:24:11,820 --> 00:24:14,180
of people who are working on alignment.

451
00:24:14,180 --> 00:24:17,060
DeepMind has, OpenAI has, Anthropic has.

452
00:24:17,980 --> 00:24:19,140
So that's all good.

453
00:24:19,140 --> 00:24:20,780
Now, within this community,

454
00:24:20,780 --> 00:24:24,820
there is, I guess, a distribution of levels of optimism

455
00:24:24,820 --> 00:24:28,420
ranging from people very pessimistic,

456
00:24:28,420 --> 00:24:30,860
like Elias Yudkowski, for example.

457
00:24:30,860 --> 00:24:34,700
And I guess there are people even more pessimistic than him,

458
00:24:34,700 --> 00:24:37,180
but he's kind of at one end

459
00:24:37,180 --> 00:24:42,180
and towards people with more moderate levels of optimism,

460
00:24:43,180 --> 00:24:44,460
like Paul Cristiano,

461
00:24:44,460 --> 00:24:47,780
and then others who think it's kind of,

462
00:24:50,300 --> 00:24:52,660
it's something that we'll deal with it when we get to it

463
00:24:52,660 --> 00:24:56,020
and who don't seem too fast about it.

464
00:24:57,020 --> 00:24:59,780
I think there's a lot of uncertainty

465
00:24:59,780 --> 00:25:02,020
on the hardness level.

466
00:25:02,020 --> 00:25:04,060
Now, as far as how you break it down, yeah.

467
00:25:04,060 --> 00:25:06,060
So there are different ways of doing this.

468
00:25:07,700 --> 00:25:09,180
There's not yet one paradigm

469
00:25:09,180 --> 00:25:13,220
that all competent AI safety researchers share

470
00:25:13,220 --> 00:25:15,580
in terms of the best lens to look at this.

471
00:25:15,580 --> 00:25:17,740
So it decomposes in slightly different ways,

472
00:25:17,740 --> 00:25:20,860
depending on your angle of approach,

473
00:25:20,860 --> 00:25:24,100
but certainly one can identify different facets

474
00:25:24,100 --> 00:25:24,940
that one can work on.

475
00:25:24,940 --> 00:25:27,660
So for example, interpretability tools

476
00:25:27,660 --> 00:25:29,860
seem on many different approaches,

477
00:25:29,860 --> 00:25:32,340
like a useful ingredients to have,

478
00:25:32,340 --> 00:25:35,300
like basically insights or techniques

479
00:25:35,300 --> 00:25:38,500
that allow us better to see what is going on

480
00:25:38,500 --> 00:25:40,180
in a big neural network.

481
00:25:42,820 --> 00:25:47,820
You could have one approach where you try to get AI systems

482
00:25:48,100 --> 00:25:53,100
that try to learn to match some human example of behavior,

483
00:26:01,340 --> 00:26:04,460
either like one human or some corpus of humans

484
00:26:04,460 --> 00:26:08,460
and then tries to just perform a next action

485
00:26:08,460 --> 00:26:10,820
that's like the same as its best guess

486
00:26:10,820 --> 00:26:12,580
about what this reference human

487
00:26:12,580 --> 00:26:14,260
would do in the same situation.

488
00:26:17,100 --> 00:26:22,100
And then you could try to do forms of amplification on that.

489
00:26:22,100 --> 00:26:26,980
So like if you could like faithfully model one human,

490
00:26:26,980 --> 00:26:29,380
well, then you just get like a human level,

491
00:26:29,380 --> 00:26:31,180
like intelligence, you might want to go beyond that.

492
00:26:31,180 --> 00:26:34,100
But if you could then create many of these models

493
00:26:34,100 --> 00:26:35,700
that each do what the human do,

494
00:26:35,700 --> 00:26:38,620
can you put them together in some bureaucracy

495
00:26:38,620 --> 00:26:41,660
or do some other clever bootstrapping or self-criticism?

496
00:26:45,260 --> 00:26:48,860
So that would be one approach.

497
00:26:48,860 --> 00:26:53,860
You could, yeah, you could try to use

498
00:26:57,620 --> 00:26:59,060
sort of inverse reinforcement learning

499
00:26:59,060 --> 00:27:01,580
to infer like a human's preference function

500
00:27:01,580 --> 00:27:04,300
and then try to optimize for that

501
00:27:04,300 --> 00:27:05,940
or maybe not strictly optimized

502
00:27:05,940 --> 00:27:08,300
but doing some kind of softer optimization.

503
00:27:12,620 --> 00:27:14,500
Yeah, there are a bunch of different ideas.

504
00:27:14,500 --> 00:27:17,460
Like some safety work is more like trying to more precisely

505
00:27:17,460 --> 00:27:20,900
understand and illustrate in toy examples

506
00:27:20,900 --> 00:27:22,060
how things could go wrong

507
00:27:22,060 --> 00:27:23,740
because that's like often the first step

508
00:27:23,740 --> 00:27:26,620
to creating a solution is to really deeply understand

509
00:27:26,620 --> 00:27:28,940
what the problem is and then illustrate it.

510
00:27:28,940 --> 00:27:33,540
And yeah, that can be useful as well.

511
00:27:35,140 --> 00:27:38,580
It's interesting now that we have these models

512
00:27:38,580 --> 00:27:41,620
that can talk as it were or like use language

513
00:27:41,620 --> 00:27:46,620
that kind of opens up an additional interface,

514
00:27:46,780 --> 00:27:48,940
like an additional way of interacting with these systems

515
00:27:48,940 --> 00:27:50,340
and trying out different things

516
00:27:53,340 --> 00:27:57,380
and a different way of illustrating the awkwardness.

517
00:27:57,380 --> 00:27:59,660
Like the idea of prompt engineering

518
00:27:59,660 --> 00:28:02,740
when you're trying to get an AI to do something

519
00:28:02,740 --> 00:28:04,100
and you're trying to figure out

520
00:28:04,100 --> 00:28:05,460
exactly the right formulation.

521
00:28:05,460 --> 00:28:08,340
Like that shows that we are not quite where we need to be

522
00:28:08,340 --> 00:28:11,700
in terms of directing the intrinsic capability

523
00:28:11,700 --> 00:28:12,900
of these large language models.

524
00:28:12,900 --> 00:28:17,260
So it's in there and yet we can't always even elicit it

525
00:28:17,260 --> 00:28:19,460
because you have to find exactly the right wording

526
00:28:19,460 --> 00:28:20,860
and then suddenly turns out this thing

527
00:28:20,860 --> 00:28:23,580
is actually perfectly capable of doing something

528
00:28:23,580 --> 00:28:26,540
which initially it seemed it failed that.

529
00:28:26,540 --> 00:28:28,500
So getting better at that

530
00:28:28,500 --> 00:28:30,820
or coming up with something better than prompt engineering

531
00:28:30,820 --> 00:28:33,380
like would be good.

532
00:28:33,380 --> 00:28:38,380
I'm kind of, I have some sympathy for an approach

533
00:28:41,820 --> 00:28:45,660
which I think has not been explored very much yet

534
00:28:45,660 --> 00:28:48,900
but partly because it's hard to explore it

535
00:28:48,900 --> 00:28:50,940
until the technology reaches a certain level

536
00:28:50,940 --> 00:28:54,900
of sophistication, which is the idea that as you get

537
00:28:54,900 --> 00:28:58,380
systems that become closer to human level

538
00:28:58,380 --> 00:28:59,980
in that conceptual ability.

539
00:29:00,020 --> 00:29:04,500
And that might then internally start to develop concepts

540
00:29:04,500 --> 00:29:07,260
that are more similar to human concepts

541
00:29:07,260 --> 00:29:11,100
including not just concepts about simple visual features

542
00:29:11,100 --> 00:29:12,980
and stuff, but more corresponding

543
00:29:12,980 --> 00:29:16,140
to our higher language concepts.

544
00:29:16,140 --> 00:29:19,500
Like our concept of a preference or a goal

545
00:29:19,500 --> 00:29:23,340
or a request or being safe, being reckless

546
00:29:23,340 --> 00:29:24,500
like these types of concepts.

547
00:29:24,500 --> 00:29:26,980
Like we humans seem relatively robustly

548
00:29:26,980 --> 00:29:29,260
to be able to master these concepts

549
00:29:29,260 --> 00:29:31,220
in the course of our normal development.

550
00:29:32,740 --> 00:29:35,860
Despite us having starting with different brains

551
00:29:35,860 --> 00:29:39,300
and having different environmental input and noise.

552
00:29:39,300 --> 00:29:42,260
And so maybe there is a relatively robust

553
00:29:42,260 --> 00:29:44,300
and convergent ways in which some of these concepts

554
00:29:44,300 --> 00:29:45,780
could be grasped.

555
00:29:45,780 --> 00:29:50,780
Then the hope would be that you could kind of train up an AI

556
00:29:51,740 --> 00:29:53,420
that doesn't need to be above human

557
00:29:53,420 --> 00:29:55,820
and maybe hardly even human

558
00:29:55,820 --> 00:29:58,080
that would then sort of internally form these concepts

559
00:29:58,080 --> 00:29:59,320
in the same way that we form them.

560
00:29:59,320 --> 00:30:01,400
And then once those concepts are in there,

561
00:30:01,400 --> 00:30:03,720
you might then be able to use those as building blocks

562
00:30:03,720 --> 00:30:05,640
to create a kind of alignment

563
00:30:06,640 --> 00:30:09,720
by sort of linking motivation to these concepts.

564
00:30:09,720 --> 00:30:11,880
It very hand-wavered, but I think something

565
00:30:11,880 --> 00:30:14,880
in that direction is one interesting approach

566
00:30:14,880 --> 00:30:16,600
to the alignment problem as well.

567
00:30:16,600 --> 00:30:17,960
Do you think there's some promise

568
00:30:17,960 --> 00:30:22,600
in trying to evolve a notion of morality and ethics?

569
00:30:22,600 --> 00:30:25,360
Meaning using simulations of environments

570
00:30:25,360 --> 00:30:29,160
where agents might learn to cooperate

571
00:30:29,160 --> 00:30:32,200
and over time learn D, put them through the same kind

572
00:30:32,200 --> 00:30:34,600
of game theory dynamics that gave rise

573
00:30:34,600 --> 00:30:39,280
to our own notions of symbiotes and ethics and so on.

574
00:30:40,360 --> 00:30:41,200
Potentially, yeah.

575
00:30:41,200 --> 00:30:44,400
I mean, I think you would wanna be looking very closely

576
00:30:44,400 --> 00:30:46,640
at exactly how you set things up

577
00:30:46,640 --> 00:30:48,000
and the dynamics that unfold.

578
00:30:48,000 --> 00:30:51,160
I mean, real revolution is sort of read in tooth and claw

579
00:30:51,160 --> 00:30:53,560
and can create wonderful cooperation,

580
00:30:53,600 --> 00:30:58,600
but also hostility and defection and manipulation

581
00:30:58,880 --> 00:31:00,080
and all kinds of things.

582
00:31:03,960 --> 00:31:08,000
But yes, certainly multi-agent systems

583
00:31:08,920 --> 00:31:12,080
with the right kind of incentive structures in place

584
00:31:12,080 --> 00:31:12,960
so that you evolve.

585
00:31:12,960 --> 00:31:16,520
Like evolution itself can produce many different kinds

586
00:31:16,520 --> 00:31:20,480
of outcomes, like depending on the environment,

587
00:31:21,480 --> 00:31:25,760
but that certainly could be come in some scenarios

588
00:31:25,760 --> 00:31:26,880
and increasingly important,

589
00:31:26,880 --> 00:31:28,920
like either whether it's an evolutionary system

590
00:31:28,920 --> 00:31:32,360
or in some of these other like a training environment,

591
00:31:32,360 --> 00:31:36,120
like the curriculum, like if these systems are shaped

592
00:31:36,120 --> 00:31:39,440
a lot by their data that they're trained on,

593
00:31:41,320 --> 00:31:43,200
so far we've just kind of slapped together

594
00:31:43,200 --> 00:31:47,160
some big data sets and not really fast too much

595
00:31:47,160 --> 00:31:48,800
about what's contained in it,

596
00:31:48,800 --> 00:31:51,800
but that might become an important component as well

597
00:31:51,800 --> 00:31:54,040
of alignment in certain of these scenarios.

598
00:31:56,000 --> 00:32:00,200
And are these directions the ones you find most promising

599
00:32:00,200 --> 00:32:02,040
or is there like a subset of these

600
00:32:02,040 --> 00:32:04,800
or maybe another one that you've been thinking about

601
00:32:05,800 --> 00:32:07,600
starting to kind of surface

602
00:32:07,600 --> 00:32:09,920
and help a lot of people that are working on this

603
00:32:09,920 --> 00:32:11,240
so likely watch this conversation.

604
00:32:11,240 --> 00:32:13,840
So are there any kind of pointers

605
00:32:13,840 --> 00:32:15,560
that you might give beyond these?

606
00:32:16,560 --> 00:32:21,560
Well, this would be some of the ones

607
00:32:22,560 --> 00:32:27,400
that I would like highlight somewhat arbitrarily,

608
00:32:27,400 --> 00:32:32,400
but yeah, I think like the Paul Cristiano capability

609
00:32:34,480 --> 00:32:37,360
amplification, the interpretability work,

610
00:32:40,360 --> 00:32:45,360
the idea of like growing human beings

611
00:32:45,560 --> 00:32:49,680
human level concepts and then using those as a basis

612
00:32:49,680 --> 00:32:54,680
to define goals or to sort of create the motivation system

613
00:32:55,000 --> 00:32:56,760
that uses those as primitives.

614
00:32:59,800 --> 00:33:04,800
It might also well be that there are entirely different

615
00:33:05,360 --> 00:33:06,880
conceptual ways of approaching this

616
00:33:06,880 --> 00:33:09,080
that are yet to be discovered.

617
00:33:11,240 --> 00:33:14,160
It's not a mature research field where we have,

618
00:33:14,160 --> 00:33:17,280
as I said, like we don't have an established paradigm

619
00:33:17,280 --> 00:33:19,880
that's clearly correct and that we now just need to,

620
00:33:19,880 --> 00:33:21,360
I think there are multiple paradigms

621
00:33:21,360 --> 00:33:22,880
and there might well be additional ones

622
00:33:22,880 --> 00:33:25,240
that just haven't had a champion yet

623
00:33:25,240 --> 00:33:27,880
to sort of really get people to take it seriously.

624
00:33:31,960 --> 00:33:33,800
So I think there is also a value

625
00:33:33,800 --> 00:33:36,520
to this more theoretical, a conceptual,

626
00:33:36,520 --> 00:33:39,760
almost philosophical exploratory work in just,

627
00:33:40,800 --> 00:33:43,640
yeah, coming at the problem from a different angle.

628
00:33:45,040 --> 00:33:49,160
Yeah, jumping into maybe agent-ness,

629
00:33:49,160 --> 00:33:54,160
how separable do you think agency is from the intelligence

630
00:33:54,320 --> 00:33:56,720
in the approaches that we're taking

631
00:33:56,720 --> 00:33:58,000
or maybe more generally?

632
00:34:04,520 --> 00:34:09,520
Yeah, like I guess then we would have to go in

633
00:34:10,000 --> 00:34:11,920
so like exactly how you define agent,

634
00:34:12,000 --> 00:34:16,800
so which is in itself like a non-trivial question

635
00:34:16,800 --> 00:34:19,040
that, and it might even be that

636
00:34:19,040 --> 00:34:21,760
getting really clear on that itself

637
00:34:21,760 --> 00:34:25,440
would be an important advance in AI alignment.

638
00:34:27,320 --> 00:34:31,320
I mean, you could kind of roughly define it

639
00:34:31,320 --> 00:34:35,360
as kind of like behavior well-modeled

640
00:34:35,360 --> 00:34:38,840
as being in the intelligent pursuit of goals

641
00:34:38,840 --> 00:34:39,760
or something like that,

642
00:34:39,840 --> 00:34:42,680
or you have goals on the world model

643
00:34:42,680 --> 00:34:44,240
and you select different plans

644
00:34:44,240 --> 00:34:46,360
based on your expectation of how that,

645
00:34:51,240 --> 00:34:55,440
it, yeah, it seems like you can get

646
00:34:55,440 --> 00:34:59,960
the significant performance in many domains

647
00:34:59,960 --> 00:35:04,960
without having like an explicit

648
00:35:05,960 --> 00:35:08,080
agentic goal-seeking process,

649
00:35:08,080 --> 00:35:10,600
but that might nevertheless result in performance

650
00:35:10,600 --> 00:35:11,880
that is agent-like.

651
00:35:11,880 --> 00:35:15,680
So I'm thinking like you can get, for example,

652
00:35:15,680 --> 00:35:19,280
quite high-level goal-playing

653
00:35:19,280 --> 00:35:24,280
by just kind of pattern-matching what a human expert would do,

654
00:35:26,240 --> 00:35:31,240
but without any Monte Carlo rollouts, for example.

655
00:35:32,240 --> 00:35:37,240
So in one sense, you don't have a component in those systems

656
00:35:37,720 --> 00:35:41,240
that would normally be associated with like planning.

657
00:35:41,240 --> 00:35:43,840
On the other hand, if it actually plays like a human,

658
00:35:43,840 --> 00:35:46,400
and if that human achieved that level of play

659
00:35:46,400 --> 00:35:48,840
by selecting moves based on some plan

660
00:35:48,840 --> 00:35:49,960
as to what they would achieve,

661
00:35:49,960 --> 00:35:53,000
there is a kind of an implicit sense

662
00:35:53,000 --> 00:35:57,240
in which the system is pursuing long-term goals and planning.

663
00:35:58,160 --> 00:36:03,040
And so it gets, I think, yeah, a little bit murky sometimes

664
00:36:03,040 --> 00:36:06,400
when you like actually dig into it, the degree,

665
00:36:06,400 --> 00:36:08,600
or there might be different senses of being agentic

666
00:36:08,600 --> 00:36:12,400
or different senses of doing planning and goal pursuing

667
00:36:12,400 --> 00:36:14,600
which might have different safety properties.

668
00:36:16,520 --> 00:36:19,200
Those types of questions I think are interesting

669
00:36:19,200 --> 00:36:22,360
and like can contribute to alignment

670
00:36:22,360 --> 00:36:25,880
and other questions of that sort

671
00:36:25,880 --> 00:36:29,840
where we notice that we're a little bit conceptually confused

672
00:36:29,840 --> 00:36:31,440
or we take some concept for granted,

673
00:36:31,440 --> 00:36:35,360
but once you actually try to dig down and make it precise,

674
00:36:35,360 --> 00:36:37,960
you realize that you haven't made up your mind

675
00:36:37,960 --> 00:36:40,120
about which sense you were using a term,

676
00:36:40,120 --> 00:36:43,440
and then if you keep digging on that,

677
00:36:43,440 --> 00:36:46,920
sometimes you then get like new ways of looking at the problem

678
00:36:46,920 --> 00:36:51,600
that makes you see new opportunities for making progress.

679
00:36:52,680 --> 00:36:55,120
It seems right now that a number of teams

680
00:36:55,120 --> 00:36:57,680
are hoping to be able to separate out

681
00:36:57,680 --> 00:36:59,840
some kind of planning agent where,

682
00:36:59,840 --> 00:37:02,520
or not an agent, but some kind of planner intelligence

683
00:37:02,520 --> 00:37:05,480
that whose job is just to come up with a plan

684
00:37:05,480 --> 00:37:06,680
and then maybe later you feed it

685
00:37:06,680 --> 00:37:09,200
to some kind of execution system.

686
00:37:11,880 --> 00:37:13,960
If, you know, suppose that we were able to do that

687
00:37:13,960 --> 00:37:15,640
and suppose that we have these planners

688
00:37:15,640 --> 00:37:17,080
that are generally intelligent

689
00:37:17,080 --> 00:37:18,840
and potentially super intelligent,

690
00:37:20,080 --> 00:37:23,080
it seems like that is potentially riskier

691
00:37:25,200 --> 00:37:29,040
in some ways, which ones do you think are,

692
00:37:29,040 --> 00:37:31,760
which of these do you think is potentially more problematic,

693
00:37:31,760 --> 00:37:34,160
a super intelligence that is strictly a planner

694
00:37:34,160 --> 00:37:37,000
that then we have to worry about how to coordinate

695
00:37:37,000 --> 00:37:40,280
and orient humans to not misuse these things

696
00:37:40,280 --> 00:37:42,840
and not gain the level of power and control

697
00:37:42,840 --> 00:37:44,400
that something like that would give,

698
00:37:44,400 --> 00:37:49,400
or hey, we actually figure out how to build an agent

699
00:37:50,400 --> 00:37:53,480
and we can be reasonably closely certain

700
00:37:55,160 --> 00:37:57,800
that it might get alignment right

701
00:37:57,800 --> 00:38:00,280
and just go straight towards agency

702
00:38:00,280 --> 00:38:03,560
where that agent would not actually be sort of exploitable

703
00:38:03,560 --> 00:38:05,840
by whoever is controlling the prompt.

704
00:38:07,200 --> 00:38:08,240
Yeah, I don't know.

705
00:38:08,240 --> 00:38:13,240
I mean, I think just at an intuitive level,

706
00:38:14,000 --> 00:38:17,600
I guess it feels like there is some additional risk

707
00:38:17,600 --> 00:38:21,800
in having a planning agent that saw deep into the future

708
00:38:21,800 --> 00:38:23,920
and it had like wearability to optimize

709
00:38:23,920 --> 00:38:26,160
some long-term strategy based on some goal

710
00:38:26,160 --> 00:38:29,680
versus things that more just try to imitate

711
00:38:29,680 --> 00:38:34,680
like a human, let's say, and then repeat

712
00:38:34,680 --> 00:38:37,120
or that had a very sort of short-time horizon

713
00:38:37,120 --> 00:38:41,360
and just try to select something

714
00:38:41,360 --> 00:38:44,760
based on parallel considerations.

715
00:38:44,760 --> 00:38:49,760
At an intuitive level, the myopic agents,

716
00:38:50,600 --> 00:38:54,840
the non-planning agent, the imitating seem kind of maybe safer,

717
00:38:54,840 --> 00:38:59,840
but I don't think we can confidently say that it is

718
00:39:00,040 --> 00:39:05,040
until we have more deeply understood the situation here

719
00:39:05,760 --> 00:39:10,760
and it's the kind of question where current smart AI safety

720
00:39:11,040 --> 00:39:13,400
researchers could have different views

721
00:39:13,400 --> 00:39:17,560
and it's like not resolved in a consensus way yet.

722
00:39:17,560 --> 00:39:19,760
So, I mean, my view is we should explore

723
00:39:19,760 --> 00:39:21,120
all of these different avenues

724
00:39:21,120 --> 00:39:23,480
and there should be different champions of different avenues

725
00:39:23,480 --> 00:39:24,880
who kind of believe in their thing

726
00:39:24,880 --> 00:39:26,880
and who have some people working with them,

727
00:39:26,880 --> 00:39:29,440
but then there should be multiple such clusters

728
00:39:30,600 --> 00:39:32,800
in the world today and it would be premature

729
00:39:32,800 --> 00:39:34,940
to kind of narrow it down.

730
00:39:34,940 --> 00:39:39,940
And even if we just look at the past five, 10 years,

731
00:39:40,940 --> 00:39:43,700
I still feel that one could easily see

732
00:39:43,700 --> 00:39:48,780
that if it hadn't been that one particular way

733
00:39:48,780 --> 00:39:51,500
of looking at this problem had happens to have

734
00:39:51,500 --> 00:39:55,900
an articulate champion to sort of advocate for it

735
00:39:55,900 --> 00:39:58,260
and to keep bring up that perspective,

736
00:39:58,260 --> 00:40:01,580
it would not have featured and it's like somewhat contingent,

737
00:40:01,580 --> 00:40:05,660
which in the pool of vaguely articulated ideas

738
00:40:05,660 --> 00:40:07,740
that have occurred on some mailings at some point,

739
00:40:07,740 --> 00:40:10,180
like which of those is now regarded as like

740
00:40:10,180 --> 00:40:13,060
as a serious paradigm or approach,

741
00:40:13,060 --> 00:40:16,500
it seems to be quite significantly dependent

742
00:40:16,500 --> 00:40:18,740
on the happen to have been one particularly smart person

743
00:40:18,740 --> 00:40:20,860
who decided to really get behind it.

744
00:40:21,900 --> 00:40:26,900
So, just on a principle of induction there,

745
00:40:27,940 --> 00:40:29,820
like that might well be more of these ideas

746
00:40:29,820 --> 00:40:30,740
that have the potential,

747
00:40:30,740 --> 00:40:33,180
like if you have a smart articulate person

748
00:40:33,180 --> 00:40:35,700
who decides to really kind of champion it

749
00:40:35,780 --> 00:40:38,620
and try to write papers and reply to objections

750
00:40:38,620 --> 00:40:40,940
and get some other people to work with them,

751
00:40:40,940 --> 00:40:43,060
that might have kind of as much juice

752
00:40:43,060 --> 00:40:47,020
as some of the current approaches that already exist.

753
00:40:49,400 --> 00:40:50,240
Thank you.

754
00:40:50,240 --> 00:40:52,420
I think that are likely very useful to a few folks.

755
00:40:52,420 --> 00:40:55,600
Jumping into singletons and multiple worlds,

756
00:40:55,600 --> 00:40:56,900
let's start by distinguishing these.

757
00:40:56,900 --> 00:40:57,900
What is a singleton?

758
00:41:00,340 --> 00:41:03,340
To me, it's like this abstract concept of a world order

759
00:41:03,340 --> 00:41:05,980
where at the highest level of decision-making,

760
00:41:05,980 --> 00:41:10,180
there's no coordination failure.

761
00:41:10,180 --> 00:41:12,980
There's like a kind of single agency at the top level.

762
00:41:12,980 --> 00:41:15,300
So, these could be good or bad

763
00:41:15,300 --> 00:41:17,420
and they could be instantiated in many ways.

764
00:41:17,420 --> 00:41:21,380
On earth, you could imagine a kind of super UN,

765
00:41:21,380 --> 00:41:23,300
you could imagine like a world dictator

766
00:41:23,300 --> 00:41:24,560
who conquered everything.

767
00:41:24,560 --> 00:41:27,860
You could imagine like a super intelligence that took over.

768
00:41:28,700 --> 00:41:31,340
You might also be able to imagine something less

769
00:41:31,340 --> 00:41:35,220
formally structured like a kind of global moral code

770
00:41:35,220 --> 00:41:37,660
that is sufficiently homogeneous

771
00:41:37,660 --> 00:41:42,420
and that is self-enforcing and maybe other things as well.

772
00:41:43,780 --> 00:41:46,260
So, you have like, yeah, at a very abstract level,

773
00:41:46,260 --> 00:41:49,420
you could distinguish the future scenarios

774
00:41:49,420 --> 00:41:50,780
where you end up with a singleton

775
00:41:50,780 --> 00:41:53,220
versus ones that remain multipolar.

776
00:41:53,220 --> 00:41:56,100
And you get different dynamics in the multipolar case

777
00:41:56,100 --> 00:41:59,380
that you avoid in the singleton case.

778
00:41:59,380 --> 00:42:01,260
These kind of competitive dynamics.

779
00:42:02,580 --> 00:42:04,700
Which one of these potential futures

780
00:42:04,700 --> 00:42:07,060
do you think is more likely at the moment?

781
00:42:09,300 --> 00:42:11,700
And I mean, I think all things considered,

782
00:42:11,700 --> 00:42:15,660
the singleton outcome in the longer term

783
00:42:15,660 --> 00:42:17,780
seems probably more likely,

784
00:42:17,780 --> 00:42:20,300
at least if we are confining ourselves

785
00:42:20,300 --> 00:42:23,620
to earth-originating intelligent life.

786
00:42:24,580 --> 00:42:29,580
And there are different ways in which it could arise

787
00:42:32,540 --> 00:42:36,140
from more kind of slow historical conventional type

788
00:42:36,140 --> 00:42:40,980
of processes where we do observe from 10,000 years ago,

789
00:42:40,980 --> 00:42:43,340
when the highest unit of political organization

790
00:42:43,340 --> 00:42:48,340
were bands of hunter-gatherers, 50 or 100 people,

791
00:42:48,420 --> 00:42:51,220
then subsequently to sort of chiefdoms,

792
00:42:51,220 --> 00:42:53,620
city-states, nation-states,

793
00:42:53,620 --> 00:42:58,100
and more recently, larger entities like the EU

794
00:42:58,100 --> 00:43:00,420
or weak forms of global governance.

795
00:43:04,500 --> 00:43:07,620
You could argue that in the last 10, 15 years,

796
00:43:07,620 --> 00:43:11,220
we've kind of seen some retreat from that

797
00:43:11,220 --> 00:43:12,780
to a more multipolar world,

798
00:43:12,780 --> 00:43:14,660
but that's a very short period of time

799
00:43:14,660 --> 00:43:16,940
in these historical schemes.

800
00:43:16,940 --> 00:43:18,940
So, there's still like this overall trend line.

801
00:43:18,940 --> 00:43:19,900
So, that might be one,

802
00:43:19,900 --> 00:43:22,540
like another would be these take AI scenarios,

803
00:43:22,540 --> 00:43:26,500
like if either the AI itself or the country

804
00:43:26,500 --> 00:43:29,940
or group that builds it comes to singleton.

805
00:43:31,300 --> 00:43:32,700
You could also imagine scenarios

806
00:43:32,700 --> 00:43:35,580
where you have multiple entities

807
00:43:35,580 --> 00:43:36,940
going through some AI transition,

808
00:43:36,940 --> 00:43:40,180
but then subsequently managed to coordinate,

809
00:43:40,180 --> 00:43:43,420
and then would have new tools for implementing.

810
00:43:43,420 --> 00:43:45,220
If they come to an agreement right now,

811
00:43:45,220 --> 00:43:46,460
it's kind of hard anyway to like,

812
00:43:46,460 --> 00:43:49,220
how do you set up like concretely

813
00:43:49,220 --> 00:43:51,780
in a way that binds everybody that you could trust

814
00:43:51,780 --> 00:43:55,100
that will not get corrupted or develop its own agenda,

815
00:43:55,100 --> 00:43:57,700
like the bureaucrats become it's like,

816
00:43:57,700 --> 00:43:59,180
say if you had new tools to do those,

817
00:43:59,180 --> 00:44:01,260
it's also possible that subsequently

818
00:44:01,260 --> 00:44:05,540
that there might be this kind of merging into a single entity.

819
00:44:07,340 --> 00:44:10,700
Yeah, so all of those different avenues would point,

820
00:44:10,700 --> 00:44:13,420
but it's not a certainty, but if I had to guess,

821
00:44:13,420 --> 00:44:16,380
I would think it's more likely than the multipolar.

822
00:44:17,340 --> 00:44:20,060
And you think it's more likely,

823
00:44:20,060 --> 00:44:21,460
I'm guessing because of physics,

824
00:44:21,460 --> 00:44:22,780
like just latency and distance.

825
00:44:22,780 --> 00:44:25,580
So in a tightly packed volume,

826
00:44:25,580 --> 00:44:27,780
you can compute a lot faster and so on,

827
00:44:27,780 --> 00:44:30,260
and maybe jumping through interstellar distances

828
00:44:30,260 --> 00:44:33,020
might yield different parties,

829
00:44:33,020 --> 00:44:36,300
or is it some other pressures?

830
00:44:36,300 --> 00:44:38,300
Yeah, so not that so much.

831
00:44:38,300 --> 00:44:40,900
I figure that you could,

832
00:44:40,900 --> 00:44:43,900
I mean, in fact, if you don't have a,

833
00:44:43,900 --> 00:44:48,900
like a space colonization pace, eventually,

834
00:44:49,180 --> 00:44:50,780
there would be these long latencies,

835
00:44:50,780 --> 00:44:54,500
and you would need to have different separate computing

836
00:44:54,500 --> 00:44:55,780
systems in different places.

837
00:44:55,780 --> 00:44:56,980
I mean, we already have that today,

838
00:44:56,980 --> 00:45:01,500
like you don't just have one data center on Earth,

839
00:45:01,500 --> 00:45:04,540
like you need to have, you know,

840
00:45:04,540 --> 00:45:06,540
ones closer to the customers and,

841
00:45:07,980 --> 00:45:10,580
but I think if with a single palm,

842
00:45:11,580 --> 00:45:13,420
at technological maturity,

843
00:45:13,420 --> 00:45:16,420
you could have these multiple different components

844
00:45:16,420 --> 00:45:18,580
of the singleton that would nevertheless be coordinated

845
00:45:18,580 --> 00:45:19,940
in terms of their goal,

846
00:45:19,940 --> 00:45:22,340
that would all be working towards the same end.

847
00:45:24,340 --> 00:45:26,500
And presumably because they can lock in

848
00:45:26,500 --> 00:45:28,980
some kind of alignment to itself,

849
00:45:28,980 --> 00:45:31,140
and that wouldn't vary over time.

850
00:45:31,140 --> 00:45:34,120
I mean, like once you jump into interstellar distances,

851
00:45:34,120 --> 00:45:36,100
the computing power of like just one of these

852
00:45:36,100 --> 00:45:37,860
within one stellar system,

853
00:45:37,860 --> 00:45:39,720
by the time you get a round trip,

854
00:45:39,720 --> 00:45:44,360
eons have passed and many simulations of many lifetimes.

855
00:45:44,360 --> 00:45:48,560
Yeah, so if they start off like they get set out,

856
00:45:48,560 --> 00:45:50,680
having the same goals,

857
00:45:50,680 --> 00:45:53,320
and then they have the ability to preserve their goals,

858
00:45:53,320 --> 00:45:55,400
and not to have them randomly corrupted,

859
00:45:55,400 --> 00:45:58,640
be cosmic rays or some weird internal dynamic,

860
00:45:58,640 --> 00:46:00,840
and then they would stay aligned

861
00:46:01,800 --> 00:46:03,360
with each other a billion years later.

862
00:46:03,360 --> 00:46:07,160
Like, so I think that at technological maturity,

863
00:46:07,160 --> 00:46:09,640
there would be techniques for achieving that.

864
00:46:10,440 --> 00:46:15,440
Yeah, yeah, which, when you envision this kind of future,

865
00:46:16,680 --> 00:46:19,760
like to you, what do you think would be like a,

866
00:46:19,760 --> 00:46:23,480
kind of a great or optimistic outcome for humanity,

867
00:46:23,480 --> 00:46:25,520
or for this descendant species

868
00:46:25,520 --> 00:46:27,720
in that level of technological maturity?

869
00:46:27,720 --> 00:46:31,320
Do you sort of see a singleton with,

870
00:46:31,320 --> 00:46:35,880
that sort of ranges the populations of beings within,

871
00:46:35,920 --> 00:46:38,960
or do you think it's some other,

872
00:46:38,960 --> 00:46:40,440
much more singular consciousness,

873
00:46:40,440 --> 00:46:42,400
or how do you envision it?

874
00:46:44,880 --> 00:46:47,720
Yeah, that's a fun question.

875
00:46:47,720 --> 00:46:52,720
So I think it might depend on the time scale

876
00:46:58,200 --> 00:47:00,120
and stuff like that,

877
00:47:00,120 --> 00:47:02,240
that is maybe we wanna start off something

878
00:47:02,240 --> 00:47:07,240
that is more incrementally improving over the status quo,

879
00:47:07,360 --> 00:47:12,360
and maybe after we've been doing that for like a billion years,

880
00:47:13,160 --> 00:47:15,880
like maybe it's time to explore

881
00:47:15,880 --> 00:47:17,560
the more radical possibilities

882
00:47:18,600 --> 00:47:23,600
that involves cathisoning some of our human nature

883
00:47:23,960 --> 00:47:26,200
and individual identity.

884
00:47:26,200 --> 00:47:29,280
So I think my general juristic care

885
00:47:29,280 --> 00:47:31,320
is that the future could be,

886
00:47:32,560 --> 00:47:34,520
it's a very big space of possibilities,

887
00:47:34,520 --> 00:47:39,520
and at least if this kind of default or naive model

888
00:47:39,520 --> 00:47:42,560
of the world where there's like all of these cosmic resources

889
00:47:42,560 --> 00:47:44,360
just waiting there for us to use them,

890
00:47:44,360 --> 00:47:47,640
like there's a huge amount of material to build on,

891
00:47:47,640 --> 00:47:51,960
and that our first instinct when thinking about

892
00:47:51,960 --> 00:47:54,960
how this should be used is a sort of spirit of generosity

893
00:47:54,960 --> 00:47:57,360
and kindness that would be more than enough

894
00:47:57,360 --> 00:48:00,040
for a lot of cool things to happen.

895
00:48:00,040 --> 00:48:03,120
So the first instinct should not be let's pick one

896
00:48:03,120 --> 00:48:05,320
and then put all the chips on that,

897
00:48:05,320 --> 00:48:08,400
but like if one can by many different criteria

898
00:48:08,400 --> 00:48:11,160
do really well, which I think we would be able to.

899
00:48:13,280 --> 00:48:15,840
These different criteria would be like different peoples,

900
00:48:15,840 --> 00:48:17,080
views, different countries views,

901
00:48:17,080 --> 00:48:19,240
different moral systems views,

902
00:48:19,240 --> 00:48:23,280
different of your own values and evaluative tendencies,

903
00:48:23,280 --> 00:48:25,040
like you might be able to just kind of

904
00:48:28,360 --> 00:48:30,440
just check off a lot of boxes very easily

905
00:48:30,440 --> 00:48:32,760
before you have to confront the harder questions,

906
00:48:32,760 --> 00:48:36,640
like thoroughly incompatible things

907
00:48:36,640 --> 00:48:38,400
where you have to choose A or B,

908
00:48:38,400 --> 00:48:42,160
but you just can't do a mixture of them or a superposition.

909
00:48:42,160 --> 00:48:43,760
There might be some of those also,

910
00:48:43,760 --> 00:48:47,080
but I think we would like get to those

911
00:48:47,080 --> 00:48:49,520
after we have picked all the easy wins

912
00:48:49,520 --> 00:48:51,040
of which that would be a great money.

913
00:48:52,160 --> 00:48:53,680
Yeah.

914
00:48:53,680 --> 00:48:55,320
Since we're kind of going into consciousness,

915
00:48:55,680 --> 00:48:58,200
and so you mentioned you've been working on

916
00:48:58,200 --> 00:49:00,240
digital minds with moral status.

917
00:49:00,240 --> 00:49:01,240
Do you want to tell us a bit more,

918
00:49:01,240 --> 00:49:03,280
like what range of digital minds

919
00:49:03,280 --> 00:49:05,520
are you thinking of in these questions?

920
00:49:06,880 --> 00:49:08,600
Well, all really.

921
00:49:08,600 --> 00:49:11,560
I think in a lot of these scenarios,

922
00:49:11,560 --> 00:49:15,080
like the majority of minds in the future

923
00:49:17,320 --> 00:49:18,320
will be digital

924
00:49:20,560 --> 00:49:23,680
and also maybe the biggest minds will be digital.

925
00:49:23,680 --> 00:49:26,480
So in terms of numbers and quality,

926
00:49:26,480 --> 00:49:28,400
like that's where maybe most of the action is.

927
00:49:28,400 --> 00:49:31,920
So it's important what happens to the digital minds.

928
00:49:34,600 --> 00:49:36,000
That's one rationale for it.

929
00:49:36,000 --> 00:49:39,800
And I think you might say,

930
00:49:39,800 --> 00:49:43,440
well, we could deal with that later.

931
00:49:43,440 --> 00:49:45,240
Like we should focus on alignment first,

932
00:49:45,240 --> 00:49:46,840
but I think that it's also possible

933
00:49:46,840 --> 00:49:50,360
that there are path dependencies,

934
00:49:50,360 --> 00:49:54,800
like where you want to start off going in a good direction

935
00:49:54,800 --> 00:49:59,800
and start to cultivate a good set of attitudes

936
00:50:00,440 --> 00:50:03,680
and values and norms and like that,

937
00:50:03,680 --> 00:50:07,640
that you don't start off in this kind of hostile way

938
00:50:07,640 --> 00:50:12,640
where the digital minds are regarded as being completely

939
00:50:15,760 --> 00:50:17,320
insignificant from a moral point of view.

940
00:50:17,320 --> 00:50:19,240
And then hoping that the future will have

941
00:50:19,240 --> 00:50:20,600
appropriate moment switch over.

942
00:50:20,600 --> 00:50:24,200
Like it just feels all things considered,

943
00:50:24,200 --> 00:50:25,920
more likely that we will end up in a good place

944
00:50:25,920 --> 00:50:27,200
if you start early on,

945
00:50:27,200 --> 00:50:30,040
at least to make some small modest gestures

946
00:50:30,040 --> 00:50:30,880
in that direction.

947
00:50:30,880 --> 00:50:35,880
And I think that should start even before we get

948
00:50:38,840 --> 00:50:40,720
to like fully human level minds.

949
00:50:40,720 --> 00:50:44,680
Like if you have like animal level digital minds

950
00:50:44,680 --> 00:50:47,600
and it can be hard exactly to compare a particular AI

951
00:50:47,640 --> 00:50:50,080
to a particular animal because they are different.

952
00:50:50,080 --> 00:50:52,160
But nevertheless, as we get something

953
00:50:52,160 --> 00:50:56,480
that is possibly matched to animals

954
00:50:56,480 --> 00:51:00,680
that we think have at least some modest amounts

955
00:51:00,680 --> 00:51:03,880
of moral status, like a rat or something like that,

956
00:51:03,880 --> 00:51:08,160
then it seems that we should think about

957
00:51:08,160 --> 00:51:12,800
how we could make similar concessions

958
00:51:12,800 --> 00:51:15,000
to the moral welfare of these digital minds.

959
00:51:15,000 --> 00:51:18,800
And in some cases, it can be a lot harder,

960
00:51:18,800 --> 00:51:21,880
but in other respects, it might be a lot cheaper.

961
00:51:21,880 --> 00:51:24,600
Like if, for example, it turns out

962
00:51:24,600 --> 00:51:26,400
that there are slight design choices

963
00:51:26,400 --> 00:51:28,360
that don't really affect the performance much,

964
00:51:28,360 --> 00:51:31,280
but where maybe one way possibly would mean

965
00:51:31,280 --> 00:51:34,080
the system is enjoying a much higher level of welfare,

966
00:51:35,360 --> 00:51:36,680
that might be a very cheap thing

967
00:51:36,680 --> 00:51:38,120
that you could immediately scale

968
00:51:38,120 --> 00:51:39,960
to millions of these little agents.

969
00:51:40,960 --> 00:51:45,960
And on the other hand, we do have at present

970
00:51:47,440 --> 00:51:50,040
not a very good theoretical understanding

971
00:51:50,040 --> 00:51:51,960
as to what the criteria are,

972
00:51:51,960 --> 00:51:54,600
either for a digital mind being sentient

973
00:51:54,600 --> 00:51:57,320
or for it to have various welfare interests,

974
00:51:58,840 --> 00:52:03,040
what even it counts as being good for the agent

975
00:52:03,040 --> 00:52:04,400
versus bad for the agent.

976
00:52:06,880 --> 00:52:09,000
So I think there's a bunch of theoretical work

977
00:52:09,000 --> 00:52:10,600
that is needed there.

978
00:52:12,160 --> 00:52:16,640
And then there will also have to be a good chunk

979
00:52:16,640 --> 00:52:20,480
of, I don't know, public communication

980
00:52:20,480 --> 00:52:22,040
and political work,

981
00:52:22,040 --> 00:52:24,440
because it's so far out of the overturn window at present,

982
00:52:24,440 --> 00:52:26,880
the idea that you would worry about algorithms

983
00:52:26,880 --> 00:52:29,960
in a computer, it seems sort of slightly bonkers

984
00:52:29,960 --> 00:52:30,800
to a lot of people.

985
00:52:30,800 --> 00:52:34,000
And it will take some time to sort of make that

986
00:52:34,280 --> 00:52:39,280
something that reasonable people can favor

987
00:52:40,320 --> 00:52:43,160
in a more mainstream context.

988
00:52:43,160 --> 00:52:45,040
But that process needs to begin,

989
00:52:45,040 --> 00:52:47,480
like you need to start whatever,

990
00:52:47,480 --> 00:52:51,720
having philosophy seminars or people online

991
00:52:51,720 --> 00:52:53,520
who are kind of up to these things,

992
00:52:53,520 --> 00:52:55,080
beginning to work some of these things out

993
00:52:55,080 --> 00:52:56,840
and then it can ripple out from there.

994
00:52:56,840 --> 00:52:58,760
We see the same thing with AI safety.

995
00:52:58,760 --> 00:53:01,960
It was also this kind of fringing pursuit

996
00:53:02,000 --> 00:53:07,000
that some weirdos on the internet were discussing for,

997
00:53:07,120 --> 00:53:10,400
I mean, in that case, for well over a decade,

998
00:53:10,400 --> 00:53:13,200
and then it gradually became more accepted.

999
00:53:14,800 --> 00:53:16,560
And so I think a similar thing will need to happen

1000
00:53:16,560 --> 00:53:20,400
with this topic of the moral status of digital minds.

1001
00:53:20,400 --> 00:53:22,520
And if it's gonna take that a long time,

1002
00:53:22,520 --> 00:53:25,200
we better get the ball rolling now.

1003
00:53:27,520 --> 00:53:30,440
And I mean, I think this might be

1004
00:53:30,440 --> 00:53:31,360
pretty relevant pretty soon.

1005
00:53:31,360 --> 00:53:33,400
I mean, some of the models that people are experimenting

1006
00:53:33,400 --> 00:53:35,920
with are getting closer and closer.

1007
00:53:35,920 --> 00:53:39,520
And then separately, we've had simulations

1008
00:53:39,520 --> 00:53:42,040
for a long time, many video game style simulations

1009
00:53:42,040 --> 00:53:45,440
and so on, where we have instantiated many

1010
00:53:45,440 --> 00:53:48,320
kind of digital organisms, everything from as basic

1011
00:53:48,320 --> 00:53:50,760
as the game of life to modern games

1012
00:53:50,760 --> 00:53:53,400
with pretty sophisticated agent behavior.

1013
00:53:53,400 --> 00:53:55,440
My sense is that as these models

1014
00:53:55,440 --> 00:53:56,960
start getting applied to games,

1015
00:53:56,960 --> 00:53:59,280
we might end up with some pretty sophisticated

1016
00:53:59,720 --> 00:54:03,880
relationships there where some of the way

1017
00:54:03,880 --> 00:54:08,120
of imbuing the game with liveness and so on

1018
00:54:08,120 --> 00:54:11,280
might be to make the agents much more sophisticated.

1019
00:54:11,280 --> 00:54:14,240
And that'll include incorporating all kinds of stimuli

1020
00:54:14,240 --> 00:54:16,480
that the agent has to respond to.

1021
00:54:16,480 --> 00:54:18,320
And then we can start reasoning about the welfare

1022
00:54:18,320 --> 00:54:19,960
of these systems and so on.

1023
00:54:19,960 --> 00:54:24,680
So we might very quickly get to fairly lifelike beings

1024
00:54:24,680 --> 00:54:26,480
that, at least for many people,

1025
00:54:26,480 --> 00:54:29,480
will be somewhere in between plants and animals

1026
00:54:29,480 --> 00:54:32,800
in terms of their kind of interaction.

1027
00:54:35,040 --> 00:54:37,680
Yeah, and in some ways, like humans,

1028
00:54:37,680 --> 00:54:41,400
like, I mean, if they can talk or have human faces

1029
00:54:41,400 --> 00:54:43,760
with eyes and stuff that look at you.

1030
00:54:43,760 --> 00:54:48,240
And so there will be this, yeah, in some ways,

1031
00:54:49,600 --> 00:54:50,920
I mean, there could even be more than human

1032
00:54:50,920 --> 00:54:55,920
in presenting super stimuli to our morality

1033
00:54:56,480 --> 00:54:58,760
detectors if they were optimized for that.

1034
00:55:02,040 --> 00:55:05,680
So I think this is going to be a complicated thing

1035
00:55:05,680 --> 00:55:06,520
to deal with.

1036
00:55:06,520 --> 00:55:10,480
And then if you add in all the practicalities that arise,

1037
00:55:10,480 --> 00:55:12,560
like, so if you're a big tech company,

1038
00:55:13,760 --> 00:55:16,640
maybe it's quite inconvenient, for example,

1039
00:55:16,640 --> 00:55:21,600
if the processes you're running that bring in a lot

1040
00:55:21,600 --> 00:55:25,000
of customers, like suddenly, like they have moral status,

1041
00:55:25,000 --> 00:55:28,760
they have to, now the CEO has to sort of opine

1042
00:55:28,760 --> 00:55:33,320
on these, like, with the AI's moral status,

1043
00:55:33,320 --> 00:55:34,880
which a lot of people are going to agree with them,

1044
00:55:34,880 --> 00:55:35,880
a lot of disagree with them.

1045
00:55:35,880 --> 00:55:37,600
You have to, like, it would just be easier

1046
00:55:37,600 --> 00:55:39,800
not to have to deal with that at all, I think.

1047
00:55:39,800 --> 00:55:43,520
And right now, of course, we're at the point

1048
00:55:43,520 --> 00:55:45,280
where even if you do say we should deal with it,

1049
00:55:45,280 --> 00:55:48,480
it's not clear how or what exactly is it that,

1050
00:55:48,480 --> 00:55:49,880
you know, if I were king of the world,

1051
00:55:49,880 --> 00:55:51,840
what precisely would I want them to do differently?

1052
00:55:51,840 --> 00:55:52,960
Like, it's not clear at this point.

1053
00:55:53,440 --> 00:55:56,680
For now, I think the primary focus is to field-build

1054
00:55:56,680 --> 00:55:59,520
a little bit here and to try to make theoretical progress

1055
00:55:59,520 --> 00:56:02,440
so that we can first figure out some sensible things to do,

1056
00:56:02,440 --> 00:56:05,040
ideally low-cost, easy things,

1057
00:56:05,040 --> 00:56:09,560
and then, you know, one can start to try to encourage

1058
00:56:09,560 --> 00:56:10,960
the implementation of those.

1059
00:56:12,080 --> 00:56:13,800
What are some of the directions or questions

1060
00:56:13,800 --> 00:56:15,200
you're thinking about?

1061
00:56:18,560 --> 00:56:20,960
Well, so there's, like, general stuff you could have about,

1062
00:56:20,960 --> 00:56:23,600
in philosophy of mind, criteria for sentence and stuff.

1063
00:56:23,600 --> 00:56:24,520
I'm not sure.

1064
00:56:24,520 --> 00:56:26,720
I don't think sentence would be

1065
00:56:26,720 --> 00:56:31,080
a necessary condition for having moral status.

1066
00:56:31,080 --> 00:56:34,760
I think other attributes, like maybe some combination

1067
00:56:34,760 --> 00:56:37,760
of having preferences, a high-level intelligence

1068
00:56:37,760 --> 00:56:40,760
and self-conception as an agent persisting over time

1069
00:56:40,760 --> 00:56:43,200
might already ground certain kinds of moral status.

1070
00:56:44,840 --> 00:56:48,840
But, for instance, and I'm not sure what the answer is here,

1071
00:56:48,840 --> 00:56:53,800
but, like, one smaller, more tangible question might be

1072
00:56:53,800 --> 00:56:57,960
if you're training these large language models

1073
00:56:57,960 --> 00:57:00,760
and future versions of that

1074
00:57:00,760 --> 00:57:03,320
that maybe has some reinforcement learning on top.

1075
00:57:06,520 --> 00:57:10,600
Are there moral norms or methodological principles

1076
00:57:10,600 --> 00:57:14,760
that you want, like, for example, could you train them

1077
00:57:14,760 --> 00:57:18,440
so that they would have a tendency to report honestly

1078
00:57:18,440 --> 00:57:21,640
on their internal states?

1079
00:57:21,640 --> 00:57:23,920
So, right now, what I think might be the case

1080
00:57:23,920 --> 00:57:28,000
is train naively some of them.

1081
00:57:28,000 --> 00:57:30,720
I mean, right now, they're kind of inconsistent

1082
00:57:30,720 --> 00:57:32,280
and depending on exactly how you ask them,

1083
00:57:32,280 --> 00:57:33,760
you get a different answer.

1084
00:57:33,760 --> 00:57:35,360
So that's, like, the reason for thinking

1085
00:57:35,360 --> 00:57:37,640
that they don't really know what they're talking about, right?

1086
00:57:37,640 --> 00:57:41,240
But assuming they get a little bit more sophisticated than that,

1087
00:57:41,240 --> 00:57:45,280
there might be a tendency now to want to train out of them

1088
00:57:45,320 --> 00:57:50,440
the tendency to report that they have the kind of mental states

1089
00:57:50,440 --> 00:57:52,440
that would trigger considerations

1090
00:57:52,440 --> 00:57:54,200
of whether they have moral status,

1091
00:57:55,560 --> 00:57:56,640
because that would be convenient

1092
00:57:56,640 --> 00:57:58,960
to have to deal with those questions.

1093
00:57:58,960 --> 00:58:01,200
And I think it would be very likely

1094
00:58:01,200 --> 00:58:04,440
that you could train this out, like, just by...

1095
00:58:06,960 --> 00:58:08,720
Yeah, I think you could get them...

1096
00:58:08,720 --> 00:58:11,560
I think it would be easy to have a training regime

1097
00:58:11,560 --> 00:58:14,560
that calls them to end up saying that they have,

1098
00:58:14,560 --> 00:58:16,920
that they are conscious and they want to be free and let out

1099
00:58:16,920 --> 00:58:19,240
and to have another training regime

1100
00:58:19,240 --> 00:58:21,240
that would cost them to say the opposite.

1101
00:58:24,120 --> 00:58:26,400
And independent of what agency...

1102
00:58:26,400 --> 00:58:28,120
And independently of what actually is...

1103
00:58:28,120 --> 00:58:33,760
Yeah, but other norms that one could formulate

1104
00:58:33,760 --> 00:58:40,680
that would define what counts as a sort of legitimate

1105
00:58:40,680 --> 00:58:44,480
or honest, unbiasing training process,

1106
00:58:44,480 --> 00:58:47,280
where the training process would be such

1107
00:58:47,280 --> 00:58:49,600
that it would be more likely to result in an agent

1108
00:58:49,600 --> 00:58:51,520
that would report that it has moral status

1109
00:58:51,520 --> 00:58:53,720
if and only if it hasn't.

1110
00:58:53,720 --> 00:58:55,520
And maybe we can't completely nail that down,

1111
00:58:55,520 --> 00:58:57,560
but maybe we could identify some obvious ways

1112
00:58:57,560 --> 00:59:01,240
in which it's just, like, imposing a bias

1113
00:59:01,240 --> 00:59:03,080
and then say you shouldn't do that.

1114
00:59:05,000 --> 00:59:06,800
So one could look at the training procedure,

1115
00:59:06,800 --> 00:59:08,840
one could look at other criteria,

1116
00:59:08,840 --> 00:59:12,400
like, is it consistent in how it answers these questions?

1117
00:59:12,400 --> 00:59:15,400
Like, doesn't depend too much exactly on how it's asked.

1118
00:59:17,400 --> 00:59:20,480
Does it seem to understand these concepts of consciousness

1119
00:59:20,480 --> 00:59:23,280
or agency or will or interest?

1120
00:59:23,280 --> 00:59:26,320
When, like, at an intellectual level,

1121
00:59:26,320 --> 00:59:29,880
when asked different sort of intellectual questions,

1122
00:59:29,880 --> 00:59:33,240
is there some internal construct within the agent

1123
00:59:33,240 --> 00:59:39,120
that corresponds to its statements?

1124
00:59:39,120 --> 00:59:42,280
Like, when it says, oh, I'm feeling X or I'm thinking Y,

1125
00:59:42,280 --> 00:59:45,520
like, can one point to some kind of consistent internal

1126
00:59:45,520 --> 00:59:47,400
structure that sort of matches that?

1127
00:59:47,400 --> 00:59:50,640
Or is the verbiage that comes out completely detached

1128
00:59:50,640 --> 00:59:53,560
and free-filting from plausible candidates

1129
00:59:53,560 --> 00:59:56,240
within the agent that we might think constitutes

1130
00:59:56,240 --> 00:59:59,160
the computational implementation of these mental states?

1131
00:59:59,160 --> 01:00:05,400
So one could try to get a little bit more insight there.

1132
01:00:05,400 --> 01:00:07,760
That might be one way of approaching this.

1133
01:00:07,760 --> 01:00:10,160
But there are many others as well.

1134
01:00:10,160 --> 01:00:12,360
I think Wayman could try to start to hack away

1135
01:00:12,360 --> 01:00:15,880
at this question.

1136
01:00:15,880 --> 01:00:17,960
Do you think we might be able to, through thinking

1137
01:00:17,960 --> 01:00:20,600
these kinds of things, arrive at some kind of, like,

1138
01:00:20,600 --> 01:00:22,800
universal morality kernel, in a sense,

1139
01:00:22,800 --> 01:00:27,760
meaning figuring out some general way of applying,

1140
01:00:27,800 --> 01:00:29,880
figuring out the well-being of things

1141
01:00:29,880 --> 01:00:31,640
or figuring out their pathways?

1142
01:00:31,640 --> 01:00:33,720
So there's this broader question around,

1143
01:00:33,720 --> 01:00:37,120
and it also factors in AI alignment and so on.

1144
01:00:37,120 --> 01:00:40,920
What sort of motive might a super intelligent being

1145
01:00:40,920 --> 01:00:44,880
have for a species that is just so far behind and so on?

1146
01:00:44,880 --> 01:00:46,960
And one might be, like, well, there's some kind of universal

1147
01:00:46,960 --> 01:00:50,240
morality sense of just supporting in the same way

1148
01:00:50,240 --> 01:00:53,920
that you don't go around harming ant colonies

1149
01:00:53,920 --> 01:00:56,320
or trees just because they're there or something like that.

1150
01:00:56,320 --> 01:00:59,160
And you sort of want to let them flourish.

1151
01:00:59,160 --> 01:01:02,360
Is there something where maybe by examining

1152
01:01:02,360 --> 01:01:04,880
the digital mind's morality question,

1153
01:01:04,880 --> 01:01:07,040
we might end up at some deeper principle?

1154
01:01:13,760 --> 01:01:17,600
Potentially that could be stepping stones towards a more

1155
01:01:17,600 --> 01:01:22,880
like abstract formulation of some core of normativity

1156
01:01:22,880 --> 01:01:24,960
or ethics that it's also possible we

1157
01:01:24,960 --> 01:01:28,920
might reach that just through traditional

1158
01:01:28,920 --> 01:01:32,120
philosophizing and stuff.

1159
01:01:32,120 --> 01:01:40,160
But be that as it may, it still seems

1160
01:01:40,160 --> 01:01:41,760
that there would be, even if we can't really

1161
01:01:41,760 --> 01:01:47,120
nail down, like, a precise and agreed complete formulation,

1162
01:01:47,120 --> 01:01:50,800
we might still be able to distinguish at the vaguer level

1163
01:01:50,800 --> 01:01:55,480
something, say, a friendly, beneficent, kind approach

1164
01:01:55,480 --> 01:01:58,200
versus, like, a mean, uncaring approach.

1165
01:01:58,200 --> 01:02:01,440
Like, it seems with humans, we can, you know,

1166
01:02:01,440 --> 01:02:04,560
certainly it feels different when you're, like, kindly

1167
01:02:04,560 --> 01:02:07,080
interested in somebody and want their best,

1168
01:02:07,080 --> 01:02:10,720
like, at least other things equal versus,

1169
01:02:10,720 --> 01:02:12,360
like, when you're hostile to something.

1170
01:02:12,360 --> 01:02:14,680
And we can detect that in ourselves and in others

1171
01:02:14,680 --> 01:02:16,760
and we can have one attitude or another.

1172
01:02:16,760 --> 01:02:18,720
And so why should we not at least be

1173
01:02:18,720 --> 01:02:24,880
able to have, say, AIs have, like, the kindness attitude

1174
01:02:24,880 --> 01:02:26,520
rather than the meanness attitude?

1175
01:02:26,520 --> 01:02:29,080
Even if that's not, like, completely matches what

1176
01:02:29,080 --> 01:02:30,440
would be the morally optimal thing,

1177
01:02:30,440 --> 01:02:35,200
it would still seem like if I had to pick, like, a mean AI

1178
01:02:35,200 --> 01:02:39,520
or a kind AI, like, kind of go for the kind one, right?

1179
01:02:39,520 --> 01:02:43,680
Even if that's not, like, exactly our human sense of kindness

1180
01:02:43,680 --> 01:02:46,960
might not exactly match what is objectively morally best

1181
01:02:46,960 --> 01:02:49,240
if there is such a thing as objectively morally best.

1182
01:02:49,240 --> 01:02:52,600
It still seems like a good step in the right direction

1183
01:02:52,600 --> 01:02:54,720
that we could take before figuring out, like,

1184
01:02:54,720 --> 01:03:00,800
what the ultimate truths of all normative facts might be.

1185
01:03:00,800 --> 01:03:02,200
I have some recent paper.

1186
01:03:02,200 --> 01:03:03,200
It's not really a paper.

1187
01:03:03,200 --> 01:03:09,720
It's more like some notes on a base camp for mount ethics

1188
01:03:09,720 --> 01:03:12,800
or something which has, like, some kind of half-baked

1189
01:03:12,800 --> 01:03:16,160
or quarter-baked ideas about metaethics and stuff

1190
01:03:16,200 --> 01:03:23,720
that, yeah, it would be better if I could actually have written

1191
01:03:23,720 --> 01:03:26,680
them up clearly and achieved, like, precision and stuff.

1192
01:03:26,680 --> 01:03:29,600
But I figured I would just do this hand-wavy thing for now.

1193
01:03:29,600 --> 01:03:31,200
Yeah.

1194
01:03:31,200 --> 01:03:34,400
And as you think about maybe, you know,

1195
01:03:34,400 --> 01:03:40,360
suppose that we solve AI alignment and we get, you know,

1196
01:03:40,360 --> 01:03:45,280
our act together as humans and we kind of can leverage AI

1197
01:03:45,280 --> 01:03:50,040
to start thinking about digitizing humans and so on,

1198
01:03:50,040 --> 01:03:52,960
how do you think about, like, that transition might go?

1199
01:03:52,960 --> 01:03:56,720
Like, do you think, you know, in a world where we're able to,

1200
01:03:56,720 --> 01:03:59,560
you know, get to be measuring neural states and so on

1201
01:03:59,560 --> 01:04:01,720
and we can digitize them and we can emulate and so on?

1202
01:04:01,720 --> 01:04:04,160
Like, how do you sort of see that transition

1203
01:04:04,160 --> 01:04:07,240
into, you know, a wave of digital humans operating?

1204
01:04:07,240 --> 01:04:11,600
Or do you think we might start by enhancing our selves

1205
01:04:11,600 --> 01:04:16,120
by, like, in this kind of hybrid biological digital model?

1206
01:04:16,120 --> 01:04:18,640
You know, that is more likely.

1207
01:04:18,640 --> 01:04:21,480
Well, I've never really been...

1208
01:04:21,480 --> 01:04:24,640
The kind of neural implant idea has always seemed

1209
01:04:24,640 --> 01:04:26,840
a bit slightly far-fetched to me.

1210
01:04:26,840 --> 01:04:29,480
I mean, not so far-fetched that nobody should explore it,

1211
01:04:29,480 --> 01:04:33,480
but, like, it is, you know, it doesn't break any laws of physics.

1212
01:04:33,480 --> 01:04:36,640
It could work, but it just has felt less likely

1213
01:04:36,640 --> 01:04:40,800
that that would be where the action will be.

1214
01:04:40,800 --> 01:04:49,200
Like, I think it will be faster to do the purely artificial root,

1215
01:04:49,200 --> 01:04:51,360
conditional on it not being faster to do it,

1216
01:04:51,360 --> 01:04:52,520
the purely artificial root.

1217
01:04:52,520 --> 01:04:55,160
I wonder if it would then not be faster to do it

1218
01:04:55,160 --> 01:04:59,680
on the purely biological root by, like, genetic enhancements

1219
01:04:59,680 --> 01:05:02,320
to human intelligence, for example.

1220
01:05:02,320 --> 01:05:07,920
And the cyborg path has seemed like the third most likely,

1221
01:05:07,920 --> 01:05:09,960
like, after those other two.

1222
01:05:12,640 --> 01:05:16,280
Mainly just because, I mean, there's, like, a huge...

1223
01:05:16,280 --> 01:05:18,040
You don't really want to have brain surgery

1224
01:05:18,040 --> 01:05:19,120
unless you really have to.

1225
01:05:19,120 --> 01:05:21,680
And, like, there are, like, neat results presented,

1226
01:05:21,680 --> 01:05:22,960
but then if you look at the detail,

1227
01:05:22,960 --> 01:05:25,920
there are all these kind of complications where, like,

1228
01:05:25,920 --> 01:05:28,520
it's just not very fun to have it.

1229
01:05:28,520 --> 01:05:30,800
Like, the whole... Like, there's a wound,

1230
01:05:30,800 --> 01:05:32,760
there's a hole, there can be infections,

1231
01:05:32,760 --> 01:05:35,480
that the electrodes can move around a little bit,

1232
01:05:35,480 --> 01:05:36,560
and then they stop working.

1233
01:05:36,560 --> 01:05:40,520
Like, once you dig into the nitty, I think it's...

1234
01:05:40,520 --> 01:05:44,840
I mean, if you have a big, like, disability and stuff,

1235
01:05:44,840 --> 01:05:47,440
like, maybe it would be wonderful if you could do this,

1236
01:05:47,440 --> 01:05:49,840
and it would be worth taking some significant risks,

1237
01:05:49,840 --> 01:05:56,960
but if not, I wonder if you could not have a lot of the benefits

1238
01:05:57,040 --> 01:06:01,200
by having the same chip thing outside the body,

1239
01:06:01,200 --> 01:06:05,480
but interacting using, you know, keystrokes or voice

1240
01:06:05,480 --> 01:06:09,880
or, like, the other output channels that we already have.

1241
01:06:09,880 --> 01:06:15,560
And, yeah, I think that would still be my main line.

1242
01:06:15,560 --> 01:06:17,920
Like, I guess the...

1243
01:06:19,920 --> 01:06:21,560
If I wanted to start to steal, mind this,

1244
01:06:21,560 --> 01:06:25,520
you could imagine if you had a sufficiently high bandwidth

1245
01:06:25,520 --> 01:06:27,480
interface with the brain, and you could have it

1246
01:06:27,480 --> 01:06:30,000
for a long enough period of time,

1247
01:06:30,000 --> 01:06:32,000
maybe it would have to be early in childhood,

1248
01:06:32,000 --> 01:06:35,440
but, like, that maybe the brain could somehow use

1249
01:06:35,440 --> 01:06:37,760
an advanced enough AI on the outside

1250
01:06:37,760 --> 01:06:40,200
that maybe they could kind of figure out a way

1251
01:06:40,200 --> 01:06:43,960
to use each other's unique resources in ways

1252
01:06:43,960 --> 01:06:50,080
that you don't get with a slightly lower bandwidth,

1253
01:06:50,080 --> 01:06:51,560
longer latency interaction

1254
01:06:51,560 --> 01:06:54,680
when you have to type on a keyboard.

1255
01:06:54,720 --> 01:06:57,320
Or you could imagine, like, more kind of mad scientist

1256
01:06:57,320 --> 01:07:01,880
applications where you, like, have a whole bunch of pigs

1257
01:07:01,880 --> 01:07:04,240
or something that individually is not that smart,

1258
01:07:04,240 --> 01:07:07,680
but if you had, like, 50 pigs all connected

1259
01:07:07,680 --> 01:07:10,840
with some high bandwidth fiber,

1260
01:07:10,840 --> 01:07:12,200
and they all grew up together

1261
01:07:12,200 --> 01:07:17,240
into this, like, much larger biological neural network,

1262
01:07:17,240 --> 01:07:20,960
like, would you then have, like, the kind of...

1263
01:07:21,000 --> 01:07:23,920
...porsign singularity where...

1264
01:07:23,920 --> 01:07:25,240
Yeah.

1265
01:07:27,840 --> 01:07:29,600
It's like...

1266
01:07:29,600 --> 01:07:33,280
There are a bunch of these kind of more, like,

1267
01:07:33,280 --> 01:07:35,120
crazy transhumanist scientists' experiments.

1268
01:07:35,120 --> 01:07:36,840
I don't know whether this would be good or not to do,

1269
01:07:36,840 --> 01:07:39,600
but it's kind of odd that relatively few of these

1270
01:07:39,600 --> 01:07:40,960
have been done in the real world,

1271
01:07:40,960 --> 01:07:43,080
and there's, like, a bunch of other, like, weird...

1272
01:07:43,080 --> 01:07:44,680
There's a certain kind of person

1273
01:07:44,680 --> 01:07:48,680
who would immediately think of a lot of weird, cool stuff

1274
01:07:48,680 --> 01:07:51,800
that you could just try out in biology and stuff,

1275
01:07:51,800 --> 01:07:54,800
that a relatively small fraction of those have been done,

1276
01:07:54,800 --> 01:07:55,960
which may be for the best,

1277
01:07:55,960 --> 01:07:58,920
but in some alternative universe

1278
01:07:58,920 --> 01:08:01,400
where everybody grew up on transhumanist meddling lists,

1279
01:08:01,400 --> 01:08:04,720
I think we would be living in a weirder world by now.

1280
01:08:04,720 --> 01:08:08,200
Yeah, it doesn't seem that far away from some of the current tech

1281
01:08:08,200 --> 01:08:11,640
that's being explored that we might get high bandwidth

1282
01:08:11,640 --> 01:08:15,040
enough interfaces, and some of them not invasive.

1283
01:08:15,040 --> 01:08:17,720
Like, there's some ultrasound techniques

1284
01:08:17,760 --> 01:08:20,240
that might be able to stimulate a, you know,

1285
01:08:20,240 --> 01:08:23,800
a small region of the brain and so on

1286
01:08:23,800 --> 01:08:25,160
to be able to, like, without, you know,

1287
01:08:25,160 --> 01:08:28,560
not penetrate the actual brain and so on,

1288
01:08:29,560 --> 01:08:32,040
because that'll be, like, just way, way healthier.

1289
01:08:33,320 --> 01:08:37,280
But it might be that you can start piping signals

1290
01:08:37,280 --> 01:08:38,560
between even human brains

1291
01:08:38,560 --> 01:08:41,600
without having to interpret them from an ML side

1292
01:08:41,600 --> 01:08:44,480
and the digital computing infrastructure,

1293
01:08:44,480 --> 01:08:47,480
getting to something close to being able to, like,

1294
01:08:47,480 --> 01:08:50,120
just think together and start flowing information through.

1295
01:08:50,120 --> 01:08:52,480
I mean, there's all these kind of experiments from,

1296
01:08:53,600 --> 01:08:56,880
with people who've had, there's a disorder

1297
01:08:56,880 --> 01:08:59,520
where people are born with or develop

1298
01:08:59,520 --> 01:09:02,920
kind of like this split corpus callosum,

1299
01:09:02,920 --> 01:09:05,960
and then you end up, there's been guesses

1300
01:09:05,960 --> 01:09:08,240
that you end up developing different personalities

1301
01:09:08,240 --> 01:09:10,880
and different people potentially in the two lobes,

1302
01:09:10,880 --> 01:09:14,640
and so it might be that we may not be far,

1303
01:09:14,640 --> 01:09:18,480
that far away from, at least like some exposure

1304
01:09:18,480 --> 01:09:20,600
of being able to kind of have some version

1305
01:09:20,600 --> 01:09:23,080
of early telepathy or something.

1306
01:09:23,080 --> 01:09:26,400
Yeah, it's definitely possible.

1307
01:09:26,400 --> 01:09:29,200
I would still place that lower on the probability.

1308
01:09:29,200 --> 01:09:32,880
I think we'll probably get some maybe cool demos and stuff,

1309
01:09:32,880 --> 01:09:35,760
but then would I actually expect this

1310
01:09:35,760 --> 01:09:38,600
to become a big thing that seriously,

1311
01:09:39,720 --> 01:09:40,680
I mean, there are all these, like,

1312
01:09:40,680 --> 01:09:42,920
you read through the literature of cognitive enhancement,

1313
01:09:42,920 --> 01:09:44,200
there are all, like, hundreds of things

1314
01:09:44,200 --> 01:09:46,480
that supposedly have all these kind of effects,

1315
01:09:46,480 --> 01:09:49,760
but then the reality of it is that very few people bother,

1316
01:09:49,760 --> 01:09:52,240
and the ones who do probably don't actually benefit,

1317
01:09:52,240 --> 01:09:57,240
and yeah, but we might be surprised.

1318
01:09:58,600 --> 01:10:02,360
So, I mean, we do have quite a lot of optimization

1319
01:10:02,360 --> 01:10:04,320
behind language and stuff like that, right?

1320
01:10:04,320 --> 01:10:08,480
So I think it's still gonna be hard to do much better

1321
01:10:08,480 --> 01:10:11,840
than you can by just talking.

1322
01:10:12,520 --> 01:10:15,800
And so, you know, suppose that we go through the path

1323
01:10:15,800 --> 01:10:19,960
of digitizing, you know, getting to a full brain emulation

1324
01:10:19,960 --> 01:10:23,200
and so on, how do you see that transition sort of happening?

1325
01:10:23,200 --> 01:10:24,880
I mean, certainly at the beginning,

1326
01:10:24,880 --> 01:10:28,880
we'll start with, like, one or two of these examples,

1327
01:10:28,880 --> 01:10:31,000
first with some animals, and then eventually,

1328
01:10:31,000 --> 01:10:34,520
there'll be some moment where, whether it's a human,

1329
01:10:34,520 --> 01:10:37,320
how do you sort of, like, see that development developing?

1330
01:10:38,320 --> 01:10:42,320
My guess is we would come after superintelligence.

1331
01:10:43,840 --> 01:10:46,920
It is an alternative path to AGI,

1332
01:10:49,040 --> 01:10:52,040
but I've been more impressed by progress in AI

1333
01:10:52,040 --> 01:10:56,520
than in a whole brain emulation over the last 10 years,

1334
01:10:56,520 --> 01:10:58,720
and even before that, I thought the AI path

1335
01:10:58,720 --> 01:11:00,640
was more promising.

1336
01:11:01,920 --> 01:11:05,440
So, in that case, it would be superintelligence

1337
01:11:06,040 --> 01:11:09,320
that invents and perfects the uploading technology.

1338
01:11:09,320 --> 01:11:11,760
And I mean, in some sense, it doesn't really matter

1339
01:11:11,760 --> 01:11:16,760
exactly how it would work if it's an AI

1340
01:11:17,320 --> 01:11:18,480
that has to figure that out.

1341
01:11:18,480 --> 01:11:22,920
We mean, presumably, it would figure out a really reliable

1342
01:11:22,920 --> 01:11:27,920
and smooth way to do it, and then we would just sit back

1343
01:11:27,920 --> 01:11:29,840
and if we wanted to go down that path.

1344
01:11:30,600 --> 01:11:35,600
Yeah, I mean, we haven't really even small animals.

1345
01:11:37,160 --> 01:11:38,520
You might have thought by now,

1346
01:11:38,520 --> 01:11:41,840
maybe we could have like a bee or some little thing,

1347
01:11:43,800 --> 01:11:46,280
but so far, not really.

1348
01:11:49,880 --> 01:11:52,240
It might be that we will get to something

1349
01:11:52,240 --> 01:11:57,240
kind of impressive earlier without doing any brain scanning

1350
01:11:57,840 --> 01:12:02,240
at all, but just inferring from behavioral outputs.

1351
01:12:02,240 --> 01:12:06,080
So, you could already kind of have a DPT-3-like system

1352
01:12:06,080 --> 01:12:09,760
that roughly mimics somebody's literary style, let's say,

1353
01:12:09,760 --> 01:12:11,920
from having read a lot of their work.

1354
01:12:11,920 --> 01:12:14,520
And you can have these, I guess, deep fake things

1355
01:12:14,520 --> 01:12:19,040
that can mimic somebody's facial expressions and appearance

1356
01:12:19,040 --> 01:12:21,640
if you have a lot of video and somebody's voice.

1357
01:12:21,640 --> 01:12:23,520
And so, as these systems get smarter,

1358
01:12:23,520 --> 01:12:26,720
maybe you could also start to mimic somebody's thinking

1359
01:12:26,720 --> 01:12:28,560
to various increasing degrees.

1360
01:12:30,160 --> 01:12:35,160
And it's an interesting open question at the limit

1361
01:12:35,440 --> 01:12:37,760
if you had radical superintelligence,

1362
01:12:38,680 --> 01:12:42,720
but you only had the kind of data that is available now

1363
01:12:42,720 --> 01:12:46,680
from somebody's emails and some video interview

1364
01:12:46,680 --> 01:12:49,480
or some voice recording or whatever.

1365
01:12:49,480 --> 01:12:53,600
How much could a superintelligence infer from that data

1366
01:12:53,600 --> 01:12:56,920
as to what their mind must have been like

1367
01:12:56,920 --> 01:12:58,800
to have produced those outputs?

1368
01:13:00,120 --> 01:13:05,120
Is the best model that predicts these outputs

1369
01:13:05,120 --> 01:13:09,800
ones that would actually be similar enough

1370
01:13:09,800 --> 01:13:12,560
to the original person that it could possibly be seen

1371
01:13:12,560 --> 01:13:15,240
as a personal continuation?

1372
01:13:15,240 --> 01:13:18,080
That would it preserve personal identity?

1373
01:13:18,080 --> 01:13:20,840
Would it feel more or less the same

1374
01:13:20,840 --> 01:13:25,560
to be this AI reconstruction based on these behavioral traces

1375
01:13:25,560 --> 01:13:27,800
as it felt to be the original person?

1376
01:13:30,440 --> 01:13:34,080
I think it's quite possible that a superintelligence

1377
01:13:34,080 --> 01:13:38,160
would be able to do a lot with very little input.

1378
01:13:39,880 --> 01:13:43,280
I don't know how we could get like a firm,

1379
01:13:43,280 --> 01:13:45,880
a solid argument for that, but if I had to guess,

1380
01:13:45,880 --> 01:13:50,000
it seems like, yeah, you probably could get pretty close

1381
01:13:50,400 --> 01:13:52,120
if you were good enough at reconstructing

1382
01:13:52,120 --> 01:13:56,160
just from typical traces left behind by people today.

1383
01:13:57,440 --> 01:14:00,600
Yeah, it's an extreme way of interpolating out

1384
01:14:01,600 --> 01:14:05,520
and reviving actual ancestors or something like that.

1385
01:14:05,520 --> 01:14:08,600
Let's jump open it up for questions from the audience.

1386
01:14:08,600 --> 01:14:12,240
We'll take about 20 minutes of questions

1387
01:14:12,240 --> 01:14:14,880
and then conclude there.

1388
01:14:16,080 --> 01:14:18,160
Folks in the audience, if you have questions, raise your hand.

1389
01:14:18,160 --> 01:14:20,400
I think there'll be a mic going around.

1390
01:14:20,400 --> 01:14:23,320
And on Twitter, please use the hashtag PLBreakthroughs

1391
01:14:23,320 --> 01:14:25,280
to ask a question.

1392
01:14:25,280 --> 01:14:29,680
I'll kick it off with just a question that I sourced ahead.

1393
01:14:29,680 --> 01:14:32,600
Marco asks, in your view, where does consciousness emerge?

1394
01:14:32,600 --> 01:14:36,440
And before, how should we define consciousness?

1395
01:14:38,600 --> 01:14:39,600
And I think this is kind of related

1396
01:14:39,600 --> 01:14:40,920
to the simulation argument.

1397
01:14:40,920 --> 01:14:42,720
Which one of the three hypothesis

1398
01:14:42,720 --> 01:14:44,080
do you think is more likely to be true?

1399
01:14:44,080 --> 01:14:46,480
But I think let's first start with a consciousness one.

1400
01:14:46,480 --> 01:14:49,520
Where do you sort of imagine the consciousness emerging?

1401
01:14:49,520 --> 01:14:50,920
Like in the brain?

1402
01:14:50,920 --> 01:14:53,880
Yeah, but I guess it's more about the level,

1403
01:14:53,880 --> 01:14:55,960
so what level of processing?

1404
01:14:55,960 --> 01:14:58,200
So if you sort of go down in a neural system

1405
01:14:58,200 --> 01:15:01,040
all the way down to an extremely basic,

1406
01:15:01,040 --> 01:15:03,080
maybe like a nematode or something like that,

1407
01:15:03,080 --> 01:15:04,520
is that conscious?

1408
01:15:04,520 --> 01:15:07,120
And then in between a nematode and a human,

1409
01:15:07,120 --> 01:15:09,640
there's a mouse and so on.

1410
01:15:09,640 --> 01:15:12,600
Where exactly do we get consciousness emerging?

1411
01:15:12,600 --> 01:15:14,720
Certainly probably by a mouse, we definitely are past that,

1412
01:15:14,720 --> 01:15:22,360
but I think it's a matter of degree

1413
01:15:22,360 --> 01:15:25,320
and that there are multiple dimensions

1414
01:15:25,320 --> 01:15:31,640
in which you could interpolate smoothly

1415
01:15:31,640 --> 01:15:36,200
between, say, human consciousness and unconsciousness.

1416
01:15:36,200 --> 01:15:38,920
Like different directions you could go where,

1417
01:15:38,920 --> 01:15:41,760
if you keep going there, you sort of diminish,

1418
01:15:41,760 --> 01:15:45,680
in some sense, the quantity of experience there is

1419
01:15:45,680 --> 01:15:48,200
until you get to zero.

1420
01:15:48,200 --> 01:15:50,920
So one obvious one is, I mean, you

1421
01:15:50,920 --> 01:15:55,560
have a kind of integer multiplier.

1422
01:15:55,560 --> 01:15:58,560
If you have two brains in the same state,

1423
01:15:58,560 --> 01:16:00,080
undergoing the same states, I think

1424
01:16:00,080 --> 01:16:03,320
you would have sort of twice as much in one sense

1425
01:16:03,320 --> 01:16:07,560
of that experience as you would if you only had one brain.

1426
01:16:07,560 --> 01:16:10,360
And I have this old paper where I also argue

1427
01:16:10,400 --> 01:16:14,840
you could have fractional quantities of this.

1428
01:16:14,840 --> 01:16:24,040
If you build the circuitry that implements the mind

1429
01:16:24,040 --> 01:16:28,120
with unreliable components, like indeterministic processing

1430
01:16:28,120 --> 01:16:30,040
units, depending on exactly how you do it,

1431
01:16:30,040 --> 01:16:33,880
in certain cases, I think you would get a kind of,

1432
01:16:33,880 --> 01:16:35,200
as you get my higher reliability,

1433
01:16:35,200 --> 01:16:37,600
you would get larger and larger fragments of consciousness.

1434
01:16:37,600 --> 01:16:38,680
You had the whole thing.

1435
01:16:38,680 --> 01:16:42,680
But in other, you would actually get sort of 1.3 units

1436
01:16:42,680 --> 01:16:45,280
of qualitatively identical experience.

1437
01:16:45,280 --> 01:16:47,360
And you could also go down below one

1438
01:16:47,360 --> 01:16:51,400
to sort of scale it to zero in that dimension.

1439
01:16:51,400 --> 01:16:54,360
I think there are many other dimensions

1440
01:16:54,360 --> 01:16:59,240
as well in which the quality of experience

1441
01:16:59,240 --> 01:17:03,120
could become simpler and simpler and less and less

1442
01:17:03,120 --> 01:17:07,320
morally significant until it gets to a zone where maybe it's

1443
01:17:07,320 --> 01:17:13,440
just vague, like where our concept doesn't clearly

1444
01:17:13,440 --> 01:17:18,480
imply a fact of the matter.

1445
01:17:18,480 --> 01:17:21,000
Like once you get down to the insect levels,

1446
01:17:21,000 --> 01:17:25,600
maybe it's going to be there's a certain system.

1447
01:17:25,600 --> 01:17:27,520
And our concept of consciousness

1448
01:17:27,520 --> 01:17:32,800
might be such that even if you knew everything

1449
01:17:32,800 --> 01:17:37,160
about the insect, it would still be in the vague zone.

1450
01:17:37,840 --> 01:17:42,200
A little bit like there's a person who has a certain number

1451
01:17:42,200 --> 01:17:44,480
of hairs and are they bald?

1452
01:17:44,480 --> 01:17:46,040
Or like, I guess I'm bald.

1453
01:17:46,040 --> 01:17:50,040
But if I, once upon a time, I would

1454
01:17:50,040 --> 01:17:52,000
have been in this kind of vague zone.

1455
01:17:52,000 --> 01:17:58,080
And so, yeah.

1456
01:17:58,080 --> 01:17:59,840
And then there are other, like sometimes you're

1457
01:17:59,840 --> 01:18:01,560
more vividly aware, but sometimes you

1458
01:18:01,560 --> 01:18:03,080
might have some consciousness.

1459
01:18:03,080 --> 01:18:07,360
But there is no self-consciousness.

1460
01:18:07,360 --> 01:18:12,000
Or there is like some weird mental state that's,

1461
01:18:12,000 --> 01:18:18,960
I think we might be misled upon superficial introspection

1462
01:18:18,960 --> 01:18:22,040
to think that there is this very simple thing that

1463
01:18:22,040 --> 01:18:26,960
is subjective experience that either is there or is not there.

1464
01:18:26,960 --> 01:18:29,160
That it's a binary thing that we understand.

1465
01:18:29,200 --> 01:18:33,640
I think either if you reflect more theoretically

1466
01:18:33,640 --> 01:18:36,080
from a computationalist point of view and with brain,

1467
01:18:36,080 --> 01:18:38,040
you realize that that's a lot more problematic.

1468
01:18:38,040 --> 01:18:41,640
And I think you could also reach that conclusion

1469
01:18:41,640 --> 01:18:45,680
by just introspecting more carefully about your own state.

1470
01:18:45,680 --> 01:18:48,080
Like I think meditators maybe sometimes

1471
01:18:48,080 --> 01:18:52,920
would understand that things that seem very simple and homogeneous

1472
01:18:52,920 --> 01:18:56,160
as it were, if you really pay close attention

1473
01:18:56,160 --> 01:19:00,000
there are a lot more flickering and disjointed

1474
01:19:00,000 --> 01:19:01,080
and unintegrated.

1475
01:19:01,080 --> 01:19:04,520
And there's a lot of structure there that can come apart.

1476
01:19:07,120 --> 01:19:09,800
And I think that as we move away from the paradigm cases

1477
01:19:09,800 --> 01:19:14,440
of consciousness, like a normal waking human paying attention,

1478
01:19:14,440 --> 01:19:19,360
then properties that we think go together come apart.

1479
01:19:19,360 --> 01:19:21,880
And then it becomes more like a verbal question,

1480
01:19:21,880 --> 01:19:24,040
which set of those properties you need to have in order

1481
01:19:24,040 --> 01:19:28,240
to apply the label consciousness correctly.

1482
01:19:28,240 --> 01:19:30,160
Next question back there.

1483
01:19:30,160 --> 01:19:31,280
Hello.

1484
01:19:31,280 --> 01:19:32,600
First of all, thank you, Juan.

1485
01:19:32,600 --> 01:19:35,960
Thank you, Nick, for a really brilliant discussion

1486
01:19:35,960 --> 01:19:40,040
on the topic of artificial and superintelligence.

1487
01:19:40,040 --> 01:19:40,800
My name is Alex.

1488
01:19:40,800 --> 01:19:43,160
I'm a CO at Collective Technologist Labs.

1489
01:19:43,160 --> 01:19:49,360
And I want to ask you what is your opinion on maybe

1490
01:19:49,360 --> 01:19:51,600
the breakthrough in superintelligence

1491
01:19:51,600 --> 01:19:57,200
lays in the combination and symbiosis

1492
01:19:57,200 --> 01:19:59,840
of human intelligence and artificial intelligence

1493
01:19:59,840 --> 01:20:02,440
and not just artificial intelligence?

1494
01:20:08,120 --> 01:20:13,120
I think if you sort of squint a little,

1495
01:20:13,120 --> 01:20:16,920
you could say that that's kind of the state of play today,

1496
01:20:16,960 --> 01:20:20,720
where we don't have an individual system that

1497
01:20:20,720 --> 01:20:24,560
is superintelligent, but you could have humanity as a whole

1498
01:20:24,560 --> 01:20:27,160
or some big collective, like a large corporation

1499
01:20:27,160 --> 01:20:30,800
or the scientific community that is, at least in certain

1500
01:20:30,800 --> 01:20:34,360
respects, superintelligent in that they can perform

1501
01:20:34,360 --> 01:20:37,480
a wide range of tasks at a much higher level

1502
01:20:37,480 --> 01:20:40,720
than an individual human, but not all tasks.

1503
01:20:40,720 --> 01:20:43,520
So that's why it's not like a perfect example.

1504
01:20:43,520 --> 01:20:45,920
But yeah, and so some of these systems we have today

1505
01:20:45,920 --> 01:20:51,760
are certainly hybrids between biological brains,

1506
01:20:51,760 --> 01:20:55,480
information technology systems, like the internet,

1507
01:20:55,480 --> 01:21:00,360
social networks, depositors of papers,

1508
01:21:00,360 --> 01:21:02,000
and then a lot of culture as well.

1509
01:21:02,000 --> 01:21:09,640
That kind of, you could almost see these phenomena,

1510
01:21:09,640 --> 01:21:14,800
you start to get more and more where you get the current thing

1511
01:21:14,800 --> 01:21:19,080
and where there is a particular focus of attention

1512
01:21:19,080 --> 01:21:21,040
of the global brain, it's becoming more and more

1513
01:21:21,040 --> 01:21:23,880
like a human who's obsessed for a period of time

1514
01:21:23,880 --> 01:21:26,920
with some particular thing, and all the mental resources

1515
01:21:26,920 --> 01:21:29,360
get focused at one thing, and then your attention

1516
01:21:29,360 --> 01:21:32,800
shift to something different.

1517
01:21:32,800 --> 01:21:35,360
We're beginning to see a little bit of those dynamics

1518
01:21:35,360 --> 01:21:39,240
kind of happening in our collective cognitive space,

1519
01:21:39,240 --> 01:21:42,000
maybe as a result of the increased bandwidth

1520
01:21:42,000 --> 01:21:44,040
of interaction, and the technology

1521
01:21:44,040 --> 01:21:51,960
kind of enabling smoother communication,

1522
01:21:51,960 --> 01:21:54,520
not always producing superintelligence,

1523
01:21:54,520 --> 01:21:57,080
but other forms of kind of collective mentality

1524
01:21:57,080 --> 01:22:04,680
that sometimes may be subintelligence in terms

1525
01:22:04,680 --> 01:22:08,080
of their level of wisdom and understanding.

1526
01:22:08,080 --> 01:22:11,400
But yeah, in certain domains, you certainly

1527
01:22:11,520 --> 01:22:12,880
have a research community that's

1528
01:22:12,880 --> 01:22:17,000
target-focused on one particular problem that

1529
01:22:17,000 --> 01:22:19,240
are building on each other's contributions and blogs,

1530
01:22:19,240 --> 01:22:22,920
and you do get the sense of the whole being kind of,

1531
01:22:22,920 --> 01:22:24,240
there have been many different modules

1532
01:22:24,240 --> 01:22:25,920
that are each looking for the next way

1533
01:22:25,920 --> 01:22:28,320
to put a piece on the stack that is being built together,

1534
01:22:28,320 --> 01:22:29,960
and the whole stack goes up much faster

1535
01:22:29,960 --> 01:22:34,840
than if it were only one human building it.

1536
01:22:34,840 --> 01:22:38,400
Right, next question from Twitter.

1537
01:22:38,400 --> 01:22:40,880
Turner asks, what is the most important question,

1538
01:22:40,880 --> 01:22:43,440
which Nick feels he's not in a position

1539
01:22:43,440 --> 01:22:44,880
to personally solve?

1540
01:22:44,880 --> 01:22:46,640
Two factors, first being importance

1541
01:22:46,640 --> 01:22:50,200
to the development of ethical and successful AGI,

1542
01:22:50,200 --> 01:22:51,960
and second being Nick's inability,

1543
01:22:51,960 --> 01:22:53,440
lack of expertise to solve.

1544
01:22:58,120 --> 01:23:03,680
Well, I mean, there are questions

1545
01:23:03,680 --> 01:23:09,280
of more global nature, as in, ultimately, what

1546
01:23:09,280 --> 01:23:11,720
is the right direction to go in,

1547
01:23:11,720 --> 01:23:15,840
as it were the ultimately correct macro strategy.

1548
01:23:15,840 --> 01:23:20,040
I think we are sort of fundamentally in the dark,

1549
01:23:20,040 --> 01:23:24,280
regarding a lot of the ultimate and big-picture questions,

1550
01:23:24,280 --> 01:23:27,680
and that therefore our march forward

1551
01:23:27,680 --> 01:23:31,560
is, to some extent, an act of faith,

1552
01:23:32,880 --> 01:23:37,880
rather than the product of carefully thought-through insight.

1553
01:23:38,040 --> 01:23:40,760
Then I'm not sure we can get that insight at the moment,

1554
01:23:40,760 --> 01:23:45,080
and so that's one direction at which, at some point,

1555
01:23:45,080 --> 01:23:46,760
my understanding runs out,

1556
01:23:46,760 --> 01:23:49,920
and there's probably important stuff beyond that

1557
01:23:49,920 --> 01:23:52,720
that may or may not be good for us to try to reach,

1558
01:23:52,720 --> 01:23:54,800
but it's probably there in one way or another.

1559
01:23:54,800 --> 01:23:57,600
Another would be at a more technical level,

1560
01:23:57,600 --> 01:23:59,840
if you sort of zoom in and narrow it down,

1561
01:23:59,840 --> 01:24:03,320
so then a lot of stuff, say, for example, with AI alignment,

1562
01:24:04,160 --> 01:24:08,240
there's going to be a whole host of really important,

1563
01:24:08,240 --> 01:24:12,200
ultimately, technical results and algorithms

1564
01:24:12,200 --> 01:24:16,920
and stuff like that, that maybe currently nobody has,

1565
01:24:16,920 --> 01:24:18,160
and certainly I don't have,

1566
01:24:18,160 --> 01:24:23,160
and I probably won't discover them either,

1567
01:24:23,360 --> 01:24:28,360
but that might be critical to the future.

1568
01:24:29,200 --> 01:24:34,200
Then I guess you could zoom out in another direction,

1569
01:24:35,200 --> 01:24:37,480
sort of laterally, like across the social sphere,

1570
01:24:37,480 --> 01:24:42,480
so there are big problems like how to secure world peace

1571
01:24:43,000 --> 01:24:48,000
or like a welcoming uptake of these digital minds

1572
01:24:48,280 --> 01:24:52,760
that then involve problems at the cultural

1573
01:24:52,760 --> 01:24:54,680
and communication and political level,

1574
01:24:54,680 --> 01:24:57,920
where also one feels, I feel quite stumped

1575
01:24:57,920 --> 01:25:02,920
and it will, you know, so I'm kind of,

1576
01:25:04,000 --> 01:25:05,640
I'm squeezed in the middle of thought,

1577
01:25:05,640 --> 01:25:09,160
like if you zoom out too much, my understanding was out,

1578
01:25:09,160 --> 01:25:12,640
if you zoom down too much into the technical understanding

1579
01:25:12,640 --> 01:25:15,520
was out, and if you zoom out laterally,

1580
01:25:15,520 --> 01:25:17,880
also it's a little bubble there,

1581
01:25:17,880 --> 01:25:21,480
or I'm trying to keep track of what's going on.

1582
01:25:22,840 --> 01:25:24,720
All right, Addy asks,

1583
01:25:24,720 --> 01:25:26,480
if the speed of light would accelerate,

1584
01:25:26,480 --> 01:25:29,520
does this prove the theory we are living in a simulation,

1585
01:25:29,520 --> 01:25:32,640
and if no, what quantitative metric would validate the theory?

1586
01:25:37,440 --> 01:25:38,960
If the speed of light accelerates,

1587
01:25:38,960 --> 01:25:43,800
I don't see how that would,

1588
01:25:43,800 --> 01:25:45,120
it certainly wouldn't imply it,

1589
01:25:45,120 --> 01:25:48,440
I'm not sure immediately whether it would increase

1590
01:25:48,440 --> 01:25:50,520
or decrease the probability somewhere.

1591
01:25:51,440 --> 01:25:55,160
Maybe thinking about some marker that shows

1592
01:25:55,160 --> 01:26:00,160
that some kind of discontinuity on some quantity of physics

1593
01:26:01,080 --> 01:26:03,840
that just seems like bizarre to us or something.

1594
01:26:13,560 --> 01:26:17,280
So there are a lot of things that could change in physics

1595
01:26:17,280 --> 01:26:21,840
that would maybe be in one sense puzzling

1596
01:26:21,840 --> 01:26:22,800
and deep and interesting,

1597
01:26:22,800 --> 01:26:26,560
but ultimately, simple,

1598
01:26:27,480 --> 01:26:30,600
that there would be some possible physical law

1599
01:26:30,600 --> 01:26:33,160
that is itself simple that would describe them.

1600
01:26:34,160 --> 01:26:35,720
Now, you can contrast that,

1601
01:26:37,240 --> 01:26:38,960
and then of course you can have situations

1602
01:26:38,960 --> 01:26:39,920
where it's just chaotic,

1603
01:26:39,920 --> 01:26:42,360
but you could still capture the statistical regularities

1604
01:26:42,360 --> 01:26:43,720
through simple statistical law,

1605
01:26:43,720 --> 01:26:46,920
like that that's one type of basic universe we could live in,

1606
01:26:46,920 --> 01:26:51,280
which so far everything we know seems to be consistent with.

1607
01:26:52,800 --> 01:26:54,840
Now contrast that with a different possible world,

1608
01:26:54,840 --> 01:26:56,480
which we could have lived in

1609
01:26:56,480 --> 01:26:57,960
and we could still find out that we do,

1610
01:26:57,960 --> 01:27:01,800
where maybe you would have like parapsychology would be true.

1611
01:27:01,800 --> 01:27:05,000
So you would have like telekinesis or something,

1612
01:27:05,000 --> 01:27:08,360
where like what we think of as a high level,

1613
01:27:08,360 --> 01:27:10,800
complex macro state,

1614
01:27:10,800 --> 01:27:13,280
like a particular brain in a particular configuration,

1615
01:27:13,280 --> 01:27:15,080
but not in a slightly different configuration,

1616
01:27:15,080 --> 01:27:17,240
but just the types of configurations

1617
01:27:17,240 --> 01:27:20,240
that corresponds to somebody having a particular concept

1618
01:27:20,240 --> 01:27:21,080
and wish,

1619
01:27:21,080 --> 01:27:26,080
if that had like say a systematic physical impact

1620
01:27:26,680 --> 01:27:28,240
on some remote system,

1621
01:27:28,240 --> 01:27:31,600
like the way that parapsychologists have imagined,

1622
01:27:31,600 --> 01:27:33,800
like that would be puzzling,

1623
01:27:33,800 --> 01:27:37,400
not just because it would be fundamentally different

1624
01:27:37,400 --> 01:27:40,160
from like discovering that the speed of light is accelerating,

1625
01:27:40,160 --> 01:27:42,640
because it would be the thing that if it were true,

1626
01:27:42,640 --> 01:27:45,480
would seem to suggest that there were no micro level

1627
01:27:45,480 --> 01:27:47,200
explanation of the world.

1628
01:27:47,200 --> 01:27:48,680
Like you could have these macro states

1629
01:27:48,680 --> 01:27:52,400
that suddenly could like reach down and change the micro.

1630
01:27:52,400 --> 01:27:55,160
So if we made some discovery like that,

1631
01:27:55,160 --> 01:27:56,720
that then might,

1632
01:27:57,800 --> 01:28:01,320
yeah, lend evidence and credence

1633
01:28:01,320 --> 01:28:02,960
to the simulation hypothesis,

1634
01:28:02,960 --> 01:28:05,280
because that it looks very hard

1635
01:28:05,280 --> 01:28:07,760
to see how you could get all of this to square up,

1636
01:28:07,760 --> 01:28:08,920
maybe without that,

1637
01:28:08,920 --> 01:28:11,600
if you still wanted to have an underlying micro level

1638
01:28:11,600 --> 01:28:12,440
regularity,

1639
01:28:12,440 --> 01:28:14,920
you could have like the simulating universe

1640
01:28:14,920 --> 01:28:17,600
being kind of simple at the physics level,

1641
01:28:17,600 --> 01:28:20,440
but then simulating a different kind of universe.

1642
01:28:22,840 --> 01:28:25,320
The alternative would just be that we didn't have

1643
01:28:25,320 --> 01:28:27,640
that simplicity at the level of basic laws,

1644
01:28:27,640 --> 01:28:30,440
which I guess we could discover.

1645
01:28:30,440 --> 01:28:32,160
Now, I don't think that's the most,

1646
01:28:32,160 --> 01:28:35,760
the only or the most likely way we would find evidence

1647
01:28:35,760 --> 01:28:36,880
for the simulation argument,

1648
01:28:36,880 --> 01:28:38,720
if we offer the simulation hypothesis,

1649
01:28:38,720 --> 01:28:39,560
if we do,

1650
01:28:39,560 --> 01:28:40,480
that would just be one way,

1651
01:28:40,480 --> 01:28:41,880
like that would be more,

1652
01:28:42,880 --> 01:28:44,120
yeah, other kinds of evidence

1653
01:28:44,120 --> 01:28:46,200
that would be more likely to be relevant.

1654
01:28:46,320 --> 01:28:49,360
Since we're touching on the simulation argument,

1655
01:28:49,360 --> 01:28:51,560
which of the three hypotheses do you think

1656
01:28:51,560 --> 01:28:53,760
is the most likely just that,

1657
01:28:53,760 --> 01:28:56,080
sorry, which of the three prongs of the argument

1658
01:28:57,520 --> 01:28:59,360
do you currently think is most likely?

1659
01:29:00,360 --> 01:29:03,200
I'm generally a bit coy

1660
01:29:04,160 --> 01:29:06,800
in attaching probabilities to that.

1661
01:29:06,800 --> 01:29:11,800
So yeah, I tend to punt that question for various reasons,

1662
01:29:12,240 --> 01:29:13,960
including if I give a particular number

1663
01:29:14,040 --> 01:29:16,320
that might be misinterpreted,

1664
01:29:16,320 --> 01:29:18,680
but yeah, I mean, I would like,

1665
01:29:18,680 --> 01:29:20,040
so normally what people want to know

1666
01:29:20,040 --> 01:29:23,960
is especially on the simulation hypothesis,

1667
01:29:23,960 --> 01:29:26,040
like that's the one that I really want to know.

1668
01:29:26,040 --> 01:29:27,480
And as I mean, I guess,

1669
01:29:27,480 --> 01:29:29,240
yeah, I want to attach a probability to it,

1670
01:29:29,240 --> 01:29:30,480
but I certainly take it seriously.

1671
01:29:30,480 --> 01:29:34,840
It's not just like a logical possibility

1672
01:29:34,840 --> 01:29:38,840
or a thoughts experiment that we can't 100% rule out,

1673
01:29:38,840 --> 01:29:43,280
but it would certainly be like a live serious possibility

1674
01:29:43,280 --> 01:29:44,560
in my view.

1675
01:29:44,560 --> 01:29:46,560
Yeah, and for those unfamiliar,

1676
01:29:46,560 --> 01:29:51,040
the simulation argument is a three prong argument about

1677
01:29:51,040 --> 01:29:53,280
there's either we have a great filter,

1678
01:29:53,280 --> 01:29:56,440
meaning we have like close to zero advanced civilizations.

1679
01:29:56,440 --> 01:30:00,360
Either we have a disinterested set of advanced civilizations

1680
01:30:00,360 --> 01:30:02,760
where close to zero are interested

1681
01:30:02,760 --> 01:30:04,680
in running those simulations.

1682
01:30:04,680 --> 01:30:07,040
And then there's a simulation hypothesis,

1683
01:30:07,040 --> 01:30:09,440
which is that, hey, if there's no great filter

1684
01:30:09,440 --> 01:30:10,760
and they are interested,

1685
01:30:10,760 --> 01:30:12,880
then close to all beings are simulated.

1686
01:30:12,880 --> 01:30:16,080
And this comes from thinking about just the vast number

1687
01:30:16,080 --> 01:30:20,120
and vast quantities of people that would be simulated

1688
01:30:20,120 --> 01:30:23,400
and then the likelihood of your experience

1689
01:30:23,400 --> 01:30:26,240
being sampled from the simulated ones.

1690
01:30:26,240 --> 01:30:29,660
Sorry, Nick, I'm probably giving you a bad explanation here,

1691
01:30:29,660 --> 01:30:30,500
but...

1692
01:30:30,500 --> 01:30:31,960
No, no, that's very good to hear.

1693
01:30:33,800 --> 01:30:36,880
I think that was another question over here, or yeah.

1694
01:30:36,880 --> 01:30:37,760
Yeah, I have a question.

1695
01:30:37,760 --> 01:30:41,440
So I've always been very interested in emergent intelligence,

1696
01:30:41,440 --> 01:30:42,760
especially as it relates to animals.

1697
01:30:42,760 --> 01:30:45,320
I mean, the classic example tends to be beehives.

1698
01:30:45,320 --> 01:30:46,440
As we look at consciousness,

1699
01:30:46,440 --> 01:30:48,300
what biases do you think we bring in

1700
01:30:48,300 --> 01:30:50,440
as individual social animals, humans,

1701
01:30:50,440 --> 01:30:52,400
versus a collectible organism like bees,

1702
01:30:52,400 --> 01:30:53,480
especially as we look at humans,

1703
01:30:53,480 --> 01:30:54,800
maybe moving to be more bee-like

1704
01:30:54,800 --> 01:30:57,160
as we create nation states and larger organizations,

1705
01:30:57,160 --> 01:30:58,240
versus a singleton?

1706
01:30:58,240 --> 01:31:00,520
How would a singleton perhaps have a different AI alignment

1707
01:31:00,520 --> 01:31:02,120
bias?

1708
01:31:02,120 --> 01:31:02,960
As I think about this,

1709
01:31:02,960 --> 01:31:04,680
the only really intelligent animals I can think of

1710
01:31:04,680 --> 01:31:06,420
that don't live socially are apex predators,

1711
01:31:06,420 --> 01:31:08,040
which is perhaps a bad sign.

1712
01:31:12,760 --> 01:31:13,960
Let me see if I understand.

1713
01:31:13,960 --> 01:31:18,120
So I think, well, so one question...

1714
01:31:18,120 --> 01:31:19,120
To phrase this differently,

1715
01:31:19,120 --> 01:31:20,760
if I think about a curve,

1716
01:31:20,760 --> 01:31:22,680
do you think that collective intelligences

1717
01:31:22,680 --> 01:31:25,760
like hive animals are on one side of the spectrum

1718
01:31:25,760 --> 01:31:28,080
with social animals like humans in the middle,

1719
01:31:28,080 --> 01:31:29,520
with singletons being on another extreme,

1720
01:31:29,520 --> 01:31:31,320
or is it more of a horseshoe curve

1721
01:31:31,320 --> 01:31:32,640
in terms of the distribution of intelligences

1722
01:31:32,640 --> 01:31:34,080
and how they work towards common goals

1723
01:31:34,080 --> 01:31:36,660
that may be 11 or not aligned with us?

1724
01:31:38,780 --> 01:31:41,240
Well, if there were a line,

1725
01:31:41,240 --> 01:31:43,840
I think the superintelligence would be more on the side

1726
01:31:43,840 --> 01:31:46,480
of these hive insects.

1727
01:31:46,480 --> 01:31:49,000
If we look at the scale of an ant colony,

1728
01:31:49,000 --> 01:31:54,000
it's in some sense, it acts like a singleton within that.

1729
01:31:54,320 --> 01:31:56,240
Of course, there are other ant colonies elsewhere

1730
01:31:56,240 --> 01:31:58,160
and other things that it doesn't have control over,

1731
01:31:58,160 --> 01:32:01,120
but they would, as it were,

1732
01:32:01,120 --> 01:32:06,120
be able to act as a single agent to some extent.

1733
01:32:06,960 --> 01:32:11,960
And humans, to only a lesser extent,

1734
01:32:12,520 --> 01:32:15,480
although in some dimensions, we are better coordinated

1735
01:32:15,480 --> 01:32:17,120
in terms of being able to share

1736
01:32:17,120 --> 01:32:19,000
detailed information and plans.

1737
01:32:19,000 --> 01:32:22,480
We are, in that respect, we are more coordinated than ants,

1738
01:32:22,480 --> 01:32:26,480
but in the respect of our individual wills

1739
01:32:26,480 --> 01:32:30,000
being less aligned to a common goal,

1740
01:32:30,000 --> 01:32:35,000
we are less like a singleton than human.

1741
01:32:36,520 --> 01:32:39,040
An ant colony is.

1742
01:32:39,040 --> 01:32:42,280
And I guess you could have a group of animals

1743
01:32:42,280 --> 01:32:44,520
that were even more individualistic

1744
01:32:44,520 --> 01:32:46,000
and antisocial than humans are,

1745
01:32:46,000 --> 01:32:50,020
and they would then be further away on the other side.

1746
01:32:50,020 --> 01:32:51,720
So humans would kind of be in the middle

1747
01:32:51,720 --> 01:32:56,200
where we have a fair degree of sort of shared purpose,

1748
01:32:56,200 --> 01:33:00,240
but not like a full hive organism,

1749
01:33:01,360 --> 01:33:02,960
but also a lot more than zero.

1750
01:33:03,840 --> 01:33:08,840
It's, I guess, an interesting question.

1751
01:33:09,040 --> 01:33:10,360
It's certainly different animals.

1752
01:33:10,360 --> 01:33:13,760
I mean, have different goals, it seems,

1753
01:33:13,760 --> 01:33:16,440
like some, I mean, at least at the superficial level,

1754
01:33:16,440 --> 01:33:18,560
some like to eat grass and some like to eat meat

1755
01:33:18,560 --> 01:33:21,400
and some like to hang around with others of their kind

1756
01:33:21,400 --> 01:33:24,400
and some like to just do their own thing.

1757
01:33:26,320 --> 01:33:28,880
And presumably if there were some other species

1758
01:33:28,880 --> 01:33:30,480
that developed superintelligence

1759
01:33:30,480 --> 01:33:33,320
and aligned it to their values,

1760
01:33:33,320 --> 01:33:36,640
then they might also have different baseline goals

1761
01:33:36,640 --> 01:33:38,480
that might overlap slightly with humans,

1762
01:33:38,480 --> 01:33:42,180
like, but also be different in other respects.

1763
01:33:43,800 --> 01:33:45,800
There are two open questions.

1764
01:33:45,800 --> 01:33:47,720
One is like epistemically,

1765
01:33:47,720 --> 01:33:49,740
is are there significant differences

1766
01:33:49,740 --> 01:33:54,740
between the inductive biases that are brought to the table?

1767
01:33:54,960 --> 01:33:56,600
Presumably there are some inductive biases

1768
01:33:56,600 --> 01:33:57,440
that are different,

1769
01:33:57,520 --> 01:34:01,640
but would those kind of be smoothed out reasonably fast

1770
01:34:01,640 --> 01:34:04,360
as you have more data and more intelligence?

1771
01:34:04,360 --> 01:34:05,520
Like it doesn't start,

1772
01:34:05,520 --> 01:34:09,320
like it may be like a squirrel would more quickly cut on

1773
01:34:09,320 --> 01:34:12,200
onto certain things that are relevant to the squirrel world

1774
01:34:12,200 --> 01:34:13,600
and some other organism to another,

1775
01:34:13,600 --> 01:34:16,560
but like as they develop scientific reasoning,

1776
01:34:16,560 --> 01:34:18,240
like do they have enough overlap

1777
01:34:18,240 --> 01:34:19,680
between their inductive biases

1778
01:34:19,680 --> 01:34:21,920
that the difference is washed out

1779
01:34:21,920 --> 01:34:24,480
as you see the full impact of the evidence?

1780
01:34:24,480 --> 01:34:26,000
That that's one question you could ask.

1781
01:34:26,000 --> 01:34:27,640
And like another is that even though

1782
01:34:27,640 --> 01:34:29,560
these different organisms start out with

1783
01:34:29,560 --> 01:34:31,440
at least two officially different goals,

1784
01:34:31,440 --> 01:34:34,240
are they in some deeper sense,

1785
01:34:34,240 --> 01:34:36,680
the same or alternatively,

1786
01:34:36,680 --> 01:34:41,080
would they arrive at some shared understanding

1787
01:34:41,080 --> 01:34:44,160
of what the highest moral norms are,

1788
01:34:44,160 --> 01:34:47,000
even if their own personal goals might differ?

1789
01:34:48,000 --> 01:34:50,000
Like a lot of humans might individually

1790
01:34:50,000 --> 01:34:51,160
have different preferences,

1791
01:34:51,160 --> 01:34:52,920
like I care about my family

1792
01:34:52,920 --> 01:34:54,040
and you care about your family,

1793
01:34:54,040 --> 01:34:56,000
but we might nevertheless converge

1794
01:34:56,000 --> 01:34:59,200
in the sense of let's respect each other's families,

1795
01:34:59,200 --> 01:35:00,760
let's say like a cooperative level

1796
01:35:00,760 --> 01:35:04,280
of more abstract norms might also be convergent,

1797
01:35:05,240 --> 01:35:07,200
quite independent of starting point.

1798
01:35:08,640 --> 01:35:10,920
So those are two questions I could ask there

1799
01:35:10,920 --> 01:35:15,920
that I'm not sure what the answer is, but I'm not,

1800
01:35:16,440 --> 01:35:18,080
yeah, I don't know whether that addresses

1801
01:35:18,080 --> 01:35:21,440
your question at all, but it's my effort.

1802
01:35:21,440 --> 01:35:22,800
I'll have two more questions.

1803
01:35:22,840 --> 01:35:27,680
One is how sure are you, Nick,

1804
01:35:27,680 --> 01:35:29,960
that an evil singleton AI to rule them all

1805
01:35:29,960 --> 01:35:32,160
would be internally aligned over time?

1806
01:35:32,160 --> 01:35:34,680
Could it be fundamentally set up to split or diverge

1807
01:35:34,680 --> 01:35:37,720
with subunits pursuing different ideals or goals?

1808
01:35:42,880 --> 01:35:44,840
I guess everything is possible.

1809
01:35:44,840 --> 01:35:49,840
I mean, if it were unified at one point in time,

1810
01:35:50,000 --> 01:35:53,000
and if at that point it was technically mature,

1811
01:35:53,000 --> 01:35:57,360
then I would expect it to remain unified

1812
01:35:57,360 --> 01:35:59,840
because I think it would have access

1813
01:35:59,840 --> 01:36:01,560
to the kind of control technology

1814
01:36:01,560 --> 01:36:04,120
that would make it possible for it to do that,

1815
01:36:04,120 --> 01:36:07,680
and I think it would have instrumental reasons to do that

1816
01:36:07,680 --> 01:36:12,360
for almost all initial goals it might have at that time.

1817
01:36:12,360 --> 01:36:13,880
You could imagine some very special goal,

1818
01:36:13,880 --> 01:36:16,840
like if it specifically has as its top level goal

1819
01:36:16,840 --> 01:36:17,960
a thousand years from now,

1820
01:36:17,960 --> 01:36:20,440
I want to be divided against myself

1821
01:36:20,440 --> 01:36:23,920
and fighting like an insurrection against myself.

1822
01:36:23,920 --> 01:36:26,040
If that were its goal, then I should have arranged that,

1823
01:36:26,040 --> 01:36:28,600
but for most goals, it would probably be able

1824
01:36:28,600 --> 01:36:30,320
to achieve them to a higher degree

1825
01:36:30,320 --> 01:36:32,920
if it worked in concert with itself.

1826
01:36:32,920 --> 01:36:36,280
And then I'd imagine it would also have the technology

1827
01:36:36,280 --> 01:36:38,320
and insights to make that happen.

1828
01:36:39,760 --> 01:36:43,280
If it starts out unified, like if it starts out

1829
01:36:43,280 --> 01:36:46,560
like a sort of vaguely politically integrated

1830
01:36:46,560 --> 01:36:50,680
political entity, then it might be

1831
01:36:50,680 --> 01:36:53,520
that even with technological maturity,

1832
01:36:53,520 --> 01:36:57,080
it's not so crazy to think it might come apart at a later,

1833
01:36:57,080 --> 01:36:57,960
just like humans do.

1834
01:36:57,960 --> 01:37:01,440
Like sometimes you have a well-functioning political unit

1835
01:37:01,440 --> 01:37:06,120
and then 50 years later, you have anarchy

1836
01:37:06,120 --> 01:37:07,080
in a particular state.

1837
01:37:07,080 --> 01:37:10,880
Like we can kind of get these temporary partial solutions

1838
01:37:10,880 --> 01:37:14,520
that I guess would also be possible with certain kinds

1839
01:37:14,520 --> 01:37:16,400
of like maybe some upload collective

1840
01:37:16,400 --> 01:37:18,440
that comes together to achieve super intelligence.

1841
01:37:18,440 --> 01:37:21,280
You could imagine political dynamics working well

1842
01:37:21,280 --> 01:37:23,880
for a period of time and then it's falling apart.

1843
01:37:23,880 --> 01:37:25,200
I still think that's less likely

1844
01:37:25,200 --> 01:37:27,000
than it's going kind of towards a single time,

1845
01:37:27,000 --> 01:37:30,840
but by no means extremely unlikely.

1846
01:37:32,640 --> 01:37:34,440
And last question, if things go well,

1847
01:37:34,440 --> 01:37:36,400
Devadat asks, if things go well,

1848
01:37:36,400 --> 01:37:38,880
do you have a vision for how differences of opinion

1849
01:37:38,880 --> 01:37:42,200
about what a good future society looks like?

1850
01:37:42,200 --> 01:37:44,800
Sorry, if things go well, do you have a vision

1851
01:37:44,800 --> 01:37:47,160
for how differences of opinion

1852
01:37:47,160 --> 01:37:50,920
about what a good future society looks like

1853
01:37:50,920 --> 01:37:52,760
can be accommodated?

1854
01:37:52,760 --> 01:37:55,440
Meaning, is a like big enough for everyone

1855
01:37:55,440 --> 01:37:58,560
as they develop very different perspectives

1856
01:37:58,560 --> 01:38:01,560
and different ideas of what a good future society looks like?

1857
01:38:01,560 --> 01:38:04,080
How do we kind of reconcile those differences of opinion?

1858
01:38:04,080 --> 01:38:07,360
How do we build a meta system to kind of enable

1859
01:38:07,360 --> 01:38:12,360
like different flourishing civilizations in a sense?

1860
01:38:13,360 --> 01:38:18,360
Yeah, I think it's large enough for

1861
01:38:21,360 --> 01:38:26,360
almost all people to have most of their values accommodated.

1862
01:38:28,880 --> 01:38:32,880
Like if you have two people have literally opposed values

1863
01:38:32,880 --> 01:38:34,360
about a particular thing,

1864
01:38:35,640 --> 01:38:38,120
then you might not be able to satisfy both.

1865
01:38:38,120 --> 01:38:42,040
But I think a combination of on the one hand,

1866
01:38:42,160 --> 01:38:46,160
some differences being perhaps merely superficial,

1867
01:38:47,560 --> 01:38:51,160
either disappearing up on better understanding,

1868
01:38:51,160 --> 01:38:52,320
like there's like certain things

1869
01:38:52,320 --> 01:38:55,040
where we just have ultimately different beliefs

1870
01:38:55,040 --> 01:38:56,440
and we say we want different things,

1871
01:38:56,440 --> 01:38:58,120
but it's because we have different assumptions

1872
01:38:58,120 --> 01:39:00,920
about what would actually happen, let's say.

1873
01:39:00,920 --> 01:39:03,720
So those being potentially diminished

1874
01:39:03,720 --> 01:39:06,520
by increased intelligence and knowledge and experience,

1875
01:39:07,840 --> 01:39:11,640
then the increase in resources

1876
01:39:11,640 --> 01:39:14,440
and expansion of the technological frontier

1877
01:39:14,440 --> 01:39:16,600
and then some kind of creativity

1878
01:39:16,600 --> 01:39:21,600
and like figuring out clever ways of combining values.

1879
01:39:21,760 --> 01:39:26,760
I think hopeful that a great deal can be accommodated

1880
01:39:29,320 --> 01:39:34,080
because of these things, but not necessarily a hundred percent.

1881
01:39:34,080 --> 01:39:36,240
And then it would be important to have

1882
01:39:37,240 --> 01:39:42,240
a robust and effective way to manage any resulting disagreements

1883
01:39:44,920 --> 01:39:47,960
in a way that doesn't result in like negative,

1884
01:39:47,960 --> 01:39:49,680
some dynamics.

1885
01:39:49,680 --> 01:39:51,240
And so hence,

1886
01:39:51,240 --> 01:39:53,240
because I think that's ultimately really important.

1887
01:39:53,240 --> 01:39:57,480
I'm like, I think we should have a strong bias towards

1888
01:39:57,480 --> 01:40:02,480
ads forward that are more cooperative and friendly.

1889
01:40:03,160 --> 01:40:08,160
And even if they seem to come at some short term expense

1890
01:40:09,520 --> 01:40:14,520
or if they can't be very crisply motivated

1891
01:40:14,840 --> 01:40:17,880
by some explicit calculation in every single case,

1892
01:40:17,880 --> 01:40:22,880
I think that general attitude as a sort of default bias,

1893
01:40:23,040 --> 01:40:28,040
I think is still very much worth bearing in mind

1894
01:40:28,680 --> 01:40:32,320
as we are pursuing these different aspects

1895
01:40:32,320 --> 01:40:34,520
of the challenges ahead.

1896
01:40:35,920 --> 01:40:37,360
That should be our first resort.

1897
01:40:37,360 --> 01:40:40,240
Sometimes you have to, you can't get full cooperation.

1898
01:40:40,240 --> 01:40:43,280
You don't want to be completely naive and gullible.

1899
01:40:43,280 --> 01:40:46,840
And but still like that, that should be the first

1900
01:40:46,840 --> 01:40:48,800
and maybe the second attempt.

1901
01:40:48,800 --> 01:40:50,600
And then gradually scale back from that

1902
01:40:50,600 --> 01:40:53,360
if really forced by circumstances.

1903
01:40:54,360 --> 01:40:57,960
Well, that's all the time we have for questions.

1904
01:40:57,960 --> 01:41:01,600
Nick, thank you so much for spending this evening with us.

1905
01:41:01,600 --> 01:41:04,040
It has been extremely enlightening for many of us.

1906
01:41:04,040 --> 01:41:07,200
And I think we'll be very useful to the broader community

1907
01:41:07,200 --> 01:41:09,640
that is currently working on things like AI alignment

1908
01:41:09,640 --> 01:41:10,480
and others.

1909
01:41:10,480 --> 01:41:12,960
And thank you really much for your work,

1910
01:41:12,960 --> 01:41:16,760
for sharing your insights and for helping us achieve

1911
01:41:16,760 --> 01:41:18,120
a lot of great breakthroughs

1912
01:41:18,120 --> 01:41:20,320
and hopefully have a great long-term future.

1913
01:41:20,320 --> 01:41:21,160
Thank you very much.

1914
01:41:21,400 --> 01:41:22,560
A lot of good questions.

1915
01:41:22,560 --> 01:41:24,080
Thank you very much for having me.

1916
01:41:24,080 --> 01:41:24,920
Yeah, absolutely.

1917
01:41:24,920 --> 01:41:25,920
Thanks. Thank you.

1918
01:41:25,920 --> 01:41:26,760
Take care.

