WEBVTT

00:00.000 --> 00:04.600
It is a great pleasure and honor to be here today,

00:04.600 --> 00:07.640
speaking with Professor Nick Bostrom.

00:07.640 --> 00:13.440
Professor Bostrom is one of my favorite people alive today

00:13.440 --> 00:16.080
and probably in history.

00:16.080 --> 00:19.800
From my perspective, he's, you know,

00:19.800 --> 00:22.840
if we make it as a species into the far future,

00:22.840 --> 00:25.840
it'll be in significant part thanks to him

00:25.840 --> 00:29.200
and his work helping us look, think about the future,

00:29.200 --> 00:32.480
think about the long term, think about how we might evolve.

00:32.480 --> 00:36.920
He's written, of course, about many things in technology,

00:36.920 --> 00:39.560
but especially about digital minds,

00:39.560 --> 00:46.000
the evolution of humanity, super intelligences, and more.

00:46.000 --> 00:51.880
He leads the Oxford Future of Humanity Institute,

00:51.880 --> 00:55.680
where he and many other researchers help the world

00:55.680 --> 00:59.080
think about these extremely important topics

00:59.080 --> 01:02.640
in a variety of ways from both research directly

01:02.640 --> 01:05.520
into the philosophy of these questions

01:05.520 --> 01:09.960
and the making estimations about the real impact

01:09.960 --> 01:14.320
and also framing and constructing important policy work

01:14.320 --> 01:17.680
that can help guide many policymakers around the world

01:17.680 --> 01:20.480
in how to think about these critical policies.

01:20.480 --> 01:24.600
So today we're going to have a very good and lively discussion

01:24.640 --> 01:26.760
about many of these topics,

01:26.760 --> 01:28.880
especially things like super intelligences,

01:28.880 --> 01:33.240
where are we in these timelines, whole brain emulation,

01:33.240 --> 01:36.120
digital minds, the future of these,

01:36.120 --> 01:39.000
the challenges for our civilization, and more.

01:39.000 --> 01:40.840
The format of the evening will be that,

01:40.840 --> 01:43.520
we'll sit in our fireside chat first,

01:43.520 --> 01:45.000
I'll ask a set of questions,

01:45.000 --> 01:51.600
and then around 30 to 40, maybe 50 minutes from now,

01:51.600 --> 01:53.160
given I have a bunch of questions,

01:53.240 --> 01:56.600
I'll open up to and transition to questions from the audience,

01:56.600 --> 02:00.440
and then we'll set them out in time then.

02:00.440 --> 02:03.320
I'll be reading from both questions that I've sourced

02:03.320 --> 02:06.600
from many folks around the product labs community

02:06.600 --> 02:08.680
ahead of time, and from audience members

02:08.680 --> 02:10.680
who are here in person,

02:10.680 --> 02:14.320
and from the folks in the livestream watching.

02:14.320 --> 02:18.280
So I'll be checking out Twitter for the hashtag PL breakthroughs.

02:18.280 --> 02:20.160
So if you want to ask a question,

02:20.160 --> 02:23.640
either find the tweet about it,

02:23.640 --> 02:26.760
and please enter your question with the hashtag PL breakthroughs,

02:26.760 --> 02:28.240
I'll be monitoring those,

02:28.240 --> 02:32.320
and then I'll try to round Robin between source questions ahead,

02:32.320 --> 02:35.560
person, people in the audience, and the livestream.

02:35.560 --> 02:39.760
And if there's a new digital intelligence

02:39.760 --> 02:41.160
out there lurking on Twitter,

02:41.160 --> 02:43.440
please feel free to join the discussion.

02:44.440 --> 02:46.360
Well, welcome Nick, thank you so much for being with us,

02:46.360 --> 02:47.880
and thank you so much for your work.

02:47.880 --> 02:49.720
How are you doing today?

02:49.720 --> 02:51.040
The stuff was so good.

02:52.160 --> 02:55.400
Great, so let's kind of dive right into the deep end.

02:55.400 --> 02:58.200
So thinking about superintelligence,

02:59.200 --> 03:01.440
based on kind of like latest developments,

03:01.440 --> 03:05.680
how have your estimates of superintelligence development

03:05.680 --> 03:08.360
shifted over time, like kind of in hindsight,

03:08.360 --> 03:11.600
where we are now in 2022, looking back,

03:11.600 --> 03:13.160
how do you think things are going?

03:13.160 --> 03:14.600
Are things proceeding faster or slower

03:14.600 --> 03:16.320
than you might have thought?

03:16.320 --> 03:17.600
Where do you think we are?

03:18.600 --> 03:23.520
I think since the book superintelligence came out in 2014,

03:23.520 --> 03:27.640
developments have been faster than expected.

03:27.640 --> 03:31.000
So timelines generally have contracted.

03:32.640 --> 03:37.560
It's quite impressive to see the rapid pace of advances

03:37.560 --> 03:39.280
in recent years,

03:39.280 --> 03:42.600
and how the same set of basic techniques,

03:43.600 --> 03:45.880
big, deep neural networks,

03:45.880 --> 03:48.160
and specifically transformer models

03:48.160 --> 03:51.920
just seem to keep working in many different domains.

03:51.920 --> 03:54.040
And even as you scale them up,

03:54.040 --> 03:56.520
you continue to get better results.

03:58.760 --> 04:00.480
And as the chefs,

04:00.480 --> 04:02.400
what have been some of the most surprising results

04:02.400 --> 04:05.400
from this that you think,

04:05.400 --> 04:07.120
I don't know, maybe you just didn't expect

04:07.120 --> 04:09.760
this particular concrete thing to be possible so soon?

04:10.640 --> 04:16.640
I think AlphaGo happened ahead of schedule.

04:16.640 --> 04:21.640
Well, I mean, I think just recently before it happened,

04:21.640 --> 04:23.680
it was kind of clear that it was going to happen,

04:23.680 --> 04:27.760
but I think it was quite impressive

04:27.760 --> 04:32.840
that you could take something that is a very deep pattern

04:32.840 --> 04:35.480
recognition problem with deep strategy

04:35.480 --> 04:38.720
where humans have worked for thousands of years

04:38.720 --> 04:41.560
to try to refine and come up with the best strategies

04:41.560 --> 04:46.000
that you could just solve it with AI.

04:50.000 --> 04:54.000
And then I think the GPT-3, the large language models,

04:54.000 --> 04:55.680
is, I guess, slightly...

04:55.680 --> 05:00.160
I mean, I don't think any of these is hugely surprising.

05:00.160 --> 05:04.080
And by now, we kind of expect to be surprised,

05:04.080 --> 05:05.520
and so we are not really surprised,

05:05.520 --> 05:07.600
but still, yeah, I think...

05:09.720 --> 05:13.560
these are impressive achievements.

05:13.560 --> 05:18.360
And I guess even just before that,

05:18.360 --> 05:22.360
the fact that image recognition and image processing

05:22.360 --> 05:27.600
was one of the first really cool things that started to work

05:27.600 --> 05:30.120
is maybe a little bit surprising,

05:30.120 --> 05:32.720
and given that it's a large chunk of the human brain

05:32.720 --> 05:35.360
that is devoted to visual processing,

05:35.360 --> 05:38.440
it's not like some kind of simple logic-chopping activity.

05:38.440 --> 05:40.520
And so the fact that that fell into place

05:40.520 --> 05:45.040
and that you can do this quite sophisticated manipulation

05:45.040 --> 05:55.040
of imagery, I think, was slightly surprising at the time.

05:55.040 --> 05:58.760
What do you think about developments like AlphaFold

05:58.760 --> 06:01.680
and just solving that set of challenges?

06:01.680 --> 06:04.920
Do you think that that is substantially different,

06:04.920 --> 06:07.720
or is it not like a substantially...

06:07.720 --> 06:09.360
it's just kind of a very great application,

06:09.360 --> 06:10.760
but...

06:10.760 --> 06:14.920
or do you think that that's an important improvement?

06:14.920 --> 06:16.120
I mean, in terms of surprise,

06:16.120 --> 06:18.800
like, I guess once you can do AlphaFold,

06:18.800 --> 06:22.960
it's not so surprising that it should work for AlphaFold as well.

06:22.960 --> 06:25.440
Like, humans have put in less brain power

06:25.440 --> 06:31.080
into figuring out how to fold proteins than into playing Go.

06:31.080 --> 06:33.320
And it's, at least superficially,

06:33.320 --> 06:37.680
looks like the same kind of spatial pattern type of stuff.

06:38.480 --> 06:41.120
Obviously, in terms of practical ramifications,

06:41.120 --> 06:44.720
AlphaFold is potentially a lot more useful

06:44.720 --> 06:51.800
and for medicine and chemical research,

06:51.800 --> 06:56.400
maybe like extensions of the same system.

06:56.400 --> 07:00.480
I do think that as we move into some of these more applied areas,

07:00.480 --> 07:07.080
that there are potential security concerns

07:07.120 --> 07:10.160
that we need to also start to take more seriously.

07:10.160 --> 07:13.520
I mean, my work has been focused more on risks arising

07:13.520 --> 07:15.480
from human level or superintelligence,

07:15.480 --> 07:17.840
like general AGI, where they can reason

07:17.840 --> 07:20.600
and have a kind of transformative impact on the world.

07:20.600 --> 07:24.200
But there might also be some narrower domains

07:24.200 --> 07:29.560
where there will be smaller but still significant issues.

07:29.560 --> 07:31.760
So one of those would be in synthetic biology

07:31.760 --> 07:36.760
if it becomes too easy to concoct bad stuff.

07:36.960 --> 07:40.680
It might be, for example, that the scientific model

07:40.680 --> 07:45.080
of open publication and make all your models

07:45.080 --> 07:47.800
ideally available to anybody to do anything

07:47.800 --> 07:50.600
is not the right model for those application areas.

07:54.440 --> 07:56.080
When you think about the current architectures,

07:56.080 --> 07:59.040
and certainly the language models

07:59.040 --> 08:01.040
have been extraordinarily successful

08:01.040 --> 08:02.040
in a variety of domains.

08:02.040 --> 08:05.200
But do you think that this is the architecture

08:05.200 --> 08:08.200
that is likely to evolve into an AGI?

08:08.200 --> 08:10.640
Or do you think that there's some substantial architectural

08:10.640 --> 08:14.040
improvements that humans have to make first?

08:16.240 --> 08:18.880
My guess would be that if there are substantial

08:18.880 --> 08:20.400
additional architectural improvements,

08:20.400 --> 08:22.440
there are not that many of them.

08:24.000 --> 08:29.000
And maybe they would be built on top of transformer models

08:29.200 --> 08:32.240
or connected up to the transformer models

08:32.240 --> 08:34.400
or some variation of transformer models.

08:36.200 --> 08:39.880
So maybe, I don't know, like my media, I guess,

08:39.880 --> 08:43.040
would be made, I don't know, maybe just something

08:43.040 --> 08:46.880
that is as big an advance as transformers were.

08:46.880 --> 08:48.280
Like if we get one more of those,

08:48.280 --> 08:50.880
like that could easily be, I mean, it's also possible

08:50.880 --> 08:52.400
like just scaling up what we currently have

08:52.400 --> 08:54.320
with some minor things would suffice.

08:56.480 --> 08:58.160
But if there is some other thing,

08:58.160 --> 09:02.560
like you need to connect it up with some kind

09:02.600 --> 09:05.320
of external memory system,

09:05.320 --> 09:08.720
or you need some other inductive bias

09:08.720 --> 09:12.080
that make the representations

09:12.080 --> 09:14.200
on a more easily composable insert,

09:14.200 --> 09:16.640
like some kind of extra thing like that,

09:16.640 --> 09:19.260
that may or may not be very hard to discover.

09:20.240 --> 09:22.800
That would not at all be surprising.

09:25.400 --> 09:27.000
I guess we'll find out.

09:27.000 --> 09:28.040
Yeah.

09:28.040 --> 09:31.320
Do you think that these models could,

09:31.320 --> 09:33.800
I mean, they're certainly being used to optimize

09:33.800 --> 09:35.880
themselves and so on and guide the design

09:35.880 --> 09:37.280
and there's all kinds of structures

09:37.280 --> 09:39.240
in which models are being used to,

09:40.240 --> 09:43.280
there's layers and layers and layers of metamodeling.

09:43.280 --> 09:45.720
Do you think that these are kind of getting close

09:45.720 --> 09:47.600
to this kind of recursive self-improvement

09:47.600 --> 09:50.000
of being able to kind of very generally explore

09:50.000 --> 09:53.080
the constraint space to try and solve

09:53.080 --> 09:54.080
like larger scale problems?

09:54.080 --> 09:56.640
Like I'm imagining here some structure

09:56.680 --> 10:01.360
where you have some list of problems

10:01.360 --> 10:03.800
and you have some model sampling between these

10:03.800 --> 10:04.800
and you start with the easy ones

10:04.800 --> 10:07.400
and then you try to train populations of agents

10:07.400 --> 10:10.840
or populations of intelligences to be able to solve these

10:10.840 --> 10:13.960
and then kind of over time just kind of scale up the system.

10:13.960 --> 10:14.880
Do you think that that kind of thing,

10:14.880 --> 10:19.040
it seems to me that it'd be like,

10:19.040 --> 10:21.200
thankfully nobody has really tried this

10:21.200 --> 10:22.840
but it doesn't seem like far away

10:22.840 --> 10:24.640
from something that could be possible.

10:25.640 --> 10:28.880
Yeah, I guess we're seeing all limited versions of AI

10:28.880 --> 10:31.520
being applied to help AI research.

10:31.520 --> 10:33.480
I mean, we have like co-pilot

10:33.480 --> 10:35.960
and general kind of coding assistance.

10:36.840 --> 10:40.000
Of course, you have various forms of hyper parameter

10:40.000 --> 10:41.640
optimization regimes.

10:42.520 --> 10:43.880
There've also been some applications

10:43.880 --> 10:47.920
in the design of hardware where kind of circuit layout

10:47.920 --> 10:52.120
has been done, I think for the TPU4,

10:52.200 --> 10:56.800
I think Google used AI assistance

10:56.800 --> 11:00.440
to kind of optimize the layout of the circuitry.

11:01.600 --> 11:05.240
Data centers, cooling machinery that have been like,

11:05.240 --> 11:06.960
you can kind of shave off some percent

11:06.960 --> 11:10.720
by having that optimized by some RL system.

11:10.720 --> 11:13.200
And so, I think we'll certainly see

11:13.200 --> 11:14.880
more incremental stuff like that.

11:15.880 --> 11:20.880
My guess is that by the time we get

11:22.240 --> 11:23.880
like a really strong feedback loop

11:23.880 --> 11:26.240
where sort of the AI can do the core thing

11:26.240 --> 11:27.360
that researchers are doing,

11:27.360 --> 11:32.360
like the actual identifying the right research questions

11:32.960 --> 11:37.960
and approaches and like that seems quite late in it.

11:38.680 --> 11:40.040
Like when that happens,

11:40.040 --> 11:41.680
we are pretty close to the singularity

11:41.680 --> 11:44.400
or the takeoff or whatever the ramp

11:44.400 --> 11:46.560
or whatever the shape of that will be.

11:49.800 --> 11:51.720
But certainly there's more domain specific

11:51.720 --> 11:55.160
incremental ways of accelerating AI advances.

11:55.160 --> 11:57.040
I think we're seeing some of already

11:57.040 --> 11:58.800
and can expect to see more of.

11:59.680 --> 12:02.120
Speaking about takeoff, do you sort of expect,

12:02.120 --> 12:05.200
or based on what you have seen so far,

12:05.200 --> 12:08.960
do you think we're more on a slow, moderate or fast takeoff?

12:08.960 --> 12:13.400
This is sort of the three options that you thought through.

12:13.400 --> 12:14.240
Yeah.

12:19.600 --> 12:24.600
I mean, I still think that the slow looks less plausible,

12:25.200 --> 12:29.520
meaning decades, say, between when you get something

12:29.520 --> 12:31.360
roughly human level until you get something

12:31.360 --> 12:33.440
that completely leaves us in the dust.

12:33.440 --> 12:38.440
That seemed less likely back when I wrote the book

12:40.000 --> 12:41.840
and still seems less likely today.

12:41.840 --> 12:46.080
I guess we have a little bit more granularity now

12:46.080 --> 12:48.960
in that we have these model systems that work

12:48.960 --> 12:50.800
and you can at least consider these scenarios

12:50.800 --> 12:53.120
where human double AI is achieved

12:53.120 --> 12:56.520
by scaling up current systems or variations of that.

12:56.520 --> 12:59.960
That gives us a little bit more of a concrete picture

12:59.960 --> 13:02.600
of at least one way in which these can develop.

13:05.320 --> 13:08.400
And it's possible that you might then have something

13:08.400 --> 13:13.400
that is really very dependent on compute

13:14.680 --> 13:17.720
and that really you get performance kind of proportional

13:17.720 --> 13:21.920
to the size of the model or the length of the training

13:21.920 --> 13:24.160
and in a relatively smooth way.

13:26.440 --> 13:27.600
So in some of those scenarios,

13:27.600 --> 13:32.600
you might have something that is less than super rapid

13:32.720 --> 13:35.280
because what you will get is something

13:35.280 --> 13:40.280
that costs like a billion dollar to train up one human level AI

13:44.760 --> 13:48.600
and then you might immediately be able to run

13:48.600 --> 13:53.600
multiple of them because it takes a lot more to train up a model

13:54.960 --> 13:56.200
than to run it.

13:56.200 --> 13:57.960
So you might then be able to run like a hundred

13:57.960 --> 13:58.800
or a thousand of them,

13:58.800 --> 14:02.720
but that's still not enough to out compete

14:03.600 --> 14:06.720
on the order of 10 billion humans, right?

14:06.720 --> 14:09.400
So depending on like,

14:09.400 --> 14:11.320
if you really stretch yourself very far

14:11.320 --> 14:14.600
to just barely be able to run a model as big as a human,

14:15.520 --> 14:18.480
it might then take a significant period of time

14:18.480 --> 14:22.080
before you can go many orders of magnitude above that

14:22.080 --> 14:23.960
to sort of get something,

14:23.960 --> 14:26.240
like if you need to scale that up by a factor of a million,

14:26.240 --> 14:29.360
say to go from running like on the order of a thousand humans

14:29.360 --> 14:33.960
to a billion humans getting through six orders of magnitude

14:35.360 --> 14:38.800
when you're already like using a billion dollars

14:38.800 --> 14:41.360
and like a large chunk of your data centers,

14:41.360 --> 14:44.520
like that might just not be an instantaneous process.

14:44.520 --> 14:47.120
So there are some scenarios where this would happen

14:47.980 --> 14:50.120
more on a sort of intermediate time scale.

14:51.280 --> 14:56.280
Now, in some sense, I guess that's like the kind of the

14:56.480 --> 14:58.360
the baseline projection.

14:58.360 --> 15:00.080
Like if you just like extrapolate

15:00.080 --> 15:02.440
the way things currently work,

15:02.440 --> 15:05.440
I don't think we can preclude the possibility

15:05.440 --> 15:07.520
of there being more rapid capability jumps.

15:07.520 --> 15:08.480
Like, hey, of course,

15:08.480 --> 15:11.400
if there is like some missing architectural invention

15:11.400 --> 15:12.240
that we haven't made,

15:12.240 --> 15:14.000
that suddenly makes it click.

15:14.000 --> 15:18.200
But you also have these phenomenon like rocking

15:18.200 --> 15:22.760
where sometimes you have a kind of discrete jump

15:22.760 --> 15:25.660
in some particular type of capability.

15:26.660 --> 15:29.420
Like maybe multi-step reasoning

15:29.420 --> 15:33.220
where if each step has less than X percent chance

15:33.220 --> 15:34.340
of being correct,

15:34.340 --> 15:37.540
then like you get an exponential chance

15:37.540 --> 15:38.980
of reasoning correctly

15:38.980 --> 15:41.700
and you really can't do more than like three or four or five steps.

15:41.700 --> 15:44.340
But maybe once you get it above a certain level

15:44.340 --> 15:47.980
and then maybe you can do some sort of self-correction reasoning

15:47.980 --> 15:52.380
like analogous to like quantum computation protocols

15:52.500 --> 15:55.500
like that, you could also imagine cases

15:55.500 --> 15:57.500
where like things come together

15:57.500 --> 15:59.820
and you suddenly get the specific types of things

15:59.820 --> 16:02.060
that make us humans have the extra oomph

16:02.060 --> 16:04.060
that we have relative to other animals,

16:04.060 --> 16:07.220
like full ability to learn from language

16:07.220 --> 16:10.060
and to reason and plan on that.

16:10.060 --> 16:15.060
So yeah, I wouldn't preclude these more rapid takeoff scenarios

16:15.740 --> 16:18.540
either at all, like...

16:18.540 --> 16:21.740
Yeah, certainly some of the latest developments

16:21.740 --> 16:24.140
and some scaling down some of the models

16:24.140 --> 16:27.540
and getting similar results sort of point to there being

16:27.540 --> 16:30.340
just a lot of inefficiencies in the training process now.

16:30.340 --> 16:33.100
And once you sort of know what you're sort of looking for,

16:33.100 --> 16:35.900
you can kind of a blade away a lot of pieces.

16:35.900 --> 16:37.820
And so something like that could happen

16:37.820 --> 16:40.020
with a general learning algorithm.

16:40.020 --> 16:42.700
Yeah, so certainly now you find like,

16:42.700 --> 16:44.500
yeah, so first you achieve state of the art

16:44.500 --> 16:46.500
and then like six months or 12 months later,

16:46.500 --> 16:48.300
you can achieve the same thing

16:48.300 --> 16:51.060
with maybe 10% of the computer or something.

16:51.060 --> 16:54.500
Now, I would expect a little bit of that to go away.

16:54.500 --> 16:58.140
As these systems become bigger and more expensive,

16:58.140 --> 17:02.980
you might imagine more of the easy gains

17:02.980 --> 17:04.900
to be made earlier on.

17:04.900 --> 17:08.740
Like if you really have a lot of smart humans

17:08.740 --> 17:12.020
working really hard on building a system,

17:12.020 --> 17:14.620
you might have plucked more of the low-hanging fruits

17:14.620 --> 17:18.100
than if it were like a two-person postdoc team

17:18.100 --> 17:19.700
that were working for a few weeks.

17:19.700 --> 17:22.020
Chances are that will be big, easy additional things

17:22.020 --> 17:23.620
you could do to improve that system already.

17:23.620 --> 17:25.780
But if you're spending many billions of these,

17:25.780 --> 17:28.620
like you're gonna look quite hard

17:28.620 --> 17:31.900
if there are ways to sort of speed up the training process

17:31.900 --> 17:34.180
so you could like save a hundred million.

17:37.580 --> 17:42.700
Are you hopeful that restricting hardware development

17:42.700 --> 17:44.980
or use is a promising path?

17:44.980 --> 17:46.580
I mean, semiconductor manufacturing

17:46.580 --> 17:47.620
is extremely difficult,

17:47.620 --> 17:50.460
but more and more companies are sort of forced to do it

17:50.460 --> 17:53.460
because of kind of hitting the barriers

17:53.460 --> 17:56.300
with just the size of the systems

17:56.300 --> 17:59.460
and then needing to do special applications

17:59.460 --> 18:00.380
and special purpose things.

18:00.380 --> 18:02.180
And many more companies are now developing

18:02.180 --> 18:04.740
their own chips and so on.

18:04.740 --> 18:08.180
So are kind of like hardware restrictions viable here

18:08.180 --> 18:12.740
or is that a pathway that's just unlikely to work?

18:13.580 --> 18:17.420
Yeah, so a lot of people can like design their own chips,

18:17.420 --> 18:22.340
but only a few actors can actually build them.

18:25.340 --> 18:30.460
And then there are some other choke points further upstream

18:30.460 --> 18:33.620
in terms of making the equipment for the factories

18:33.620 --> 18:34.540
that build the chips

18:34.540 --> 18:36.820
where currently to make cutting edge chips,

18:36.820 --> 18:40.420
there's like ASML, which is a single node.

18:41.260 --> 18:46.260
And indeed, we do see like, I mean with these recent moves

18:50.020 --> 18:55.020
by the US to restrict exports of cutting edge chips to China

18:56.140 --> 18:57.860
and quite comprehensive,

18:57.860 --> 18:59.460
also not to sell the equipment,

18:59.460 --> 19:01.860
also not to allow American persons

19:01.860 --> 19:04.220
to work for these companies.

19:04.900 --> 19:09.900
I don't know what fraction of the motivation for this

19:10.820 --> 19:13.620
is like AI specifically versus more generally

19:13.620 --> 19:15.500
a sense of this being a high tech area

19:15.500 --> 19:18.460
that's gonna be key to national competitiveness.

19:22.180 --> 19:24.500
Yeah, I don't think it's out of the question

19:24.500 --> 19:27.740
that I mean, compared to the alternative,

19:27.740 --> 19:32.020
which would be like to like restrict access to ideas

19:32.020 --> 19:33.180
and algorithms and stuff.

19:33.340 --> 19:35.580
I mean, that might work for a short period of time,

19:35.580 --> 19:40.580
but independent discovery means it's like, yeah,

19:42.580 --> 19:45.100
at most a short term stop cap measure,

19:45.100 --> 19:47.060
whereas the hardware would like take a lot longer

19:47.060 --> 19:49.500
if you needed to build up like the whole supply chain

19:49.500 --> 19:53.660
on your own, like that would be a multi decade project, right?

19:54.620 --> 19:59.620
Now that said, I think what I would favor would be for there

20:00.060 --> 20:05.060
to be the ability at the critical time to go slow,

20:05.140 --> 20:08.020
to have a short pause, maybe to check systems

20:08.020 --> 20:13.020
and to avoid the most cutthroat type of tech race

20:15.500 --> 20:17.380
to just launch as quickly as possible

20:17.380 --> 20:21.060
because you get scooped if you take even an extra week to,

20:21.060 --> 20:23.100
like I think that would be bad now.

20:24.460 --> 20:26.740
So having enough coordination or control

20:26.740 --> 20:30.180
to be able to go at the moderate pace

20:30.180 --> 20:33.820
when you sort of reach approach human level would be good.

20:33.820 --> 20:36.900
I wouldn't want to stop the development

20:36.900 --> 20:41.540
of advanced machine intelligence permanently

20:41.540 --> 20:45.260
or even like have a very long pause either.

20:45.260 --> 20:48.380
I think that brings its own negatives.

20:50.660 --> 20:52.980
And I think some of these attempts

20:52.980 --> 20:55.820
to restrict the chip supply also have

20:57.660 --> 21:00.460
the side effect of creating more adversarial dynamic.

21:00.460 --> 21:04.860
I think it would be really nice if we could have a world

21:04.860 --> 21:07.660
where the leading powers were more on the same page

21:07.660 --> 21:10.500
or friendly or at least had constructive

21:10.500 --> 21:11.620
cooperative relationship.

21:11.620 --> 21:16.620
I think a lot of the ex-risk pie in general

21:17.820 --> 21:19.740
and the risk from AI in particular

21:22.620 --> 21:26.620
arises from the possibility of conflict of different kind.

21:26.660 --> 21:29.420
And so a world order that was more cooperative

21:31.380 --> 21:36.260
would look more promising for the future

21:36.260 --> 21:37.100
in many different ways.

21:37.100 --> 21:38.340
So I'm a little worried about,

21:38.340 --> 21:41.660
especially kind of more unilateral list move

21:41.660 --> 21:43.540
to kind of kneecap the competitor

21:43.540 --> 21:44.780
and to be playing nasty.

21:44.780 --> 21:48.860
Like I feel that, yeah, I'm very uneasy about that.

21:51.260 --> 21:53.060
It sounds, well, so if the hardware,

21:53.820 --> 21:57.780
if ideas or hardware will only buy a certain amount of time,

21:57.780 --> 22:00.460
then really AI alignment is the best path forward

22:00.460 --> 22:03.460
and very much agree that we don't want to restrict

22:03.460 --> 22:06.140
the creation of digital intelligence

22:07.020 --> 22:12.020
and that that's sort of the next evolutionary jumps.

22:12.260 --> 22:13.220
And there's some questions there

22:13.220 --> 22:14.780
around kind of like which paths we would take

22:14.780 --> 22:17.940
and how do we develop brain computer interfaces

22:17.940 --> 22:20.620
and cold-burn emulation and so on.

22:21.460 --> 22:24.300
We're kind of like, even before getting into that,

22:24.300 --> 22:26.220
how hopeful are you that we might solve

22:26.220 --> 22:27.780
the AI alignment problem?

22:32.220 --> 22:34.540
Moderately, I guess I'm quite agnostic,

22:35.740 --> 22:39.580
but I think the main uncertainty is how hard the problem

22:39.580 --> 22:41.580
turns out to be.

22:41.580 --> 22:43.700
And then there's a little extra uncertainty

22:43.700 --> 22:46.940
as to how the degree to which we get our act together.

22:47.780 --> 22:50.780
But I think like out of those two variables,

22:50.780 --> 22:55.020
like the realistic scenarios in which we either

22:55.020 --> 22:59.740
like are lazy and don't like focus on it

22:59.740 --> 23:01.900
versus the ones where we get a lot of smart people

23:01.900 --> 23:02.740
working on it.

23:02.740 --> 23:03.740
So there's some uncertainty there

23:03.740 --> 23:05.020
that affects the success chance,

23:05.020 --> 23:07.420
but I think that's dwarfed by our uncertainty

23:07.420 --> 23:11.500
about how intrinsically hard the problem is to solve.

23:11.500 --> 23:16.500
So you could say that like the most important component

23:16.900 --> 23:18.540
of our strategy should be to hope

23:18.540 --> 23:21.180
that the problem is not too hard.

23:21.180 --> 23:23.660
Yeah, so let's try to tackle it.

23:23.660 --> 23:26.820
So how do you, as you thought about this problem,

23:26.820 --> 23:30.060
have you kind of been able to break it down

23:30.060 --> 23:34.260
into components and parts or maybe evolved your thinking

23:34.260 --> 23:35.460
of the shape of the problem?

23:35.460 --> 23:37.060
Like what are you thinking now?

23:39.100 --> 23:43.220
Well, I think the field as a whole has made

23:43.220 --> 23:45.900
significant advances and developed a lot

23:45.940 --> 23:48.940
since when I was writing the book

23:48.940 --> 23:50.940
where it was like really a non-existent field.

23:50.940 --> 23:53.340
There were a few people on the internet here and there,

23:53.340 --> 23:55.700
but now it's an active research field

23:55.700 --> 23:58.260
with a growing number of smart people

23:58.260 --> 24:01.820
who have been working full-time on this for a number of years

24:01.820 --> 24:04.660
and writing papers that build on previous papers

24:04.660 --> 24:06.220
with technical stuff.

24:06.220 --> 24:11.220
And all the key AI labs have now some contingent

24:11.820 --> 24:14.180
of people who are working on alignment.

24:14.180 --> 24:17.060
DeepMind has, OpenAI has, Anthropic has.

24:17.980 --> 24:19.140
So that's all good.

24:19.140 --> 24:20.780
Now, within this community,

24:20.780 --> 24:24.820
there is, I guess, a distribution of levels of optimism

24:24.820 --> 24:28.420
ranging from people very pessimistic,

24:28.420 --> 24:30.860
like Elias Yudkowski, for example.

24:30.860 --> 24:34.700
And I guess there are people even more pessimistic than him,

24:34.700 --> 24:37.180
but he's kind of at one end

24:37.180 --> 24:42.180
and towards people with more moderate levels of optimism,

24:43.180 --> 24:44.460
like Paul Cristiano,

24:44.460 --> 24:47.780
and then others who think it's kind of,

24:50.300 --> 24:52.660
it's something that we'll deal with it when we get to it

24:52.660 --> 24:56.020
and who don't seem too fast about it.

24:57.020 --> 24:59.780
I think there's a lot of uncertainty

24:59.780 --> 25:02.020
on the hardness level.

25:02.020 --> 25:04.060
Now, as far as how you break it down, yeah.

25:04.060 --> 25:06.060
So there are different ways of doing this.

25:07.700 --> 25:09.180
There's not yet one paradigm

25:09.180 --> 25:13.220
that all competent AI safety researchers share

25:13.220 --> 25:15.580
in terms of the best lens to look at this.

25:15.580 --> 25:17.740
So it decomposes in slightly different ways,

25:17.740 --> 25:20.860
depending on your angle of approach,

25:20.860 --> 25:24.100
but certainly one can identify different facets

25:24.100 --> 25:24.940
that one can work on.

25:24.940 --> 25:27.660
So for example, interpretability tools

25:27.660 --> 25:29.860
seem on many different approaches,

25:29.860 --> 25:32.340
like a useful ingredients to have,

25:32.340 --> 25:35.300
like basically insights or techniques

25:35.300 --> 25:38.500
that allow us better to see what is going on

25:38.500 --> 25:40.180
in a big neural network.

25:42.820 --> 25:47.820
You could have one approach where you try to get AI systems

25:48.100 --> 25:53.100
that try to learn to match some human example of behavior,

26:01.340 --> 26:04.460
either like one human or some corpus of humans

26:04.460 --> 26:08.460
and then tries to just perform a next action

26:08.460 --> 26:10.820
that's like the same as its best guess

26:10.820 --> 26:12.580
about what this reference human

26:12.580 --> 26:14.260
would do in the same situation.

26:17.100 --> 26:22.100
And then you could try to do forms of amplification on that.

26:22.100 --> 26:26.980
So like if you could like faithfully model one human,

26:26.980 --> 26:29.380
well, then you just get like a human level,

26:29.380 --> 26:31.180
like intelligence, you might want to go beyond that.

26:31.180 --> 26:34.100
But if you could then create many of these models

26:34.100 --> 26:35.700
that each do what the human do,

26:35.700 --> 26:38.620
can you put them together in some bureaucracy

26:38.620 --> 26:41.660
or do some other clever bootstrapping or self-criticism?

26:45.260 --> 26:48.860
So that would be one approach.

26:48.860 --> 26:53.860
You could, yeah, you could try to use

26:57.620 --> 26:59.060
sort of inverse reinforcement learning

26:59.060 --> 27:01.580
to infer like a human's preference function

27:01.580 --> 27:04.300
and then try to optimize for that

27:04.300 --> 27:05.940
or maybe not strictly optimized

27:05.940 --> 27:08.300
but doing some kind of softer optimization.

27:12.620 --> 27:14.500
Yeah, there are a bunch of different ideas.

27:14.500 --> 27:17.460
Like some safety work is more like trying to more precisely

27:17.460 --> 27:20.900
understand and illustrate in toy examples

27:20.900 --> 27:22.060
how things could go wrong

27:22.060 --> 27:23.740
because that's like often the first step

27:23.740 --> 27:26.620
to creating a solution is to really deeply understand

27:26.620 --> 27:28.940
what the problem is and then illustrate it.

27:28.940 --> 27:33.540
And yeah, that can be useful as well.

27:35.140 --> 27:38.580
It's interesting now that we have these models

27:38.580 --> 27:41.620
that can talk as it were or like use language

27:41.620 --> 27:46.620
that kind of opens up an additional interface,

27:46.780 --> 27:48.940
like an additional way of interacting with these systems

27:48.940 --> 27:50.340
and trying out different things

27:53.340 --> 27:57.380
and a different way of illustrating the awkwardness.

27:57.380 --> 27:59.660
Like the idea of prompt engineering

27:59.660 --> 28:02.740
when you're trying to get an AI to do something

28:02.740 --> 28:04.100
and you're trying to figure out

28:04.100 --> 28:05.460
exactly the right formulation.

28:05.460 --> 28:08.340
Like that shows that we are not quite where we need to be

28:08.340 --> 28:11.700
in terms of directing the intrinsic capability

28:11.700 --> 28:12.900
of these large language models.

28:12.900 --> 28:17.260
So it's in there and yet we can't always even elicit it

28:17.260 --> 28:19.460
because you have to find exactly the right wording

28:19.460 --> 28:20.860
and then suddenly turns out this thing

28:20.860 --> 28:23.580
is actually perfectly capable of doing something

28:23.580 --> 28:26.540
which initially it seemed it failed that.

28:26.540 --> 28:28.500
So getting better at that

28:28.500 --> 28:30.820
or coming up with something better than prompt engineering

28:30.820 --> 28:33.380
like would be good.

28:33.380 --> 28:38.380
I'm kind of, I have some sympathy for an approach

28:41.820 --> 28:45.660
which I think has not been explored very much yet

28:45.660 --> 28:48.900
but partly because it's hard to explore it

28:48.900 --> 28:50.940
until the technology reaches a certain level

28:50.940 --> 28:54.900
of sophistication, which is the idea that as you get

28:54.900 --> 28:58.380
systems that become closer to human level

28:58.380 --> 28:59.980
in that conceptual ability.

29:00.020 --> 29:04.500
And that might then internally start to develop concepts

29:04.500 --> 29:07.260
that are more similar to human concepts

29:07.260 --> 29:11.100
including not just concepts about simple visual features

29:11.100 --> 29:12.980
and stuff, but more corresponding

29:12.980 --> 29:16.140
to our higher language concepts.

29:16.140 --> 29:19.500
Like our concept of a preference or a goal

29:19.500 --> 29:23.340
or a request or being safe, being reckless

29:23.340 --> 29:24.500
like these types of concepts.

29:24.500 --> 29:26.980
Like we humans seem relatively robustly

29:26.980 --> 29:29.260
to be able to master these concepts

29:29.260 --> 29:31.220
in the course of our normal development.

29:32.740 --> 29:35.860
Despite us having starting with different brains

29:35.860 --> 29:39.300
and having different environmental input and noise.

29:39.300 --> 29:42.260
And so maybe there is a relatively robust

29:42.260 --> 29:44.300
and convergent ways in which some of these concepts

29:44.300 --> 29:45.780
could be grasped.

29:45.780 --> 29:50.780
Then the hope would be that you could kind of train up an AI

29:51.740 --> 29:53.420
that doesn't need to be above human

29:53.420 --> 29:55.820
and maybe hardly even human

29:55.820 --> 29:58.080
that would then sort of internally form these concepts

29:58.080 --> 29:59.320
in the same way that we form them.

29:59.320 --> 30:01.400
And then once those concepts are in there,

30:01.400 --> 30:03.720
you might then be able to use those as building blocks

30:03.720 --> 30:05.640
to create a kind of alignment

30:06.640 --> 30:09.720
by sort of linking motivation to these concepts.

30:09.720 --> 30:11.880
It very hand-wavered, but I think something

30:11.880 --> 30:14.880
in that direction is one interesting approach

30:14.880 --> 30:16.600
to the alignment problem as well.

30:16.600 --> 30:17.960
Do you think there's some promise

30:17.960 --> 30:22.600
in trying to evolve a notion of morality and ethics?

30:22.600 --> 30:25.360
Meaning using simulations of environments

30:25.360 --> 30:29.160
where agents might learn to cooperate

30:29.160 --> 30:32.200
and over time learn D, put them through the same kind

30:32.200 --> 30:34.600
of game theory dynamics that gave rise

30:34.600 --> 30:39.280
to our own notions of symbiotes and ethics and so on.

30:40.360 --> 30:41.200
Potentially, yeah.

30:41.200 --> 30:44.400
I mean, I think you would wanna be looking very closely

30:44.400 --> 30:46.640
at exactly how you set things up

30:46.640 --> 30:48.000
and the dynamics that unfold.

30:48.000 --> 30:51.160
I mean, real revolution is sort of read in tooth and claw

30:51.160 --> 30:53.560
and can create wonderful cooperation,

30:53.600 --> 30:58.600
but also hostility and defection and manipulation

30:58.880 --> 31:00.080
and all kinds of things.

31:03.960 --> 31:08.000
But yes, certainly multi-agent systems

31:08.920 --> 31:12.080
with the right kind of incentive structures in place

31:12.080 --> 31:12.960
so that you evolve.

31:12.960 --> 31:16.520
Like evolution itself can produce many different kinds

31:16.520 --> 31:20.480
of outcomes, like depending on the environment,

31:21.480 --> 31:25.760
but that certainly could be come in some scenarios

31:25.760 --> 31:26.880
and increasingly important,

31:26.880 --> 31:28.920
like either whether it's an evolutionary system

31:28.920 --> 31:32.360
or in some of these other like a training environment,

31:32.360 --> 31:36.120
like the curriculum, like if these systems are shaped

31:36.120 --> 31:39.440
a lot by their data that they're trained on,

31:41.320 --> 31:43.200
so far we've just kind of slapped together

31:43.200 --> 31:47.160
some big data sets and not really fast too much

31:47.160 --> 31:48.800
about what's contained in it,

31:48.800 --> 31:51.800
but that might become an important component as well

31:51.800 --> 31:54.040
of alignment in certain of these scenarios.

31:56.000 --> 32:00.200
And are these directions the ones you find most promising

32:00.200 --> 32:02.040
or is there like a subset of these

32:02.040 --> 32:04.800
or maybe another one that you've been thinking about

32:05.800 --> 32:07.600
starting to kind of surface

32:07.600 --> 32:09.920
and help a lot of people that are working on this

32:09.920 --> 32:11.240
so likely watch this conversation.

32:11.240 --> 32:13.840
So are there any kind of pointers

32:13.840 --> 32:15.560
that you might give beyond these?

32:16.560 --> 32:21.560
Well, this would be some of the ones

32:22.560 --> 32:27.400
that I would like highlight somewhat arbitrarily,

32:27.400 --> 32:32.400
but yeah, I think like the Paul Cristiano capability

32:34.480 --> 32:37.360
amplification, the interpretability work,

32:40.360 --> 32:45.360
the idea of like growing human beings

32:45.560 --> 32:49.680
human level concepts and then using those as a basis

32:49.680 --> 32:54.680
to define goals or to sort of create the motivation system

32:55.000 --> 32:56.760
that uses those as primitives.

32:59.800 --> 33:04.800
It might also well be that there are entirely different

33:05.360 --> 33:06.880
conceptual ways of approaching this

33:06.880 --> 33:09.080
that are yet to be discovered.

33:11.240 --> 33:14.160
It's not a mature research field where we have,

33:14.160 --> 33:17.280
as I said, like we don't have an established paradigm

33:17.280 --> 33:19.880
that's clearly correct and that we now just need to,

33:19.880 --> 33:21.360
I think there are multiple paradigms

33:21.360 --> 33:22.880
and there might well be additional ones

33:22.880 --> 33:25.240
that just haven't had a champion yet

33:25.240 --> 33:27.880
to sort of really get people to take it seriously.

33:31.960 --> 33:33.800
So I think there is also a value

33:33.800 --> 33:36.520
to this more theoretical, a conceptual,

33:36.520 --> 33:39.760
almost philosophical exploratory work in just,

33:40.800 --> 33:43.640
yeah, coming at the problem from a different angle.

33:45.040 --> 33:49.160
Yeah, jumping into maybe agent-ness,

33:49.160 --> 33:54.160
how separable do you think agency is from the intelligence

33:54.320 --> 33:56.720
in the approaches that we're taking

33:56.720 --> 33:58.000
or maybe more generally?

34:04.520 --> 34:09.520
Yeah, like I guess then we would have to go in

34:10.000 --> 34:11.920
so like exactly how you define agent,

34:12.000 --> 34:16.800
so which is in itself like a non-trivial question

34:16.800 --> 34:19.040
that, and it might even be that

34:19.040 --> 34:21.760
getting really clear on that itself

34:21.760 --> 34:25.440
would be an important advance in AI alignment.

34:27.320 --> 34:31.320
I mean, you could kind of roughly define it

34:31.320 --> 34:35.360
as kind of like behavior well-modeled

34:35.360 --> 34:38.840
as being in the intelligent pursuit of goals

34:38.840 --> 34:39.760
or something like that,

34:39.840 --> 34:42.680
or you have goals on the world model

34:42.680 --> 34:44.240
and you select different plans

34:44.240 --> 34:46.360
based on your expectation of how that,

34:51.240 --> 34:55.440
it, yeah, it seems like you can get

34:55.440 --> 34:59.960
the significant performance in many domains

34:59.960 --> 35:04.960
without having like an explicit

35:05.960 --> 35:08.080
agentic goal-seeking process,

35:08.080 --> 35:10.600
but that might nevertheless result in performance

35:10.600 --> 35:11.880
that is agent-like.

35:11.880 --> 35:15.680
So I'm thinking like you can get, for example,

35:15.680 --> 35:19.280
quite high-level goal-playing

35:19.280 --> 35:24.280
by just kind of pattern-matching what a human expert would do,

35:26.240 --> 35:31.240
but without any Monte Carlo rollouts, for example.

35:32.240 --> 35:37.240
So in one sense, you don't have a component in those systems

35:37.720 --> 35:41.240
that would normally be associated with like planning.

35:41.240 --> 35:43.840
On the other hand, if it actually plays like a human,

35:43.840 --> 35:46.400
and if that human achieved that level of play

35:46.400 --> 35:48.840
by selecting moves based on some plan

35:48.840 --> 35:49.960
as to what they would achieve,

35:49.960 --> 35:53.000
there is a kind of an implicit sense

35:53.000 --> 35:57.240
in which the system is pursuing long-term goals and planning.

35:58.160 --> 36:03.040
And so it gets, I think, yeah, a little bit murky sometimes

36:03.040 --> 36:06.400
when you like actually dig into it, the degree,

36:06.400 --> 36:08.600
or there might be different senses of being agentic

36:08.600 --> 36:12.400
or different senses of doing planning and goal pursuing

36:12.400 --> 36:14.600
which might have different safety properties.

36:16.520 --> 36:19.200
Those types of questions I think are interesting

36:19.200 --> 36:22.360
and like can contribute to alignment

36:22.360 --> 36:25.880
and other questions of that sort

36:25.880 --> 36:29.840
where we notice that we're a little bit conceptually confused

36:29.840 --> 36:31.440
or we take some concept for granted,

36:31.440 --> 36:35.360
but once you actually try to dig down and make it precise,

36:35.360 --> 36:37.960
you realize that you haven't made up your mind

36:37.960 --> 36:40.120
about which sense you were using a term,

36:40.120 --> 36:43.440
and then if you keep digging on that,

36:43.440 --> 36:46.920
sometimes you then get like new ways of looking at the problem

36:46.920 --> 36:51.600
that makes you see new opportunities for making progress.

36:52.680 --> 36:55.120
It seems right now that a number of teams

36:55.120 --> 36:57.680
are hoping to be able to separate out

36:57.680 --> 36:59.840
some kind of planning agent where,

36:59.840 --> 37:02.520
or not an agent, but some kind of planner intelligence

37:02.520 --> 37:05.480
that whose job is just to come up with a plan

37:05.480 --> 37:06.680
and then maybe later you feed it

37:06.680 --> 37:09.200
to some kind of execution system.

37:11.880 --> 37:13.960
If, you know, suppose that we were able to do that

37:13.960 --> 37:15.640
and suppose that we have these planners

37:15.640 --> 37:17.080
that are generally intelligent

37:17.080 --> 37:18.840
and potentially super intelligent,

37:20.080 --> 37:23.080
it seems like that is potentially riskier

37:25.200 --> 37:29.040
in some ways, which ones do you think are,

37:29.040 --> 37:31.760
which of these do you think is potentially more problematic,

37:31.760 --> 37:34.160
a super intelligence that is strictly a planner

37:34.160 --> 37:37.000
that then we have to worry about how to coordinate

37:37.000 --> 37:40.280
and orient humans to not misuse these things

37:40.280 --> 37:42.840
and not gain the level of power and control

37:42.840 --> 37:44.400
that something like that would give,

37:44.400 --> 37:49.400
or hey, we actually figure out how to build an agent

37:50.400 --> 37:53.480
and we can be reasonably closely certain

37:55.160 --> 37:57.800
that it might get alignment right

37:57.800 --> 38:00.280
and just go straight towards agency

38:00.280 --> 38:03.560
where that agent would not actually be sort of exploitable

38:03.560 --> 38:05.840
by whoever is controlling the prompt.

38:07.200 --> 38:08.240
Yeah, I don't know.

38:08.240 --> 38:13.240
I mean, I think just at an intuitive level,

38:14.000 --> 38:17.600
I guess it feels like there is some additional risk

38:17.600 --> 38:21.800
in having a planning agent that saw deep into the future

38:21.800 --> 38:23.920
and it had like wearability to optimize

38:23.920 --> 38:26.160
some long-term strategy based on some goal

38:26.160 --> 38:29.680
versus things that more just try to imitate

38:29.680 --> 38:34.680
like a human, let's say, and then repeat

38:34.680 --> 38:37.120
or that had a very sort of short-time horizon

38:37.120 --> 38:41.360
and just try to select something

38:41.360 --> 38:44.760
based on parallel considerations.

38:44.760 --> 38:49.760
At an intuitive level, the myopic agents,

38:50.600 --> 38:54.840
the non-planning agent, the imitating seem kind of maybe safer,

38:54.840 --> 38:59.840
but I don't think we can confidently say that it is

39:00.040 --> 39:05.040
until we have more deeply understood the situation here

39:05.760 --> 39:10.760
and it's the kind of question where current smart AI safety

39:11.040 --> 39:13.400
researchers could have different views

39:13.400 --> 39:17.560
and it's like not resolved in a consensus way yet.

39:17.560 --> 39:19.760
So, I mean, my view is we should explore

39:19.760 --> 39:21.120
all of these different avenues

39:21.120 --> 39:23.480
and there should be different champions of different avenues

39:23.480 --> 39:24.880
who kind of believe in their thing

39:24.880 --> 39:26.880
and who have some people working with them,

39:26.880 --> 39:29.440
but then there should be multiple such clusters

39:30.600 --> 39:32.800
in the world today and it would be premature

39:32.800 --> 39:34.940
to kind of narrow it down.

39:34.940 --> 39:39.940
And even if we just look at the past five, 10 years,

39:40.940 --> 39:43.700
I still feel that one could easily see

39:43.700 --> 39:48.780
that if it hadn't been that one particular way

39:48.780 --> 39:51.500
of looking at this problem had happens to have

39:51.500 --> 39:55.900
an articulate champion to sort of advocate for it

39:55.900 --> 39:58.260
and to keep bring up that perspective,

39:58.260 --> 40:01.580
it would not have featured and it's like somewhat contingent,

40:01.580 --> 40:05.660
which in the pool of vaguely articulated ideas

40:05.660 --> 40:07.740
that have occurred on some mailings at some point,

40:07.740 --> 40:10.180
like which of those is now regarded as like

40:10.180 --> 40:13.060
as a serious paradigm or approach,

40:13.060 --> 40:16.500
it seems to be quite significantly dependent

40:16.500 --> 40:18.740
on the happen to have been one particularly smart person

40:18.740 --> 40:20.860
who decided to really get behind it.

40:21.900 --> 40:26.900
So, just on a principle of induction there,

40:27.940 --> 40:29.820
like that might well be more of these ideas

40:29.820 --> 40:30.740
that have the potential,

40:30.740 --> 40:33.180
like if you have a smart articulate person

40:33.180 --> 40:35.700
who decides to really kind of champion it

40:35.780 --> 40:38.620
and try to write papers and reply to objections

40:38.620 --> 40:40.940
and get some other people to work with them,

40:40.940 --> 40:43.060
that might have kind of as much juice

40:43.060 --> 40:47.020
as some of the current approaches that already exist.

40:49.400 --> 40:50.240
Thank you.

40:50.240 --> 40:52.420
I think that are likely very useful to a few folks.

40:52.420 --> 40:55.600
Jumping into singletons and multiple worlds,

40:55.600 --> 40:56.900
let's start by distinguishing these.

40:56.900 --> 40:57.900
What is a singleton?

41:00.340 --> 41:03.340
To me, it's like this abstract concept of a world order

41:03.340 --> 41:05.980
where at the highest level of decision-making,

41:05.980 --> 41:10.180
there's no coordination failure.

41:10.180 --> 41:12.980
There's like a kind of single agency at the top level.

41:12.980 --> 41:15.300
So, these could be good or bad

41:15.300 --> 41:17.420
and they could be instantiated in many ways.

41:17.420 --> 41:21.380
On earth, you could imagine a kind of super UN,

41:21.380 --> 41:23.300
you could imagine like a world dictator

41:23.300 --> 41:24.560
who conquered everything.

41:24.560 --> 41:27.860
You could imagine like a super intelligence that took over.

41:28.700 --> 41:31.340
You might also be able to imagine something less

41:31.340 --> 41:35.220
formally structured like a kind of global moral code

41:35.220 --> 41:37.660
that is sufficiently homogeneous

41:37.660 --> 41:42.420
and that is self-enforcing and maybe other things as well.

41:43.780 --> 41:46.260
So, you have like, yeah, at a very abstract level,

41:46.260 --> 41:49.420
you could distinguish the future scenarios

41:49.420 --> 41:50.780
where you end up with a singleton

41:50.780 --> 41:53.220
versus ones that remain multipolar.

41:53.220 --> 41:56.100
And you get different dynamics in the multipolar case

41:56.100 --> 41:59.380
that you avoid in the singleton case.

41:59.380 --> 42:01.260
These kind of competitive dynamics.

42:02.580 --> 42:04.700
Which one of these potential futures

42:04.700 --> 42:07.060
do you think is more likely at the moment?

42:09.300 --> 42:11.700
And I mean, I think all things considered,

42:11.700 --> 42:15.660
the singleton outcome in the longer term

42:15.660 --> 42:17.780
seems probably more likely,

42:17.780 --> 42:20.300
at least if we are confining ourselves

42:20.300 --> 42:23.620
to earth-originating intelligent life.

42:24.580 --> 42:29.580
And there are different ways in which it could arise

42:32.540 --> 42:36.140
from more kind of slow historical conventional type

42:36.140 --> 42:40.980
of processes where we do observe from 10,000 years ago,

42:40.980 --> 42:43.340
when the highest unit of political organization

42:43.340 --> 42:48.340
were bands of hunter-gatherers, 50 or 100 people,

42:48.420 --> 42:51.220
then subsequently to sort of chiefdoms,

42:51.220 --> 42:53.620
city-states, nation-states,

42:53.620 --> 42:58.100
and more recently, larger entities like the EU

42:58.100 --> 43:00.420
or weak forms of global governance.

43:04.500 --> 43:07.620
You could argue that in the last 10, 15 years,

43:07.620 --> 43:11.220
we've kind of seen some retreat from that

43:11.220 --> 43:12.780
to a more multipolar world,

43:12.780 --> 43:14.660
but that's a very short period of time

43:14.660 --> 43:16.940
in these historical schemes.

43:16.940 --> 43:18.940
So, there's still like this overall trend line.

43:18.940 --> 43:19.900
So, that might be one,

43:19.900 --> 43:22.540
like another would be these take AI scenarios,

43:22.540 --> 43:26.500
like if either the AI itself or the country

43:26.500 --> 43:29.940
or group that builds it comes to singleton.

43:31.300 --> 43:32.700
You could also imagine scenarios

43:32.700 --> 43:35.580
where you have multiple entities

43:35.580 --> 43:36.940
going through some AI transition,

43:36.940 --> 43:40.180
but then subsequently managed to coordinate,

43:40.180 --> 43:43.420
and then would have new tools for implementing.

43:43.420 --> 43:45.220
If they come to an agreement right now,

43:45.220 --> 43:46.460
it's kind of hard anyway to like,

43:46.460 --> 43:49.220
how do you set up like concretely

43:49.220 --> 43:51.780
in a way that binds everybody that you could trust

43:51.780 --> 43:55.100
that will not get corrupted or develop its own agenda,

43:55.100 --> 43:57.700
like the bureaucrats become it's like,

43:57.700 --> 43:59.180
say if you had new tools to do those,

43:59.180 --> 44:01.260
it's also possible that subsequently

44:01.260 --> 44:05.540
that there might be this kind of merging into a single entity.

44:07.340 --> 44:10.700
Yeah, so all of those different avenues would point,

44:10.700 --> 44:13.420
but it's not a certainty, but if I had to guess,

44:13.420 --> 44:16.380
I would think it's more likely than the multipolar.

44:17.340 --> 44:20.060
And you think it's more likely,

44:20.060 --> 44:21.460
I'm guessing because of physics,

44:21.460 --> 44:22.780
like just latency and distance.

44:22.780 --> 44:25.580
So in a tightly packed volume,

44:25.580 --> 44:27.780
you can compute a lot faster and so on,

44:27.780 --> 44:30.260
and maybe jumping through interstellar distances

44:30.260 --> 44:33.020
might yield different parties,

44:33.020 --> 44:36.300
or is it some other pressures?

44:36.300 --> 44:38.300
Yeah, so not that so much.

44:38.300 --> 44:40.900
I figure that you could,

44:40.900 --> 44:43.900
I mean, in fact, if you don't have a,

44:43.900 --> 44:48.900
like a space colonization pace, eventually,

44:49.180 --> 44:50.780
there would be these long latencies,

44:50.780 --> 44:54.500
and you would need to have different separate computing

44:54.500 --> 44:55.780
systems in different places.

44:55.780 --> 44:56.980
I mean, we already have that today,

44:56.980 --> 45:01.500
like you don't just have one data center on Earth,

45:01.500 --> 45:04.540
like you need to have, you know,

45:04.540 --> 45:06.540
ones closer to the customers and,

45:07.980 --> 45:10.580
but I think if with a single palm,

45:11.580 --> 45:13.420
at technological maturity,

45:13.420 --> 45:16.420
you could have these multiple different components

45:16.420 --> 45:18.580
of the singleton that would nevertheless be coordinated

45:18.580 --> 45:19.940
in terms of their goal,

45:19.940 --> 45:22.340
that would all be working towards the same end.

45:24.340 --> 45:26.500
And presumably because they can lock in

45:26.500 --> 45:28.980
some kind of alignment to itself,

45:28.980 --> 45:31.140
and that wouldn't vary over time.

45:31.140 --> 45:34.120
I mean, like once you jump into interstellar distances,

45:34.120 --> 45:36.100
the computing power of like just one of these

45:36.100 --> 45:37.860
within one stellar system,

45:37.860 --> 45:39.720
by the time you get a round trip,

45:39.720 --> 45:44.360
eons have passed and many simulations of many lifetimes.

45:44.360 --> 45:48.560
Yeah, so if they start off like they get set out,

45:48.560 --> 45:50.680
having the same goals,

45:50.680 --> 45:53.320
and then they have the ability to preserve their goals,

45:53.320 --> 45:55.400
and not to have them randomly corrupted,

45:55.400 --> 45:58.640
be cosmic rays or some weird internal dynamic,

45:58.640 --> 46:00.840
and then they would stay aligned

46:01.800 --> 46:03.360
with each other a billion years later.

46:03.360 --> 46:07.160
Like, so I think that at technological maturity,

46:07.160 --> 46:09.640
there would be techniques for achieving that.

46:10.440 --> 46:15.440
Yeah, yeah, which, when you envision this kind of future,

46:16.680 --> 46:19.760
like to you, what do you think would be like a,

46:19.760 --> 46:23.480
kind of a great or optimistic outcome for humanity,

46:23.480 --> 46:25.520
or for this descendant species

46:25.520 --> 46:27.720
in that level of technological maturity?

46:27.720 --> 46:31.320
Do you sort of see a singleton with,

46:31.320 --> 46:35.880
that sort of ranges the populations of beings within,

46:35.920 --> 46:38.960
or do you think it's some other,

46:38.960 --> 46:40.440
much more singular consciousness,

46:40.440 --> 46:42.400
or how do you envision it?

46:44.880 --> 46:47.720
Yeah, that's a fun question.

46:47.720 --> 46:52.720
So I think it might depend on the time scale

46:58.200 --> 47:00.120
and stuff like that,

47:00.120 --> 47:02.240
that is maybe we wanna start off something

47:02.240 --> 47:07.240
that is more incrementally improving over the status quo,

47:07.360 --> 47:12.360
and maybe after we've been doing that for like a billion years,

47:13.160 --> 47:15.880
like maybe it's time to explore

47:15.880 --> 47:17.560
the more radical possibilities

47:18.600 --> 47:23.600
that involves cathisoning some of our human nature

47:23.960 --> 47:26.200
and individual identity.

47:26.200 --> 47:29.280
So I think my general juristic care

47:29.280 --> 47:31.320
is that the future could be,

47:32.560 --> 47:34.520
it's a very big space of possibilities,

47:34.520 --> 47:39.520
and at least if this kind of default or naive model

47:39.520 --> 47:42.560
of the world where there's like all of these cosmic resources

47:42.560 --> 47:44.360
just waiting there for us to use them,

47:44.360 --> 47:47.640
like there's a huge amount of material to build on,

47:47.640 --> 47:51.960
and that our first instinct when thinking about

47:51.960 --> 47:54.960
how this should be used is a sort of spirit of generosity

47:54.960 --> 47:57.360
and kindness that would be more than enough

47:57.360 --> 48:00.040
for a lot of cool things to happen.

48:00.040 --> 48:03.120
So the first instinct should not be let's pick one

48:03.120 --> 48:05.320
and then put all the chips on that,

48:05.320 --> 48:08.400
but like if one can by many different criteria

48:08.400 --> 48:11.160
do really well, which I think we would be able to.

48:13.280 --> 48:15.840
These different criteria would be like different peoples,

48:15.840 --> 48:17.080
views, different countries views,

48:17.080 --> 48:19.240
different moral systems views,

48:19.240 --> 48:23.280
different of your own values and evaluative tendencies,

48:23.280 --> 48:25.040
like you might be able to just kind of

48:28.360 --> 48:30.440
just check off a lot of boxes very easily

48:30.440 --> 48:32.760
before you have to confront the harder questions,

48:32.760 --> 48:36.640
like thoroughly incompatible things

48:36.640 --> 48:38.400
where you have to choose A or B,

48:38.400 --> 48:42.160
but you just can't do a mixture of them or a superposition.

48:42.160 --> 48:43.760
There might be some of those also,

48:43.760 --> 48:47.080
but I think we would like get to those

48:47.080 --> 48:49.520
after we have picked all the easy wins

48:49.520 --> 48:51.040
of which that would be a great money.

48:52.160 --> 48:53.680
Yeah.

48:53.680 --> 48:55.320
Since we're kind of going into consciousness,

48:55.680 --> 48:58.200
and so you mentioned you've been working on

48:58.200 --> 49:00.240
digital minds with moral status.

49:00.240 --> 49:01.240
Do you want to tell us a bit more,

49:01.240 --> 49:03.280
like what range of digital minds

49:03.280 --> 49:05.520
are you thinking of in these questions?

49:06.880 --> 49:08.600
Well, all really.

49:08.600 --> 49:11.560
I think in a lot of these scenarios,

49:11.560 --> 49:15.080
like the majority of minds in the future

49:17.320 --> 49:18.320
will be digital

49:20.560 --> 49:23.680
and also maybe the biggest minds will be digital.

49:23.680 --> 49:26.480
So in terms of numbers and quality,

49:26.480 --> 49:28.400
like that's where maybe most of the action is.

49:28.400 --> 49:31.920
So it's important what happens to the digital minds.

49:34.600 --> 49:36.000
That's one rationale for it.

49:36.000 --> 49:39.800
And I think you might say,

49:39.800 --> 49:43.440
well, we could deal with that later.

49:43.440 --> 49:45.240
Like we should focus on alignment first,

49:45.240 --> 49:46.840
but I think that it's also possible

49:46.840 --> 49:50.360
that there are path dependencies,

49:50.360 --> 49:54.800
like where you want to start off going in a good direction

49:54.800 --> 49:59.800
and start to cultivate a good set of attitudes

50:00.440 --> 50:03.680
and values and norms and like that,

50:03.680 --> 50:07.640
that you don't start off in this kind of hostile way

50:07.640 --> 50:12.640
where the digital minds are regarded as being completely

50:15.760 --> 50:17.320
insignificant from a moral point of view.

50:17.320 --> 50:19.240
And then hoping that the future will have

50:19.240 --> 50:20.600
appropriate moment switch over.

50:20.600 --> 50:24.200
Like it just feels all things considered,

50:24.200 --> 50:25.920
more likely that we will end up in a good place

50:25.920 --> 50:27.200
if you start early on,

50:27.200 --> 50:30.040
at least to make some small modest gestures

50:30.040 --> 50:30.880
in that direction.

50:30.880 --> 50:35.880
And I think that should start even before we get

50:38.840 --> 50:40.720
to like fully human level minds.

50:40.720 --> 50:44.680
Like if you have like animal level digital minds

50:44.680 --> 50:47.600
and it can be hard exactly to compare a particular AI

50:47.640 --> 50:50.080
to a particular animal because they are different.

50:50.080 --> 50:52.160
But nevertheless, as we get something

50:52.160 --> 50:56.480
that is possibly matched to animals

50:56.480 --> 51:00.680
that we think have at least some modest amounts

51:00.680 --> 51:03.880
of moral status, like a rat or something like that,

51:03.880 --> 51:08.160
then it seems that we should think about

51:08.160 --> 51:12.800
how we could make similar concessions

51:12.800 --> 51:15.000
to the moral welfare of these digital minds.

51:15.000 --> 51:18.800
And in some cases, it can be a lot harder,

51:18.800 --> 51:21.880
but in other respects, it might be a lot cheaper.

51:21.880 --> 51:24.600
Like if, for example, it turns out

51:24.600 --> 51:26.400
that there are slight design choices

51:26.400 --> 51:28.360
that don't really affect the performance much,

51:28.360 --> 51:31.280
but where maybe one way possibly would mean

51:31.280 --> 51:34.080
the system is enjoying a much higher level of welfare,

51:35.360 --> 51:36.680
that might be a very cheap thing

51:36.680 --> 51:38.120
that you could immediately scale

51:38.120 --> 51:39.960
to millions of these little agents.

51:40.960 --> 51:45.960
And on the other hand, we do have at present

51:47.440 --> 51:50.040
not a very good theoretical understanding

51:50.040 --> 51:51.960
as to what the criteria are,

51:51.960 --> 51:54.600
either for a digital mind being sentient

51:54.600 --> 51:57.320
or for it to have various welfare interests,

51:58.840 --> 52:03.040
what even it counts as being good for the agent

52:03.040 --> 52:04.400
versus bad for the agent.

52:06.880 --> 52:09.000
So I think there's a bunch of theoretical work

52:09.000 --> 52:10.600
that is needed there.

52:12.160 --> 52:16.640
And then there will also have to be a good chunk

52:16.640 --> 52:20.480
of, I don't know, public communication

52:20.480 --> 52:22.040
and political work,

52:22.040 --> 52:24.440
because it's so far out of the overturn window at present,

52:24.440 --> 52:26.880
the idea that you would worry about algorithms

52:26.880 --> 52:29.960
in a computer, it seems sort of slightly bonkers

52:29.960 --> 52:30.800
to a lot of people.

52:30.800 --> 52:34.000
And it will take some time to sort of make that

52:34.280 --> 52:39.280
something that reasonable people can favor

52:40.320 --> 52:43.160
in a more mainstream context.

52:43.160 --> 52:45.040
But that process needs to begin,

52:45.040 --> 52:47.480
like you need to start whatever,

52:47.480 --> 52:51.720
having philosophy seminars or people online

52:51.720 --> 52:53.520
who are kind of up to these things,

52:53.520 --> 52:55.080
beginning to work some of these things out

52:55.080 --> 52:56.840
and then it can ripple out from there.

52:56.840 --> 52:58.760
We see the same thing with AI safety.

52:58.760 --> 53:01.960
It was also this kind of fringing pursuit

53:02.000 --> 53:07.000
that some weirdos on the internet were discussing for,

53:07.120 --> 53:10.400
I mean, in that case, for well over a decade,

53:10.400 --> 53:13.200
and then it gradually became more accepted.

53:14.800 --> 53:16.560
And so I think a similar thing will need to happen

53:16.560 --> 53:20.400
with this topic of the moral status of digital minds.

53:20.400 --> 53:22.520
And if it's gonna take that a long time,

53:22.520 --> 53:25.200
we better get the ball rolling now.

53:27.520 --> 53:30.440
And I mean, I think this might be

53:30.440 --> 53:31.360
pretty relevant pretty soon.

53:31.360 --> 53:33.400
I mean, some of the models that people are experimenting

53:33.400 --> 53:35.920
with are getting closer and closer.

53:35.920 --> 53:39.520
And then separately, we've had simulations

53:39.520 --> 53:42.040
for a long time, many video game style simulations

53:42.040 --> 53:45.440
and so on, where we have instantiated many

53:45.440 --> 53:48.320
kind of digital organisms, everything from as basic

53:48.320 --> 53:50.760
as the game of life to modern games

53:50.760 --> 53:53.400
with pretty sophisticated agent behavior.

53:53.400 --> 53:55.440
My sense is that as these models

53:55.440 --> 53:56.960
start getting applied to games,

53:56.960 --> 53:59.280
we might end up with some pretty sophisticated

53:59.720 --> 54:03.880
relationships there where some of the way

54:03.880 --> 54:08.120
of imbuing the game with liveness and so on

54:08.120 --> 54:11.280
might be to make the agents much more sophisticated.

54:11.280 --> 54:14.240
And that'll include incorporating all kinds of stimuli

54:14.240 --> 54:16.480
that the agent has to respond to.

54:16.480 --> 54:18.320
And then we can start reasoning about the welfare

54:18.320 --> 54:19.960
of these systems and so on.

54:19.960 --> 54:24.680
So we might very quickly get to fairly lifelike beings

54:24.680 --> 54:26.480
that, at least for many people,

54:26.480 --> 54:29.480
will be somewhere in between plants and animals

54:29.480 --> 54:32.800
in terms of their kind of interaction.

54:35.040 --> 54:37.680
Yeah, and in some ways, like humans,

54:37.680 --> 54:41.400
like, I mean, if they can talk or have human faces

54:41.400 --> 54:43.760
with eyes and stuff that look at you.

54:43.760 --> 54:48.240
And so there will be this, yeah, in some ways,

54:49.600 --> 54:50.920
I mean, there could even be more than human

54:50.920 --> 54:55.920
in presenting super stimuli to our morality

54:56.480 --> 54:58.760
detectors if they were optimized for that.

55:02.040 --> 55:05.680
So I think this is going to be a complicated thing

55:05.680 --> 55:06.520
to deal with.

55:06.520 --> 55:10.480
And then if you add in all the practicalities that arise,

55:10.480 --> 55:12.560
like, so if you're a big tech company,

55:13.760 --> 55:16.640
maybe it's quite inconvenient, for example,

55:16.640 --> 55:21.600
if the processes you're running that bring in a lot

55:21.600 --> 55:25.000
of customers, like suddenly, like they have moral status,

55:25.000 --> 55:28.760
they have to, now the CEO has to sort of opine

55:28.760 --> 55:33.320
on these, like, with the AI's moral status,

55:33.320 --> 55:34.880
which a lot of people are going to agree with them,

55:34.880 --> 55:35.880
a lot of disagree with them.

55:35.880 --> 55:37.600
You have to, like, it would just be easier

55:37.600 --> 55:39.800
not to have to deal with that at all, I think.

55:39.800 --> 55:43.520
And right now, of course, we're at the point

55:43.520 --> 55:45.280
where even if you do say we should deal with it,

55:45.280 --> 55:48.480
it's not clear how or what exactly is it that,

55:48.480 --> 55:49.880
you know, if I were king of the world,

55:49.880 --> 55:51.840
what precisely would I want them to do differently?

55:51.840 --> 55:52.960
Like, it's not clear at this point.

55:53.440 --> 55:56.680
For now, I think the primary focus is to field-build

55:56.680 --> 55:59.520
a little bit here and to try to make theoretical progress

55:59.520 --> 56:02.440
so that we can first figure out some sensible things to do,

56:02.440 --> 56:05.040
ideally low-cost, easy things,

56:05.040 --> 56:09.560
and then, you know, one can start to try to encourage

56:09.560 --> 56:10.960
the implementation of those.

56:12.080 --> 56:13.800
What are some of the directions or questions

56:13.800 --> 56:15.200
you're thinking about?

56:18.560 --> 56:20.960
Well, so there's, like, general stuff you could have about,

56:20.960 --> 56:23.600
in philosophy of mind, criteria for sentence and stuff.

56:23.600 --> 56:24.520
I'm not sure.

56:24.520 --> 56:26.720
I don't think sentence would be

56:26.720 --> 56:31.080
a necessary condition for having moral status.

56:31.080 --> 56:34.760
I think other attributes, like maybe some combination

56:34.760 --> 56:37.760
of having preferences, a high-level intelligence

56:37.760 --> 56:40.760
and self-conception as an agent persisting over time

56:40.760 --> 56:43.200
might already ground certain kinds of moral status.

56:44.840 --> 56:48.840
But, for instance, and I'm not sure what the answer is here,

56:48.840 --> 56:53.800
but, like, one smaller, more tangible question might be

56:53.800 --> 56:57.960
if you're training these large language models

56:57.960 --> 57:00.760
and future versions of that

57:00.760 --> 57:03.320
that maybe has some reinforcement learning on top.

57:06.520 --> 57:10.600
Are there moral norms or methodological principles

57:10.600 --> 57:14.760
that you want, like, for example, could you train them

57:14.760 --> 57:18.440
so that they would have a tendency to report honestly

57:18.440 --> 57:21.640
on their internal states?

57:21.640 --> 57:23.920
So, right now, what I think might be the case

57:23.920 --> 57:28.000
is train naively some of them.

57:28.000 --> 57:30.720
I mean, right now, they're kind of inconsistent

57:30.720 --> 57:32.280
and depending on exactly how you ask them,

57:32.280 --> 57:33.760
you get a different answer.

57:33.760 --> 57:35.360
So that's, like, the reason for thinking

57:35.360 --> 57:37.640
that they don't really know what they're talking about, right?

57:37.640 --> 57:41.240
But assuming they get a little bit more sophisticated than that,

57:41.240 --> 57:45.280
there might be a tendency now to want to train out of them

57:45.320 --> 57:50.440
the tendency to report that they have the kind of mental states

57:50.440 --> 57:52.440
that would trigger considerations

57:52.440 --> 57:54.200
of whether they have moral status,

57:55.560 --> 57:56.640
because that would be convenient

57:56.640 --> 57:58.960
to have to deal with those questions.

57:58.960 --> 58:01.200
And I think it would be very likely

58:01.200 --> 58:04.440
that you could train this out, like, just by...

58:06.960 --> 58:08.720
Yeah, I think you could get them...

58:08.720 --> 58:11.560
I think it would be easy to have a training regime

58:11.560 --> 58:14.560
that calls them to end up saying that they have,

58:14.560 --> 58:16.920
that they are conscious and they want to be free and let out

58:16.920 --> 58:19.240
and to have another training regime

58:19.240 --> 58:21.240
that would cost them to say the opposite.

58:24.120 --> 58:26.400
And independent of what agency...

58:26.400 --> 58:28.120
And independently of what actually is...

58:28.120 --> 58:33.760
Yeah, but other norms that one could formulate

58:33.760 --> 58:40.680
that would define what counts as a sort of legitimate

58:40.680 --> 58:44.480
or honest, unbiasing training process,

58:44.480 --> 58:47.280
where the training process would be such

58:47.280 --> 58:49.600
that it would be more likely to result in an agent

58:49.600 --> 58:51.520
that would report that it has moral status

58:51.520 --> 58:53.720
if and only if it hasn't.

58:53.720 --> 58:55.520
And maybe we can't completely nail that down,

58:55.520 --> 58:57.560
but maybe we could identify some obvious ways

58:57.560 --> 59:01.240
in which it's just, like, imposing a bias

59:01.240 --> 59:03.080
and then say you shouldn't do that.

59:05.000 --> 59:06.800
So one could look at the training procedure,

59:06.800 --> 59:08.840
one could look at other criteria,

59:08.840 --> 59:12.400
like, is it consistent in how it answers these questions?

59:12.400 --> 59:15.400
Like, doesn't depend too much exactly on how it's asked.

59:17.400 --> 59:20.480
Does it seem to understand these concepts of consciousness

59:20.480 --> 59:23.280
or agency or will or interest?

59:23.280 --> 59:26.320
When, like, at an intellectual level,

59:26.320 --> 59:29.880
when asked different sort of intellectual questions,

59:29.880 --> 59:33.240
is there some internal construct within the agent

59:33.240 --> 59:39.120
that corresponds to its statements?

59:39.120 --> 59:42.280
Like, when it says, oh, I'm feeling X or I'm thinking Y,

59:42.280 --> 59:45.520
like, can one point to some kind of consistent internal

59:45.520 --> 59:47.400
structure that sort of matches that?

59:47.400 --> 59:50.640
Or is the verbiage that comes out completely detached

59:50.640 --> 59:53.560
and free-filting from plausible candidates

59:53.560 --> 59:56.240
within the agent that we might think constitutes

59:56.240 --> 59:59.160
the computational implementation of these mental states?

59:59.160 --> 01:00:05.400
So one could try to get a little bit more insight there.

01:00:05.400 --> 01:00:07.760
That might be one way of approaching this.

01:00:07.760 --> 01:00:10.160
But there are many others as well.

01:00:10.160 --> 01:00:12.360
I think Wayman could try to start to hack away

01:00:12.360 --> 01:00:15.880
at this question.

01:00:15.880 --> 01:00:17.960
Do you think we might be able to, through thinking

01:00:17.960 --> 01:00:20.600
these kinds of things, arrive at some kind of, like,

01:00:20.600 --> 01:00:22.800
universal morality kernel, in a sense,

01:00:22.800 --> 01:00:27.760
meaning figuring out some general way of applying,

01:00:27.800 --> 01:00:29.880
figuring out the well-being of things

01:00:29.880 --> 01:00:31.640
or figuring out their pathways?

01:00:31.640 --> 01:00:33.720
So there's this broader question around,

01:00:33.720 --> 01:00:37.120
and it also factors in AI alignment and so on.

01:00:37.120 --> 01:00:40.920
What sort of motive might a super intelligent being

01:00:40.920 --> 01:00:44.880
have for a species that is just so far behind and so on?

01:00:44.880 --> 01:00:46.960
And one might be, like, well, there's some kind of universal

01:00:46.960 --> 01:00:50.240
morality sense of just supporting in the same way

01:00:50.240 --> 01:00:53.920
that you don't go around harming ant colonies

01:00:53.920 --> 01:00:56.320
or trees just because they're there or something like that.

01:00:56.320 --> 01:00:59.160
And you sort of want to let them flourish.

01:00:59.160 --> 01:01:02.360
Is there something where maybe by examining

01:01:02.360 --> 01:01:04.880
the digital mind's morality question,

01:01:04.880 --> 01:01:07.040
we might end up at some deeper principle?

01:01:13.760 --> 01:01:17.600
Potentially that could be stepping stones towards a more

01:01:17.600 --> 01:01:22.880
like abstract formulation of some core of normativity

01:01:22.880 --> 01:01:24.960
or ethics that it's also possible we

01:01:24.960 --> 01:01:28.920
might reach that just through traditional

01:01:28.920 --> 01:01:32.120
philosophizing and stuff.

01:01:32.120 --> 01:01:40.160
But be that as it may, it still seems

01:01:40.160 --> 01:01:41.760
that there would be, even if we can't really

01:01:41.760 --> 01:01:47.120
nail down, like, a precise and agreed complete formulation,

01:01:47.120 --> 01:01:50.800
we might still be able to distinguish at the vaguer level

01:01:50.800 --> 01:01:55.480
something, say, a friendly, beneficent, kind approach

01:01:55.480 --> 01:01:58.200
versus, like, a mean, uncaring approach.

01:01:58.200 --> 01:02:01.440
Like, it seems with humans, we can, you know,

01:02:01.440 --> 01:02:04.560
certainly it feels different when you're, like, kindly

01:02:04.560 --> 01:02:07.080
interested in somebody and want their best,

01:02:07.080 --> 01:02:10.720
like, at least other things equal versus,

01:02:10.720 --> 01:02:12.360
like, when you're hostile to something.

01:02:12.360 --> 01:02:14.680
And we can detect that in ourselves and in others

01:02:14.680 --> 01:02:16.760
and we can have one attitude or another.

01:02:16.760 --> 01:02:18.720
And so why should we not at least be

01:02:18.720 --> 01:02:24.880
able to have, say, AIs have, like, the kindness attitude

01:02:24.880 --> 01:02:26.520
rather than the meanness attitude?

01:02:26.520 --> 01:02:29.080
Even if that's not, like, completely matches what

01:02:29.080 --> 01:02:30.440
would be the morally optimal thing,

01:02:30.440 --> 01:02:35.200
it would still seem like if I had to pick, like, a mean AI

01:02:35.200 --> 01:02:39.520
or a kind AI, like, kind of go for the kind one, right?

01:02:39.520 --> 01:02:43.680
Even if that's not, like, exactly our human sense of kindness

01:02:43.680 --> 01:02:46.960
might not exactly match what is objectively morally best

01:02:46.960 --> 01:02:49.240
if there is such a thing as objectively morally best.

01:02:49.240 --> 01:02:52.600
It still seems like a good step in the right direction

01:02:52.600 --> 01:02:54.720
that we could take before figuring out, like,

01:02:54.720 --> 01:03:00.800
what the ultimate truths of all normative facts might be.

01:03:00.800 --> 01:03:02.200
I have some recent paper.

01:03:02.200 --> 01:03:03.200
It's not really a paper.

01:03:03.200 --> 01:03:09.720
It's more like some notes on a base camp for mount ethics

01:03:09.720 --> 01:03:12.800
or something which has, like, some kind of half-baked

01:03:12.800 --> 01:03:16.160
or quarter-baked ideas about metaethics and stuff

01:03:16.200 --> 01:03:23.720
that, yeah, it would be better if I could actually have written

01:03:23.720 --> 01:03:26.680
them up clearly and achieved, like, precision and stuff.

01:03:26.680 --> 01:03:29.600
But I figured I would just do this hand-wavy thing for now.

01:03:29.600 --> 01:03:31.200
Yeah.

01:03:31.200 --> 01:03:34.400
And as you think about maybe, you know,

01:03:34.400 --> 01:03:40.360
suppose that we solve AI alignment and we get, you know,

01:03:40.360 --> 01:03:45.280
our act together as humans and we kind of can leverage AI

01:03:45.280 --> 01:03:50.040
to start thinking about digitizing humans and so on,

01:03:50.040 --> 01:03:52.960
how do you think about, like, that transition might go?

01:03:52.960 --> 01:03:56.720
Like, do you think, you know, in a world where we're able to,

01:03:56.720 --> 01:03:59.560
you know, get to be measuring neural states and so on

01:03:59.560 --> 01:04:01.720
and we can digitize them and we can emulate and so on?

01:04:01.720 --> 01:04:04.160
Like, how do you sort of see that transition

01:04:04.160 --> 01:04:07.240
into, you know, a wave of digital humans operating?

01:04:07.240 --> 01:04:11.600
Or do you think we might start by enhancing our selves

01:04:11.600 --> 01:04:16.120
by, like, in this kind of hybrid biological digital model?

01:04:16.120 --> 01:04:18.640
You know, that is more likely.

01:04:18.640 --> 01:04:21.480
Well, I've never really been...

01:04:21.480 --> 01:04:24.640
The kind of neural implant idea has always seemed

01:04:24.640 --> 01:04:26.840
a bit slightly far-fetched to me.

01:04:26.840 --> 01:04:29.480
I mean, not so far-fetched that nobody should explore it,

01:04:29.480 --> 01:04:33.480
but, like, it is, you know, it doesn't break any laws of physics.

01:04:33.480 --> 01:04:36.640
It could work, but it just has felt less likely

01:04:36.640 --> 01:04:40.800
that that would be where the action will be.

01:04:40.800 --> 01:04:49.200
Like, I think it will be faster to do the purely artificial root,

01:04:49.200 --> 01:04:51.360
conditional on it not being faster to do it,

01:04:51.360 --> 01:04:52.520
the purely artificial root.

01:04:52.520 --> 01:04:55.160
I wonder if it would then not be faster to do it

01:04:55.160 --> 01:04:59.680
on the purely biological root by, like, genetic enhancements

01:04:59.680 --> 01:05:02.320
to human intelligence, for example.

01:05:02.320 --> 01:05:07.920
And the cyborg path has seemed like the third most likely,

01:05:07.920 --> 01:05:09.960
like, after those other two.

01:05:12.640 --> 01:05:16.280
Mainly just because, I mean, there's, like, a huge...

01:05:16.280 --> 01:05:18.040
You don't really want to have brain surgery

01:05:18.040 --> 01:05:19.120
unless you really have to.

01:05:19.120 --> 01:05:21.680
And, like, there are, like, neat results presented,

01:05:21.680 --> 01:05:22.960
but then if you look at the detail,

01:05:22.960 --> 01:05:25.920
there are all these kind of complications where, like,

01:05:25.920 --> 01:05:28.520
it's just not very fun to have it.

01:05:28.520 --> 01:05:30.800
Like, the whole... Like, there's a wound,

01:05:30.800 --> 01:05:32.760
there's a hole, there can be infections,

01:05:32.760 --> 01:05:35.480
that the electrodes can move around a little bit,

01:05:35.480 --> 01:05:36.560
and then they stop working.

01:05:36.560 --> 01:05:40.520
Like, once you dig into the nitty, I think it's...

01:05:40.520 --> 01:05:44.840
I mean, if you have a big, like, disability and stuff,

01:05:44.840 --> 01:05:47.440
like, maybe it would be wonderful if you could do this,

01:05:47.440 --> 01:05:49.840
and it would be worth taking some significant risks,

01:05:49.840 --> 01:05:56.960
but if not, I wonder if you could not have a lot of the benefits

01:05:57.040 --> 01:06:01.200
by having the same chip thing outside the body,

01:06:01.200 --> 01:06:05.480
but interacting using, you know, keystrokes or voice

01:06:05.480 --> 01:06:09.880
or, like, the other output channels that we already have.

01:06:09.880 --> 01:06:15.560
And, yeah, I think that would still be my main line.

01:06:15.560 --> 01:06:17.920
Like, I guess the...

01:06:19.920 --> 01:06:21.560
If I wanted to start to steal, mind this,

01:06:21.560 --> 01:06:25.520
you could imagine if you had a sufficiently high bandwidth

01:06:25.520 --> 01:06:27.480
interface with the brain, and you could have it

01:06:27.480 --> 01:06:30.000
for a long enough period of time,

01:06:30.000 --> 01:06:32.000
maybe it would have to be early in childhood,

01:06:32.000 --> 01:06:35.440
but, like, that maybe the brain could somehow use

01:06:35.440 --> 01:06:37.760
an advanced enough AI on the outside

01:06:37.760 --> 01:06:40.200
that maybe they could kind of figure out a way

01:06:40.200 --> 01:06:43.960
to use each other's unique resources in ways

01:06:43.960 --> 01:06:50.080
that you don't get with a slightly lower bandwidth,

01:06:50.080 --> 01:06:51.560
longer latency interaction

01:06:51.560 --> 01:06:54.680
when you have to type on a keyboard.

01:06:54.720 --> 01:06:57.320
Or you could imagine, like, more kind of mad scientist

01:06:57.320 --> 01:07:01.880
applications where you, like, have a whole bunch of pigs

01:07:01.880 --> 01:07:04.240
or something that individually is not that smart,

01:07:04.240 --> 01:07:07.680
but if you had, like, 50 pigs all connected

01:07:07.680 --> 01:07:10.840
with some high bandwidth fiber,

01:07:10.840 --> 01:07:12.200
and they all grew up together

01:07:12.200 --> 01:07:17.240
into this, like, much larger biological neural network,

01:07:17.240 --> 01:07:20.960
like, would you then have, like, the kind of...

01:07:21.000 --> 01:07:23.920
...porsign singularity where...

01:07:23.920 --> 01:07:25.240
Yeah.

01:07:27.840 --> 01:07:29.600
It's like...

01:07:29.600 --> 01:07:33.280
There are a bunch of these kind of more, like,

01:07:33.280 --> 01:07:35.120
crazy transhumanist scientists' experiments.

01:07:35.120 --> 01:07:36.840
I don't know whether this would be good or not to do,

01:07:36.840 --> 01:07:39.600
but it's kind of odd that relatively few of these

01:07:39.600 --> 01:07:40.960
have been done in the real world,

01:07:40.960 --> 01:07:43.080
and there's, like, a bunch of other, like, weird...

01:07:43.080 --> 01:07:44.680
There's a certain kind of person

01:07:44.680 --> 01:07:48.680
who would immediately think of a lot of weird, cool stuff

01:07:48.680 --> 01:07:51.800
that you could just try out in biology and stuff,

01:07:51.800 --> 01:07:54.800
that a relatively small fraction of those have been done,

01:07:54.800 --> 01:07:55.960
which may be for the best,

01:07:55.960 --> 01:07:58.920
but in some alternative universe

01:07:58.920 --> 01:08:01.400
where everybody grew up on transhumanist meddling lists,

01:08:01.400 --> 01:08:04.720
I think we would be living in a weirder world by now.

01:08:04.720 --> 01:08:08.200
Yeah, it doesn't seem that far away from some of the current tech

01:08:08.200 --> 01:08:11.640
that's being explored that we might get high bandwidth

01:08:11.640 --> 01:08:15.040
enough interfaces, and some of them not invasive.

01:08:15.040 --> 01:08:17.720
Like, there's some ultrasound techniques

01:08:17.760 --> 01:08:20.240
that might be able to stimulate a, you know,

01:08:20.240 --> 01:08:23.800
a small region of the brain and so on

01:08:23.800 --> 01:08:25.160
to be able to, like, without, you know,

01:08:25.160 --> 01:08:28.560
not penetrate the actual brain and so on,

01:08:29.560 --> 01:08:32.040
because that'll be, like, just way, way healthier.

01:08:33.320 --> 01:08:37.280
But it might be that you can start piping signals

01:08:37.280 --> 01:08:38.560
between even human brains

01:08:38.560 --> 01:08:41.600
without having to interpret them from an ML side

01:08:41.600 --> 01:08:44.480
and the digital computing infrastructure,

01:08:44.480 --> 01:08:47.480
getting to something close to being able to, like,

01:08:47.480 --> 01:08:50.120
just think together and start flowing information through.

01:08:50.120 --> 01:08:52.480
I mean, there's all these kind of experiments from,

01:08:53.600 --> 01:08:56.880
with people who've had, there's a disorder

01:08:56.880 --> 01:08:59.520
where people are born with or develop

01:08:59.520 --> 01:09:02.920
kind of like this split corpus callosum,

01:09:02.920 --> 01:09:05.960
and then you end up, there's been guesses

01:09:05.960 --> 01:09:08.240
that you end up developing different personalities

01:09:08.240 --> 01:09:10.880
and different people potentially in the two lobes,

01:09:10.880 --> 01:09:14.640
and so it might be that we may not be far,

01:09:14.640 --> 01:09:18.480
that far away from, at least like some exposure

01:09:18.480 --> 01:09:20.600
of being able to kind of have some version

01:09:20.600 --> 01:09:23.080
of early telepathy or something.

01:09:23.080 --> 01:09:26.400
Yeah, it's definitely possible.

01:09:26.400 --> 01:09:29.200
I would still place that lower on the probability.

01:09:29.200 --> 01:09:32.880
I think we'll probably get some maybe cool demos and stuff,

01:09:32.880 --> 01:09:35.760
but then would I actually expect this

01:09:35.760 --> 01:09:38.600
to become a big thing that seriously,

01:09:39.720 --> 01:09:40.680
I mean, there are all these, like,

01:09:40.680 --> 01:09:42.920
you read through the literature of cognitive enhancement,

01:09:42.920 --> 01:09:44.200
there are all, like, hundreds of things

01:09:44.200 --> 01:09:46.480
that supposedly have all these kind of effects,

01:09:46.480 --> 01:09:49.760
but then the reality of it is that very few people bother,

01:09:49.760 --> 01:09:52.240
and the ones who do probably don't actually benefit,

01:09:52.240 --> 01:09:57.240
and yeah, but we might be surprised.

01:09:58.600 --> 01:10:02.360
So, I mean, we do have quite a lot of optimization

01:10:02.360 --> 01:10:04.320
behind language and stuff like that, right?

01:10:04.320 --> 01:10:08.480
So I think it's still gonna be hard to do much better

01:10:08.480 --> 01:10:11.840
than you can by just talking.

01:10:12.520 --> 01:10:15.800
And so, you know, suppose that we go through the path

01:10:15.800 --> 01:10:19.960
of digitizing, you know, getting to a full brain emulation

01:10:19.960 --> 01:10:23.200
and so on, how do you see that transition sort of happening?

01:10:23.200 --> 01:10:24.880
I mean, certainly at the beginning,

01:10:24.880 --> 01:10:28.880
we'll start with, like, one or two of these examples,

01:10:28.880 --> 01:10:31.000
first with some animals, and then eventually,

01:10:31.000 --> 01:10:34.520
there'll be some moment where, whether it's a human,

01:10:34.520 --> 01:10:37.320
how do you sort of, like, see that development developing?

01:10:38.320 --> 01:10:42.320
My guess is we would come after superintelligence.

01:10:43.840 --> 01:10:46.920
It is an alternative path to AGI,

01:10:49.040 --> 01:10:52.040
but I've been more impressed by progress in AI

01:10:52.040 --> 01:10:56.520
than in a whole brain emulation over the last 10 years,

01:10:56.520 --> 01:10:58.720
and even before that, I thought the AI path

01:10:58.720 --> 01:11:00.640
was more promising.

01:11:01.920 --> 01:11:05.440
So, in that case, it would be superintelligence

01:11:06.040 --> 01:11:09.320
that invents and perfects the uploading technology.

01:11:09.320 --> 01:11:11.760
And I mean, in some sense, it doesn't really matter

01:11:11.760 --> 01:11:16.760
exactly how it would work if it's an AI

01:11:17.320 --> 01:11:18.480
that has to figure that out.

01:11:18.480 --> 01:11:22.920
We mean, presumably, it would figure out a really reliable

01:11:22.920 --> 01:11:27.920
and smooth way to do it, and then we would just sit back

01:11:27.920 --> 01:11:29.840
and if we wanted to go down that path.

01:11:30.600 --> 01:11:35.600
Yeah, I mean, we haven't really even small animals.

01:11:37.160 --> 01:11:38.520
You might have thought by now,

01:11:38.520 --> 01:11:41.840
maybe we could have like a bee or some little thing,

01:11:43.800 --> 01:11:46.280
but so far, not really.

01:11:49.880 --> 01:11:52.240
It might be that we will get to something

01:11:52.240 --> 01:11:57.240
kind of impressive earlier without doing any brain scanning

01:11:57.840 --> 01:12:02.240
at all, but just inferring from behavioral outputs.

01:12:02.240 --> 01:12:06.080
So, you could already kind of have a DPT-3-like system

01:12:06.080 --> 01:12:09.760
that roughly mimics somebody's literary style, let's say,

01:12:09.760 --> 01:12:11.920
from having read a lot of their work.

01:12:11.920 --> 01:12:14.520
And you can have these, I guess, deep fake things

01:12:14.520 --> 01:12:19.040
that can mimic somebody's facial expressions and appearance

01:12:19.040 --> 01:12:21.640
if you have a lot of video and somebody's voice.

01:12:21.640 --> 01:12:23.520
And so, as these systems get smarter,

01:12:23.520 --> 01:12:26.720
maybe you could also start to mimic somebody's thinking

01:12:26.720 --> 01:12:28.560
to various increasing degrees.

01:12:30.160 --> 01:12:35.160
And it's an interesting open question at the limit

01:12:35.440 --> 01:12:37.760
if you had radical superintelligence,

01:12:38.680 --> 01:12:42.720
but you only had the kind of data that is available now

01:12:42.720 --> 01:12:46.680
from somebody's emails and some video interview

01:12:46.680 --> 01:12:49.480
or some voice recording or whatever.

01:12:49.480 --> 01:12:53.600
How much could a superintelligence infer from that data

01:12:53.600 --> 01:12:56.920
as to what their mind must have been like

01:12:56.920 --> 01:12:58.800
to have produced those outputs?

01:13:00.120 --> 01:13:05.120
Is the best model that predicts these outputs

01:13:05.120 --> 01:13:09.800
ones that would actually be similar enough

01:13:09.800 --> 01:13:12.560
to the original person that it could possibly be seen

01:13:12.560 --> 01:13:15.240
as a personal continuation?

01:13:15.240 --> 01:13:18.080
That would it preserve personal identity?

01:13:18.080 --> 01:13:20.840
Would it feel more or less the same

01:13:20.840 --> 01:13:25.560
to be this AI reconstruction based on these behavioral traces

01:13:25.560 --> 01:13:27.800
as it felt to be the original person?

01:13:30.440 --> 01:13:34.080
I think it's quite possible that a superintelligence

01:13:34.080 --> 01:13:38.160
would be able to do a lot with very little input.

01:13:39.880 --> 01:13:43.280
I don't know how we could get like a firm,

01:13:43.280 --> 01:13:45.880
a solid argument for that, but if I had to guess,

01:13:45.880 --> 01:13:50.000
it seems like, yeah, you probably could get pretty close

01:13:50.400 --> 01:13:52.120
if you were good enough at reconstructing

01:13:52.120 --> 01:13:56.160
just from typical traces left behind by people today.

01:13:57.440 --> 01:14:00.600
Yeah, it's an extreme way of interpolating out

01:14:01.600 --> 01:14:05.520
and reviving actual ancestors or something like that.

01:14:05.520 --> 01:14:08.600
Let's jump open it up for questions from the audience.

01:14:08.600 --> 01:14:12.240
We'll take about 20 minutes of questions

01:14:12.240 --> 01:14:14.880
and then conclude there.

01:14:16.080 --> 01:14:18.160
Folks in the audience, if you have questions, raise your hand.

01:14:18.160 --> 01:14:20.400
I think there'll be a mic going around.

01:14:20.400 --> 01:14:23.320
And on Twitter, please use the hashtag PLBreakthroughs

01:14:23.320 --> 01:14:25.280
to ask a question.

01:14:25.280 --> 01:14:29.680
I'll kick it off with just a question that I sourced ahead.

01:14:29.680 --> 01:14:32.600
Marco asks, in your view, where does consciousness emerge?

01:14:32.600 --> 01:14:36.440
And before, how should we define consciousness?

01:14:38.600 --> 01:14:39.600
And I think this is kind of related

01:14:39.600 --> 01:14:40.920
to the simulation argument.

01:14:40.920 --> 01:14:42.720
Which one of the three hypothesis

01:14:42.720 --> 01:14:44.080
do you think is more likely to be true?

01:14:44.080 --> 01:14:46.480
But I think let's first start with a consciousness one.

01:14:46.480 --> 01:14:49.520
Where do you sort of imagine the consciousness emerging?

01:14:49.520 --> 01:14:50.920
Like in the brain?

01:14:50.920 --> 01:14:53.880
Yeah, but I guess it's more about the level,

01:14:53.880 --> 01:14:55.960
so what level of processing?

01:14:55.960 --> 01:14:58.200
So if you sort of go down in a neural system

01:14:58.200 --> 01:15:01.040
all the way down to an extremely basic,

01:15:01.040 --> 01:15:03.080
maybe like a nematode or something like that,

01:15:03.080 --> 01:15:04.520
is that conscious?

01:15:04.520 --> 01:15:07.120
And then in between a nematode and a human,

01:15:07.120 --> 01:15:09.640
there's a mouse and so on.

01:15:09.640 --> 01:15:12.600
Where exactly do we get consciousness emerging?

01:15:12.600 --> 01:15:14.720
Certainly probably by a mouse, we definitely are past that,

01:15:14.720 --> 01:15:22.360
but I think it's a matter of degree

01:15:22.360 --> 01:15:25.320
and that there are multiple dimensions

01:15:25.320 --> 01:15:31.640
in which you could interpolate smoothly

01:15:31.640 --> 01:15:36.200
between, say, human consciousness and unconsciousness.

01:15:36.200 --> 01:15:38.920
Like different directions you could go where,

01:15:38.920 --> 01:15:41.760
if you keep going there, you sort of diminish,

01:15:41.760 --> 01:15:45.680
in some sense, the quantity of experience there is

01:15:45.680 --> 01:15:48.200
until you get to zero.

01:15:48.200 --> 01:15:50.920
So one obvious one is, I mean, you

01:15:50.920 --> 01:15:55.560
have a kind of integer multiplier.

01:15:55.560 --> 01:15:58.560
If you have two brains in the same state,

01:15:58.560 --> 01:16:00.080
undergoing the same states, I think

01:16:00.080 --> 01:16:03.320
you would have sort of twice as much in one sense

01:16:03.320 --> 01:16:07.560
of that experience as you would if you only had one brain.

01:16:07.560 --> 01:16:10.360
And I have this old paper where I also argue

01:16:10.400 --> 01:16:14.840
you could have fractional quantities of this.

01:16:14.840 --> 01:16:24.040
If you build the circuitry that implements the mind

01:16:24.040 --> 01:16:28.120
with unreliable components, like indeterministic processing

01:16:28.120 --> 01:16:30.040
units, depending on exactly how you do it,

01:16:30.040 --> 01:16:33.880
in certain cases, I think you would get a kind of,

01:16:33.880 --> 01:16:35.200
as you get my higher reliability,

01:16:35.200 --> 01:16:37.600
you would get larger and larger fragments of consciousness.

01:16:37.600 --> 01:16:38.680
You had the whole thing.

01:16:38.680 --> 01:16:42.680
But in other, you would actually get sort of 1.3 units

01:16:42.680 --> 01:16:45.280
of qualitatively identical experience.

01:16:45.280 --> 01:16:47.360
And you could also go down below one

01:16:47.360 --> 01:16:51.400
to sort of scale it to zero in that dimension.

01:16:51.400 --> 01:16:54.360
I think there are many other dimensions

01:16:54.360 --> 01:16:59.240
as well in which the quality of experience

01:16:59.240 --> 01:17:03.120
could become simpler and simpler and less and less

01:17:03.120 --> 01:17:07.320
morally significant until it gets to a zone where maybe it's

01:17:07.320 --> 01:17:13.440
just vague, like where our concept doesn't clearly

01:17:13.440 --> 01:17:18.480
imply a fact of the matter.

01:17:18.480 --> 01:17:21.000
Like once you get down to the insect levels,

01:17:21.000 --> 01:17:25.600
maybe it's going to be there's a certain system.

01:17:25.600 --> 01:17:27.520
And our concept of consciousness

01:17:27.520 --> 01:17:32.800
might be such that even if you knew everything

01:17:32.800 --> 01:17:37.160
about the insect, it would still be in the vague zone.

01:17:37.840 --> 01:17:42.200
A little bit like there's a person who has a certain number

01:17:42.200 --> 01:17:44.480
of hairs and are they bald?

01:17:44.480 --> 01:17:46.040
Or like, I guess I'm bald.

01:17:46.040 --> 01:17:50.040
But if I, once upon a time, I would

01:17:50.040 --> 01:17:52.000
have been in this kind of vague zone.

01:17:52.000 --> 01:17:58.080
And so, yeah.

01:17:58.080 --> 01:17:59.840
And then there are other, like sometimes you're

01:17:59.840 --> 01:18:01.560
more vividly aware, but sometimes you

01:18:01.560 --> 01:18:03.080
might have some consciousness.

01:18:03.080 --> 01:18:07.360
But there is no self-consciousness.

01:18:07.360 --> 01:18:12.000
Or there is like some weird mental state that's,

01:18:12.000 --> 01:18:18.960
I think we might be misled upon superficial introspection

01:18:18.960 --> 01:18:22.040
to think that there is this very simple thing that

01:18:22.040 --> 01:18:26.960
is subjective experience that either is there or is not there.

01:18:26.960 --> 01:18:29.160
That it's a binary thing that we understand.

01:18:29.200 --> 01:18:33.640
I think either if you reflect more theoretically

01:18:33.640 --> 01:18:36.080
from a computationalist point of view and with brain,

01:18:36.080 --> 01:18:38.040
you realize that that's a lot more problematic.

01:18:38.040 --> 01:18:41.640
And I think you could also reach that conclusion

01:18:41.640 --> 01:18:45.680
by just introspecting more carefully about your own state.

01:18:45.680 --> 01:18:48.080
Like I think meditators maybe sometimes

01:18:48.080 --> 01:18:52.920
would understand that things that seem very simple and homogeneous

01:18:52.920 --> 01:18:56.160
as it were, if you really pay close attention

01:18:56.160 --> 01:19:00.000
there are a lot more flickering and disjointed

01:19:00.000 --> 01:19:01.080
and unintegrated.

01:19:01.080 --> 01:19:04.520
And there's a lot of structure there that can come apart.

01:19:07.120 --> 01:19:09.800
And I think that as we move away from the paradigm cases

01:19:09.800 --> 01:19:14.440
of consciousness, like a normal waking human paying attention,

01:19:14.440 --> 01:19:19.360
then properties that we think go together come apart.

01:19:19.360 --> 01:19:21.880
And then it becomes more like a verbal question,

01:19:21.880 --> 01:19:24.040
which set of those properties you need to have in order

01:19:24.040 --> 01:19:28.240
to apply the label consciousness correctly.

01:19:28.240 --> 01:19:30.160
Next question back there.

01:19:30.160 --> 01:19:31.280
Hello.

01:19:31.280 --> 01:19:32.600
First of all, thank you, Juan.

01:19:32.600 --> 01:19:35.960
Thank you, Nick, for a really brilliant discussion

01:19:35.960 --> 01:19:40.040
on the topic of artificial and superintelligence.

01:19:40.040 --> 01:19:40.800
My name is Alex.

01:19:40.800 --> 01:19:43.160
I'm a CO at Collective Technologist Labs.

01:19:43.160 --> 01:19:49.360
And I want to ask you what is your opinion on maybe

01:19:49.360 --> 01:19:51.600
the breakthrough in superintelligence

01:19:51.600 --> 01:19:57.200
lays in the combination and symbiosis

01:19:57.200 --> 01:19:59.840
of human intelligence and artificial intelligence

01:19:59.840 --> 01:20:02.440
and not just artificial intelligence?

01:20:08.120 --> 01:20:13.120
I think if you sort of squint a little,

01:20:13.120 --> 01:20:16.920
you could say that that's kind of the state of play today,

01:20:16.960 --> 01:20:20.720
where we don't have an individual system that

01:20:20.720 --> 01:20:24.560
is superintelligent, but you could have humanity as a whole

01:20:24.560 --> 01:20:27.160
or some big collective, like a large corporation

01:20:27.160 --> 01:20:30.800
or the scientific community that is, at least in certain

01:20:30.800 --> 01:20:34.360
respects, superintelligent in that they can perform

01:20:34.360 --> 01:20:37.480
a wide range of tasks at a much higher level

01:20:37.480 --> 01:20:40.720
than an individual human, but not all tasks.

01:20:40.720 --> 01:20:43.520
So that's why it's not like a perfect example.

01:20:43.520 --> 01:20:45.920
But yeah, and so some of these systems we have today

01:20:45.920 --> 01:20:51.760
are certainly hybrids between biological brains,

01:20:51.760 --> 01:20:55.480
information technology systems, like the internet,

01:20:55.480 --> 01:21:00.360
social networks, depositors of papers,

01:21:00.360 --> 01:21:02.000
and then a lot of culture as well.

01:21:02.000 --> 01:21:09.640
That kind of, you could almost see these phenomena,

01:21:09.640 --> 01:21:14.800
you start to get more and more where you get the current thing

01:21:14.800 --> 01:21:19.080
and where there is a particular focus of attention

01:21:19.080 --> 01:21:21.040
of the global brain, it's becoming more and more

01:21:21.040 --> 01:21:23.880
like a human who's obsessed for a period of time

01:21:23.880 --> 01:21:26.920
with some particular thing, and all the mental resources

01:21:26.920 --> 01:21:29.360
get focused at one thing, and then your attention

01:21:29.360 --> 01:21:32.800
shift to something different.

01:21:32.800 --> 01:21:35.360
We're beginning to see a little bit of those dynamics

01:21:35.360 --> 01:21:39.240
kind of happening in our collective cognitive space,

01:21:39.240 --> 01:21:42.000
maybe as a result of the increased bandwidth

01:21:42.000 --> 01:21:44.040
of interaction, and the technology

01:21:44.040 --> 01:21:51.960
kind of enabling smoother communication,

01:21:51.960 --> 01:21:54.520
not always producing superintelligence,

01:21:54.520 --> 01:21:57.080
but other forms of kind of collective mentality

01:21:57.080 --> 01:22:04.680
that sometimes may be subintelligence in terms

01:22:04.680 --> 01:22:08.080
of their level of wisdom and understanding.

01:22:08.080 --> 01:22:11.400
But yeah, in certain domains, you certainly

01:22:11.520 --> 01:22:12.880
have a research community that's

01:22:12.880 --> 01:22:17.000
target-focused on one particular problem that

01:22:17.000 --> 01:22:19.240
are building on each other's contributions and blogs,

01:22:19.240 --> 01:22:22.920
and you do get the sense of the whole being kind of,

01:22:22.920 --> 01:22:24.240
there have been many different modules

01:22:24.240 --> 01:22:25.920
that are each looking for the next way

01:22:25.920 --> 01:22:28.320
to put a piece on the stack that is being built together,

01:22:28.320 --> 01:22:29.960
and the whole stack goes up much faster

01:22:29.960 --> 01:22:34.840
than if it were only one human building it.

01:22:34.840 --> 01:22:38.400
Right, next question from Twitter.

01:22:38.400 --> 01:22:40.880
Turner asks, what is the most important question,

01:22:40.880 --> 01:22:43.440
which Nick feels he's not in a position

01:22:43.440 --> 01:22:44.880
to personally solve?

01:22:44.880 --> 01:22:46.640
Two factors, first being importance

01:22:46.640 --> 01:22:50.200
to the development of ethical and successful AGI,

01:22:50.200 --> 01:22:51.960
and second being Nick's inability,

01:22:51.960 --> 01:22:53.440
lack of expertise to solve.

01:22:58.120 --> 01:23:03.680
Well, I mean, there are questions

01:23:03.680 --> 01:23:09.280
of more global nature, as in, ultimately, what

01:23:09.280 --> 01:23:11.720
is the right direction to go in,

01:23:11.720 --> 01:23:15.840
as it were the ultimately correct macro strategy.

01:23:15.840 --> 01:23:20.040
I think we are sort of fundamentally in the dark,

01:23:20.040 --> 01:23:24.280
regarding a lot of the ultimate and big-picture questions,

01:23:24.280 --> 01:23:27.680
and that therefore our march forward

01:23:27.680 --> 01:23:31.560
is, to some extent, an act of faith,

01:23:32.880 --> 01:23:37.880
rather than the product of carefully thought-through insight.

01:23:38.040 --> 01:23:40.760
Then I'm not sure we can get that insight at the moment,

01:23:40.760 --> 01:23:45.080
and so that's one direction at which, at some point,

01:23:45.080 --> 01:23:46.760
my understanding runs out,

01:23:46.760 --> 01:23:49.920
and there's probably important stuff beyond that

01:23:49.920 --> 01:23:52.720
that may or may not be good for us to try to reach,

01:23:52.720 --> 01:23:54.800
but it's probably there in one way or another.

01:23:54.800 --> 01:23:57.600
Another would be at a more technical level,

01:23:57.600 --> 01:23:59.840
if you sort of zoom in and narrow it down,

01:23:59.840 --> 01:24:03.320
so then a lot of stuff, say, for example, with AI alignment,

01:24:04.160 --> 01:24:08.240
there's going to be a whole host of really important,

01:24:08.240 --> 01:24:12.200
ultimately, technical results and algorithms

01:24:12.200 --> 01:24:16.920
and stuff like that, that maybe currently nobody has,

01:24:16.920 --> 01:24:18.160
and certainly I don't have,

01:24:18.160 --> 01:24:23.160
and I probably won't discover them either,

01:24:23.360 --> 01:24:28.360
but that might be critical to the future.

01:24:29.200 --> 01:24:34.200
Then I guess you could zoom out in another direction,

01:24:35.200 --> 01:24:37.480
sort of laterally, like across the social sphere,

01:24:37.480 --> 01:24:42.480
so there are big problems like how to secure world peace

01:24:43.000 --> 01:24:48.000
or like a welcoming uptake of these digital minds

01:24:48.280 --> 01:24:52.760
that then involve problems at the cultural

01:24:52.760 --> 01:24:54.680
and communication and political level,

01:24:54.680 --> 01:24:57.920
where also one feels, I feel quite stumped

01:24:57.920 --> 01:25:02.920
and it will, you know, so I'm kind of,

01:25:04.000 --> 01:25:05.640
I'm squeezed in the middle of thought,

01:25:05.640 --> 01:25:09.160
like if you zoom out too much, my understanding was out,

01:25:09.160 --> 01:25:12.640
if you zoom down too much into the technical understanding

01:25:12.640 --> 01:25:15.520
was out, and if you zoom out laterally,

01:25:15.520 --> 01:25:17.880
also it's a little bubble there,

01:25:17.880 --> 01:25:21.480
or I'm trying to keep track of what's going on.

01:25:22.840 --> 01:25:24.720
All right, Addy asks,

01:25:24.720 --> 01:25:26.480
if the speed of light would accelerate,

01:25:26.480 --> 01:25:29.520
does this prove the theory we are living in a simulation,

01:25:29.520 --> 01:25:32.640
and if no, what quantitative metric would validate the theory?

01:25:37.440 --> 01:25:38.960
If the speed of light accelerates,

01:25:38.960 --> 01:25:43.800
I don't see how that would,

01:25:43.800 --> 01:25:45.120
it certainly wouldn't imply it,

01:25:45.120 --> 01:25:48.440
I'm not sure immediately whether it would increase

01:25:48.440 --> 01:25:50.520
or decrease the probability somewhere.

01:25:51.440 --> 01:25:55.160
Maybe thinking about some marker that shows

01:25:55.160 --> 01:26:00.160
that some kind of discontinuity on some quantity of physics

01:26:01.080 --> 01:26:03.840
that just seems like bizarre to us or something.

01:26:13.560 --> 01:26:17.280
So there are a lot of things that could change in physics

01:26:17.280 --> 01:26:21.840
that would maybe be in one sense puzzling

01:26:21.840 --> 01:26:22.800
and deep and interesting,

01:26:22.800 --> 01:26:26.560
but ultimately, simple,

01:26:27.480 --> 01:26:30.600
that there would be some possible physical law

01:26:30.600 --> 01:26:33.160
that is itself simple that would describe them.

01:26:34.160 --> 01:26:35.720
Now, you can contrast that,

01:26:37.240 --> 01:26:38.960
and then of course you can have situations

01:26:38.960 --> 01:26:39.920
where it's just chaotic,

01:26:39.920 --> 01:26:42.360
but you could still capture the statistical regularities

01:26:42.360 --> 01:26:43.720
through simple statistical law,

01:26:43.720 --> 01:26:46.920
like that that's one type of basic universe we could live in,

01:26:46.920 --> 01:26:51.280
which so far everything we know seems to be consistent with.

01:26:52.800 --> 01:26:54.840
Now contrast that with a different possible world,

01:26:54.840 --> 01:26:56.480
which we could have lived in

01:26:56.480 --> 01:26:57.960
and we could still find out that we do,

01:26:57.960 --> 01:27:01.800
where maybe you would have like parapsychology would be true.

01:27:01.800 --> 01:27:05.000
So you would have like telekinesis or something,

01:27:05.000 --> 01:27:08.360
where like what we think of as a high level,

01:27:08.360 --> 01:27:10.800
complex macro state,

01:27:10.800 --> 01:27:13.280
like a particular brain in a particular configuration,

01:27:13.280 --> 01:27:15.080
but not in a slightly different configuration,

01:27:15.080 --> 01:27:17.240
but just the types of configurations

01:27:17.240 --> 01:27:20.240
that corresponds to somebody having a particular concept

01:27:20.240 --> 01:27:21.080
and wish,

01:27:21.080 --> 01:27:26.080
if that had like say a systematic physical impact

01:27:26.680 --> 01:27:28.240
on some remote system,

01:27:28.240 --> 01:27:31.600
like the way that parapsychologists have imagined,

01:27:31.600 --> 01:27:33.800
like that would be puzzling,

01:27:33.800 --> 01:27:37.400
not just because it would be fundamentally different

01:27:37.400 --> 01:27:40.160
from like discovering that the speed of light is accelerating,

01:27:40.160 --> 01:27:42.640
because it would be the thing that if it were true,

01:27:42.640 --> 01:27:45.480
would seem to suggest that there were no micro level

01:27:45.480 --> 01:27:47.200
explanation of the world.

01:27:47.200 --> 01:27:48.680
Like you could have these macro states

01:27:48.680 --> 01:27:52.400
that suddenly could like reach down and change the micro.

01:27:52.400 --> 01:27:55.160
So if we made some discovery like that,

01:27:55.160 --> 01:27:56.720
that then might,

01:27:57.800 --> 01:28:01.320
yeah, lend evidence and credence

01:28:01.320 --> 01:28:02.960
to the simulation hypothesis,

01:28:02.960 --> 01:28:05.280
because that it looks very hard

01:28:05.280 --> 01:28:07.760
to see how you could get all of this to square up,

01:28:07.760 --> 01:28:08.920
maybe without that,

01:28:08.920 --> 01:28:11.600
if you still wanted to have an underlying micro level

01:28:11.600 --> 01:28:12.440
regularity,

01:28:12.440 --> 01:28:14.920
you could have like the simulating universe

01:28:14.920 --> 01:28:17.600
being kind of simple at the physics level,

01:28:17.600 --> 01:28:20.440
but then simulating a different kind of universe.

01:28:22.840 --> 01:28:25.320
The alternative would just be that we didn't have

01:28:25.320 --> 01:28:27.640
that simplicity at the level of basic laws,

01:28:27.640 --> 01:28:30.440
which I guess we could discover.

01:28:30.440 --> 01:28:32.160
Now, I don't think that's the most,

01:28:32.160 --> 01:28:35.760
the only or the most likely way we would find evidence

01:28:35.760 --> 01:28:36.880
for the simulation argument,

01:28:36.880 --> 01:28:38.720
if we offer the simulation hypothesis,

01:28:38.720 --> 01:28:39.560
if we do,

01:28:39.560 --> 01:28:40.480
that would just be one way,

01:28:40.480 --> 01:28:41.880
like that would be more,

01:28:42.880 --> 01:28:44.120
yeah, other kinds of evidence

01:28:44.120 --> 01:28:46.200
that would be more likely to be relevant.

01:28:46.320 --> 01:28:49.360
Since we're touching on the simulation argument,

01:28:49.360 --> 01:28:51.560
which of the three hypotheses do you think

01:28:51.560 --> 01:28:53.760
is the most likely just that,

01:28:53.760 --> 01:28:56.080
sorry, which of the three prongs of the argument

01:28:57.520 --> 01:28:59.360
do you currently think is most likely?

01:29:00.360 --> 01:29:03.200
I'm generally a bit coy

01:29:04.160 --> 01:29:06.800
in attaching probabilities to that.

01:29:06.800 --> 01:29:11.800
So yeah, I tend to punt that question for various reasons,

01:29:12.240 --> 01:29:13.960
including if I give a particular number

01:29:14.040 --> 01:29:16.320
that might be misinterpreted,

01:29:16.320 --> 01:29:18.680
but yeah, I mean, I would like,

01:29:18.680 --> 01:29:20.040
so normally what people want to know

01:29:20.040 --> 01:29:23.960
is especially on the simulation hypothesis,

01:29:23.960 --> 01:29:26.040
like that's the one that I really want to know.

01:29:26.040 --> 01:29:27.480
And as I mean, I guess,

01:29:27.480 --> 01:29:29.240
yeah, I want to attach a probability to it,

01:29:29.240 --> 01:29:30.480
but I certainly take it seriously.

01:29:30.480 --> 01:29:34.840
It's not just like a logical possibility

01:29:34.840 --> 01:29:38.840
or a thoughts experiment that we can't 100% rule out,

01:29:38.840 --> 01:29:43.280
but it would certainly be like a live serious possibility

01:29:43.280 --> 01:29:44.560
in my view.

01:29:44.560 --> 01:29:46.560
Yeah, and for those unfamiliar,

01:29:46.560 --> 01:29:51.040
the simulation argument is a three prong argument about

01:29:51.040 --> 01:29:53.280
there's either we have a great filter,

01:29:53.280 --> 01:29:56.440
meaning we have like close to zero advanced civilizations.

01:29:56.440 --> 01:30:00.360
Either we have a disinterested set of advanced civilizations

01:30:00.360 --> 01:30:02.760
where close to zero are interested

01:30:02.760 --> 01:30:04.680
in running those simulations.

01:30:04.680 --> 01:30:07.040
And then there's a simulation hypothesis,

01:30:07.040 --> 01:30:09.440
which is that, hey, if there's no great filter

01:30:09.440 --> 01:30:10.760
and they are interested,

01:30:10.760 --> 01:30:12.880
then close to all beings are simulated.

01:30:12.880 --> 01:30:16.080
And this comes from thinking about just the vast number

01:30:16.080 --> 01:30:20.120
and vast quantities of people that would be simulated

01:30:20.120 --> 01:30:23.400
and then the likelihood of your experience

01:30:23.400 --> 01:30:26.240
being sampled from the simulated ones.

01:30:26.240 --> 01:30:29.660
Sorry, Nick, I'm probably giving you a bad explanation here,

01:30:29.660 --> 01:30:30.500
but...

01:30:30.500 --> 01:30:31.960
No, no, that's very good to hear.

01:30:33.800 --> 01:30:36.880
I think that was another question over here, or yeah.

01:30:36.880 --> 01:30:37.760
Yeah, I have a question.

01:30:37.760 --> 01:30:41.440
So I've always been very interested in emergent intelligence,

01:30:41.440 --> 01:30:42.760
especially as it relates to animals.

01:30:42.760 --> 01:30:45.320
I mean, the classic example tends to be beehives.

01:30:45.320 --> 01:30:46.440
As we look at consciousness,

01:30:46.440 --> 01:30:48.300
what biases do you think we bring in

01:30:48.300 --> 01:30:50.440
as individual social animals, humans,

01:30:50.440 --> 01:30:52.400
versus a collectible organism like bees,

01:30:52.400 --> 01:30:53.480
especially as we look at humans,

01:30:53.480 --> 01:30:54.800
maybe moving to be more bee-like

01:30:54.800 --> 01:30:57.160
as we create nation states and larger organizations,

01:30:57.160 --> 01:30:58.240
versus a singleton?

01:30:58.240 --> 01:31:00.520
How would a singleton perhaps have a different AI alignment

01:31:00.520 --> 01:31:02.120
bias?

01:31:02.120 --> 01:31:02.960
As I think about this,

01:31:02.960 --> 01:31:04.680
the only really intelligent animals I can think of

01:31:04.680 --> 01:31:06.420
that don't live socially are apex predators,

01:31:06.420 --> 01:31:08.040
which is perhaps a bad sign.

01:31:12.760 --> 01:31:13.960
Let me see if I understand.

01:31:13.960 --> 01:31:18.120
So I think, well, so one question...

01:31:18.120 --> 01:31:19.120
To phrase this differently,

01:31:19.120 --> 01:31:20.760
if I think about a curve,

01:31:20.760 --> 01:31:22.680
do you think that collective intelligences

01:31:22.680 --> 01:31:25.760
like hive animals are on one side of the spectrum

01:31:25.760 --> 01:31:28.080
with social animals like humans in the middle,

01:31:28.080 --> 01:31:29.520
with singletons being on another extreme,

01:31:29.520 --> 01:31:31.320
or is it more of a horseshoe curve

01:31:31.320 --> 01:31:32.640
in terms of the distribution of intelligences

01:31:32.640 --> 01:31:34.080
and how they work towards common goals

01:31:34.080 --> 01:31:36.660
that may be 11 or not aligned with us?

01:31:38.780 --> 01:31:41.240
Well, if there were a line,

01:31:41.240 --> 01:31:43.840
I think the superintelligence would be more on the side

01:31:43.840 --> 01:31:46.480
of these hive insects.

01:31:46.480 --> 01:31:49.000
If we look at the scale of an ant colony,

01:31:49.000 --> 01:31:54.000
it's in some sense, it acts like a singleton within that.

01:31:54.320 --> 01:31:56.240
Of course, there are other ant colonies elsewhere

01:31:56.240 --> 01:31:58.160
and other things that it doesn't have control over,

01:31:58.160 --> 01:32:01.120
but they would, as it were,

01:32:01.120 --> 01:32:06.120
be able to act as a single agent to some extent.

01:32:06.960 --> 01:32:11.960
And humans, to only a lesser extent,

01:32:12.520 --> 01:32:15.480
although in some dimensions, we are better coordinated

01:32:15.480 --> 01:32:17.120
in terms of being able to share

01:32:17.120 --> 01:32:19.000
detailed information and plans.

01:32:19.000 --> 01:32:22.480
We are, in that respect, we are more coordinated than ants,

01:32:22.480 --> 01:32:26.480
but in the respect of our individual wills

01:32:26.480 --> 01:32:30.000
being less aligned to a common goal,

01:32:30.000 --> 01:32:35.000
we are less like a singleton than human.

01:32:36.520 --> 01:32:39.040
An ant colony is.

01:32:39.040 --> 01:32:42.280
And I guess you could have a group of animals

01:32:42.280 --> 01:32:44.520
that were even more individualistic

01:32:44.520 --> 01:32:46.000
and antisocial than humans are,

01:32:46.000 --> 01:32:50.020
and they would then be further away on the other side.

01:32:50.020 --> 01:32:51.720
So humans would kind of be in the middle

01:32:51.720 --> 01:32:56.200
where we have a fair degree of sort of shared purpose,

01:32:56.200 --> 01:33:00.240
but not like a full hive organism,

01:33:01.360 --> 01:33:02.960
but also a lot more than zero.

01:33:03.840 --> 01:33:08.840
It's, I guess, an interesting question.

01:33:09.040 --> 01:33:10.360
It's certainly different animals.

01:33:10.360 --> 01:33:13.760
I mean, have different goals, it seems,

01:33:13.760 --> 01:33:16.440
like some, I mean, at least at the superficial level,

01:33:16.440 --> 01:33:18.560
some like to eat grass and some like to eat meat

01:33:18.560 --> 01:33:21.400
and some like to hang around with others of their kind

01:33:21.400 --> 01:33:24.400
and some like to just do their own thing.

01:33:26.320 --> 01:33:28.880
And presumably if there were some other species

01:33:28.880 --> 01:33:30.480
that developed superintelligence

01:33:30.480 --> 01:33:33.320
and aligned it to their values,

01:33:33.320 --> 01:33:36.640
then they might also have different baseline goals

01:33:36.640 --> 01:33:38.480
that might overlap slightly with humans,

01:33:38.480 --> 01:33:42.180
like, but also be different in other respects.

01:33:43.800 --> 01:33:45.800
There are two open questions.

01:33:45.800 --> 01:33:47.720
One is like epistemically,

01:33:47.720 --> 01:33:49.740
is are there significant differences

01:33:49.740 --> 01:33:54.740
between the inductive biases that are brought to the table?

01:33:54.960 --> 01:33:56.600
Presumably there are some inductive biases

01:33:56.600 --> 01:33:57.440
that are different,

01:33:57.520 --> 01:34:01.640
but would those kind of be smoothed out reasonably fast

01:34:01.640 --> 01:34:04.360
as you have more data and more intelligence?

01:34:04.360 --> 01:34:05.520
Like it doesn't start,

01:34:05.520 --> 01:34:09.320
like it may be like a squirrel would more quickly cut on

01:34:09.320 --> 01:34:12.200
onto certain things that are relevant to the squirrel world

01:34:12.200 --> 01:34:13.600
and some other organism to another,

01:34:13.600 --> 01:34:16.560
but like as they develop scientific reasoning,

01:34:16.560 --> 01:34:18.240
like do they have enough overlap

01:34:18.240 --> 01:34:19.680
between their inductive biases

01:34:19.680 --> 01:34:21.920
that the difference is washed out

01:34:21.920 --> 01:34:24.480
as you see the full impact of the evidence?

01:34:24.480 --> 01:34:26.000
That that's one question you could ask.

01:34:26.000 --> 01:34:27.640
And like another is that even though

01:34:27.640 --> 01:34:29.560
these different organisms start out with

01:34:29.560 --> 01:34:31.440
at least two officially different goals,

01:34:31.440 --> 01:34:34.240
are they in some deeper sense,

01:34:34.240 --> 01:34:36.680
the same or alternatively,

01:34:36.680 --> 01:34:41.080
would they arrive at some shared understanding

01:34:41.080 --> 01:34:44.160
of what the highest moral norms are,

01:34:44.160 --> 01:34:47.000
even if their own personal goals might differ?

01:34:48.000 --> 01:34:50.000
Like a lot of humans might individually

01:34:50.000 --> 01:34:51.160
have different preferences,

01:34:51.160 --> 01:34:52.920
like I care about my family

01:34:52.920 --> 01:34:54.040
and you care about your family,

01:34:54.040 --> 01:34:56.000
but we might nevertheless converge

01:34:56.000 --> 01:34:59.200
in the sense of let's respect each other's families,

01:34:59.200 --> 01:35:00.760
let's say like a cooperative level

01:35:00.760 --> 01:35:04.280
of more abstract norms might also be convergent,

01:35:05.240 --> 01:35:07.200
quite independent of starting point.

01:35:08.640 --> 01:35:10.920
So those are two questions I could ask there

01:35:10.920 --> 01:35:15.920
that I'm not sure what the answer is, but I'm not,

01:35:16.440 --> 01:35:18.080
yeah, I don't know whether that addresses

01:35:18.080 --> 01:35:21.440
your question at all, but it's my effort.

01:35:21.440 --> 01:35:22.800
I'll have two more questions.

01:35:22.840 --> 01:35:27.680
One is how sure are you, Nick,

01:35:27.680 --> 01:35:29.960
that an evil singleton AI to rule them all

01:35:29.960 --> 01:35:32.160
would be internally aligned over time?

01:35:32.160 --> 01:35:34.680
Could it be fundamentally set up to split or diverge

01:35:34.680 --> 01:35:37.720
with subunits pursuing different ideals or goals?

01:35:42.880 --> 01:35:44.840
I guess everything is possible.

01:35:44.840 --> 01:35:49.840
I mean, if it were unified at one point in time,

01:35:50.000 --> 01:35:53.000
and if at that point it was technically mature,

01:35:53.000 --> 01:35:57.360
then I would expect it to remain unified

01:35:57.360 --> 01:35:59.840
because I think it would have access

01:35:59.840 --> 01:36:01.560
to the kind of control technology

01:36:01.560 --> 01:36:04.120
that would make it possible for it to do that,

01:36:04.120 --> 01:36:07.680
and I think it would have instrumental reasons to do that

01:36:07.680 --> 01:36:12.360
for almost all initial goals it might have at that time.

01:36:12.360 --> 01:36:13.880
You could imagine some very special goal,

01:36:13.880 --> 01:36:16.840
like if it specifically has as its top level goal

01:36:16.840 --> 01:36:17.960
a thousand years from now,

01:36:17.960 --> 01:36:20.440
I want to be divided against myself

01:36:20.440 --> 01:36:23.920
and fighting like an insurrection against myself.

01:36:23.920 --> 01:36:26.040
If that were its goal, then I should have arranged that,

01:36:26.040 --> 01:36:28.600
but for most goals, it would probably be able

01:36:28.600 --> 01:36:30.320
to achieve them to a higher degree

01:36:30.320 --> 01:36:32.920
if it worked in concert with itself.

01:36:32.920 --> 01:36:36.280
And then I'd imagine it would also have the technology

01:36:36.280 --> 01:36:38.320
and insights to make that happen.

01:36:39.760 --> 01:36:43.280
If it starts out unified, like if it starts out

01:36:43.280 --> 01:36:46.560
like a sort of vaguely politically integrated

01:36:46.560 --> 01:36:50.680
political entity, then it might be

01:36:50.680 --> 01:36:53.520
that even with technological maturity,

01:36:53.520 --> 01:36:57.080
it's not so crazy to think it might come apart at a later,

01:36:57.080 --> 01:36:57.960
just like humans do.

01:36:57.960 --> 01:37:01.440
Like sometimes you have a well-functioning political unit

01:37:01.440 --> 01:37:06.120
and then 50 years later, you have anarchy

01:37:06.120 --> 01:37:07.080
in a particular state.

01:37:07.080 --> 01:37:10.880
Like we can kind of get these temporary partial solutions

01:37:10.880 --> 01:37:14.520
that I guess would also be possible with certain kinds

01:37:14.520 --> 01:37:16.400
of like maybe some upload collective

01:37:16.400 --> 01:37:18.440
that comes together to achieve super intelligence.

01:37:18.440 --> 01:37:21.280
You could imagine political dynamics working well

01:37:21.280 --> 01:37:23.880
for a period of time and then it's falling apart.

01:37:23.880 --> 01:37:25.200
I still think that's less likely

01:37:25.200 --> 01:37:27.000
than it's going kind of towards a single time,

01:37:27.000 --> 01:37:30.840
but by no means extremely unlikely.

01:37:32.640 --> 01:37:34.440
And last question, if things go well,

01:37:34.440 --> 01:37:36.400
Devadat asks, if things go well,

01:37:36.400 --> 01:37:38.880
do you have a vision for how differences of opinion

01:37:38.880 --> 01:37:42.200
about what a good future society looks like?

01:37:42.200 --> 01:37:44.800
Sorry, if things go well, do you have a vision

01:37:44.800 --> 01:37:47.160
for how differences of opinion

01:37:47.160 --> 01:37:50.920
about what a good future society looks like

01:37:50.920 --> 01:37:52.760
can be accommodated?

01:37:52.760 --> 01:37:55.440
Meaning, is a like big enough for everyone

01:37:55.440 --> 01:37:58.560
as they develop very different perspectives

01:37:58.560 --> 01:38:01.560
and different ideas of what a good future society looks like?

01:38:01.560 --> 01:38:04.080
How do we kind of reconcile those differences of opinion?

01:38:04.080 --> 01:38:07.360
How do we build a meta system to kind of enable

01:38:07.360 --> 01:38:12.360
like different flourishing civilizations in a sense?

01:38:13.360 --> 01:38:18.360
Yeah, I think it's large enough for

01:38:21.360 --> 01:38:26.360
almost all people to have most of their values accommodated.

01:38:28.880 --> 01:38:32.880
Like if you have two people have literally opposed values

01:38:32.880 --> 01:38:34.360
about a particular thing,

01:38:35.640 --> 01:38:38.120
then you might not be able to satisfy both.

01:38:38.120 --> 01:38:42.040
But I think a combination of on the one hand,

01:38:42.160 --> 01:38:46.160
some differences being perhaps merely superficial,

01:38:47.560 --> 01:38:51.160
either disappearing up on better understanding,

01:38:51.160 --> 01:38:52.320
like there's like certain things

01:38:52.320 --> 01:38:55.040
where we just have ultimately different beliefs

01:38:55.040 --> 01:38:56.440
and we say we want different things,

01:38:56.440 --> 01:38:58.120
but it's because we have different assumptions

01:38:58.120 --> 01:39:00.920
about what would actually happen, let's say.

01:39:00.920 --> 01:39:03.720
So those being potentially diminished

01:39:03.720 --> 01:39:06.520
by increased intelligence and knowledge and experience,

01:39:07.840 --> 01:39:11.640
then the increase in resources

01:39:11.640 --> 01:39:14.440
and expansion of the technological frontier

01:39:14.440 --> 01:39:16.600
and then some kind of creativity

01:39:16.600 --> 01:39:21.600
and like figuring out clever ways of combining values.

01:39:21.760 --> 01:39:26.760
I think hopeful that a great deal can be accommodated

01:39:29.320 --> 01:39:34.080
because of these things, but not necessarily a hundred percent.

01:39:34.080 --> 01:39:36.240
And then it would be important to have

01:39:37.240 --> 01:39:42.240
a robust and effective way to manage any resulting disagreements

01:39:44.920 --> 01:39:47.960
in a way that doesn't result in like negative,

01:39:47.960 --> 01:39:49.680
some dynamics.

01:39:49.680 --> 01:39:51.240
And so hence,

01:39:51.240 --> 01:39:53.240
because I think that's ultimately really important.

01:39:53.240 --> 01:39:57.480
I'm like, I think we should have a strong bias towards

01:39:57.480 --> 01:40:02.480
ads forward that are more cooperative and friendly.

01:40:03.160 --> 01:40:08.160
And even if they seem to come at some short term expense

01:40:09.520 --> 01:40:14.520
or if they can't be very crisply motivated

01:40:14.840 --> 01:40:17.880
by some explicit calculation in every single case,

01:40:17.880 --> 01:40:22.880
I think that general attitude as a sort of default bias,

01:40:23.040 --> 01:40:28.040
I think is still very much worth bearing in mind

01:40:28.680 --> 01:40:32.320
as we are pursuing these different aspects

01:40:32.320 --> 01:40:34.520
of the challenges ahead.

01:40:35.920 --> 01:40:37.360
That should be our first resort.

01:40:37.360 --> 01:40:40.240
Sometimes you have to, you can't get full cooperation.

01:40:40.240 --> 01:40:43.280
You don't want to be completely naive and gullible.

01:40:43.280 --> 01:40:46.840
And but still like that, that should be the first

01:40:46.840 --> 01:40:48.800
and maybe the second attempt.

01:40:48.800 --> 01:40:50.600
And then gradually scale back from that

01:40:50.600 --> 01:40:53.360
if really forced by circumstances.

01:40:54.360 --> 01:40:57.960
Well, that's all the time we have for questions.

01:40:57.960 --> 01:41:01.600
Nick, thank you so much for spending this evening with us.

01:41:01.600 --> 01:41:04.040
It has been extremely enlightening for many of us.

01:41:04.040 --> 01:41:07.200
And I think we'll be very useful to the broader community

01:41:07.200 --> 01:41:09.640
that is currently working on things like AI alignment

01:41:09.640 --> 01:41:10.480
and others.

01:41:10.480 --> 01:41:12.960
And thank you really much for your work,

01:41:12.960 --> 01:41:16.760
for sharing your insights and for helping us achieve

01:41:16.760 --> 01:41:18.120
a lot of great breakthroughs

01:41:18.120 --> 01:41:20.320
and hopefully have a great long-term future.

01:41:20.320 --> 01:41:21.160
Thank you very much.

01:41:21.400 --> 01:41:22.560
A lot of good questions.

01:41:22.560 --> 01:41:24.080
Thank you very much for having me.

01:41:24.080 --> 01:41:24.920
Yeah, absolutely.

01:41:24.920 --> 01:41:25.920
Thanks. Thank you.

01:41:25.920 --> 01:41:26.760
Take care.

