We're joined next by Greg Brockman, President, Chairman and Founder of Open AI and Alexander
Wang, CEO and Founder of Scale AI.
Open AI is a research and deployment company whose mission is to ensure general-purpose
artificial intelligence benefits all of humanity.
For Open AI, Greg was the CTO of Stripe, which he helped build from 4 to 250 employees.
Please join me in welcoming to the stage Greg Brockman and Alexander Wang.
Hey Greg.
Hey.
Thanks for making it.
Absolutely.
I'd be happy to be here.
I want to start actually, I don't know if you remember this, but we first met at this
summer camp called Spark where you gave a presentation about, at the time you were the
CTO of Stripe and you gave this presentation about sort of like everything you had accomplished
and I was a member of that camp and it was extremely memorable.
You had a lot of good sound bites.
I'm glad that it landed.
Kind of a full circle moment.
Well I think to start out with, I mean you've been CTO and now you're President of Open
AI, but CTO of two incredibly iconic companies, Stripe and Open AI, in some ways probably two
of the most iconic startups of the past decade.
I wanted to start out just by asking in what ways are the two organizations the same and
being CTO the same and in what ways are they different?
Thank you for the kind of words.
I think that one thing that's very interesting to me about kind of having been part of both
of these organizations is seeing how much groups of people are kind of the same regardless
of what the problem in front of you is.
So I think that a lot of how we approached Stripe was thinking from first principles.
I remember when we were pre-launch and we had some buzz going because we had some early
customers and one of my friends took me out to launch, he was like all right, I've been
hearing about this Stripe thing, what's your secret sauce?
I was like, I mean we just make payments really good and he's like no, no, no, come on, you
can tell me, what's the secret sauce?
And really that was the secret sauce, right, is that we had just rethought every single
piece of what we were doing from the ground up from first principles, not sort of locked
into the way that people had been doing it and we asked how should it be, like where's
all the pain and does it need to be there?
I think that in AI we did much the same, we really thought about okay, there's this field
that we're entering and that we hire a lot of people who had been in the field, but a
lot of us also hadn't been in the field and we came to it with beginner's eyes and I think
that that approach of just not being beholden to all the ways people were doing it, but
also becoming expert in the way that things have been done because if you just throw everything
out, like you're also just going to be starting from scratch in a not helpful way.
So I think that that maybe is the deepest commonality between them.
But obviously very different organizations, for Stripe I think that we ran the traditional
startup playbook.
You basically come up with the innovation and you just build, build, build.
You get in front of customers from day one, the story is that we gave the first API to
a customer who charged a credit card and he was like, I would like my money now please
and we were like, huh, I guess we should build that.
Open AI, we had research to do, like where's the customer?
And it really took us I guess five years, starting late 2015 and it's really not until
2020 that we had our very first product.
And so I think that that sort of figuring out like what you're even supposed to work
on, like did you do a good job, should you feel good on a day-to-day basis?
I think that all of that had to come from within rather than from without.
Yeah.
Well, actually I want to go back to this point that you mentioned around first principles
thinking.
It's very interesting because even I remember like maybe 2020 or 2021, you know, you would
sort of post-GPD3, you would talk to other researchers in the field and even they would
still, you know, there's still like some degree of skepticism over the sort of like core concept
of scaling up these models and if there were still gains to be had, et cetera.
And I think, you know, I don't know the story but it seems like the sort of research sort
of intuition that led to GPD3, Dolly 2 that have really ushered in kind of a new era of
AI were probably, you know, somewhat against the grain or somewhat unintuitive at the time.
You know, one question I have for you is, you know, I think now looking back it's obviously
very obvious to point out GPD3, Dolly 2 basically have fundamentally accelerated AI progress
and its relevance to the world and its relevance to every industry and sort of have created
the sort of most recent AI wave.
How is that matched up against your expectations when you were building these technologies?
You know?
Yeah.
Well, I think the thing that's most interesting to me is that those models you mentioned are
kind of overnight successes that took many, many years to create.
And so, you know, from the outside it looks like, wow, you just like produce this model
and that model and really on the inside, the GPT arc, that's a five-year arc, right?
It really started with sentiment neuron paper which is back in 2017.
Do you remember that paper?
I remember the paper.
It was very cool.
Yeah.
But it felt very novel.
It felt very novel.
Yeah.
Very few people remember it.
It's, you know, it was this very early result where we basically had been training a LSTM
at the time to predict the next character in text.
So, we basically showed a bunch of Amazon reviews.
We said, what's the next character and of course it's going to learn where the commas
go.
Where the periods go.
But of course it's not going to understand anything.
But we found a single neuron in that model that had learned a state-of-the-art sentiment
analysis classifier.
I can tell you this is a positive review or negative review.
That's understanding.
You know, I don't know what understanding means but it's semantics for sure.
And that for us was like, okay, this is going to work.
The transformer came out late 2017 and my co-founder Ilya immediately was like, that's
the thing.
That's what we've been waiting for.
So, you take this sort of very early nascent result, put in a transformer and then that's
GPT-1.
GPT-2 is you just keep pushing it.
And you know, I think that the algorithm we kind of run internally is that we do these
little sort of get signs of life and you have to be very, very careful to distinguish signs
of life from like kind of just pushing too hard on a specific data set that isn't really
going to keep going.
But if you kind of build those right intuitions then you know, okay, now is the time to put
in more compute.
Now is the time to put in more researchers.
Now is the time to like really scale it up.
And so GPT-2 obviously was exciting and that we were all like, well, we look at the curves.
You know, the bigger we made this model, the more compute we put in, the more data we put
in, the more we just sort of got all the engineering details right, those curves just got better.
And so actually, you know, our goal was just to break the paradigm.
It was just push it until the curve stopped looking good and we still haven't managed
to accomplish it.
Yeah.
Well, I think one of the things, at least for me and probably for many people who initially
played with GPT-3, the like shocking thing was not, I mean, it wasn't necessarily that
even the model got better and better performance on, you know, established tasks is that it
sort of had all these qualitatively new behaviors that were felt very magical.
And even now, you know, there's prompts that, you know, you'll see on Twitter or whatnot
that are sort of really shocking.
I mean, did you have these sort of like early moments when like you had the early model
results were like, holy crap, this is like, this is magic.
Yeah.
Well, I think that the earliest one that I remember was around code.
I mean, just, you know, at the time, totally mind blowing that you could just write a function
name and a doc string kind of describing what the function should do and actually write
it.
Not super complicated functions, right?
But just that it was able to, you know, you ask for, you know, something to take a couple
lines and they would be able to really do it.
You modify things a little bit to make sure it hadn't just memorized it.
Make sure enough that it would write out the modified code.
And I think, you know, the overall thing that's really interesting about the paradigm of a
GPT-3 is that where it really comes from is that I, you know, we kind of had this picture
that, look, the problem with these models is that they're great within their data distribution.
But as soon as you're outside of that distribution, like all bets are off.
And so what if you just make the whole world, the whole universe, be the data distribution?
You put the whole internet in there.
And I think that what we've really seen is that these models, that they really are able
to generalize extremely well within the kinds of things that they've seen.
You know, again, different question if it's never seen anything like it.
I mean, humans are also not very good at things you've never seen before.
But I think that that picture of just, like, all of the different things that it's seen
in all these different configurations is almost unimaginable.
There's no human who's been able to consume, you know, 40 terabytes worth of text.
And so I think that we just keep seeing surprises where you just ask for, one of my favorite
ones actually was this teacher-student interaction where I was the teacher model as the student
and I managed to teach it how to sort numbers.
And you just kind of have these experiences where, like, that's what it should be like
to interact with an AI.
Yeah.
I mean, it's incredibly shocking.
You know, one of the things I'm curious to get your thoughts on is, I think, in the path
of developing GP3, you know, required, I think probably the jump from GP2 to GP3 required
a lot of conviction because, you know, you all were spending probably a fair amount on
a compute at the time to be able to train these models and there were probably a lot
of experiments that didn't work and so you had to be willing to keep going after it.
Did that phase of the journey, sort of this, like, GP2 to GP3 jump, was it scary?
Did you have doubts?
Or were you very confident that, hey, you know, we're going to scale this up and even
though we're going to not get it right the first few times, it's going to be amazing?
Yeah.
And to your point that scale was not an obvious thing, not the company, but the scaling things
up, at the time, the funny thing is actually our very first scale result that just sort
of convinced us that this is the right way to approach things.
You push it until it breaks.
Not necessarily that more compute is just magically always going to solve your problem.
It was Dota.
That was, you know, playing competitive video games and there we kind of went through this
whole, that was a three-year arc where we started out with something that didn't do
anything, finally beat, like, you know, the in-house team, then we managed to go beat
the pros and at each step it was just kind of pushing in all dimensions, right, is make
the model bigger, it's to, you know, sort of, again, fix all the bugs and you just kind
of keep iterating on every single dimension and every single dimension yields returns.
And so I think that we did very much the same thing where for GP2, you know, it's not as
simple as saying, okay, like, clearly you just need to, like, you know, crank up this
one variable and you just do it in one shot.
It's this, like, sort of iterative, like, stepping through the space on each axis at
every single time.
And so I think that on the one hand it does require conviction because you do need to
say we're going to, like, carve out a big compute budget so that you're not constantly
not kind of fighting other people for the big supercomputers, but on the other hand,
I think it's also very iterative and you don't have to make scary irreversible decisions
because in each step you get feedback from reality.
And I think that that key of, like, both the big picture, thinking of what if this works
and make sure that you're really set up for success, but also don't blindly spend a year
of your organization on just, like, pursuing a thing that might not pan out.
I think that balancing those two is what was really key.
Yeah, I mean, one of the cool things is you sort of walk through this and talk through
the insights is that the sort of organizational learnings were really critical in this entire
sort of path-dependent sort of path to GP3.
It sort of, you know, it makes sense when you say it that sort of insights from Dota 2
and insights from the sentiment neuron were sort of like the key, these were like the
key nuggets that led to the sort of, like, crystallized idea of, you know, scaling up
and building GP3, but it's very unintuitive from the outside and sort of, I think it's
almost a statement of innovation in some sense is that, you know, you're going to piece together
this sort of, like, disparate collection of insights that you gather from a wide variety
of experiments and eventually you sort of, like, get the ingredients together and you
build something.
Yeah, that's the first principle is thinking in action.
Yeah.
You know, I think that the story of AI, I don't know if you think about this at all, but I
think about this a little bit, I think the story of AI to date and especially the past
few years and the story of open AI is probably going to be something that historians are
going to study for, you know, decades and decades to come.
Are there any fun stories from the journey of creating some of these foundation models
that you think deserve to be in the history books?
Well, I'll tell you my actual favorite story from the Dota days.
So, you know, we've been working on this system and, you know, actually the funny thing is
at the very beginning we wrote down our list of milestones.
On this date we're going to be Jonas, our best open AI employee who also had many thousands
of hours of Dota 2 gameplay.
This date we're going to beat the semi-pros, you know, this date we're going to beat the
pros and so it's supposed to be like June 6th or something, June 6th rolls around, we
don't have anything.
Like, you know, he just crushes us and two weeks go by, three weeks go by, we keep pushing
back that deadline by a week every week and then one day we actually do beat him.
And you know, I think my conclusion was that like it wasn't actually actionable to sort
of set those goals of outputs, you can only control your inputs, you can control the experiments
you run.
And so we just managed the project very differently after that and the thing that was so crazy
to me still is that, you know, so a week before the international, which is the like world
championships we're going to show up, we're going to play 1v1 against the best players
in the world, we finally started beating our semi-protester and we're like, okay, maybe
this is actually going to happen.
But then we learned that he actually was on like vacation, he didn't have his like real
setup and so we're like, oh no, like this is not going to go well.
So we show up, you know, we continue to train, we like kind of do like a Hail Mary of like
scaling things up, biggest scale we've ever done and we show up at the international and
we play against, you know, like sort of low lowest, you know, low ranked pro, like a previous
pro and we go 3-0-3-0-2-1.
So we basically win-win and then we did have one loss and we take a look at it and it's
like this item that we've never trained with, we've never seen before, oh wow, okay, we
need to add that and, you know, do it fast.
And so the team stays up all night putting this thing into the training, getting the
whole thing launched and, you know, again, like we did double the scale where we're basically
maxing out our CPU cores at this point and start training and, you know, we're supposed
to play against the top pros in the world, fortunately they can't do the next day so
we get an additional day of training and the number two person comes in, he plays against
us and we win-win-win-win-win and he's like, okay, but I beat this but the top player is
never going to lose to this thing or sorry, yeah, the top player is going to crash this
thing and fortunately because he had spent so long playing that that guy couldn't come
that day so we got one more day of training and that one more day of training was enough
and so I think it's just the story of like you can really see the improvement and at
each step we could see new behaviors that the system have learned and I think that that
experience of just sort of watching a girl up in front of you is just something that
was really amazing.
I'm actually surprised because you would, I sensibly you'd probably train the sort
of like agents for a long, long, long time going into the international and surprised
at each incremental day.
Yeah, so this is something I think has changed over time so at the time we basically had
two weeks worth of training was like the whole model run and so you'd start from scratch
each time and the thing that was really funny in the middle was that, you know, we put in
this new item, we were training it and when we took out of training it was the best spot
we ever saw except that our semi-protester was looking at it and was like this bot is
doing something really dumb, it's just sitting there in the first wave and taking all this
damage it doesn't have to, I'm going to go beat it, he ran it and go fight it and he
lost.
It's like that was weird and he did it like five more times and he lost each time but
then he figured out a strategy that actually does work which is you, you realize what was
going on was it was baiting him, it had learned to deceive, you know it actually learned that
what you do is that like you pretend oh I'm just a weak little bot, I don't know what
I'm doing and then you know a person comes in and you're just like smack.
And so the way you defeat that is that you actually, you don't fall for the bait, right,
you let the bot take all this damage and sit there and get weaker and then you finally
go in for the kill and so there we actually stitched together our good bot for the first
wave with the deceived bot thereafter and so there was a lot of this sort of like really
examining what was going on in the systems because it's such a limited domain, you know
it's a complicated domain but it's very, very interpretable.
It meant that we could observe behaviors like this and figure out how to engineer around
them but once we graduated from the 1v1 version of the game to the full 5v5 you know much
more like you know like competitive basketball or something rather than heads up, suddenly
all of our analysis of the behavior stopped working, right, that we used to have someone
who just literally would watch the bot play and be like oh we have this bug in the training
we got to go fix that, for 5v5 we just could not do that and I think that's kind of where
we've graduated as a field is that too when you look at GPT-3 and the mistakes it makes
sometimes people ask well why didn't it make that mistake and sometimes you can interpret
it but sometimes it's also a little bit like asking well you know why did you make a mistake
on some tests it's like well you think you know but like your explanation isn't always
very good and I think that to do complicated behaviors sometimes there's a very complicated
explanation.
Yeah.
Have you read this short story I think it's like the Lifecycle of Software Objects by
Ted Chang?
I think I have but I don't recall the details.
It's like it's about how they're these AI pets and they sort of like keep learning new
and new behaviors it's very reminiscent of describing these Dota agents.
Yeah yeah I think we'll see that kind of thing in our future somewhere.
I want to kind of go back you know one of the things we've known each other for many
years long before you know these foundation models and even before this competition Dota
2 and one thing I vividly remember is how sort of optimistic and confident you were
in sort of this sort of path of increasing and increasing AI capability you know sort
of I remember the time as maybe 2016, 2017 it felt very striking because it was sort
of like you know with these algorithms they're still pretty weak and you were always very
confident like oh yeah they're just going to keep getting better and better and better
and you know you're very a lot of confidence in that.
What were the things that back then gave you sort of the resolve or confidence in the
and the optimism in the technology?
Yeah I mean at some level you know to have that kind of belief and conviction and something
that hasn't happened yet it's a very intuitive thing.
I mean I remember when I was in school and showed up excited about doing NLP research
I went and tracked down an NLP professor and I was like please can I do some research for
you and he's like okay he shows me these like parsed trees and stuff and I look at that
and I was like this is never going to work right and you know to explain like why does
it feel like it's not going to work it just doesn't have the right properties right it
just felt like you're going to pour all this human like engineering and intuition and effort
into the system and I know I can't even describe how language works.
Yep.
Right it just feels like there's just something inherently missing but I think neural nets
have the opposite property.
Neural nets is very clear this is a system that absorbs data it absorbs compute it's
like a sponge that just like slurps everything up and so it has the right form factor but
the thing that's always been missing as well can you train it right?
Do you have enough data do you have enough compute do you have enough ability to like
have a learning algorithm that can shovel the stuff in efficiently in a way that it comes
out in some way that generalizes like that's the thing that's been missing and I think
what became kind of clear you know the field really I think got its most recent resurgence
in 2012 with the Alex in that paper and I think that there that was the first time where
you had a neural net that really just crushed a task right that it was like people had spent
decades on computer vision and suddenly it's like well I'm so sorry but this approach has
just supplanted you by this this massive gap and I think that you just started to see it
spread right that it was almost like you had these these all these disparate departments
and there was this wall that was being knocked down day after day and I think that when you
see a trend like that were things that have been long-standing and very deeply established
in these ways of thinking these great debates that have gone on for a long time and suddenly
you're seeing a repeated result that is consistent with the history I think that that for me
is maybe the most clear sign that it's like something is going to work and there's a real
sort of exponential that is waiting to unfold.
And then you know were there what were the what were the moments if any of of doubt and
you know let's let's chart the path I think open I start in 2016 yeah yeah I'd say December
2015 you know 2016 okay great December 2015 till now were there any moments of of doubt
in the technology or was it sort of always hey this is you know this is clearly the way
of the future yeah I mean I think that doubt is a strong word there's definitely moments
like I think to build something you you're always doubting right that you're always like
you've got to be questioning every single bit of your implementation like anytime you
see like a graph that's wiggling in a weird way you've got to go figure it out you can't
just be like I'm sure that the AI's will sort it out.
And so I think there was like lots of sort of tactical doubt lots of like sort of worries
that were not quite doing it right lots of like redoing the calculations to figure out
like hey how big of a model do you think you're going to need lots of mistakes for sure like
a good example of this is the scaling laws so we did this study to actually start to
really scientifically understand how do models improve as you push on various axes so as
you pour more computing as you pour more data in and one conclusion that we had at one point
was that basically that there's you know sort of a limited amount of data that you want
to pour into these models and that there's kind of this very this very clear curve and
that one thing that we realized only years later was actually that we'd read the curves
a little bit wrong and you actually want to be trained for way more tokens way more data
than anyone had expected and that did I you know that there's definitely these moments
where these things that just didn't quite click where it's like just didn't add up that
we were training for so little and that you know something conclusions that you drew downstream
but then you realize there was a foundational assumption that was wrong and suddenly things
make way more sense so I think it's a little bit like you know physics in some sense for
like do you do doubt physics it's like I kind of do I think all physics is wrong right
but like only so wrong right it's like we clearly haven't reconciled like quantum and
relativity is that there's like something wrong there but that that wrongness is actually
an opportunity it's actually a sign of you have this things are useful right really like
it's affected our lives and it's actually like pretty great I'm very happy with what
physics has done but also there's fruit and so I think that that for me that's always been
the feeling that there's something here and that you know if we do keep pushing and somehow
the scaling laws all peter out right and they suddenly drop off a cliff and we can't make
any further progress like that would be the most exciting time in this field because we
would finally reach the limit of technology we would finally learn something and then
we would finally have a picture of what the next thing to do is yeah that's super it actually
reminds me of this one of the stripe operating principles which is I think micro pessimists
macro optimists yep yep yes and it's very I mean it's very resonant but obviously like
very related to what you're talking about which is these you know you have to be extremely
pessimistic we're extremely questioning in the moments of the technology but then obviously
on a long enough time horizon incredible stuff pops out yep you got to be excited like I
think that this is just an exciting field and it's a scary field as well you got to have
some amount of just like awe at the fact that you have these these models that they started
as just random numbers right and then you have build these massive supercomputers these massive
data sets and you do a ton of the engineering work you do a ton of these algorithmic developments
you put them all into a package right and we don't really have other technologies that
work like this like I think the fact to me the most fundamental picture this like sponge
that you just kind of pour stuff into and you get this model it's reusable and works
across all these different areas like you can't do that for traditional software right
traditional software is it's just you know human effort writing down all the rules and
that's where the return comes from but you can't you know maybe you have like a spark
cluster that does some stuff but that's not that's not the cake and in in neural networks
it really is yeah you know I want to kind of switch gears to thinking about the the
sort of future and and and looking forward at what kind of what's next what do you think
I mean I'll ask this sort of as broadly as possible to start with what do you think the
future of AI holds yeah I think that the future of AI is going to again be both exciting and
a source of a lot of change and I think that that is something that you know part of our
mission is to try to help facilitate that as positive way as possible I think that kind
of you know the super high level I kind of feel like AI was like you know something that
for the you know 2020 10s was like kind of cool you know it's the game of like publishing
papers and you play some video games and like you know it's just like it's just like fun
good science I think it's really interesting that 2020 kicked off with GPT 3 which is really
the first model that was commercially useful just as the model like literally put an API
on top of it people just talk to it and people build products on top of it and you know that
you know one of our early customers just you know just raised it at 1.5 billion valuation
which to me is is a really wonderful thing to realize that you build this model and it
creates so much value for so many different people and I think that we're still in such
early days for what these models can do and so I think that what I'm most excited about
from just seeing GPT 3 seeing Dolly is thinking about the the sort of economic value that
it can create for people and I think that there's a lot of other pieces to it in terms
of like you know that everyone's going to be more creative if you want to like I can't
draw but now I can create images now I could take a picture of this in my head and I can
actually see it on on a page and one of my favorite applications of Dolly is actually
people who are 3D physical artists you know something's like a sculptor and now they
can actually get a great rendering of the thing that they have in mind by kind of just
like iterating with this machine and they go build it right I think that this sort of
amplification of what humans can do is what these systems are for and so I think that
for this decade I think what we're really going to see is these tools just sort of
proliferating they're going to be everywhere they're going to be baked into every company
I think it's kind of like the internet transition that you know that it was kind of like if
you're a company like what's your internet strategy and you know 1990 it's like what
even is this thing you know and in 2000 it's like huh maybe it's interesting and there
there's a little you know boom and bust and here we are today even talk about an internet
strategy is like it's just so integral to every business it's not even like it's not
even a separate thing right it's just like it's just part of like your it's like your
payroll strategy right it's like it's not like a separate part of your business that
you can pick or choose whether you're going to have it and I think that AI is going to
be much the same I think there will be a transition point right I think that it's it's
interesting like our mission is really about building artificial general intelligence right
really trying to build machines that are able to perform whole tasks right that are you know
push this technology to its limit and build machines that are able to you know our charter
definition is outperform humans at most economically valuable work and there's a question of the
timeline but I think that that picture of you know you have these tools that are creative
that help everyone amplify and what happens when they do become so capable that they're
able to perform these tasks even autonomously and I think that actually the implications
of that are different from what people expect I think that it's much more like you know that
I think there's still going to be this amplification but I think that there the change is going
to be just very hard to predict and unexpected and I think that really thinking about how
all of that sort of value gets distributed how to make sure that it's sort of pointed
at solving these like hard challenges that humans you know maybe are unable to solve
ourselves you know the climate change and you know universal education and things like
that and really transitioning to this like AI powered world I think is going to be just
like a real sort of challenge for the whole you know all of humanity to work together
on. Yeah I mean I totally agree one thing that I think is almost funny with how the timing
of all these technologies have worked out is that you know last year everyone was talking
about Web 3 as crypto and now it feels very obvious that AI is the actual Web 3 you know.
We'll take Web 4. Yeah Web 4 we'll skip we'll skip over one but sort of like Web 1 was just
reading Web 2 was reading and writing and now Web 3 or 4 depending on what we want to
say is ads, computer reading, computer write and it's sort of this incredible new phase.
You know one so I think I think you mentioned two two directions here that I think are really
interesting one is the sort of advancement and sort of proliferation of GP3 and Dolly
and sort of the existing tools becoming more and more economically useful and there's
sort of this continued improvement of the algorithms themselves towards sort of towards
sort of AGI. What do you think and obviously don't reveal any opening secrets but what
do you think this sort of like roadmap to AGI looks like from where we are now? I mean
I think that humanity to a large extent has been on the AGI roadmap for a very very long
time. I think even looking at just the history of neural networks in particular you know
on the one hand we say hey 2012 like that was the moment like everything changed you
know that like you look at these we have all these curves of how much compute people put
into the landmark results it was going like 10x year over year still continuing by the
way that's that's a decade of 10x year over year that's insane and the thing is we actually
did a study to then look back at previous results all the way back to you know say the
perceptron in 1959 and you actually find that there's basically a very smooth curve back
there as well. The amount of compute going into all the landmark results was exactly
Moore's law and it kind of makes sense right it's like that people were not willing to
spend more money they wanted to spend a constant amount of money on these experiments because
you're starving grad students like you know you can only get so much computer time and
that the results got better and better the more compute was available to them and I think
that that is so interesting that yeah basically what changed in 2012 was that we said okay
we're just gonna like you know we are gonna spend more money we're gonna build massive
supercomputers now because the ROI is there but that fundamentally the curve if you control
for that that cost factor it looks exactly the same and so I think that basically this
picture of building more capable models by pouring more compute into them by getting
better at harnessing this technology of neural networks back propagation I think that has
been very invariant and the details you know maybe change a little bit you know do you want to
work on GPT-3 do you want to work on whisper like do you pour in your your you know speech
data do you pour in text data from the internet and to me those details I think you know they
matter in the like in the like sense of like what are you gonna work on today and you know
what are you gonna download but if you zoom out you look at the scale of like these this
technology I think it actually sort of doesn't matter so much I think kind of what we're
building it's almost like building computers like you think about the Haiti and Moore's
law right where it's just like there's a new chip that comes out and there's a new chip that
comes out and it's kind of like what's the you know what's the path to building the best
computer the answer is well you just keep building the next best chip and you keep building
the next best chip and you keep getting better peripherals and all these you know keep working
every single piece of the technology and so I think this full stack of better GPUs great
software for utilizing them neural networks that we learned to harness more and more the
scaling laws doing all the science alignment extremely important making sure these models
not just are smart but actually are aligned with what humans intend all of that I think is the
stack and so I think that you know what our goal is is just to keep doing something that was
previously impossible every single year so you know that I guess well you should check back
in a year but hopefully 2023 we'll all forget about Dolly 2 and GPT 3 and we'll be talking
about something new and I think as long as we continue that like you cannot continue that
path without ending up somewhere amazing yeah I mean I think I actually remember this in I
think probably 2017 you were sort of very still quite you were very excited about sort of the
sort of Moore's law continuing and that that sort of creating a lot more opportunity for you
know neural networks and AI and that's that's sort of played out are you worried about the sort
of proverbial end of Moore's law kind of causing a stall out in in progress so I'm not worried
about it per se like I think the way to think about this right because I think we you know we
often get caught in this debate of like is it all about scale or is it all about algorithms is
all about data and the answer is that's a wrong question right it's really like you multiply
together these factors and the best thing to do when you're multiplying together multiple terms
is that you actually kind of want them all to be equal and I think that the answer is like it's
been great for the past you know seven years that we've been able to just pour more dollars to
build bigger computers that's one way to get ahead of Moore's law at some point they're just
aren't more dollars right there aren't more grains of sand to you know that have been turned into
into these these wonderful computers that we use so there is a limit there that we have not yet
hit but when you do that does not stall all progress right you still have algorithmic progress
and there we've again done studies and we've shown that actually if you take like an example is if
you look at the amount of compute it takes to hit the same performance so to train you know a state
of the art that you know 2012 or 2014 vision model that that computes also falling exponentially
we're basically making exponential progress in algorithms not at the same rate as we are able
to you know sort of build bigger computers but that is an amazing force too you know it's like
I've got this exponential I've got that exponential like let's not even talk about the data exponential
so I think that that the truth is that we will find a way I think that the history of this field
is just so consistent and I think that that you know humanity is just so innovative that I think
that that we're not going to hit a wall for the foreseeable future and do you think that you know
one of the one of the interesting juxtapositions of of today just from a scientific perspective is
a relative slowing in nearly every other science and there's you know there's a lot of research
that sort of demonstrated that science on the whole slowing and then comparatively the sort of
acceleration of artificial intelligence and sort of this this you know in many ways this
renaissance that we're entering right now do you do you fear that at some point AI will similarly
sort of reach these points of admission mark returns and slow relative like in much in the
same way that other sciences have or do you think that's so far away that you know well I think
two things I mean I think that there's there's always s-curves although I think that something
is also interesting about s-curves is that there tends to be paradigm shifts like have you ever
read Singularity is Near no I haven't yeah so this is the Ray Kurzweil book from like 2004
something and I always thought just based on the reputation it's going to be kind of a crazy book
but if you actually read it it's the most dry boring reading you'll ever do and it's basically
just curve after curve of different industries within computing showing how the performance has
changed over time and it's you know basically the conclusion he comes to is that there's this
repeated pattern that seems to happen across you know memory across number of transistors on the
chip you know etc etc where you kind of have an s-curve of the current paradigm and then you have
paradigm shift and that example he talks about is you know thinking about let's talk about CDs
right so you talk about great you know CD adoption it's like you know it's great s-curve it's
suddenly everywhere it's like everyone's got a CD player like it's just the technology of the day
and people get really excited about doing more of the same thing it's like Blu-ray that's the thing
you know and so then everyone starts investing in Blu-rays and somehow it just doesn't take off
and it's because it's just more of the same and it's like you know it's not backwards compatible
and so it's just not really worth it but the real paradigm shift was streaming right suddenly
you have this new adoption curve this new s-curve that just is this like totally different way
and the way we got to fast computers was basically five different paradigm shifts
across a hundred years and so I think that that's maybe a story here too which is like
there's gonna be an s-curve in what we're doing right now and that there will be a paradigm shift
when you hit it and I think that that again speaks to the ingenuity of humans but I think there's
also a second thing where my other answer is to some extent it doesn't matter because the thing
about this field is that it's useful now right that kind of the goal that I think we've always
had for AI was to actually make us so computers are just way more helpful like you think about what
computers have done for humanity right like how many problems they've helped us solve they've
created new problems as well but I think that on net that they've helped us solve way more problems
than they've created and I think they've kind of just changed the nature of how we interact with
each other about how you know like it's just like hard to get lost anymore right you just
pull out google maps I think there's really amazing problems that are now within our reach
that just would not have been otherwise and I think that AI like we're starting to crack that
nut we're starting to be able to you know like it's I think it's kind of interesting you could
get a co-pilot you know which which we we power we have the models that power it and that the way
that that is useful to people is that provides very low latency suggestions right it's basically
an autocomplete for code and that you know there's a very strict latency budget you know if you're
more than you know 1500 milliseconds to get a autocomplete suggestion it's worthless like no
one wants that you've already moved on but I think that what we really want to build the next
phase is machines that help you produce that are able to produce artifacts that are materially
interesting on their own so not just interesting because it's like a fast suggestion to you but
because it's actually a quality answer and you're starting to see if you talk to our current gbt
iteration you can ask it to write some poems and it writes way better poetry than I can
it actually wrote a poem for my wife that made us both cry like you know yeah I I cannot do that
myself but now I can you know by partnering with this with this machine and I think that
that's the real story right is really trying to get these tools out and everywhere and yeah you
know if what we're doing right now stalls out I don't think that removes the value from what we're
able to create yeah by the way it's depressing that the the attention span of most engineers is
only 1500 seconds but you know it is what it is I what uh what if anything you know I think
one of the things that if I recall spurred you to to work on opening I was was sort of
also being concerned about the sort of potential negative consequences of of the technology
um what at this point looking forward what are your sort of biggest concerns or what are you
afraid of with with artificial intelligence that you sort of urge everyone in the field to sort of
help avoid yeah so I think that one thing that's very interesting about AI is that you know if you
talked to certainly you know 10 years ago if you looked at every article about it you talked to
someone on the street terminator is the main thing that comes up right so I think there's always been
this feeling around AI that is sort of you know that there's an element of fear mixed with the
you know sometimes people don't see any potential or sometimes you know they realize that there is
the potential but like really trying to figure out and navigate it and I think that that picture
the specifics you know I think that that we're starting to see a little bit more but I think
the high level picture of this is technology that's very powerful and it can be powerful in
positive ways and negative ways um I think is extremely correct and I think it's very important
not just to be you know starry eyed optimist everything's just going to work itself out
but also not to be you know sort of doomsday like everything is terrible and you know humanity is
over because I don't think that's at all true I think this technology can be the the best thing
that is the way we've ever created and help us be the best versions of ourselves but I think
that it requires very careful navigating of the space and it's not something that just is for you
know companies of Silicon Valley to figure out I think it's it's really all of humanity kind of
challenge um so I think that we're going to go through different phases I think that that right
now I you know we're kind of starting to build systems where that I you know you think about
misuse is the most clear problem and the systems themselves are still not very powerful right
that the kinds of things you worry about for GPT-3 are you know important problems you think
about bias and representation you think about the system sort of you know sort of saying the wrong
thing you know but but its action is really in your mind right it's it's sort of words on a page
that then you know words on a page are very powerful but that they don't themselves have
direct action in the world but you think about something like codex our code writing system
which is a little bit more like a robot because it has it emits code and if you were to just execute
that code directly it can actually directly have actuators into the world and making sure that that's
aligned and doing the right kinds of things not having buggy code and not writing viruses and that
kind of thing like that's really important and so I think that that figuring out what values go
into this these machines and that they're operating according to those values that's going to be very
very critical figure out how to avoid misuse and sort of regulate that both at a sort of societal
level at a you know technical level all of that is very important and I and I do think that there
is also a point where the technology itself you have to think about that it's going to be extremely
powerful and you think about a system that's you know sort of talking to lots of humans and
is operating unchecked that's the kind of thing that you should worry about you know we already
worry about that think about the companies right that lots of people are using this you know social
media platform or you know any of the technologies that we use and how much influence those can have
in the world and those aren't systems that have you know sort of deep sort of behaviors that are
emergent from from from what they've learned and so I think that that figuring out the technical
controls to make sure that these systems remain in service of humanity and sort of to
actually empower and accelerate all of us that I think is is also a very critical thing so it's
kind of this like a ramping set of stakes and making sure that we're building systems that are
aligned with with our values and figuring out what that even means like what what is the values
of humanity that should be in the system and that I think is not going to be an easy problem
you know one question and this this may be the last question that I have for you is sort of one
of the thing one of the conclusions of if the technology such that scale continues to be the
the sort of one of the more important things you know scale whether it's data or better algorithms
or scale compute then it it the technology itself will tend towards sort of this game theoretical
proliferation mode where it's sort of like people are going to compete and you see some of this today
even with the large tech companies and you guys obviously people are going to compete to sort of
build the bigger supercomputers that have the better performance and you have the bigger supercomputer
you have sort of supremacy over the other supercomputers and sort of there's like this
you know laddering the stakes and sort of and proliferation is really the sort of the right
word do you think that that is a version of the future or do you think that there's sort of
some path in which this becomes a much more sort of like open and useful not this sort of like
tool for nation states or large companies to compete with one another I think that the future
that seems to be unfolding is kind of a you know replay of how say computing technology has played
out more broadly I think that that it is still going to be the case that you're going to have
these increasingly massive supercomputers that are in the hands of only a few that are able to just
create models that can just do crazy things that no one else can do but I don't think that that
removes the value from the massive set of things that people are going to do with these models
and so you know I think that balancing the like super powerful very dual use extremely you know
think of these like almost like you know these massive like you know sort of systems that are
you know we think about like a nuclear reactor and it's like you know it's like these like giant
like sort of you know systems that you should approach with great care um and you think about
like you know by contrast think about wind turbines and like there's lots of wind turbines
everywhere and actually that if you add up the amount of value from wind turbines versus nuclear
reactors like I think actually that the balance is probably in favor of wind turbines and so I
think that that's kind of the future we're going to is that the AI technology is going to be everywhere
and there's going to be lots of value that's delivered by having open source models that are
integrated to every business and that people are building all sorts of crazy applications on top of
and that's something that we really want to support and promote and you also have to have this
dual answer for what you do with the with the new extremely capable stuff that's just a mile ahead
of everything else and that's something you have to treat with kid gloves with with more care
and I think that that balance is tricky it's not easy um that's something that we as an organization
have been trying to straddle and I think that you know we've had real existential struggles
internally trying to figure out you know like our goal is to empower everyone it's really to uh
to to bring everyone along to this AI transition and the best way to do that I think that our
picture of it has changed as the technology has unfolded and I think that we're starting to get a
sense of of you know where this can go it's really exciting to see all the the energy of all these
builders coming in because I think that like you said people are starting to realize like AI is
really going to work and it's time to build yeah well um this was an incredible conversation thank
you so much Greg next time we speak I'll make you uh read the poem that all right there we go
cool thank you so much thank you so much
