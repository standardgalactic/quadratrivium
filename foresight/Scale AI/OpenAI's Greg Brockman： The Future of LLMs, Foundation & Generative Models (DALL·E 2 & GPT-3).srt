1
00:00:00,000 --> 00:00:14,720
We're joined next by Greg Brockman, President, Chairman and Founder of Open AI and Alexander

2
00:00:14,720 --> 00:00:18,360
Wang, CEO and Founder of Scale AI.

3
00:00:18,360 --> 00:00:23,520
Open AI is a research and deployment company whose mission is to ensure general-purpose

4
00:00:23,520 --> 00:00:27,280
artificial intelligence benefits all of humanity.

5
00:00:27,280 --> 00:00:34,520
For Open AI, Greg was the CTO of Stripe, which he helped build from 4 to 250 employees.

6
00:00:34,520 --> 00:00:40,520
Please join me in welcoming to the stage Greg Brockman and Alexander Wang.

7
00:00:40,520 --> 00:00:42,520
Hey Greg.

8
00:00:42,520 --> 00:00:44,520
Hey.

9
00:00:44,520 --> 00:00:47,000
Thanks for making it.

10
00:00:47,000 --> 00:00:48,000
Absolutely.

11
00:00:48,000 --> 00:00:55,840
I'd be happy to be here.

12
00:00:55,840 --> 00:01:00,880
I want to start actually, I don't know if you remember this, but we first met at this

13
00:01:00,880 --> 00:01:07,000
summer camp called Spark where you gave a presentation about, at the time you were the

14
00:01:07,000 --> 00:01:10,560
CTO of Stripe and you gave this presentation about sort of like everything you had accomplished

15
00:01:10,560 --> 00:01:13,880
and I was a member of that camp and it was extremely memorable.

16
00:01:13,880 --> 00:01:15,880
You had a lot of good sound bites.

17
00:01:15,880 --> 00:01:18,520
I'm glad that it landed.

18
00:01:18,520 --> 00:01:20,960
Kind of a full circle moment.

19
00:01:20,960 --> 00:01:25,400
Well I think to start out with, I mean you've been CTO and now you're President of Open

20
00:01:25,440 --> 00:01:32,840
AI, but CTO of two incredibly iconic companies, Stripe and Open AI, in some ways probably two

21
00:01:32,840 --> 00:01:36,960
of the most iconic startups of the past decade.

22
00:01:36,960 --> 00:01:43,320
I wanted to start out just by asking in what ways are the two organizations the same and

23
00:01:43,320 --> 00:01:46,280
being CTO the same and in what ways are they different?

24
00:01:46,280 --> 00:01:48,520
Thank you for the kind of words.

25
00:01:48,520 --> 00:01:53,360
I think that one thing that's very interesting to me about kind of having been part of both

26
00:01:53,360 --> 00:01:59,280
of these organizations is seeing how much groups of people are kind of the same regardless

27
00:01:59,280 --> 00:02:02,160
of what the problem in front of you is.

28
00:02:02,160 --> 00:02:07,320
So I think that a lot of how we approached Stripe was thinking from first principles.

29
00:02:07,320 --> 00:02:12,000
I remember when we were pre-launch and we had some buzz going because we had some early

30
00:02:12,000 --> 00:02:16,480
customers and one of my friends took me out to launch, he was like all right, I've been

31
00:02:16,480 --> 00:02:19,720
hearing about this Stripe thing, what's your secret sauce?

32
00:02:19,800 --> 00:02:24,480
I was like, I mean we just make payments really good and he's like no, no, no, come on, you

33
00:02:24,480 --> 00:02:27,040
can tell me, what's the secret sauce?

34
00:02:27,040 --> 00:02:30,800
And really that was the secret sauce, right, is that we had just rethought every single

35
00:02:30,800 --> 00:02:34,480
piece of what we were doing from the ground up from first principles, not sort of locked

36
00:02:34,480 --> 00:02:37,880
into the way that people had been doing it and we asked how should it be, like where's

37
00:02:37,880 --> 00:02:40,960
all the pain and does it need to be there?

38
00:02:40,960 --> 00:02:44,840
I think that in AI we did much the same, we really thought about okay, there's this field

39
00:02:44,840 --> 00:02:48,000
that we're entering and that we hire a lot of people who had been in the field, but a

40
00:02:48,000 --> 00:02:51,440
lot of us also hadn't been in the field and we came to it with beginner's eyes and I think

41
00:02:51,440 --> 00:02:55,360
that that approach of just not being beholden to all the ways people were doing it, but

42
00:02:55,360 --> 00:02:59,840
also becoming expert in the way that things have been done because if you just throw everything

43
00:02:59,840 --> 00:03:04,600
out, like you're also just going to be starting from scratch in a not helpful way.

44
00:03:04,600 --> 00:03:08,360
So I think that that maybe is the deepest commonality between them.

45
00:03:08,360 --> 00:03:13,640
But obviously very different organizations, for Stripe I think that we ran the traditional

46
00:03:13,640 --> 00:03:14,800
startup playbook.

47
00:03:14,800 --> 00:03:18,240
You basically come up with the innovation and you just build, build, build.

48
00:03:18,240 --> 00:03:26,440
You get in front of customers from day one, the story is that we gave the first API to

49
00:03:26,440 --> 00:03:30,080
a customer who charged a credit card and he was like, I would like my money now please

50
00:03:30,080 --> 00:03:34,840
and we were like, huh, I guess we should build that.

51
00:03:34,840 --> 00:03:39,800
Open AI, we had research to do, like where's the customer?

52
00:03:39,800 --> 00:03:45,080
And it really took us I guess five years, starting late 2015 and it's really not until

53
00:03:45,080 --> 00:03:48,240
2020 that we had our very first product.

54
00:03:48,240 --> 00:03:52,400
And so I think that that sort of figuring out like what you're even supposed to work

55
00:03:52,400 --> 00:03:57,520
on, like did you do a good job, should you feel good on a day-to-day basis?

56
00:03:57,520 --> 00:04:00,720
I think that all of that had to come from within rather than from without.

57
00:04:00,720 --> 00:04:01,720
Yeah.

58
00:04:01,720 --> 00:04:04,760
Well, actually I want to go back to this point that you mentioned around first principles

59
00:04:04,760 --> 00:04:05,760
thinking.

60
00:04:05,760 --> 00:04:11,560
It's very interesting because even I remember like maybe 2020 or 2021, you know, you would

61
00:04:11,560 --> 00:04:16,880
sort of post-GPD3, you would talk to other researchers in the field and even they would

62
00:04:16,880 --> 00:04:22,080
still, you know, there's still like some degree of skepticism over the sort of like core concept

63
00:04:22,080 --> 00:04:26,000
of scaling up these models and if there were still gains to be had, et cetera.

64
00:04:26,000 --> 00:04:31,600
And I think, you know, I don't know the story but it seems like the sort of research sort

65
00:04:31,600 --> 00:04:38,360
of intuition that led to GPD3, Dolly 2 that have really ushered in kind of a new era of

66
00:04:38,360 --> 00:04:44,760
AI were probably, you know, somewhat against the grain or somewhat unintuitive at the time.

67
00:04:44,760 --> 00:04:50,800
You know, one question I have for you is, you know, I think now looking back it's obviously

68
00:04:50,800 --> 00:04:57,840
very obvious to point out GPD3, Dolly 2 basically have fundamentally accelerated AI progress

69
00:04:57,840 --> 00:05:02,520
and its relevance to the world and its relevance to every industry and sort of have created

70
00:05:02,520 --> 00:05:07,040
the sort of most recent AI wave.

71
00:05:07,040 --> 00:05:11,680
How is that matched up against your expectations when you were building these technologies?

72
00:05:11,680 --> 00:05:12,680
You know?

73
00:05:12,680 --> 00:05:13,680
Yeah.

74
00:05:13,680 --> 00:05:17,720
Well, I think the thing that's most interesting to me is that those models you mentioned are

75
00:05:17,720 --> 00:05:21,640
kind of overnight successes that took many, many years to create.

76
00:05:21,640 --> 00:05:24,760
And so, you know, from the outside it looks like, wow, you just like produce this model

77
00:05:24,760 --> 00:05:29,520
and that model and really on the inside, the GPT arc, that's a five-year arc, right?

78
00:05:29,520 --> 00:05:33,640
It really started with sentiment neuron paper which is back in 2017.

79
00:05:33,640 --> 00:05:34,640
Do you remember that paper?

80
00:05:34,640 --> 00:05:35,640
I remember the paper.

81
00:05:35,640 --> 00:05:36,640
It was very cool.

82
00:05:36,640 --> 00:05:37,640
Yeah.

83
00:05:37,640 --> 00:05:38,640
But it felt very novel.

84
00:05:38,640 --> 00:05:39,640
It felt very novel.

85
00:05:39,640 --> 00:05:40,640
Yeah.

86
00:05:40,640 --> 00:05:41,640
Very few people remember it.

87
00:05:41,640 --> 00:05:45,400
It's, you know, it was this very early result where we basically had been training a LSTM

88
00:05:45,400 --> 00:05:48,040
at the time to predict the next character in text.

89
00:05:48,040 --> 00:05:49,800
So, we basically showed a bunch of Amazon reviews.

90
00:05:49,800 --> 00:05:52,880
We said, what's the next character and of course it's going to learn where the commas

91
00:05:52,880 --> 00:05:53,880
go.

92
00:05:53,880 --> 00:05:54,880
Where the periods go.

93
00:05:54,880 --> 00:05:57,800
But of course it's not going to understand anything.

94
00:05:57,800 --> 00:06:02,360
But we found a single neuron in that model that had learned a state-of-the-art sentiment

95
00:06:02,360 --> 00:06:03,360
analysis classifier.

96
00:06:03,360 --> 00:06:06,080
I can tell you this is a positive review or negative review.

97
00:06:06,080 --> 00:06:07,080
That's understanding.

98
00:06:07,080 --> 00:06:10,560
You know, I don't know what understanding means but it's semantics for sure.

99
00:06:10,560 --> 00:06:14,040
And that for us was like, okay, this is going to work.

100
00:06:14,040 --> 00:06:20,120
The transformer came out late 2017 and my co-founder Ilya immediately was like, that's

101
00:06:20,120 --> 00:06:21,120
the thing.

102
00:06:21,120 --> 00:06:22,120
That's what we've been waiting for.

103
00:06:22,120 --> 00:06:26,560
So, you take this sort of very early nascent result, put in a transformer and then that's

104
00:06:26,560 --> 00:06:27,560
GPT-1.

105
00:06:27,560 --> 00:06:30,040
GPT-2 is you just keep pushing it.

106
00:06:30,040 --> 00:06:33,840
And you know, I think that the algorithm we kind of run internally is that we do these

107
00:06:33,840 --> 00:06:38,240
little sort of get signs of life and you have to be very, very careful to distinguish signs

108
00:06:38,240 --> 00:06:41,840
of life from like kind of just pushing too hard on a specific data set that isn't really

109
00:06:41,840 --> 00:06:42,840
going to keep going.

110
00:06:42,840 --> 00:06:46,280
But if you kind of build those right intuitions then you know, okay, now is the time to put

111
00:06:46,280 --> 00:06:47,280
in more compute.

112
00:06:47,280 --> 00:06:48,280
Now is the time to put in more researchers.

113
00:06:48,280 --> 00:06:50,640
Now is the time to like really scale it up.

114
00:06:50,640 --> 00:06:56,040
And so GPT-2 obviously was exciting and that we were all like, well, we look at the curves.

115
00:06:56,040 --> 00:06:58,880
You know, the bigger we made this model, the more compute we put in, the more data we put

116
00:06:58,880 --> 00:07:03,800
in, the more we just sort of got all the engineering details right, those curves just got better.

117
00:07:03,800 --> 00:07:06,880
And so actually, you know, our goal was just to break the paradigm.

118
00:07:06,880 --> 00:07:10,600
It was just push it until the curve stopped looking good and we still haven't managed

119
00:07:10,600 --> 00:07:11,600
to accomplish it.

120
00:07:11,600 --> 00:07:12,600
Yeah.

121
00:07:12,600 --> 00:07:18,120
Well, I think one of the things, at least for me and probably for many people who initially

122
00:07:18,120 --> 00:07:22,440
played with GPT-3, the like shocking thing was not, I mean, it wasn't necessarily that

123
00:07:22,440 --> 00:07:27,800
even the model got better and better performance on, you know, established tasks is that it

124
00:07:27,800 --> 00:07:32,280
sort of had all these qualitatively new behaviors that were felt very magical.

125
00:07:32,280 --> 00:07:35,920
And even now, you know, there's prompts that, you know, you'll see on Twitter or whatnot

126
00:07:35,920 --> 00:07:38,160
that are sort of really shocking.

127
00:07:38,160 --> 00:07:44,280
I mean, did you have these sort of like early moments when like you had the early model

128
00:07:44,280 --> 00:07:48,160
results were like, holy crap, this is like, this is magic.

129
00:07:48,160 --> 00:07:49,160
Yeah.

130
00:07:49,160 --> 00:07:52,840
Well, I think that the earliest one that I remember was around code.

131
00:07:52,840 --> 00:07:57,680
I mean, just, you know, at the time, totally mind blowing that you could just write a function

132
00:07:57,680 --> 00:08:01,400
name and a doc string kind of describing what the function should do and actually write

133
00:08:01,400 --> 00:08:02,400
it.

134
00:08:02,400 --> 00:08:03,720
Not super complicated functions, right?

135
00:08:03,720 --> 00:08:07,400
But just that it was able to, you know, you ask for, you know, something to take a couple

136
00:08:07,400 --> 00:08:09,600
lines and they would be able to really do it.

137
00:08:09,600 --> 00:08:12,160
You modify things a little bit to make sure it hadn't just memorized it.

138
00:08:12,160 --> 00:08:15,000
Make sure enough that it would write out the modified code.

139
00:08:15,000 --> 00:08:18,480
And I think, you know, the overall thing that's really interesting about the paradigm of a

140
00:08:18,480 --> 00:08:24,280
GPT-3 is that where it really comes from is that I, you know, we kind of had this picture

141
00:08:24,280 --> 00:08:29,160
that, look, the problem with these models is that they're great within their data distribution.

142
00:08:29,160 --> 00:08:32,680
But as soon as you're outside of that distribution, like all bets are off.

143
00:08:32,680 --> 00:08:36,600
And so what if you just make the whole world, the whole universe, be the data distribution?

144
00:08:36,600 --> 00:08:38,680
You put the whole internet in there.

145
00:08:38,680 --> 00:08:45,400
And I think that what we've really seen is that these models, that they really are able

146
00:08:45,400 --> 00:08:48,840
to generalize extremely well within the kinds of things that they've seen.

147
00:08:48,840 --> 00:08:51,120
You know, again, different question if it's never seen anything like it.

148
00:08:51,120 --> 00:08:54,640
I mean, humans are also not very good at things you've never seen before.

149
00:08:54,640 --> 00:08:59,840
But I think that that picture of just, like, all of the different things that it's seen

150
00:08:59,840 --> 00:09:02,160
in all these different configurations is almost unimaginable.

151
00:09:02,160 --> 00:09:06,600
There's no human who's been able to consume, you know, 40 terabytes worth of text.

152
00:09:06,600 --> 00:09:11,520
And so I think that we just keep seeing surprises where you just ask for, one of my favorite

153
00:09:11,520 --> 00:09:16,120
ones actually was this teacher-student interaction where I was the teacher model as the student

154
00:09:16,120 --> 00:09:18,320
and I managed to teach it how to sort numbers.

155
00:09:18,320 --> 00:09:21,240
And you just kind of have these experiences where, like, that's what it should be like

156
00:09:21,240 --> 00:09:22,720
to interact with an AI.

157
00:09:22,720 --> 00:09:23,720
Yeah.

158
00:09:23,720 --> 00:09:25,600
I mean, it's incredibly shocking.

159
00:09:25,600 --> 00:09:31,080
You know, one of the things I'm curious to get your thoughts on is, I think, in the path

160
00:09:31,080 --> 00:09:37,200
of developing GP3, you know, required, I think probably the jump from GP2 to GP3 required

161
00:09:37,200 --> 00:09:42,000
a lot of conviction because, you know, you all were spending probably a fair amount on

162
00:09:42,000 --> 00:09:44,960
a compute at the time to be able to train these models and there were probably a lot

163
00:09:44,960 --> 00:09:49,360
of experiments that didn't work and so you had to be willing to keep going after it.

164
00:09:49,360 --> 00:09:55,000
Did that phase of the journey, sort of this, like, GP2 to GP3 jump, was it scary?

165
00:09:55,000 --> 00:09:56,200
Did you have doubts?

166
00:09:56,200 --> 00:09:59,840
Or were you very confident that, hey, you know, we're going to scale this up and even

167
00:09:59,840 --> 00:10:04,040
though we're going to not get it right the first few times, it's going to be amazing?

168
00:10:04,040 --> 00:10:05,040
Yeah.

169
00:10:05,040 --> 00:10:10,360
And to your point that scale was not an obvious thing, not the company, but the scaling things

170
00:10:10,360 --> 00:10:16,000
up, at the time, the funny thing is actually our very first scale result that just sort

171
00:10:16,000 --> 00:10:19,320
of convinced us that this is the right way to approach things.

172
00:10:19,320 --> 00:10:20,640
You push it until it breaks.

173
00:10:20,640 --> 00:10:24,240
Not necessarily that more compute is just magically always going to solve your problem.

174
00:10:24,240 --> 00:10:25,240
It was Dota.

175
00:10:25,240 --> 00:10:28,080
That was, you know, playing competitive video games and there we kind of went through this

176
00:10:28,080 --> 00:10:32,120
whole, that was a three-year arc where we started out with something that didn't do

177
00:10:32,120 --> 00:10:36,000
anything, finally beat, like, you know, the in-house team, then we managed to go beat

178
00:10:36,000 --> 00:10:41,080
the pros and at each step it was just kind of pushing in all dimensions, right, is make

179
00:10:41,080 --> 00:10:46,240
the model bigger, it's to, you know, sort of, again, fix all the bugs and you just kind

180
00:10:46,240 --> 00:10:50,920
of keep iterating on every single dimension and every single dimension yields returns.

181
00:10:50,920 --> 00:10:55,560
And so I think that we did very much the same thing where for GP2, you know, it's not as

182
00:10:55,600 --> 00:11:00,240
simple as saying, okay, like, clearly you just need to, like, you know, crank up this

183
00:11:00,240 --> 00:11:02,440
one variable and you just do it in one shot.

184
00:11:02,440 --> 00:11:07,080
It's this, like, sort of iterative, like, stepping through the space on each axis at

185
00:11:07,080 --> 00:11:08,520
every single time.

186
00:11:08,520 --> 00:11:11,720
And so I think that on the one hand it does require conviction because you do need to

187
00:11:11,720 --> 00:11:15,360
say we're going to, like, carve out a big compute budget so that you're not constantly

188
00:11:15,360 --> 00:11:19,880
not kind of fighting other people for the big supercomputers, but on the other hand,

189
00:11:19,880 --> 00:11:23,560
I think it's also very iterative and you don't have to make scary irreversible decisions

190
00:11:23,600 --> 00:11:26,400
because in each step you get feedback from reality.

191
00:11:26,400 --> 00:11:31,800
And I think that that key of, like, both the big picture, thinking of what if this works

192
00:11:31,800 --> 00:11:36,840
and make sure that you're really set up for success, but also don't blindly spend a year

193
00:11:36,840 --> 00:11:40,320
of your organization on just, like, pursuing a thing that might not pan out.

194
00:11:40,320 --> 00:11:43,000
I think that balancing those two is what was really key.

195
00:11:43,000 --> 00:11:48,240
Yeah, I mean, one of the cool things is you sort of walk through this and talk through

196
00:11:48,240 --> 00:11:53,520
the insights is that the sort of organizational learnings were really critical in this entire

197
00:11:53,880 --> 00:11:59,320
sort of path-dependent sort of path to GP3.

198
00:11:59,320 --> 00:12:05,080
It sort of, you know, it makes sense when you say it that sort of insights from Dota 2

199
00:12:05,080 --> 00:12:09,040
and insights from the sentiment neuron were sort of like the key, these were like the

200
00:12:09,040 --> 00:12:13,040
key nuggets that led to the sort of, like, crystallized idea of, you know, scaling up

201
00:12:13,040 --> 00:12:17,520
and building GP3, but it's very unintuitive from the outside and sort of, I think it's

202
00:12:17,520 --> 00:12:22,680
almost a statement of innovation in some sense is that, you know, you're going to piece together

203
00:12:22,720 --> 00:12:26,720
this sort of, like, disparate collection of insights that you gather from a wide variety

204
00:12:26,720 --> 00:12:31,280
of experiments and eventually you sort of, like, get the ingredients together and you

205
00:12:31,280 --> 00:12:32,280
build something.

206
00:12:32,280 --> 00:12:34,880
Yeah, that's the first principle is thinking in action.

207
00:12:34,880 --> 00:12:35,880
Yeah.

208
00:12:35,880 --> 00:12:40,000
You know, I think that the story of AI, I don't know if you think about this at all, but I

209
00:12:40,000 --> 00:12:44,320
think about this a little bit, I think the story of AI to date and especially the past

210
00:12:44,320 --> 00:12:48,520
few years and the story of open AI is probably going to be something that historians are

211
00:12:48,520 --> 00:12:52,840
going to study for, you know, decades and decades to come.

212
00:12:52,840 --> 00:12:57,840
Are there any fun stories from the journey of creating some of these foundation models

213
00:12:57,840 --> 00:13:00,920
that you think deserve to be in the history books?

214
00:13:00,920 --> 00:13:06,600
Well, I'll tell you my actual favorite story from the Dota days.

215
00:13:06,600 --> 00:13:11,400
So, you know, we've been working on this system and, you know, actually the funny thing is

216
00:13:11,400 --> 00:13:14,360
at the very beginning we wrote down our list of milestones.

217
00:13:14,360 --> 00:13:20,200
On this date we're going to be Jonas, our best open AI employee who also had many thousands

218
00:13:20,200 --> 00:13:22,720
of hours of Dota 2 gameplay.

219
00:13:22,720 --> 00:13:25,240
This date we're going to beat the semi-pros, you know, this date we're going to beat the

220
00:13:25,240 --> 00:13:29,720
pros and so it's supposed to be like June 6th or something, June 6th rolls around, we

221
00:13:29,720 --> 00:13:30,720
don't have anything.

222
00:13:30,720 --> 00:13:34,480
Like, you know, he just crushes us and two weeks go by, three weeks go by, we keep pushing

223
00:13:34,480 --> 00:13:39,080
back that deadline by a week every week and then one day we actually do beat him.

224
00:13:39,080 --> 00:13:44,040
And you know, I think my conclusion was that like it wasn't actually actionable to sort

225
00:13:44,040 --> 00:13:48,720
of set those goals of outputs, you can only control your inputs, you can control the experiments

226
00:13:48,720 --> 00:13:49,720
you run.

227
00:13:49,720 --> 00:13:53,680
And so we just managed the project very differently after that and the thing that was so crazy

228
00:13:53,680 --> 00:13:58,160
to me still is that, you know, so a week before the international, which is the like world

229
00:13:58,160 --> 00:14:01,000
championships we're going to show up, we're going to play 1v1 against the best players

230
00:14:01,000 --> 00:14:05,120
in the world, we finally started beating our semi-protester and we're like, okay, maybe

231
00:14:05,120 --> 00:14:07,240
this is actually going to happen.

232
00:14:07,240 --> 00:14:10,360
But then we learned that he actually was on like vacation, he didn't have his like real

233
00:14:10,360 --> 00:14:14,360
setup and so we're like, oh no, like this is not going to go well.

234
00:14:14,360 --> 00:14:17,960
So we show up, you know, we continue to train, we like kind of do like a Hail Mary of like

235
00:14:17,960 --> 00:14:23,240
scaling things up, biggest scale we've ever done and we show up at the international and

236
00:14:23,240 --> 00:14:28,640
we play against, you know, like sort of low lowest, you know, low ranked pro, like a previous

237
00:14:28,640 --> 00:14:33,600
pro and we go 3-0-3-0-2-1.

238
00:14:33,600 --> 00:14:37,960
So we basically win-win and then we did have one loss and we take a look at it and it's

239
00:14:37,960 --> 00:14:41,360
like this item that we've never trained with, we've never seen before, oh wow, okay, we

240
00:14:41,360 --> 00:14:44,200
need to add that and, you know, do it fast.

241
00:14:44,200 --> 00:14:47,480
And so the team stays up all night putting this thing into the training, getting the

242
00:14:47,480 --> 00:14:51,520
whole thing launched and, you know, again, like we did double the scale where we're basically

243
00:14:51,520 --> 00:14:57,880
maxing out our CPU cores at this point and start training and, you know, we're supposed

244
00:14:57,880 --> 00:15:03,640
to play against the top pros in the world, fortunately they can't do the next day so

245
00:15:03,640 --> 00:15:09,320
we get an additional day of training and the number two person comes in, he plays against

246
00:15:09,320 --> 00:15:17,040
us and we win-win-win-win-win and he's like, okay, but I beat this but the top player is

247
00:15:17,040 --> 00:15:21,200
never going to lose to this thing or sorry, yeah, the top player is going to crash this

248
00:15:21,280 --> 00:15:27,800
thing and fortunately because he had spent so long playing that that guy couldn't come

249
00:15:27,800 --> 00:15:31,720
that day so we got one more day of training and that one more day of training was enough

250
00:15:31,720 --> 00:15:35,440
and so I think it's just the story of like you can really see the improvement and at

251
00:15:35,440 --> 00:15:40,360
each step we could see new behaviors that the system have learned and I think that that

252
00:15:40,360 --> 00:15:43,640
experience of just sort of watching a girl up in front of you is just something that

253
00:15:43,640 --> 00:15:45,140
was really amazing.

254
00:15:45,140 --> 00:15:50,740
I'm actually surprised because you would, I sensibly you'd probably train the sort

255
00:15:50,740 --> 00:15:54,660
of like agents for a long, long, long time going into the international and surprised

256
00:15:54,660 --> 00:15:55,900
at each incremental day.

257
00:15:55,900 --> 00:15:59,300
Yeah, so this is something I think has changed over time so at the time we basically had

258
00:15:59,300 --> 00:16:02,700
two weeks worth of training was like the whole model run and so you'd start from scratch

259
00:16:02,700 --> 00:16:07,860
each time and the thing that was really funny in the middle was that, you know, we put in

260
00:16:07,860 --> 00:16:11,420
this new item, we were training it and when we took out of training it was the best spot

261
00:16:11,420 --> 00:16:16,660
we ever saw except that our semi-protester was looking at it and was like this bot is

262
00:16:16,660 --> 00:16:19,700
doing something really dumb, it's just sitting there in the first wave and taking all this

263
00:16:19,700 --> 00:16:23,060
damage it doesn't have to, I'm going to go beat it, he ran it and go fight it and he

264
00:16:23,060 --> 00:16:24,060
lost.

265
00:16:24,060 --> 00:16:27,660
It's like that was weird and he did it like five more times and he lost each time but

266
00:16:27,660 --> 00:16:30,660
then he figured out a strategy that actually does work which is you, you realize what was

267
00:16:30,660 --> 00:16:34,460
going on was it was baiting him, it had learned to deceive, you know it actually learned that

268
00:16:34,460 --> 00:16:37,580
what you do is that like you pretend oh I'm just a weak little bot, I don't know what

269
00:16:37,580 --> 00:16:42,260
I'm doing and then you know a person comes in and you're just like smack.

270
00:16:42,260 --> 00:16:46,660
And so the way you defeat that is that you actually, you don't fall for the bait, right,

271
00:16:46,660 --> 00:16:50,380
you let the bot take all this damage and sit there and get weaker and then you finally

272
00:16:50,380 --> 00:16:55,580
go in for the kill and so there we actually stitched together our good bot for the first

273
00:16:55,580 --> 00:16:59,660
wave with the deceived bot thereafter and so there was a lot of this sort of like really

274
00:16:59,660 --> 00:17:04,460
examining what was going on in the systems because it's such a limited domain, you know

275
00:17:04,460 --> 00:17:07,860
it's a complicated domain but it's very, very interpretable.

276
00:17:07,860 --> 00:17:11,180
It meant that we could observe behaviors like this and figure out how to engineer around

277
00:17:11,180 --> 00:17:16,380
them but once we graduated from the 1v1 version of the game to the full 5v5 you know much

278
00:17:16,380 --> 00:17:21,660
more like you know like competitive basketball or something rather than heads up, suddenly

279
00:17:21,660 --> 00:17:26,260
all of our analysis of the behavior stopped working, right, that we used to have someone

280
00:17:26,260 --> 00:17:29,500
who just literally would watch the bot play and be like oh we have this bug in the training

281
00:17:29,500 --> 00:17:33,260
we got to go fix that, for 5v5 we just could not do that and I think that's kind of where

282
00:17:33,260 --> 00:17:37,820
we've graduated as a field is that too when you look at GPT-3 and the mistakes it makes

283
00:17:37,820 --> 00:17:42,060
sometimes people ask well why didn't it make that mistake and sometimes you can interpret

284
00:17:42,060 --> 00:17:45,940
it but sometimes it's also a little bit like asking well you know why did you make a mistake

285
00:17:45,940 --> 00:17:49,540
on some tests it's like well you think you know but like your explanation isn't always

286
00:17:49,540 --> 00:17:53,580
very good and I think that to do complicated behaviors sometimes there's a very complicated

287
00:17:53,580 --> 00:17:54,580
explanation.

288
00:17:54,580 --> 00:17:55,580
Yeah.

289
00:17:55,580 --> 00:18:00,260
Have you read this short story I think it's like the Lifecycle of Software Objects by

290
00:18:00,260 --> 00:18:01,260
Ted Chang?

291
00:18:01,260 --> 00:18:03,260
I think I have but I don't recall the details.

292
00:18:03,260 --> 00:18:06,420
It's like it's about how they're these AI pets and they sort of like keep learning new

293
00:18:06,420 --> 00:18:10,620
and new behaviors it's very reminiscent of describing these Dota agents.

294
00:18:10,620 --> 00:18:15,180
Yeah yeah I think we'll see that kind of thing in our future somewhere.

295
00:18:15,180 --> 00:18:19,260
I want to kind of go back you know one of the things we've known each other for many

296
00:18:19,260 --> 00:18:26,380
years long before you know these foundation models and even before this competition Dota

297
00:18:26,380 --> 00:18:33,940
2 and one thing I vividly remember is how sort of optimistic and confident you were

298
00:18:33,940 --> 00:18:39,300
in sort of this sort of path of increasing and increasing AI capability you know sort

299
00:18:39,300 --> 00:18:44,500
of I remember the time as maybe 2016, 2017 it felt very striking because it was sort

300
00:18:44,500 --> 00:18:51,460
of like you know with these algorithms they're still pretty weak and you were always very

301
00:18:51,460 --> 00:18:54,940
confident like oh yeah they're just going to keep getting better and better and better

302
00:18:54,940 --> 00:18:57,140
and you know you're very a lot of confidence in that.

303
00:18:57,140 --> 00:19:03,860
What were the things that back then gave you sort of the resolve or confidence in the

304
00:19:03,860 --> 00:19:06,060
and the optimism in the technology?

305
00:19:06,060 --> 00:19:13,540
Yeah I mean at some level you know to have that kind of belief and conviction and something

306
00:19:13,540 --> 00:19:15,860
that hasn't happened yet it's a very intuitive thing.

307
00:19:15,860 --> 00:19:21,100
I mean I remember when I was in school and showed up excited about doing NLP research

308
00:19:21,100 --> 00:19:24,620
I went and tracked down an NLP professor and I was like please can I do some research for

309
00:19:24,620 --> 00:19:29,380
you and he's like okay he shows me these like parsed trees and stuff and I look at that

310
00:19:29,380 --> 00:19:33,940
and I was like this is never going to work right and you know to explain like why does

311
00:19:33,940 --> 00:19:36,660
it feel like it's not going to work it just doesn't have the right properties right it

312
00:19:36,660 --> 00:19:41,460
just felt like you're going to pour all this human like engineering and intuition and effort

313
00:19:41,460 --> 00:19:45,300
into the system and I know I can't even describe how language works.

314
00:19:45,300 --> 00:19:46,300
Yep.

315
00:19:46,300 --> 00:19:49,620
Right it just feels like there's just something inherently missing but I think neural nets

316
00:19:49,620 --> 00:19:51,420
have the opposite property.

317
00:19:51,420 --> 00:19:55,580
Neural nets is very clear this is a system that absorbs data it absorbs compute it's

318
00:19:55,580 --> 00:20:00,060
like a sponge that just like slurps everything up and so it has the right form factor but

319
00:20:00,060 --> 00:20:03,380
the thing that's always been missing as well can you train it right?

320
00:20:03,380 --> 00:20:07,300
Do you have enough data do you have enough compute do you have enough ability to like

321
00:20:07,300 --> 00:20:10,420
have a learning algorithm that can shovel the stuff in efficiently in a way that it comes

322
00:20:10,420 --> 00:20:14,380
out in some way that generalizes like that's the thing that's been missing and I think

323
00:20:14,380 --> 00:20:19,500
what became kind of clear you know the field really I think got its most recent resurgence

324
00:20:19,580 --> 00:20:27,660
in 2012 with the Alex in that paper and I think that there that was the first time where

325
00:20:27,660 --> 00:20:31,820
you had a neural net that really just crushed a task right that it was like people had spent

326
00:20:31,820 --> 00:20:36,940
decades on computer vision and suddenly it's like well I'm so sorry but this approach has

327
00:20:36,940 --> 00:20:41,380
just supplanted you by this this massive gap and I think that you just started to see it

328
00:20:41,380 --> 00:20:46,140
spread right that it was almost like you had these these all these disparate departments

329
00:20:46,140 --> 00:20:51,460
and there was this wall that was being knocked down day after day and I think that when you

330
00:20:51,460 --> 00:20:55,620
see a trend like that were things that have been long-standing and very deeply established

331
00:20:55,620 --> 00:20:59,820
in these ways of thinking these great debates that have gone on for a long time and suddenly

332
00:20:59,820 --> 00:21:05,380
you're seeing a repeated result that is consistent with the history I think that that for me

333
00:21:05,380 --> 00:21:08,940
is maybe the most clear sign that it's like something is going to work and there's a real

334
00:21:08,940 --> 00:21:12,060
sort of exponential that is waiting to unfold.

335
00:21:12,060 --> 00:21:17,940
And then you know were there what were the what were the moments if any of of doubt and

336
00:21:17,940 --> 00:21:22,620
you know let's let's chart the path I think open I start in 2016 yeah yeah I'd say December

337
00:21:22,620 --> 00:21:28,620
2015 you know 2016 okay great December 2015 till now were there any moments of of doubt

338
00:21:28,620 --> 00:21:33,820
in the technology or was it sort of always hey this is you know this is clearly the way

339
00:21:33,820 --> 00:21:39,500
of the future yeah I mean I think that doubt is a strong word there's definitely moments

340
00:21:39,500 --> 00:21:44,100
like I think to build something you you're always doubting right that you're always like

341
00:21:44,100 --> 00:21:47,660
you've got to be questioning every single bit of your implementation like anytime you

342
00:21:47,660 --> 00:21:51,140
see like a graph that's wiggling in a weird way you've got to go figure it out you can't

343
00:21:51,140 --> 00:21:55,940
just be like I'm sure that the AI's will sort it out.

344
00:21:55,940 --> 00:22:00,460
And so I think there was like lots of sort of tactical doubt lots of like sort of worries

345
00:22:00,460 --> 00:22:04,500
that were not quite doing it right lots of like redoing the calculations to figure out

346
00:22:04,500 --> 00:22:08,820
like hey how big of a model do you think you're going to need lots of mistakes for sure like

347
00:22:08,820 --> 00:22:13,700
a good example of this is the scaling laws so we did this study to actually start to

348
00:22:13,700 --> 00:22:18,700
really scientifically understand how do models improve as you push on various axes so as

349
00:22:18,700 --> 00:22:22,940
you pour more computing as you pour more data in and one conclusion that we had at one point

350
00:22:22,940 --> 00:22:28,620
was that basically that there's you know sort of a limited amount of data that you want

351
00:22:28,620 --> 00:22:32,500
to pour into these models and that there's kind of this very this very clear curve and

352
00:22:32,500 --> 00:22:36,700
that one thing that we realized only years later was actually that we'd read the curves

353
00:22:36,700 --> 00:22:41,120
a little bit wrong and you actually want to be trained for way more tokens way more data

354
00:22:41,120 --> 00:22:45,220
than anyone had expected and that did I you know that there's definitely these moments

355
00:22:45,220 --> 00:22:48,780
where these things that just didn't quite click where it's like just didn't add up that

356
00:22:48,780 --> 00:22:52,740
we were training for so little and that you know something conclusions that you drew downstream

357
00:22:52,740 --> 00:22:57,060
but then you realize there was a foundational assumption that was wrong and suddenly things

358
00:22:57,060 --> 00:23:00,780
make way more sense so I think it's a little bit like you know physics in some sense for

359
00:23:00,780 --> 00:23:04,620
like do you do doubt physics it's like I kind of do I think all physics is wrong right

360
00:23:04,620 --> 00:23:08,740
but like only so wrong right it's like we clearly haven't reconciled like quantum and

361
00:23:08,740 --> 00:23:12,620
relativity is that there's like something wrong there but that that wrongness is actually

362
00:23:12,620 --> 00:23:16,900
an opportunity it's actually a sign of you have this things are useful right really like

363
00:23:16,900 --> 00:23:20,060
it's affected our lives and it's actually like pretty great I'm very happy with what

364
00:23:20,060 --> 00:23:24,580
physics has done but also there's fruit and so I think that that for me that's always been

365
00:23:24,580 --> 00:23:29,620
the feeling that there's something here and that you know if we do keep pushing and somehow

366
00:23:29,620 --> 00:23:32,860
the scaling laws all peter out right and they suddenly drop off a cliff and we can't make

367
00:23:32,940 --> 00:23:37,140
any further progress like that would be the most exciting time in this field because we

368
00:23:37,140 --> 00:23:41,180
would finally reach the limit of technology we would finally learn something and then

369
00:23:41,180 --> 00:23:44,860
we would finally have a picture of what the next thing to do is yeah that's super it actually

370
00:23:44,860 --> 00:23:51,100
reminds me of this one of the stripe operating principles which is I think micro pessimists

371
00:23:51,100 --> 00:23:56,380
macro optimists yep yep yes and it's very I mean it's very resonant but obviously like

372
00:23:56,380 --> 00:24:01,980
very related to what you're talking about which is these you know you have to be extremely

373
00:24:01,980 --> 00:24:06,220
pessimistic we're extremely questioning in the moments of the technology but then obviously

374
00:24:06,220 --> 00:24:11,140
on a long enough time horizon incredible stuff pops out yep you got to be excited like I

375
00:24:11,140 --> 00:24:15,820
think that this is just an exciting field and it's a scary field as well you got to have

376
00:24:15,820 --> 00:24:20,980
some amount of just like awe at the fact that you have these these models that they started

377
00:24:20,980 --> 00:24:25,460
as just random numbers right and then you have build these massive supercomputers these massive

378
00:24:25,460 --> 00:24:30,500
data sets and you do a ton of the engineering work you do a ton of these algorithmic developments

379
00:24:30,580 --> 00:24:34,340
you put them all into a package right and we don't really have other technologies that

380
00:24:34,340 --> 00:24:39,220
work like this like I think the fact to me the most fundamental picture this like sponge

381
00:24:39,220 --> 00:24:43,260
that you just kind of pour stuff into and you get this model it's reusable and works

382
00:24:43,260 --> 00:24:47,420
across all these different areas like you can't do that for traditional software right

383
00:24:47,420 --> 00:24:51,380
traditional software is it's just you know human effort writing down all the rules and

384
00:24:51,380 --> 00:24:54,780
that's where the return comes from but you can't you know maybe you have like a spark

385
00:24:54,780 --> 00:24:59,940
cluster that does some stuff but that's not that's not the cake and in in neural networks

386
00:24:59,940 --> 00:25:04,900
it really is yeah you know I want to kind of switch gears to thinking about the the

387
00:25:04,900 --> 00:25:10,460
sort of future and and and looking forward at what kind of what's next what do you think

388
00:25:10,460 --> 00:25:13,800
I mean I'll ask this sort of as broadly as possible to start with what do you think the

389
00:25:13,800 --> 00:25:22,300
future of AI holds yeah I think that the future of AI is going to again be both exciting and

390
00:25:22,300 --> 00:25:25,620
a source of a lot of change and I think that that is something that you know part of our

391
00:25:25,640 --> 00:25:31,180
mission is to try to help facilitate that as positive way as possible I think that kind

392
00:25:31,180 --> 00:25:36,700
of you know the super high level I kind of feel like AI was like you know something that

393
00:25:36,700 --> 00:25:40,620
for the you know 2020 10s was like kind of cool you know it's the game of like publishing

394
00:25:40,620 --> 00:25:44,340
papers and you play some video games and like you know it's just like it's just like fun

395
00:25:44,340 --> 00:25:51,820
good science I think it's really interesting that 2020 kicked off with GPT 3 which is really

396
00:25:51,900 --> 00:25:56,620
the first model that was commercially useful just as the model like literally put an API

397
00:25:56,620 --> 00:26:01,780
on top of it people just talk to it and people build products on top of it and you know that

398
00:26:01,780 --> 00:26:06,260
you know one of our early customers just you know just raised it at 1.5 billion valuation

399
00:26:06,260 --> 00:26:10,540
which to me is is a really wonderful thing to realize that you build this model and it

400
00:26:10,540 --> 00:26:15,900
creates so much value for so many different people and I think that we're still in such

401
00:26:15,900 --> 00:26:20,300
early days for what these models can do and so I think that what I'm most excited about

402
00:26:20,340 --> 00:26:26,620
from just seeing GPT 3 seeing Dolly is thinking about the the sort of economic value that

403
00:26:26,620 --> 00:26:30,620
it can create for people and I think that there's a lot of other pieces to it in terms

404
00:26:30,620 --> 00:26:35,260
of like you know that everyone's going to be more creative if you want to like I can't

405
00:26:35,260 --> 00:26:38,780
draw but now I can create images now I could take a picture of this in my head and I can

406
00:26:38,780 --> 00:26:42,940
actually see it on on a page and one of my favorite applications of Dolly is actually

407
00:26:42,940 --> 00:26:48,020
people who are 3D physical artists you know something's like a sculptor and now they

408
00:26:48,060 --> 00:26:51,500
can actually get a great rendering of the thing that they have in mind by kind of just

409
00:26:51,500 --> 00:26:55,460
like iterating with this machine and they go build it right I think that this sort of

410
00:26:55,460 --> 00:27:00,580
amplification of what humans can do is what these systems are for and so I think that

411
00:27:00,580 --> 00:27:03,740
for this decade I think what we're really going to see is these tools just sort of

412
00:27:03,740 --> 00:27:07,100
proliferating they're going to be everywhere they're going to be baked into every company

413
00:27:07,100 --> 00:27:11,620
I think it's kind of like the internet transition that you know that it was kind of like if

414
00:27:11,620 --> 00:27:15,500
you're a company like what's your internet strategy and you know 1990 it's like what

415
00:27:15,540 --> 00:27:19,540
even is this thing you know and in 2000 it's like huh maybe it's interesting and there

416
00:27:19,540 --> 00:27:24,340
there's a little you know boom and bust and here we are today even talk about an internet

417
00:27:24,340 --> 00:27:28,060
strategy is like it's just so integral to every business it's not even like it's not

418
00:27:28,060 --> 00:27:31,380
even a separate thing right it's just like it's just part of like your it's like your

419
00:27:31,380 --> 00:27:34,380
payroll strategy right it's like it's not like a separate part of your business that

420
00:27:34,380 --> 00:27:37,740
you can pick or choose whether you're going to have it and I think that AI is going to

421
00:27:37,740 --> 00:27:41,820
be much the same I think there will be a transition point right I think that it's it's

422
00:27:41,820 --> 00:27:45,060
interesting like our mission is really about building artificial general intelligence right

423
00:27:45,100 --> 00:27:49,620
really trying to build machines that are able to perform whole tasks right that are you know

424
00:27:49,620 --> 00:27:54,660
push this technology to its limit and build machines that are able to you know our charter

425
00:27:54,660 --> 00:27:58,540
definition is outperform humans at most economically valuable work and there's a question of the

426
00:27:58,540 --> 00:28:02,980
timeline but I think that that picture of you know you have these tools that are creative

427
00:28:02,980 --> 00:28:07,700
that help everyone amplify and what happens when they do become so capable that they're

428
00:28:07,700 --> 00:28:12,380
able to perform these tasks even autonomously and I think that actually the implications

429
00:28:12,380 --> 00:28:16,460
of that are different from what people expect I think that it's much more like you know that

430
00:28:16,460 --> 00:28:20,860
I think there's still going to be this amplification but I think that there the change is going

431
00:28:20,860 --> 00:28:26,300
to be just very hard to predict and unexpected and I think that really thinking about how

432
00:28:26,300 --> 00:28:30,820
all of that sort of value gets distributed how to make sure that it's sort of pointed

433
00:28:30,820 --> 00:28:35,020
at solving these like hard challenges that humans you know maybe are unable to solve

434
00:28:35,020 --> 00:28:38,780
ourselves you know the climate change and you know universal education and things like

435
00:28:38,860 --> 00:28:44,220
that and really transitioning to this like AI powered world I think is going to be just

436
00:28:44,220 --> 00:28:48,140
like a real sort of challenge for the whole you know all of humanity to work together

437
00:28:48,140 --> 00:28:53,380
on. Yeah I mean I totally agree one thing that I think is almost funny with how the timing

438
00:28:53,380 --> 00:28:56,820
of all these technologies have worked out is that you know last year everyone was talking

439
00:28:56,820 --> 00:29:04,100
about Web 3 as crypto and now it feels very obvious that AI is the actual Web 3 you know.

440
00:29:04,180 --> 00:29:08,540
We'll take Web 4. Yeah Web 4 we'll skip we'll skip over one but sort of like Web 1 was just

441
00:29:08,540 --> 00:29:14,140
reading Web 2 was reading and writing and now Web 3 or 4 depending on what we want to

442
00:29:14,140 --> 00:29:20,540
say is ads, computer reading, computer write and it's sort of this incredible new phase.

443
00:29:20,540 --> 00:29:24,420
You know one so I think I think you mentioned two two directions here that I think are really

444
00:29:24,420 --> 00:29:31,780
interesting one is the sort of advancement and sort of proliferation of GP3 and Dolly

445
00:29:31,860 --> 00:29:35,660
and sort of the existing tools becoming more and more economically useful and there's

446
00:29:35,660 --> 00:29:40,460
sort of this continued improvement of the algorithms themselves towards sort of towards

447
00:29:40,460 --> 00:29:44,860
sort of AGI. What do you think and obviously don't reveal any opening secrets but what

448
00:29:44,860 --> 00:29:49,140
do you think this sort of like roadmap to AGI looks like from where we are now? I mean

449
00:29:49,140 --> 00:29:54,700
I think that humanity to a large extent has been on the AGI roadmap for a very very long

450
00:29:54,700 --> 00:30:00,900
time. I think even looking at just the history of neural networks in particular you know

451
00:30:00,940 --> 00:30:04,820
on the one hand we say hey 2012 like that was the moment like everything changed you

452
00:30:04,820 --> 00:30:08,500
know that like you look at these we have all these curves of how much compute people put

453
00:30:08,500 --> 00:30:12,060
into the landmark results it was going like 10x year over year still continuing by the

454
00:30:12,060 --> 00:30:19,220
way that's that's a decade of 10x year over year that's insane and the thing is we actually

455
00:30:19,220 --> 00:30:25,780
did a study to then look back at previous results all the way back to you know say the

456
00:30:25,820 --> 00:30:32,340
perceptron in 1959 and you actually find that there's basically a very smooth curve back

457
00:30:32,340 --> 00:30:36,220
there as well. The amount of compute going into all the landmark results was exactly

458
00:30:36,220 --> 00:30:41,020
Moore's law and it kind of makes sense right it's like that people were not willing to

459
00:30:41,020 --> 00:30:45,580
spend more money they wanted to spend a constant amount of money on these experiments because

460
00:30:45,580 --> 00:30:50,060
you're starving grad students like you know you can only get so much computer time and

461
00:30:50,100 --> 00:30:56,380
that the results got better and better the more compute was available to them and I think

462
00:30:56,380 --> 00:30:59,740
that that is so interesting that yeah basically what changed in 2012 was that we said okay

463
00:30:59,740 --> 00:31:02,660
we're just gonna like you know we are gonna spend more money we're gonna build massive

464
00:31:02,660 --> 00:31:08,500
supercomputers now because the ROI is there but that fundamentally the curve if you control

465
00:31:08,500 --> 00:31:12,100
for that that cost factor it looks exactly the same and so I think that basically this

466
00:31:12,100 --> 00:31:16,500
picture of building more capable models by pouring more compute into them by getting

467
00:31:16,580 --> 00:31:21,260
better at harnessing this technology of neural networks back propagation I think that has

468
00:31:21,260 --> 00:31:24,940
been very invariant and the details you know maybe change a little bit you know do you want to

469
00:31:24,940 --> 00:31:30,140
work on GPT-3 do you want to work on whisper like do you pour in your your you know speech

470
00:31:30,140 --> 00:31:34,700
data do you pour in text data from the internet and to me those details I think you know they

471
00:31:34,700 --> 00:31:38,460
matter in the like in the like sense of like what are you gonna work on today and you know

472
00:31:38,460 --> 00:31:42,020
what are you gonna download but if you zoom out you look at the scale of like these this

473
00:31:42,060 --> 00:31:47,300
technology I think it actually sort of doesn't matter so much I think kind of what we're

474
00:31:47,300 --> 00:31:49,940
building it's almost like building computers like you think about the Haiti and Moore's

475
00:31:49,940 --> 00:31:52,900
law right where it's just like there's a new chip that comes out and there's a new chip that

476
00:31:52,900 --> 00:31:56,380
comes out and it's kind of like what's the you know what's the path to building the best

477
00:31:56,380 --> 00:31:59,860
computer the answer is well you just keep building the next best chip and you keep building

478
00:31:59,860 --> 00:32:03,620
the next best chip and you keep getting better peripherals and all these you know keep working

479
00:32:03,620 --> 00:32:09,220
every single piece of the technology and so I think this full stack of better GPUs great

480
00:32:09,260 --> 00:32:13,060
software for utilizing them neural networks that we learned to harness more and more the

481
00:32:13,060 --> 00:32:17,340
scaling laws doing all the science alignment extremely important making sure these models

482
00:32:17,340 --> 00:32:22,940
not just are smart but actually are aligned with what humans intend all of that I think is the

483
00:32:22,940 --> 00:32:27,300
stack and so I think that you know what our goal is is just to keep doing something that was

484
00:32:27,300 --> 00:32:31,540
previously impossible every single year so you know that I guess well you should check back

485
00:32:31,540 --> 00:32:37,420
in a year but hopefully 2023 we'll all forget about Dolly 2 and GPT 3 and we'll be talking

486
00:32:37,420 --> 00:32:42,820
about something new and I think as long as we continue that like you cannot continue that

487
00:32:42,820 --> 00:32:49,020
path without ending up somewhere amazing yeah I mean I think I actually remember this in I

488
00:32:49,020 --> 00:32:55,980
think probably 2017 you were sort of very still quite you were very excited about sort of the

489
00:32:55,980 --> 00:33:02,980
sort of Moore's law continuing and that that sort of creating a lot more opportunity for you

490
00:33:02,980 --> 00:33:08,420
know neural networks and AI and that's that's sort of played out are you worried about the sort

491
00:33:08,420 --> 00:33:16,380
of proverbial end of Moore's law kind of causing a stall out in in progress so I'm not worried

492
00:33:16,380 --> 00:33:19,820
about it per se like I think the way to think about this right because I think we you know we

493
00:33:19,820 --> 00:33:23,660
often get caught in this debate of like is it all about scale or is it all about algorithms is

494
00:33:23,660 --> 00:33:28,540
all about data and the answer is that's a wrong question right it's really like you multiply

495
00:33:28,540 --> 00:33:31,660
together these factors and the best thing to do when you're multiplying together multiple terms

496
00:33:31,740 --> 00:33:36,060
is that you actually kind of want them all to be equal and I think that the answer is like it's

497
00:33:36,060 --> 00:33:41,980
been great for the past you know seven years that we've been able to just pour more dollars to

498
00:33:41,980 --> 00:33:45,740
build bigger computers that's one way to get ahead of Moore's law at some point they're just

499
00:33:45,740 --> 00:33:49,900
aren't more dollars right there aren't more grains of sand to you know that have been turned into

500
00:33:49,900 --> 00:33:56,860
into these these wonderful computers that we use so there is a limit there that we have not yet

501
00:33:56,940 --> 00:34:02,620
hit but when you do that does not stall all progress right you still have algorithmic progress

502
00:34:02,620 --> 00:34:07,100
and there we've again done studies and we've shown that actually if you take like an example is if

503
00:34:07,100 --> 00:34:12,220
you look at the amount of compute it takes to hit the same performance so to train you know a state

504
00:34:12,220 --> 00:34:19,740
of the art that you know 2012 or 2014 vision model that that computes also falling exponentially

505
00:34:19,740 --> 00:34:24,460
we're basically making exponential progress in algorithms not at the same rate as we are able

506
00:34:24,460 --> 00:34:29,180
to you know sort of build bigger computers but that is an amazing force too you know it's like

507
00:34:29,180 --> 00:34:33,020
I've got this exponential I've got that exponential like let's not even talk about the data exponential

508
00:34:33,020 --> 00:34:37,900
so I think that that the truth is that we will find a way I think that the history of this field

509
00:34:37,900 --> 00:34:43,020
is just so consistent and I think that that you know humanity is just so innovative that I think

510
00:34:43,020 --> 00:34:49,660
that that we're not going to hit a wall for the foreseeable future and do you think that you know

511
00:34:49,660 --> 00:34:56,380
one of the one of the interesting juxtapositions of of today just from a scientific perspective is

512
00:34:56,380 --> 00:35:01,740
a relative slowing in nearly every other science and there's you know there's a lot of research

513
00:35:01,740 --> 00:35:05,740
that sort of demonstrated that science on the whole slowing and then comparatively the sort of

514
00:35:05,740 --> 00:35:09,980
acceleration of artificial intelligence and sort of this this you know in many ways this

515
00:35:09,980 --> 00:35:16,220
renaissance that we're entering right now do you do you fear that at some point AI will similarly

516
00:35:16,220 --> 00:35:20,860
sort of reach these points of admission mark returns and slow relative like in much in the

517
00:35:20,860 --> 00:35:25,580
same way that other sciences have or do you think that's so far away that you know well I think

518
00:35:25,580 --> 00:35:29,820
two things I mean I think that there's there's always s-curves although I think that something

519
00:35:29,820 --> 00:35:33,980
is also interesting about s-curves is that there tends to be paradigm shifts like have you ever

520
00:35:33,980 --> 00:35:40,060
read Singularity is Near no I haven't yeah so this is the Ray Kurzweil book from like 2004

521
00:35:40,060 --> 00:35:45,420
something and I always thought just based on the reputation it's going to be kind of a crazy book

522
00:35:45,420 --> 00:35:50,140
but if you actually read it it's the most dry boring reading you'll ever do and it's basically

523
00:35:50,140 --> 00:35:56,540
just curve after curve of different industries within computing showing how the performance has

524
00:35:56,540 --> 00:36:02,060
changed over time and it's you know basically the conclusion he comes to is that there's this

525
00:36:02,060 --> 00:36:07,020
repeated pattern that seems to happen across you know memory across number of transistors on the

526
00:36:07,020 --> 00:36:12,220
chip you know etc etc where you kind of have an s-curve of the current paradigm and then you have

527
00:36:12,300 --> 00:36:19,580
paradigm shift and that example he talks about is you know thinking about let's talk about CDs

528
00:36:19,580 --> 00:36:23,500
right so you talk about great you know CD adoption it's like you know it's great s-curve it's

529
00:36:23,500 --> 00:36:27,340
suddenly everywhere it's like everyone's got a CD player like it's just the technology of the day

530
00:36:27,900 --> 00:36:31,500
and people get really excited about doing more of the same thing it's like Blu-ray that's the thing

531
00:36:31,500 --> 00:36:36,460
you know and so then everyone starts investing in Blu-rays and somehow it just doesn't take off

532
00:36:36,460 --> 00:36:39,740
and it's because it's just more of the same and it's like you know it's not backwards compatible

533
00:36:39,740 --> 00:36:43,580
and so it's just not really worth it but the real paradigm shift was streaming right suddenly

534
00:36:43,580 --> 00:36:47,420
you have this new adoption curve this new s-curve that just is this like totally different way

535
00:36:47,420 --> 00:36:50,540
and the way we got to fast computers was basically five different paradigm shifts

536
00:36:50,540 --> 00:36:54,380
across a hundred years and so I think that that's maybe a story here too which is like

537
00:36:54,380 --> 00:36:58,140
there's gonna be an s-curve in what we're doing right now and that there will be a paradigm shift

538
00:36:58,140 --> 00:37:02,140
when you hit it and I think that that again speaks to the ingenuity of humans but I think there's

539
00:37:02,140 --> 00:37:09,260
also a second thing where my other answer is to some extent it doesn't matter because the thing

540
00:37:09,260 --> 00:37:14,220
about this field is that it's useful now right that kind of the goal that I think we've always

541
00:37:14,220 --> 00:37:19,100
had for AI was to actually make us so computers are just way more helpful like you think about what

542
00:37:19,100 --> 00:37:22,780
computers have done for humanity right like how many problems they've helped us solve they've

543
00:37:22,780 --> 00:37:26,540
created new problems as well but I think that on net that they've helped us solve way more problems

544
00:37:26,540 --> 00:37:30,540
than they've created and I think they've kind of just changed the nature of how we interact with

545
00:37:30,540 --> 00:37:34,780
each other about how you know like it's just like hard to get lost anymore right you just

546
00:37:34,780 --> 00:37:39,020
pull out google maps I think there's really amazing problems that are now within our reach

547
00:37:39,020 --> 00:37:43,740
that just would not have been otherwise and I think that AI like we're starting to crack that

548
00:37:43,740 --> 00:37:47,900
nut we're starting to be able to you know like it's I think it's kind of interesting you could

549
00:37:47,900 --> 00:37:54,220
get a co-pilot you know which which we we power we have the models that power it and that the way

550
00:37:54,220 --> 00:37:58,300
that that is useful to people is that provides very low latency suggestions right it's basically

551
00:37:58,300 --> 00:38:03,020
an autocomplete for code and that you know there's a very strict latency budget you know if you're

552
00:38:03,020 --> 00:38:07,740
more than you know 1500 milliseconds to get a autocomplete suggestion it's worthless like no

553
00:38:07,740 --> 00:38:13,660
one wants that you've already moved on but I think that what we really want to build the next

554
00:38:13,660 --> 00:38:19,100
phase is machines that help you produce that are able to produce artifacts that are materially

555
00:38:19,100 --> 00:38:23,100
interesting on their own so not just interesting because it's like a fast suggestion to you but

556
00:38:23,100 --> 00:38:27,260
because it's actually a quality answer and you're starting to see if you talk to our current gbt

557
00:38:27,260 --> 00:38:31,100
iteration you can ask it to write some poems and it writes way better poetry than I can

558
00:38:31,820 --> 00:38:39,820
it actually wrote a poem for my wife that made us both cry like you know yeah I I cannot do that

559
00:38:39,820 --> 00:38:45,660
myself but now I can you know by partnering with this with this machine and I think that

560
00:38:45,660 --> 00:38:50,140
that's the real story right is really trying to get these tools out and everywhere and yeah you

561
00:38:50,140 --> 00:38:55,020
know if what we're doing right now stalls out I don't think that removes the value from what we're

562
00:38:55,020 --> 00:39:01,020
able to create yeah by the way it's depressing that the the attention span of most engineers is

563
00:39:01,020 --> 00:39:09,340
only 1500 seconds but you know it is what it is I what uh what if anything you know I think

564
00:39:10,060 --> 00:39:15,260
one of the things that if I recall spurred you to to work on opening I was was sort of

565
00:39:16,140 --> 00:39:20,780
also being concerned about the sort of potential negative consequences of of the technology

566
00:39:20,780 --> 00:39:25,900
um what at this point looking forward what are your sort of biggest concerns or what are you

567
00:39:25,900 --> 00:39:30,300
afraid of with with artificial intelligence that you sort of urge everyone in the field to sort of

568
00:39:30,300 --> 00:39:36,540
help avoid yeah so I think that one thing that's very interesting about AI is that you know if you

569
00:39:36,540 --> 00:39:41,180
talked to certainly you know 10 years ago if you looked at every article about it you talked to

570
00:39:41,180 --> 00:39:45,100
someone on the street terminator is the main thing that comes up right so I think there's always been

571
00:39:45,180 --> 00:39:51,820
this feeling around AI that is sort of you know that there's an element of fear mixed with the

572
00:39:51,820 --> 00:39:54,940
you know sometimes people don't see any potential or sometimes you know they realize that there is

573
00:39:54,940 --> 00:40:00,380
the potential but like really trying to figure out and navigate it and I think that that picture

574
00:40:00,380 --> 00:40:04,060
the specifics you know I think that that we're starting to see a little bit more but I think

575
00:40:04,060 --> 00:40:07,900
the high level picture of this is technology that's very powerful and it can be powerful in

576
00:40:07,900 --> 00:40:12,380
positive ways and negative ways um I think is extremely correct and I think it's very important

577
00:40:12,380 --> 00:40:16,220
not just to be you know starry eyed optimist everything's just going to work itself out

578
00:40:16,220 --> 00:40:21,900
but also not to be you know sort of doomsday like everything is terrible and you know humanity is

579
00:40:21,900 --> 00:40:25,980
over because I don't think that's at all true I think this technology can be the the best thing

580
00:40:25,980 --> 00:40:29,980
that is the way we've ever created and help us be the best versions of ourselves but I think

581
00:40:29,980 --> 00:40:35,020
that it requires very careful navigating of the space and it's not something that just is for you

582
00:40:35,020 --> 00:40:38,380
know companies of Silicon Valley to figure out I think it's it's really all of humanity kind of

583
00:40:38,380 --> 00:40:43,660
challenge um so I think that we're going to go through different phases I think that that right

584
00:40:43,660 --> 00:40:48,540
now I you know we're kind of starting to build systems where that I you know you think about

585
00:40:49,260 --> 00:40:55,020
misuse is the most clear problem and the systems themselves are still not very powerful right

586
00:40:55,020 --> 00:40:59,500
that the kinds of things you worry about for GPT-3 are you know important problems you think

587
00:40:59,500 --> 00:41:04,140
about bias and representation you think about the system sort of you know sort of saying the wrong

588
00:41:04,140 --> 00:41:08,300
thing you know but but its action is really in your mind right it's it's sort of words on a page

589
00:41:08,300 --> 00:41:12,380
that then you know words on a page are very powerful but that they don't themselves have

590
00:41:12,380 --> 00:41:16,460
direct action in the world but you think about something like codex our code writing system

591
00:41:16,460 --> 00:41:21,500
which is a little bit more like a robot because it has it emits code and if you were to just execute

592
00:41:21,500 --> 00:41:26,700
that code directly it can actually directly have actuators into the world and making sure that that's

593
00:41:26,700 --> 00:41:31,500
aligned and doing the right kinds of things not having buggy code and not writing viruses and that

594
00:41:31,500 --> 00:41:35,260
kind of thing like that's really important and so I think that that figuring out what values go

595
00:41:35,260 --> 00:41:39,020
into this these machines and that they're operating according to those values that's going to be very

596
00:41:39,020 --> 00:41:45,020
very critical figure out how to avoid misuse and sort of regulate that both at a sort of societal

597
00:41:45,020 --> 00:41:50,700
level at a you know technical level all of that is very important and I and I do think that there

598
00:41:50,700 --> 00:41:55,260
is also a point where the technology itself you have to think about that it's going to be extremely

599
00:41:55,260 --> 00:42:00,540
powerful and you think about a system that's you know sort of talking to lots of humans and

600
00:42:00,620 --> 00:42:03,740
is operating unchecked that's the kind of thing that you should worry about you know we already

601
00:42:03,740 --> 00:42:08,460
worry about that think about the companies right that lots of people are using this you know social

602
00:42:08,460 --> 00:42:12,860
media platform or you know any of the technologies that we use and how much influence those can have

603
00:42:12,860 --> 00:42:19,740
in the world and those aren't systems that have you know sort of deep sort of behaviors that are

604
00:42:19,740 --> 00:42:24,300
emergent from from from what they've learned and so I think that that figuring out the technical

605
00:42:24,300 --> 00:42:29,500
controls to make sure that these systems remain in service of humanity and sort of to

606
00:42:29,580 --> 00:42:35,340
actually empower and accelerate all of us that I think is is also a very critical thing so it's

607
00:42:35,340 --> 00:42:39,340
kind of this like a ramping set of stakes and making sure that we're building systems that are

608
00:42:39,340 --> 00:42:44,220
aligned with with our values and figuring out what that even means like what what is the values

609
00:42:44,220 --> 00:42:47,900
of humanity that should be in the system and that I think is not going to be an easy problem

610
00:42:48,780 --> 00:42:54,620
you know one question and this this may be the last question that I have for you is sort of one

611
00:42:54,620 --> 00:43:01,900
of the thing one of the conclusions of if the technology such that scale continues to be the

612
00:43:01,900 --> 00:43:06,300
the sort of one of the more important things you know scale whether it's data or better algorithms

613
00:43:06,300 --> 00:43:13,660
or scale compute then it it the technology itself will tend towards sort of this game theoretical

614
00:43:13,660 --> 00:43:18,780
proliferation mode where it's sort of like people are going to compete and you see some of this today

615
00:43:18,780 --> 00:43:22,060
even with the large tech companies and you guys obviously people are going to compete to sort of

616
00:43:22,060 --> 00:43:26,620
build the bigger supercomputers that have the better performance and you have the bigger supercomputer

617
00:43:26,620 --> 00:43:30,300
you have sort of supremacy over the other supercomputers and sort of there's like this

618
00:43:31,180 --> 00:43:36,380
you know laddering the stakes and sort of and proliferation is really the sort of the right

619
00:43:36,380 --> 00:43:41,500
word do you think that that is a version of the future or do you think that there's sort of

620
00:43:41,500 --> 00:43:47,100
some path in which this becomes a much more sort of like open and useful not this sort of like

621
00:43:47,180 --> 00:43:53,500
tool for nation states or large companies to compete with one another I think that the future

622
00:43:53,500 --> 00:44:02,300
that seems to be unfolding is kind of a you know replay of how say computing technology has played

623
00:44:02,300 --> 00:44:08,060
out more broadly I think that that it is still going to be the case that you're going to have

624
00:44:08,060 --> 00:44:13,180
these increasingly massive supercomputers that are in the hands of only a few that are able to just

625
00:44:13,180 --> 00:44:17,980
create models that can just do crazy things that no one else can do but I don't think that that

626
00:44:17,980 --> 00:44:22,860
removes the value from the massive set of things that people are going to do with these models

627
00:44:22,860 --> 00:44:28,780
and so you know I think that balancing the like super powerful very dual use extremely you know

628
00:44:28,780 --> 00:44:33,660
think of these like almost like you know these massive like you know sort of systems that are

629
00:44:33,660 --> 00:44:36,620
you know we think about like a nuclear reactor and it's like you know it's like these like giant

630
00:44:36,620 --> 00:44:41,100
like sort of you know systems that you should approach with great care um and you think about

631
00:44:41,100 --> 00:44:45,180
like you know by contrast think about wind turbines and like there's lots of wind turbines

632
00:44:45,180 --> 00:44:48,220
everywhere and actually that if you add up the amount of value from wind turbines versus nuclear

633
00:44:48,220 --> 00:44:51,900
reactors like I think actually that the balance is probably in favor of wind turbines and so I

634
00:44:51,900 --> 00:44:56,220
think that that's kind of the future we're going to is that the AI technology is going to be everywhere

635
00:44:56,220 --> 00:45:01,420
and there's going to be lots of value that's delivered by having open source models that are

636
00:45:01,420 --> 00:45:05,100
integrated to every business and that people are building all sorts of crazy applications on top of

637
00:45:05,100 --> 00:45:09,100
and that's something that we really want to support and promote and you also have to have this

638
00:45:09,100 --> 00:45:13,420
dual answer for what you do with the with the new extremely capable stuff that's just a mile ahead

639
00:45:13,420 --> 00:45:18,140
of everything else and that's something you have to treat with kid gloves with with more care

640
00:45:18,140 --> 00:45:22,540
and I think that that balance is tricky it's not easy um that's something that we as an organization

641
00:45:22,540 --> 00:45:25,740
have been trying to straddle and I think that you know we've had real existential struggles

642
00:45:25,740 --> 00:45:29,980
internally trying to figure out you know like our goal is to empower everyone it's really to uh

643
00:45:29,980 --> 00:45:35,660
to to bring everyone along to this AI transition and the best way to do that I think that our

644
00:45:35,660 --> 00:45:39,740
picture of it has changed as the technology has unfolded and I think that we're starting to get a

645
00:45:39,740 --> 00:45:44,940
sense of of you know where this can go it's really exciting to see all the the energy of all these

646
00:45:44,940 --> 00:45:48,460
builders coming in because I think that like you said people are starting to realize like AI is

647
00:45:48,460 --> 00:45:53,580
really going to work and it's time to build yeah well um this was an incredible conversation thank

648
00:45:53,580 --> 00:45:58,860
you so much Greg next time we speak I'll make you uh read the poem that all right there we go

649
00:45:58,860 --> 00:46:02,060
cool thank you so much thank you so much

