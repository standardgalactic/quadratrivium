start	end	text
0	14720	We're joined next by Greg Brockman, President, Chairman and Founder of Open AI and Alexander
14720	18360	Wang, CEO and Founder of Scale AI.
18360	23520	Open AI is a research and deployment company whose mission is to ensure general-purpose
23520	27280	artificial intelligence benefits all of humanity.
27280	34520	For Open AI, Greg was the CTO of Stripe, which he helped build from 4 to 250 employees.
34520	40520	Please join me in welcoming to the stage Greg Brockman and Alexander Wang.
40520	42520	Hey Greg.
42520	44520	Hey.
44520	47000	Thanks for making it.
47000	48000	Absolutely.
48000	55840	I'd be happy to be here.
55840	60880	I want to start actually, I don't know if you remember this, but we first met at this
60880	67000	summer camp called Spark where you gave a presentation about, at the time you were the
67000	70560	CTO of Stripe and you gave this presentation about sort of like everything you had accomplished
70560	73880	and I was a member of that camp and it was extremely memorable.
73880	75880	You had a lot of good sound bites.
75880	78520	I'm glad that it landed.
78520	80960	Kind of a full circle moment.
80960	85400	Well I think to start out with, I mean you've been CTO and now you're President of Open
85440	92840	AI, but CTO of two incredibly iconic companies, Stripe and Open AI, in some ways probably two
92840	96960	of the most iconic startups of the past decade.
96960	103320	I wanted to start out just by asking in what ways are the two organizations the same and
103320	106280	being CTO the same and in what ways are they different?
106280	108520	Thank you for the kind of words.
108520	113360	I think that one thing that's very interesting to me about kind of having been part of both
113360	119280	of these organizations is seeing how much groups of people are kind of the same regardless
119280	122160	of what the problem in front of you is.
122160	127320	So I think that a lot of how we approached Stripe was thinking from first principles.
127320	132000	I remember when we were pre-launch and we had some buzz going because we had some early
132000	136480	customers and one of my friends took me out to launch, he was like all right, I've been
136480	139720	hearing about this Stripe thing, what's your secret sauce?
139800	144480	I was like, I mean we just make payments really good and he's like no, no, no, come on, you
144480	147040	can tell me, what's the secret sauce?
147040	150800	And really that was the secret sauce, right, is that we had just rethought every single
150800	154480	piece of what we were doing from the ground up from first principles, not sort of locked
154480	157880	into the way that people had been doing it and we asked how should it be, like where's
157880	160960	all the pain and does it need to be there?
160960	164840	I think that in AI we did much the same, we really thought about okay, there's this field
164840	168000	that we're entering and that we hire a lot of people who had been in the field, but a
168000	171440	lot of us also hadn't been in the field and we came to it with beginner's eyes and I think
171440	175360	that that approach of just not being beholden to all the ways people were doing it, but
175360	179840	also becoming expert in the way that things have been done because if you just throw everything
179840	184600	out, like you're also just going to be starting from scratch in a not helpful way.
184600	188360	So I think that that maybe is the deepest commonality between them.
188360	193640	But obviously very different organizations, for Stripe I think that we ran the traditional
193640	194800	startup playbook.
194800	198240	You basically come up with the innovation and you just build, build, build.
198240	206440	You get in front of customers from day one, the story is that we gave the first API to
206440	210080	a customer who charged a credit card and he was like, I would like my money now please
210080	214840	and we were like, huh, I guess we should build that.
214840	219800	Open AI, we had research to do, like where's the customer?
219800	225080	And it really took us I guess five years, starting late 2015 and it's really not until
225080	228240	2020 that we had our very first product.
228240	232400	And so I think that that sort of figuring out like what you're even supposed to work
232400	237520	on, like did you do a good job, should you feel good on a day-to-day basis?
237520	240720	I think that all of that had to come from within rather than from without.
240720	241720	Yeah.
241720	244760	Well, actually I want to go back to this point that you mentioned around first principles
244760	245760	thinking.
245760	251560	It's very interesting because even I remember like maybe 2020 or 2021, you know, you would
251560	256880	sort of post-GPD3, you would talk to other researchers in the field and even they would
256880	262080	still, you know, there's still like some degree of skepticism over the sort of like core concept
262080	266000	of scaling up these models and if there were still gains to be had, et cetera.
266000	271600	And I think, you know, I don't know the story but it seems like the sort of research sort
271600	278360	of intuition that led to GPD3, Dolly 2 that have really ushered in kind of a new era of
278360	284760	AI were probably, you know, somewhat against the grain or somewhat unintuitive at the time.
284760	290800	You know, one question I have for you is, you know, I think now looking back it's obviously
290800	297840	very obvious to point out GPD3, Dolly 2 basically have fundamentally accelerated AI progress
297840	302520	and its relevance to the world and its relevance to every industry and sort of have created
302520	307040	the sort of most recent AI wave.
307040	311680	How is that matched up against your expectations when you were building these technologies?
311680	312680	You know?
312680	313680	Yeah.
313680	317720	Well, I think the thing that's most interesting to me is that those models you mentioned are
317720	321640	kind of overnight successes that took many, many years to create.
321640	324760	And so, you know, from the outside it looks like, wow, you just like produce this model
324760	329520	and that model and really on the inside, the GPT arc, that's a five-year arc, right?
329520	333640	It really started with sentiment neuron paper which is back in 2017.
333640	334640	Do you remember that paper?
334640	335640	I remember the paper.
335640	336640	It was very cool.
336640	337640	Yeah.
337640	338640	But it felt very novel.
338640	339640	It felt very novel.
339640	340640	Yeah.
340640	341640	Very few people remember it.
341640	345400	It's, you know, it was this very early result where we basically had been training a LSTM
345400	348040	at the time to predict the next character in text.
348040	349800	So, we basically showed a bunch of Amazon reviews.
349800	352880	We said, what's the next character and of course it's going to learn where the commas
352880	353880	go.
353880	354880	Where the periods go.
354880	357800	But of course it's not going to understand anything.
357800	362360	But we found a single neuron in that model that had learned a state-of-the-art sentiment
362360	363360	analysis classifier.
363360	366080	I can tell you this is a positive review or negative review.
366080	367080	That's understanding.
367080	370560	You know, I don't know what understanding means but it's semantics for sure.
370560	374040	And that for us was like, okay, this is going to work.
374040	380120	The transformer came out late 2017 and my co-founder Ilya immediately was like, that's
380120	381120	the thing.
381120	382120	That's what we've been waiting for.
382120	386560	So, you take this sort of very early nascent result, put in a transformer and then that's
386560	387560	GPT-1.
387560	390040	GPT-2 is you just keep pushing it.
390040	393840	And you know, I think that the algorithm we kind of run internally is that we do these
393840	398240	little sort of get signs of life and you have to be very, very careful to distinguish signs
398240	401840	of life from like kind of just pushing too hard on a specific data set that isn't really
401840	402840	going to keep going.
402840	406280	But if you kind of build those right intuitions then you know, okay, now is the time to put
406280	407280	in more compute.
407280	408280	Now is the time to put in more researchers.
408280	410640	Now is the time to like really scale it up.
410640	416040	And so GPT-2 obviously was exciting and that we were all like, well, we look at the curves.
416040	418880	You know, the bigger we made this model, the more compute we put in, the more data we put
418880	423800	in, the more we just sort of got all the engineering details right, those curves just got better.
423800	426880	And so actually, you know, our goal was just to break the paradigm.
426880	430600	It was just push it until the curve stopped looking good and we still haven't managed
430600	431600	to accomplish it.
431600	432600	Yeah.
432600	438120	Well, I think one of the things, at least for me and probably for many people who initially
438120	442440	played with GPT-3, the like shocking thing was not, I mean, it wasn't necessarily that
442440	447800	even the model got better and better performance on, you know, established tasks is that it
447800	452280	sort of had all these qualitatively new behaviors that were felt very magical.
452280	455920	And even now, you know, there's prompts that, you know, you'll see on Twitter or whatnot
455920	458160	that are sort of really shocking.
458160	464280	I mean, did you have these sort of like early moments when like you had the early model
464280	468160	results were like, holy crap, this is like, this is magic.
468160	469160	Yeah.
469160	472840	Well, I think that the earliest one that I remember was around code.
472840	477680	I mean, just, you know, at the time, totally mind blowing that you could just write a function
477680	481400	name and a doc string kind of describing what the function should do and actually write
481400	482400	it.
482400	483720	Not super complicated functions, right?
483720	487400	But just that it was able to, you know, you ask for, you know, something to take a couple
487400	489600	lines and they would be able to really do it.
489600	492160	You modify things a little bit to make sure it hadn't just memorized it.
492160	495000	Make sure enough that it would write out the modified code.
495000	498480	And I think, you know, the overall thing that's really interesting about the paradigm of a
498480	504280	GPT-3 is that where it really comes from is that I, you know, we kind of had this picture
504280	509160	that, look, the problem with these models is that they're great within their data distribution.
509160	512680	But as soon as you're outside of that distribution, like all bets are off.
512680	516600	And so what if you just make the whole world, the whole universe, be the data distribution?
516600	518680	You put the whole internet in there.
518680	525400	And I think that what we've really seen is that these models, that they really are able
525400	528840	to generalize extremely well within the kinds of things that they've seen.
528840	531120	You know, again, different question if it's never seen anything like it.
531120	534640	I mean, humans are also not very good at things you've never seen before.
534640	539840	But I think that that picture of just, like, all of the different things that it's seen
539840	542160	in all these different configurations is almost unimaginable.
542160	546600	There's no human who's been able to consume, you know, 40 terabytes worth of text.
546600	551520	And so I think that we just keep seeing surprises where you just ask for, one of my favorite
551520	556120	ones actually was this teacher-student interaction where I was the teacher model as the student
556120	558320	and I managed to teach it how to sort numbers.
558320	561240	And you just kind of have these experiences where, like, that's what it should be like
561240	562720	to interact with an AI.
562720	563720	Yeah.
563720	565600	I mean, it's incredibly shocking.
565600	571080	You know, one of the things I'm curious to get your thoughts on is, I think, in the path
571080	577200	of developing GP3, you know, required, I think probably the jump from GP2 to GP3 required
577200	582000	a lot of conviction because, you know, you all were spending probably a fair amount on
582000	584960	a compute at the time to be able to train these models and there were probably a lot
584960	589360	of experiments that didn't work and so you had to be willing to keep going after it.
589360	595000	Did that phase of the journey, sort of this, like, GP2 to GP3 jump, was it scary?
595000	596200	Did you have doubts?
596200	599840	Or were you very confident that, hey, you know, we're going to scale this up and even
599840	604040	though we're going to not get it right the first few times, it's going to be amazing?
604040	605040	Yeah.
605040	610360	And to your point that scale was not an obvious thing, not the company, but the scaling things
610360	616000	up, at the time, the funny thing is actually our very first scale result that just sort
616000	619320	of convinced us that this is the right way to approach things.
619320	620640	You push it until it breaks.
620640	624240	Not necessarily that more compute is just magically always going to solve your problem.
624240	625240	It was Dota.
625240	628080	That was, you know, playing competitive video games and there we kind of went through this
628080	632120	whole, that was a three-year arc where we started out with something that didn't do
632120	636000	anything, finally beat, like, you know, the in-house team, then we managed to go beat
636000	641080	the pros and at each step it was just kind of pushing in all dimensions, right, is make
641080	646240	the model bigger, it's to, you know, sort of, again, fix all the bugs and you just kind
646240	650920	of keep iterating on every single dimension and every single dimension yields returns.
650920	655560	And so I think that we did very much the same thing where for GP2, you know, it's not as
655600	660240	simple as saying, okay, like, clearly you just need to, like, you know, crank up this
660240	662440	one variable and you just do it in one shot.
662440	667080	It's this, like, sort of iterative, like, stepping through the space on each axis at
667080	668520	every single time.
668520	671720	And so I think that on the one hand it does require conviction because you do need to
671720	675360	say we're going to, like, carve out a big compute budget so that you're not constantly
675360	679880	not kind of fighting other people for the big supercomputers, but on the other hand,
679880	683560	I think it's also very iterative and you don't have to make scary irreversible decisions
683600	686400	because in each step you get feedback from reality.
686400	691800	And I think that that key of, like, both the big picture, thinking of what if this works
691800	696840	and make sure that you're really set up for success, but also don't blindly spend a year
696840	700320	of your organization on just, like, pursuing a thing that might not pan out.
700320	703000	I think that balancing those two is what was really key.
703000	708240	Yeah, I mean, one of the cool things is you sort of walk through this and talk through
708240	713520	the insights is that the sort of organizational learnings were really critical in this entire
713880	719320	sort of path-dependent sort of path to GP3.
719320	725080	It sort of, you know, it makes sense when you say it that sort of insights from Dota 2
725080	729040	and insights from the sentiment neuron were sort of like the key, these were like the
729040	733040	key nuggets that led to the sort of, like, crystallized idea of, you know, scaling up
733040	737520	and building GP3, but it's very unintuitive from the outside and sort of, I think it's
737520	742680	almost a statement of innovation in some sense is that, you know, you're going to piece together
742720	746720	this sort of, like, disparate collection of insights that you gather from a wide variety
746720	751280	of experiments and eventually you sort of, like, get the ingredients together and you
751280	752280	build something.
752280	754880	Yeah, that's the first principle is thinking in action.
754880	755880	Yeah.
755880	760000	You know, I think that the story of AI, I don't know if you think about this at all, but I
760000	764320	think about this a little bit, I think the story of AI to date and especially the past
764320	768520	few years and the story of open AI is probably going to be something that historians are
768520	772840	going to study for, you know, decades and decades to come.
772840	777840	Are there any fun stories from the journey of creating some of these foundation models
777840	780920	that you think deserve to be in the history books?
780920	786600	Well, I'll tell you my actual favorite story from the Dota days.
786600	791400	So, you know, we've been working on this system and, you know, actually the funny thing is
791400	794360	at the very beginning we wrote down our list of milestones.
794360	800200	On this date we're going to be Jonas, our best open AI employee who also had many thousands
800200	802720	of hours of Dota 2 gameplay.
802720	805240	This date we're going to beat the semi-pros, you know, this date we're going to beat the
805240	809720	pros and so it's supposed to be like June 6th or something, June 6th rolls around, we
809720	810720	don't have anything.
810720	814480	Like, you know, he just crushes us and two weeks go by, three weeks go by, we keep pushing
814480	819080	back that deadline by a week every week and then one day we actually do beat him.
819080	824040	And you know, I think my conclusion was that like it wasn't actually actionable to sort
824040	828720	of set those goals of outputs, you can only control your inputs, you can control the experiments
828720	829720	you run.
829720	833680	And so we just managed the project very differently after that and the thing that was so crazy
833680	838160	to me still is that, you know, so a week before the international, which is the like world
838160	841000	championships we're going to show up, we're going to play 1v1 against the best players
841000	845120	in the world, we finally started beating our semi-protester and we're like, okay, maybe
845120	847240	this is actually going to happen.
847240	850360	But then we learned that he actually was on like vacation, he didn't have his like real
850360	854360	setup and so we're like, oh no, like this is not going to go well.
854360	857960	So we show up, you know, we continue to train, we like kind of do like a Hail Mary of like
857960	863240	scaling things up, biggest scale we've ever done and we show up at the international and
863240	868640	we play against, you know, like sort of low lowest, you know, low ranked pro, like a previous
868640	873600	pro and we go 3-0-3-0-2-1.
873600	877960	So we basically win-win and then we did have one loss and we take a look at it and it's
877960	881360	like this item that we've never trained with, we've never seen before, oh wow, okay, we
881360	884200	need to add that and, you know, do it fast.
884200	887480	And so the team stays up all night putting this thing into the training, getting the
887480	891520	whole thing launched and, you know, again, like we did double the scale where we're basically
891520	897880	maxing out our CPU cores at this point and start training and, you know, we're supposed
897880	903640	to play against the top pros in the world, fortunately they can't do the next day so
903640	909320	we get an additional day of training and the number two person comes in, he plays against
909320	917040	us and we win-win-win-win-win and he's like, okay, but I beat this but the top player is
917040	921200	never going to lose to this thing or sorry, yeah, the top player is going to crash this
921280	927800	thing and fortunately because he had spent so long playing that that guy couldn't come
927800	931720	that day so we got one more day of training and that one more day of training was enough
931720	935440	and so I think it's just the story of like you can really see the improvement and at
935440	940360	each step we could see new behaviors that the system have learned and I think that that
940360	943640	experience of just sort of watching a girl up in front of you is just something that
943640	945140	was really amazing.
945140	950740	I'm actually surprised because you would, I sensibly you'd probably train the sort
950740	954660	of like agents for a long, long, long time going into the international and surprised
954660	955900	at each incremental day.
955900	959300	Yeah, so this is something I think has changed over time so at the time we basically had
959300	962700	two weeks worth of training was like the whole model run and so you'd start from scratch
962700	967860	each time and the thing that was really funny in the middle was that, you know, we put in
967860	971420	this new item, we were training it and when we took out of training it was the best spot
971420	976660	we ever saw except that our semi-protester was looking at it and was like this bot is
976660	979700	doing something really dumb, it's just sitting there in the first wave and taking all this
979700	983060	damage it doesn't have to, I'm going to go beat it, he ran it and go fight it and he
983060	984060	lost.
984060	987660	It's like that was weird and he did it like five more times and he lost each time but
987660	990660	then he figured out a strategy that actually does work which is you, you realize what was
990660	994460	going on was it was baiting him, it had learned to deceive, you know it actually learned that
994460	997580	what you do is that like you pretend oh I'm just a weak little bot, I don't know what
997580	1002260	I'm doing and then you know a person comes in and you're just like smack.
1002260	1006660	And so the way you defeat that is that you actually, you don't fall for the bait, right,
1006660	1010380	you let the bot take all this damage and sit there and get weaker and then you finally
1010380	1015580	go in for the kill and so there we actually stitched together our good bot for the first
1015580	1019660	wave with the deceived bot thereafter and so there was a lot of this sort of like really
1019660	1024460	examining what was going on in the systems because it's such a limited domain, you know
1024460	1027860	it's a complicated domain but it's very, very interpretable.
1027860	1031180	It meant that we could observe behaviors like this and figure out how to engineer around
1031180	1036380	them but once we graduated from the 1v1 version of the game to the full 5v5 you know much
1036380	1041660	more like you know like competitive basketball or something rather than heads up, suddenly
1041660	1046260	all of our analysis of the behavior stopped working, right, that we used to have someone
1046260	1049500	who just literally would watch the bot play and be like oh we have this bug in the training
1049500	1053260	we got to go fix that, for 5v5 we just could not do that and I think that's kind of where
1053260	1057820	we've graduated as a field is that too when you look at GPT-3 and the mistakes it makes
1057820	1062060	sometimes people ask well why didn't it make that mistake and sometimes you can interpret
1062060	1065940	it but sometimes it's also a little bit like asking well you know why did you make a mistake
1065940	1069540	on some tests it's like well you think you know but like your explanation isn't always
1069540	1073580	very good and I think that to do complicated behaviors sometimes there's a very complicated
1073580	1074580	explanation.
1074580	1075580	Yeah.
1075580	1080260	Have you read this short story I think it's like the Lifecycle of Software Objects by
1080260	1081260	Ted Chang?
1081260	1083260	I think I have but I don't recall the details.
1083260	1086420	It's like it's about how they're these AI pets and they sort of like keep learning new
1086420	1090620	and new behaviors it's very reminiscent of describing these Dota agents.
1090620	1095180	Yeah yeah I think we'll see that kind of thing in our future somewhere.
1095180	1099260	I want to kind of go back you know one of the things we've known each other for many
1099260	1106380	years long before you know these foundation models and even before this competition Dota
1106380	1113940	2 and one thing I vividly remember is how sort of optimistic and confident you were
1113940	1119300	in sort of this sort of path of increasing and increasing AI capability you know sort
1119300	1124500	of I remember the time as maybe 2016, 2017 it felt very striking because it was sort
1124500	1131460	of like you know with these algorithms they're still pretty weak and you were always very
1131460	1134940	confident like oh yeah they're just going to keep getting better and better and better
1134940	1137140	and you know you're very a lot of confidence in that.
1137140	1143860	What were the things that back then gave you sort of the resolve or confidence in the
1143860	1146060	and the optimism in the technology?
1146060	1153540	Yeah I mean at some level you know to have that kind of belief and conviction and something
1153540	1155860	that hasn't happened yet it's a very intuitive thing.
1155860	1161100	I mean I remember when I was in school and showed up excited about doing NLP research
1161100	1164620	I went and tracked down an NLP professor and I was like please can I do some research for
1164620	1169380	you and he's like okay he shows me these like parsed trees and stuff and I look at that
1169380	1173940	and I was like this is never going to work right and you know to explain like why does
1173940	1176660	it feel like it's not going to work it just doesn't have the right properties right it
1176660	1181460	just felt like you're going to pour all this human like engineering and intuition and effort
1181460	1185300	into the system and I know I can't even describe how language works.
1185300	1186300	Yep.
1186300	1189620	Right it just feels like there's just something inherently missing but I think neural nets
1189620	1191420	have the opposite property.
1191420	1195580	Neural nets is very clear this is a system that absorbs data it absorbs compute it's
1195580	1200060	like a sponge that just like slurps everything up and so it has the right form factor but
1200060	1203380	the thing that's always been missing as well can you train it right?
1203380	1207300	Do you have enough data do you have enough compute do you have enough ability to like
1207300	1210420	have a learning algorithm that can shovel the stuff in efficiently in a way that it comes
1210420	1214380	out in some way that generalizes like that's the thing that's been missing and I think
1214380	1219500	what became kind of clear you know the field really I think got its most recent resurgence
1219580	1227660	in 2012 with the Alex in that paper and I think that there that was the first time where
1227660	1231820	you had a neural net that really just crushed a task right that it was like people had spent
1231820	1236940	decades on computer vision and suddenly it's like well I'm so sorry but this approach has
1236940	1241380	just supplanted you by this this massive gap and I think that you just started to see it
1241380	1246140	spread right that it was almost like you had these these all these disparate departments
1246140	1251460	and there was this wall that was being knocked down day after day and I think that when you
1251460	1255620	see a trend like that were things that have been long-standing and very deeply established
1255620	1259820	in these ways of thinking these great debates that have gone on for a long time and suddenly
1259820	1265380	you're seeing a repeated result that is consistent with the history I think that that for me
1265380	1268940	is maybe the most clear sign that it's like something is going to work and there's a real
1268940	1272060	sort of exponential that is waiting to unfold.
1272060	1277940	And then you know were there what were the what were the moments if any of of doubt and
1277940	1282620	you know let's let's chart the path I think open I start in 2016 yeah yeah I'd say December
1282620	1288620	2015 you know 2016 okay great December 2015 till now were there any moments of of doubt
1288620	1293820	in the technology or was it sort of always hey this is you know this is clearly the way
1293820	1299500	of the future yeah I mean I think that doubt is a strong word there's definitely moments
1299500	1304100	like I think to build something you you're always doubting right that you're always like
1304100	1307660	you've got to be questioning every single bit of your implementation like anytime you
1307660	1311140	see like a graph that's wiggling in a weird way you've got to go figure it out you can't
1311140	1315940	just be like I'm sure that the AI's will sort it out.
1315940	1320460	And so I think there was like lots of sort of tactical doubt lots of like sort of worries
1320460	1324500	that were not quite doing it right lots of like redoing the calculations to figure out
1324500	1328820	like hey how big of a model do you think you're going to need lots of mistakes for sure like
1328820	1333700	a good example of this is the scaling laws so we did this study to actually start to
1333700	1338700	really scientifically understand how do models improve as you push on various axes so as
1338700	1342940	you pour more computing as you pour more data in and one conclusion that we had at one point
1342940	1348620	was that basically that there's you know sort of a limited amount of data that you want
1348620	1352500	to pour into these models and that there's kind of this very this very clear curve and
1352500	1356700	that one thing that we realized only years later was actually that we'd read the curves
1356700	1361120	a little bit wrong and you actually want to be trained for way more tokens way more data
1361120	1365220	than anyone had expected and that did I you know that there's definitely these moments
1365220	1368780	where these things that just didn't quite click where it's like just didn't add up that
1368780	1372740	we were training for so little and that you know something conclusions that you drew downstream
1372740	1377060	but then you realize there was a foundational assumption that was wrong and suddenly things
1377060	1380780	make way more sense so I think it's a little bit like you know physics in some sense for
1380780	1384620	like do you do doubt physics it's like I kind of do I think all physics is wrong right
1384620	1388740	but like only so wrong right it's like we clearly haven't reconciled like quantum and
1388740	1392620	relativity is that there's like something wrong there but that that wrongness is actually
1392620	1396900	an opportunity it's actually a sign of you have this things are useful right really like
1396900	1400060	it's affected our lives and it's actually like pretty great I'm very happy with what
1400060	1404580	physics has done but also there's fruit and so I think that that for me that's always been
1404580	1409620	the feeling that there's something here and that you know if we do keep pushing and somehow
1409620	1412860	the scaling laws all peter out right and they suddenly drop off a cliff and we can't make
1412940	1417140	any further progress like that would be the most exciting time in this field because we
1417140	1421180	would finally reach the limit of technology we would finally learn something and then
1421180	1424860	we would finally have a picture of what the next thing to do is yeah that's super it actually
1424860	1431100	reminds me of this one of the stripe operating principles which is I think micro pessimists
1431100	1436380	macro optimists yep yep yes and it's very I mean it's very resonant but obviously like
1436380	1441980	very related to what you're talking about which is these you know you have to be extremely
1441980	1446220	pessimistic we're extremely questioning in the moments of the technology but then obviously
1446220	1451140	on a long enough time horizon incredible stuff pops out yep you got to be excited like I
1451140	1455820	think that this is just an exciting field and it's a scary field as well you got to have
1455820	1460980	some amount of just like awe at the fact that you have these these models that they started
1460980	1465460	as just random numbers right and then you have build these massive supercomputers these massive
1465460	1470500	data sets and you do a ton of the engineering work you do a ton of these algorithmic developments
1470580	1474340	you put them all into a package right and we don't really have other technologies that
1474340	1479220	work like this like I think the fact to me the most fundamental picture this like sponge
1479220	1483260	that you just kind of pour stuff into and you get this model it's reusable and works
1483260	1487420	across all these different areas like you can't do that for traditional software right
1487420	1491380	traditional software is it's just you know human effort writing down all the rules and
1491380	1494780	that's where the return comes from but you can't you know maybe you have like a spark
1494780	1499940	cluster that does some stuff but that's not that's not the cake and in in neural networks
1499940	1504900	it really is yeah you know I want to kind of switch gears to thinking about the the
1504900	1510460	sort of future and and and looking forward at what kind of what's next what do you think
1510460	1513800	I mean I'll ask this sort of as broadly as possible to start with what do you think the
1513800	1522300	future of AI holds yeah I think that the future of AI is going to again be both exciting and
1522300	1525620	a source of a lot of change and I think that that is something that you know part of our
1525640	1531180	mission is to try to help facilitate that as positive way as possible I think that kind
1531180	1536700	of you know the super high level I kind of feel like AI was like you know something that
1536700	1540620	for the you know 2020 10s was like kind of cool you know it's the game of like publishing
1540620	1544340	papers and you play some video games and like you know it's just like it's just like fun
1544340	1551820	good science I think it's really interesting that 2020 kicked off with GPT 3 which is really
1551900	1556620	the first model that was commercially useful just as the model like literally put an API
1556620	1561780	on top of it people just talk to it and people build products on top of it and you know that
1561780	1566260	you know one of our early customers just you know just raised it at 1.5 billion valuation
1566260	1570540	which to me is is a really wonderful thing to realize that you build this model and it
1570540	1575900	creates so much value for so many different people and I think that we're still in such
1575900	1580300	early days for what these models can do and so I think that what I'm most excited about
1580340	1586620	from just seeing GPT 3 seeing Dolly is thinking about the the sort of economic value that
1586620	1590620	it can create for people and I think that there's a lot of other pieces to it in terms
1590620	1595260	of like you know that everyone's going to be more creative if you want to like I can't
1595260	1598780	draw but now I can create images now I could take a picture of this in my head and I can
1598780	1602940	actually see it on on a page and one of my favorite applications of Dolly is actually
1602940	1608020	people who are 3D physical artists you know something's like a sculptor and now they
1608060	1611500	can actually get a great rendering of the thing that they have in mind by kind of just
1611500	1615460	like iterating with this machine and they go build it right I think that this sort of
1615460	1620580	amplification of what humans can do is what these systems are for and so I think that
1620580	1623740	for this decade I think what we're really going to see is these tools just sort of
1623740	1627100	proliferating they're going to be everywhere they're going to be baked into every company
1627100	1631620	I think it's kind of like the internet transition that you know that it was kind of like if
1631620	1635500	you're a company like what's your internet strategy and you know 1990 it's like what
1635540	1639540	even is this thing you know and in 2000 it's like huh maybe it's interesting and there
1639540	1644340	there's a little you know boom and bust and here we are today even talk about an internet
1644340	1648060	strategy is like it's just so integral to every business it's not even like it's not
1648060	1651380	even a separate thing right it's just like it's just part of like your it's like your
1651380	1654380	payroll strategy right it's like it's not like a separate part of your business that
1654380	1657740	you can pick or choose whether you're going to have it and I think that AI is going to
1657740	1661820	be much the same I think there will be a transition point right I think that it's it's
1661820	1665060	interesting like our mission is really about building artificial general intelligence right
1665100	1669620	really trying to build machines that are able to perform whole tasks right that are you know
1669620	1674660	push this technology to its limit and build machines that are able to you know our charter
1674660	1678540	definition is outperform humans at most economically valuable work and there's a question of the
1678540	1682980	timeline but I think that that picture of you know you have these tools that are creative
1682980	1687700	that help everyone amplify and what happens when they do become so capable that they're
1687700	1692380	able to perform these tasks even autonomously and I think that actually the implications
1692380	1696460	of that are different from what people expect I think that it's much more like you know that
1696460	1700860	I think there's still going to be this amplification but I think that there the change is going
1700860	1706300	to be just very hard to predict and unexpected and I think that really thinking about how
1706300	1710820	all of that sort of value gets distributed how to make sure that it's sort of pointed
1710820	1715020	at solving these like hard challenges that humans you know maybe are unable to solve
1715020	1718780	ourselves you know the climate change and you know universal education and things like
1718860	1724220	that and really transitioning to this like AI powered world I think is going to be just
1724220	1728140	like a real sort of challenge for the whole you know all of humanity to work together
1728140	1733380	on. Yeah I mean I totally agree one thing that I think is almost funny with how the timing
1733380	1736820	of all these technologies have worked out is that you know last year everyone was talking
1736820	1744100	about Web 3 as crypto and now it feels very obvious that AI is the actual Web 3 you know.
1744180	1748540	We'll take Web 4. Yeah Web 4 we'll skip we'll skip over one but sort of like Web 1 was just
1748540	1754140	reading Web 2 was reading and writing and now Web 3 or 4 depending on what we want to
1754140	1760540	say is ads, computer reading, computer write and it's sort of this incredible new phase.
1760540	1764420	You know one so I think I think you mentioned two two directions here that I think are really
1764420	1771780	interesting one is the sort of advancement and sort of proliferation of GP3 and Dolly
1771860	1775660	and sort of the existing tools becoming more and more economically useful and there's
1775660	1780460	sort of this continued improvement of the algorithms themselves towards sort of towards
1780460	1784860	sort of AGI. What do you think and obviously don't reveal any opening secrets but what
1784860	1789140	do you think this sort of like roadmap to AGI looks like from where we are now? I mean
1789140	1794700	I think that humanity to a large extent has been on the AGI roadmap for a very very long
1794700	1800900	time. I think even looking at just the history of neural networks in particular you know
1800940	1804820	on the one hand we say hey 2012 like that was the moment like everything changed you
1804820	1808500	know that like you look at these we have all these curves of how much compute people put
1808500	1812060	into the landmark results it was going like 10x year over year still continuing by the
1812060	1819220	way that's that's a decade of 10x year over year that's insane and the thing is we actually
1819220	1825780	did a study to then look back at previous results all the way back to you know say the
1825820	1832340	perceptron in 1959 and you actually find that there's basically a very smooth curve back
1832340	1836220	there as well. The amount of compute going into all the landmark results was exactly
1836220	1841020	Moore's law and it kind of makes sense right it's like that people were not willing to
1841020	1845580	spend more money they wanted to spend a constant amount of money on these experiments because
1845580	1850060	you're starving grad students like you know you can only get so much computer time and
1850100	1856380	that the results got better and better the more compute was available to them and I think
1856380	1859740	that that is so interesting that yeah basically what changed in 2012 was that we said okay
1859740	1862660	we're just gonna like you know we are gonna spend more money we're gonna build massive
1862660	1868500	supercomputers now because the ROI is there but that fundamentally the curve if you control
1868500	1872100	for that that cost factor it looks exactly the same and so I think that basically this
1872100	1876500	picture of building more capable models by pouring more compute into them by getting
1876580	1881260	better at harnessing this technology of neural networks back propagation I think that has
1881260	1884940	been very invariant and the details you know maybe change a little bit you know do you want to
1884940	1890140	work on GPT-3 do you want to work on whisper like do you pour in your your you know speech
1890140	1894700	data do you pour in text data from the internet and to me those details I think you know they
1894700	1898460	matter in the like in the like sense of like what are you gonna work on today and you know
1898460	1902020	what are you gonna download but if you zoom out you look at the scale of like these this
1902060	1907300	technology I think it actually sort of doesn't matter so much I think kind of what we're
1907300	1909940	building it's almost like building computers like you think about the Haiti and Moore's
1909940	1912900	law right where it's just like there's a new chip that comes out and there's a new chip that
1912900	1916380	comes out and it's kind of like what's the you know what's the path to building the best
1916380	1919860	computer the answer is well you just keep building the next best chip and you keep building
1919860	1923620	the next best chip and you keep getting better peripherals and all these you know keep working
1923620	1929220	every single piece of the technology and so I think this full stack of better GPUs great
1929260	1933060	software for utilizing them neural networks that we learned to harness more and more the
1933060	1937340	scaling laws doing all the science alignment extremely important making sure these models
1937340	1942940	not just are smart but actually are aligned with what humans intend all of that I think is the
1942940	1947300	stack and so I think that you know what our goal is is just to keep doing something that was
1947300	1951540	previously impossible every single year so you know that I guess well you should check back
1951540	1957420	in a year but hopefully 2023 we'll all forget about Dolly 2 and GPT 3 and we'll be talking
1957420	1962820	about something new and I think as long as we continue that like you cannot continue that
1962820	1969020	path without ending up somewhere amazing yeah I mean I think I actually remember this in I
1969020	1975980	think probably 2017 you were sort of very still quite you were very excited about sort of the
1975980	1982980	sort of Moore's law continuing and that that sort of creating a lot more opportunity for you
1982980	1988420	know neural networks and AI and that's that's sort of played out are you worried about the sort
1988420	1996380	of proverbial end of Moore's law kind of causing a stall out in in progress so I'm not worried
1996380	1999820	about it per se like I think the way to think about this right because I think we you know we
1999820	2003660	often get caught in this debate of like is it all about scale or is it all about algorithms is
2003660	2008540	all about data and the answer is that's a wrong question right it's really like you multiply
2008540	2011660	together these factors and the best thing to do when you're multiplying together multiple terms
2011740	2016060	is that you actually kind of want them all to be equal and I think that the answer is like it's
2016060	2021980	been great for the past you know seven years that we've been able to just pour more dollars to
2021980	2025740	build bigger computers that's one way to get ahead of Moore's law at some point they're just
2025740	2029900	aren't more dollars right there aren't more grains of sand to you know that have been turned into
2029900	2036860	into these these wonderful computers that we use so there is a limit there that we have not yet
2036940	2042620	hit but when you do that does not stall all progress right you still have algorithmic progress
2042620	2047100	and there we've again done studies and we've shown that actually if you take like an example is if
2047100	2052220	you look at the amount of compute it takes to hit the same performance so to train you know a state
2052220	2059740	of the art that you know 2012 or 2014 vision model that that computes also falling exponentially
2059740	2064460	we're basically making exponential progress in algorithms not at the same rate as we are able
2064460	2069180	to you know sort of build bigger computers but that is an amazing force too you know it's like
2069180	2073020	I've got this exponential I've got that exponential like let's not even talk about the data exponential
2073020	2077900	so I think that that the truth is that we will find a way I think that the history of this field
2077900	2083020	is just so consistent and I think that that you know humanity is just so innovative that I think
2083020	2089660	that that we're not going to hit a wall for the foreseeable future and do you think that you know
2089660	2096380	one of the one of the interesting juxtapositions of of today just from a scientific perspective is
2096380	2101740	a relative slowing in nearly every other science and there's you know there's a lot of research
2101740	2105740	that sort of demonstrated that science on the whole slowing and then comparatively the sort of
2105740	2109980	acceleration of artificial intelligence and sort of this this you know in many ways this
2109980	2116220	renaissance that we're entering right now do you do you fear that at some point AI will similarly
2116220	2120860	sort of reach these points of admission mark returns and slow relative like in much in the
2120860	2125580	same way that other sciences have or do you think that's so far away that you know well I think
2125580	2129820	two things I mean I think that there's there's always s-curves although I think that something
2129820	2133980	is also interesting about s-curves is that there tends to be paradigm shifts like have you ever
2133980	2140060	read Singularity is Near no I haven't yeah so this is the Ray Kurzweil book from like 2004
2140060	2145420	something and I always thought just based on the reputation it's going to be kind of a crazy book
2145420	2150140	but if you actually read it it's the most dry boring reading you'll ever do and it's basically
2150140	2156540	just curve after curve of different industries within computing showing how the performance has
2156540	2162060	changed over time and it's you know basically the conclusion he comes to is that there's this
2162060	2167020	repeated pattern that seems to happen across you know memory across number of transistors on the
2167020	2172220	chip you know etc etc where you kind of have an s-curve of the current paradigm and then you have
2172300	2179580	paradigm shift and that example he talks about is you know thinking about let's talk about CDs
2179580	2183500	right so you talk about great you know CD adoption it's like you know it's great s-curve it's
2183500	2187340	suddenly everywhere it's like everyone's got a CD player like it's just the technology of the day
2187900	2191500	and people get really excited about doing more of the same thing it's like Blu-ray that's the thing
2191500	2196460	you know and so then everyone starts investing in Blu-rays and somehow it just doesn't take off
2196460	2199740	and it's because it's just more of the same and it's like you know it's not backwards compatible
2199740	2203580	and so it's just not really worth it but the real paradigm shift was streaming right suddenly
2203580	2207420	you have this new adoption curve this new s-curve that just is this like totally different way
2207420	2210540	and the way we got to fast computers was basically five different paradigm shifts
2210540	2214380	across a hundred years and so I think that that's maybe a story here too which is like
2214380	2218140	there's gonna be an s-curve in what we're doing right now and that there will be a paradigm shift
2218140	2222140	when you hit it and I think that that again speaks to the ingenuity of humans but I think there's
2222140	2229260	also a second thing where my other answer is to some extent it doesn't matter because the thing
2229260	2234220	about this field is that it's useful now right that kind of the goal that I think we've always
2234220	2239100	had for AI was to actually make us so computers are just way more helpful like you think about what
2239100	2242780	computers have done for humanity right like how many problems they've helped us solve they've
2242780	2246540	created new problems as well but I think that on net that they've helped us solve way more problems
2246540	2250540	than they've created and I think they've kind of just changed the nature of how we interact with
2250540	2254780	each other about how you know like it's just like hard to get lost anymore right you just
2254780	2259020	pull out google maps I think there's really amazing problems that are now within our reach
2259020	2263740	that just would not have been otherwise and I think that AI like we're starting to crack that
2263740	2267900	nut we're starting to be able to you know like it's I think it's kind of interesting you could
2267900	2274220	get a co-pilot you know which which we we power we have the models that power it and that the way
2274220	2278300	that that is useful to people is that provides very low latency suggestions right it's basically
2278300	2283020	an autocomplete for code and that you know there's a very strict latency budget you know if you're
2283020	2287740	more than you know 1500 milliseconds to get a autocomplete suggestion it's worthless like no
2287740	2293660	one wants that you've already moved on but I think that what we really want to build the next
2293660	2299100	phase is machines that help you produce that are able to produce artifacts that are materially
2299100	2303100	interesting on their own so not just interesting because it's like a fast suggestion to you but
2303100	2307260	because it's actually a quality answer and you're starting to see if you talk to our current gbt
2307260	2311100	iteration you can ask it to write some poems and it writes way better poetry than I can
2311820	2319820	it actually wrote a poem for my wife that made us both cry like you know yeah I I cannot do that
2319820	2325660	myself but now I can you know by partnering with this with this machine and I think that
2325660	2330140	that's the real story right is really trying to get these tools out and everywhere and yeah you
2330140	2335020	know if what we're doing right now stalls out I don't think that removes the value from what we're
2335020	2341020	able to create yeah by the way it's depressing that the the attention span of most engineers is
2341020	2349340	only 1500 seconds but you know it is what it is I what uh what if anything you know I think
2350060	2355260	one of the things that if I recall spurred you to to work on opening I was was sort of
2356140	2360780	also being concerned about the sort of potential negative consequences of of the technology
2360780	2365900	um what at this point looking forward what are your sort of biggest concerns or what are you
2365900	2370300	afraid of with with artificial intelligence that you sort of urge everyone in the field to sort of
2370300	2376540	help avoid yeah so I think that one thing that's very interesting about AI is that you know if you
2376540	2381180	talked to certainly you know 10 years ago if you looked at every article about it you talked to
2381180	2385100	someone on the street terminator is the main thing that comes up right so I think there's always been
2385180	2391820	this feeling around AI that is sort of you know that there's an element of fear mixed with the
2391820	2394940	you know sometimes people don't see any potential or sometimes you know they realize that there is
2394940	2400380	the potential but like really trying to figure out and navigate it and I think that that picture
2400380	2404060	the specifics you know I think that that we're starting to see a little bit more but I think
2404060	2407900	the high level picture of this is technology that's very powerful and it can be powerful in
2407900	2412380	positive ways and negative ways um I think is extremely correct and I think it's very important
2412380	2416220	not just to be you know starry eyed optimist everything's just going to work itself out
2416220	2421900	but also not to be you know sort of doomsday like everything is terrible and you know humanity is
2421900	2425980	over because I don't think that's at all true I think this technology can be the the best thing
2425980	2429980	that is the way we've ever created and help us be the best versions of ourselves but I think
2429980	2435020	that it requires very careful navigating of the space and it's not something that just is for you
2435020	2438380	know companies of Silicon Valley to figure out I think it's it's really all of humanity kind of
2438380	2443660	challenge um so I think that we're going to go through different phases I think that that right
2443660	2448540	now I you know we're kind of starting to build systems where that I you know you think about
2449260	2455020	misuse is the most clear problem and the systems themselves are still not very powerful right
2455020	2459500	that the kinds of things you worry about for GPT-3 are you know important problems you think
2459500	2464140	about bias and representation you think about the system sort of you know sort of saying the wrong
2464140	2468300	thing you know but but its action is really in your mind right it's it's sort of words on a page
2468300	2472380	that then you know words on a page are very powerful but that they don't themselves have
2472380	2476460	direct action in the world but you think about something like codex our code writing system
2476460	2481500	which is a little bit more like a robot because it has it emits code and if you were to just execute
2481500	2486700	that code directly it can actually directly have actuators into the world and making sure that that's
2486700	2491500	aligned and doing the right kinds of things not having buggy code and not writing viruses and that
2491500	2495260	kind of thing like that's really important and so I think that that figuring out what values go
2495260	2499020	into this these machines and that they're operating according to those values that's going to be very
2499020	2505020	very critical figure out how to avoid misuse and sort of regulate that both at a sort of societal
2505020	2510700	level at a you know technical level all of that is very important and I and I do think that there
2510700	2515260	is also a point where the technology itself you have to think about that it's going to be extremely
2515260	2520540	powerful and you think about a system that's you know sort of talking to lots of humans and
2520620	2523740	is operating unchecked that's the kind of thing that you should worry about you know we already
2523740	2528460	worry about that think about the companies right that lots of people are using this you know social
2528460	2532860	media platform or you know any of the technologies that we use and how much influence those can have
2532860	2539740	in the world and those aren't systems that have you know sort of deep sort of behaviors that are
2539740	2544300	emergent from from from what they've learned and so I think that that figuring out the technical
2544300	2549500	controls to make sure that these systems remain in service of humanity and sort of to
2549580	2555340	actually empower and accelerate all of us that I think is is also a very critical thing so it's
2555340	2559340	kind of this like a ramping set of stakes and making sure that we're building systems that are
2559340	2564220	aligned with with our values and figuring out what that even means like what what is the values
2564220	2567900	of humanity that should be in the system and that I think is not going to be an easy problem
2568780	2574620	you know one question and this this may be the last question that I have for you is sort of one
2574620	2581900	of the thing one of the conclusions of if the technology such that scale continues to be the
2581900	2586300	the sort of one of the more important things you know scale whether it's data or better algorithms
2586300	2593660	or scale compute then it it the technology itself will tend towards sort of this game theoretical
2593660	2598780	proliferation mode where it's sort of like people are going to compete and you see some of this today
2598780	2602060	even with the large tech companies and you guys obviously people are going to compete to sort of
2602060	2606620	build the bigger supercomputers that have the better performance and you have the bigger supercomputer
2606620	2610300	you have sort of supremacy over the other supercomputers and sort of there's like this
2611180	2616380	you know laddering the stakes and sort of and proliferation is really the sort of the right
2616380	2621500	word do you think that that is a version of the future or do you think that there's sort of
2621500	2627100	some path in which this becomes a much more sort of like open and useful not this sort of like
2627180	2633500	tool for nation states or large companies to compete with one another I think that the future
2633500	2642300	that seems to be unfolding is kind of a you know replay of how say computing technology has played
2642300	2648060	out more broadly I think that that it is still going to be the case that you're going to have
2648060	2653180	these increasingly massive supercomputers that are in the hands of only a few that are able to just
2653180	2657980	create models that can just do crazy things that no one else can do but I don't think that that
2657980	2662860	removes the value from the massive set of things that people are going to do with these models
2662860	2668780	and so you know I think that balancing the like super powerful very dual use extremely you know
2668780	2673660	think of these like almost like you know these massive like you know sort of systems that are
2673660	2676620	you know we think about like a nuclear reactor and it's like you know it's like these like giant
2676620	2681100	like sort of you know systems that you should approach with great care um and you think about
2681100	2685180	like you know by contrast think about wind turbines and like there's lots of wind turbines
2685180	2688220	everywhere and actually that if you add up the amount of value from wind turbines versus nuclear
2688220	2691900	reactors like I think actually that the balance is probably in favor of wind turbines and so I
2691900	2696220	think that that's kind of the future we're going to is that the AI technology is going to be everywhere
2696220	2701420	and there's going to be lots of value that's delivered by having open source models that are
2701420	2705100	integrated to every business and that people are building all sorts of crazy applications on top of
2705100	2709100	and that's something that we really want to support and promote and you also have to have this
2709100	2713420	dual answer for what you do with the with the new extremely capable stuff that's just a mile ahead
2713420	2718140	of everything else and that's something you have to treat with kid gloves with with more care
2718140	2722540	and I think that that balance is tricky it's not easy um that's something that we as an organization
2722540	2725740	have been trying to straddle and I think that you know we've had real existential struggles
2725740	2729980	internally trying to figure out you know like our goal is to empower everyone it's really to uh
2729980	2735660	to to bring everyone along to this AI transition and the best way to do that I think that our
2735660	2739740	picture of it has changed as the technology has unfolded and I think that we're starting to get a
2739740	2744940	sense of of you know where this can go it's really exciting to see all the the energy of all these
2744940	2748460	builders coming in because I think that like you said people are starting to realize like AI is
2748460	2753580	really going to work and it's time to build yeah well um this was an incredible conversation thank
2753580	2758860	you so much Greg next time we speak I'll make you uh read the poem that all right there we go
2758860	2762060	cool thank you so much thank you so much
