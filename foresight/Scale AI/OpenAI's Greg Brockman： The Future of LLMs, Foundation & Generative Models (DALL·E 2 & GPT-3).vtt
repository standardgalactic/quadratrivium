WEBVTT

00:00.000 --> 00:14.720
We're joined next by Greg Brockman, President, Chairman and Founder of Open AI and Alexander

00:14.720 --> 00:18.360
Wang, CEO and Founder of Scale AI.

00:18.360 --> 00:23.520
Open AI is a research and deployment company whose mission is to ensure general-purpose

00:23.520 --> 00:27.280
artificial intelligence benefits all of humanity.

00:27.280 --> 00:34.520
For Open AI, Greg was the CTO of Stripe, which he helped build from 4 to 250 employees.

00:34.520 --> 00:40.520
Please join me in welcoming to the stage Greg Brockman and Alexander Wang.

00:40.520 --> 00:42.520
Hey Greg.

00:42.520 --> 00:44.520
Hey.

00:44.520 --> 00:47.000
Thanks for making it.

00:47.000 --> 00:48.000
Absolutely.

00:48.000 --> 00:55.840
I'd be happy to be here.

00:55.840 --> 01:00.880
I want to start actually, I don't know if you remember this, but we first met at this

01:00.880 --> 01:07.000
summer camp called Spark where you gave a presentation about, at the time you were the

01:07.000 --> 01:10.560
CTO of Stripe and you gave this presentation about sort of like everything you had accomplished

01:10.560 --> 01:13.880
and I was a member of that camp and it was extremely memorable.

01:13.880 --> 01:15.880
You had a lot of good sound bites.

01:15.880 --> 01:18.520
I'm glad that it landed.

01:18.520 --> 01:20.960
Kind of a full circle moment.

01:20.960 --> 01:25.400
Well I think to start out with, I mean you've been CTO and now you're President of Open

01:25.440 --> 01:32.840
AI, but CTO of two incredibly iconic companies, Stripe and Open AI, in some ways probably two

01:32.840 --> 01:36.960
of the most iconic startups of the past decade.

01:36.960 --> 01:43.320
I wanted to start out just by asking in what ways are the two organizations the same and

01:43.320 --> 01:46.280
being CTO the same and in what ways are they different?

01:46.280 --> 01:48.520
Thank you for the kind of words.

01:48.520 --> 01:53.360
I think that one thing that's very interesting to me about kind of having been part of both

01:53.360 --> 01:59.280
of these organizations is seeing how much groups of people are kind of the same regardless

01:59.280 --> 02:02.160
of what the problem in front of you is.

02:02.160 --> 02:07.320
So I think that a lot of how we approached Stripe was thinking from first principles.

02:07.320 --> 02:12.000
I remember when we were pre-launch and we had some buzz going because we had some early

02:12.000 --> 02:16.480
customers and one of my friends took me out to launch, he was like all right, I've been

02:16.480 --> 02:19.720
hearing about this Stripe thing, what's your secret sauce?

02:19.800 --> 02:24.480
I was like, I mean we just make payments really good and he's like no, no, no, come on, you

02:24.480 --> 02:27.040
can tell me, what's the secret sauce?

02:27.040 --> 02:30.800
And really that was the secret sauce, right, is that we had just rethought every single

02:30.800 --> 02:34.480
piece of what we were doing from the ground up from first principles, not sort of locked

02:34.480 --> 02:37.880
into the way that people had been doing it and we asked how should it be, like where's

02:37.880 --> 02:40.960
all the pain and does it need to be there?

02:40.960 --> 02:44.840
I think that in AI we did much the same, we really thought about okay, there's this field

02:44.840 --> 02:48.000
that we're entering and that we hire a lot of people who had been in the field, but a

02:48.000 --> 02:51.440
lot of us also hadn't been in the field and we came to it with beginner's eyes and I think

02:51.440 --> 02:55.360
that that approach of just not being beholden to all the ways people were doing it, but

02:55.360 --> 02:59.840
also becoming expert in the way that things have been done because if you just throw everything

02:59.840 --> 03:04.600
out, like you're also just going to be starting from scratch in a not helpful way.

03:04.600 --> 03:08.360
So I think that that maybe is the deepest commonality between them.

03:08.360 --> 03:13.640
But obviously very different organizations, for Stripe I think that we ran the traditional

03:13.640 --> 03:14.800
startup playbook.

03:14.800 --> 03:18.240
You basically come up with the innovation and you just build, build, build.

03:18.240 --> 03:26.440
You get in front of customers from day one, the story is that we gave the first API to

03:26.440 --> 03:30.080
a customer who charged a credit card and he was like, I would like my money now please

03:30.080 --> 03:34.840
and we were like, huh, I guess we should build that.

03:34.840 --> 03:39.800
Open AI, we had research to do, like where's the customer?

03:39.800 --> 03:45.080
And it really took us I guess five years, starting late 2015 and it's really not until

03:45.080 --> 03:48.240
2020 that we had our very first product.

03:48.240 --> 03:52.400
And so I think that that sort of figuring out like what you're even supposed to work

03:52.400 --> 03:57.520
on, like did you do a good job, should you feel good on a day-to-day basis?

03:57.520 --> 04:00.720
I think that all of that had to come from within rather than from without.

04:00.720 --> 04:01.720
Yeah.

04:01.720 --> 04:04.760
Well, actually I want to go back to this point that you mentioned around first principles

04:04.760 --> 04:05.760
thinking.

04:05.760 --> 04:11.560
It's very interesting because even I remember like maybe 2020 or 2021, you know, you would

04:11.560 --> 04:16.880
sort of post-GPD3, you would talk to other researchers in the field and even they would

04:16.880 --> 04:22.080
still, you know, there's still like some degree of skepticism over the sort of like core concept

04:22.080 --> 04:26.000
of scaling up these models and if there were still gains to be had, et cetera.

04:26.000 --> 04:31.600
And I think, you know, I don't know the story but it seems like the sort of research sort

04:31.600 --> 04:38.360
of intuition that led to GPD3, Dolly 2 that have really ushered in kind of a new era of

04:38.360 --> 04:44.760
AI were probably, you know, somewhat against the grain or somewhat unintuitive at the time.

04:44.760 --> 04:50.800
You know, one question I have for you is, you know, I think now looking back it's obviously

04:50.800 --> 04:57.840
very obvious to point out GPD3, Dolly 2 basically have fundamentally accelerated AI progress

04:57.840 --> 05:02.520
and its relevance to the world and its relevance to every industry and sort of have created

05:02.520 --> 05:07.040
the sort of most recent AI wave.

05:07.040 --> 05:11.680
How is that matched up against your expectations when you were building these technologies?

05:11.680 --> 05:12.680
You know?

05:12.680 --> 05:13.680
Yeah.

05:13.680 --> 05:17.720
Well, I think the thing that's most interesting to me is that those models you mentioned are

05:17.720 --> 05:21.640
kind of overnight successes that took many, many years to create.

05:21.640 --> 05:24.760
And so, you know, from the outside it looks like, wow, you just like produce this model

05:24.760 --> 05:29.520
and that model and really on the inside, the GPT arc, that's a five-year arc, right?

05:29.520 --> 05:33.640
It really started with sentiment neuron paper which is back in 2017.

05:33.640 --> 05:34.640
Do you remember that paper?

05:34.640 --> 05:35.640
I remember the paper.

05:35.640 --> 05:36.640
It was very cool.

05:36.640 --> 05:37.640
Yeah.

05:37.640 --> 05:38.640
But it felt very novel.

05:38.640 --> 05:39.640
It felt very novel.

05:39.640 --> 05:40.640
Yeah.

05:40.640 --> 05:41.640
Very few people remember it.

05:41.640 --> 05:45.400
It's, you know, it was this very early result where we basically had been training a LSTM

05:45.400 --> 05:48.040
at the time to predict the next character in text.

05:48.040 --> 05:49.800
So, we basically showed a bunch of Amazon reviews.

05:49.800 --> 05:52.880
We said, what's the next character and of course it's going to learn where the commas

05:52.880 --> 05:53.880
go.

05:53.880 --> 05:54.880
Where the periods go.

05:54.880 --> 05:57.800
But of course it's not going to understand anything.

05:57.800 --> 06:02.360
But we found a single neuron in that model that had learned a state-of-the-art sentiment

06:02.360 --> 06:03.360
analysis classifier.

06:03.360 --> 06:06.080
I can tell you this is a positive review or negative review.

06:06.080 --> 06:07.080
That's understanding.

06:07.080 --> 06:10.560
You know, I don't know what understanding means but it's semantics for sure.

06:10.560 --> 06:14.040
And that for us was like, okay, this is going to work.

06:14.040 --> 06:20.120
The transformer came out late 2017 and my co-founder Ilya immediately was like, that's

06:20.120 --> 06:21.120
the thing.

06:21.120 --> 06:22.120
That's what we've been waiting for.

06:22.120 --> 06:26.560
So, you take this sort of very early nascent result, put in a transformer and then that's

06:26.560 --> 06:27.560
GPT-1.

06:27.560 --> 06:30.040
GPT-2 is you just keep pushing it.

06:30.040 --> 06:33.840
And you know, I think that the algorithm we kind of run internally is that we do these

06:33.840 --> 06:38.240
little sort of get signs of life and you have to be very, very careful to distinguish signs

06:38.240 --> 06:41.840
of life from like kind of just pushing too hard on a specific data set that isn't really

06:41.840 --> 06:42.840
going to keep going.

06:42.840 --> 06:46.280
But if you kind of build those right intuitions then you know, okay, now is the time to put

06:46.280 --> 06:47.280
in more compute.

06:47.280 --> 06:48.280
Now is the time to put in more researchers.

06:48.280 --> 06:50.640
Now is the time to like really scale it up.

06:50.640 --> 06:56.040
And so GPT-2 obviously was exciting and that we were all like, well, we look at the curves.

06:56.040 --> 06:58.880
You know, the bigger we made this model, the more compute we put in, the more data we put

06:58.880 --> 07:03.800
in, the more we just sort of got all the engineering details right, those curves just got better.

07:03.800 --> 07:06.880
And so actually, you know, our goal was just to break the paradigm.

07:06.880 --> 07:10.600
It was just push it until the curve stopped looking good and we still haven't managed

07:10.600 --> 07:11.600
to accomplish it.

07:11.600 --> 07:12.600
Yeah.

07:12.600 --> 07:18.120
Well, I think one of the things, at least for me and probably for many people who initially

07:18.120 --> 07:22.440
played with GPT-3, the like shocking thing was not, I mean, it wasn't necessarily that

07:22.440 --> 07:27.800
even the model got better and better performance on, you know, established tasks is that it

07:27.800 --> 07:32.280
sort of had all these qualitatively new behaviors that were felt very magical.

07:32.280 --> 07:35.920
And even now, you know, there's prompts that, you know, you'll see on Twitter or whatnot

07:35.920 --> 07:38.160
that are sort of really shocking.

07:38.160 --> 07:44.280
I mean, did you have these sort of like early moments when like you had the early model

07:44.280 --> 07:48.160
results were like, holy crap, this is like, this is magic.

07:48.160 --> 07:49.160
Yeah.

07:49.160 --> 07:52.840
Well, I think that the earliest one that I remember was around code.

07:52.840 --> 07:57.680
I mean, just, you know, at the time, totally mind blowing that you could just write a function

07:57.680 --> 08:01.400
name and a doc string kind of describing what the function should do and actually write

08:01.400 --> 08:02.400
it.

08:02.400 --> 08:03.720
Not super complicated functions, right?

08:03.720 --> 08:07.400
But just that it was able to, you know, you ask for, you know, something to take a couple

08:07.400 --> 08:09.600
lines and they would be able to really do it.

08:09.600 --> 08:12.160
You modify things a little bit to make sure it hadn't just memorized it.

08:12.160 --> 08:15.000
Make sure enough that it would write out the modified code.

08:15.000 --> 08:18.480
And I think, you know, the overall thing that's really interesting about the paradigm of a

08:18.480 --> 08:24.280
GPT-3 is that where it really comes from is that I, you know, we kind of had this picture

08:24.280 --> 08:29.160
that, look, the problem with these models is that they're great within their data distribution.

08:29.160 --> 08:32.680
But as soon as you're outside of that distribution, like all bets are off.

08:32.680 --> 08:36.600
And so what if you just make the whole world, the whole universe, be the data distribution?

08:36.600 --> 08:38.680
You put the whole internet in there.

08:38.680 --> 08:45.400
And I think that what we've really seen is that these models, that they really are able

08:45.400 --> 08:48.840
to generalize extremely well within the kinds of things that they've seen.

08:48.840 --> 08:51.120
You know, again, different question if it's never seen anything like it.

08:51.120 --> 08:54.640
I mean, humans are also not very good at things you've never seen before.

08:54.640 --> 08:59.840
But I think that that picture of just, like, all of the different things that it's seen

08:59.840 --> 09:02.160
in all these different configurations is almost unimaginable.

09:02.160 --> 09:06.600
There's no human who's been able to consume, you know, 40 terabytes worth of text.

09:06.600 --> 09:11.520
And so I think that we just keep seeing surprises where you just ask for, one of my favorite

09:11.520 --> 09:16.120
ones actually was this teacher-student interaction where I was the teacher model as the student

09:16.120 --> 09:18.320
and I managed to teach it how to sort numbers.

09:18.320 --> 09:21.240
And you just kind of have these experiences where, like, that's what it should be like

09:21.240 --> 09:22.720
to interact with an AI.

09:22.720 --> 09:23.720
Yeah.

09:23.720 --> 09:25.600
I mean, it's incredibly shocking.

09:25.600 --> 09:31.080
You know, one of the things I'm curious to get your thoughts on is, I think, in the path

09:31.080 --> 09:37.200
of developing GP3, you know, required, I think probably the jump from GP2 to GP3 required

09:37.200 --> 09:42.000
a lot of conviction because, you know, you all were spending probably a fair amount on

09:42.000 --> 09:44.960
a compute at the time to be able to train these models and there were probably a lot

09:44.960 --> 09:49.360
of experiments that didn't work and so you had to be willing to keep going after it.

09:49.360 --> 09:55.000
Did that phase of the journey, sort of this, like, GP2 to GP3 jump, was it scary?

09:55.000 --> 09:56.200
Did you have doubts?

09:56.200 --> 09:59.840
Or were you very confident that, hey, you know, we're going to scale this up and even

09:59.840 --> 10:04.040
though we're going to not get it right the first few times, it's going to be amazing?

10:04.040 --> 10:05.040
Yeah.

10:05.040 --> 10:10.360
And to your point that scale was not an obvious thing, not the company, but the scaling things

10:10.360 --> 10:16.000
up, at the time, the funny thing is actually our very first scale result that just sort

10:16.000 --> 10:19.320
of convinced us that this is the right way to approach things.

10:19.320 --> 10:20.640
You push it until it breaks.

10:20.640 --> 10:24.240
Not necessarily that more compute is just magically always going to solve your problem.

10:24.240 --> 10:25.240
It was Dota.

10:25.240 --> 10:28.080
That was, you know, playing competitive video games and there we kind of went through this

10:28.080 --> 10:32.120
whole, that was a three-year arc where we started out with something that didn't do

10:32.120 --> 10:36.000
anything, finally beat, like, you know, the in-house team, then we managed to go beat

10:36.000 --> 10:41.080
the pros and at each step it was just kind of pushing in all dimensions, right, is make

10:41.080 --> 10:46.240
the model bigger, it's to, you know, sort of, again, fix all the bugs and you just kind

10:46.240 --> 10:50.920
of keep iterating on every single dimension and every single dimension yields returns.

10:50.920 --> 10:55.560
And so I think that we did very much the same thing where for GP2, you know, it's not as

10:55.600 --> 11:00.240
simple as saying, okay, like, clearly you just need to, like, you know, crank up this

11:00.240 --> 11:02.440
one variable and you just do it in one shot.

11:02.440 --> 11:07.080
It's this, like, sort of iterative, like, stepping through the space on each axis at

11:07.080 --> 11:08.520
every single time.

11:08.520 --> 11:11.720
And so I think that on the one hand it does require conviction because you do need to

11:11.720 --> 11:15.360
say we're going to, like, carve out a big compute budget so that you're not constantly

11:15.360 --> 11:19.880
not kind of fighting other people for the big supercomputers, but on the other hand,

11:19.880 --> 11:23.560
I think it's also very iterative and you don't have to make scary irreversible decisions

11:23.600 --> 11:26.400
because in each step you get feedback from reality.

11:26.400 --> 11:31.800
And I think that that key of, like, both the big picture, thinking of what if this works

11:31.800 --> 11:36.840
and make sure that you're really set up for success, but also don't blindly spend a year

11:36.840 --> 11:40.320
of your organization on just, like, pursuing a thing that might not pan out.

11:40.320 --> 11:43.000
I think that balancing those two is what was really key.

11:43.000 --> 11:48.240
Yeah, I mean, one of the cool things is you sort of walk through this and talk through

11:48.240 --> 11:53.520
the insights is that the sort of organizational learnings were really critical in this entire

11:53.880 --> 11:59.320
sort of path-dependent sort of path to GP3.

11:59.320 --> 12:05.080
It sort of, you know, it makes sense when you say it that sort of insights from Dota 2

12:05.080 --> 12:09.040
and insights from the sentiment neuron were sort of like the key, these were like the

12:09.040 --> 12:13.040
key nuggets that led to the sort of, like, crystallized idea of, you know, scaling up

12:13.040 --> 12:17.520
and building GP3, but it's very unintuitive from the outside and sort of, I think it's

12:17.520 --> 12:22.680
almost a statement of innovation in some sense is that, you know, you're going to piece together

12:22.720 --> 12:26.720
this sort of, like, disparate collection of insights that you gather from a wide variety

12:26.720 --> 12:31.280
of experiments and eventually you sort of, like, get the ingredients together and you

12:31.280 --> 12:32.280
build something.

12:32.280 --> 12:34.880
Yeah, that's the first principle is thinking in action.

12:34.880 --> 12:35.880
Yeah.

12:35.880 --> 12:40.000
You know, I think that the story of AI, I don't know if you think about this at all, but I

12:40.000 --> 12:44.320
think about this a little bit, I think the story of AI to date and especially the past

12:44.320 --> 12:48.520
few years and the story of open AI is probably going to be something that historians are

12:48.520 --> 12:52.840
going to study for, you know, decades and decades to come.

12:52.840 --> 12:57.840
Are there any fun stories from the journey of creating some of these foundation models

12:57.840 --> 13:00.920
that you think deserve to be in the history books?

13:00.920 --> 13:06.600
Well, I'll tell you my actual favorite story from the Dota days.

13:06.600 --> 13:11.400
So, you know, we've been working on this system and, you know, actually the funny thing is

13:11.400 --> 13:14.360
at the very beginning we wrote down our list of milestones.

13:14.360 --> 13:20.200
On this date we're going to be Jonas, our best open AI employee who also had many thousands

13:20.200 --> 13:22.720
of hours of Dota 2 gameplay.

13:22.720 --> 13:25.240
This date we're going to beat the semi-pros, you know, this date we're going to beat the

13:25.240 --> 13:29.720
pros and so it's supposed to be like June 6th or something, June 6th rolls around, we

13:29.720 --> 13:30.720
don't have anything.

13:30.720 --> 13:34.480
Like, you know, he just crushes us and two weeks go by, three weeks go by, we keep pushing

13:34.480 --> 13:39.080
back that deadline by a week every week and then one day we actually do beat him.

13:39.080 --> 13:44.040
And you know, I think my conclusion was that like it wasn't actually actionable to sort

13:44.040 --> 13:48.720
of set those goals of outputs, you can only control your inputs, you can control the experiments

13:48.720 --> 13:49.720
you run.

13:49.720 --> 13:53.680
And so we just managed the project very differently after that and the thing that was so crazy

13:53.680 --> 13:58.160
to me still is that, you know, so a week before the international, which is the like world

13:58.160 --> 14:01.000
championships we're going to show up, we're going to play 1v1 against the best players

14:01.000 --> 14:05.120
in the world, we finally started beating our semi-protester and we're like, okay, maybe

14:05.120 --> 14:07.240
this is actually going to happen.

14:07.240 --> 14:10.360
But then we learned that he actually was on like vacation, he didn't have his like real

14:10.360 --> 14:14.360
setup and so we're like, oh no, like this is not going to go well.

14:14.360 --> 14:17.960
So we show up, you know, we continue to train, we like kind of do like a Hail Mary of like

14:17.960 --> 14:23.240
scaling things up, biggest scale we've ever done and we show up at the international and

14:23.240 --> 14:28.640
we play against, you know, like sort of low lowest, you know, low ranked pro, like a previous

14:28.640 --> 14:33.600
pro and we go 3-0-3-0-2-1.

14:33.600 --> 14:37.960
So we basically win-win and then we did have one loss and we take a look at it and it's

14:37.960 --> 14:41.360
like this item that we've never trained with, we've never seen before, oh wow, okay, we

14:41.360 --> 14:44.200
need to add that and, you know, do it fast.

14:44.200 --> 14:47.480
And so the team stays up all night putting this thing into the training, getting the

14:47.480 --> 14:51.520
whole thing launched and, you know, again, like we did double the scale where we're basically

14:51.520 --> 14:57.880
maxing out our CPU cores at this point and start training and, you know, we're supposed

14:57.880 --> 15:03.640
to play against the top pros in the world, fortunately they can't do the next day so

15:03.640 --> 15:09.320
we get an additional day of training and the number two person comes in, he plays against

15:09.320 --> 15:17.040
us and we win-win-win-win-win and he's like, okay, but I beat this but the top player is

15:17.040 --> 15:21.200
never going to lose to this thing or sorry, yeah, the top player is going to crash this

15:21.280 --> 15:27.800
thing and fortunately because he had spent so long playing that that guy couldn't come

15:27.800 --> 15:31.720
that day so we got one more day of training and that one more day of training was enough

15:31.720 --> 15:35.440
and so I think it's just the story of like you can really see the improvement and at

15:35.440 --> 15:40.360
each step we could see new behaviors that the system have learned and I think that that

15:40.360 --> 15:43.640
experience of just sort of watching a girl up in front of you is just something that

15:43.640 --> 15:45.140
was really amazing.

15:45.140 --> 15:50.740
I'm actually surprised because you would, I sensibly you'd probably train the sort

15:50.740 --> 15:54.660
of like agents for a long, long, long time going into the international and surprised

15:54.660 --> 15:55.900
at each incremental day.

15:55.900 --> 15:59.300
Yeah, so this is something I think has changed over time so at the time we basically had

15:59.300 --> 16:02.700
two weeks worth of training was like the whole model run and so you'd start from scratch

16:02.700 --> 16:07.860
each time and the thing that was really funny in the middle was that, you know, we put in

16:07.860 --> 16:11.420
this new item, we were training it and when we took out of training it was the best spot

16:11.420 --> 16:16.660
we ever saw except that our semi-protester was looking at it and was like this bot is

16:16.660 --> 16:19.700
doing something really dumb, it's just sitting there in the first wave and taking all this

16:19.700 --> 16:23.060
damage it doesn't have to, I'm going to go beat it, he ran it and go fight it and he

16:23.060 --> 16:24.060
lost.

16:24.060 --> 16:27.660
It's like that was weird and he did it like five more times and he lost each time but

16:27.660 --> 16:30.660
then he figured out a strategy that actually does work which is you, you realize what was

16:30.660 --> 16:34.460
going on was it was baiting him, it had learned to deceive, you know it actually learned that

16:34.460 --> 16:37.580
what you do is that like you pretend oh I'm just a weak little bot, I don't know what

16:37.580 --> 16:42.260
I'm doing and then you know a person comes in and you're just like smack.

16:42.260 --> 16:46.660
And so the way you defeat that is that you actually, you don't fall for the bait, right,

16:46.660 --> 16:50.380
you let the bot take all this damage and sit there and get weaker and then you finally

16:50.380 --> 16:55.580
go in for the kill and so there we actually stitched together our good bot for the first

16:55.580 --> 16:59.660
wave with the deceived bot thereafter and so there was a lot of this sort of like really

16:59.660 --> 17:04.460
examining what was going on in the systems because it's such a limited domain, you know

17:04.460 --> 17:07.860
it's a complicated domain but it's very, very interpretable.

17:07.860 --> 17:11.180
It meant that we could observe behaviors like this and figure out how to engineer around

17:11.180 --> 17:16.380
them but once we graduated from the 1v1 version of the game to the full 5v5 you know much

17:16.380 --> 17:21.660
more like you know like competitive basketball or something rather than heads up, suddenly

17:21.660 --> 17:26.260
all of our analysis of the behavior stopped working, right, that we used to have someone

17:26.260 --> 17:29.500
who just literally would watch the bot play and be like oh we have this bug in the training

17:29.500 --> 17:33.260
we got to go fix that, for 5v5 we just could not do that and I think that's kind of where

17:33.260 --> 17:37.820
we've graduated as a field is that too when you look at GPT-3 and the mistakes it makes

17:37.820 --> 17:42.060
sometimes people ask well why didn't it make that mistake and sometimes you can interpret

17:42.060 --> 17:45.940
it but sometimes it's also a little bit like asking well you know why did you make a mistake

17:45.940 --> 17:49.540
on some tests it's like well you think you know but like your explanation isn't always

17:49.540 --> 17:53.580
very good and I think that to do complicated behaviors sometimes there's a very complicated

17:53.580 --> 17:54.580
explanation.

17:54.580 --> 17:55.580
Yeah.

17:55.580 --> 18:00.260
Have you read this short story I think it's like the Lifecycle of Software Objects by

18:00.260 --> 18:01.260
Ted Chang?

18:01.260 --> 18:03.260
I think I have but I don't recall the details.

18:03.260 --> 18:06.420
It's like it's about how they're these AI pets and they sort of like keep learning new

18:06.420 --> 18:10.620
and new behaviors it's very reminiscent of describing these Dota agents.

18:10.620 --> 18:15.180
Yeah yeah I think we'll see that kind of thing in our future somewhere.

18:15.180 --> 18:19.260
I want to kind of go back you know one of the things we've known each other for many

18:19.260 --> 18:26.380
years long before you know these foundation models and even before this competition Dota

18:26.380 --> 18:33.940
2 and one thing I vividly remember is how sort of optimistic and confident you were

18:33.940 --> 18:39.300
in sort of this sort of path of increasing and increasing AI capability you know sort

18:39.300 --> 18:44.500
of I remember the time as maybe 2016, 2017 it felt very striking because it was sort

18:44.500 --> 18:51.460
of like you know with these algorithms they're still pretty weak and you were always very

18:51.460 --> 18:54.940
confident like oh yeah they're just going to keep getting better and better and better

18:54.940 --> 18:57.140
and you know you're very a lot of confidence in that.

18:57.140 --> 19:03.860
What were the things that back then gave you sort of the resolve or confidence in the

19:03.860 --> 19:06.060
and the optimism in the technology?

19:06.060 --> 19:13.540
Yeah I mean at some level you know to have that kind of belief and conviction and something

19:13.540 --> 19:15.860
that hasn't happened yet it's a very intuitive thing.

19:15.860 --> 19:21.100
I mean I remember when I was in school and showed up excited about doing NLP research

19:21.100 --> 19:24.620
I went and tracked down an NLP professor and I was like please can I do some research for

19:24.620 --> 19:29.380
you and he's like okay he shows me these like parsed trees and stuff and I look at that

19:29.380 --> 19:33.940
and I was like this is never going to work right and you know to explain like why does

19:33.940 --> 19:36.660
it feel like it's not going to work it just doesn't have the right properties right it

19:36.660 --> 19:41.460
just felt like you're going to pour all this human like engineering and intuition and effort

19:41.460 --> 19:45.300
into the system and I know I can't even describe how language works.

19:45.300 --> 19:46.300
Yep.

19:46.300 --> 19:49.620
Right it just feels like there's just something inherently missing but I think neural nets

19:49.620 --> 19:51.420
have the opposite property.

19:51.420 --> 19:55.580
Neural nets is very clear this is a system that absorbs data it absorbs compute it's

19:55.580 --> 20:00.060
like a sponge that just like slurps everything up and so it has the right form factor but

20:00.060 --> 20:03.380
the thing that's always been missing as well can you train it right?

20:03.380 --> 20:07.300
Do you have enough data do you have enough compute do you have enough ability to like

20:07.300 --> 20:10.420
have a learning algorithm that can shovel the stuff in efficiently in a way that it comes

20:10.420 --> 20:14.380
out in some way that generalizes like that's the thing that's been missing and I think

20:14.380 --> 20:19.500
what became kind of clear you know the field really I think got its most recent resurgence

20:19.580 --> 20:27.660
in 2012 with the Alex in that paper and I think that there that was the first time where

20:27.660 --> 20:31.820
you had a neural net that really just crushed a task right that it was like people had spent

20:31.820 --> 20:36.940
decades on computer vision and suddenly it's like well I'm so sorry but this approach has

20:36.940 --> 20:41.380
just supplanted you by this this massive gap and I think that you just started to see it

20:41.380 --> 20:46.140
spread right that it was almost like you had these these all these disparate departments

20:46.140 --> 20:51.460
and there was this wall that was being knocked down day after day and I think that when you

20:51.460 --> 20:55.620
see a trend like that were things that have been long-standing and very deeply established

20:55.620 --> 20:59.820
in these ways of thinking these great debates that have gone on for a long time and suddenly

20:59.820 --> 21:05.380
you're seeing a repeated result that is consistent with the history I think that that for me

21:05.380 --> 21:08.940
is maybe the most clear sign that it's like something is going to work and there's a real

21:08.940 --> 21:12.060
sort of exponential that is waiting to unfold.

21:12.060 --> 21:17.940
And then you know were there what were the what were the moments if any of of doubt and

21:17.940 --> 21:22.620
you know let's let's chart the path I think open I start in 2016 yeah yeah I'd say December

21:22.620 --> 21:28.620
2015 you know 2016 okay great December 2015 till now were there any moments of of doubt

21:28.620 --> 21:33.820
in the technology or was it sort of always hey this is you know this is clearly the way

21:33.820 --> 21:39.500
of the future yeah I mean I think that doubt is a strong word there's definitely moments

21:39.500 --> 21:44.100
like I think to build something you you're always doubting right that you're always like

21:44.100 --> 21:47.660
you've got to be questioning every single bit of your implementation like anytime you

21:47.660 --> 21:51.140
see like a graph that's wiggling in a weird way you've got to go figure it out you can't

21:51.140 --> 21:55.940
just be like I'm sure that the AI's will sort it out.

21:55.940 --> 22:00.460
And so I think there was like lots of sort of tactical doubt lots of like sort of worries

22:00.460 --> 22:04.500
that were not quite doing it right lots of like redoing the calculations to figure out

22:04.500 --> 22:08.820
like hey how big of a model do you think you're going to need lots of mistakes for sure like

22:08.820 --> 22:13.700
a good example of this is the scaling laws so we did this study to actually start to

22:13.700 --> 22:18.700
really scientifically understand how do models improve as you push on various axes so as

22:18.700 --> 22:22.940
you pour more computing as you pour more data in and one conclusion that we had at one point

22:22.940 --> 22:28.620
was that basically that there's you know sort of a limited amount of data that you want

22:28.620 --> 22:32.500
to pour into these models and that there's kind of this very this very clear curve and

22:32.500 --> 22:36.700
that one thing that we realized only years later was actually that we'd read the curves

22:36.700 --> 22:41.120
a little bit wrong and you actually want to be trained for way more tokens way more data

22:41.120 --> 22:45.220
than anyone had expected and that did I you know that there's definitely these moments

22:45.220 --> 22:48.780
where these things that just didn't quite click where it's like just didn't add up that

22:48.780 --> 22:52.740
we were training for so little and that you know something conclusions that you drew downstream

22:52.740 --> 22:57.060
but then you realize there was a foundational assumption that was wrong and suddenly things

22:57.060 --> 23:00.780
make way more sense so I think it's a little bit like you know physics in some sense for

23:00.780 --> 23:04.620
like do you do doubt physics it's like I kind of do I think all physics is wrong right

23:04.620 --> 23:08.740
but like only so wrong right it's like we clearly haven't reconciled like quantum and

23:08.740 --> 23:12.620
relativity is that there's like something wrong there but that that wrongness is actually

23:12.620 --> 23:16.900
an opportunity it's actually a sign of you have this things are useful right really like

23:16.900 --> 23:20.060
it's affected our lives and it's actually like pretty great I'm very happy with what

23:20.060 --> 23:24.580
physics has done but also there's fruit and so I think that that for me that's always been

23:24.580 --> 23:29.620
the feeling that there's something here and that you know if we do keep pushing and somehow

23:29.620 --> 23:32.860
the scaling laws all peter out right and they suddenly drop off a cliff and we can't make

23:32.940 --> 23:37.140
any further progress like that would be the most exciting time in this field because we

23:37.140 --> 23:41.180
would finally reach the limit of technology we would finally learn something and then

23:41.180 --> 23:44.860
we would finally have a picture of what the next thing to do is yeah that's super it actually

23:44.860 --> 23:51.100
reminds me of this one of the stripe operating principles which is I think micro pessimists

23:51.100 --> 23:56.380
macro optimists yep yep yes and it's very I mean it's very resonant but obviously like

23:56.380 --> 24:01.980
very related to what you're talking about which is these you know you have to be extremely

24:01.980 --> 24:06.220
pessimistic we're extremely questioning in the moments of the technology but then obviously

24:06.220 --> 24:11.140
on a long enough time horizon incredible stuff pops out yep you got to be excited like I

24:11.140 --> 24:15.820
think that this is just an exciting field and it's a scary field as well you got to have

24:15.820 --> 24:20.980
some amount of just like awe at the fact that you have these these models that they started

24:20.980 --> 24:25.460
as just random numbers right and then you have build these massive supercomputers these massive

24:25.460 --> 24:30.500
data sets and you do a ton of the engineering work you do a ton of these algorithmic developments

24:30.580 --> 24:34.340
you put them all into a package right and we don't really have other technologies that

24:34.340 --> 24:39.220
work like this like I think the fact to me the most fundamental picture this like sponge

24:39.220 --> 24:43.260
that you just kind of pour stuff into and you get this model it's reusable and works

24:43.260 --> 24:47.420
across all these different areas like you can't do that for traditional software right

24:47.420 --> 24:51.380
traditional software is it's just you know human effort writing down all the rules and

24:51.380 --> 24:54.780
that's where the return comes from but you can't you know maybe you have like a spark

24:54.780 --> 24:59.940
cluster that does some stuff but that's not that's not the cake and in in neural networks

24:59.940 --> 25:04.900
it really is yeah you know I want to kind of switch gears to thinking about the the

25:04.900 --> 25:10.460
sort of future and and and looking forward at what kind of what's next what do you think

25:10.460 --> 25:13.800
I mean I'll ask this sort of as broadly as possible to start with what do you think the

25:13.800 --> 25:22.300
future of AI holds yeah I think that the future of AI is going to again be both exciting and

25:22.300 --> 25:25.620
a source of a lot of change and I think that that is something that you know part of our

25:25.640 --> 25:31.180
mission is to try to help facilitate that as positive way as possible I think that kind

25:31.180 --> 25:36.700
of you know the super high level I kind of feel like AI was like you know something that

25:36.700 --> 25:40.620
for the you know 2020 10s was like kind of cool you know it's the game of like publishing

25:40.620 --> 25:44.340
papers and you play some video games and like you know it's just like it's just like fun

25:44.340 --> 25:51.820
good science I think it's really interesting that 2020 kicked off with GPT 3 which is really

25:51.900 --> 25:56.620
the first model that was commercially useful just as the model like literally put an API

25:56.620 --> 26:01.780
on top of it people just talk to it and people build products on top of it and you know that

26:01.780 --> 26:06.260
you know one of our early customers just you know just raised it at 1.5 billion valuation

26:06.260 --> 26:10.540
which to me is is a really wonderful thing to realize that you build this model and it

26:10.540 --> 26:15.900
creates so much value for so many different people and I think that we're still in such

26:15.900 --> 26:20.300
early days for what these models can do and so I think that what I'm most excited about

26:20.340 --> 26:26.620
from just seeing GPT 3 seeing Dolly is thinking about the the sort of economic value that

26:26.620 --> 26:30.620
it can create for people and I think that there's a lot of other pieces to it in terms

26:30.620 --> 26:35.260
of like you know that everyone's going to be more creative if you want to like I can't

26:35.260 --> 26:38.780
draw but now I can create images now I could take a picture of this in my head and I can

26:38.780 --> 26:42.940
actually see it on on a page and one of my favorite applications of Dolly is actually

26:42.940 --> 26:48.020
people who are 3D physical artists you know something's like a sculptor and now they

26:48.060 --> 26:51.500
can actually get a great rendering of the thing that they have in mind by kind of just

26:51.500 --> 26:55.460
like iterating with this machine and they go build it right I think that this sort of

26:55.460 --> 27:00.580
amplification of what humans can do is what these systems are for and so I think that

27:00.580 --> 27:03.740
for this decade I think what we're really going to see is these tools just sort of

27:03.740 --> 27:07.100
proliferating they're going to be everywhere they're going to be baked into every company

27:07.100 --> 27:11.620
I think it's kind of like the internet transition that you know that it was kind of like if

27:11.620 --> 27:15.500
you're a company like what's your internet strategy and you know 1990 it's like what

27:15.540 --> 27:19.540
even is this thing you know and in 2000 it's like huh maybe it's interesting and there

27:19.540 --> 27:24.340
there's a little you know boom and bust and here we are today even talk about an internet

27:24.340 --> 27:28.060
strategy is like it's just so integral to every business it's not even like it's not

27:28.060 --> 27:31.380
even a separate thing right it's just like it's just part of like your it's like your

27:31.380 --> 27:34.380
payroll strategy right it's like it's not like a separate part of your business that

27:34.380 --> 27:37.740
you can pick or choose whether you're going to have it and I think that AI is going to

27:37.740 --> 27:41.820
be much the same I think there will be a transition point right I think that it's it's

27:41.820 --> 27:45.060
interesting like our mission is really about building artificial general intelligence right

27:45.100 --> 27:49.620
really trying to build machines that are able to perform whole tasks right that are you know

27:49.620 --> 27:54.660
push this technology to its limit and build machines that are able to you know our charter

27:54.660 --> 27:58.540
definition is outperform humans at most economically valuable work and there's a question of the

27:58.540 --> 28:02.980
timeline but I think that that picture of you know you have these tools that are creative

28:02.980 --> 28:07.700
that help everyone amplify and what happens when they do become so capable that they're

28:07.700 --> 28:12.380
able to perform these tasks even autonomously and I think that actually the implications

28:12.380 --> 28:16.460
of that are different from what people expect I think that it's much more like you know that

28:16.460 --> 28:20.860
I think there's still going to be this amplification but I think that there the change is going

28:20.860 --> 28:26.300
to be just very hard to predict and unexpected and I think that really thinking about how

28:26.300 --> 28:30.820
all of that sort of value gets distributed how to make sure that it's sort of pointed

28:30.820 --> 28:35.020
at solving these like hard challenges that humans you know maybe are unable to solve

28:35.020 --> 28:38.780
ourselves you know the climate change and you know universal education and things like

28:38.860 --> 28:44.220
that and really transitioning to this like AI powered world I think is going to be just

28:44.220 --> 28:48.140
like a real sort of challenge for the whole you know all of humanity to work together

28:48.140 --> 28:53.380
on. Yeah I mean I totally agree one thing that I think is almost funny with how the timing

28:53.380 --> 28:56.820
of all these technologies have worked out is that you know last year everyone was talking

28:56.820 --> 29:04.100
about Web 3 as crypto and now it feels very obvious that AI is the actual Web 3 you know.

29:04.180 --> 29:08.540
We'll take Web 4. Yeah Web 4 we'll skip we'll skip over one but sort of like Web 1 was just

29:08.540 --> 29:14.140
reading Web 2 was reading and writing and now Web 3 or 4 depending on what we want to

29:14.140 --> 29:20.540
say is ads, computer reading, computer write and it's sort of this incredible new phase.

29:20.540 --> 29:24.420
You know one so I think I think you mentioned two two directions here that I think are really

29:24.420 --> 29:31.780
interesting one is the sort of advancement and sort of proliferation of GP3 and Dolly

29:31.860 --> 29:35.660
and sort of the existing tools becoming more and more economically useful and there's

29:35.660 --> 29:40.460
sort of this continued improvement of the algorithms themselves towards sort of towards

29:40.460 --> 29:44.860
sort of AGI. What do you think and obviously don't reveal any opening secrets but what

29:44.860 --> 29:49.140
do you think this sort of like roadmap to AGI looks like from where we are now? I mean

29:49.140 --> 29:54.700
I think that humanity to a large extent has been on the AGI roadmap for a very very long

29:54.700 --> 30:00.900
time. I think even looking at just the history of neural networks in particular you know

30:00.940 --> 30:04.820
on the one hand we say hey 2012 like that was the moment like everything changed you

30:04.820 --> 30:08.500
know that like you look at these we have all these curves of how much compute people put

30:08.500 --> 30:12.060
into the landmark results it was going like 10x year over year still continuing by the

30:12.060 --> 30:19.220
way that's that's a decade of 10x year over year that's insane and the thing is we actually

30:19.220 --> 30:25.780
did a study to then look back at previous results all the way back to you know say the

30:25.820 --> 30:32.340
perceptron in 1959 and you actually find that there's basically a very smooth curve back

30:32.340 --> 30:36.220
there as well. The amount of compute going into all the landmark results was exactly

30:36.220 --> 30:41.020
Moore's law and it kind of makes sense right it's like that people were not willing to

30:41.020 --> 30:45.580
spend more money they wanted to spend a constant amount of money on these experiments because

30:45.580 --> 30:50.060
you're starving grad students like you know you can only get so much computer time and

30:50.100 --> 30:56.380
that the results got better and better the more compute was available to them and I think

30:56.380 --> 30:59.740
that that is so interesting that yeah basically what changed in 2012 was that we said okay

30:59.740 --> 31:02.660
we're just gonna like you know we are gonna spend more money we're gonna build massive

31:02.660 --> 31:08.500
supercomputers now because the ROI is there but that fundamentally the curve if you control

31:08.500 --> 31:12.100
for that that cost factor it looks exactly the same and so I think that basically this

31:12.100 --> 31:16.500
picture of building more capable models by pouring more compute into them by getting

31:16.580 --> 31:21.260
better at harnessing this technology of neural networks back propagation I think that has

31:21.260 --> 31:24.940
been very invariant and the details you know maybe change a little bit you know do you want to

31:24.940 --> 31:30.140
work on GPT-3 do you want to work on whisper like do you pour in your your you know speech

31:30.140 --> 31:34.700
data do you pour in text data from the internet and to me those details I think you know they

31:34.700 --> 31:38.460
matter in the like in the like sense of like what are you gonna work on today and you know

31:38.460 --> 31:42.020
what are you gonna download but if you zoom out you look at the scale of like these this

31:42.060 --> 31:47.300
technology I think it actually sort of doesn't matter so much I think kind of what we're

31:47.300 --> 31:49.940
building it's almost like building computers like you think about the Haiti and Moore's

31:49.940 --> 31:52.900
law right where it's just like there's a new chip that comes out and there's a new chip that

31:52.900 --> 31:56.380
comes out and it's kind of like what's the you know what's the path to building the best

31:56.380 --> 31:59.860
computer the answer is well you just keep building the next best chip and you keep building

31:59.860 --> 32:03.620
the next best chip and you keep getting better peripherals and all these you know keep working

32:03.620 --> 32:09.220
every single piece of the technology and so I think this full stack of better GPUs great

32:09.260 --> 32:13.060
software for utilizing them neural networks that we learned to harness more and more the

32:13.060 --> 32:17.340
scaling laws doing all the science alignment extremely important making sure these models

32:17.340 --> 32:22.940
not just are smart but actually are aligned with what humans intend all of that I think is the

32:22.940 --> 32:27.300
stack and so I think that you know what our goal is is just to keep doing something that was

32:27.300 --> 32:31.540
previously impossible every single year so you know that I guess well you should check back

32:31.540 --> 32:37.420
in a year but hopefully 2023 we'll all forget about Dolly 2 and GPT 3 and we'll be talking

32:37.420 --> 32:42.820
about something new and I think as long as we continue that like you cannot continue that

32:42.820 --> 32:49.020
path without ending up somewhere amazing yeah I mean I think I actually remember this in I

32:49.020 --> 32:55.980
think probably 2017 you were sort of very still quite you were very excited about sort of the

32:55.980 --> 33:02.980
sort of Moore's law continuing and that that sort of creating a lot more opportunity for you

33:02.980 --> 33:08.420
know neural networks and AI and that's that's sort of played out are you worried about the sort

33:08.420 --> 33:16.380
of proverbial end of Moore's law kind of causing a stall out in in progress so I'm not worried

33:16.380 --> 33:19.820
about it per se like I think the way to think about this right because I think we you know we

33:19.820 --> 33:23.660
often get caught in this debate of like is it all about scale or is it all about algorithms is

33:23.660 --> 33:28.540
all about data and the answer is that's a wrong question right it's really like you multiply

33:28.540 --> 33:31.660
together these factors and the best thing to do when you're multiplying together multiple terms

33:31.740 --> 33:36.060
is that you actually kind of want them all to be equal and I think that the answer is like it's

33:36.060 --> 33:41.980
been great for the past you know seven years that we've been able to just pour more dollars to

33:41.980 --> 33:45.740
build bigger computers that's one way to get ahead of Moore's law at some point they're just

33:45.740 --> 33:49.900
aren't more dollars right there aren't more grains of sand to you know that have been turned into

33:49.900 --> 33:56.860
into these these wonderful computers that we use so there is a limit there that we have not yet

33:56.940 --> 34:02.620
hit but when you do that does not stall all progress right you still have algorithmic progress

34:02.620 --> 34:07.100
and there we've again done studies and we've shown that actually if you take like an example is if

34:07.100 --> 34:12.220
you look at the amount of compute it takes to hit the same performance so to train you know a state

34:12.220 --> 34:19.740
of the art that you know 2012 or 2014 vision model that that computes also falling exponentially

34:19.740 --> 34:24.460
we're basically making exponential progress in algorithms not at the same rate as we are able

34:24.460 --> 34:29.180
to you know sort of build bigger computers but that is an amazing force too you know it's like

34:29.180 --> 34:33.020
I've got this exponential I've got that exponential like let's not even talk about the data exponential

34:33.020 --> 34:37.900
so I think that that the truth is that we will find a way I think that the history of this field

34:37.900 --> 34:43.020
is just so consistent and I think that that you know humanity is just so innovative that I think

34:43.020 --> 34:49.660
that that we're not going to hit a wall for the foreseeable future and do you think that you know

34:49.660 --> 34:56.380
one of the one of the interesting juxtapositions of of today just from a scientific perspective is

34:56.380 --> 35:01.740
a relative slowing in nearly every other science and there's you know there's a lot of research

35:01.740 --> 35:05.740
that sort of demonstrated that science on the whole slowing and then comparatively the sort of

35:05.740 --> 35:09.980
acceleration of artificial intelligence and sort of this this you know in many ways this

35:09.980 --> 35:16.220
renaissance that we're entering right now do you do you fear that at some point AI will similarly

35:16.220 --> 35:20.860
sort of reach these points of admission mark returns and slow relative like in much in the

35:20.860 --> 35:25.580
same way that other sciences have or do you think that's so far away that you know well I think

35:25.580 --> 35:29.820
two things I mean I think that there's there's always s-curves although I think that something

35:29.820 --> 35:33.980
is also interesting about s-curves is that there tends to be paradigm shifts like have you ever

35:33.980 --> 35:40.060
read Singularity is Near no I haven't yeah so this is the Ray Kurzweil book from like 2004

35:40.060 --> 35:45.420
something and I always thought just based on the reputation it's going to be kind of a crazy book

35:45.420 --> 35:50.140
but if you actually read it it's the most dry boring reading you'll ever do and it's basically

35:50.140 --> 35:56.540
just curve after curve of different industries within computing showing how the performance has

35:56.540 --> 36:02.060
changed over time and it's you know basically the conclusion he comes to is that there's this

36:02.060 --> 36:07.020
repeated pattern that seems to happen across you know memory across number of transistors on the

36:07.020 --> 36:12.220
chip you know etc etc where you kind of have an s-curve of the current paradigm and then you have

36:12.300 --> 36:19.580
paradigm shift and that example he talks about is you know thinking about let's talk about CDs

36:19.580 --> 36:23.500
right so you talk about great you know CD adoption it's like you know it's great s-curve it's

36:23.500 --> 36:27.340
suddenly everywhere it's like everyone's got a CD player like it's just the technology of the day

36:27.900 --> 36:31.500
and people get really excited about doing more of the same thing it's like Blu-ray that's the thing

36:31.500 --> 36:36.460
you know and so then everyone starts investing in Blu-rays and somehow it just doesn't take off

36:36.460 --> 36:39.740
and it's because it's just more of the same and it's like you know it's not backwards compatible

36:39.740 --> 36:43.580
and so it's just not really worth it but the real paradigm shift was streaming right suddenly

36:43.580 --> 36:47.420
you have this new adoption curve this new s-curve that just is this like totally different way

36:47.420 --> 36:50.540
and the way we got to fast computers was basically five different paradigm shifts

36:50.540 --> 36:54.380
across a hundred years and so I think that that's maybe a story here too which is like

36:54.380 --> 36:58.140
there's gonna be an s-curve in what we're doing right now and that there will be a paradigm shift

36:58.140 --> 37:02.140
when you hit it and I think that that again speaks to the ingenuity of humans but I think there's

37:02.140 --> 37:09.260
also a second thing where my other answer is to some extent it doesn't matter because the thing

37:09.260 --> 37:14.220
about this field is that it's useful now right that kind of the goal that I think we've always

37:14.220 --> 37:19.100
had for AI was to actually make us so computers are just way more helpful like you think about what

37:19.100 --> 37:22.780
computers have done for humanity right like how many problems they've helped us solve they've

37:22.780 --> 37:26.540
created new problems as well but I think that on net that they've helped us solve way more problems

37:26.540 --> 37:30.540
than they've created and I think they've kind of just changed the nature of how we interact with

37:30.540 --> 37:34.780
each other about how you know like it's just like hard to get lost anymore right you just

37:34.780 --> 37:39.020
pull out google maps I think there's really amazing problems that are now within our reach

37:39.020 --> 37:43.740
that just would not have been otherwise and I think that AI like we're starting to crack that

37:43.740 --> 37:47.900
nut we're starting to be able to you know like it's I think it's kind of interesting you could

37:47.900 --> 37:54.220
get a co-pilot you know which which we we power we have the models that power it and that the way

37:54.220 --> 37:58.300
that that is useful to people is that provides very low latency suggestions right it's basically

37:58.300 --> 38:03.020
an autocomplete for code and that you know there's a very strict latency budget you know if you're

38:03.020 --> 38:07.740
more than you know 1500 milliseconds to get a autocomplete suggestion it's worthless like no

38:07.740 --> 38:13.660
one wants that you've already moved on but I think that what we really want to build the next

38:13.660 --> 38:19.100
phase is machines that help you produce that are able to produce artifacts that are materially

38:19.100 --> 38:23.100
interesting on their own so not just interesting because it's like a fast suggestion to you but

38:23.100 --> 38:27.260
because it's actually a quality answer and you're starting to see if you talk to our current gbt

38:27.260 --> 38:31.100
iteration you can ask it to write some poems and it writes way better poetry than I can

38:31.820 --> 38:39.820
it actually wrote a poem for my wife that made us both cry like you know yeah I I cannot do that

38:39.820 --> 38:45.660
myself but now I can you know by partnering with this with this machine and I think that

38:45.660 --> 38:50.140
that's the real story right is really trying to get these tools out and everywhere and yeah you

38:50.140 --> 38:55.020
know if what we're doing right now stalls out I don't think that removes the value from what we're

38:55.020 --> 39:01.020
able to create yeah by the way it's depressing that the the attention span of most engineers is

39:01.020 --> 39:09.340
only 1500 seconds but you know it is what it is I what uh what if anything you know I think

39:10.060 --> 39:15.260
one of the things that if I recall spurred you to to work on opening I was was sort of

39:16.140 --> 39:20.780
also being concerned about the sort of potential negative consequences of of the technology

39:20.780 --> 39:25.900
um what at this point looking forward what are your sort of biggest concerns or what are you

39:25.900 --> 39:30.300
afraid of with with artificial intelligence that you sort of urge everyone in the field to sort of

39:30.300 --> 39:36.540
help avoid yeah so I think that one thing that's very interesting about AI is that you know if you

39:36.540 --> 39:41.180
talked to certainly you know 10 years ago if you looked at every article about it you talked to

39:41.180 --> 39:45.100
someone on the street terminator is the main thing that comes up right so I think there's always been

39:45.180 --> 39:51.820
this feeling around AI that is sort of you know that there's an element of fear mixed with the

39:51.820 --> 39:54.940
you know sometimes people don't see any potential or sometimes you know they realize that there is

39:54.940 --> 40:00.380
the potential but like really trying to figure out and navigate it and I think that that picture

40:00.380 --> 40:04.060
the specifics you know I think that that we're starting to see a little bit more but I think

40:04.060 --> 40:07.900
the high level picture of this is technology that's very powerful and it can be powerful in

40:07.900 --> 40:12.380
positive ways and negative ways um I think is extremely correct and I think it's very important

40:12.380 --> 40:16.220
not just to be you know starry eyed optimist everything's just going to work itself out

40:16.220 --> 40:21.900
but also not to be you know sort of doomsday like everything is terrible and you know humanity is

40:21.900 --> 40:25.980
over because I don't think that's at all true I think this technology can be the the best thing

40:25.980 --> 40:29.980
that is the way we've ever created and help us be the best versions of ourselves but I think

40:29.980 --> 40:35.020
that it requires very careful navigating of the space and it's not something that just is for you

40:35.020 --> 40:38.380
know companies of Silicon Valley to figure out I think it's it's really all of humanity kind of

40:38.380 --> 40:43.660
challenge um so I think that we're going to go through different phases I think that that right

40:43.660 --> 40:48.540
now I you know we're kind of starting to build systems where that I you know you think about

40:49.260 --> 40:55.020
misuse is the most clear problem and the systems themselves are still not very powerful right

40:55.020 --> 40:59.500
that the kinds of things you worry about for GPT-3 are you know important problems you think

40:59.500 --> 41:04.140
about bias and representation you think about the system sort of you know sort of saying the wrong

41:04.140 --> 41:08.300
thing you know but but its action is really in your mind right it's it's sort of words on a page

41:08.300 --> 41:12.380
that then you know words on a page are very powerful but that they don't themselves have

41:12.380 --> 41:16.460
direct action in the world but you think about something like codex our code writing system

41:16.460 --> 41:21.500
which is a little bit more like a robot because it has it emits code and if you were to just execute

41:21.500 --> 41:26.700
that code directly it can actually directly have actuators into the world and making sure that that's

41:26.700 --> 41:31.500
aligned and doing the right kinds of things not having buggy code and not writing viruses and that

41:31.500 --> 41:35.260
kind of thing like that's really important and so I think that that figuring out what values go

41:35.260 --> 41:39.020
into this these machines and that they're operating according to those values that's going to be very

41:39.020 --> 41:45.020
very critical figure out how to avoid misuse and sort of regulate that both at a sort of societal

41:45.020 --> 41:50.700
level at a you know technical level all of that is very important and I and I do think that there

41:50.700 --> 41:55.260
is also a point where the technology itself you have to think about that it's going to be extremely

41:55.260 --> 42:00.540
powerful and you think about a system that's you know sort of talking to lots of humans and

42:00.620 --> 42:03.740
is operating unchecked that's the kind of thing that you should worry about you know we already

42:03.740 --> 42:08.460
worry about that think about the companies right that lots of people are using this you know social

42:08.460 --> 42:12.860
media platform or you know any of the technologies that we use and how much influence those can have

42:12.860 --> 42:19.740
in the world and those aren't systems that have you know sort of deep sort of behaviors that are

42:19.740 --> 42:24.300
emergent from from from what they've learned and so I think that that figuring out the technical

42:24.300 --> 42:29.500
controls to make sure that these systems remain in service of humanity and sort of to

42:29.580 --> 42:35.340
actually empower and accelerate all of us that I think is is also a very critical thing so it's

42:35.340 --> 42:39.340
kind of this like a ramping set of stakes and making sure that we're building systems that are

42:39.340 --> 42:44.220
aligned with with our values and figuring out what that even means like what what is the values

42:44.220 --> 42:47.900
of humanity that should be in the system and that I think is not going to be an easy problem

42:48.780 --> 42:54.620
you know one question and this this may be the last question that I have for you is sort of one

42:54.620 --> 43:01.900
of the thing one of the conclusions of if the technology such that scale continues to be the

43:01.900 --> 43:06.300
the sort of one of the more important things you know scale whether it's data or better algorithms

43:06.300 --> 43:13.660
or scale compute then it it the technology itself will tend towards sort of this game theoretical

43:13.660 --> 43:18.780
proliferation mode where it's sort of like people are going to compete and you see some of this today

43:18.780 --> 43:22.060
even with the large tech companies and you guys obviously people are going to compete to sort of

43:22.060 --> 43:26.620
build the bigger supercomputers that have the better performance and you have the bigger supercomputer

43:26.620 --> 43:30.300
you have sort of supremacy over the other supercomputers and sort of there's like this

43:31.180 --> 43:36.380
you know laddering the stakes and sort of and proliferation is really the sort of the right

43:36.380 --> 43:41.500
word do you think that that is a version of the future or do you think that there's sort of

43:41.500 --> 43:47.100
some path in which this becomes a much more sort of like open and useful not this sort of like

43:47.180 --> 43:53.500
tool for nation states or large companies to compete with one another I think that the future

43:53.500 --> 44:02.300
that seems to be unfolding is kind of a you know replay of how say computing technology has played

44:02.300 --> 44:08.060
out more broadly I think that that it is still going to be the case that you're going to have

44:08.060 --> 44:13.180
these increasingly massive supercomputers that are in the hands of only a few that are able to just

44:13.180 --> 44:17.980
create models that can just do crazy things that no one else can do but I don't think that that

44:17.980 --> 44:22.860
removes the value from the massive set of things that people are going to do with these models

44:22.860 --> 44:28.780
and so you know I think that balancing the like super powerful very dual use extremely you know

44:28.780 --> 44:33.660
think of these like almost like you know these massive like you know sort of systems that are

44:33.660 --> 44:36.620
you know we think about like a nuclear reactor and it's like you know it's like these like giant

44:36.620 --> 44:41.100
like sort of you know systems that you should approach with great care um and you think about

44:41.100 --> 44:45.180
like you know by contrast think about wind turbines and like there's lots of wind turbines

44:45.180 --> 44:48.220
everywhere and actually that if you add up the amount of value from wind turbines versus nuclear

44:48.220 --> 44:51.900
reactors like I think actually that the balance is probably in favor of wind turbines and so I

44:51.900 --> 44:56.220
think that that's kind of the future we're going to is that the AI technology is going to be everywhere

44:56.220 --> 45:01.420
and there's going to be lots of value that's delivered by having open source models that are

45:01.420 --> 45:05.100
integrated to every business and that people are building all sorts of crazy applications on top of

45:05.100 --> 45:09.100
and that's something that we really want to support and promote and you also have to have this

45:09.100 --> 45:13.420
dual answer for what you do with the with the new extremely capable stuff that's just a mile ahead

45:13.420 --> 45:18.140
of everything else and that's something you have to treat with kid gloves with with more care

45:18.140 --> 45:22.540
and I think that that balance is tricky it's not easy um that's something that we as an organization

45:22.540 --> 45:25.740
have been trying to straddle and I think that you know we've had real existential struggles

45:25.740 --> 45:29.980
internally trying to figure out you know like our goal is to empower everyone it's really to uh

45:29.980 --> 45:35.660
to to bring everyone along to this AI transition and the best way to do that I think that our

45:35.660 --> 45:39.740
picture of it has changed as the technology has unfolded and I think that we're starting to get a

45:39.740 --> 45:44.940
sense of of you know where this can go it's really exciting to see all the the energy of all these

45:44.940 --> 45:48.460
builders coming in because I think that like you said people are starting to realize like AI is

45:48.460 --> 45:53.580
really going to work and it's time to build yeah well um this was an incredible conversation thank

45:53.580 --> 45:58.860
you so much Greg next time we speak I'll make you uh read the poem that all right there we go

45:58.860 --> 46:02.060
cool thank you so much thank you so much

