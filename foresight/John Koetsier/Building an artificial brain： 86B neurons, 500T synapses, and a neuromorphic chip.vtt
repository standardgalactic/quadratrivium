WEBVTT

00:00.000 --> 00:03.680
Ultimately, we see this as kind of like Lego pieces that, you know,

00:03.720 --> 00:08.000
due to their low power footprints, we'll be able to concatenate them together

00:08.000 --> 00:12.480
using things like chip lead integration, advanced packaging, and ultimately

00:12.520 --> 00:17.240
scale out these systems to be brain scale, 86 billion neurons,

00:17.320 --> 00:23.320
500 trillion synapses, and low power enough that they can exist in autonomous devices.

00:23.880 --> 00:28.080
Can we build a computer chip that operates like the brain?

00:28.640 --> 00:32.960
We've seen neuromorphic computer chips from companies like Intel before,

00:32.960 --> 00:37.080
like the Luigi chip, which Intel claimed had a sense of smell.

00:37.600 --> 00:42.240
Rain neuromorphics, however, says it has an end-to-end analog chip,

00:42.520 --> 00:47.280
a neuromorphic processing unit that they say is the world's first end-to-end

00:47.280 --> 00:49.880
analog trainable AI circuit.

00:50.200 --> 00:55.480
It's a fully analog neural network, and it's a thousand times more energy

00:55.480 --> 00:58.280
efficient than today's best processors.

00:58.560 --> 01:01.800
Here to chat is Rain neuromorphic CEO, Gordon Wilson.

01:02.080 --> 01:02.800
Welcome, Gordon.

01:03.880 --> 01:05.040
Thanks so much for having me, John.

01:05.080 --> 01:05.760
It's good to be here.

01:06.840 --> 01:07.840
It's good to have you.

01:08.080 --> 01:11.360
When I see neuromorphics in the inbox, I got to check it out.

01:11.360 --> 01:12.560
It's a cool space.

01:13.320 --> 01:14.240
Let's start here.

01:14.280 --> 01:15.880
What the heck is an analog chip?

01:17.160 --> 01:18.320
Yeah, great question.

01:18.320 --> 01:22.600
And I think a great kicking off point because it really allows us to frame kind

01:22.600 --> 01:26.600
of what we're doing compared to what has been done for this past decade.

01:27.160 --> 01:32.320
And I think it's easiest to understand what an analog chip is in contrast

01:32.320 --> 01:37.960
to the neural networks, the AI that have defined this last decade of deep learning.

01:38.560 --> 01:43.880
So in 2012, we kind of had a big event that started this new era, this AI

01:43.880 --> 01:50.640
Renaissance, and we are seeing these massive neural networks grow in size

01:50.640 --> 01:52.920
and grow in capabilities since then.

01:53.280 --> 01:57.440
But all of those neural networks that we've seen in this deep learning

01:57.440 --> 02:00.040
world are neural simulations.

02:00.440 --> 02:05.840
They are abstractions that are written in software and the neurons and synapses

02:05.840 --> 02:10.480
that are defined in that software don't physically exist, but rather they

02:10.480 --> 02:13.640
sit on this highest layer usually written in Python and that are then

02:13.640 --> 02:17.640
translated through many layers of abstraction down until it gets to the

02:17.640 --> 02:22.600
digital circuit, most of the time a GPU graphics processing unit where it

02:22.600 --> 02:26.440
then processes that math that represents that neural network at the top.

02:28.280 --> 02:31.520
An analog chip and a neuromorphic chip is different.

02:31.880 --> 02:36.240
It's not a neural simulation, but rather it's a neural circuit.

02:36.640 --> 02:41.360
It's a physical collection of neurons and synapses as opposed to an

02:41.360 --> 02:43.320
abstraction of neurons and synapses.

02:43.880 --> 02:46.440
And this is very similar to the brain.

02:46.800 --> 02:49.960
The brain is a collection of physical neurons and synapses.

02:50.000 --> 02:54.520
It's governed by the laws of physics and it achieves all that it does with

02:54.640 --> 03:00.640
extraordinary scale and extraordinary efficiency within the bounds of the

03:00.640 --> 03:01.520
physical world.

03:02.000 --> 03:06.440
So an analog chip, as we're building it, is trying to achieve something

03:06.440 --> 03:13.160
simple, trying to find ways to learn, find ways to scale all within the physical

03:13.160 --> 03:13.480
domain.

03:14.480 --> 03:16.920
So let's dig into that just a little deeper.

03:17.480 --> 03:19.560
Great, great overview.

03:20.120 --> 03:21.480
What does that mean?

03:22.040 --> 03:26.440
Typically a chip will think in terms of or operate in terms of on or off one or

03:26.440 --> 03:27.280
zero, right?

03:27.280 --> 03:28.840
Binary logic, right?

03:29.920 --> 03:33.880
What does an analog chip, how does that work?

03:33.880 --> 03:34.720
What's that look like?

03:35.720 --> 03:41.400
So digital chips are, as you said, built on the very bottom on zeros and

03:41.960 --> 03:45.480
ones on this Boolean logic of on or off.

03:45.920 --> 03:48.760
And all of the other logic is then constructed on top of that.

03:49.240 --> 03:53.360
When you zoom down to the bottom of an analog chip, you don't have zeros or

03:53.360 --> 03:59.800
ones, you have gradients of information, you have voltages and currents and

03:59.800 --> 04:03.920
resistances, you have physical quantities that you're measuring that

04:03.920 --> 04:07.160
represents the mathematical operations that you're performing.

04:07.600 --> 04:11.160
And you're exploiting the relationship between those physical quantities to

04:11.160 --> 04:15.200
then perform these very complex neural operations.

04:15.680 --> 04:19.440
So an example of this is like matrix vector multiplication.

04:19.440 --> 04:24.600
This is the backbone of most neural network math.

04:25.160 --> 04:31.040
And GPUs do this by parallelizing these multiplications across a lot of

04:31.040 --> 04:35.360
digital cores and doing precise digital multiplication in addition.

04:36.360 --> 04:41.200
In an analog chip, as we are building it, we're not doing this with highly

04:41.200 --> 04:45.760
precise digital math, but instead we have the activations of the neurons

04:45.760 --> 04:47.120
represented by voltages.

04:47.520 --> 04:52.920
We have the weights of the synapses represented by resistances, which are

04:52.920 --> 04:54.960
held in components called memristors.

04:55.480 --> 04:59.320
And when that voltage passes across that resistance, you have a natural

04:59.320 --> 05:03.440
relationship between voltage and resistance that's multiplicative to

05:03.600 --> 05:07.000
receive a current, you read out a current and that's your output.

05:07.440 --> 05:12.720
So an analog chip works by kind of first understanding these physical

05:12.720 --> 05:17.960
relationships between electrical quantities and exploiting those to do

05:17.960 --> 05:20.960
the math, to make the physics do the math for us.

05:23.000 --> 05:30.480
That sounds super interesting and it sounds at once very, very complex and

05:30.480 --> 05:33.760
at the same time kind of simple, right?

05:34.080 --> 05:39.480
And is that one of the key reasons why your chip is so much more energy efficient?

05:39.480 --> 05:42.760
I mean, you're claiming a thousand times more energy efficient.

05:43.440 --> 05:44.760
Yeah, exactly.

05:45.120 --> 05:47.880
That really is, I think, the most fundamental reason.

05:47.880 --> 05:52.800
And I think when you consider the comparison again to the neural simulation,

05:53.440 --> 05:57.920
the neural network in that simulation exists so many layers above and requires

05:57.920 --> 06:03.560
to be translated and then performed on a circuit that is really not that

06:03.560 --> 06:06.640
well optimized for neural math.

06:07.640 --> 06:11.400
And, but in our case, the circuit is the neural network.

06:11.760 --> 06:14.920
And because it is, it exists in that very bottom layer, it is on the

06:14.920 --> 06:20.400
substrate of the chip itself, you can achieve some extraordinary gains in

06:20.400 --> 06:24.960
both speed and improvement and power reduction, which of course gives you

06:24.960 --> 06:26.560
that energy efficiency gain.

06:27.280 --> 06:31.200
So, so yeah, it's because we're building the physical thing on that bottom layer.

06:31.880 --> 06:36.280
This episode of Tech First is sponsored by Smart One.

06:36.680 --> 06:41.400
Smart One is a smart vending machine platform that transforms traditional

06:41.400 --> 06:47.160
vending hardware into smarter, better, faster, automated retail kiosks, a

06:47.160 --> 06:49.200
convenient store without the store.

06:49.400 --> 06:53.320
Learn more at smrt1.ca.

06:53.920 --> 06:55.840
That I'm fascinated.

06:56.560 --> 07:02.720
And I'm wondering with Boolean logic, old school computer chips, you kind

07:02.720 --> 07:06.920
of have to simulate reality and go through those multiple layers of

07:06.920 --> 07:11.320
translation because you're not on bare metal, you know, as the, the old

07:11.320 --> 07:13.200
timers and computing would tell us, right?

07:13.680 --> 07:16.480
Go through various layers of translation before you're actually hitting

07:16.480 --> 07:19.280
computer, machine instruction, machine language.

07:19.760 --> 07:25.880
And you're kind of modeling reality or what you're computing in reality.

07:25.920 --> 07:27.920
Is, is that an accurate way of thinking about it?

07:27.960 --> 07:28.320
Yes.

07:28.760 --> 07:31.080
That's an exact way, a really great way to put it.

07:31.400 --> 07:35.480
You know, I sometimes use the metaphor, like what would be easier, you know,

07:35.480 --> 07:39.680
to, to assess your ability to kick a soccer ball on a field, right?

07:39.680 --> 07:45.520
Would you rather, you know, reconstruct all of the physics of your body and of

07:45.520 --> 07:52.040
the soccer field and of the ball and simulate that kick and, you know, observe

07:52.040 --> 07:55.400
that in the simulated world, or would you just rather walk out of the pitch

07:55.760 --> 07:56.480
and kick the ball?

07:57.040 --> 08:01.640
You know, it's for us, it feels very obvious that, you know, the most natural

08:01.640 --> 08:07.920
way to build a neural network, uh, to make it as efficient as the brain is to

08:07.920 --> 08:10.080
build it physically, just as the brain does.

08:12.200 --> 08:14.480
Are you trying to build an artificial brain?

08:15.640 --> 08:16.600
Yes, we are.

08:17.200 --> 08:18.520
That is our goal.

08:18.680 --> 08:22.600
You know, we have kind of two missions that are very complimentary.

08:22.640 --> 08:24.640
Uh, one of them is to build a brain.

08:24.960 --> 08:27.320
And the other one is to actually understand it.

08:27.680 --> 08:32.800
You know, we really love the, and believe the notion that you can't fully

08:32.800 --> 08:36.560
understand a system until you have, have reconstructed it and built it.

08:37.000 --> 08:39.800
Uh, you know, this go back to Feynman and many others.

08:40.280 --> 08:44.840
Uh, but what we believe we've developed really are kind of the core technologies

08:44.840 --> 08:49.160
that allow us to first build just kind of unit level chips that address, you

08:49.160 --> 08:50.440
know, near term problems.

08:51.000 --> 08:54.760
But ultimately we see this as kind of like Lego pieces that, you know,

08:54.840 --> 08:58.480
due to their low power footprints, we'll be able to concatenate them

08:58.480 --> 09:02.920
together using things like chiplet integration, advanced packaging, and

09:02.920 --> 09:08.840
ultimately scale out these systems to be brain scale, 86 billion neurons, 500

09:08.840 --> 09:14.440
trillion synapses, and low power enough that they can exist in autonomous devices.

09:15.000 --> 09:19.720
Because today, trading is so expensive that first week, we consider

09:19.720 --> 09:21.840
trading inferences, the separate problems.

09:21.840 --> 09:24.640
And I don't think they really should be considered as these separate

09:24.640 --> 09:29.360
and distinct problems, but it's so expensive that we can't even conceive

09:29.440 --> 09:32.840
of putting training of natural language in an autonomous machine.

09:33.440 --> 09:35.560
And yet humans do it all the time.

09:36.000 --> 09:40.160
So that is what we are trying to achieve to, to recapitulate the brain in

09:40.160 --> 09:44.800
hardware and ultimately give machines all of the capabilities that we

09:44.840 --> 09:45.960
recognize in ourselves.

09:46.600 --> 09:52.120
So Gordon, you're a pretty soft spoken guy and you sound like a very thoughtful

09:52.120 --> 10:00.360
guy. And in that very soft spoken way, you're saying absolutely ginormous things.

10:01.640 --> 10:04.160
We're talking Frankenstein level stuff, right?

10:04.160 --> 10:07.800
I mean, you understand the gravity of what you're talking about, right?

10:08.520 --> 10:09.320
Absolutely.

10:09.320 --> 10:14.720
No, this is the scope and impact of our work is not.

10:14.840 --> 10:18.280
I mean, this is something we've been working on for nearly five years and we

10:18.280 --> 10:22.040
recognize that if we are successful in achieving this, this, this will be

10:22.040 --> 10:24.480
historic and, and massively consequential.

10:25.040 --> 10:29.920
But, you know, what motivates us, you know, is of course the, the impact on

10:29.920 --> 10:35.920
positively improving human life, you know, and we can have personalized medicine,

10:35.920 --> 10:38.800
personalized education, we can automate all of labor.

10:39.480 --> 10:42.080
I mean, this is the world that we want to realize.

10:42.120 --> 10:45.000
I think many people are already in this consensus that artificial

10:45.000 --> 10:49.320
intelligence is going to be kind of the defining technology of the century.

10:49.640 --> 10:53.640
But most people don't, don't know that the hardware that supports it is,

10:53.640 --> 10:54.920
is the bottleneck right now.

10:55.640 --> 11:00.280
So, so yes, we recognize the scope of this and, you know, it's, it's a big

11:00.320 --> 11:01.720
mission and a big task.

11:01.760 --> 11:05.840
But, you know, I think that we're, we're approaching it the right way.

11:05.840 --> 11:10.400
And we're also very conscientious of the fact that we're, you know,

11:10.400 --> 11:16.320
conscientious of the fact that we don't want, that there, there are good ways

11:16.320 --> 11:18.440
of implementing AI and there are also not good ways.

11:19.240 --> 11:23.000
And we don't think AI should be used everywhere for every purpose, but there

11:23.000 --> 11:29.320
are guidelines and ethics that should really direct how we build and implement

11:29.480 --> 11:30.080
these systems.

11:31.160 --> 11:32.280
And it's controversial.

11:32.280 --> 11:35.240
It's also political in the AI space, right?

11:35.240 --> 11:39.320
In terms of what you're building, what you're looking at, people who claim

11:39.440 --> 11:44.160
to be working on general AI, for instance, you know, there's a lot of

11:44.160 --> 11:48.120
scrutiny there, there was a, I believe a Google engineer who last week said,

11:48.120 --> 11:51.560
I think that some of our systems are, are approaching consciousness.

11:51.960 --> 11:57.600
And, and there were a ton of people jumping all over him, probably correctly.

11:57.840 --> 12:03.520
But, you know, perhaps in a bit of a mob mentality for daring to suggest

12:03.520 --> 12:08.480
that and, and saying, no, we're, we're so far out there, maybe let's come back

12:08.480 --> 12:16.120
here, because you and your co-founder have studied the human brain significantly.

12:16.640 --> 12:20.120
What do we learn about AI from the way that our brains work?

12:21.200 --> 12:21.560
Absolutely.

12:21.560 --> 12:22.960
That's a great question, John.

12:23.320 --> 12:29.800
And I'd say that there are really kind of two categories of clues that we look

12:29.800 --> 12:33.480
at, you know, from the brain that then inform our hardware.

12:34.480 --> 12:38.920
The first is, how does the brain learn so efficiently?

12:39.200 --> 12:44.520
You know, it, the brain trains and learns with both very few examples.

12:45.520 --> 12:48.840
We, we learn with, with one example or two examples, one shot learning, two

12:48.840 --> 12:51.560
shot learning, and we can generalize extraordinarily well.

12:52.280 --> 12:55.560
So learning training happens very, very efficiently.

12:56.000 --> 13:01.000
And there are the learning rule of the brain, the algorithm the brain uses is

13:01.000 --> 13:06.560
not fully understood or identified, but we do know that there are certain

13:06.600 --> 13:09.480
requirements that, that algorithm must have.

13:10.320 --> 13:14.440
So one of those is called a local learning rule.

13:15.000 --> 13:19.120
So this, if you're, for the listeners who are familiar with back propagation,

13:19.120 --> 13:23.800
which is the industry standard algorithm for digital AI, this says what's

13:23.800 --> 13:25.160
called a global learning.

13:25.840 --> 13:27.640
So first let's say, well, what is a learning rule?

13:27.960 --> 13:33.280
A learning rule is how any given synapse in a neural network knows, should

13:33.280 --> 13:38.320
it become stronger or weaker for the whole system to become smarter, better

13:38.320 --> 13:39.720
at its assigned task.

13:40.280 --> 13:44.120
And in back propagation of the digital world, for you to calculate whether

13:44.120 --> 13:47.600
this one synapse should be stronger or weaker, you need to do a math equation

13:47.600 --> 13:49.640
that incorporates the entire network.

13:49.960 --> 13:52.280
You need to differentiate against the entire system.

13:52.800 --> 13:57.440
That's really mathematically expensive, but not only is it expensive, it's

13:57.440 --> 14:00.920
also impossible for the brain to be doing the same thing.

14:01.200 --> 14:05.120
There's no like agent sitting in our brain observing the whole system and

14:05.120 --> 14:07.680
then doing a math problem to update every synapse.

14:07.680 --> 14:08.640
It's, it's impossible.

14:08.960 --> 14:12.400
The brain has to be operating with a local learning rule.

14:13.240 --> 14:17.600
And so a local learning rule is so that that synapse can just observe what's

14:17.600 --> 14:20.240
right nearby and still know I become stronger.

14:21.280 --> 14:23.840
So that's one of the clues in the algorithm side.

14:23.840 --> 14:28.200
And that's one of the pieces that makes our learning algorithm so special,

14:28.200 --> 14:33.920
that it is as smart as back propagation, but it does so with a local learning rule.

14:35.320 --> 14:36.560
Well, you burst my bubble there.

14:36.560 --> 14:39.320
And there's no homunculus, I believe you pronounce it.

14:39.320 --> 14:42.880
I mean, there's no little guy in a little control room in my brain.

14:44.640 --> 14:45.400
Unfortunate.

14:46.440 --> 14:46.800
Yeah.

14:46.960 --> 14:49.080
And, and there's another side of the clue.

14:49.080 --> 14:52.800
So I'd say the first is on like how it learns the learning algorithm.

14:52.880 --> 14:54.920
The other is about scale.

14:55.080 --> 14:58.160
How does the brain achieve its massive scale?

14:58.200 --> 15:02.280
86 million neurons, 500 trillion synapses, and still process and move

15:02.280 --> 15:04.440
information so quickly, so efficiently.

15:05.840 --> 15:10.240
So, and in that case, we take clues from like the topology of the brain.

15:10.520 --> 15:13.920
And in fact, and the specific thing is called sparsity.

15:14.480 --> 15:19.640
The idea that you don't have to connect every neuron to every other neuron in

15:19.640 --> 15:21.840
the system for it to be well connected.

15:22.440 --> 15:27.960
Now, again, to contrast to the status quo, digital ALI in deep learning is,

15:28.600 --> 15:32.600
was primarily built on what are called fully connected layers in neural networks

15:32.600 --> 15:35.720
where you have a layer of neurons and another layer of neurons and you

15:35.720 --> 15:39.440
connect every neuron here to every neuron in the next layer.

15:40.000 --> 15:40.360
Whoa.

15:40.360 --> 15:45.600
This was, that was the natural way to do it on GPUs because GPUs do these

15:45.600 --> 15:49.120
dense matrix multiplications, which correspond to this fully connected,

15:49.160 --> 15:50.360
densely connected systems.

15:51.360 --> 15:53.800
Now, our brain is very different.

15:54.040 --> 15:57.960
Our brain of all the possible connections that could exist between

15:57.960 --> 16:03.120
neurons, it's something like just a fraction or one percent of those

16:03.120 --> 16:04.000
connections exist.

16:04.520 --> 16:10.040
And yet at the same time, the path for information to travel from one neuron

16:10.040 --> 16:11.880
to any other neuron is very short.

16:12.200 --> 16:16.440
The average path length is about four jumps for information to traverse

16:16.440 --> 16:17.680
anywhere in the brain.

16:17.880 --> 16:19.640
It's extremely well connected.

16:20.200 --> 16:23.800
So there are these special patterns of connectivity.

16:24.040 --> 16:26.240
One of them is called a small world network.

16:26.720 --> 16:30.360
And if you've ever heard of a small world network, it's a network pattern

16:30.360 --> 16:34.040
that also mirrors human social networks that gives rise to the six

16:34.040 --> 16:36.800
degrees of separation property and human connections.

16:37.560 --> 16:40.000
And the idea is you can have lots of local connections.

16:40.040 --> 16:41.520
You want to be connected to your neighbors.

16:41.520 --> 16:43.320
You're likely to be connected to your neighbors.

16:43.320 --> 16:45.200
And that's a very short path to bridge.

16:45.720 --> 16:48.880
And then you want some long distance connections.

16:49.520 --> 16:55.400
And you can create these very well connected networks at very large scale

16:55.760 --> 16:59.320
when you implement these intelligent forms of sparsity.

16:59.760 --> 17:04.920
So sparsity is core to our scaling architecture.

17:05.360 --> 17:07.680
And really, and just to kind of summarize there.

17:07.760 --> 17:10.800
So we have these are the two core technologies we've developed.

17:11.000 --> 17:14.880
A learning algorithm that learns of the local learning rule and a scaling

17:14.880 --> 17:19.520
architecture that scales to massive sizes of neural networks using intelligent sparsity.

17:21.200 --> 17:23.120
I knew Kevin Bacon would come up some point.

17:23.200 --> 17:25.320
I mean, inevitable.

17:26.880 --> 17:32.280
Last year, I believe you sort of taped a functional chip together.

17:32.280 --> 17:33.640
Your first prototype.

17:34.000 --> 17:37.240
Where are you on the journey to shipping a fully functional chip?

17:38.680 --> 17:43.760
So we have, I'd say in the last four years, you know, we have done

17:44.240 --> 17:50.440
such expansive work that has been mostly, I would say, qualified as a research.

17:50.840 --> 17:52.640
We've been exploring different algorithms.

17:52.640 --> 17:54.480
We've been exploring different architectures.

17:55.280 --> 17:59.240
Originally, we have this concept using random nanowire meshes.

17:59.800 --> 18:03.040
Turns out it's not very manufacturable and better to build things

18:03.040 --> 18:05.480
that you can manufacture today easily.

18:05.480 --> 18:10.240
But in this last year, we kind of crystallized kind of our two core technologies.

18:10.240 --> 18:13.000
Like what is the learning algorithm and what is that scaling architecture?

18:13.280 --> 18:15.520
And then developed hardware prototypes of each.

18:16.240 --> 18:20.800
We still have a good amount of time to engineer this completely and to get to market.

18:20.800 --> 18:26.600
We we hope to get to market, you know, on the order of full scale shipping 2025.

18:28.040 --> 18:33.480
But that said, you know, building the world's first analog neural network is not easy.

18:34.760 --> 18:38.960
You know, to iterate through this and get it fully fully up at scale.

18:39.920 --> 18:41.280
And you just got some help there.

18:41.280 --> 18:42.200
You raised some funds.

18:42.800 --> 18:43.520
Yeah. Yeah.

18:43.520 --> 18:47.880
Yeah. So we just closed a 25 million series A, which is thrilling.

18:48.360 --> 18:51.920
You know, we had used eight million dollars for the last four years

18:51.920 --> 18:56.000
and honestly had gone through some challenging times where, you know,

18:56.680 --> 18:58.600
we got really close.

18:58.600 --> 19:01.680
You know, one of our first tape out prototypes didn't function

19:01.680 --> 19:02.720
precisely as designed.

19:02.720 --> 19:04.120
It was a very silly error.

19:04.120 --> 19:08.640
But in ships, it takes a long time to iterate and even and resolve.

19:08.640 --> 19:10.640
You can't just debug like software.

19:10.680 --> 19:12.080
So we went through a lot.

19:12.080 --> 19:13.000
We learned a lot.

19:13.000 --> 19:16.880
And now we have 25 million in the bank and we're hiring like crazy.

19:18.000 --> 19:20.720
Wonderful. It goes quickly, I know, from personal experience.

19:23.200 --> 19:26.640
So talk about what this will enable.

19:27.560 --> 19:29.320
You're shipping this ship.

19:29.320 --> 19:31.440
What will it enable?

19:31.440 --> 19:38.760
Yeah. So we want to not enable just like an incremental improvement in AI.

19:38.920 --> 19:43.680
You know, I think that there are a lot of folks and video included

19:43.680 --> 19:48.600
on the digital hardware roadmap and because digital hardware is so mature,

19:49.000 --> 19:52.480
it's just it's kind of incremental gains that we're getting at this point.

19:52.480 --> 19:56.080
And I think there's still more improvement to be geeked out on that roadmap.

19:56.600 --> 20:02.120
But we're trying to enable a new roadmap that is really a step change

20:02.200 --> 20:04.040
in performance improvement.

20:04.040 --> 20:08.600
And so we want to enter the market really at a thousand X energy

20:08.600 --> 20:11.280
efficiency improvement over status quo hardware.

20:11.560 --> 20:15.320
And at a thousand X comes from about a 10 X reduction in power

20:15.720 --> 20:18.240
alongside a 100 X improvement in speed.

20:19.120 --> 20:24.400
And when you can do that and importantly, not just for inference,

20:24.400 --> 20:28.640
we're talking about training and inference in the same platform.

20:29.040 --> 20:34.480
It unlocks possibilities that have just been inconceivable until until now.

20:35.280 --> 20:39.200
For one, you know, currently people consider training and inference

20:39.200 --> 20:43.120
as these kind of separate problems that we need separate platforms of hardware with.

20:43.600 --> 20:46.880
You know, we train up in a GPU cloud system,

20:46.880 --> 20:50.000
and then we might upload those weights onto a more efficient chip

20:50.000 --> 20:52.600
and deploy it out into the world.

20:52.600 --> 20:56.920
I think that would be the first step for kind of low power inference in devices.

20:56.920 --> 21:00.800
But we don't want devices just to be pre-programmed

21:00.800 --> 21:02.840
and just do what they do in the world.

21:02.840 --> 21:05.600
We want devices to learn on their own.

21:05.600 --> 21:07.920
We want devices to have an adaptive brain

21:07.920 --> 21:12.440
that's continuously learning from a changing environment and from a changing self.

21:12.880 --> 21:14.400
So imagine a robot, right?

21:14.400 --> 21:16.560
We have we're we're eventually in our lifetime.

21:16.560 --> 21:19.000
We're going to have robots for everything, you know,

21:19.000 --> 21:21.560
but maybe it's a construction robot that's helping, you know,

21:21.560 --> 21:23.720
repair our streets or build our homes.

21:23.720 --> 21:31.080
The joints on that robot are going to erode and face damage in unique ways.

21:31.440 --> 21:35.560
And it needs to learn how to adjust its own movement

21:35.560 --> 21:40.400
so it can maintain its performance based on its own kind of evolution

21:40.400 --> 21:42.760
and transition in its physical self.

21:42.760 --> 21:46.000
Also, the robots might be adjusting to new environments.

21:46.400 --> 21:50.880
And we'd like that ability to be baked in so they can continuously and adaptively learn.

21:51.680 --> 21:54.880
This also really enables personalization.

21:54.880 --> 22:00.320
It enables machines to get to know us and for us to have the assurance

22:00.360 --> 22:03.920
that that knowledge and data of ourselves is in that robot

22:03.920 --> 22:07.800
and doesn't believe that robot, I think is something we will will be very assuring.

22:08.640 --> 22:11.720
But but that's a huge piece, you know, training and inference

22:11.720 --> 22:15.480
in the same platform that is untethered.

22:16.800 --> 22:18.760
What's really interesting about that?

22:18.760 --> 22:20.320
Oh, there's a number of things, obviously.

22:20.320 --> 22:23.800
But I mean, I'm just thinking of a limping robot.

22:23.800 --> 22:27.240
For instance, you know, we limp when we injure a joint

22:27.280 --> 22:34.280
and that is our adaptation to the limitation in a knee or a hip or something like that.

22:34.280 --> 22:35.480
And we function with that.

22:35.480 --> 22:40.240
And, you know, maybe we'll repair the robot, but maybe the robot is inaccessible

22:40.240 --> 22:45.080
or maybe the robot is on Mars or Pluto or who knows where or maybe it's too expensive.

22:45.080 --> 22:48.120
So limping and getting, you know, that's interesting.

22:48.400 --> 22:51.160
The other thing that you mentioned that's super interesting is

22:52.040 --> 22:54.600
we want AI to make our lives better

22:54.800 --> 22:57.440
and we want robots to make our lives better.

22:57.560 --> 23:02.160
But that doesn't mean that we want Amazon to know our deepest thoughts.

23:02.280 --> 23:06.680
That doesn't mean that we want Google to know everything about our personal finances.

23:07.040 --> 23:12.240
You know, if we can have AI that is personal, that, you know,

23:12.240 --> 23:14.760
sure, it comes from somewhere and we've purchased it.

23:14.760 --> 23:18.960
Or, you know, if you start to get into it, if you start to look at general AI,

23:18.960 --> 23:21.000
you start thinking, do you purchase that?

23:21.000 --> 23:22.200
Do you recruit that?

23:22.200 --> 23:24.120
Do you adopt that?

23:24.120 --> 23:27.760
Lots of questions there, but it's nice if you can get a system

23:28.560 --> 23:33.280
that learns you, understands you and it stays in some sort of privacy corridor there.

23:33.960 --> 23:36.280
Really, really interesting stuff.

23:37.720 --> 23:41.680
How it's so hard to predict the future.

23:42.160 --> 23:44.360
You mentioned about speed.

23:44.360 --> 23:45.680
That's always a moving target, right?

23:45.680 --> 23:48.400
You want a hundred X speed, but you see what speed is right now.

23:48.920 --> 23:52.720
In terms of adding speed right now, we seem to not be making chips

23:52.720 --> 23:54.360
much faster per se.

23:54.360 --> 24:00.560
We're adding more chipsets on a die or we're we're creating chips

24:00.560 --> 24:04.680
that are more optimized for this task or for that task or for energy efficiency

24:04.680 --> 24:08.880
or for whatever, and that's how we're making overall devices faster.

24:09.200 --> 24:12.480
So are you are you keeping that sort of moving target in mind

24:12.480 --> 24:16.240
as you look at the performance levels that you want to hit?

24:17.000 --> 24:20.640
So we believe we can achieve that one hundred X

24:20.680 --> 24:25.480
and proven speed and beyond because we're taking a different tack entirely

24:25.480 --> 24:28.760
because we're not on that roadmap of digital hardware

24:29.080 --> 24:33.240
that is very mature chips and they're eking out performance

24:33.240 --> 24:37.680
from any number of, you know, kind of well trod playbooks.

24:38.360 --> 24:40.920
In the case of a fully analog neuromorphic chip,

24:41.440 --> 24:44.520
you know, you have a neural network where you have analog neurons

24:44.520 --> 24:46.480
and synapses that are connected in sequence.

24:46.960 --> 24:52.600
And, you know, you can compare this to to other analog mixed signal chips.

24:52.600 --> 24:56.120
So people are starting to get a lot of speed improvements

24:56.120 --> 24:59.440
by moving away from digital and doing, say, the synaptic operations

24:59.440 --> 25:02.960
either with photonic components or flash components.

25:03.560 --> 25:07.560
But in all of those cases, they still actually have to translate

25:07.560 --> 25:12.200
between their kind of physical, their either physics or optical

25:12.200 --> 25:14.880
to digital for their neurons.

25:14.920 --> 25:17.800
And so they have these analog synapses, digital neurons,

25:17.800 --> 25:21.720
they kind of go back and forth and that requires clocking

25:21.720 --> 25:24.160
that requires slowing down the system.

25:24.160 --> 25:27.720
So in a fully analog chip, when it when it's designed,

25:27.800 --> 25:32.480
well, you can have input to output and have the signal flow

25:32.480 --> 25:35.280
at wire speed from end to end.

25:35.760 --> 25:38.000
That is the full potential of an analog chip.

25:38.000 --> 25:41.920
And that's why the speed here is so extraordinary

25:42.520 --> 25:45.480
because we're no longer working in this digitally clocked world.

25:45.680 --> 25:48.480
But we're again, exploiting the physics of the system,

25:48.480 --> 25:50.760
the physical nature of the system to do that math for us.

25:51.120 --> 25:55.320
And you can perform inference as just a wave of electricity from input out.

25:56.800 --> 25:57.960
Amazing.

25:57.960 --> 26:01.040
I know that Tesla, for instance, is doing

26:01.600 --> 26:05.560
we can debate whether the term full is is an appropriate modifier there,

26:05.560 --> 26:08.320
but full self driving on atom chips, right?

26:08.600 --> 26:11.160
Not saying that all the training and learning is happening there,

26:11.160 --> 26:14.160
but that's what's actively involved in the vehicle.

26:14.400 --> 26:18.640
So just imagining what this could do for self driving,

26:18.640 --> 26:21.560
for automated machinery, for robotics, for AI.

26:22.000 --> 26:23.400
It's kind of mind blowing.

26:23.400 --> 26:25.360
Gordon, thank you for spending this time with us.

26:25.360 --> 26:27.320
I really do appreciate it.

26:27.320 --> 26:30.320
Thank you so much for the time, John. It was a pleasure.

