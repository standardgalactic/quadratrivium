start	end	text
0	3680	Ultimately, we see this as kind of like Lego pieces that, you know,
3720	8000	due to their low power footprints, we'll be able to concatenate them together
8000	12480	using things like chip lead integration, advanced packaging, and ultimately
12520	17240	scale out these systems to be brain scale, 86 billion neurons,
17320	23320	500 trillion synapses, and low power enough that they can exist in autonomous devices.
23880	28080	Can we build a computer chip that operates like the brain?
28640	32960	We've seen neuromorphic computer chips from companies like Intel before,
32960	37080	like the Luigi chip, which Intel claimed had a sense of smell.
37600	42240	Rain neuromorphics, however, says it has an end-to-end analog chip,
42520	47280	a neuromorphic processing unit that they say is the world's first end-to-end
47280	49880	analog trainable AI circuit.
50200	55480	It's a fully analog neural network, and it's a thousand times more energy
55480	58280	efficient than today's best processors.
58560	61800	Here to chat is Rain neuromorphic CEO, Gordon Wilson.
62080	62800	Welcome, Gordon.
63880	65040	Thanks so much for having me, John.
65080	65760	It's good to be here.
66840	67840	It's good to have you.
68080	71360	When I see neuromorphics in the inbox, I got to check it out.
71360	72560	It's a cool space.
73320	74240	Let's start here.
74280	75880	What the heck is an analog chip?
77160	78320	Yeah, great question.
78320	82600	And I think a great kicking off point because it really allows us to frame kind
82600	86600	of what we're doing compared to what has been done for this past decade.
87160	92320	And I think it's easiest to understand what an analog chip is in contrast
92320	97960	to the neural networks, the AI that have defined this last decade of deep learning.
98560	103880	So in 2012, we kind of had a big event that started this new era, this AI
103880	110640	Renaissance, and we are seeing these massive neural networks grow in size
110640	112920	and grow in capabilities since then.
113280	117440	But all of those neural networks that we've seen in this deep learning
117440	120040	world are neural simulations.
120440	125840	They are abstractions that are written in software and the neurons and synapses
125840	130480	that are defined in that software don't physically exist, but rather they
130480	133640	sit on this highest layer usually written in Python and that are then
133640	137640	translated through many layers of abstraction down until it gets to the
137640	142600	digital circuit, most of the time a GPU graphics processing unit where it
142600	146440	then processes that math that represents that neural network at the top.
148280	151520	An analog chip and a neuromorphic chip is different.
151880	156240	It's not a neural simulation, but rather it's a neural circuit.
156640	161360	It's a physical collection of neurons and synapses as opposed to an
161360	163320	abstraction of neurons and synapses.
163880	166440	And this is very similar to the brain.
166800	169960	The brain is a collection of physical neurons and synapses.
170000	174520	It's governed by the laws of physics and it achieves all that it does with
174640	180640	extraordinary scale and extraordinary efficiency within the bounds of the
180640	181520	physical world.
182000	186440	So an analog chip, as we're building it, is trying to achieve something
186440	193160	simple, trying to find ways to learn, find ways to scale all within the physical
193160	193480	domain.
194480	196920	So let's dig into that just a little deeper.
197480	199560	Great, great overview.
200120	201480	What does that mean?
202040	206440	Typically a chip will think in terms of or operate in terms of on or off one or
206440	207280	zero, right?
207280	208840	Binary logic, right?
209920	213880	What does an analog chip, how does that work?
213880	214720	What's that look like?
215720	221400	So digital chips are, as you said, built on the very bottom on zeros and
221960	225480	ones on this Boolean logic of on or off.
225920	228760	And all of the other logic is then constructed on top of that.
229240	233360	When you zoom down to the bottom of an analog chip, you don't have zeros or
233360	239800	ones, you have gradients of information, you have voltages and currents and
239800	243920	resistances, you have physical quantities that you're measuring that
243920	247160	represents the mathematical operations that you're performing.
247600	251160	And you're exploiting the relationship between those physical quantities to
251160	255200	then perform these very complex neural operations.
255680	259440	So an example of this is like matrix vector multiplication.
259440	264600	This is the backbone of most neural network math.
265160	271040	And GPUs do this by parallelizing these multiplications across a lot of
271040	275360	digital cores and doing precise digital multiplication in addition.
276360	281200	In an analog chip, as we are building it, we're not doing this with highly
281200	285760	precise digital math, but instead we have the activations of the neurons
285760	287120	represented by voltages.
287520	292920	We have the weights of the synapses represented by resistances, which are
292920	294960	held in components called memristors.
295480	299320	And when that voltage passes across that resistance, you have a natural
299320	303440	relationship between voltage and resistance that's multiplicative to
303600	307000	receive a current, you read out a current and that's your output.
307440	312720	So an analog chip works by kind of first understanding these physical
312720	317960	relationships between electrical quantities and exploiting those to do
317960	320960	the math, to make the physics do the math for us.
323000	330480	That sounds super interesting and it sounds at once very, very complex and
330480	333760	at the same time kind of simple, right?
334080	339480	And is that one of the key reasons why your chip is so much more energy efficient?
339480	342760	I mean, you're claiming a thousand times more energy efficient.
343440	344760	Yeah, exactly.
345120	347880	That really is, I think, the most fundamental reason.
347880	352800	And I think when you consider the comparison again to the neural simulation,
353440	357920	the neural network in that simulation exists so many layers above and requires
357920	363560	to be translated and then performed on a circuit that is really not that
363560	366640	well optimized for neural math.
367640	371400	And, but in our case, the circuit is the neural network.
371760	374920	And because it is, it exists in that very bottom layer, it is on the
374920	380400	substrate of the chip itself, you can achieve some extraordinary gains in
380400	384960	both speed and improvement and power reduction, which of course gives you
384960	386560	that energy efficiency gain.
387280	391200	So, so yeah, it's because we're building the physical thing on that bottom layer.
391880	396280	This episode of Tech First is sponsored by Smart One.
396680	401400	Smart One is a smart vending machine platform that transforms traditional
401400	407160	vending hardware into smarter, better, faster, automated retail kiosks, a
407160	409200	convenient store without the store.
409400	413320	Learn more at smrt1.ca.
413920	415840	That I'm fascinated.
416560	422720	And I'm wondering with Boolean logic, old school computer chips, you kind
422720	426920	of have to simulate reality and go through those multiple layers of
426920	431320	translation because you're not on bare metal, you know, as the, the old
431320	433200	timers and computing would tell us, right?
433680	436480	Go through various layers of translation before you're actually hitting
436480	439280	computer, machine instruction, machine language.
439760	445880	And you're kind of modeling reality or what you're computing in reality.
445920	447920	Is, is that an accurate way of thinking about it?
447960	448320	Yes.
448760	451080	That's an exact way, a really great way to put it.
451400	455480	You know, I sometimes use the metaphor, like what would be easier, you know,
455480	459680	to, to assess your ability to kick a soccer ball on a field, right?
459680	465520	Would you rather, you know, reconstruct all of the physics of your body and of
465520	472040	the soccer field and of the ball and simulate that kick and, you know, observe
472040	475400	that in the simulated world, or would you just rather walk out of the pitch
475760	476480	and kick the ball?
477040	481640	You know, it's for us, it feels very obvious that, you know, the most natural
481640	487920	way to build a neural network, uh, to make it as efficient as the brain is to
487920	490080	build it physically, just as the brain does.
492200	494480	Are you trying to build an artificial brain?
495640	496600	Yes, we are.
497200	498520	That is our goal.
498680	502600	You know, we have kind of two missions that are very complimentary.
502640	504640	Uh, one of them is to build a brain.
504960	507320	And the other one is to actually understand it.
507680	512800	You know, we really love the, and believe the notion that you can't fully
512800	516560	understand a system until you have, have reconstructed it and built it.
517000	519800	Uh, you know, this go back to Feynman and many others.
520280	524840	Uh, but what we believe we've developed really are kind of the core technologies
524840	529160	that allow us to first build just kind of unit level chips that address, you
529160	530440	know, near term problems.
531000	534760	But ultimately we see this as kind of like Lego pieces that, you know,
534840	538480	due to their low power footprints, we'll be able to concatenate them
538480	542920	together using things like chiplet integration, advanced packaging, and
542920	548840	ultimately scale out these systems to be brain scale, 86 billion neurons, 500
548840	554440	trillion synapses, and low power enough that they can exist in autonomous devices.
555000	559720	Because today, trading is so expensive that first week, we consider
559720	561840	trading inferences, the separate problems.
561840	564640	And I don't think they really should be considered as these separate
564640	569360	and distinct problems, but it's so expensive that we can't even conceive
569440	572840	of putting training of natural language in an autonomous machine.
573440	575560	And yet humans do it all the time.
576000	580160	So that is what we are trying to achieve to, to recapitulate the brain in
580160	584800	hardware and ultimately give machines all of the capabilities that we
584840	585960	recognize in ourselves.
586600	592120	So Gordon, you're a pretty soft spoken guy and you sound like a very thoughtful
592120	600360	guy. And in that very soft spoken way, you're saying absolutely ginormous things.
601640	604160	We're talking Frankenstein level stuff, right?
604160	607800	I mean, you understand the gravity of what you're talking about, right?
608520	609320	Absolutely.
609320	614720	No, this is the scope and impact of our work is not.
614840	618280	I mean, this is something we've been working on for nearly five years and we
618280	622040	recognize that if we are successful in achieving this, this, this will be
622040	624480	historic and, and massively consequential.
625040	629920	But, you know, what motivates us, you know, is of course the, the impact on
629920	635920	positively improving human life, you know, and we can have personalized medicine,
635920	638800	personalized education, we can automate all of labor.
639480	642080	I mean, this is the world that we want to realize.
642120	645000	I think many people are already in this consensus that artificial
645000	649320	intelligence is going to be kind of the defining technology of the century.
649640	653640	But most people don't, don't know that the hardware that supports it is,
653640	654920	is the bottleneck right now.
655640	660280	So, so yes, we recognize the scope of this and, you know, it's, it's a big
660320	661720	mission and a big task.
661760	665840	But, you know, I think that we're, we're approaching it the right way.
665840	670400	And we're also very conscientious of the fact that we're, you know,
670400	676320	conscientious of the fact that we don't want, that there, there are good ways
676320	678440	of implementing AI and there are also not good ways.
679240	683000	And we don't think AI should be used everywhere for every purpose, but there
683000	689320	are guidelines and ethics that should really direct how we build and implement
689480	690080	these systems.
691160	692280	And it's controversial.
692280	695240	It's also political in the AI space, right?
695240	699320	In terms of what you're building, what you're looking at, people who claim
699440	704160	to be working on general AI, for instance, you know, there's a lot of
704160	708120	scrutiny there, there was a, I believe a Google engineer who last week said,
708120	711560	I think that some of our systems are, are approaching consciousness.
711960	717600	And, and there were a ton of people jumping all over him, probably correctly.
717840	723520	But, you know, perhaps in a bit of a mob mentality for daring to suggest
723520	728480	that and, and saying, no, we're, we're so far out there, maybe let's come back
728480	736120	here, because you and your co-founder have studied the human brain significantly.
736640	740120	What do we learn about AI from the way that our brains work?
741200	741560	Absolutely.
741560	742960	That's a great question, John.
743320	749800	And I'd say that there are really kind of two categories of clues that we look
749800	753480	at, you know, from the brain that then inform our hardware.
754480	758920	The first is, how does the brain learn so efficiently?
759200	764520	You know, it, the brain trains and learns with both very few examples.
765520	768840	We, we learn with, with one example or two examples, one shot learning, two
768840	771560	shot learning, and we can generalize extraordinarily well.
772280	775560	So learning training happens very, very efficiently.
776000	781000	And there are the learning rule of the brain, the algorithm the brain uses is
781000	786560	not fully understood or identified, but we do know that there are certain
786600	789480	requirements that, that algorithm must have.
790320	794440	So one of those is called a local learning rule.
795000	799120	So this, if you're, for the listeners who are familiar with back propagation,
799120	803800	which is the industry standard algorithm for digital AI, this says what's
803800	805160	called a global learning.
805840	807640	So first let's say, well, what is a learning rule?
807960	813280	A learning rule is how any given synapse in a neural network knows, should
813280	818320	it become stronger or weaker for the whole system to become smarter, better
818320	819720	at its assigned task.
820280	824120	And in back propagation of the digital world, for you to calculate whether
824120	827600	this one synapse should be stronger or weaker, you need to do a math equation
827600	829640	that incorporates the entire network.
829960	832280	You need to differentiate against the entire system.
832800	837440	That's really mathematically expensive, but not only is it expensive, it's
837440	840920	also impossible for the brain to be doing the same thing.
841200	845120	There's no like agent sitting in our brain observing the whole system and
845120	847680	then doing a math problem to update every synapse.
847680	848640	It's, it's impossible.
848960	852400	The brain has to be operating with a local learning rule.
853240	857600	And so a local learning rule is so that that synapse can just observe what's
857600	860240	right nearby and still know I become stronger.
861280	863840	So that's one of the clues in the algorithm side.
863840	868200	And that's one of the pieces that makes our learning algorithm so special,
868200	873920	that it is as smart as back propagation, but it does so with a local learning rule.
875320	876560	Well, you burst my bubble there.
876560	879320	And there's no homunculus, I believe you pronounce it.
879320	882880	I mean, there's no little guy in a little control room in my brain.
884640	885400	Unfortunate.
886440	886800	Yeah.
886960	889080	And, and there's another side of the clue.
889080	892800	So I'd say the first is on like how it learns the learning algorithm.
892880	894920	The other is about scale.
895080	898160	How does the brain achieve its massive scale?
898200	902280	86 million neurons, 500 trillion synapses, and still process and move
902280	904440	information so quickly, so efficiently.
905840	910240	So, and in that case, we take clues from like the topology of the brain.
910520	913920	And in fact, and the specific thing is called sparsity.
914480	919640	The idea that you don't have to connect every neuron to every other neuron in
919640	921840	the system for it to be well connected.
922440	927960	Now, again, to contrast to the status quo, digital ALI in deep learning is,
928600	932600	was primarily built on what are called fully connected layers in neural networks
932600	935720	where you have a layer of neurons and another layer of neurons and you
935720	939440	connect every neuron here to every neuron in the next layer.
940000	940360	Whoa.
940360	945600	This was, that was the natural way to do it on GPUs because GPUs do these
945600	949120	dense matrix multiplications, which correspond to this fully connected,
949160	950360	densely connected systems.
951360	953800	Now, our brain is very different.
954040	957960	Our brain of all the possible connections that could exist between
957960	963120	neurons, it's something like just a fraction or one percent of those
963120	964000	connections exist.
964520	970040	And yet at the same time, the path for information to travel from one neuron
970040	971880	to any other neuron is very short.
972200	976440	The average path length is about four jumps for information to traverse
976440	977680	anywhere in the brain.
977880	979640	It's extremely well connected.
980200	983800	So there are these special patterns of connectivity.
984040	986240	One of them is called a small world network.
986720	990360	And if you've ever heard of a small world network, it's a network pattern
990360	994040	that also mirrors human social networks that gives rise to the six
994040	996800	degrees of separation property and human connections.
997560	1000000	And the idea is you can have lots of local connections.
1000040	1001520	You want to be connected to your neighbors.
1001520	1003320	You're likely to be connected to your neighbors.
1003320	1005200	And that's a very short path to bridge.
1005720	1008880	And then you want some long distance connections.
1009520	1015400	And you can create these very well connected networks at very large scale
1015760	1019320	when you implement these intelligent forms of sparsity.
1019760	1024920	So sparsity is core to our scaling architecture.
1025360	1027680	And really, and just to kind of summarize there.
1027760	1030800	So we have these are the two core technologies we've developed.
1031000	1034880	A learning algorithm that learns of the local learning rule and a scaling
1034880	1039520	architecture that scales to massive sizes of neural networks using intelligent sparsity.
1041200	1043120	I knew Kevin Bacon would come up some point.
1043200	1045320	I mean, inevitable.
1046880	1052280	Last year, I believe you sort of taped a functional chip together.
1052280	1053640	Your first prototype.
1054000	1057240	Where are you on the journey to shipping a fully functional chip?
1058680	1063760	So we have, I'd say in the last four years, you know, we have done
1064240	1070440	such expansive work that has been mostly, I would say, qualified as a research.
1070840	1072640	We've been exploring different algorithms.
1072640	1074480	We've been exploring different architectures.
1075280	1079240	Originally, we have this concept using random nanowire meshes.
1079800	1083040	Turns out it's not very manufacturable and better to build things
1083040	1085480	that you can manufacture today easily.
1085480	1090240	But in this last year, we kind of crystallized kind of our two core technologies.
1090240	1093000	Like what is the learning algorithm and what is that scaling architecture?
1093280	1095520	And then developed hardware prototypes of each.
1096240	1100800	We still have a good amount of time to engineer this completely and to get to market.
1100800	1106600	We we hope to get to market, you know, on the order of full scale shipping 2025.
1108040	1113480	But that said, you know, building the world's first analog neural network is not easy.
1114760	1118960	You know, to iterate through this and get it fully fully up at scale.
1119920	1121280	And you just got some help there.
1121280	1122200	You raised some funds.
1122800	1123520	Yeah. Yeah.
1123520	1127880	Yeah. So we just closed a 25 million series A, which is thrilling.
1128360	1131920	You know, we had used eight million dollars for the last four years
1131920	1136000	and honestly had gone through some challenging times where, you know,
1136680	1138600	we got really close.
1138600	1141680	You know, one of our first tape out prototypes didn't function
1141680	1142720	precisely as designed.
1142720	1144120	It was a very silly error.
1144120	1148640	But in ships, it takes a long time to iterate and even and resolve.
1148640	1150640	You can't just debug like software.
1150680	1152080	So we went through a lot.
1152080	1153000	We learned a lot.
1153000	1156880	And now we have 25 million in the bank and we're hiring like crazy.
1158000	1160720	Wonderful. It goes quickly, I know, from personal experience.
1163200	1166640	So talk about what this will enable.
1167560	1169320	You're shipping this ship.
1169320	1171440	What will it enable?
1171440	1178760	Yeah. So we want to not enable just like an incremental improvement in AI.
1178920	1183680	You know, I think that there are a lot of folks and video included
1183680	1188600	on the digital hardware roadmap and because digital hardware is so mature,
1189000	1192480	it's just it's kind of incremental gains that we're getting at this point.
1192480	1196080	And I think there's still more improvement to be geeked out on that roadmap.
1196600	1202120	But we're trying to enable a new roadmap that is really a step change
1202200	1204040	in performance improvement.
1204040	1208600	And so we want to enter the market really at a thousand X energy
1208600	1211280	efficiency improvement over status quo hardware.
1211560	1215320	And at a thousand X comes from about a 10 X reduction in power
1215720	1218240	alongside a 100 X improvement in speed.
1219120	1224400	And when you can do that and importantly, not just for inference,
1224400	1228640	we're talking about training and inference in the same platform.
1229040	1234480	It unlocks possibilities that have just been inconceivable until until now.
1235280	1239200	For one, you know, currently people consider training and inference
1239200	1243120	as these kind of separate problems that we need separate platforms of hardware with.
1243600	1246880	You know, we train up in a GPU cloud system,
1246880	1250000	and then we might upload those weights onto a more efficient chip
1250000	1252600	and deploy it out into the world.
1252600	1256920	I think that would be the first step for kind of low power inference in devices.
1256920	1260800	But we don't want devices just to be pre-programmed
1260800	1262840	and just do what they do in the world.
1262840	1265600	We want devices to learn on their own.
1265600	1267920	We want devices to have an adaptive brain
1267920	1272440	that's continuously learning from a changing environment and from a changing self.
1272880	1274400	So imagine a robot, right?
1274400	1276560	We have we're we're eventually in our lifetime.
1276560	1279000	We're going to have robots for everything, you know,
1279000	1281560	but maybe it's a construction robot that's helping, you know,
1281560	1283720	repair our streets or build our homes.
1283720	1291080	The joints on that robot are going to erode and face damage in unique ways.
1291440	1295560	And it needs to learn how to adjust its own movement
1295560	1300400	so it can maintain its performance based on its own kind of evolution
1300400	1302760	and transition in its physical self.
1302760	1306000	Also, the robots might be adjusting to new environments.
1306400	1310880	And we'd like that ability to be baked in so they can continuously and adaptively learn.
1311680	1314880	This also really enables personalization.
1314880	1320320	It enables machines to get to know us and for us to have the assurance
1320360	1323920	that that knowledge and data of ourselves is in that robot
1323920	1327800	and doesn't believe that robot, I think is something we will will be very assuring.
1328640	1331720	But but that's a huge piece, you know, training and inference
1331720	1335480	in the same platform that is untethered.
1336800	1338760	What's really interesting about that?
1338760	1340320	Oh, there's a number of things, obviously.
1340320	1343800	But I mean, I'm just thinking of a limping robot.
1343800	1347240	For instance, you know, we limp when we injure a joint
1347280	1354280	and that is our adaptation to the limitation in a knee or a hip or something like that.
1354280	1355480	And we function with that.
1355480	1360240	And, you know, maybe we'll repair the robot, but maybe the robot is inaccessible
1360240	1365080	or maybe the robot is on Mars or Pluto or who knows where or maybe it's too expensive.
1365080	1368120	So limping and getting, you know, that's interesting.
1368400	1371160	The other thing that you mentioned that's super interesting is
1372040	1374600	we want AI to make our lives better
1374800	1377440	and we want robots to make our lives better.
1377560	1382160	But that doesn't mean that we want Amazon to know our deepest thoughts.
1382280	1386680	That doesn't mean that we want Google to know everything about our personal finances.
1387040	1392240	You know, if we can have AI that is personal, that, you know,
1392240	1394760	sure, it comes from somewhere and we've purchased it.
1394760	1398960	Or, you know, if you start to get into it, if you start to look at general AI,
1398960	1401000	you start thinking, do you purchase that?
1401000	1402200	Do you recruit that?
1402200	1404120	Do you adopt that?
1404120	1407760	Lots of questions there, but it's nice if you can get a system
1408560	1413280	that learns you, understands you and it stays in some sort of privacy corridor there.
1413960	1416280	Really, really interesting stuff.
1417720	1421680	How it's so hard to predict the future.
1422160	1424360	You mentioned about speed.
1424360	1425680	That's always a moving target, right?
1425680	1428400	You want a hundred X speed, but you see what speed is right now.
1428920	1432720	In terms of adding speed right now, we seem to not be making chips
1432720	1434360	much faster per se.
1434360	1440560	We're adding more chipsets on a die or we're we're creating chips
1440560	1444680	that are more optimized for this task or for that task or for energy efficiency
1444680	1448880	or for whatever, and that's how we're making overall devices faster.
1449200	1452480	So are you are you keeping that sort of moving target in mind
1452480	1456240	as you look at the performance levels that you want to hit?
1457000	1460640	So we believe we can achieve that one hundred X
1460680	1465480	and proven speed and beyond because we're taking a different tack entirely
1465480	1468760	because we're not on that roadmap of digital hardware
1469080	1473240	that is very mature chips and they're eking out performance
1473240	1477680	from any number of, you know, kind of well trod playbooks.
1478360	1480920	In the case of a fully analog neuromorphic chip,
1481440	1484520	you know, you have a neural network where you have analog neurons
1484520	1486480	and synapses that are connected in sequence.
1486960	1492600	And, you know, you can compare this to to other analog mixed signal chips.
1492600	1496120	So people are starting to get a lot of speed improvements
1496120	1499440	by moving away from digital and doing, say, the synaptic operations
1499440	1502960	either with photonic components or flash components.
1503560	1507560	But in all of those cases, they still actually have to translate
1507560	1512200	between their kind of physical, their either physics or optical
1512200	1514880	to digital for their neurons.
1514920	1517800	And so they have these analog synapses, digital neurons,
1517800	1521720	they kind of go back and forth and that requires clocking
1521720	1524160	that requires slowing down the system.
1524160	1527720	So in a fully analog chip, when it when it's designed,
1527800	1532480	well, you can have input to output and have the signal flow
1532480	1535280	at wire speed from end to end.
1535760	1538000	That is the full potential of an analog chip.
1538000	1541920	And that's why the speed here is so extraordinary
1542520	1545480	because we're no longer working in this digitally clocked world.
1545680	1548480	But we're again, exploiting the physics of the system,
1548480	1550760	the physical nature of the system to do that math for us.
1551120	1555320	And you can perform inference as just a wave of electricity from input out.
1556800	1557960	Amazing.
1557960	1561040	I know that Tesla, for instance, is doing
1561600	1565560	we can debate whether the term full is is an appropriate modifier there,
1565560	1568320	but full self driving on atom chips, right?
1568600	1571160	Not saying that all the training and learning is happening there,
1571160	1574160	but that's what's actively involved in the vehicle.
1574400	1578640	So just imagining what this could do for self driving,
1578640	1581560	for automated machinery, for robotics, for AI.
1582000	1583400	It's kind of mind blowing.
1583400	1585360	Gordon, thank you for spending this time with us.
1585360	1587320	I really do appreciate it.
1587320	1590320	Thank you so much for the time, John. It was a pleasure.
