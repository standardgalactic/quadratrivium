1
00:00:00,000 --> 00:00:03,680
Ultimately, we see this as kind of like Lego pieces that, you know,

2
00:00:03,720 --> 00:00:08,000
due to their low power footprints, we'll be able to concatenate them together

3
00:00:08,000 --> 00:00:12,480
using things like chip lead integration, advanced packaging, and ultimately

4
00:00:12,520 --> 00:00:17,240
scale out these systems to be brain scale, 86 billion neurons,

5
00:00:17,320 --> 00:00:23,320
500 trillion synapses, and low power enough that they can exist in autonomous devices.

6
00:00:23,880 --> 00:00:28,080
Can we build a computer chip that operates like the brain?

7
00:00:28,640 --> 00:00:32,960
We've seen neuromorphic computer chips from companies like Intel before,

8
00:00:32,960 --> 00:00:37,080
like the Luigi chip, which Intel claimed had a sense of smell.

9
00:00:37,600 --> 00:00:42,240
Rain neuromorphics, however, says it has an end-to-end analog chip,

10
00:00:42,520 --> 00:00:47,280
a neuromorphic processing unit that they say is the world's first end-to-end

11
00:00:47,280 --> 00:00:49,880
analog trainable AI circuit.

12
00:00:50,200 --> 00:00:55,480
It's a fully analog neural network, and it's a thousand times more energy

13
00:00:55,480 --> 00:00:58,280
efficient than today's best processors.

14
00:00:58,560 --> 00:01:01,800
Here to chat is Rain neuromorphic CEO, Gordon Wilson.

15
00:01:02,080 --> 00:01:02,800
Welcome, Gordon.

16
00:01:03,880 --> 00:01:05,040
Thanks so much for having me, John.

17
00:01:05,080 --> 00:01:05,760
It's good to be here.

18
00:01:06,840 --> 00:01:07,840
It's good to have you.

19
00:01:08,080 --> 00:01:11,360
When I see neuromorphics in the inbox, I got to check it out.

20
00:01:11,360 --> 00:01:12,560
It's a cool space.

21
00:01:13,320 --> 00:01:14,240
Let's start here.

22
00:01:14,280 --> 00:01:15,880
What the heck is an analog chip?

23
00:01:17,160 --> 00:01:18,320
Yeah, great question.

24
00:01:18,320 --> 00:01:22,600
And I think a great kicking off point because it really allows us to frame kind

25
00:01:22,600 --> 00:01:26,600
of what we're doing compared to what has been done for this past decade.

26
00:01:27,160 --> 00:01:32,320
And I think it's easiest to understand what an analog chip is in contrast

27
00:01:32,320 --> 00:01:37,960
to the neural networks, the AI that have defined this last decade of deep learning.

28
00:01:38,560 --> 00:01:43,880
So in 2012, we kind of had a big event that started this new era, this AI

29
00:01:43,880 --> 00:01:50,640
Renaissance, and we are seeing these massive neural networks grow in size

30
00:01:50,640 --> 00:01:52,920
and grow in capabilities since then.

31
00:01:53,280 --> 00:01:57,440
But all of those neural networks that we've seen in this deep learning

32
00:01:57,440 --> 00:02:00,040
world are neural simulations.

33
00:02:00,440 --> 00:02:05,840
They are abstractions that are written in software and the neurons and synapses

34
00:02:05,840 --> 00:02:10,480
that are defined in that software don't physically exist, but rather they

35
00:02:10,480 --> 00:02:13,640
sit on this highest layer usually written in Python and that are then

36
00:02:13,640 --> 00:02:17,640
translated through many layers of abstraction down until it gets to the

37
00:02:17,640 --> 00:02:22,600
digital circuit, most of the time a GPU graphics processing unit where it

38
00:02:22,600 --> 00:02:26,440
then processes that math that represents that neural network at the top.

39
00:02:28,280 --> 00:02:31,520
An analog chip and a neuromorphic chip is different.

40
00:02:31,880 --> 00:02:36,240
It's not a neural simulation, but rather it's a neural circuit.

41
00:02:36,640 --> 00:02:41,360
It's a physical collection of neurons and synapses as opposed to an

42
00:02:41,360 --> 00:02:43,320
abstraction of neurons and synapses.

43
00:02:43,880 --> 00:02:46,440
And this is very similar to the brain.

44
00:02:46,800 --> 00:02:49,960
The brain is a collection of physical neurons and synapses.

45
00:02:50,000 --> 00:02:54,520
It's governed by the laws of physics and it achieves all that it does with

46
00:02:54,640 --> 00:03:00,640
extraordinary scale and extraordinary efficiency within the bounds of the

47
00:03:00,640 --> 00:03:01,520
physical world.

48
00:03:02,000 --> 00:03:06,440
So an analog chip, as we're building it, is trying to achieve something

49
00:03:06,440 --> 00:03:13,160
simple, trying to find ways to learn, find ways to scale all within the physical

50
00:03:13,160 --> 00:03:13,480
domain.

51
00:03:14,480 --> 00:03:16,920
So let's dig into that just a little deeper.

52
00:03:17,480 --> 00:03:19,560
Great, great overview.

53
00:03:20,120 --> 00:03:21,480
What does that mean?

54
00:03:22,040 --> 00:03:26,440
Typically a chip will think in terms of or operate in terms of on or off one or

55
00:03:26,440 --> 00:03:27,280
zero, right?

56
00:03:27,280 --> 00:03:28,840
Binary logic, right?

57
00:03:29,920 --> 00:03:33,880
What does an analog chip, how does that work?

58
00:03:33,880 --> 00:03:34,720
What's that look like?

59
00:03:35,720 --> 00:03:41,400
So digital chips are, as you said, built on the very bottom on zeros and

60
00:03:41,960 --> 00:03:45,480
ones on this Boolean logic of on or off.

61
00:03:45,920 --> 00:03:48,760
And all of the other logic is then constructed on top of that.

62
00:03:49,240 --> 00:03:53,360
When you zoom down to the bottom of an analog chip, you don't have zeros or

63
00:03:53,360 --> 00:03:59,800
ones, you have gradients of information, you have voltages and currents and

64
00:03:59,800 --> 00:04:03,920
resistances, you have physical quantities that you're measuring that

65
00:04:03,920 --> 00:04:07,160
represents the mathematical operations that you're performing.

66
00:04:07,600 --> 00:04:11,160
And you're exploiting the relationship between those physical quantities to

67
00:04:11,160 --> 00:04:15,200
then perform these very complex neural operations.

68
00:04:15,680 --> 00:04:19,440
So an example of this is like matrix vector multiplication.

69
00:04:19,440 --> 00:04:24,600
This is the backbone of most neural network math.

70
00:04:25,160 --> 00:04:31,040
And GPUs do this by parallelizing these multiplications across a lot of

71
00:04:31,040 --> 00:04:35,360
digital cores and doing precise digital multiplication in addition.

72
00:04:36,360 --> 00:04:41,200
In an analog chip, as we are building it, we're not doing this with highly

73
00:04:41,200 --> 00:04:45,760
precise digital math, but instead we have the activations of the neurons

74
00:04:45,760 --> 00:04:47,120
represented by voltages.

75
00:04:47,520 --> 00:04:52,920
We have the weights of the synapses represented by resistances, which are

76
00:04:52,920 --> 00:04:54,960
held in components called memristors.

77
00:04:55,480 --> 00:04:59,320
And when that voltage passes across that resistance, you have a natural

78
00:04:59,320 --> 00:05:03,440
relationship between voltage and resistance that's multiplicative to

79
00:05:03,600 --> 00:05:07,000
receive a current, you read out a current and that's your output.

80
00:05:07,440 --> 00:05:12,720
So an analog chip works by kind of first understanding these physical

81
00:05:12,720 --> 00:05:17,960
relationships between electrical quantities and exploiting those to do

82
00:05:17,960 --> 00:05:20,960
the math, to make the physics do the math for us.

83
00:05:23,000 --> 00:05:30,480
That sounds super interesting and it sounds at once very, very complex and

84
00:05:30,480 --> 00:05:33,760
at the same time kind of simple, right?

85
00:05:34,080 --> 00:05:39,480
And is that one of the key reasons why your chip is so much more energy efficient?

86
00:05:39,480 --> 00:05:42,760
I mean, you're claiming a thousand times more energy efficient.

87
00:05:43,440 --> 00:05:44,760
Yeah, exactly.

88
00:05:45,120 --> 00:05:47,880
That really is, I think, the most fundamental reason.

89
00:05:47,880 --> 00:05:52,800
And I think when you consider the comparison again to the neural simulation,

90
00:05:53,440 --> 00:05:57,920
the neural network in that simulation exists so many layers above and requires

91
00:05:57,920 --> 00:06:03,560
to be translated and then performed on a circuit that is really not that

92
00:06:03,560 --> 00:06:06,640
well optimized for neural math.

93
00:06:07,640 --> 00:06:11,400
And, but in our case, the circuit is the neural network.

94
00:06:11,760 --> 00:06:14,920
And because it is, it exists in that very bottom layer, it is on the

95
00:06:14,920 --> 00:06:20,400
substrate of the chip itself, you can achieve some extraordinary gains in

96
00:06:20,400 --> 00:06:24,960
both speed and improvement and power reduction, which of course gives you

97
00:06:24,960 --> 00:06:26,560
that energy efficiency gain.

98
00:06:27,280 --> 00:06:31,200
So, so yeah, it's because we're building the physical thing on that bottom layer.

99
00:06:31,880 --> 00:06:36,280
This episode of Tech First is sponsored by Smart One.

100
00:06:36,680 --> 00:06:41,400
Smart One is a smart vending machine platform that transforms traditional

101
00:06:41,400 --> 00:06:47,160
vending hardware into smarter, better, faster, automated retail kiosks, a

102
00:06:47,160 --> 00:06:49,200
convenient store without the store.

103
00:06:49,400 --> 00:06:53,320
Learn more at smrt1.ca.

104
00:06:53,920 --> 00:06:55,840
That I'm fascinated.

105
00:06:56,560 --> 00:07:02,720
And I'm wondering with Boolean logic, old school computer chips, you kind

106
00:07:02,720 --> 00:07:06,920
of have to simulate reality and go through those multiple layers of

107
00:07:06,920 --> 00:07:11,320
translation because you're not on bare metal, you know, as the, the old

108
00:07:11,320 --> 00:07:13,200
timers and computing would tell us, right?

109
00:07:13,680 --> 00:07:16,480
Go through various layers of translation before you're actually hitting

110
00:07:16,480 --> 00:07:19,280
computer, machine instruction, machine language.

111
00:07:19,760 --> 00:07:25,880
And you're kind of modeling reality or what you're computing in reality.

112
00:07:25,920 --> 00:07:27,920
Is, is that an accurate way of thinking about it?

113
00:07:27,960 --> 00:07:28,320
Yes.

114
00:07:28,760 --> 00:07:31,080
That's an exact way, a really great way to put it.

115
00:07:31,400 --> 00:07:35,480
You know, I sometimes use the metaphor, like what would be easier, you know,

116
00:07:35,480 --> 00:07:39,680
to, to assess your ability to kick a soccer ball on a field, right?

117
00:07:39,680 --> 00:07:45,520
Would you rather, you know, reconstruct all of the physics of your body and of

118
00:07:45,520 --> 00:07:52,040
the soccer field and of the ball and simulate that kick and, you know, observe

119
00:07:52,040 --> 00:07:55,400
that in the simulated world, or would you just rather walk out of the pitch

120
00:07:55,760 --> 00:07:56,480
and kick the ball?

121
00:07:57,040 --> 00:08:01,640
You know, it's for us, it feels very obvious that, you know, the most natural

122
00:08:01,640 --> 00:08:07,920
way to build a neural network, uh, to make it as efficient as the brain is to

123
00:08:07,920 --> 00:08:10,080
build it physically, just as the brain does.

124
00:08:12,200 --> 00:08:14,480
Are you trying to build an artificial brain?

125
00:08:15,640 --> 00:08:16,600
Yes, we are.

126
00:08:17,200 --> 00:08:18,520
That is our goal.

127
00:08:18,680 --> 00:08:22,600
You know, we have kind of two missions that are very complimentary.

128
00:08:22,640 --> 00:08:24,640
Uh, one of them is to build a brain.

129
00:08:24,960 --> 00:08:27,320
And the other one is to actually understand it.

130
00:08:27,680 --> 00:08:32,800
You know, we really love the, and believe the notion that you can't fully

131
00:08:32,800 --> 00:08:36,560
understand a system until you have, have reconstructed it and built it.

132
00:08:37,000 --> 00:08:39,800
Uh, you know, this go back to Feynman and many others.

133
00:08:40,280 --> 00:08:44,840
Uh, but what we believe we've developed really are kind of the core technologies

134
00:08:44,840 --> 00:08:49,160
that allow us to first build just kind of unit level chips that address, you

135
00:08:49,160 --> 00:08:50,440
know, near term problems.

136
00:08:51,000 --> 00:08:54,760
But ultimately we see this as kind of like Lego pieces that, you know,

137
00:08:54,840 --> 00:08:58,480
due to their low power footprints, we'll be able to concatenate them

138
00:08:58,480 --> 00:09:02,920
together using things like chiplet integration, advanced packaging, and

139
00:09:02,920 --> 00:09:08,840
ultimately scale out these systems to be brain scale, 86 billion neurons, 500

140
00:09:08,840 --> 00:09:14,440
trillion synapses, and low power enough that they can exist in autonomous devices.

141
00:09:15,000 --> 00:09:19,720
Because today, trading is so expensive that first week, we consider

142
00:09:19,720 --> 00:09:21,840
trading inferences, the separate problems.

143
00:09:21,840 --> 00:09:24,640
And I don't think they really should be considered as these separate

144
00:09:24,640 --> 00:09:29,360
and distinct problems, but it's so expensive that we can't even conceive

145
00:09:29,440 --> 00:09:32,840
of putting training of natural language in an autonomous machine.

146
00:09:33,440 --> 00:09:35,560
And yet humans do it all the time.

147
00:09:36,000 --> 00:09:40,160
So that is what we are trying to achieve to, to recapitulate the brain in

148
00:09:40,160 --> 00:09:44,800
hardware and ultimately give machines all of the capabilities that we

149
00:09:44,840 --> 00:09:45,960
recognize in ourselves.

150
00:09:46,600 --> 00:09:52,120
So Gordon, you're a pretty soft spoken guy and you sound like a very thoughtful

151
00:09:52,120 --> 00:10:00,360
guy. And in that very soft spoken way, you're saying absolutely ginormous things.

152
00:10:01,640 --> 00:10:04,160
We're talking Frankenstein level stuff, right?

153
00:10:04,160 --> 00:10:07,800
I mean, you understand the gravity of what you're talking about, right?

154
00:10:08,520 --> 00:10:09,320
Absolutely.

155
00:10:09,320 --> 00:10:14,720
No, this is the scope and impact of our work is not.

156
00:10:14,840 --> 00:10:18,280
I mean, this is something we've been working on for nearly five years and we

157
00:10:18,280 --> 00:10:22,040
recognize that if we are successful in achieving this, this, this will be

158
00:10:22,040 --> 00:10:24,480
historic and, and massively consequential.

159
00:10:25,040 --> 00:10:29,920
But, you know, what motivates us, you know, is of course the, the impact on

160
00:10:29,920 --> 00:10:35,920
positively improving human life, you know, and we can have personalized medicine,

161
00:10:35,920 --> 00:10:38,800
personalized education, we can automate all of labor.

162
00:10:39,480 --> 00:10:42,080
I mean, this is the world that we want to realize.

163
00:10:42,120 --> 00:10:45,000
I think many people are already in this consensus that artificial

164
00:10:45,000 --> 00:10:49,320
intelligence is going to be kind of the defining technology of the century.

165
00:10:49,640 --> 00:10:53,640
But most people don't, don't know that the hardware that supports it is,

166
00:10:53,640 --> 00:10:54,920
is the bottleneck right now.

167
00:10:55,640 --> 00:11:00,280
So, so yes, we recognize the scope of this and, you know, it's, it's a big

168
00:11:00,320 --> 00:11:01,720
mission and a big task.

169
00:11:01,760 --> 00:11:05,840
But, you know, I think that we're, we're approaching it the right way.

170
00:11:05,840 --> 00:11:10,400
And we're also very conscientious of the fact that we're, you know,

171
00:11:10,400 --> 00:11:16,320
conscientious of the fact that we don't want, that there, there are good ways

172
00:11:16,320 --> 00:11:18,440
of implementing AI and there are also not good ways.

173
00:11:19,240 --> 00:11:23,000
And we don't think AI should be used everywhere for every purpose, but there

174
00:11:23,000 --> 00:11:29,320
are guidelines and ethics that should really direct how we build and implement

175
00:11:29,480 --> 00:11:30,080
these systems.

176
00:11:31,160 --> 00:11:32,280
And it's controversial.

177
00:11:32,280 --> 00:11:35,240
It's also political in the AI space, right?

178
00:11:35,240 --> 00:11:39,320
In terms of what you're building, what you're looking at, people who claim

179
00:11:39,440 --> 00:11:44,160
to be working on general AI, for instance, you know, there's a lot of

180
00:11:44,160 --> 00:11:48,120
scrutiny there, there was a, I believe a Google engineer who last week said,

181
00:11:48,120 --> 00:11:51,560
I think that some of our systems are, are approaching consciousness.

182
00:11:51,960 --> 00:11:57,600
And, and there were a ton of people jumping all over him, probably correctly.

183
00:11:57,840 --> 00:12:03,520
But, you know, perhaps in a bit of a mob mentality for daring to suggest

184
00:12:03,520 --> 00:12:08,480
that and, and saying, no, we're, we're so far out there, maybe let's come back

185
00:12:08,480 --> 00:12:16,120
here, because you and your co-founder have studied the human brain significantly.

186
00:12:16,640 --> 00:12:20,120
What do we learn about AI from the way that our brains work?

187
00:12:21,200 --> 00:12:21,560
Absolutely.

188
00:12:21,560 --> 00:12:22,960
That's a great question, John.

189
00:12:23,320 --> 00:12:29,800
And I'd say that there are really kind of two categories of clues that we look

190
00:12:29,800 --> 00:12:33,480
at, you know, from the brain that then inform our hardware.

191
00:12:34,480 --> 00:12:38,920
The first is, how does the brain learn so efficiently?

192
00:12:39,200 --> 00:12:44,520
You know, it, the brain trains and learns with both very few examples.

193
00:12:45,520 --> 00:12:48,840
We, we learn with, with one example or two examples, one shot learning, two

194
00:12:48,840 --> 00:12:51,560
shot learning, and we can generalize extraordinarily well.

195
00:12:52,280 --> 00:12:55,560
So learning training happens very, very efficiently.

196
00:12:56,000 --> 00:13:01,000
And there are the learning rule of the brain, the algorithm the brain uses is

197
00:13:01,000 --> 00:13:06,560
not fully understood or identified, but we do know that there are certain

198
00:13:06,600 --> 00:13:09,480
requirements that, that algorithm must have.

199
00:13:10,320 --> 00:13:14,440
So one of those is called a local learning rule.

200
00:13:15,000 --> 00:13:19,120
So this, if you're, for the listeners who are familiar with back propagation,

201
00:13:19,120 --> 00:13:23,800
which is the industry standard algorithm for digital AI, this says what's

202
00:13:23,800 --> 00:13:25,160
called a global learning.

203
00:13:25,840 --> 00:13:27,640
So first let's say, well, what is a learning rule?

204
00:13:27,960 --> 00:13:33,280
A learning rule is how any given synapse in a neural network knows, should

205
00:13:33,280 --> 00:13:38,320
it become stronger or weaker for the whole system to become smarter, better

206
00:13:38,320 --> 00:13:39,720
at its assigned task.

207
00:13:40,280 --> 00:13:44,120
And in back propagation of the digital world, for you to calculate whether

208
00:13:44,120 --> 00:13:47,600
this one synapse should be stronger or weaker, you need to do a math equation

209
00:13:47,600 --> 00:13:49,640
that incorporates the entire network.

210
00:13:49,960 --> 00:13:52,280
You need to differentiate against the entire system.

211
00:13:52,800 --> 00:13:57,440
That's really mathematically expensive, but not only is it expensive, it's

212
00:13:57,440 --> 00:14:00,920
also impossible for the brain to be doing the same thing.

213
00:14:01,200 --> 00:14:05,120
There's no like agent sitting in our brain observing the whole system and

214
00:14:05,120 --> 00:14:07,680
then doing a math problem to update every synapse.

215
00:14:07,680 --> 00:14:08,640
It's, it's impossible.

216
00:14:08,960 --> 00:14:12,400
The brain has to be operating with a local learning rule.

217
00:14:13,240 --> 00:14:17,600
And so a local learning rule is so that that synapse can just observe what's

218
00:14:17,600 --> 00:14:20,240
right nearby and still know I become stronger.

219
00:14:21,280 --> 00:14:23,840
So that's one of the clues in the algorithm side.

220
00:14:23,840 --> 00:14:28,200
And that's one of the pieces that makes our learning algorithm so special,

221
00:14:28,200 --> 00:14:33,920
that it is as smart as back propagation, but it does so with a local learning rule.

222
00:14:35,320 --> 00:14:36,560
Well, you burst my bubble there.

223
00:14:36,560 --> 00:14:39,320
And there's no homunculus, I believe you pronounce it.

224
00:14:39,320 --> 00:14:42,880
I mean, there's no little guy in a little control room in my brain.

225
00:14:44,640 --> 00:14:45,400
Unfortunate.

226
00:14:46,440 --> 00:14:46,800
Yeah.

227
00:14:46,960 --> 00:14:49,080
And, and there's another side of the clue.

228
00:14:49,080 --> 00:14:52,800
So I'd say the first is on like how it learns the learning algorithm.

229
00:14:52,880 --> 00:14:54,920
The other is about scale.

230
00:14:55,080 --> 00:14:58,160
How does the brain achieve its massive scale?

231
00:14:58,200 --> 00:15:02,280
86 million neurons, 500 trillion synapses, and still process and move

232
00:15:02,280 --> 00:15:04,440
information so quickly, so efficiently.

233
00:15:05,840 --> 00:15:10,240
So, and in that case, we take clues from like the topology of the brain.

234
00:15:10,520 --> 00:15:13,920
And in fact, and the specific thing is called sparsity.

235
00:15:14,480 --> 00:15:19,640
The idea that you don't have to connect every neuron to every other neuron in

236
00:15:19,640 --> 00:15:21,840
the system for it to be well connected.

237
00:15:22,440 --> 00:15:27,960
Now, again, to contrast to the status quo, digital ALI in deep learning is,

238
00:15:28,600 --> 00:15:32,600
was primarily built on what are called fully connected layers in neural networks

239
00:15:32,600 --> 00:15:35,720
where you have a layer of neurons and another layer of neurons and you

240
00:15:35,720 --> 00:15:39,440
connect every neuron here to every neuron in the next layer.

241
00:15:40,000 --> 00:15:40,360
Whoa.

242
00:15:40,360 --> 00:15:45,600
This was, that was the natural way to do it on GPUs because GPUs do these

243
00:15:45,600 --> 00:15:49,120
dense matrix multiplications, which correspond to this fully connected,

244
00:15:49,160 --> 00:15:50,360
densely connected systems.

245
00:15:51,360 --> 00:15:53,800
Now, our brain is very different.

246
00:15:54,040 --> 00:15:57,960
Our brain of all the possible connections that could exist between

247
00:15:57,960 --> 00:16:03,120
neurons, it's something like just a fraction or one percent of those

248
00:16:03,120 --> 00:16:04,000
connections exist.

249
00:16:04,520 --> 00:16:10,040
And yet at the same time, the path for information to travel from one neuron

250
00:16:10,040 --> 00:16:11,880
to any other neuron is very short.

251
00:16:12,200 --> 00:16:16,440
The average path length is about four jumps for information to traverse

252
00:16:16,440 --> 00:16:17,680
anywhere in the brain.

253
00:16:17,880 --> 00:16:19,640
It's extremely well connected.

254
00:16:20,200 --> 00:16:23,800
So there are these special patterns of connectivity.

255
00:16:24,040 --> 00:16:26,240
One of them is called a small world network.

256
00:16:26,720 --> 00:16:30,360
And if you've ever heard of a small world network, it's a network pattern

257
00:16:30,360 --> 00:16:34,040
that also mirrors human social networks that gives rise to the six

258
00:16:34,040 --> 00:16:36,800
degrees of separation property and human connections.

259
00:16:37,560 --> 00:16:40,000
And the idea is you can have lots of local connections.

260
00:16:40,040 --> 00:16:41,520
You want to be connected to your neighbors.

261
00:16:41,520 --> 00:16:43,320
You're likely to be connected to your neighbors.

262
00:16:43,320 --> 00:16:45,200
And that's a very short path to bridge.

263
00:16:45,720 --> 00:16:48,880
And then you want some long distance connections.

264
00:16:49,520 --> 00:16:55,400
And you can create these very well connected networks at very large scale

265
00:16:55,760 --> 00:16:59,320
when you implement these intelligent forms of sparsity.

266
00:16:59,760 --> 00:17:04,920
So sparsity is core to our scaling architecture.

267
00:17:05,360 --> 00:17:07,680
And really, and just to kind of summarize there.

268
00:17:07,760 --> 00:17:10,800
So we have these are the two core technologies we've developed.

269
00:17:11,000 --> 00:17:14,880
A learning algorithm that learns of the local learning rule and a scaling

270
00:17:14,880 --> 00:17:19,520
architecture that scales to massive sizes of neural networks using intelligent sparsity.

271
00:17:21,200 --> 00:17:23,120
I knew Kevin Bacon would come up some point.

272
00:17:23,200 --> 00:17:25,320
I mean, inevitable.

273
00:17:26,880 --> 00:17:32,280
Last year, I believe you sort of taped a functional chip together.

274
00:17:32,280 --> 00:17:33,640
Your first prototype.

275
00:17:34,000 --> 00:17:37,240
Where are you on the journey to shipping a fully functional chip?

276
00:17:38,680 --> 00:17:43,760
So we have, I'd say in the last four years, you know, we have done

277
00:17:44,240 --> 00:17:50,440
such expansive work that has been mostly, I would say, qualified as a research.

278
00:17:50,840 --> 00:17:52,640
We've been exploring different algorithms.

279
00:17:52,640 --> 00:17:54,480
We've been exploring different architectures.

280
00:17:55,280 --> 00:17:59,240
Originally, we have this concept using random nanowire meshes.

281
00:17:59,800 --> 00:18:03,040
Turns out it's not very manufacturable and better to build things

282
00:18:03,040 --> 00:18:05,480
that you can manufacture today easily.

283
00:18:05,480 --> 00:18:10,240
But in this last year, we kind of crystallized kind of our two core technologies.

284
00:18:10,240 --> 00:18:13,000
Like what is the learning algorithm and what is that scaling architecture?

285
00:18:13,280 --> 00:18:15,520
And then developed hardware prototypes of each.

286
00:18:16,240 --> 00:18:20,800
We still have a good amount of time to engineer this completely and to get to market.

287
00:18:20,800 --> 00:18:26,600
We we hope to get to market, you know, on the order of full scale shipping 2025.

288
00:18:28,040 --> 00:18:33,480
But that said, you know, building the world's first analog neural network is not easy.

289
00:18:34,760 --> 00:18:38,960
You know, to iterate through this and get it fully fully up at scale.

290
00:18:39,920 --> 00:18:41,280
And you just got some help there.

291
00:18:41,280 --> 00:18:42,200
You raised some funds.

292
00:18:42,800 --> 00:18:43,520
Yeah. Yeah.

293
00:18:43,520 --> 00:18:47,880
Yeah. So we just closed a 25 million series A, which is thrilling.

294
00:18:48,360 --> 00:18:51,920
You know, we had used eight million dollars for the last four years

295
00:18:51,920 --> 00:18:56,000
and honestly had gone through some challenging times where, you know,

296
00:18:56,680 --> 00:18:58,600
we got really close.

297
00:18:58,600 --> 00:19:01,680
You know, one of our first tape out prototypes didn't function

298
00:19:01,680 --> 00:19:02,720
precisely as designed.

299
00:19:02,720 --> 00:19:04,120
It was a very silly error.

300
00:19:04,120 --> 00:19:08,640
But in ships, it takes a long time to iterate and even and resolve.

301
00:19:08,640 --> 00:19:10,640
You can't just debug like software.

302
00:19:10,680 --> 00:19:12,080
So we went through a lot.

303
00:19:12,080 --> 00:19:13,000
We learned a lot.

304
00:19:13,000 --> 00:19:16,880
And now we have 25 million in the bank and we're hiring like crazy.

305
00:19:18,000 --> 00:19:20,720
Wonderful. It goes quickly, I know, from personal experience.

306
00:19:23,200 --> 00:19:26,640
So talk about what this will enable.

307
00:19:27,560 --> 00:19:29,320
You're shipping this ship.

308
00:19:29,320 --> 00:19:31,440
What will it enable?

309
00:19:31,440 --> 00:19:38,760
Yeah. So we want to not enable just like an incremental improvement in AI.

310
00:19:38,920 --> 00:19:43,680
You know, I think that there are a lot of folks and video included

311
00:19:43,680 --> 00:19:48,600
on the digital hardware roadmap and because digital hardware is so mature,

312
00:19:49,000 --> 00:19:52,480
it's just it's kind of incremental gains that we're getting at this point.

313
00:19:52,480 --> 00:19:56,080
And I think there's still more improvement to be geeked out on that roadmap.

314
00:19:56,600 --> 00:20:02,120
But we're trying to enable a new roadmap that is really a step change

315
00:20:02,200 --> 00:20:04,040
in performance improvement.

316
00:20:04,040 --> 00:20:08,600
And so we want to enter the market really at a thousand X energy

317
00:20:08,600 --> 00:20:11,280
efficiency improvement over status quo hardware.

318
00:20:11,560 --> 00:20:15,320
And at a thousand X comes from about a 10 X reduction in power

319
00:20:15,720 --> 00:20:18,240
alongside a 100 X improvement in speed.

320
00:20:19,120 --> 00:20:24,400
And when you can do that and importantly, not just for inference,

321
00:20:24,400 --> 00:20:28,640
we're talking about training and inference in the same platform.

322
00:20:29,040 --> 00:20:34,480
It unlocks possibilities that have just been inconceivable until until now.

323
00:20:35,280 --> 00:20:39,200
For one, you know, currently people consider training and inference

324
00:20:39,200 --> 00:20:43,120
as these kind of separate problems that we need separate platforms of hardware with.

325
00:20:43,600 --> 00:20:46,880
You know, we train up in a GPU cloud system,

326
00:20:46,880 --> 00:20:50,000
and then we might upload those weights onto a more efficient chip

327
00:20:50,000 --> 00:20:52,600
and deploy it out into the world.

328
00:20:52,600 --> 00:20:56,920
I think that would be the first step for kind of low power inference in devices.

329
00:20:56,920 --> 00:21:00,800
But we don't want devices just to be pre-programmed

330
00:21:00,800 --> 00:21:02,840
and just do what they do in the world.

331
00:21:02,840 --> 00:21:05,600
We want devices to learn on their own.

332
00:21:05,600 --> 00:21:07,920
We want devices to have an adaptive brain

333
00:21:07,920 --> 00:21:12,440
that's continuously learning from a changing environment and from a changing self.

334
00:21:12,880 --> 00:21:14,400
So imagine a robot, right?

335
00:21:14,400 --> 00:21:16,560
We have we're we're eventually in our lifetime.

336
00:21:16,560 --> 00:21:19,000
We're going to have robots for everything, you know,

337
00:21:19,000 --> 00:21:21,560
but maybe it's a construction robot that's helping, you know,

338
00:21:21,560 --> 00:21:23,720
repair our streets or build our homes.

339
00:21:23,720 --> 00:21:31,080
The joints on that robot are going to erode and face damage in unique ways.

340
00:21:31,440 --> 00:21:35,560
And it needs to learn how to adjust its own movement

341
00:21:35,560 --> 00:21:40,400
so it can maintain its performance based on its own kind of evolution

342
00:21:40,400 --> 00:21:42,760
and transition in its physical self.

343
00:21:42,760 --> 00:21:46,000
Also, the robots might be adjusting to new environments.

344
00:21:46,400 --> 00:21:50,880
And we'd like that ability to be baked in so they can continuously and adaptively learn.

345
00:21:51,680 --> 00:21:54,880
This also really enables personalization.

346
00:21:54,880 --> 00:22:00,320
It enables machines to get to know us and for us to have the assurance

347
00:22:00,360 --> 00:22:03,920
that that knowledge and data of ourselves is in that robot

348
00:22:03,920 --> 00:22:07,800
and doesn't believe that robot, I think is something we will will be very assuring.

349
00:22:08,640 --> 00:22:11,720
But but that's a huge piece, you know, training and inference

350
00:22:11,720 --> 00:22:15,480
in the same platform that is untethered.

351
00:22:16,800 --> 00:22:18,760
What's really interesting about that?

352
00:22:18,760 --> 00:22:20,320
Oh, there's a number of things, obviously.

353
00:22:20,320 --> 00:22:23,800
But I mean, I'm just thinking of a limping robot.

354
00:22:23,800 --> 00:22:27,240
For instance, you know, we limp when we injure a joint

355
00:22:27,280 --> 00:22:34,280
and that is our adaptation to the limitation in a knee or a hip or something like that.

356
00:22:34,280 --> 00:22:35,480
And we function with that.

357
00:22:35,480 --> 00:22:40,240
And, you know, maybe we'll repair the robot, but maybe the robot is inaccessible

358
00:22:40,240 --> 00:22:45,080
or maybe the robot is on Mars or Pluto or who knows where or maybe it's too expensive.

359
00:22:45,080 --> 00:22:48,120
So limping and getting, you know, that's interesting.

360
00:22:48,400 --> 00:22:51,160
The other thing that you mentioned that's super interesting is

361
00:22:52,040 --> 00:22:54,600
we want AI to make our lives better

362
00:22:54,800 --> 00:22:57,440
and we want robots to make our lives better.

363
00:22:57,560 --> 00:23:02,160
But that doesn't mean that we want Amazon to know our deepest thoughts.

364
00:23:02,280 --> 00:23:06,680
That doesn't mean that we want Google to know everything about our personal finances.

365
00:23:07,040 --> 00:23:12,240
You know, if we can have AI that is personal, that, you know,

366
00:23:12,240 --> 00:23:14,760
sure, it comes from somewhere and we've purchased it.

367
00:23:14,760 --> 00:23:18,960
Or, you know, if you start to get into it, if you start to look at general AI,

368
00:23:18,960 --> 00:23:21,000
you start thinking, do you purchase that?

369
00:23:21,000 --> 00:23:22,200
Do you recruit that?

370
00:23:22,200 --> 00:23:24,120
Do you adopt that?

371
00:23:24,120 --> 00:23:27,760
Lots of questions there, but it's nice if you can get a system

372
00:23:28,560 --> 00:23:33,280
that learns you, understands you and it stays in some sort of privacy corridor there.

373
00:23:33,960 --> 00:23:36,280
Really, really interesting stuff.

374
00:23:37,720 --> 00:23:41,680
How it's so hard to predict the future.

375
00:23:42,160 --> 00:23:44,360
You mentioned about speed.

376
00:23:44,360 --> 00:23:45,680
That's always a moving target, right?

377
00:23:45,680 --> 00:23:48,400
You want a hundred X speed, but you see what speed is right now.

378
00:23:48,920 --> 00:23:52,720
In terms of adding speed right now, we seem to not be making chips

379
00:23:52,720 --> 00:23:54,360
much faster per se.

380
00:23:54,360 --> 00:24:00,560
We're adding more chipsets on a die or we're we're creating chips

381
00:24:00,560 --> 00:24:04,680
that are more optimized for this task or for that task or for energy efficiency

382
00:24:04,680 --> 00:24:08,880
or for whatever, and that's how we're making overall devices faster.

383
00:24:09,200 --> 00:24:12,480
So are you are you keeping that sort of moving target in mind

384
00:24:12,480 --> 00:24:16,240
as you look at the performance levels that you want to hit?

385
00:24:17,000 --> 00:24:20,640
So we believe we can achieve that one hundred X

386
00:24:20,680 --> 00:24:25,480
and proven speed and beyond because we're taking a different tack entirely

387
00:24:25,480 --> 00:24:28,760
because we're not on that roadmap of digital hardware

388
00:24:29,080 --> 00:24:33,240
that is very mature chips and they're eking out performance

389
00:24:33,240 --> 00:24:37,680
from any number of, you know, kind of well trod playbooks.

390
00:24:38,360 --> 00:24:40,920
In the case of a fully analog neuromorphic chip,

391
00:24:41,440 --> 00:24:44,520
you know, you have a neural network where you have analog neurons

392
00:24:44,520 --> 00:24:46,480
and synapses that are connected in sequence.

393
00:24:46,960 --> 00:24:52,600
And, you know, you can compare this to to other analog mixed signal chips.

394
00:24:52,600 --> 00:24:56,120
So people are starting to get a lot of speed improvements

395
00:24:56,120 --> 00:24:59,440
by moving away from digital and doing, say, the synaptic operations

396
00:24:59,440 --> 00:25:02,960
either with photonic components or flash components.

397
00:25:03,560 --> 00:25:07,560
But in all of those cases, they still actually have to translate

398
00:25:07,560 --> 00:25:12,200
between their kind of physical, their either physics or optical

399
00:25:12,200 --> 00:25:14,880
to digital for their neurons.

400
00:25:14,920 --> 00:25:17,800
And so they have these analog synapses, digital neurons,

401
00:25:17,800 --> 00:25:21,720
they kind of go back and forth and that requires clocking

402
00:25:21,720 --> 00:25:24,160
that requires slowing down the system.

403
00:25:24,160 --> 00:25:27,720
So in a fully analog chip, when it when it's designed,

404
00:25:27,800 --> 00:25:32,480
well, you can have input to output and have the signal flow

405
00:25:32,480 --> 00:25:35,280
at wire speed from end to end.

406
00:25:35,760 --> 00:25:38,000
That is the full potential of an analog chip.

407
00:25:38,000 --> 00:25:41,920
And that's why the speed here is so extraordinary

408
00:25:42,520 --> 00:25:45,480
because we're no longer working in this digitally clocked world.

409
00:25:45,680 --> 00:25:48,480
But we're again, exploiting the physics of the system,

410
00:25:48,480 --> 00:25:50,760
the physical nature of the system to do that math for us.

411
00:25:51,120 --> 00:25:55,320
And you can perform inference as just a wave of electricity from input out.

412
00:25:56,800 --> 00:25:57,960
Amazing.

413
00:25:57,960 --> 00:26:01,040
I know that Tesla, for instance, is doing

414
00:26:01,600 --> 00:26:05,560
we can debate whether the term full is is an appropriate modifier there,

415
00:26:05,560 --> 00:26:08,320
but full self driving on atom chips, right?

416
00:26:08,600 --> 00:26:11,160
Not saying that all the training and learning is happening there,

417
00:26:11,160 --> 00:26:14,160
but that's what's actively involved in the vehicle.

418
00:26:14,400 --> 00:26:18,640
So just imagining what this could do for self driving,

419
00:26:18,640 --> 00:26:21,560
for automated machinery, for robotics, for AI.

420
00:26:22,000 --> 00:26:23,400
It's kind of mind blowing.

421
00:26:23,400 --> 00:26:25,360
Gordon, thank you for spending this time with us.

422
00:26:25,360 --> 00:26:27,320
I really do appreciate it.

423
00:26:27,320 --> 00:26:30,320
Thank you so much for the time, John. It was a pleasure.

