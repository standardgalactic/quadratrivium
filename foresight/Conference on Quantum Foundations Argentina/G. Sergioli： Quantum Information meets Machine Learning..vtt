WEBVTT

00:00.000 --> 00:09.840
Okay, we'll start with the second part of the first session this morning.

00:09.840 --> 00:17.280
The first talk is in charge of Schubert Cercione from the University of Caglione.

00:18.560 --> 00:24.480
Thank you, thank you very much and thank you to all the organizers who invited me. For me it's

00:24.480 --> 00:31.520
very proud and very proud to be invited to this 10th conference of Quantum Foundation.

00:32.080 --> 00:41.280
I'm very happy to be part of this very prominent panel of scientists. I warmly thank all the

00:41.280 --> 00:49.600
organizers and especially Federico Olic and Martin Bosic, who are also a strict friend of

00:49.600 --> 00:57.600
Italian University and of Cagliari and also personal friends. So what I want to introduce

00:57.600 --> 01:04.800
here is Quantum Information Meet Smashing Learning. So basically I like to introduce

01:04.800 --> 01:13.280
generally this topic by starting from a provocative question. I ask you, can you show me a picture

01:13.280 --> 01:24.800
of your Christmas 2018? I am sure that you can and I am sure that if you try to find a picture

01:24.800 --> 01:30.960
of your Christmas, for instance in your mobile, you will find it but probably you need time to

01:30.960 --> 01:38.800
find it just because now we use to make a lot a lot of pics with our mobile because the memory

01:38.800 --> 01:48.720
that our mobile has now is very, very huge and many, many pics can be kept, can be considered,

01:48.720 --> 02:01.760
can be stored in a cell for instance in a mobile. So what we can say is that we can,

02:01.760 --> 02:10.000
can we say that many data corresponds to many information? Basically not because information

02:10.000 --> 02:19.200
means recover from the data what we really need. So the fact that now we have to deal with a very,

02:19.200 --> 02:26.800
very huge amount of information means that potentially we can have a lot of information

02:26.800 --> 02:34.960
but the problem is the process we need to do in order to extrapolate this information

02:34.960 --> 02:46.560
from this big amount of data. So this is the reason why in the last decades big data science

02:46.560 --> 02:54.880
corresponds to face with just this kind of process, this kind of problem and so the problem is how

02:54.880 --> 03:03.680
to manage huge amount of data in order to retrieve in the short time as possible the information

03:03.680 --> 03:11.680
we need, the information we need. Okay so this is why big data born in the in the in the last

03:11.680 --> 03:17.760
decade. So this is just a very brief summary of what I said in these five minutes. So in other

03:17.760 --> 03:24.000
words, enlarging the dimension of the data set is a good thing because it increases the potential

03:24.000 --> 03:30.000
information but it's also bad because it makes the extraction of the required information out.

03:31.040 --> 03:39.600
So from another side you know very well that you know very well how quantum information

03:40.320 --> 03:46.720
born, how quantum information became in the recent past a really new discipline of

03:50.720 --> 03:56.080
quantum theory in general, quantum computing mostly, and we know that the advantage of

03:56.080 --> 04:03.360
quantum computation are most of the the advantages are most of all based on the fact that we have

04:03.440 --> 04:07.600
a speed up of the computation. We can obtain, we can

04:10.000 --> 04:16.800
perform a computer and strong and hard computation in a very very shorter, a shorter time

04:16.800 --> 04:28.240
with respect to a classical, a classical one. Okay so so we know also that quantum computation

04:28.240 --> 04:36.480
is not so futuristic. We can say that till 10 maybe 20 years ago we were speaking about

04:36.480 --> 04:42.720
quantum computation as something that was phantasyntific, that was not really concrete,

04:42.720 --> 04:50.480
but we know that the application in quantum computation has a very very speed up in the

04:50.480 --> 04:56.880
last in the last decade and now we know that the quantum processor already exists and we know

04:56.880 --> 05:04.560
that also by using for instance our our computer in our house we can connect with the IBM quantum

05:04.560 --> 05:11.680
experience and we can just design a quantum algorithm and by a click we can send this

05:11.680 --> 05:19.600
quantum algorithm to a real quantum computer that will perform it, that will give us back the result

05:19.600 --> 05:28.160
as a as a statistical result. So basically the quantum computation is now real, is now real

05:28.160 --> 05:37.360
and also we know that this this topic is is going on and on and for instance just one year ago,

05:37.360 --> 05:43.600
just one year ago in in china that was there was was declared that the

05:43.600 --> 05:49.520
achievement of quantum supremacy because it was possible to perform a calculation in

05:50.560 --> 05:58.000
200 seconds, a kind of calculation that will take more than half a billion years on the

05:58.000 --> 06:06.320
world's faster, no quantum or classical computer. So now we really trust in quantum computation,

06:06.320 --> 06:13.440
we really trust in this. So we were speaking about the data and about quantum computation

06:13.840 --> 06:19.760
but because well summarizing the main problem of big data when machine learning with big data

06:19.760 --> 06:25.360
is the time complexity of the computation but the main advantage of quantum computation is

06:25.360 --> 06:32.640
the possibility to reproduce to use the time complexity. So in a certain sense,

06:32.640 --> 06:39.120
quantum computation and machine learning appears as a very natural and maybe unavoidable,

06:39.120 --> 06:47.840
unavoidable connection but the connection can be deeper and deeper with respect to what we can

06:47.840 --> 06:55.520
think. Indeed the question that they want to ask you in this in this talk is can quantum

06:55.520 --> 07:04.240
information theory help classical machine learning, classical by using classical computer only

07:04.240 --> 07:11.360
and we will provide a positive answer to this question. So this approach that we will speak

07:11.360 --> 07:19.440
is known as a quantum-inspired machine learning and the important thing is that well very often

07:20.720 --> 07:30.320
we know that in many disciplines we speak about the inspiration from quantum mechanics.

07:30.320 --> 07:37.600
Okay now what I want to focus on is the fact that our inspiration is not just an analogy,

07:37.600 --> 07:46.480
it's not a metaphor but it's just a really inspiration regarding a real inspiration to quantum

07:46.480 --> 07:55.920
work in order to have a real benefit in machine learning and in the next minutes I aim to show

07:55.920 --> 08:04.800
you what they mean by real inspiration. Okay in particular we focus on the classification problem.

08:04.800 --> 08:11.120
What is the classification problem? Okay the general idea is that the quantum classification

08:11.120 --> 08:17.440
in the idea that we want to perform is the idea of a quantum classifier where quantum

08:17.440 --> 08:21.760
classification is the idea of quantum classification is to formalize the standard

08:22.480 --> 08:28.000
classification problems in terms of the mathematical quantum object and then

08:28.640 --> 08:34.480
inspired by certain properties of quantum state discrimination we define a quantum

08:34.480 --> 08:42.000
classifier that will provide interesting and real good advantages. The procedure is

08:42.000 --> 08:48.640
based on three fundamental steps that we will describe not now but a few slides later encoding

08:48.640 --> 08:55.600
classification and the coding but we will meet this slide again in a bit. First of all

08:56.960 --> 09:00.000
I want to very very quickly summarize what is the

09:02.080 --> 09:08.400
sorry the general framework of classification problem. So when we have to classify an object

09:08.400 --> 09:17.280
we describe this object as a vector where each component of this vector is a feature of this

09:17.360 --> 09:25.520
object like this object could be a cat and this feature could be the length of the tail,

09:26.080 --> 09:32.640
the weight of the cat and so on and we have different classes of objects for instance

09:32.640 --> 09:40.240
dog and cats and what and so on and this so a pattern is represented by a pair where this is

09:40.240 --> 09:46.720
the vector the object represented by a vector and this is the label that characterized the class

09:46.720 --> 09:53.760
for instance class for instance label one means class of the cat, label two means class of the

09:53.760 --> 10:01.040
dogs for instance okay and the goal of the classification is to have a classifier

10:03.280 --> 10:15.360
I mean a function classifier is a function that allows us to study the vector and gives as an output

10:15.360 --> 10:23.760
the label so looking at the vector looking at the object I can say you if this object is a cat or is

10:23.760 --> 10:33.520
a dog basically okay okay okay so in a general supervised scenario I will be very very very

10:34.320 --> 10:41.920
I will skip many slides but but sometime I can focus on some technical data sometimes not

10:42.000 --> 10:51.600
generally generally in what we say supervised learning we mean that we consider a data set

10:52.160 --> 11:00.640
for instance a data set of dogs and a data set of cats for instance and we divide this data set

11:00.640 --> 11:07.520
at the beginning in two parts the first part is given by the training set this is the set

11:07.520 --> 11:15.520
useful in order to train our algorithm and this is the test set but the set is a set of

11:15.520 --> 11:23.600
object that we use in order to know whether our algorithm is good or not just in order to know

11:23.600 --> 11:33.600
just in order to measure the performance of our algorithm and so obviously then all of these

11:33.680 --> 11:40.240
objects are already labelled so we already know the level of the object so the goal is just to

11:40.240 --> 11:50.320
evaluate how the right if the classifier is good or not so we use a already labelled set of object

11:50.320 --> 11:56.640
we divide it in training and test and obviously also the training is divided with respect to the

11:56.640 --> 12:04.480
classes the classes will get the classes of dogs the classes of foxes and so on and so on okay so

12:05.200 --> 12:14.800
the the first idea of the quantum classification is just to translate formally translate each

12:14.800 --> 12:23.120
object each vector each cat that is represented by a vector in terms of a quantum object that we

12:23.120 --> 12:29.920
represent by a density operator row and there are many many ways to encode this is just encoding to

12:29.920 --> 12:38.720
encode a real vector in terms of the quantum object okay in terms of a density operator obviously

12:38.720 --> 12:47.440
it is only a formal translation we are not transforming a cat in a photon or a dog in

12:47.440 --> 12:58.160
an electron obviously okay so now once we have translated all the data set of real data in terms

12:58.160 --> 13:07.280
of the data set of quantum data or quantum data again we can consider the distinction between

13:07.920 --> 13:17.680
training and test training and test again and the idea is just to obtain a quantum classifier

13:17.680 --> 13:26.800
that is a function that has as input a quantum object of the quantum test set and has the

13:26.800 --> 13:35.360
output level so given this quantum object that is the density operator that represents an unknown

13:35.360 --> 13:44.400
object we can say that we can we can obtain by the classifier that this object is a dog okay so

13:45.040 --> 13:52.560
obviously what we have to show is that this translation from classical to quantum object

13:53.120 --> 13:58.960
has some benefit provides some benefit otherwise it's totally useless okay so okay

13:59.040 --> 14:09.040
now let us talk regarding the general setting in quantum state discrimination okay that probably

14:09.760 --> 14:18.080
most of you already know but I very briefly summarize here let R be a set of density operator

14:18.080 --> 14:24.000
and suppose that Thelis wishes to communicate information to Bobo by using a quantum system

14:24.000 --> 14:32.720
that is one of the system belonging to this state okay to the same Thelis prepares a quantum system

14:32.720 --> 14:42.800
in one of this state in one of this state and hands the system over to Bobo in principle Bobo has

14:42.800 --> 14:53.520
a complete knowledge about R so Bob knows in knows already knows which are row one row two row

14:53.520 --> 15:03.920
M but he does not know the actual state of the system that he have received by Thelis so in order to

15:03.920 --> 15:15.120
so that the Bob's task is to determine which one is among this state in one shot just by making a

15:15.120 --> 15:26.240
single von Neumann measurement over all possible sorry or possible physical observable so there

15:26.240 --> 15:34.400
and so the possibility is that the measure the outcome of the measurement could be just the

15:34.400 --> 15:41.840
row that Thelis sent to Bob or not or can be another one in the first case okay we say that Bob

15:42.400 --> 15:51.520
succeeds in another case we say that Bob fails okay so what is the probability for Bob to

15:51.520 --> 15:59.440
succeed okay given a set R or quantum state and a von Neumann measurement M the average

15:59.440 --> 16:07.520
probability to Bob for Bob to perform a correct discrimination so to correct individuate which

16:07.520 --> 16:15.760
is the state that was sent from Thelis is given by this quantity is given by this quantity and

16:16.720 --> 16:27.200
the aim of course is the maximize maximize this probability so to minimize the probability to

16:27.200 --> 16:36.160
have an error okay so the question is what what is the measure the measure that maximize the

16:36.160 --> 16:42.880
probability to have the correct discrimination and this measure this measure was obtained by

16:43.840 --> 16:52.560
by this strategy okay the strategy is is related only to the binary case so consider to have just

16:52.560 --> 17:01.520
row one and row two and if we consider this quantity p1 row one minus p2 row two where p1

17:01.520 --> 17:09.760
and p2 are a priori probability okay that can be obtained in some different but empirical way for

17:09.760 --> 17:16.880
instance we obtain this quantity we calculate the positive and negative eigenvalues of this

17:16.880 --> 17:25.760
quantity we consider the respective eigenvectors and we consider the sum of the projector built

17:25.760 --> 17:33.360
over all of each of these eigenvectors in this way we can obtain p plus and p minus

17:33.920 --> 17:41.520
and hence from showered that this p plus and p minus is a von Neumann measurement that is optimal

17:41.520 --> 17:48.000
that is the optimal measure for the discrimination problem for the binary discrimination problem

17:48.000 --> 17:55.200
that we have introduced at the moment so this is the this is the bound this is the bound and

17:56.160 --> 18:02.480
intuitively p plus and p minus represent the property for the system to be correctly

18:02.480 --> 18:09.760
identified as being the state row one or row two respectively okay and that's from bound can be

18:09.760 --> 18:17.840
seen as a measurement of distinguishability between row one and row two so now again with

18:17.840 --> 18:25.920
this light because now we have all the ingredients we need in order to put together this discrimination

18:25.920 --> 18:33.200
not to put together classification problem and quantum state discrimination the first stage

18:33.200 --> 18:41.760
is the encoding we say that we need to have real object and translate the real vector and translate

18:41.760 --> 18:48.880
them in terms of quantum object like a density operators like a pure states and for instance

18:48.880 --> 18:55.520
it is easy to see because there are several way several way to do this this is one simple way for

18:55.520 --> 19:06.160
instance by using the stereographic projection we map each object in this simple case two feature

19:06.160 --> 19:13.840
object in a point over a sphere and of a surface one sphere and that's correspond to the point of

19:13.840 --> 19:21.680
the block sphere so a pure density matrix a pure state a pure state but there are many other

19:21.680 --> 19:29.680
possibilities to encode real object in terms in terms of density matrix pure density matrix okay

19:29.680 --> 19:39.600
so once we do that once we have set of quantum states set of density operators

19:40.240 --> 19:48.240
for each training set we for each training set for each class of the training set we we represent

19:48.240 --> 19:55.280
each class by a quantum centroid that is defined in this way this is just the beginning of our

19:55.360 --> 20:03.040
orbit and it is no longer a pure state of course because it is just the sum of all of the state

20:03.040 --> 20:10.400
of all the encoded vector belonging to a to a given class to a given class okay obviously the

20:10.400 --> 20:16.480
quantum centroid are mixed states are mixed states and they are not the encoding of the

20:16.480 --> 20:22.160
respective classical centroid they are a totally new object a totally new object that

20:22.160 --> 20:29.920
has not any counterpart in the real world in the world of real object in the world of real vectors

20:29.920 --> 20:40.640
okay okay so now we use quantum state discrimination we use the Elstrom measurement

20:40.640 --> 20:48.720
that is optimal so we use p plus and p minus that we built we built by considering

20:52.160 --> 21:00.000
as the two states to discriminate we will consider the two centroids so let us consider

21:00.000 --> 21:06.960
a case where we want to discriminate between cats and dogs we consider the training set of the cats

21:07.600 --> 21:14.400
we are encoding this set in terms of quantum object we consider this you can see that the

21:14.400 --> 21:20.800
centroid of this class we do the same for the dogs and we have these two centroid these two

21:20.800 --> 21:28.160
quantum centroid these two quantum centroid define the finding are two density operator

21:28.160 --> 21:34.560
and from this from these two quantum centroids we can apply the Elstrom

21:37.360 --> 21:44.400
discrimination and we cannot take the Elstrom measurement the optimal Elstrom measurement

21:44.400 --> 21:55.520
p plus and p minus okay so once we pick another quantum object from the test set we have to do

21:55.520 --> 22:02.480
we have to say okay this quantum object is a set or is a dog in order to determine if this

22:03.280 --> 22:10.000
we consider this classifier that is called the Elstrom quantum classifier we consider

22:10.960 --> 22:22.080
these two values if the trace of p plus times rho is greater than p minus times rho we say that

22:23.040 --> 22:32.080
in a certain sense rho x is closer to the centroid of the cat otherwise rho x is closer to

22:32.080 --> 22:41.520
the centroid of of the dog and so we can make a really a proper classification a proper classification

22:41.520 --> 22:50.320
okay why we use quantum state discrimination what is the intuition that that is above this idea

22:50.880 --> 22:56.240
the application of quantum state discrimination for classification is inspired by the idea that

22:56.880 --> 23:05.040
better is the discrimination between two centroid and better I can distinguish between two classes

23:05.040 --> 23:13.120
and so more performance is the classifier in other words greater is the Elstrom bound that

23:13.120 --> 23:18.080
remind me I said it is that can be considered as a measurement of distinguishability between two

23:18.080 --> 23:25.360
centroid and greater would be for instance the accuracy of the classifier where the accuracy

23:25.360 --> 23:34.480
means just the number of times that the classification was correct over the number of total over the

23:34.480 --> 23:44.320
total number of experiments okay okay this idea is not only an intuitive idea but that was just

23:45.280 --> 23:56.000
exploited that was just proven by an experiment so we have provided an experiment where this

23:56.000 --> 24:07.760
classifier has been applied to 40 different data sets and compared with 11 different and

24:07.840 --> 24:14.560
general well-performing classifier standard classifier for instance what we can see is that

24:14.560 --> 24:21.680
generally generally once the Elstrom bound increase the Elstrom bound increases

24:22.400 --> 24:29.760
together with the increasing of the balance of accuracy so this idea seems to be correct

24:29.760 --> 24:36.240
that increasing the measurement of distinguishability between two centroid it

24:37.360 --> 24:44.320
is together with the increasing of the accuracy of the classifier so this intuition is corroborated

24:44.320 --> 24:51.040
by the experience is corroborated by the experience and okay we have many data when we

24:52.000 --> 25:00.000
perform it when we compare this classifier is Elstrom classifier with many standard classifier

25:00.000 --> 25:09.360
and we show that our classifier is generally better not always not always but okay the average

25:12.160 --> 25:20.960
performance of the quantum inspired classifier is very very high with respect to the others

25:20.960 --> 25:27.360
standard classical classifier okay now I have many data to show but okay this is

25:28.240 --> 25:35.440
just a comparison okay are we okay in these data in these tables you can realize these

25:38.320 --> 25:45.200
I don't say supremacy but something like this of our Elstrom classifier with respect to the other

25:45.280 --> 25:51.200
but that's not all that's not all indeed a sharp difference between classical and quantum

25:51.200 --> 25:58.480
information theory is also based on the fact that sometime made considering a tensor copy

25:58.480 --> 26:05.600
of an object can provide a benefit in our computation can provide let's say a kind of

26:05.600 --> 26:12.560
additional information with respect to the original information that is given by the

26:12.560 --> 26:21.120
initial single state so what we do is just repeat exactly the same algorithm that I have described

26:21.120 --> 26:29.440
you above before but by considering but off after the encoding after the encoding instead of the

26:29.440 --> 26:36.320
density operator raw the time obtained from the encoding by the encoding from the initial

26:36.320 --> 26:44.480
object instead of this row I consider all times itself and times I calculate a new centroid

26:44.480 --> 26:53.120
and I define again a new classifier in a new Elstrom of several in the new dimension has been a new

26:53.120 --> 27:01.920
k dimensional space in a new larger space and again I define a new Elstrom quantum classifier

27:02.800 --> 27:12.160
let's say k tensor product Elstrom quantum classifier what we see what we see is that by increasing

27:12.160 --> 27:19.680
the number of the copy also there's strong bound increase and so we see that the accuracy that

27:19.680 --> 27:26.960
we obtain after making this copy this copy in a just in a computational way just by making

27:27.360 --> 27:35.040
a in bi-mathematica okay or by fighting this copy but what we see is that by making this

27:35.040 --> 27:43.600
procedure we have benefit because we increase the accuracy of the of the process we increase

27:43.600 --> 27:52.000
again the accuracy of the process okay and again we have data to to show okay we have something but

27:52.000 --> 28:01.040
okay I want to use just the last five minutes talk maybe to show you a practical experiment

28:01.040 --> 28:09.760
a practical experiment that was provided together with the University of Cambridge and the Institute

28:09.760 --> 28:18.320
of Molecular Bioimaging of the University of Catania and the topic is a chronogenic essay

28:18.320 --> 28:25.600
the chronogenic essay is a quantification technique of the survival degree of an in vitro cell

28:25.600 --> 28:32.720
cultures which is based on the ability of a single cell to grow and form a colony of cell

28:32.720 --> 28:39.440
this colony is not a good thing that is a symbol of something with a of something

28:39.440 --> 28:46.480
disease something great disease the purpose is to count the number of their colonies so

28:46.560 --> 28:52.960
the picture that we are today like picture like this where this is a colony this is a colony

28:52.960 --> 29:02.320
this is a colony and so on okay recent results show how bi-classification between pixel x

29:03.920 --> 29:11.040
belongs to the colony or pixel x belongs to the belongs to the background by making this

29:11.040 --> 29:17.520
classification it is possible to have information about the number of of the colonies there is a

29:17.520 --> 29:26.080
correlation between this this is a biological result in a in a in a quantum imagine and biology

29:27.920 --> 29:36.160
fields that this result but it is it is an assist for us because it allows us to move

29:36.160 --> 29:43.200
the chronogenic essay in a binary classification context so for each pixel if we are able to

29:43.200 --> 29:50.800
classify if this pixel is in the colony or it is in the in the background we can have some result

29:50.800 --> 29:59.520
regarding the number of the code that is in the chronogenic essay and so you have 10 minutes

29:59.600 --> 30:07.040
including the question okay so just I thank you I just spend a few minutes to conclude

30:07.040 --> 30:16.240
and I and I leave a few minutes also for four questions okay okay so the experiment

30:16.240 --> 30:23.520
involved with the many many many data about 10 millions of data and the result that we had

30:23.520 --> 30:30.160
was against that the performance of our elstrom classifier with respect to other in this case

30:30.720 --> 30:38.080
18 well-performance classifier is very good because our classifier in most of the cases

30:38.080 --> 30:46.080
was one of the best one of the best with respect to to the other and that was very very nice for us

30:46.080 --> 30:53.600
so let me conclude with some open problem okay the first open problem is the is this

30:54.560 --> 31:01.440
the generalization of this quantum is pirate quantum is pirate and now we know why I say quantum

31:01.440 --> 31:07.040
is pirate and now we know why I really say that this this inspiration is useful it's not only

31:07.040 --> 31:15.520
a matter of metaphor but this quantum inspiration was based on only a binary quantum state discrimination

31:16.400 --> 31:24.400
so the challenge is to have a multiclass classifier a multiclass classifier and the second goal

31:24.400 --> 31:33.440
should be just to use this quantum inspired algorithm also in a quantum computer just in order

31:33.440 --> 31:42.960
to come from one to be spirited to one so we have some partial result regarding both of these

31:43.520 --> 31:51.120
this point and basically regarding the idea of multiclass classifier we are working on this but

31:52.080 --> 32:00.640
we are we are taking inspiration from the pretty good measurement that is not an optimal but

32:00.640 --> 32:10.080
sub the optimal measurement that allows us to have a multi a quantum multiclassifier

32:10.960 --> 32:17.680
and on the other hand well I am not time to show you the detail of the pretty good

32:17.680 --> 32:23.280
classifier but the idea is the same but instead of to have an optimal we have a sub optimal

32:23.280 --> 32:31.840
measurement but but also we this sub optimal measurement can be used for any classification

32:31.840 --> 32:39.760
instead of only binary classification and also the final challenge as I said is just to

32:41.040 --> 32:48.720
come from a quantum inspired to really quantum machine learning in order to put together the

32:48.720 --> 32:54.880
double benefit of quantum so increase the accuracy but also speed up the computation

32:55.520 --> 33:02.080
well by appealing to the neomarks deletion theorem is it possible in principle to reproduce

33:02.080 --> 33:08.560
and POVM measurement and for instance also the health strong of the pretty good measurement

33:08.640 --> 33:15.600
this is an this is an example but we are working on this and till now we have only only partial

33:15.600 --> 33:24.480
result regarding this but the works are still in progress and okay this is the the bibliography

33:24.480 --> 33:32.080
that the reference that you can refer to and okay thank you all for listening thank you

33:32.640 --> 33:42.480
okay thank you Shusepe for your interest in talk now we have some minutes

33:44.800 --> 33:49.840
questions and comments if you want to make a question please

33:49.840 --> 33:59.120
and I have a little question

34:03.040 --> 34:10.960
for your future but do you know if there is a mathematical relation between the

34:10.960 --> 34:22.880
hand strong bound and the accuracy of your classifier okay so yes are you asking if

34:23.680 --> 34:27.200
there is a connection between a strong bound and accuracy

34:29.840 --> 34:36.000
right okay well basically not basically not because it's very strange because

34:36.960 --> 34:44.400
there are really two topics that are that was never merged together between with within

34:45.680 --> 34:53.440
before now because in a certain sense health strong was not thinking about the classification

34:53.440 --> 34:59.280
problem so in a certain sense health strong well quantum state discrimination

35:00.000 --> 35:08.000
and has not the idea of what is the accuracy because the accuracy could be calculated empirically

35:08.000 --> 35:17.200
empirically only by making the experiment what the what the theoretically we have is this result

35:17.200 --> 35:24.160
is this result that is interesting regarding that strong bound and it is easy to misleading

35:24.240 --> 35:32.160
because easy to misleading because we are not saying that if okay we are not saying this

35:32.160 --> 35:38.880
that strong bound between two states is less of that strong bound between the states

35:38.880 --> 35:44.560
that you obtain by making the tensor copy of itself by itself because in this case it is

35:45.360 --> 35:52.400
an already well known result here we are saying this something different because this object

35:52.400 --> 36:05.600
our quantum centroid this tensor k row is not row times row times row k times k times it is

36:05.600 --> 36:16.000
something different is the centroid that we obtain by making in principle k copies of each

36:16.000 --> 36:24.800
object of the data set and after making the centroid so in a certain sense this is a first

36:24.800 --> 36:32.480
let me say that theoretical result that put together quantum state discrimination and

36:32.480 --> 36:38.000
classification problem because there is the idea of centroid and there is the idea of

36:38.000 --> 36:47.360
a strong bound put in together i hope to answer to your question thank you very much welcome

36:50.000 --> 36:51.600
another question or comment

36:54.160 --> 37:00.000
if not thank you again for your talk thank you all thank you

