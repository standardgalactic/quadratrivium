{"text": " Okay, we'll start with the second part of the first session this morning. The first talk is in charge of Schubert Cercione from the University of Caglione. Thank you, thank you very much and thank you to all the organizers who invited me. For me it's very proud and very proud to be invited to this 10th conference of Quantum Foundation. I'm very happy to be part of this very prominent panel of scientists. I warmly thank all the organizers and especially Federico Olic and Martin Bosic, who are also a strict friend of Italian University and of Cagliari and also personal friends. So what I want to introduce here is Quantum Information Meet Smashing Learning. So basically I like to introduce generally this topic by starting from a provocative question. I ask you, can you show me a picture of your Christmas 2018? I am sure that you can and I am sure that if you try to find a picture of your Christmas, for instance in your mobile, you will find it but probably you need time to find it just because now we use to make a lot a lot of pics with our mobile because the memory that our mobile has now is very, very huge and many, many pics can be kept, can be considered, can be stored in a cell for instance in a mobile. So what we can say is that we can, can we say that many data corresponds to many information? Basically not because information means recover from the data what we really need. So the fact that now we have to deal with a very, very huge amount of information means that potentially we can have a lot of information but the problem is the process we need to do in order to extrapolate this information from this big amount of data. So this is the reason why in the last decades big data science corresponds to face with just this kind of process, this kind of problem and so the problem is how to manage huge amount of data in order to retrieve in the short time as possible the information we need, the information we need. Okay so this is why big data born in the in the in the last decade. So this is just a very brief summary of what I said in these five minutes. So in other words, enlarging the dimension of the data set is a good thing because it increases the potential information but it's also bad because it makes the extraction of the required information out. So from another side you know very well that you know very well how quantum information born, how quantum information became in the recent past a really new discipline of quantum theory in general, quantum computing mostly, and we know that the advantage of quantum computation are most of the the advantages are most of all based on the fact that we have a speed up of the computation. We can obtain, we can perform a computer and strong and hard computation in a very very shorter, a shorter time with respect to a classical, a classical one. Okay so so we know also that quantum computation is not so futuristic. We can say that till 10 maybe 20 years ago we were speaking about quantum computation as something that was phantasyntific, that was not really concrete, but we know that the application in quantum computation has a very very speed up in the last in the last decade and now we know that the quantum processor already exists and we know that also by using for instance our our computer in our house we can connect with the IBM quantum experience and we can just design a quantum algorithm and by a click we can send this quantum algorithm to a real quantum computer that will perform it, that will give us back the result as a as a statistical result. So basically the quantum computation is now real, is now real and also we know that this this topic is is going on and on and for instance just one year ago, just one year ago in in china that was there was was declared that the achievement of quantum supremacy because it was possible to perform a calculation in 200 seconds, a kind of calculation that will take more than half a billion years on the world's faster, no quantum or classical computer. So now we really trust in quantum computation, we really trust in this. So we were speaking about the data and about quantum computation but because well summarizing the main problem of big data when machine learning with big data is the time complexity of the computation but the main advantage of quantum computation is the possibility to reproduce to use the time complexity. So in a certain sense, quantum computation and machine learning appears as a very natural and maybe unavoidable, unavoidable connection but the connection can be deeper and deeper with respect to what we can think. Indeed the question that they want to ask you in this in this talk is can quantum information theory help classical machine learning, classical by using classical computer only and we will provide a positive answer to this question. So this approach that we will speak is known as a quantum-inspired machine learning and the important thing is that well very often we know that in many disciplines we speak about the inspiration from quantum mechanics. Okay now what I want to focus on is the fact that our inspiration is not just an analogy, it's not a metaphor but it's just a really inspiration regarding a real inspiration to quantum work in order to have a real benefit in machine learning and in the next minutes I aim to show you what they mean by real inspiration. Okay in particular we focus on the classification problem. What is the classification problem? Okay the general idea is that the quantum classification in the idea that we want to perform is the idea of a quantum classifier where quantum classification is the idea of quantum classification is to formalize the standard classification problems in terms of the mathematical quantum object and then inspired by certain properties of quantum state discrimination we define a quantum classifier that will provide interesting and real good advantages. The procedure is based on three fundamental steps that we will describe not now but a few slides later encoding classification and the coding but we will meet this slide again in a bit. First of all I want to very very quickly summarize what is the sorry the general framework of classification problem. So when we have to classify an object we describe this object as a vector where each component of this vector is a feature of this object like this object could be a cat and this feature could be the length of the tail, the weight of the cat and so on and we have different classes of objects for instance dog and cats and what and so on and this so a pattern is represented by a pair where this is the vector the object represented by a vector and this is the label that characterized the class for instance class for instance label one means class of the cat, label two means class of the dogs for instance okay and the goal of the classification is to have a classifier I mean a function classifier is a function that allows us to study the vector and gives as an output the label so looking at the vector looking at the object I can say you if this object is a cat or is a dog basically okay okay okay so in a general supervised scenario I will be very very very I will skip many slides but but sometime I can focus on some technical data sometimes not generally generally in what we say supervised learning we mean that we consider a data set for instance a data set of dogs and a data set of cats for instance and we divide this data set at the beginning in two parts the first part is given by the training set this is the set useful in order to train our algorithm and this is the test set but the set is a set of object that we use in order to know whether our algorithm is good or not just in order to know just in order to measure the performance of our algorithm and so obviously then all of these objects are already labelled so we already know the level of the object so the goal is just to evaluate how the right if the classifier is good or not so we use a already labelled set of object we divide it in training and test and obviously also the training is divided with respect to the classes the classes will get the classes of dogs the classes of foxes and so on and so on okay so the the first idea of the quantum classification is just to translate formally translate each object each vector each cat that is represented by a vector in terms of a quantum object that we represent by a density operator row and there are many many ways to encode this is just encoding to encode a real vector in terms of the quantum object okay in terms of a density operator obviously it is only a formal translation we are not transforming a cat in a photon or a dog in an electron obviously okay so now once we have translated all the data set of real data in terms of the data set of quantum data or quantum data again we can consider the distinction between training and test training and test again and the idea is just to obtain a quantum classifier that is a function that has as input a quantum object of the quantum test set and has the output level so given this quantum object that is the density operator that represents an unknown object we can say that we can we can obtain by the classifier that this object is a dog okay so obviously what we have to show is that this translation from classical to quantum object has some benefit provides some benefit otherwise it's totally useless okay so okay now let us talk regarding the general setting in quantum state discrimination okay that probably most of you already know but I very briefly summarize here let R be a set of density operator and suppose that Thelis wishes to communicate information to Bobo by using a quantum system that is one of the system belonging to this state okay to the same Thelis prepares a quantum system in one of this state in one of this state and hands the system over to Bobo in principle Bobo has a complete knowledge about R so Bob knows in knows already knows which are row one row two row M but he does not know the actual state of the system that he have received by Thelis so in order to so that the Bob's task is to determine which one is among this state in one shot just by making a single von Neumann measurement over all possible sorry or possible physical observable so there and so the possibility is that the measure the outcome of the measurement could be just the row that Thelis sent to Bob or not or can be another one in the first case okay we say that Bob succeeds in another case we say that Bob fails okay so what is the probability for Bob to succeed okay given a set R or quantum state and a von Neumann measurement M the average probability to Bob for Bob to perform a correct discrimination so to correct individuate which is the state that was sent from Thelis is given by this quantity is given by this quantity and the aim of course is the maximize maximize this probability so to minimize the probability to have an error okay so the question is what what is the measure the measure that maximize the probability to have the correct discrimination and this measure this measure was obtained by by this strategy okay the strategy is is related only to the binary case so consider to have just row one and row two and if we consider this quantity p1 row one minus p2 row two where p1 and p2 are a priori probability okay that can be obtained in some different but empirical way for instance we obtain this quantity we calculate the positive and negative eigenvalues of this quantity we consider the respective eigenvectors and we consider the sum of the projector built over all of each of these eigenvectors in this way we can obtain p plus and p minus and hence from showered that this p plus and p minus is a von Neumann measurement that is optimal that is the optimal measure for the discrimination problem for the binary discrimination problem that we have introduced at the moment so this is the this is the bound this is the bound and intuitively p plus and p minus represent the property for the system to be correctly identified as being the state row one or row two respectively okay and that's from bound can be seen as a measurement of distinguishability between row one and row two so now again with this light because now we have all the ingredients we need in order to put together this discrimination not to put together classification problem and quantum state discrimination the first stage is the encoding we say that we need to have real object and translate the real vector and translate them in terms of quantum object like a density operators like a pure states and for instance it is easy to see because there are several way several way to do this this is one simple way for instance by using the stereographic projection we map each object in this simple case two feature object in a point over a sphere and of a surface one sphere and that's correspond to the point of the block sphere so a pure density matrix a pure state a pure state but there are many other possibilities to encode real object in terms in terms of density matrix pure density matrix okay so once we do that once we have set of quantum states set of density operators for each training set we for each training set for each class of the training set we we represent each class by a quantum centroid that is defined in this way this is just the beginning of our orbit and it is no longer a pure state of course because it is just the sum of all of the state of all the encoded vector belonging to a to a given class to a given class okay obviously the quantum centroid are mixed states are mixed states and they are not the encoding of the respective classical centroid they are a totally new object a totally new object that has not any counterpart in the real world in the world of real object in the world of real vectors okay okay so now we use quantum state discrimination we use the Elstrom measurement that is optimal so we use p plus and p minus that we built we built by considering as the two states to discriminate we will consider the two centroids so let us consider a case where we want to discriminate between cats and dogs we consider the training set of the cats we are encoding this set in terms of quantum object we consider this you can see that the centroid of this class we do the same for the dogs and we have these two centroid these two quantum centroid these two quantum centroid define the finding are two density operator and from this from these two quantum centroids we can apply the Elstrom discrimination and we cannot take the Elstrom measurement the optimal Elstrom measurement p plus and p minus okay so once we pick another quantum object from the test set we have to do we have to say okay this quantum object is a set or is a dog in order to determine if this we consider this classifier that is called the Elstrom quantum classifier we consider these two values if the trace of p plus times rho is greater than p minus times rho we say that in a certain sense rho x is closer to the centroid of the cat otherwise rho x is closer to the centroid of of the dog and so we can make a really a proper classification a proper classification okay why we use quantum state discrimination what is the intuition that that is above this idea the application of quantum state discrimination for classification is inspired by the idea that better is the discrimination between two centroid and better I can distinguish between two classes and so more performance is the classifier in other words greater is the Elstrom bound that remind me I said it is that can be considered as a measurement of distinguishability between two centroid and greater would be for instance the accuracy of the classifier where the accuracy means just the number of times that the classification was correct over the number of total over the total number of experiments okay okay this idea is not only an intuitive idea but that was just exploited that was just proven by an experiment so we have provided an experiment where this classifier has been applied to 40 different data sets and compared with 11 different and general well-performing classifier standard classifier for instance what we can see is that generally generally once the Elstrom bound increase the Elstrom bound increases together with the increasing of the balance of accuracy so this idea seems to be correct that increasing the measurement of distinguishability between two centroid it is together with the increasing of the accuracy of the classifier so this intuition is corroborated by the experience is corroborated by the experience and okay we have many data when we perform it when we compare this classifier is Elstrom classifier with many standard classifier and we show that our classifier is generally better not always not always but okay the average performance of the quantum inspired classifier is very very high with respect to the others standard classical classifier okay now I have many data to show but okay this is just a comparison okay are we okay in these data in these tables you can realize these I don't say supremacy but something like this of our Elstrom classifier with respect to the other but that's not all that's not all indeed a sharp difference between classical and quantum information theory is also based on the fact that sometime made considering a tensor copy of an object can provide a benefit in our computation can provide let's say a kind of additional information with respect to the original information that is given by the initial single state so what we do is just repeat exactly the same algorithm that I have described you above before but by considering but off after the encoding after the encoding instead of the density operator raw the time obtained from the encoding by the encoding from the initial object instead of this row I consider all times itself and times I calculate a new centroid and I define again a new classifier in a new Elstrom of several in the new dimension has been a new k dimensional space in a new larger space and again I define a new Elstrom quantum classifier let's say k tensor product Elstrom quantum classifier what we see what we see is that by increasing the number of the copy also there's strong bound increase and so we see that the accuracy that we obtain after making this copy this copy in a just in a computational way just by making a in bi-mathematica okay or by fighting this copy but what we see is that by making this procedure we have benefit because we increase the accuracy of the of the process we increase again the accuracy of the process okay and again we have data to to show okay we have something but okay I want to use just the last five minutes talk maybe to show you a practical experiment a practical experiment that was provided together with the University of Cambridge and the Institute of Molecular Bioimaging of the University of Catania and the topic is a chronogenic essay the chronogenic essay is a quantification technique of the survival degree of an in vitro cell cultures which is based on the ability of a single cell to grow and form a colony of cell this colony is not a good thing that is a symbol of something with a of something disease something great disease the purpose is to count the number of their colonies so the picture that we are today like picture like this where this is a colony this is a colony this is a colony and so on okay recent results show how bi-classification between pixel x belongs to the colony or pixel x belongs to the belongs to the background by making this classification it is possible to have information about the number of of the colonies there is a correlation between this this is a biological result in a in a in a quantum imagine and biology fields that this result but it is it is an assist for us because it allows us to move the chronogenic essay in a binary classification context so for each pixel if we are able to classify if this pixel is in the colony or it is in the in the background we can have some result regarding the number of the code that is in the chronogenic essay and so you have 10 minutes including the question okay so just I thank you I just spend a few minutes to conclude and I and I leave a few minutes also for four questions okay okay so the experiment involved with the many many many data about 10 millions of data and the result that we had was against that the performance of our elstrom classifier with respect to other in this case 18 well-performance classifier is very good because our classifier in most of the cases was one of the best one of the best with respect to to the other and that was very very nice for us so let me conclude with some open problem okay the first open problem is the is this the generalization of this quantum is pirate quantum is pirate and now we know why I say quantum is pirate and now we know why I really say that this this inspiration is useful it's not only a matter of metaphor but this quantum inspiration was based on only a binary quantum state discrimination so the challenge is to have a multiclass classifier a multiclass classifier and the second goal should be just to use this quantum inspired algorithm also in a quantum computer just in order to come from one to be spirited to one so we have some partial result regarding both of these this point and basically regarding the idea of multiclass classifier we are working on this but we are we are taking inspiration from the pretty good measurement that is not an optimal but sub the optimal measurement that allows us to have a multi a quantum multiclassifier and on the other hand well I am not time to show you the detail of the pretty good classifier but the idea is the same but instead of to have an optimal we have a sub optimal measurement but but also we this sub optimal measurement can be used for any classification instead of only binary classification and also the final challenge as I said is just to come from a quantum inspired to really quantum machine learning in order to put together the double benefit of quantum so increase the accuracy but also speed up the computation well by appealing to the neomarks deletion theorem is it possible in principle to reproduce and POVM measurement and for instance also the health strong of the pretty good measurement this is an this is an example but we are working on this and till now we have only only partial result regarding this but the works are still in progress and okay this is the the bibliography that the reference that you can refer to and okay thank you all for listening thank you okay thank you Shusepe for your interest in talk now we have some minutes questions and comments if you want to make a question please and I have a little question for your future but do you know if there is a mathematical relation between the hand strong bound and the accuracy of your classifier okay so yes are you asking if there is a connection between a strong bound and accuracy right okay well basically not basically not because it's very strange because there are really two topics that are that was never merged together between with within before now because in a certain sense health strong was not thinking about the classification problem so in a certain sense health strong well quantum state discrimination and has not the idea of what is the accuracy because the accuracy could be calculated empirically empirically only by making the experiment what the what the theoretically we have is this result is this result that is interesting regarding that strong bound and it is easy to misleading because easy to misleading because we are not saying that if okay we are not saying this that strong bound between two states is less of that strong bound between the states that you obtain by making the tensor copy of itself by itself because in this case it is an already well known result here we are saying this something different because this object our quantum centroid this tensor k row is not row times row times row k times k times it is something different is the centroid that we obtain by making in principle k copies of each object of the data set and after making the centroid so in a certain sense this is a first let me say that theoretical result that put together quantum state discrimination and classification problem because there is the idea of centroid and there is the idea of a strong bound put in together i hope to answer to your question thank you very much welcome another question or comment if not thank you again for your talk thank you all thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.84, "text": " Okay, we'll start with the second part of the first session this morning.", "tokens": [50364, 1033, 11, 321, 603, 722, 365, 264, 1150, 644, 295, 264, 700, 5481, 341, 2446, 13, 50856], "temperature": 0.0, "avg_logprob": -0.38701309877283435, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.08146059513092041}, {"id": 1, "seek": 0, "start": 9.84, "end": 17.28, "text": " The first talk is in charge of Schubert Cercione from the University of Caglione.", "tokens": [50856, 440, 700, 751, 307, 294, 4602, 295, 2065, 84, 4290, 383, 2869, 5328, 490, 264, 3535, 295, 383, 559, 75, 5328, 13, 51228], "temperature": 0.0, "avg_logprob": -0.38701309877283435, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.08146059513092041}, {"id": 2, "seek": 0, "start": 18.56, "end": 24.48, "text": " Thank you, thank you very much and thank you to all the organizers who invited me. For me it's", "tokens": [51292, 1044, 291, 11, 1309, 291, 588, 709, 293, 1309, 291, 281, 439, 264, 35071, 567, 9185, 385, 13, 1171, 385, 309, 311, 51588], "temperature": 0.0, "avg_logprob": -0.38701309877283435, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.08146059513092041}, {"id": 3, "seek": 2448, "start": 24.48, "end": 31.52, "text": " very proud and very proud to be invited to this 10th conference of Quantum Foundation.", "tokens": [50364, 588, 4570, 293, 588, 4570, 281, 312, 9185, 281, 341, 1266, 392, 7586, 295, 44964, 10335, 13, 50716], "temperature": 0.0, "avg_logprob": -0.22982857043926533, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.22336021065711975}, {"id": 4, "seek": 2448, "start": 32.08, "end": 41.28, "text": " I'm very happy to be part of this very prominent panel of scientists. I warmly thank all the", "tokens": [50744, 286, 478, 588, 2055, 281, 312, 644, 295, 341, 588, 17034, 4831, 295, 7708, 13, 286, 4561, 356, 1309, 439, 264, 51204], "temperature": 0.0, "avg_logprob": -0.22982857043926533, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.22336021065711975}, {"id": 5, "seek": 2448, "start": 41.28, "end": 49.6, "text": " organizers and especially Federico Olic and Martin Bosic, who are also a strict friend of", "tokens": [51204, 35071, 293, 2318, 45545, 2789, 422, 1050, 293, 9184, 22264, 299, 11, 567, 366, 611, 257, 10910, 1277, 295, 51620], "temperature": 0.0, "avg_logprob": -0.22982857043926533, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.22336021065711975}, {"id": 6, "seek": 4960, "start": 49.6, "end": 57.6, "text": " Italian University and of Cagliari and also personal friends. So what I want to introduce", "tokens": [50364, 10003, 3535, 293, 295, 383, 559, 2081, 3504, 293, 611, 2973, 1855, 13, 407, 437, 286, 528, 281, 5366, 50764], "temperature": 0.0, "avg_logprob": -0.2132849163479275, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0772911012172699}, {"id": 7, "seek": 4960, "start": 57.6, "end": 64.8, "text": " here is Quantum Information Meet Smashing Learning. So basically I like to introduce", "tokens": [50764, 510, 307, 44964, 15357, 22963, 318, 3799, 571, 15205, 13, 407, 1936, 286, 411, 281, 5366, 51124], "temperature": 0.0, "avg_logprob": -0.2132849163479275, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0772911012172699}, {"id": 8, "seek": 4960, "start": 64.8, "end": 73.28, "text": " generally this topic by starting from a provocative question. I ask you, can you show me a picture", "tokens": [51124, 5101, 341, 4829, 538, 2891, 490, 257, 47663, 1168, 13, 286, 1029, 291, 11, 393, 291, 855, 385, 257, 3036, 51548], "temperature": 0.0, "avg_logprob": -0.2132849163479275, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0772911012172699}, {"id": 9, "seek": 7328, "start": 73.28, "end": 84.8, "text": " of your Christmas 2018? I am sure that you can and I am sure that if you try to find a picture", "tokens": [50364, 295, 428, 5272, 6096, 30, 286, 669, 988, 300, 291, 393, 293, 286, 669, 988, 300, 498, 291, 853, 281, 915, 257, 3036, 50940], "temperature": 0.0, "avg_logprob": -0.11543720298343235, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.16354060173034668}, {"id": 10, "seek": 7328, "start": 84.8, "end": 90.96000000000001, "text": " of your Christmas, for instance in your mobile, you will find it but probably you need time to", "tokens": [50940, 295, 428, 5272, 11, 337, 5197, 294, 428, 6013, 11, 291, 486, 915, 309, 457, 1391, 291, 643, 565, 281, 51248], "temperature": 0.0, "avg_logprob": -0.11543720298343235, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.16354060173034668}, {"id": 11, "seek": 7328, "start": 90.96000000000001, "end": 98.8, "text": " find it just because now we use to make a lot a lot of pics with our mobile because the memory", "tokens": [51248, 915, 309, 445, 570, 586, 321, 764, 281, 652, 257, 688, 257, 688, 295, 46690, 365, 527, 6013, 570, 264, 4675, 51640], "temperature": 0.0, "avg_logprob": -0.11543720298343235, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.16354060173034668}, {"id": 12, "seek": 9880, "start": 98.8, "end": 108.72, "text": " that our mobile has now is very, very huge and many, many pics can be kept, can be considered,", "tokens": [50364, 300, 527, 6013, 575, 586, 307, 588, 11, 588, 2603, 293, 867, 11, 867, 46690, 393, 312, 4305, 11, 393, 312, 4888, 11, 50860], "temperature": 0.0, "avg_logprob": -0.17558959886139514, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.020826183259487152}, {"id": 13, "seek": 9880, "start": 108.72, "end": 121.75999999999999, "text": " can be stored in a cell for instance in a mobile. So what we can say is that we can,", "tokens": [50860, 393, 312, 12187, 294, 257, 2815, 337, 5197, 294, 257, 6013, 13, 407, 437, 321, 393, 584, 307, 300, 321, 393, 11, 51512], "temperature": 0.0, "avg_logprob": -0.17558959886139514, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.020826183259487152}, {"id": 14, "seek": 12176, "start": 121.76, "end": 130.0, "text": " can we say that many data corresponds to many information? Basically not because information", "tokens": [50364, 393, 321, 584, 300, 867, 1412, 23249, 281, 867, 1589, 30, 8537, 406, 570, 1589, 50776], "temperature": 0.0, "avg_logprob": -0.06377300278085177, "compression_ratio": 1.701219512195122, "no_speech_prob": 0.022024240344762802}, {"id": 15, "seek": 12176, "start": 130.0, "end": 139.20000000000002, "text": " means recover from the data what we really need. So the fact that now we have to deal with a very,", "tokens": [50776, 1355, 8114, 490, 264, 1412, 437, 321, 534, 643, 13, 407, 264, 1186, 300, 586, 321, 362, 281, 2028, 365, 257, 588, 11, 51236], "temperature": 0.0, "avg_logprob": -0.06377300278085177, "compression_ratio": 1.701219512195122, "no_speech_prob": 0.022024240344762802}, {"id": 16, "seek": 12176, "start": 139.20000000000002, "end": 146.8, "text": " very huge amount of information means that potentially we can have a lot of information", "tokens": [51236, 588, 2603, 2372, 295, 1589, 1355, 300, 7263, 321, 393, 362, 257, 688, 295, 1589, 51616], "temperature": 0.0, "avg_logprob": -0.06377300278085177, "compression_ratio": 1.701219512195122, "no_speech_prob": 0.022024240344762802}, {"id": 17, "seek": 14680, "start": 146.8, "end": 154.96, "text": " but the problem is the process we need to do in order to extrapolate this information", "tokens": [50364, 457, 264, 1154, 307, 264, 1399, 321, 643, 281, 360, 294, 1668, 281, 48224, 473, 341, 1589, 50772], "temperature": 0.0, "avg_logprob": -0.13452343608057776, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.012236036360263824}, {"id": 18, "seek": 14680, "start": 154.96, "end": 166.56, "text": " from this big amount of data. So this is the reason why in the last decades big data science", "tokens": [50772, 490, 341, 955, 2372, 295, 1412, 13, 407, 341, 307, 264, 1778, 983, 294, 264, 1036, 7878, 955, 1412, 3497, 51352], "temperature": 0.0, "avg_logprob": -0.13452343608057776, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.012236036360263824}, {"id": 19, "seek": 16656, "start": 166.56, "end": 174.88, "text": " corresponds to face with just this kind of process, this kind of problem and so the problem is how", "tokens": [50364, 23249, 281, 1851, 365, 445, 341, 733, 295, 1399, 11, 341, 733, 295, 1154, 293, 370, 264, 1154, 307, 577, 50780], "temperature": 0.0, "avg_logprob": -0.22597444230231686, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.04825223609805107}, {"id": 20, "seek": 16656, "start": 174.88, "end": 183.68, "text": " to manage huge amount of data in order to retrieve in the short time as possible the information", "tokens": [50780, 281, 3067, 2603, 2372, 295, 1412, 294, 1668, 281, 30254, 294, 264, 2099, 565, 382, 1944, 264, 1589, 51220], "temperature": 0.0, "avg_logprob": -0.22597444230231686, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.04825223609805107}, {"id": 21, "seek": 16656, "start": 183.68, "end": 191.68, "text": " we need, the information we need. Okay so this is why big data born in the in the in the last", "tokens": [51220, 321, 643, 11, 264, 1589, 321, 643, 13, 1033, 370, 341, 307, 983, 955, 1412, 4232, 294, 264, 294, 264, 294, 264, 1036, 51620], "temperature": 0.0, "avg_logprob": -0.22597444230231686, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.04825223609805107}, {"id": 22, "seek": 19168, "start": 191.68, "end": 197.76000000000002, "text": " decade. So this is just a very brief summary of what I said in these five minutes. So in other", "tokens": [50364, 10378, 13, 407, 341, 307, 445, 257, 588, 5353, 12691, 295, 437, 286, 848, 294, 613, 1732, 2077, 13, 407, 294, 661, 50668], "temperature": 0.0, "avg_logprob": -0.15231855710347494, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.021825555711984634}, {"id": 23, "seek": 19168, "start": 197.76000000000002, "end": 204.0, "text": " words, enlarging the dimension of the data set is a good thing because it increases the potential", "tokens": [50668, 2283, 11, 31976, 3249, 264, 10139, 295, 264, 1412, 992, 307, 257, 665, 551, 570, 309, 8637, 264, 3995, 50980], "temperature": 0.0, "avg_logprob": -0.15231855710347494, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.021825555711984634}, {"id": 24, "seek": 19168, "start": 204.0, "end": 210.0, "text": " information but it's also bad because it makes the extraction of the required information out.", "tokens": [50980, 1589, 457, 309, 311, 611, 1578, 570, 309, 1669, 264, 30197, 295, 264, 4739, 1589, 484, 13, 51280], "temperature": 0.0, "avg_logprob": -0.15231855710347494, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.021825555711984634}, {"id": 25, "seek": 19168, "start": 211.04000000000002, "end": 219.60000000000002, "text": " So from another side you know very well that you know very well how quantum information", "tokens": [51332, 407, 490, 1071, 1252, 291, 458, 588, 731, 300, 291, 458, 588, 731, 577, 13018, 1589, 51760], "temperature": 0.0, "avg_logprob": -0.15231855710347494, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.021825555711984634}, {"id": 26, "seek": 21960, "start": 220.32, "end": 226.72, "text": " born, how quantum information became in the recent past a really new discipline of", "tokens": [50400, 4232, 11, 577, 13018, 1589, 3062, 294, 264, 5162, 1791, 257, 534, 777, 13635, 295, 50720], "temperature": 0.0, "avg_logprob": -0.14356379673398775, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.02627025544643402}, {"id": 27, "seek": 21960, "start": 230.72, "end": 236.07999999999998, "text": " quantum theory in general, quantum computing mostly, and we know that the advantage of", "tokens": [50920, 13018, 5261, 294, 2674, 11, 13018, 15866, 5240, 11, 293, 321, 458, 300, 264, 5002, 295, 51188], "temperature": 0.0, "avg_logprob": -0.14356379673398775, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.02627025544643402}, {"id": 28, "seek": 21960, "start": 236.07999999999998, "end": 243.35999999999999, "text": " quantum computation are most of the the advantages are most of all based on the fact that we have", "tokens": [51188, 13018, 24903, 366, 881, 295, 264, 264, 14906, 366, 881, 295, 439, 2361, 322, 264, 1186, 300, 321, 362, 51552], "temperature": 0.0, "avg_logprob": -0.14356379673398775, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.02627025544643402}, {"id": 29, "seek": 24336, "start": 243.44000000000003, "end": 247.60000000000002, "text": " a speed up of the computation. We can obtain, we can", "tokens": [50368, 257, 3073, 493, 295, 264, 24903, 13, 492, 393, 12701, 11, 321, 393, 50576], "temperature": 0.0, "avg_logprob": -0.21453472605922766, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.015759659931063652}, {"id": 30, "seek": 24336, "start": 250.0, "end": 256.8, "text": " perform a computer and strong and hard computation in a very very shorter, a shorter time", "tokens": [50696, 2042, 257, 3820, 293, 2068, 293, 1152, 24903, 294, 257, 588, 588, 11639, 11, 257, 11639, 565, 51036], "temperature": 0.0, "avg_logprob": -0.21453472605922766, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.015759659931063652}, {"id": 31, "seek": 24336, "start": 256.8, "end": 268.24, "text": " with respect to a classical, a classical one. Okay so so we know also that quantum computation", "tokens": [51036, 365, 3104, 281, 257, 13735, 11, 257, 13735, 472, 13, 1033, 370, 370, 321, 458, 611, 300, 13018, 24903, 51608], "temperature": 0.0, "avg_logprob": -0.21453472605922766, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.015759659931063652}, {"id": 32, "seek": 26824, "start": 268.24, "end": 276.48, "text": " is not so futuristic. We can say that till 10 maybe 20 years ago we were speaking about", "tokens": [50364, 307, 406, 370, 44932, 13, 492, 393, 584, 300, 4288, 1266, 1310, 945, 924, 2057, 321, 645, 4124, 466, 50776], "temperature": 0.0, "avg_logprob": -0.19187366671678496, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.026500720530748367}, {"id": 33, "seek": 26824, "start": 276.48, "end": 282.72, "text": " quantum computation as something that was phantasyntific, that was not really concrete,", "tokens": [50776, 13018, 24903, 382, 746, 300, 390, 903, 394, 5871, 580, 1089, 11, 300, 390, 406, 534, 9859, 11, 51088], "temperature": 0.0, "avg_logprob": -0.19187366671678496, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.026500720530748367}, {"id": 34, "seek": 26824, "start": 282.72, "end": 290.48, "text": " but we know that the application in quantum computation has a very very speed up in the", "tokens": [51088, 457, 321, 458, 300, 264, 3861, 294, 13018, 24903, 575, 257, 588, 588, 3073, 493, 294, 264, 51476], "temperature": 0.0, "avg_logprob": -0.19187366671678496, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.026500720530748367}, {"id": 35, "seek": 26824, "start": 290.48, "end": 296.88, "text": " last in the last decade and now we know that the quantum processor already exists and we know", "tokens": [51476, 1036, 294, 264, 1036, 10378, 293, 586, 321, 458, 300, 264, 13018, 15321, 1217, 8198, 293, 321, 458, 51796], "temperature": 0.0, "avg_logprob": -0.19187366671678496, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.026500720530748367}, {"id": 36, "seek": 29688, "start": 296.88, "end": 304.56, "text": " that also by using for instance our our computer in our house we can connect with the IBM quantum", "tokens": [50364, 300, 611, 538, 1228, 337, 5197, 527, 527, 3820, 294, 527, 1782, 321, 393, 1745, 365, 264, 23487, 13018, 50748], "temperature": 0.0, "avg_logprob": -0.09878367469424293, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.006121047772467136}, {"id": 37, "seek": 29688, "start": 304.56, "end": 311.68, "text": " experience and we can just design a quantum algorithm and by a click we can send this", "tokens": [50748, 1752, 293, 321, 393, 445, 1715, 257, 13018, 9284, 293, 538, 257, 2052, 321, 393, 2845, 341, 51104], "temperature": 0.0, "avg_logprob": -0.09878367469424293, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.006121047772467136}, {"id": 38, "seek": 29688, "start": 311.68, "end": 319.6, "text": " quantum algorithm to a real quantum computer that will perform it, that will give us back the result", "tokens": [51104, 13018, 9284, 281, 257, 957, 13018, 3820, 300, 486, 2042, 309, 11, 300, 486, 976, 505, 646, 264, 1874, 51500], "temperature": 0.0, "avg_logprob": -0.09878367469424293, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.006121047772467136}, {"id": 39, "seek": 31960, "start": 319.6, "end": 328.16, "text": " as a as a statistical result. So basically the quantum computation is now real, is now real", "tokens": [50364, 382, 257, 382, 257, 22820, 1874, 13, 407, 1936, 264, 13018, 24903, 307, 586, 957, 11, 307, 586, 957, 50792], "temperature": 0.0, "avg_logprob": -0.21632738411426544, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.03574339300394058}, {"id": 40, "seek": 31960, "start": 328.16, "end": 337.36, "text": " and also we know that this this topic is is going on and on and for instance just one year ago,", "tokens": [50792, 293, 611, 321, 458, 300, 341, 341, 4829, 307, 307, 516, 322, 293, 322, 293, 337, 5197, 445, 472, 1064, 2057, 11, 51252], "temperature": 0.0, "avg_logprob": -0.21632738411426544, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.03574339300394058}, {"id": 41, "seek": 31960, "start": 337.36, "end": 343.6, "text": " just one year ago in in china that was there was was declared that the", "tokens": [51252, 445, 472, 1064, 2057, 294, 294, 43668, 300, 390, 456, 390, 390, 15489, 300, 264, 51564], "temperature": 0.0, "avg_logprob": -0.21632738411426544, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.03574339300394058}, {"id": 42, "seek": 34360, "start": 343.6, "end": 349.52000000000004, "text": " achievement of quantum supremacy because it was possible to perform a calculation in", "tokens": [50364, 15838, 295, 13018, 35572, 570, 309, 390, 1944, 281, 2042, 257, 17108, 294, 50660], "temperature": 0.0, "avg_logprob": -0.16690850257873535, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.012462198734283447}, {"id": 43, "seek": 34360, "start": 350.56, "end": 358.0, "text": " 200 seconds, a kind of calculation that will take more than half a billion years on the", "tokens": [50712, 2331, 3949, 11, 257, 733, 295, 17108, 300, 486, 747, 544, 813, 1922, 257, 5218, 924, 322, 264, 51084], "temperature": 0.0, "avg_logprob": -0.16690850257873535, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.012462198734283447}, {"id": 44, "seek": 34360, "start": 358.0, "end": 366.32000000000005, "text": " world's faster, no quantum or classical computer. So now we really trust in quantum computation,", "tokens": [51084, 1002, 311, 4663, 11, 572, 13018, 420, 13735, 3820, 13, 407, 586, 321, 534, 3361, 294, 13018, 24903, 11, 51500], "temperature": 0.0, "avg_logprob": -0.16690850257873535, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.012462198734283447}, {"id": 45, "seek": 34360, "start": 366.32000000000005, "end": 373.44, "text": " we really trust in this. So we were speaking about the data and about quantum computation", "tokens": [51500, 321, 534, 3361, 294, 341, 13, 407, 321, 645, 4124, 466, 264, 1412, 293, 466, 13018, 24903, 51856], "temperature": 0.0, "avg_logprob": -0.16690850257873535, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.012462198734283447}, {"id": 46, "seek": 37360, "start": 373.84000000000003, "end": 379.76000000000005, "text": " but because well summarizing the main problem of big data when machine learning with big data", "tokens": [50376, 457, 570, 731, 14611, 3319, 264, 2135, 1154, 295, 955, 1412, 562, 3479, 2539, 365, 955, 1412, 50672], "temperature": 0.0, "avg_logprob": -0.09782088125074231, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0068504721857607365}, {"id": 47, "seek": 37360, "start": 379.76000000000005, "end": 385.36, "text": " is the time complexity of the computation but the main advantage of quantum computation is", "tokens": [50672, 307, 264, 565, 14024, 295, 264, 24903, 457, 264, 2135, 5002, 295, 13018, 24903, 307, 50952], "temperature": 0.0, "avg_logprob": -0.09782088125074231, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0068504721857607365}, {"id": 48, "seek": 37360, "start": 385.36, "end": 392.64000000000004, "text": " the possibility to reproduce to use the time complexity. So in a certain sense,", "tokens": [50952, 264, 7959, 281, 29501, 281, 764, 264, 565, 14024, 13, 407, 294, 257, 1629, 2020, 11, 51316], "temperature": 0.0, "avg_logprob": -0.09782088125074231, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0068504721857607365}, {"id": 49, "seek": 37360, "start": 392.64000000000004, "end": 399.12, "text": " quantum computation and machine learning appears as a very natural and maybe unavoidable,", "tokens": [51316, 13018, 24903, 293, 3479, 2539, 7038, 382, 257, 588, 3303, 293, 1310, 36541, 17079, 712, 11, 51640], "temperature": 0.0, "avg_logprob": -0.09782088125074231, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0068504721857607365}, {"id": 50, "seek": 39912, "start": 399.12, "end": 407.84000000000003, "text": " unavoidable connection but the connection can be deeper and deeper with respect to what we can", "tokens": [50364, 36541, 17079, 712, 4984, 457, 264, 4984, 393, 312, 7731, 293, 7731, 365, 3104, 281, 437, 321, 393, 50800], "temperature": 0.0, "avg_logprob": -0.12049845169330466, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.008254696615040302}, {"id": 51, "seek": 39912, "start": 407.84000000000003, "end": 415.52, "text": " think. Indeed the question that they want to ask you in this in this talk is can quantum", "tokens": [50800, 519, 13, 15061, 264, 1168, 300, 436, 528, 281, 1029, 291, 294, 341, 294, 341, 751, 307, 393, 13018, 51184], "temperature": 0.0, "avg_logprob": -0.12049845169330466, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.008254696615040302}, {"id": 52, "seek": 39912, "start": 415.52, "end": 424.24, "text": " information theory help classical machine learning, classical by using classical computer only", "tokens": [51184, 1589, 5261, 854, 13735, 3479, 2539, 11, 13735, 538, 1228, 13735, 3820, 787, 51620], "temperature": 0.0, "avg_logprob": -0.12049845169330466, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.008254696615040302}, {"id": 53, "seek": 42424, "start": 424.24, "end": 431.36, "text": " and we will provide a positive answer to this question. So this approach that we will speak", "tokens": [50364, 293, 321, 486, 2893, 257, 3353, 1867, 281, 341, 1168, 13, 407, 341, 3109, 300, 321, 486, 1710, 50720], "temperature": 0.0, "avg_logprob": -0.16419682502746583, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.012185770086944103}, {"id": 54, "seek": 42424, "start": 431.36, "end": 439.44, "text": " is known as a quantum-inspired machine learning and the important thing is that well very often", "tokens": [50720, 307, 2570, 382, 257, 13018, 12, 31637, 1824, 3479, 2539, 293, 264, 1021, 551, 307, 300, 731, 588, 2049, 51124], "temperature": 0.0, "avg_logprob": -0.16419682502746583, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.012185770086944103}, {"id": 55, "seek": 42424, "start": 440.72, "end": 450.32, "text": " we know that in many disciplines we speak about the inspiration from quantum mechanics.", "tokens": [51188, 321, 458, 300, 294, 867, 21919, 321, 1710, 466, 264, 10249, 490, 13018, 12939, 13, 51668], "temperature": 0.0, "avg_logprob": -0.16419682502746583, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.012185770086944103}, {"id": 56, "seek": 45032, "start": 450.32, "end": 457.59999999999997, "text": " Okay now what I want to focus on is the fact that our inspiration is not just an analogy,", "tokens": [50364, 1033, 586, 437, 286, 528, 281, 1879, 322, 307, 264, 1186, 300, 527, 10249, 307, 406, 445, 364, 21663, 11, 50728], "temperature": 0.0, "avg_logprob": -0.16346932902480615, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.026229679584503174}, {"id": 57, "seek": 45032, "start": 457.59999999999997, "end": 466.48, "text": " it's not a metaphor but it's just a really inspiration regarding a real inspiration to quantum", "tokens": [50728, 309, 311, 406, 257, 19157, 457, 309, 311, 445, 257, 534, 10249, 8595, 257, 957, 10249, 281, 13018, 51172], "temperature": 0.0, "avg_logprob": -0.16346932902480615, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.026229679584503174}, {"id": 58, "seek": 45032, "start": 466.48, "end": 475.92, "text": " work in order to have a real benefit in machine learning and in the next minutes I aim to show", "tokens": [51172, 589, 294, 1668, 281, 362, 257, 957, 5121, 294, 3479, 2539, 293, 294, 264, 958, 2077, 286, 5939, 281, 855, 51644], "temperature": 0.0, "avg_logprob": -0.16346932902480615, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.026229679584503174}, {"id": 59, "seek": 47592, "start": 475.92, "end": 484.8, "text": " you what they mean by real inspiration. Okay in particular we focus on the classification problem.", "tokens": [50364, 291, 437, 436, 914, 538, 957, 10249, 13, 1033, 294, 1729, 321, 1879, 322, 264, 21538, 1154, 13, 50808], "temperature": 0.0, "avg_logprob": -0.14735185777818835, "compression_ratio": 2.136904761904762, "no_speech_prob": 0.008723144419491291}, {"id": 60, "seek": 47592, "start": 484.8, "end": 491.12, "text": " What is the classification problem? Okay the general idea is that the quantum classification", "tokens": [50808, 708, 307, 264, 21538, 1154, 30, 1033, 264, 2674, 1558, 307, 300, 264, 13018, 21538, 51124], "temperature": 0.0, "avg_logprob": -0.14735185777818835, "compression_ratio": 2.136904761904762, "no_speech_prob": 0.008723144419491291}, {"id": 61, "seek": 47592, "start": 491.12, "end": 497.44, "text": " in the idea that we want to perform is the idea of a quantum classifier where quantum", "tokens": [51124, 294, 264, 1558, 300, 321, 528, 281, 2042, 307, 264, 1558, 295, 257, 13018, 1508, 9902, 689, 13018, 51440], "temperature": 0.0, "avg_logprob": -0.14735185777818835, "compression_ratio": 2.136904761904762, "no_speech_prob": 0.008723144419491291}, {"id": 62, "seek": 47592, "start": 497.44, "end": 501.76, "text": " classification is the idea of quantum classification is to formalize the standard", "tokens": [51440, 21538, 307, 264, 1558, 295, 13018, 21538, 307, 281, 9860, 1125, 264, 3832, 51656], "temperature": 0.0, "avg_logprob": -0.14735185777818835, "compression_ratio": 2.136904761904762, "no_speech_prob": 0.008723144419491291}, {"id": 63, "seek": 50176, "start": 502.48, "end": 508.0, "text": " classification problems in terms of the mathematical quantum object and then", "tokens": [50400, 21538, 2740, 294, 2115, 295, 264, 18894, 13018, 2657, 293, 550, 50676], "temperature": 0.0, "avg_logprob": -0.10956008732318878, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.006565435323864222}, {"id": 64, "seek": 50176, "start": 508.64, "end": 514.48, "text": " inspired by certain properties of quantum state discrimination we define a quantum", "tokens": [50708, 7547, 538, 1629, 7221, 295, 13018, 1785, 15973, 321, 6964, 257, 13018, 51000], "temperature": 0.0, "avg_logprob": -0.10956008732318878, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.006565435323864222}, {"id": 65, "seek": 50176, "start": 514.48, "end": 522.0, "text": " classifier that will provide interesting and real good advantages. The procedure is", "tokens": [51000, 1508, 9902, 300, 486, 2893, 1880, 293, 957, 665, 14906, 13, 440, 10747, 307, 51376], "temperature": 0.0, "avg_logprob": -0.10956008732318878, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.006565435323864222}, {"id": 66, "seek": 50176, "start": 522.0, "end": 528.64, "text": " based on three fundamental steps that we will describe not now but a few slides later encoding", "tokens": [51376, 2361, 322, 1045, 8088, 4439, 300, 321, 486, 6786, 406, 586, 457, 257, 1326, 9788, 1780, 43430, 51708], "temperature": 0.0, "avg_logprob": -0.10956008732318878, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.006565435323864222}, {"id": 67, "seek": 52864, "start": 528.64, "end": 535.6, "text": " classification and the coding but we will meet this slide again in a bit. First of all", "tokens": [50364, 21538, 293, 264, 17720, 457, 321, 486, 1677, 341, 4137, 797, 294, 257, 857, 13, 2386, 295, 439, 50712], "temperature": 0.0, "avg_logprob": -0.11897472540537517, "compression_ratio": 1.61, "no_speech_prob": 0.006592940539121628}, {"id": 68, "seek": 52864, "start": 536.96, "end": 540.0, "text": " I want to very very quickly summarize what is the", "tokens": [50780, 286, 528, 281, 588, 588, 2661, 20858, 437, 307, 264, 50932], "temperature": 0.0, "avg_logprob": -0.11897472540537517, "compression_ratio": 1.61, "no_speech_prob": 0.006592940539121628}, {"id": 69, "seek": 52864, "start": 542.08, "end": 548.4, "text": " sorry the general framework of classification problem. So when we have to classify an object", "tokens": [51036, 2597, 264, 2674, 8388, 295, 21538, 1154, 13, 407, 562, 321, 362, 281, 33872, 364, 2657, 51352], "temperature": 0.0, "avg_logprob": -0.11897472540537517, "compression_ratio": 1.61, "no_speech_prob": 0.006592940539121628}, {"id": 70, "seek": 52864, "start": 548.4, "end": 557.28, "text": " we describe this object as a vector where each component of this vector is a feature of this", "tokens": [51352, 321, 6786, 341, 2657, 382, 257, 8062, 689, 1184, 6542, 295, 341, 8062, 307, 257, 4111, 295, 341, 51796], "temperature": 0.0, "avg_logprob": -0.11897472540537517, "compression_ratio": 1.61, "no_speech_prob": 0.006592940539121628}, {"id": 71, "seek": 55728, "start": 557.36, "end": 565.52, "text": " object like this object could be a cat and this feature could be the length of the tail,", "tokens": [50368, 2657, 411, 341, 2657, 727, 312, 257, 3857, 293, 341, 4111, 727, 312, 264, 4641, 295, 264, 6838, 11, 50776], "temperature": 0.0, "avg_logprob": -0.1578554425920759, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.012306048534810543}, {"id": 72, "seek": 55728, "start": 566.0799999999999, "end": 572.64, "text": " the weight of the cat and so on and we have different classes of objects for instance", "tokens": [50804, 264, 3364, 295, 264, 3857, 293, 370, 322, 293, 321, 362, 819, 5359, 295, 6565, 337, 5197, 51132], "temperature": 0.0, "avg_logprob": -0.1578554425920759, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.012306048534810543}, {"id": 73, "seek": 55728, "start": 572.64, "end": 580.24, "text": " dog and cats and what and so on and this so a pattern is represented by a pair where this is", "tokens": [51132, 3000, 293, 11111, 293, 437, 293, 370, 322, 293, 341, 370, 257, 5102, 307, 10379, 538, 257, 6119, 689, 341, 307, 51512], "temperature": 0.0, "avg_logprob": -0.1578554425920759, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.012306048534810543}, {"id": 74, "seek": 55728, "start": 580.24, "end": 586.72, "text": " the vector the object represented by a vector and this is the label that characterized the class", "tokens": [51512, 264, 8062, 264, 2657, 10379, 538, 257, 8062, 293, 341, 307, 264, 7645, 300, 29361, 264, 1508, 51836], "temperature": 0.0, "avg_logprob": -0.1578554425920759, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.012306048534810543}, {"id": 75, "seek": 58672, "start": 586.72, "end": 593.76, "text": " for instance class for instance label one means class of the cat, label two means class of the", "tokens": [50364, 337, 5197, 1508, 337, 5197, 7645, 472, 1355, 1508, 295, 264, 3857, 11, 7645, 732, 1355, 1508, 295, 264, 50716], "temperature": 0.0, "avg_logprob": -0.1495097279548645, "compression_ratio": 1.8972602739726028, "no_speech_prob": 0.004521877039223909}, {"id": 76, "seek": 58672, "start": 593.76, "end": 601.0400000000001, "text": " dogs for instance okay and the goal of the classification is to have a classifier", "tokens": [50716, 7197, 337, 5197, 1392, 293, 264, 3387, 295, 264, 21538, 307, 281, 362, 257, 1508, 9902, 51080], "temperature": 0.0, "avg_logprob": -0.1495097279548645, "compression_ratio": 1.8972602739726028, "no_speech_prob": 0.004521877039223909}, {"id": 77, "seek": 58672, "start": 603.28, "end": 615.36, "text": " I mean a function classifier is a function that allows us to study the vector and gives as an output", "tokens": [51192, 286, 914, 257, 2445, 1508, 9902, 307, 257, 2445, 300, 4045, 505, 281, 2979, 264, 8062, 293, 2709, 382, 364, 5598, 51796], "temperature": 0.0, "avg_logprob": -0.1495097279548645, "compression_ratio": 1.8972602739726028, "no_speech_prob": 0.004521877039223909}, {"id": 78, "seek": 61536, "start": 615.36, "end": 623.76, "text": " the label so looking at the vector looking at the object I can say you if this object is a cat or is", "tokens": [50364, 264, 7645, 370, 1237, 412, 264, 8062, 1237, 412, 264, 2657, 286, 393, 584, 291, 498, 341, 2657, 307, 257, 3857, 420, 307, 50784], "temperature": 0.0, "avg_logprob": -0.1412034323721221, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.005985351279377937}, {"id": 79, "seek": 61536, "start": 623.76, "end": 633.52, "text": " a dog basically okay okay okay so in a general supervised scenario I will be very very very", "tokens": [50784, 257, 3000, 1936, 1392, 1392, 1392, 370, 294, 257, 2674, 46533, 9005, 286, 486, 312, 588, 588, 588, 51272], "temperature": 0.0, "avg_logprob": -0.1412034323721221, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.005985351279377937}, {"id": 80, "seek": 61536, "start": 634.32, "end": 641.9200000000001, "text": " I will skip many slides but but sometime I can focus on some technical data sometimes not", "tokens": [51312, 286, 486, 10023, 867, 9788, 457, 457, 15053, 286, 393, 1879, 322, 512, 6191, 1412, 2171, 406, 51692], "temperature": 0.0, "avg_logprob": -0.1412034323721221, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.005985351279377937}, {"id": 81, "seek": 64192, "start": 642.0, "end": 651.5999999999999, "text": " generally generally in what we say supervised learning we mean that we consider a data set", "tokens": [50368, 5101, 5101, 294, 437, 321, 584, 46533, 2539, 321, 914, 300, 321, 1949, 257, 1412, 992, 50848], "temperature": 0.0, "avg_logprob": -0.12963023781776428, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.0037950053811073303}, {"id": 82, "seek": 64192, "start": 652.16, "end": 660.64, "text": " for instance a data set of dogs and a data set of cats for instance and we divide this data set", "tokens": [50876, 337, 5197, 257, 1412, 992, 295, 7197, 293, 257, 1412, 992, 295, 11111, 337, 5197, 293, 321, 9845, 341, 1412, 992, 51300], "temperature": 0.0, "avg_logprob": -0.12963023781776428, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.0037950053811073303}, {"id": 83, "seek": 64192, "start": 660.64, "end": 667.52, "text": " at the beginning in two parts the first part is given by the training set this is the set", "tokens": [51300, 412, 264, 2863, 294, 732, 3166, 264, 700, 644, 307, 2212, 538, 264, 3097, 992, 341, 307, 264, 992, 51644], "temperature": 0.0, "avg_logprob": -0.12963023781776428, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.0037950053811073303}, {"id": 84, "seek": 66752, "start": 667.52, "end": 675.52, "text": " useful in order to train our algorithm and this is the test set but the set is a set of", "tokens": [50364, 4420, 294, 1668, 281, 3847, 527, 9284, 293, 341, 307, 264, 1500, 992, 457, 264, 992, 307, 257, 992, 295, 50764], "temperature": 0.0, "avg_logprob": -0.11988015541663537, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02141735516488552}, {"id": 85, "seek": 66752, "start": 675.52, "end": 683.6, "text": " object that we use in order to know whether our algorithm is good or not just in order to know", "tokens": [50764, 2657, 300, 321, 764, 294, 1668, 281, 458, 1968, 527, 9284, 307, 665, 420, 406, 445, 294, 1668, 281, 458, 51168], "temperature": 0.0, "avg_logprob": -0.11988015541663537, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02141735516488552}, {"id": 86, "seek": 66752, "start": 683.6, "end": 693.6, "text": " just in order to measure the performance of our algorithm and so obviously then all of these", "tokens": [51168, 445, 294, 1668, 281, 3481, 264, 3389, 295, 527, 9284, 293, 370, 2745, 550, 439, 295, 613, 51668], "temperature": 0.0, "avg_logprob": -0.11988015541663537, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02141735516488552}, {"id": 87, "seek": 69360, "start": 693.6800000000001, "end": 700.24, "text": " objects are already labelled so we already know the level of the object so the goal is just to", "tokens": [50368, 6565, 366, 1217, 2715, 41307, 370, 321, 1217, 458, 264, 1496, 295, 264, 2657, 370, 264, 3387, 307, 445, 281, 50696], "temperature": 0.0, "avg_logprob": -0.12007912467507754, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.012700317427515984}, {"id": 88, "seek": 69360, "start": 700.24, "end": 710.32, "text": " evaluate how the right if the classifier is good or not so we use a already labelled set of object", "tokens": [50696, 13059, 577, 264, 558, 498, 264, 1508, 9902, 307, 665, 420, 406, 370, 321, 764, 257, 1217, 2715, 41307, 992, 295, 2657, 51200], "temperature": 0.0, "avg_logprob": -0.12007912467507754, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.012700317427515984}, {"id": 89, "seek": 69360, "start": 710.32, "end": 716.64, "text": " we divide it in training and test and obviously also the training is divided with respect to the", "tokens": [51200, 321, 9845, 309, 294, 3097, 293, 1500, 293, 2745, 611, 264, 3097, 307, 6666, 365, 3104, 281, 264, 51516], "temperature": 0.0, "avg_logprob": -0.12007912467507754, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.012700317427515984}, {"id": 90, "seek": 71664, "start": 716.64, "end": 724.48, "text": " classes the classes will get the classes of dogs the classes of foxes and so on and so on okay so", "tokens": [50364, 5359, 264, 5359, 486, 483, 264, 5359, 295, 7197, 264, 5359, 295, 21026, 279, 293, 370, 322, 293, 370, 322, 1392, 370, 50756], "temperature": 0.0, "avg_logprob": -0.13330864906311035, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.007325257174670696}, {"id": 91, "seek": 71664, "start": 725.1999999999999, "end": 734.8, "text": " the the first idea of the quantum classification is just to translate formally translate each", "tokens": [50792, 264, 264, 700, 1558, 295, 264, 13018, 21538, 307, 445, 281, 13799, 25983, 13799, 1184, 51272], "temperature": 0.0, "avg_logprob": -0.13330864906311035, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.007325257174670696}, {"id": 92, "seek": 71664, "start": 734.8, "end": 743.12, "text": " object each vector each cat that is represented by a vector in terms of a quantum object that we", "tokens": [51272, 2657, 1184, 8062, 1184, 3857, 300, 307, 10379, 538, 257, 8062, 294, 2115, 295, 257, 13018, 2657, 300, 321, 51688], "temperature": 0.0, "avg_logprob": -0.13330864906311035, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.007325257174670696}, {"id": 93, "seek": 74312, "start": 743.12, "end": 749.92, "text": " represent by a density operator row and there are many many ways to encode this is just encoding to", "tokens": [50364, 2906, 538, 257, 10305, 12973, 5386, 293, 456, 366, 867, 867, 2098, 281, 2058, 1429, 341, 307, 445, 43430, 281, 50704], "temperature": 0.0, "avg_logprob": -0.10728415575894443, "compression_ratio": 1.7361963190184049, "no_speech_prob": 0.008854318410158157}, {"id": 94, "seek": 74312, "start": 749.92, "end": 758.72, "text": " encode a real vector in terms of the quantum object okay in terms of a density operator obviously", "tokens": [50704, 2058, 1429, 257, 957, 8062, 294, 2115, 295, 264, 13018, 2657, 1392, 294, 2115, 295, 257, 10305, 12973, 2745, 51144], "temperature": 0.0, "avg_logprob": -0.10728415575894443, "compression_ratio": 1.7361963190184049, "no_speech_prob": 0.008854318410158157}, {"id": 95, "seek": 74312, "start": 758.72, "end": 767.44, "text": " it is only a formal translation we are not transforming a cat in a photon or a dog in", "tokens": [51144, 309, 307, 787, 257, 9860, 12853, 321, 366, 406, 27210, 257, 3857, 294, 257, 37443, 420, 257, 3000, 294, 51580], "temperature": 0.0, "avg_logprob": -0.10728415575894443, "compression_ratio": 1.7361963190184049, "no_speech_prob": 0.008854318410158157}, {"id": 96, "seek": 76744, "start": 767.44, "end": 778.1600000000001, "text": " an electron obviously okay so now once we have translated all the data set of real data in terms", "tokens": [50364, 364, 6084, 2745, 1392, 370, 586, 1564, 321, 362, 16805, 439, 264, 1412, 992, 295, 957, 1412, 294, 2115, 50900], "temperature": 0.0, "avg_logprob": -0.06623671736036028, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0037806241307407618}, {"id": 97, "seek": 76744, "start": 778.1600000000001, "end": 787.2800000000001, "text": " of the data set of quantum data or quantum data again we can consider the distinction between", "tokens": [50900, 295, 264, 1412, 992, 295, 13018, 1412, 420, 13018, 1412, 797, 321, 393, 1949, 264, 16844, 1296, 51356], "temperature": 0.0, "avg_logprob": -0.06623671736036028, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0037806241307407618}, {"id": 98, "seek": 78728, "start": 787.92, "end": 797.68, "text": " training and test training and test again and the idea is just to obtain a quantum classifier", "tokens": [50396, 3097, 293, 1500, 3097, 293, 1500, 797, 293, 264, 1558, 307, 445, 281, 12701, 257, 13018, 1508, 9902, 50884], "temperature": 0.0, "avg_logprob": -0.08212281055137759, "compression_ratio": 1.8486842105263157, "no_speech_prob": 0.025258483365178108}, {"id": 99, "seek": 78728, "start": 797.68, "end": 806.8, "text": " that is a function that has as input a quantum object of the quantum test set and has the", "tokens": [50884, 300, 307, 257, 2445, 300, 575, 382, 4846, 257, 13018, 2657, 295, 264, 13018, 1500, 992, 293, 575, 264, 51340], "temperature": 0.0, "avg_logprob": -0.08212281055137759, "compression_ratio": 1.8486842105263157, "no_speech_prob": 0.025258483365178108}, {"id": 100, "seek": 78728, "start": 806.8, "end": 815.36, "text": " output level so given this quantum object that is the density operator that represents an unknown", "tokens": [51340, 5598, 1496, 370, 2212, 341, 13018, 2657, 300, 307, 264, 10305, 12973, 300, 8855, 364, 9841, 51768], "temperature": 0.0, "avg_logprob": -0.08212281055137759, "compression_ratio": 1.8486842105263157, "no_speech_prob": 0.025258483365178108}, {"id": 101, "seek": 81536, "start": 815.36, "end": 824.4, "text": " object we can say that we can we can obtain by the classifier that this object is a dog okay so", "tokens": [50364, 2657, 321, 393, 584, 300, 321, 393, 321, 393, 12701, 538, 264, 1508, 9902, 300, 341, 2657, 307, 257, 3000, 1392, 370, 50816], "temperature": 0.0, "avg_logprob": -0.08015062041201834, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.0033461053390055895}, {"id": 102, "seek": 81536, "start": 825.04, "end": 832.5600000000001, "text": " obviously what we have to show is that this translation from classical to quantum object", "tokens": [50848, 2745, 437, 321, 362, 281, 855, 307, 300, 341, 12853, 490, 13735, 281, 13018, 2657, 51224], "temperature": 0.0, "avg_logprob": -0.08015062041201834, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.0033461053390055895}, {"id": 103, "seek": 81536, "start": 833.12, "end": 838.96, "text": " has some benefit provides some benefit otherwise it's totally useless okay so okay", "tokens": [51252, 575, 512, 5121, 6417, 512, 5121, 5911, 309, 311, 3879, 14115, 1392, 370, 1392, 51544], "temperature": 0.0, "avg_logprob": -0.08015062041201834, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.0033461053390055895}, {"id": 104, "seek": 83896, "start": 839.0400000000001, "end": 849.0400000000001, "text": " now let us talk regarding the general setting in quantum state discrimination okay that probably", "tokens": [50368, 586, 718, 505, 751, 8595, 264, 2674, 3287, 294, 13018, 1785, 15973, 1392, 300, 1391, 50868], "temperature": 0.0, "avg_logprob": -0.19191546440124513, "compression_ratio": 1.5, "no_speech_prob": 0.002340105827897787}, {"id": 105, "seek": 83896, "start": 849.76, "end": 858.08, "text": " most of you already know but I very briefly summarize here let R be a set of density operator", "tokens": [50904, 881, 295, 291, 1217, 458, 457, 286, 588, 10515, 20858, 510, 718, 497, 312, 257, 992, 295, 10305, 12973, 51320], "temperature": 0.0, "avg_logprob": -0.19191546440124513, "compression_ratio": 1.5, "no_speech_prob": 0.002340105827897787}, {"id": 106, "seek": 83896, "start": 858.08, "end": 864.0, "text": " and suppose that Thelis wishes to communicate information to Bobo by using a quantum system", "tokens": [51320, 293, 7297, 300, 334, 338, 271, 15065, 281, 7890, 1589, 281, 6085, 78, 538, 1228, 257, 13018, 1185, 51616], "temperature": 0.0, "avg_logprob": -0.19191546440124513, "compression_ratio": 1.5, "no_speech_prob": 0.002340105827897787}, {"id": 107, "seek": 86400, "start": 864.0, "end": 872.72, "text": " that is one of the system belonging to this state okay to the same Thelis prepares a quantum system", "tokens": [50364, 300, 307, 472, 295, 264, 1185, 22957, 281, 341, 1785, 1392, 281, 264, 912, 334, 338, 271, 39418, 257, 13018, 1185, 50800], "temperature": 0.0, "avg_logprob": -0.14211960792541503, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.012303734198212624}, {"id": 108, "seek": 86400, "start": 872.72, "end": 882.8, "text": " in one of this state in one of this state and hands the system over to Bobo in principle Bobo has", "tokens": [50800, 294, 472, 295, 341, 1785, 294, 472, 295, 341, 1785, 293, 2377, 264, 1185, 670, 281, 6085, 78, 294, 8665, 6085, 78, 575, 51304], "temperature": 0.0, "avg_logprob": -0.14211960792541503, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.012303734198212624}, {"id": 109, "seek": 88280, "start": 882.8, "end": 893.52, "text": " a complete knowledge about R so Bob knows in knows already knows which are row one row two row", "tokens": [50364, 257, 3566, 3601, 466, 497, 370, 6085, 3255, 294, 3255, 1217, 3255, 597, 366, 5386, 472, 5386, 732, 5386, 50900], "temperature": 0.0, "avg_logprob": -0.1647961674904337, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.03533724695444107}, {"id": 110, "seek": 88280, "start": 893.52, "end": 903.92, "text": " M but he does not know the actual state of the system that he have received by Thelis so in order to", "tokens": [50900, 376, 457, 415, 775, 406, 458, 264, 3539, 1785, 295, 264, 1185, 300, 415, 362, 4613, 538, 334, 338, 271, 370, 294, 1668, 281, 51420], "temperature": 0.0, "avg_logprob": -0.1647961674904337, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.03533724695444107}, {"id": 111, "seek": 90392, "start": 903.92, "end": 915.12, "text": " so that the Bob's task is to determine which one is among this state in one shot just by making a", "tokens": [50364, 370, 300, 264, 6085, 311, 5633, 307, 281, 6997, 597, 472, 307, 3654, 341, 1785, 294, 472, 3347, 445, 538, 1455, 257, 50924], "temperature": 0.0, "avg_logprob": -0.19008880191379124, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.02875857800245285}, {"id": 112, "seek": 90392, "start": 915.12, "end": 926.24, "text": " single von Neumann measurement over all possible sorry or possible physical observable so there", "tokens": [50924, 2167, 2957, 1734, 449, 969, 13160, 670, 439, 1944, 2597, 420, 1944, 4001, 9951, 712, 370, 456, 51480], "temperature": 0.0, "avg_logprob": -0.19008880191379124, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.02875857800245285}, {"id": 113, "seek": 92624, "start": 926.24, "end": 934.4, "text": " and so the possibility is that the measure the outcome of the measurement could be just the", "tokens": [50364, 293, 370, 264, 7959, 307, 300, 264, 3481, 264, 9700, 295, 264, 13160, 727, 312, 445, 264, 50772], "temperature": 0.0, "avg_logprob": -0.1586528749608282, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.025691432878375053}, {"id": 114, "seek": 92624, "start": 934.4, "end": 941.84, "text": " row that Thelis sent to Bob or not or can be another one in the first case okay we say that Bob", "tokens": [50772, 5386, 300, 334, 338, 271, 2279, 281, 6085, 420, 406, 420, 393, 312, 1071, 472, 294, 264, 700, 1389, 1392, 321, 584, 300, 6085, 51144], "temperature": 0.0, "avg_logprob": -0.1586528749608282, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.025691432878375053}, {"id": 115, "seek": 92624, "start": 942.4, "end": 951.52, "text": " succeeds in another case we say that Bob fails okay so what is the probability for Bob to", "tokens": [51172, 49263, 294, 1071, 1389, 321, 584, 300, 6085, 18199, 1392, 370, 437, 307, 264, 8482, 337, 6085, 281, 51628], "temperature": 0.0, "avg_logprob": -0.1586528749608282, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.025691432878375053}, {"id": 116, "seek": 95152, "start": 951.52, "end": 959.4399999999999, "text": " succeed okay given a set R or quantum state and a von Neumann measurement M the average", "tokens": [50364, 7754, 1392, 2212, 257, 992, 497, 420, 13018, 1785, 293, 257, 2957, 1734, 449, 969, 13160, 376, 264, 4274, 50760], "temperature": 0.0, "avg_logprob": -0.10720586031675339, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.019110308960080147}, {"id": 117, "seek": 95152, "start": 959.4399999999999, "end": 967.52, "text": " probability to Bob for Bob to perform a correct discrimination so to correct individuate which", "tokens": [50760, 8482, 281, 6085, 337, 6085, 281, 2042, 257, 3006, 15973, 370, 281, 3006, 2461, 10107, 597, 51164], "temperature": 0.0, "avg_logprob": -0.10720586031675339, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.019110308960080147}, {"id": 118, "seek": 95152, "start": 967.52, "end": 975.76, "text": " is the state that was sent from Thelis is given by this quantity is given by this quantity and", "tokens": [51164, 307, 264, 1785, 300, 390, 2279, 490, 334, 338, 271, 307, 2212, 538, 341, 11275, 307, 2212, 538, 341, 11275, 293, 51576], "temperature": 0.0, "avg_logprob": -0.10720586031675339, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.019110308960080147}, {"id": 119, "seek": 97576, "start": 976.72, "end": 987.2, "text": " the aim of course is the maximize maximize this probability so to minimize the probability to", "tokens": [50412, 264, 5939, 295, 1164, 307, 264, 19874, 19874, 341, 8482, 370, 281, 17522, 264, 8482, 281, 50936], "temperature": 0.0, "avg_logprob": -0.12962887116840907, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.015960339456796646}, {"id": 120, "seek": 97576, "start": 987.2, "end": 996.16, "text": " have an error okay so the question is what what is the measure the measure that maximize the", "tokens": [50936, 362, 364, 6713, 1392, 370, 264, 1168, 307, 437, 437, 307, 264, 3481, 264, 3481, 300, 19874, 264, 51384], "temperature": 0.0, "avg_logprob": -0.12962887116840907, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.015960339456796646}, {"id": 121, "seek": 97576, "start": 996.16, "end": 1002.88, "text": " probability to have the correct discrimination and this measure this measure was obtained by", "tokens": [51384, 8482, 281, 362, 264, 3006, 15973, 293, 341, 3481, 341, 3481, 390, 14879, 538, 51720], "temperature": 0.0, "avg_logprob": -0.12962887116840907, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.015960339456796646}, {"id": 122, "seek": 100288, "start": 1003.84, "end": 1012.56, "text": " by this strategy okay the strategy is is related only to the binary case so consider to have just", "tokens": [50412, 538, 341, 5206, 1392, 264, 5206, 307, 307, 4077, 787, 281, 264, 17434, 1389, 370, 1949, 281, 362, 445, 50848], "temperature": 0.0, "avg_logprob": -0.13599017007010325, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.01143348403275013}, {"id": 123, "seek": 100288, "start": 1012.56, "end": 1021.52, "text": " row one and row two and if we consider this quantity p1 row one minus p2 row two where p1", "tokens": [50848, 5386, 472, 293, 5386, 732, 293, 498, 321, 1949, 341, 11275, 280, 16, 5386, 472, 3175, 280, 17, 5386, 732, 689, 280, 16, 51296], "temperature": 0.0, "avg_logprob": -0.13599017007010325, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.01143348403275013}, {"id": 124, "seek": 100288, "start": 1021.52, "end": 1029.76, "text": " and p2 are a priori probability okay that can be obtained in some different but empirical way for", "tokens": [51296, 293, 280, 17, 366, 257, 4059, 72, 8482, 1392, 300, 393, 312, 14879, 294, 512, 819, 457, 31886, 636, 337, 51708], "temperature": 0.0, "avg_logprob": -0.13599017007010325, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.01143348403275013}, {"id": 125, "seek": 102976, "start": 1029.76, "end": 1036.8799999999999, "text": " instance we obtain this quantity we calculate the positive and negative eigenvalues of this", "tokens": [50364, 5197, 321, 12701, 341, 11275, 321, 8873, 264, 3353, 293, 3671, 10446, 46033, 295, 341, 50720], "temperature": 0.0, "avg_logprob": -0.07684355576833089, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.011106165125966072}, {"id": 126, "seek": 102976, "start": 1036.8799999999999, "end": 1045.76, "text": " quantity we consider the respective eigenvectors and we consider the sum of the projector built", "tokens": [50720, 11275, 321, 1949, 264, 23649, 10446, 303, 5547, 293, 321, 1949, 264, 2408, 295, 264, 39792, 3094, 51164], "temperature": 0.0, "avg_logprob": -0.07684355576833089, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.011106165125966072}, {"id": 127, "seek": 102976, "start": 1045.76, "end": 1053.36, "text": " over all of each of these eigenvectors in this way we can obtain p plus and p minus", "tokens": [51164, 670, 439, 295, 1184, 295, 613, 10446, 303, 5547, 294, 341, 636, 321, 393, 12701, 280, 1804, 293, 280, 3175, 51544], "temperature": 0.0, "avg_logprob": -0.07684355576833089, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.011106165125966072}, {"id": 128, "seek": 105336, "start": 1053.9199999999998, "end": 1061.52, "text": " and hence from showered that this p plus and p minus is a von Neumann measurement that is optimal", "tokens": [50392, 293, 16678, 490, 855, 4073, 300, 341, 280, 1804, 293, 280, 3175, 307, 257, 2957, 1734, 449, 969, 13160, 300, 307, 16252, 50772], "temperature": 0.0, "avg_logprob": -0.21016745269298553, "compression_ratio": 1.9133333333333333, "no_speech_prob": 0.015044056810438633}, {"id": 129, "seek": 105336, "start": 1061.52, "end": 1068.0, "text": " that is the optimal measure for the discrimination problem for the binary discrimination problem", "tokens": [50772, 300, 307, 264, 16252, 3481, 337, 264, 15973, 1154, 337, 264, 17434, 15973, 1154, 51096], "temperature": 0.0, "avg_logprob": -0.21016745269298553, "compression_ratio": 1.9133333333333333, "no_speech_prob": 0.015044056810438633}, {"id": 130, "seek": 105336, "start": 1068.0, "end": 1075.1999999999998, "text": " that we have introduced at the moment so this is the this is the bound this is the bound and", "tokens": [51096, 300, 321, 362, 7268, 412, 264, 1623, 370, 341, 307, 264, 341, 307, 264, 5472, 341, 307, 264, 5472, 293, 51456], "temperature": 0.0, "avg_logprob": -0.21016745269298553, "compression_ratio": 1.9133333333333333, "no_speech_prob": 0.015044056810438633}, {"id": 131, "seek": 107520, "start": 1076.16, "end": 1082.48, "text": " intuitively p plus and p minus represent the property for the system to be correctly", "tokens": [50412, 46506, 280, 1804, 293, 280, 3175, 2906, 264, 4707, 337, 264, 1185, 281, 312, 8944, 50728], "temperature": 0.0, "avg_logprob": -0.15543433367195775, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.011284712702035904}, {"id": 132, "seek": 107520, "start": 1082.48, "end": 1089.76, "text": " identified as being the state row one or row two respectively okay and that's from bound can be", "tokens": [50728, 9234, 382, 885, 264, 1785, 5386, 472, 420, 5386, 732, 25009, 1392, 293, 300, 311, 490, 5472, 393, 312, 51092], "temperature": 0.0, "avg_logprob": -0.15543433367195775, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.011284712702035904}, {"id": 133, "seek": 107520, "start": 1089.76, "end": 1097.8400000000001, "text": " seen as a measurement of distinguishability between row one and row two so now again with", "tokens": [51092, 1612, 382, 257, 13160, 295, 20206, 2310, 1296, 5386, 472, 293, 5386, 732, 370, 586, 797, 365, 51496], "temperature": 0.0, "avg_logprob": -0.15543433367195775, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.011284712702035904}, {"id": 134, "seek": 109784, "start": 1097.84, "end": 1105.9199999999998, "text": " this light because now we have all the ingredients we need in order to put together this discrimination", "tokens": [50364, 341, 1442, 570, 586, 321, 362, 439, 264, 6952, 321, 643, 294, 1668, 281, 829, 1214, 341, 15973, 50768], "temperature": 0.0, "avg_logprob": -0.14860567553290005, "compression_ratio": 1.7771084337349397, "no_speech_prob": 0.04584914818406105}, {"id": 135, "seek": 109784, "start": 1105.9199999999998, "end": 1113.1999999999998, "text": " not to put together classification problem and quantum state discrimination the first stage", "tokens": [50768, 406, 281, 829, 1214, 21538, 1154, 293, 13018, 1785, 15973, 264, 700, 3233, 51132], "temperature": 0.0, "avg_logprob": -0.14860567553290005, "compression_ratio": 1.7771084337349397, "no_speech_prob": 0.04584914818406105}, {"id": 136, "seek": 109784, "start": 1113.1999999999998, "end": 1121.76, "text": " is the encoding we say that we need to have real object and translate the real vector and translate", "tokens": [51132, 307, 264, 43430, 321, 584, 300, 321, 643, 281, 362, 957, 2657, 293, 13799, 264, 957, 8062, 293, 13799, 51560], "temperature": 0.0, "avg_logprob": -0.14860567553290005, "compression_ratio": 1.7771084337349397, "no_speech_prob": 0.04584914818406105}, {"id": 137, "seek": 112176, "start": 1121.76, "end": 1128.8799999999999, "text": " them in terms of quantum object like a density operators like a pure states and for instance", "tokens": [50364, 552, 294, 2115, 295, 13018, 2657, 411, 257, 10305, 19077, 411, 257, 6075, 4368, 293, 337, 5197, 50720], "temperature": 0.0, "avg_logprob": -0.09684910849919395, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.021165797486901283}, {"id": 138, "seek": 112176, "start": 1128.8799999999999, "end": 1135.52, "text": " it is easy to see because there are several way several way to do this this is one simple way for", "tokens": [50720, 309, 307, 1858, 281, 536, 570, 456, 366, 2940, 636, 2940, 636, 281, 360, 341, 341, 307, 472, 2199, 636, 337, 51052], "temperature": 0.0, "avg_logprob": -0.09684910849919395, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.021165797486901283}, {"id": 139, "seek": 112176, "start": 1135.52, "end": 1146.16, "text": " instance by using the stereographic projection we map each object in this simple case two feature", "tokens": [51052, 5197, 538, 1228, 264, 12730, 12295, 22743, 321, 4471, 1184, 2657, 294, 341, 2199, 1389, 732, 4111, 51584], "temperature": 0.0, "avg_logprob": -0.09684910849919395, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.021165797486901283}, {"id": 140, "seek": 114616, "start": 1146.16, "end": 1153.8400000000001, "text": " object in a point over a sphere and of a surface one sphere and that's correspond to the point of", "tokens": [50364, 2657, 294, 257, 935, 670, 257, 16687, 293, 295, 257, 3753, 472, 16687, 293, 300, 311, 6805, 281, 264, 935, 295, 50748], "temperature": 0.0, "avg_logprob": -0.12580619225135217, "compression_ratio": 1.9261744966442953, "no_speech_prob": 0.020411401987075806}, {"id": 141, "seek": 114616, "start": 1153.8400000000001, "end": 1161.68, "text": " the block sphere so a pure density matrix a pure state a pure state but there are many other", "tokens": [50748, 264, 3461, 16687, 370, 257, 6075, 10305, 8141, 257, 6075, 1785, 257, 6075, 1785, 457, 456, 366, 867, 661, 51140], "temperature": 0.0, "avg_logprob": -0.12580619225135217, "compression_ratio": 1.9261744966442953, "no_speech_prob": 0.020411401987075806}, {"id": 142, "seek": 114616, "start": 1161.68, "end": 1169.68, "text": " possibilities to encode real object in terms in terms of density matrix pure density matrix okay", "tokens": [51140, 12178, 281, 2058, 1429, 957, 2657, 294, 2115, 294, 2115, 295, 10305, 8141, 6075, 10305, 8141, 1392, 51540], "temperature": 0.0, "avg_logprob": -0.12580619225135217, "compression_ratio": 1.9261744966442953, "no_speech_prob": 0.020411401987075806}, {"id": 143, "seek": 116968, "start": 1169.68, "end": 1179.6000000000001, "text": " so once we do that once we have set of quantum states set of density operators", "tokens": [50364, 370, 1564, 321, 360, 300, 1564, 321, 362, 992, 295, 13018, 4368, 992, 295, 10305, 19077, 50860], "temperature": 0.0, "avg_logprob": -0.11445927241491893, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.0053616478107869625}, {"id": 144, "seek": 116968, "start": 1180.24, "end": 1188.24, "text": " for each training set we for each training set for each class of the training set we we represent", "tokens": [50892, 337, 1184, 3097, 992, 321, 337, 1184, 3097, 992, 337, 1184, 1508, 295, 264, 3097, 992, 321, 321, 2906, 51292], "temperature": 0.0, "avg_logprob": -0.11445927241491893, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.0053616478107869625}, {"id": 145, "seek": 116968, "start": 1188.24, "end": 1195.28, "text": " each class by a quantum centroid that is defined in this way this is just the beginning of our", "tokens": [51292, 1184, 1508, 538, 257, 13018, 1489, 6490, 300, 307, 7642, 294, 341, 636, 341, 307, 445, 264, 2863, 295, 527, 51644], "temperature": 0.0, "avg_logprob": -0.11445927241491893, "compression_ratio": 1.908450704225352, "no_speech_prob": 0.0053616478107869625}, {"id": 146, "seek": 119528, "start": 1195.36, "end": 1203.04, "text": " orbit and it is no longer a pure state of course because it is just the sum of all of the state", "tokens": [50368, 13991, 293, 309, 307, 572, 2854, 257, 6075, 1785, 295, 1164, 570, 309, 307, 445, 264, 2408, 295, 439, 295, 264, 1785, 50752], "temperature": 0.0, "avg_logprob": -0.12648339498610722, "compression_ratio": 1.9836065573770492, "no_speech_prob": 0.007108088117092848}, {"id": 147, "seek": 119528, "start": 1203.04, "end": 1210.3999999999999, "text": " of all the encoded vector belonging to a to a given class to a given class okay obviously the", "tokens": [50752, 295, 439, 264, 2058, 12340, 8062, 22957, 281, 257, 281, 257, 2212, 1508, 281, 257, 2212, 1508, 1392, 2745, 264, 51120], "temperature": 0.0, "avg_logprob": -0.12648339498610722, "compression_ratio": 1.9836065573770492, "no_speech_prob": 0.007108088117092848}, {"id": 148, "seek": 119528, "start": 1210.3999999999999, "end": 1216.48, "text": " quantum centroid are mixed states are mixed states and they are not the encoding of the", "tokens": [51120, 13018, 1489, 6490, 366, 7467, 4368, 366, 7467, 4368, 293, 436, 366, 406, 264, 43430, 295, 264, 51424], "temperature": 0.0, "avg_logprob": -0.12648339498610722, "compression_ratio": 1.9836065573770492, "no_speech_prob": 0.007108088117092848}, {"id": 149, "seek": 119528, "start": 1216.48, "end": 1222.16, "text": " respective classical centroid they are a totally new object a totally new object that", "tokens": [51424, 23649, 13735, 1489, 6490, 436, 366, 257, 3879, 777, 2657, 257, 3879, 777, 2657, 300, 51708], "temperature": 0.0, "avg_logprob": -0.12648339498610722, "compression_ratio": 1.9836065573770492, "no_speech_prob": 0.007108088117092848}, {"id": 150, "seek": 122216, "start": 1222.16, "end": 1229.92, "text": " has not any counterpart in the real world in the world of real object in the world of real vectors", "tokens": [50364, 575, 406, 604, 22335, 294, 264, 957, 1002, 294, 264, 1002, 295, 957, 2657, 294, 264, 1002, 295, 957, 18875, 50752], "temperature": 0.0, "avg_logprob": -0.13993428548177084, "compression_ratio": 1.7207792207792207, "no_speech_prob": 0.0029221156146377325}, {"id": 151, "seek": 122216, "start": 1229.92, "end": 1240.64, "text": " okay okay so now we use quantum state discrimination we use the Elstrom measurement", "tokens": [50752, 1392, 1392, 370, 586, 321, 764, 13018, 1785, 15973, 321, 764, 264, 2699, 38673, 13160, 51288], "temperature": 0.0, "avg_logprob": -0.13993428548177084, "compression_ratio": 1.7207792207792207, "no_speech_prob": 0.0029221156146377325}, {"id": 152, "seek": 122216, "start": 1240.64, "end": 1248.72, "text": " that is optimal so we use p plus and p minus that we built we built by considering", "tokens": [51288, 300, 307, 16252, 370, 321, 764, 280, 1804, 293, 280, 3175, 300, 321, 3094, 321, 3094, 538, 8079, 51692], "temperature": 0.0, "avg_logprob": -0.13993428548177084, "compression_ratio": 1.7207792207792207, "no_speech_prob": 0.0029221156146377325}, {"id": 153, "seek": 125216, "start": 1252.16, "end": 1260.0, "text": " as the two states to discriminate we will consider the two centroids so let us consider", "tokens": [50364, 382, 264, 732, 4368, 281, 47833, 321, 486, 1949, 264, 732, 1489, 340, 3742, 370, 718, 505, 1949, 50756], "temperature": 0.0, "avg_logprob": -0.11509625856266466, "compression_ratio": 2.005434782608696, "no_speech_prob": 0.008757242932915688}, {"id": 154, "seek": 125216, "start": 1260.0, "end": 1266.96, "text": " a case where we want to discriminate between cats and dogs we consider the training set of the cats", "tokens": [50756, 257, 1389, 689, 321, 528, 281, 47833, 1296, 11111, 293, 7197, 321, 1949, 264, 3097, 992, 295, 264, 11111, 51104], "temperature": 0.0, "avg_logprob": -0.11509625856266466, "compression_ratio": 2.005434782608696, "no_speech_prob": 0.008757242932915688}, {"id": 155, "seek": 125216, "start": 1267.6000000000001, "end": 1274.4, "text": " we are encoding this set in terms of quantum object we consider this you can see that the", "tokens": [51136, 321, 366, 43430, 341, 992, 294, 2115, 295, 13018, 2657, 321, 1949, 341, 291, 393, 536, 300, 264, 51476], "temperature": 0.0, "avg_logprob": -0.11509625856266466, "compression_ratio": 2.005434782608696, "no_speech_prob": 0.008757242932915688}, {"id": 156, "seek": 125216, "start": 1274.4, "end": 1280.8000000000002, "text": " centroid of this class we do the same for the dogs and we have these two centroid these two", "tokens": [51476, 1489, 6490, 295, 341, 1508, 321, 360, 264, 912, 337, 264, 7197, 293, 321, 362, 613, 732, 1489, 6490, 613, 732, 51796], "temperature": 0.0, "avg_logprob": -0.11509625856266466, "compression_ratio": 2.005434782608696, "no_speech_prob": 0.008757242932915688}, {"id": 157, "seek": 128080, "start": 1280.8, "end": 1288.1599999999999, "text": " quantum centroid these two quantum centroid define the finding are two density operator", "tokens": [50364, 13018, 1489, 6490, 613, 732, 13018, 1489, 6490, 6964, 264, 5006, 366, 732, 10305, 12973, 50732], "temperature": 0.0, "avg_logprob": -0.2630370338008089, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.014000057242810726}, {"id": 158, "seek": 128080, "start": 1288.1599999999999, "end": 1294.56, "text": " and from this from these two quantum centroids we can apply the Elstrom", "tokens": [50732, 293, 490, 341, 490, 613, 732, 13018, 1489, 340, 3742, 321, 393, 3079, 264, 2699, 38673, 51052], "temperature": 0.0, "avg_logprob": -0.2630370338008089, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.014000057242810726}, {"id": 159, "seek": 128080, "start": 1297.36, "end": 1304.3999999999999, "text": " discrimination and we cannot take the Elstrom measurement the optimal Elstrom measurement", "tokens": [51192, 15973, 293, 321, 2644, 747, 264, 2699, 38673, 13160, 264, 16252, 2699, 38673, 13160, 51544], "temperature": 0.0, "avg_logprob": -0.2630370338008089, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.014000057242810726}, {"id": 160, "seek": 130440, "start": 1304.4, "end": 1315.52, "text": " p plus and p minus okay so once we pick another quantum object from the test set we have to do", "tokens": [50364, 280, 1804, 293, 280, 3175, 1392, 370, 1564, 321, 1888, 1071, 13018, 2657, 490, 264, 1500, 992, 321, 362, 281, 360, 50920], "temperature": 0.0, "avg_logprob": -0.09715736273563269, "compression_ratio": 1.8066666666666666, "no_speech_prob": 0.012244137935340405}, {"id": 161, "seek": 130440, "start": 1315.52, "end": 1322.48, "text": " we have to say okay this quantum object is a set or is a dog in order to determine if this", "tokens": [50920, 321, 362, 281, 584, 1392, 341, 13018, 2657, 307, 257, 992, 420, 307, 257, 3000, 294, 1668, 281, 6997, 498, 341, 51268], "temperature": 0.0, "avg_logprob": -0.09715736273563269, "compression_ratio": 1.8066666666666666, "no_speech_prob": 0.012244137935340405}, {"id": 162, "seek": 130440, "start": 1323.2800000000002, "end": 1330.0, "text": " we consider this classifier that is called the Elstrom quantum classifier we consider", "tokens": [51308, 321, 1949, 341, 1508, 9902, 300, 307, 1219, 264, 2699, 38673, 13018, 1508, 9902, 321, 1949, 51644], "temperature": 0.0, "avg_logprob": -0.09715736273563269, "compression_ratio": 1.8066666666666666, "no_speech_prob": 0.012244137935340405}, {"id": 163, "seek": 133000, "start": 1330.96, "end": 1342.08, "text": " these two values if the trace of p plus times rho is greater than p minus times rho we say that", "tokens": [50412, 613, 732, 4190, 498, 264, 13508, 295, 280, 1804, 1413, 20293, 307, 5044, 813, 280, 3175, 1413, 20293, 321, 584, 300, 50968], "temperature": 0.0, "avg_logprob": -0.08095389604568481, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.01571238785982132}, {"id": 164, "seek": 133000, "start": 1343.04, "end": 1352.08, "text": " in a certain sense rho x is closer to the centroid of the cat otherwise rho x is closer to", "tokens": [51016, 294, 257, 1629, 2020, 20293, 2031, 307, 4966, 281, 264, 1489, 6490, 295, 264, 3857, 5911, 20293, 2031, 307, 4966, 281, 51468], "temperature": 0.0, "avg_logprob": -0.08095389604568481, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.01571238785982132}, {"id": 165, "seek": 135208, "start": 1352.08, "end": 1361.52, "text": " the centroid of of the dog and so we can make a really a proper classification a proper classification", "tokens": [50364, 264, 1489, 6490, 295, 295, 264, 3000, 293, 370, 321, 393, 652, 257, 534, 257, 2296, 21538, 257, 2296, 21538, 50836], "temperature": 0.0, "avg_logprob": -0.0757364580186747, "compression_ratio": 1.9470198675496688, "no_speech_prob": 0.00931911263614893}, {"id": 166, "seek": 135208, "start": 1361.52, "end": 1370.32, "text": " okay why we use quantum state discrimination what is the intuition that that is above this idea", "tokens": [50836, 1392, 983, 321, 764, 13018, 1785, 15973, 437, 307, 264, 24002, 300, 300, 307, 3673, 341, 1558, 51276], "temperature": 0.0, "avg_logprob": -0.0757364580186747, "compression_ratio": 1.9470198675496688, "no_speech_prob": 0.00931911263614893}, {"id": 167, "seek": 135208, "start": 1370.8799999999999, "end": 1376.24, "text": " the application of quantum state discrimination for classification is inspired by the idea that", "tokens": [51304, 264, 3861, 295, 13018, 1785, 15973, 337, 21538, 307, 7547, 538, 264, 1558, 300, 51572], "temperature": 0.0, "avg_logprob": -0.0757364580186747, "compression_ratio": 1.9470198675496688, "no_speech_prob": 0.00931911263614893}, {"id": 168, "seek": 137624, "start": 1376.88, "end": 1385.04, "text": " better is the discrimination between two centroid and better I can distinguish between two classes", "tokens": [50396, 1101, 307, 264, 15973, 1296, 732, 1489, 6490, 293, 1101, 286, 393, 20206, 1296, 732, 5359, 50804], "temperature": 0.0, "avg_logprob": -0.1024812746651565, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.009957804344594479}, {"id": 169, "seek": 137624, "start": 1385.04, "end": 1393.1200000000001, "text": " and so more performance is the classifier in other words greater is the Elstrom bound that", "tokens": [50804, 293, 370, 544, 3389, 307, 264, 1508, 9902, 294, 661, 2283, 5044, 307, 264, 2699, 38673, 5472, 300, 51208], "temperature": 0.0, "avg_logprob": -0.1024812746651565, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.009957804344594479}, {"id": 170, "seek": 137624, "start": 1393.1200000000001, "end": 1398.08, "text": " remind me I said it is that can be considered as a measurement of distinguishability between two", "tokens": [51208, 4160, 385, 286, 848, 309, 307, 300, 393, 312, 4888, 382, 257, 13160, 295, 20206, 2310, 1296, 732, 51456], "temperature": 0.0, "avg_logprob": -0.1024812746651565, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.009957804344594479}, {"id": 171, "seek": 137624, "start": 1398.08, "end": 1405.36, "text": " centroid and greater would be for instance the accuracy of the classifier where the accuracy", "tokens": [51456, 1489, 6490, 293, 5044, 576, 312, 337, 5197, 264, 14170, 295, 264, 1508, 9902, 689, 264, 14170, 51820], "temperature": 0.0, "avg_logprob": -0.1024812746651565, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.009957804344594479}, {"id": 172, "seek": 140536, "start": 1405.36, "end": 1414.4799999999998, "text": " means just the number of times that the classification was correct over the number of total over the", "tokens": [50364, 1355, 445, 264, 1230, 295, 1413, 300, 264, 21538, 390, 3006, 670, 264, 1230, 295, 3217, 670, 264, 50820], "temperature": 0.0, "avg_logprob": -0.09796253840128581, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0023480767849832773}, {"id": 173, "seek": 140536, "start": 1414.4799999999998, "end": 1424.32, "text": " total number of experiments okay okay this idea is not only an intuitive idea but that was just", "tokens": [50820, 3217, 1230, 295, 12050, 1392, 1392, 341, 1558, 307, 406, 787, 364, 21769, 1558, 457, 300, 390, 445, 51312], "temperature": 0.0, "avg_logprob": -0.09796253840128581, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0023480767849832773}, {"id": 174, "seek": 142432, "start": 1425.28, "end": 1436.0, "text": " exploited that was just proven by an experiment so we have provided an experiment where this", "tokens": [50412, 40918, 300, 390, 445, 12785, 538, 364, 5120, 370, 321, 362, 5649, 364, 5120, 689, 341, 50948], "temperature": 0.0, "avg_logprob": -0.19565454282258687, "compression_ratio": 1.4715447154471544, "no_speech_prob": 0.017732728272676468}, {"id": 175, "seek": 142432, "start": 1436.0, "end": 1447.76, "text": " classifier has been applied to 40 different data sets and compared with 11 different and", "tokens": [50948, 1508, 9902, 575, 668, 6456, 281, 3356, 819, 1412, 6352, 293, 5347, 365, 2975, 819, 293, 51536], "temperature": 0.0, "avg_logprob": -0.19565454282258687, "compression_ratio": 1.4715447154471544, "no_speech_prob": 0.017732728272676468}, {"id": 176, "seek": 144776, "start": 1447.84, "end": 1454.56, "text": " general well-performing classifier standard classifier for instance what we can see is that", "tokens": [50368, 2674, 731, 12, 26765, 278, 1508, 9902, 3832, 1508, 9902, 337, 5197, 437, 321, 393, 536, 307, 300, 50704], "temperature": 0.0, "avg_logprob": -0.1533607309514826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.007570204325020313}, {"id": 177, "seek": 144776, "start": 1454.56, "end": 1461.68, "text": " generally generally once the Elstrom bound increase the Elstrom bound increases", "tokens": [50704, 5101, 5101, 1564, 264, 2699, 38673, 5472, 3488, 264, 2699, 38673, 5472, 8637, 51060], "temperature": 0.0, "avg_logprob": -0.1533607309514826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.007570204325020313}, {"id": 178, "seek": 144776, "start": 1462.4, "end": 1469.76, "text": " together with the increasing of the balance of accuracy so this idea seems to be correct", "tokens": [51096, 1214, 365, 264, 5662, 295, 264, 4772, 295, 14170, 370, 341, 1558, 2544, 281, 312, 3006, 51464], "temperature": 0.0, "avg_logprob": -0.1533607309514826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.007570204325020313}, {"id": 179, "seek": 146976, "start": 1469.76, "end": 1476.24, "text": " that increasing the measurement of distinguishability between two centroid it", "tokens": [50364, 300, 5662, 264, 13160, 295, 20206, 2310, 1296, 732, 1489, 6490, 309, 50688], "temperature": 0.0, "avg_logprob": -0.09656030253360146, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.013327606953680515}, {"id": 180, "seek": 146976, "start": 1477.36, "end": 1484.32, "text": " is together with the increasing of the accuracy of the classifier so this intuition is corroborated", "tokens": [50744, 307, 1214, 365, 264, 5662, 295, 264, 14170, 295, 264, 1508, 9902, 370, 341, 24002, 307, 45125, 3918, 770, 51092], "temperature": 0.0, "avg_logprob": -0.09656030253360146, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.013327606953680515}, {"id": 181, "seek": 146976, "start": 1484.32, "end": 1491.04, "text": " by the experience is corroborated by the experience and okay we have many data when we", "tokens": [51092, 538, 264, 1752, 307, 45125, 3918, 770, 538, 264, 1752, 293, 1392, 321, 362, 867, 1412, 562, 321, 51428], "temperature": 0.0, "avg_logprob": -0.09656030253360146, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.013327606953680515}, {"id": 182, "seek": 149104, "start": 1492.0, "end": 1500.0, "text": " perform it when we compare this classifier is Elstrom classifier with many standard classifier", "tokens": [50412, 2042, 309, 562, 321, 6794, 341, 1508, 9902, 307, 2699, 38673, 1508, 9902, 365, 867, 3832, 1508, 9902, 50812], "temperature": 0.0, "avg_logprob": -0.14664875666300456, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.021315310150384903}, {"id": 183, "seek": 149104, "start": 1500.0, "end": 1509.36, "text": " and we show that our classifier is generally better not always not always but okay the average", "tokens": [50812, 293, 321, 855, 300, 527, 1508, 9902, 307, 5101, 1101, 406, 1009, 406, 1009, 457, 1392, 264, 4274, 51280], "temperature": 0.0, "avg_logprob": -0.14664875666300456, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.021315310150384903}, {"id": 184, "seek": 149104, "start": 1512.1599999999999, "end": 1520.96, "text": " performance of the quantum inspired classifier is very very high with respect to the others", "tokens": [51420, 3389, 295, 264, 13018, 7547, 1508, 9902, 307, 588, 588, 1090, 365, 3104, 281, 264, 2357, 51860], "temperature": 0.0, "avg_logprob": -0.14664875666300456, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.021315310150384903}, {"id": 185, "seek": 152096, "start": 1520.96, "end": 1527.3600000000001, "text": " standard classical classifier okay now I have many data to show but okay this is", "tokens": [50364, 3832, 13735, 1508, 9902, 1392, 586, 286, 362, 867, 1412, 281, 855, 457, 1392, 341, 307, 50684], "temperature": 0.0, "avg_logprob": -0.16130022142754227, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.010621579363942146}, {"id": 186, "seek": 152096, "start": 1528.24, "end": 1535.44, "text": " just a comparison okay are we okay in these data in these tables you can realize these", "tokens": [50728, 445, 257, 9660, 1392, 366, 321, 1392, 294, 613, 1412, 294, 613, 8020, 291, 393, 4325, 613, 51088], "temperature": 0.0, "avg_logprob": -0.16130022142754227, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.010621579363942146}, {"id": 187, "seek": 152096, "start": 1538.32, "end": 1545.2, "text": " I don't say supremacy but something like this of our Elstrom classifier with respect to the other", "tokens": [51232, 286, 500, 380, 584, 35572, 457, 746, 411, 341, 295, 527, 2699, 38673, 1508, 9902, 365, 3104, 281, 264, 661, 51576], "temperature": 0.0, "avg_logprob": -0.16130022142754227, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.010621579363942146}, {"id": 188, "seek": 154520, "start": 1545.28, "end": 1551.2, "text": " but that's not all that's not all indeed a sharp difference between classical and quantum", "tokens": [50368, 457, 300, 311, 406, 439, 300, 311, 406, 439, 6451, 257, 8199, 2649, 1296, 13735, 293, 13018, 50664], "temperature": 0.0, "avg_logprob": -0.09420315206867375, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.012573913671076298}, {"id": 189, "seek": 154520, "start": 1551.2, "end": 1558.48, "text": " information theory is also based on the fact that sometime made considering a tensor copy", "tokens": [50664, 1589, 5261, 307, 611, 2361, 322, 264, 1186, 300, 15053, 1027, 8079, 257, 40863, 5055, 51028], "temperature": 0.0, "avg_logprob": -0.09420315206867375, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.012573913671076298}, {"id": 190, "seek": 154520, "start": 1558.48, "end": 1565.6000000000001, "text": " of an object can provide a benefit in our computation can provide let's say a kind of", "tokens": [51028, 295, 364, 2657, 393, 2893, 257, 5121, 294, 527, 24903, 393, 2893, 718, 311, 584, 257, 733, 295, 51384], "temperature": 0.0, "avg_logprob": -0.09420315206867375, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.012573913671076298}, {"id": 191, "seek": 154520, "start": 1565.6000000000001, "end": 1572.56, "text": " additional information with respect to the original information that is given by the", "tokens": [51384, 4497, 1589, 365, 3104, 281, 264, 3380, 1589, 300, 307, 2212, 538, 264, 51732], "temperature": 0.0, "avg_logprob": -0.09420315206867375, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.012573913671076298}, {"id": 192, "seek": 157256, "start": 1572.56, "end": 1581.12, "text": " initial single state so what we do is just repeat exactly the same algorithm that I have described", "tokens": [50364, 5883, 2167, 1785, 370, 437, 321, 360, 307, 445, 7149, 2293, 264, 912, 9284, 300, 286, 362, 7619, 50792], "temperature": 0.0, "avg_logprob": -0.12611214045820565, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.010726860724389553}, {"id": 193, "seek": 157256, "start": 1581.12, "end": 1589.44, "text": " you above before but by considering but off after the encoding after the encoding instead of the", "tokens": [50792, 291, 3673, 949, 457, 538, 8079, 457, 766, 934, 264, 43430, 934, 264, 43430, 2602, 295, 264, 51208], "temperature": 0.0, "avg_logprob": -0.12611214045820565, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.010726860724389553}, {"id": 194, "seek": 157256, "start": 1589.44, "end": 1596.32, "text": " density operator raw the time obtained from the encoding by the encoding from the initial", "tokens": [51208, 10305, 12973, 8936, 264, 565, 14879, 490, 264, 43430, 538, 264, 43430, 490, 264, 5883, 51552], "temperature": 0.0, "avg_logprob": -0.12611214045820565, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.010726860724389553}, {"id": 195, "seek": 159632, "start": 1596.32, "end": 1604.48, "text": " object instead of this row I consider all times itself and times I calculate a new centroid", "tokens": [50364, 2657, 2602, 295, 341, 5386, 286, 1949, 439, 1413, 2564, 293, 1413, 286, 8873, 257, 777, 1489, 6490, 50772], "temperature": 0.0, "avg_logprob": -0.1917903703801772, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.03417853266000748}, {"id": 196, "seek": 159632, "start": 1604.48, "end": 1613.12, "text": " and I define again a new classifier in a new Elstrom of several in the new dimension has been a new", "tokens": [50772, 293, 286, 6964, 797, 257, 777, 1508, 9902, 294, 257, 777, 2699, 38673, 295, 2940, 294, 264, 777, 10139, 575, 668, 257, 777, 51204], "temperature": 0.0, "avg_logprob": -0.1917903703801772, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.03417853266000748}, {"id": 197, "seek": 159632, "start": 1613.12, "end": 1621.9199999999998, "text": " k dimensional space in a new larger space and again I define a new Elstrom quantum classifier", "tokens": [51204, 350, 18795, 1901, 294, 257, 777, 4833, 1901, 293, 797, 286, 6964, 257, 777, 2699, 38673, 13018, 1508, 9902, 51644], "temperature": 0.0, "avg_logprob": -0.1917903703801772, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.03417853266000748}, {"id": 198, "seek": 162192, "start": 1622.8000000000002, "end": 1632.16, "text": " let's say k tensor product Elstrom quantum classifier what we see what we see is that by increasing", "tokens": [50408, 718, 311, 584, 350, 40863, 1674, 2699, 38673, 13018, 1508, 9902, 437, 321, 536, 437, 321, 536, 307, 300, 538, 5662, 50876], "temperature": 0.0, "avg_logprob": -0.14654666727239435, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.012004713527858257}, {"id": 199, "seek": 162192, "start": 1632.16, "end": 1639.68, "text": " the number of the copy also there's strong bound increase and so we see that the accuracy that", "tokens": [50876, 264, 1230, 295, 264, 5055, 611, 456, 311, 2068, 5472, 3488, 293, 370, 321, 536, 300, 264, 14170, 300, 51252], "temperature": 0.0, "avg_logprob": -0.14654666727239435, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.012004713527858257}, {"id": 200, "seek": 162192, "start": 1639.68, "end": 1646.96, "text": " we obtain after making this copy this copy in a just in a computational way just by making", "tokens": [51252, 321, 12701, 934, 1455, 341, 5055, 341, 5055, 294, 257, 445, 294, 257, 28270, 636, 445, 538, 1455, 51616], "temperature": 0.0, "avg_logprob": -0.14654666727239435, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.012004713527858257}, {"id": 201, "seek": 164696, "start": 1647.3600000000001, "end": 1655.04, "text": " a in bi-mathematica okay or by fighting this copy but what we see is that by making this", "tokens": [50384, 257, 294, 3228, 12, 24761, 8615, 2262, 1392, 420, 538, 5237, 341, 5055, 457, 437, 321, 536, 307, 300, 538, 1455, 341, 50768], "temperature": 0.0, "avg_logprob": -0.16117150855786871, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.026501599699258804}, {"id": 202, "seek": 164696, "start": 1655.04, "end": 1663.6000000000001, "text": " procedure we have benefit because we increase the accuracy of the of the process we increase", "tokens": [50768, 10747, 321, 362, 5121, 570, 321, 3488, 264, 14170, 295, 264, 295, 264, 1399, 321, 3488, 51196], "temperature": 0.0, "avg_logprob": -0.16117150855786871, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.026501599699258804}, {"id": 203, "seek": 164696, "start": 1663.6000000000001, "end": 1672.0, "text": " again the accuracy of the process okay and again we have data to to show okay we have something but", "tokens": [51196, 797, 264, 14170, 295, 264, 1399, 1392, 293, 797, 321, 362, 1412, 281, 281, 855, 1392, 321, 362, 746, 457, 51616], "temperature": 0.0, "avg_logprob": -0.16117150855786871, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.026501599699258804}, {"id": 204, "seek": 167200, "start": 1672.0, "end": 1681.04, "text": " okay I want to use just the last five minutes talk maybe to show you a practical experiment", "tokens": [50364, 1392, 286, 528, 281, 764, 445, 264, 1036, 1732, 2077, 751, 1310, 281, 855, 291, 257, 8496, 5120, 50816], "temperature": 0.0, "avg_logprob": -0.12965037392788245, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.015291554853320122}, {"id": 205, "seek": 167200, "start": 1681.04, "end": 1689.76, "text": " a practical experiment that was provided together with the University of Cambridge and the Institute", "tokens": [50816, 257, 8496, 5120, 300, 390, 5649, 1214, 365, 264, 3535, 295, 24876, 293, 264, 9446, 51252], "temperature": 0.0, "avg_logprob": -0.12965037392788245, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.015291554853320122}, {"id": 206, "seek": 167200, "start": 1689.76, "end": 1698.32, "text": " of Molecular Bioimaging of the University of Catania and the topic is a chronogenic essay", "tokens": [51252, 295, 46914, 17792, 26840, 332, 3568, 295, 264, 3535, 295, 9565, 5609, 293, 264, 4829, 307, 257, 19393, 25473, 16238, 51680], "temperature": 0.0, "avg_logprob": -0.12965037392788245, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.015291554853320122}, {"id": 207, "seek": 169832, "start": 1698.32, "end": 1705.6, "text": " the chronogenic essay is a quantification technique of the survival degree of an in vitro cell", "tokens": [50364, 264, 19393, 25473, 16238, 307, 257, 4426, 3774, 6532, 295, 264, 12559, 4314, 295, 364, 294, 9467, 340, 2815, 50728], "temperature": 0.0, "avg_logprob": -0.11796649694442748, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.012890092097222805}, {"id": 208, "seek": 169832, "start": 1705.6, "end": 1712.72, "text": " cultures which is based on the ability of a single cell to grow and form a colony of cell", "tokens": [50728, 12951, 597, 307, 2361, 322, 264, 3485, 295, 257, 2167, 2815, 281, 1852, 293, 1254, 257, 23028, 295, 2815, 51084], "temperature": 0.0, "avg_logprob": -0.11796649694442748, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.012890092097222805}, {"id": 209, "seek": 169832, "start": 1712.72, "end": 1719.4399999999998, "text": " this colony is not a good thing that is a symbol of something with a of something", "tokens": [51084, 341, 23028, 307, 406, 257, 665, 551, 300, 307, 257, 5986, 295, 746, 365, 257, 295, 746, 51420], "temperature": 0.0, "avg_logprob": -0.11796649694442748, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.012890092097222805}, {"id": 210, "seek": 169832, "start": 1719.4399999999998, "end": 1726.48, "text": " disease something great disease the purpose is to count the number of their colonies so", "tokens": [51420, 4752, 746, 869, 4752, 264, 4334, 307, 281, 1207, 264, 1230, 295, 641, 27981, 370, 51772], "temperature": 0.0, "avg_logprob": -0.11796649694442748, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.012890092097222805}, {"id": 211, "seek": 172648, "start": 1726.56, "end": 1732.96, "text": " the picture that we are today like picture like this where this is a colony this is a colony", "tokens": [50368, 264, 3036, 300, 321, 366, 965, 411, 3036, 411, 341, 689, 341, 307, 257, 23028, 341, 307, 257, 23028, 50688], "temperature": 0.0, "avg_logprob": -0.14287753332228886, "compression_ratio": 1.9357142857142857, "no_speech_prob": 0.004396462347358465}, {"id": 212, "seek": 172648, "start": 1732.96, "end": 1742.32, "text": " this is a colony and so on okay recent results show how bi-classification between pixel x", "tokens": [50688, 341, 307, 257, 23028, 293, 370, 322, 1392, 5162, 3542, 855, 577, 3228, 12, 11665, 3774, 1296, 19261, 2031, 51156], "temperature": 0.0, "avg_logprob": -0.14287753332228886, "compression_ratio": 1.9357142857142857, "no_speech_prob": 0.004396462347358465}, {"id": 213, "seek": 172648, "start": 1743.92, "end": 1751.04, "text": " belongs to the colony or pixel x belongs to the belongs to the background by making this", "tokens": [51236, 12953, 281, 264, 23028, 420, 19261, 2031, 12953, 281, 264, 12953, 281, 264, 3678, 538, 1455, 341, 51592], "temperature": 0.0, "avg_logprob": -0.14287753332228886, "compression_ratio": 1.9357142857142857, "no_speech_prob": 0.004396462347358465}, {"id": 214, "seek": 175104, "start": 1751.04, "end": 1757.52, "text": " classification it is possible to have information about the number of of the colonies there is a", "tokens": [50364, 21538, 309, 307, 1944, 281, 362, 1589, 466, 264, 1230, 295, 295, 264, 27981, 456, 307, 257, 50688], "temperature": 0.0, "avg_logprob": -0.19011432893814578, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.021065089851617813}, {"id": 215, "seek": 175104, "start": 1757.52, "end": 1766.08, "text": " correlation between this this is a biological result in a in a in a quantum imagine and biology", "tokens": [50688, 20009, 1296, 341, 341, 307, 257, 13910, 1874, 294, 257, 294, 257, 294, 257, 13018, 3811, 293, 14956, 51116], "temperature": 0.0, "avg_logprob": -0.19011432893814578, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.021065089851617813}, {"id": 216, "seek": 175104, "start": 1767.92, "end": 1776.1599999999999, "text": " fields that this result but it is it is an assist for us because it allows us to move", "tokens": [51208, 7909, 300, 341, 1874, 457, 309, 307, 309, 307, 364, 4255, 337, 505, 570, 309, 4045, 505, 281, 1286, 51620], "temperature": 0.0, "avg_logprob": -0.19011432893814578, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.021065089851617813}, {"id": 217, "seek": 177616, "start": 1776.16, "end": 1783.2, "text": " the chronogenic essay in a binary classification context so for each pixel if we are able to", "tokens": [50364, 264, 19393, 25473, 16238, 294, 257, 17434, 21538, 4319, 370, 337, 1184, 19261, 498, 321, 366, 1075, 281, 50716], "temperature": 0.0, "avg_logprob": -0.15915221878976532, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.025590144097805023}, {"id": 218, "seek": 177616, "start": 1783.2, "end": 1790.8000000000002, "text": " classify if this pixel is in the colony or it is in the in the background we can have some result", "tokens": [50716, 33872, 498, 341, 19261, 307, 294, 264, 23028, 420, 309, 307, 294, 264, 294, 264, 3678, 321, 393, 362, 512, 1874, 51096], "temperature": 0.0, "avg_logprob": -0.15915221878976532, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.025590144097805023}, {"id": 219, "seek": 177616, "start": 1790.8000000000002, "end": 1799.52, "text": " regarding the number of the code that is in the chronogenic essay and so you have 10 minutes", "tokens": [51096, 8595, 264, 1230, 295, 264, 3089, 300, 307, 294, 264, 19393, 25473, 16238, 293, 370, 291, 362, 1266, 2077, 51532], "temperature": 0.0, "avg_logprob": -0.15915221878976532, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.025590144097805023}, {"id": 220, "seek": 179952, "start": 1799.6, "end": 1807.04, "text": " including the question okay so just I thank you I just spend a few minutes to conclude", "tokens": [50368, 3009, 264, 1168, 1392, 370, 445, 286, 1309, 291, 286, 445, 3496, 257, 1326, 2077, 281, 16886, 50740], "temperature": 0.0, "avg_logprob": -0.19185590744018555, "compression_ratio": 1.6518987341772151, "no_speech_prob": 0.03520521894097328}, {"id": 221, "seek": 179952, "start": 1807.04, "end": 1816.24, "text": " and I and I leave a few minutes also for four questions okay okay so the experiment", "tokens": [50740, 293, 286, 293, 286, 1856, 257, 1326, 2077, 611, 337, 1451, 1651, 1392, 1392, 370, 264, 5120, 51200], "temperature": 0.0, "avg_logprob": -0.19185590744018555, "compression_ratio": 1.6518987341772151, "no_speech_prob": 0.03520521894097328}, {"id": 222, "seek": 179952, "start": 1816.24, "end": 1823.52, "text": " involved with the many many many data about 10 millions of data and the result that we had", "tokens": [51200, 3288, 365, 264, 867, 867, 867, 1412, 466, 1266, 6803, 295, 1412, 293, 264, 1874, 300, 321, 632, 51564], "temperature": 0.0, "avg_logprob": -0.19185590744018555, "compression_ratio": 1.6518987341772151, "no_speech_prob": 0.03520521894097328}, {"id": 223, "seek": 182352, "start": 1823.52, "end": 1830.16, "text": " was against that the performance of our elstrom classifier with respect to other in this case", "tokens": [50364, 390, 1970, 300, 264, 3389, 295, 527, 806, 38673, 1508, 9902, 365, 3104, 281, 661, 294, 341, 1389, 50696], "temperature": 0.0, "avg_logprob": -0.0842051861891106, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.034694086760282516}, {"id": 224, "seek": 182352, "start": 1830.72, "end": 1838.08, "text": " 18 well-performance classifier is very good because our classifier in most of the cases", "tokens": [50724, 2443, 731, 12, 50242, 1508, 9902, 307, 588, 665, 570, 527, 1508, 9902, 294, 881, 295, 264, 3331, 51092], "temperature": 0.0, "avg_logprob": -0.0842051861891106, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.034694086760282516}, {"id": 225, "seek": 182352, "start": 1838.08, "end": 1846.08, "text": " was one of the best one of the best with respect to to the other and that was very very nice for us", "tokens": [51092, 390, 472, 295, 264, 1151, 472, 295, 264, 1151, 365, 3104, 281, 281, 264, 661, 293, 300, 390, 588, 588, 1481, 337, 505, 51492], "temperature": 0.0, "avg_logprob": -0.0842051861891106, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.034694086760282516}, {"id": 226, "seek": 184608, "start": 1846.08, "end": 1853.6, "text": " so let me conclude with some open problem okay the first open problem is the is this", "tokens": [50364, 370, 718, 385, 16886, 365, 512, 1269, 1154, 1392, 264, 700, 1269, 1154, 307, 264, 307, 341, 50740], "temperature": 0.0, "avg_logprob": -0.14715704103795493, "compression_ratio": 2.015873015873016, "no_speech_prob": 0.009537053294479847}, {"id": 227, "seek": 184608, "start": 1854.56, "end": 1861.4399999999998, "text": " the generalization of this quantum is pirate quantum is pirate and now we know why I say quantum", "tokens": [50788, 264, 2674, 2144, 295, 341, 13018, 307, 27424, 13018, 307, 27424, 293, 586, 321, 458, 983, 286, 584, 13018, 51132], "temperature": 0.0, "avg_logprob": -0.14715704103795493, "compression_ratio": 2.015873015873016, "no_speech_prob": 0.009537053294479847}, {"id": 228, "seek": 184608, "start": 1861.4399999999998, "end": 1867.04, "text": " is pirate and now we know why I really say that this this inspiration is useful it's not only", "tokens": [51132, 307, 27424, 293, 586, 321, 458, 983, 286, 534, 584, 300, 341, 341, 10249, 307, 4420, 309, 311, 406, 787, 51412], "temperature": 0.0, "avg_logprob": -0.14715704103795493, "compression_ratio": 2.015873015873016, "no_speech_prob": 0.009537053294479847}, {"id": 229, "seek": 184608, "start": 1867.04, "end": 1875.52, "text": " a matter of metaphor but this quantum inspiration was based on only a binary quantum state discrimination", "tokens": [51412, 257, 1871, 295, 19157, 457, 341, 13018, 10249, 390, 2361, 322, 787, 257, 17434, 13018, 1785, 15973, 51836], "temperature": 0.0, "avg_logprob": -0.14715704103795493, "compression_ratio": 2.015873015873016, "no_speech_prob": 0.009537053294479847}, {"id": 230, "seek": 187608, "start": 1876.3999999999999, "end": 1884.3999999999999, "text": " so the challenge is to have a multiclass classifier a multiclass classifier and the second goal", "tokens": [50380, 370, 264, 3430, 307, 281, 362, 257, 30608, 14549, 1508, 9902, 257, 30608, 14549, 1508, 9902, 293, 264, 1150, 3387, 50780], "temperature": 0.0, "avg_logprob": -0.16405008756197414, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.005654752720147371}, {"id": 231, "seek": 187608, "start": 1884.3999999999999, "end": 1893.4399999999998, "text": " should be just to use this quantum inspired algorithm also in a quantum computer just in order", "tokens": [50780, 820, 312, 445, 281, 764, 341, 13018, 7547, 9284, 611, 294, 257, 13018, 3820, 445, 294, 1668, 51232], "temperature": 0.0, "avg_logprob": -0.16405008756197414, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.005654752720147371}, {"id": 232, "seek": 187608, "start": 1893.4399999999998, "end": 1902.96, "text": " to come from one to be spirited to one so we have some partial result regarding both of these", "tokens": [51232, 281, 808, 490, 472, 281, 312, 10733, 1226, 281, 472, 370, 321, 362, 512, 14641, 1874, 8595, 1293, 295, 613, 51708], "temperature": 0.0, "avg_logprob": -0.16405008756197414, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.005654752720147371}, {"id": 233, "seek": 190296, "start": 1903.52, "end": 1911.1200000000001, "text": " this point and basically regarding the idea of multiclass classifier we are working on this but", "tokens": [50392, 341, 935, 293, 1936, 8595, 264, 1558, 295, 30608, 14549, 1508, 9902, 321, 366, 1364, 322, 341, 457, 50772], "temperature": 0.0, "avg_logprob": -0.16544146456960904, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.02097870223224163}, {"id": 234, "seek": 190296, "start": 1912.08, "end": 1920.64, "text": " we are we are taking inspiration from the pretty good measurement that is not an optimal but", "tokens": [50820, 321, 366, 321, 366, 1940, 10249, 490, 264, 1238, 665, 13160, 300, 307, 406, 364, 16252, 457, 51248], "temperature": 0.0, "avg_logprob": -0.16544146456960904, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.02097870223224163}, {"id": 235, "seek": 190296, "start": 1920.64, "end": 1930.08, "text": " sub the optimal measurement that allows us to have a multi a quantum multiclassifier", "tokens": [51248, 1422, 264, 16252, 13160, 300, 4045, 505, 281, 362, 257, 4825, 257, 13018, 30608, 14549, 9902, 51720], "temperature": 0.0, "avg_logprob": -0.16544146456960904, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.02097870223224163}, {"id": 236, "seek": 193008, "start": 1930.96, "end": 1937.6799999999998, "text": " and on the other hand well I am not time to show you the detail of the pretty good", "tokens": [50408, 293, 322, 264, 661, 1011, 731, 286, 669, 406, 565, 281, 855, 291, 264, 2607, 295, 264, 1238, 665, 50744], "temperature": 0.0, "avg_logprob": -0.146057587635668, "compression_ratio": 1.83419689119171, "no_speech_prob": 0.01183279324322939}, {"id": 237, "seek": 193008, "start": 1937.6799999999998, "end": 1943.28, "text": " classifier but the idea is the same but instead of to have an optimal we have a sub optimal", "tokens": [50744, 1508, 9902, 457, 264, 1558, 307, 264, 912, 457, 2602, 295, 281, 362, 364, 16252, 321, 362, 257, 1422, 16252, 51024], "temperature": 0.0, "avg_logprob": -0.146057587635668, "compression_ratio": 1.83419689119171, "no_speech_prob": 0.01183279324322939}, {"id": 238, "seek": 193008, "start": 1943.28, "end": 1951.84, "text": " measurement but but also we this sub optimal measurement can be used for any classification", "tokens": [51024, 13160, 457, 457, 611, 321, 341, 1422, 16252, 13160, 393, 312, 1143, 337, 604, 21538, 51452], "temperature": 0.0, "avg_logprob": -0.146057587635668, "compression_ratio": 1.83419689119171, "no_speech_prob": 0.01183279324322939}, {"id": 239, "seek": 193008, "start": 1951.84, "end": 1959.76, "text": " instead of only binary classification and also the final challenge as I said is just to", "tokens": [51452, 2602, 295, 787, 17434, 21538, 293, 611, 264, 2572, 3430, 382, 286, 848, 307, 445, 281, 51848], "temperature": 0.0, "avg_logprob": -0.146057587635668, "compression_ratio": 1.83419689119171, "no_speech_prob": 0.01183279324322939}, {"id": 240, "seek": 196008, "start": 1961.04, "end": 1968.72, "text": " come from a quantum inspired to really quantum machine learning in order to put together the", "tokens": [50412, 808, 490, 257, 13018, 7547, 281, 534, 13018, 3479, 2539, 294, 1668, 281, 829, 1214, 264, 50796], "temperature": 0.0, "avg_logprob": -0.151136272831967, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0043594189919531345}, {"id": 241, "seek": 196008, "start": 1968.72, "end": 1974.8799999999999, "text": " double benefit of quantum so increase the accuracy but also speed up the computation", "tokens": [50796, 3834, 5121, 295, 13018, 370, 3488, 264, 14170, 457, 611, 3073, 493, 264, 24903, 51104], "temperature": 0.0, "avg_logprob": -0.151136272831967, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0043594189919531345}, {"id": 242, "seek": 196008, "start": 1975.52, "end": 1982.08, "text": " well by appealing to the neomarks deletion theorem is it possible in principle to reproduce", "tokens": [51136, 731, 538, 23842, 281, 264, 408, 298, 20851, 1103, 302, 313, 20904, 307, 309, 1944, 294, 8665, 281, 29501, 51464], "temperature": 0.0, "avg_logprob": -0.151136272831967, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0043594189919531345}, {"id": 243, "seek": 196008, "start": 1982.08, "end": 1988.56, "text": " and POVM measurement and for instance also the health strong of the pretty good measurement", "tokens": [51464, 293, 22299, 53, 44, 13160, 293, 337, 5197, 611, 264, 1585, 2068, 295, 264, 1238, 665, 13160, 51788], "temperature": 0.0, "avg_logprob": -0.151136272831967, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0043594189919531345}, {"id": 244, "seek": 198856, "start": 1988.6399999999999, "end": 1995.6, "text": " this is an this is an example but we are working on this and till now we have only only partial", "tokens": [50368, 341, 307, 364, 341, 307, 364, 1365, 457, 321, 366, 1364, 322, 341, 293, 4288, 586, 321, 362, 787, 787, 14641, 50716], "temperature": 0.0, "avg_logprob": -0.15241436660289764, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.018718376755714417}, {"id": 245, "seek": 198856, "start": 1995.6, "end": 2004.48, "text": " result regarding this but the works are still in progress and okay this is the the bibliography", "tokens": [50716, 1874, 8595, 341, 457, 264, 1985, 366, 920, 294, 4205, 293, 1392, 341, 307, 264, 264, 34344, 5820, 51160], "temperature": 0.0, "avg_logprob": -0.15241436660289764, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.018718376755714417}, {"id": 246, "seek": 198856, "start": 2004.48, "end": 2012.08, "text": " that the reference that you can refer to and okay thank you all for listening thank you", "tokens": [51160, 300, 264, 6408, 300, 291, 393, 2864, 281, 293, 1392, 1309, 291, 439, 337, 4764, 1309, 291, 51540], "temperature": 0.0, "avg_logprob": -0.15241436660289764, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.018718376755714417}, {"id": 247, "seek": 201208, "start": 2012.6399999999999, "end": 2022.48, "text": " okay thank you Shusepe for your interest in talk now we have some minutes", "tokens": [50392, 1392, 1309, 291, 1160, 438, 494, 337, 428, 1179, 294, 751, 586, 321, 362, 512, 2077, 50884], "temperature": 0.0, "avg_logprob": -0.5062155868067886, "compression_ratio": 1.3267326732673268, "no_speech_prob": 0.035890426486730576}, {"id": 248, "seek": 201208, "start": 2024.8, "end": 2029.84, "text": " questions and comments if you want to make a question please", "tokens": [51000, 1651, 293, 3053, 498, 291, 528, 281, 652, 257, 1168, 1767, 51252], "temperature": 0.0, "avg_logprob": -0.5062155868067886, "compression_ratio": 1.3267326732673268, "no_speech_prob": 0.035890426486730576}, {"id": 249, "seek": 202984, "start": 2029.84, "end": 2039.12, "text": " and I have a little question", "tokens": [50364, 293, 286, 362, 257, 707, 1168, 50828], "temperature": 0.0, "avg_logprob": -0.37683257350215205, "compression_ratio": 1.2134831460674158, "no_speech_prob": 0.04205431789159775}, {"id": 250, "seek": 202984, "start": 2043.04, "end": 2050.96, "text": " for your future but do you know if there is a mathematical relation between the", "tokens": [51024, 337, 428, 2027, 457, 360, 291, 458, 498, 456, 307, 257, 18894, 9721, 1296, 264, 51420], "temperature": 0.0, "avg_logprob": -0.37683257350215205, "compression_ratio": 1.2134831460674158, "no_speech_prob": 0.04205431789159775}, {"id": 251, "seek": 205096, "start": 2050.96, "end": 2062.88, "text": " hand strong bound and the accuracy of your classifier okay so yes are you asking if", "tokens": [50364, 1011, 2068, 5472, 293, 264, 14170, 295, 428, 1508, 9902, 1392, 370, 2086, 366, 291, 3365, 498, 50960], "temperature": 0.0, "avg_logprob": -0.22528072198232016, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.023908210918307304}, {"id": 252, "seek": 205096, "start": 2063.68, "end": 2067.2, "text": " there is a connection between a strong bound and accuracy", "tokens": [51000, 456, 307, 257, 4984, 1296, 257, 2068, 5472, 293, 14170, 51176], "temperature": 0.0, "avg_logprob": -0.22528072198232016, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.023908210918307304}, {"id": 253, "seek": 205096, "start": 2069.84, "end": 2076.0, "text": " right okay well basically not basically not because it's very strange because", "tokens": [51308, 558, 1392, 731, 1936, 406, 1936, 406, 570, 309, 311, 588, 5861, 570, 51616], "temperature": 0.0, "avg_logprob": -0.22528072198232016, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.023908210918307304}, {"id": 254, "seek": 207600, "start": 2076.96, "end": 2084.4, "text": " there are really two topics that are that was never merged together between with within", "tokens": [50412, 456, 366, 534, 732, 8378, 300, 366, 300, 390, 1128, 36427, 1214, 1296, 365, 1951, 50784], "temperature": 0.0, "avg_logprob": -0.1486674404144287, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.018073949962854385}, {"id": 255, "seek": 207600, "start": 2085.68, "end": 2093.44, "text": " before now because in a certain sense health strong was not thinking about the classification", "tokens": [50848, 949, 586, 570, 294, 257, 1629, 2020, 1585, 2068, 390, 406, 1953, 466, 264, 21538, 51236], "temperature": 0.0, "avg_logprob": -0.1486674404144287, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.018073949962854385}, {"id": 256, "seek": 207600, "start": 2093.44, "end": 2099.28, "text": " problem so in a certain sense health strong well quantum state discrimination", "tokens": [51236, 1154, 370, 294, 257, 1629, 2020, 1585, 2068, 731, 13018, 1785, 15973, 51528], "temperature": 0.0, "avg_logprob": -0.1486674404144287, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.018073949962854385}, {"id": 257, "seek": 209928, "start": 2100.0, "end": 2108.0, "text": " and has not the idea of what is the accuracy because the accuracy could be calculated empirically", "tokens": [50400, 293, 575, 406, 264, 1558, 295, 437, 307, 264, 14170, 570, 264, 14170, 727, 312, 15598, 25790, 984, 50800], "temperature": 0.0, "avg_logprob": -0.20236690973831437, "compression_ratio": 1.821656050955414, "no_speech_prob": 0.03480079770088196}, {"id": 258, "seek": 209928, "start": 2108.0, "end": 2117.2000000000003, "text": " empirically only by making the experiment what the what the theoretically we have is this result", "tokens": [50800, 25790, 984, 787, 538, 1455, 264, 5120, 437, 264, 437, 264, 29400, 321, 362, 307, 341, 1874, 51260], "temperature": 0.0, "avg_logprob": -0.20236690973831437, "compression_ratio": 1.821656050955414, "no_speech_prob": 0.03480079770088196}, {"id": 259, "seek": 209928, "start": 2117.2000000000003, "end": 2124.1600000000003, "text": " is this result that is interesting regarding that strong bound and it is easy to misleading", "tokens": [51260, 307, 341, 1874, 300, 307, 1880, 8595, 300, 2068, 5472, 293, 309, 307, 1858, 281, 36429, 51608], "temperature": 0.0, "avg_logprob": -0.20236690973831437, "compression_ratio": 1.821656050955414, "no_speech_prob": 0.03480079770088196}, {"id": 260, "seek": 212416, "start": 2124.24, "end": 2132.16, "text": " because easy to misleading because we are not saying that if okay we are not saying this", "tokens": [50368, 570, 1858, 281, 36429, 570, 321, 366, 406, 1566, 300, 498, 1392, 321, 366, 406, 1566, 341, 50764], "temperature": 0.0, "avg_logprob": -0.14544962565104166, "compression_ratio": 1.9832402234636872, "no_speech_prob": 0.06752842664718628}, {"id": 261, "seek": 212416, "start": 2132.16, "end": 2138.8799999999997, "text": " that strong bound between two states is less of that strong bound between the states", "tokens": [50764, 300, 2068, 5472, 1296, 732, 4368, 307, 1570, 295, 300, 2068, 5472, 1296, 264, 4368, 51100], "temperature": 0.0, "avg_logprob": -0.14544962565104166, "compression_ratio": 1.9832402234636872, "no_speech_prob": 0.06752842664718628}, {"id": 262, "seek": 212416, "start": 2138.8799999999997, "end": 2144.56, "text": " that you obtain by making the tensor copy of itself by itself because in this case it is", "tokens": [51100, 300, 291, 12701, 538, 1455, 264, 40863, 5055, 295, 2564, 538, 2564, 570, 294, 341, 1389, 309, 307, 51384], "temperature": 0.0, "avg_logprob": -0.14544962565104166, "compression_ratio": 1.9832402234636872, "no_speech_prob": 0.06752842664718628}, {"id": 263, "seek": 212416, "start": 2145.3599999999997, "end": 2152.3999999999996, "text": " an already well known result here we are saying this something different because this object", "tokens": [51424, 364, 1217, 731, 2570, 1874, 510, 321, 366, 1566, 341, 746, 819, 570, 341, 2657, 51776], "temperature": 0.0, "avg_logprob": -0.14544962565104166, "compression_ratio": 1.9832402234636872, "no_speech_prob": 0.06752842664718628}, {"id": 264, "seek": 215240, "start": 2152.4, "end": 2165.6, "text": " our quantum centroid this tensor k row is not row times row times row k times k times it is", "tokens": [50364, 527, 13018, 1489, 6490, 341, 40863, 350, 5386, 307, 406, 5386, 1413, 5386, 1413, 5386, 350, 1413, 350, 1413, 309, 307, 51024], "temperature": 0.0, "avg_logprob": -0.1252826452255249, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.010158604942262173}, {"id": 265, "seek": 215240, "start": 2165.6, "end": 2176.0, "text": " something different is the centroid that we obtain by making in principle k copies of each", "tokens": [51024, 746, 819, 307, 264, 1489, 6490, 300, 321, 12701, 538, 1455, 294, 8665, 350, 14341, 295, 1184, 51544], "temperature": 0.0, "avg_logprob": -0.1252826452255249, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.010158604942262173}, {"id": 266, "seek": 217600, "start": 2176.0, "end": 2184.8, "text": " object of the data set and after making the centroid so in a certain sense this is a first", "tokens": [50364, 2657, 295, 264, 1412, 992, 293, 934, 1455, 264, 1489, 6490, 370, 294, 257, 1629, 2020, 341, 307, 257, 700, 50804], "temperature": 0.0, "avg_logprob": -0.1021052410728053, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.024777494370937347}, {"id": 267, "seek": 217600, "start": 2184.8, "end": 2192.48, "text": " let me say that theoretical result that put together quantum state discrimination and", "tokens": [50804, 718, 385, 584, 300, 20864, 1874, 300, 829, 1214, 13018, 1785, 15973, 293, 51188], "temperature": 0.0, "avg_logprob": -0.1021052410728053, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.024777494370937347}, {"id": 268, "seek": 217600, "start": 2192.48, "end": 2198.0, "text": " classification problem because there is the idea of centroid and there is the idea of", "tokens": [51188, 21538, 1154, 570, 456, 307, 264, 1558, 295, 1489, 6490, 293, 456, 307, 264, 1558, 295, 51464], "temperature": 0.0, "avg_logprob": -0.1021052410728053, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.024777494370937347}, {"id": 269, "seek": 219800, "start": 2198.0, "end": 2207.36, "text": " a strong bound put in together i hope to answer to your question thank you very much welcome", "tokens": [50364, 257, 2068, 5472, 829, 294, 1214, 741, 1454, 281, 1867, 281, 428, 1168, 1309, 291, 588, 709, 2928, 50832], "temperature": 0.0, "avg_logprob": -0.3014670099530901, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.21093197166919708}, {"id": 270, "seek": 219800, "start": 2210.0, "end": 2211.6, "text": " another question or comment", "tokens": [50964, 1071, 1168, 420, 2871, 51044], "temperature": 0.0, "avg_logprob": -0.3014670099530901, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.21093197166919708}, {"id": 271, "seek": 219800, "start": 2214.16, "end": 2220.0, "text": " if not thank you again for your talk thank you all thank you", "tokens": [51172, 498, 406, 1309, 291, 797, 337, 428, 751, 1309, 291, 439, 1309, 291, 51464], "temperature": 0.0, "avg_logprob": -0.3014670099530901, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.21093197166919708}], "language": "en"}