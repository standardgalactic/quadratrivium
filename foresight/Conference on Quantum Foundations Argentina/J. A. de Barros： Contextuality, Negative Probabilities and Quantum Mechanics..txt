OK, well, it's kind of like a hard task
to start after such a great talk.
I didn't get the whole talk.
It's the end of the semester here,
but it was very interesting what I saw,
and I'm going to sure read more about it.
So what I want to talk today is about negative probabilities
in quantum mechanics, and in particular with respect
to contextuality.
And I think it ties a little bit with the previous talk,
because you can think of that as a way of describing
certain quantum systems that's epistemic.
But at the end of the day, my interest
is in trying to rule out some possible ontological models
as well.
Anyway, so here's the outline of my talk today.
I'm going to start with trying to give some motivation
about negative probabilities, and then
I'm going to talk about how is it that negative probabilities
actually come into this story through contextuality.
And then after that, I'm going to give a definition
of negative probabilities that is more formal than usually one
would find.
And there is a reason for doing that,
and hopefully the motivation is going to make it clear
what is the reason.
So the starting point is the realization
that quantum mechanics, as the previous speaker mentioned,
is a probability theory, in a certain sense.
So what we have in quantum mechanics
is a state for a physical system.
And that state is a vector in a hyperspace,
or a density matrix, whatever.
And we can use that state to infer outcomes of experiments
probabilistically.
So that's what quantum theory gives us,
is this probabilistic description of physical systems.
However, this probabilistic description
that quantum theory gives to us is not fully compatible
with classical probability theory,
in the sense that for some set of observables
for a quantum system, if we look at the experimental outcomes
that quantum theory predicts, you cannot have a joint
probability distribution that describes all those observables
within classical theory.
We have kind of like a name for that.
We say that those theories are contextual.
And in the section about contextuality,
I'm going to outline a little bit more detail of what we mean
by that for those who are not aware of that.
Probably all of you are, but anyway.
So since we cannot describe quantum mechanics
with classic probability theory, it's
natural to ask whether we can modify classic probability theory
to describe quantum systems.
So is it possible that some different probability theory
can actually help us get a better understanding of what's
going on in these quantum systems?
And there are different ways to modify
classic probability theory.
One of them is, well, actually, quantum theory itself,
where we use a probability that's over vectors
in hybrid space.
And that effectively modifies probability theory,
in the sense that not everything is commasurable.
And the algebra of events that you have
is restricted to a lattice.
So you don't have the standard Boolean algebra
of all the events.
And you cannot give a measure of all the events
at the same time.
So that's one possibility is modifying
the algebra of classic probabilities,
because classic probabilities assumes that the events
form an algebra that's a Boolean algebra.
The other way is to modify the additivity rule.
We know, for example, that probability theory, if two sets
belong to this algebra of measurable events,
they are compute.
We compute the probability of those two sets,
the disjoint sets.
Together, the probability of two disjoint sets
is the sum of those probabilities of those sets.
So that's the additivity rule.
So one of the things that we can do
is we can modify that additivity rule.
And that's something that comes out
of definite work on upper and lower probabilities.
And the other possibility is the one
that we're going to be following here
is to modify probabilities such that we don't restrict ourselves
to non-negative values.
So those are ways of extending probability theory.
And so when we think about negative probabilities
or probabilities where the non-negativity of probabilities
is denied, we need to think about it carefully.
And how exactly do we want to define that?
So what we're going to be trying to do today
is introduce a way to describe quantum systems that
violate this non-negatives, the negativeness property,
such that we have, first, a major theoretic formalism that
does that, and second, that we capture
the essence of what we want to have in quantum systems
and physical systems in that theory of probability.
So that's essentially what we want to do today.
So let's start with contextuality.
And I'm going to have a little bit
of a weird, non-historic approach to contextuality
that starts with a historic comment, which
is contextuality originated.
The idea of contextuality originates in language.
And it was well known in the 20th century
in works in pragmatics, for example.
But take the sentence, marry a student near the bank,
observing people.
We can understand the sentence differently,
depending on whether it's preceded or it's followed
by distinct sentences.
For example, take this very same sentence followed
by this statement.
She paused to notice a child amuse herself
with the fishes jumping in the river.
When we read that sentence and we read the first one
together of the second one about a child looking
at fishes in the river, we understand
that bank refers to the bank of a river.
But if, on the other hand, instead of this,
we followed marry a student near the bank,
with this sentence, she noticed the manager punctually
leaving at noon, something that would help with the heist.
We associate bank not with the bank of a river,
but we associate it with a financial institution,
the physical manifestation of a financial institution.
So in this example, the first sentence,
how we understand it, changes of the context
when it's put together with this child looking at fishes,
when it's put together with heist.
So that is the sense in which linguists talk
about contextuality of language.
And we can understand contextuality of language
in terms of a very logic approach to language, which
is the truth functional approach, in which propositions
are the way in which we understand meaning of sentences.
So like, for example, take the two propositions, A and B,
A being marry a student near the river bank,
and B being marry a student near the building
of a financial institution.
Those two sentences, they have different truth values
depending on the context.
In the context of river, A is true and B is false,
whereas in the context of heist, A is false and B is true.
So it's in this sense of truth values
that we can understand contextuality
and actually even relates contextuality in language
to contextuality in quantum mechanics
via random variables.
So let's look at how contextuality in this case
will show up in terms of random variables.
So we can think about those statements A and B
as being modeled by random variables A and B.
I'm going to represent them in both face here
just to distinguish between the statements A and B,
the propositions A and B. And these random variables A and B
modeled it by being dichotomous random variables
with values plus and minus one.
With minus one, for example, corresponding
to the statement being false and plus one corresponding
to the statement being true.
It's kind of like silly to think about random variables
for this particular example because they are not,
they're deterministic random variables
because once a statement is true,
A is always going to be one, right?
And if B is false, B is always going to be minus one.
So it's kind of silly to think about
in terms of random variables,
but there's nothing that forbids us from doing that.
And contextuality here means that there is no single
random variable A and B that represents those concepts
A and B in different contexts,
or represent those truth values in different contexts.
The reason is that a random variable is defined
within a probability space.
And the probabilities, they cannot represent
both A and B simultaneously.
So this is the contextuality that comes in linguistics.
It's, we can call this as a direct influence
of the context on the random variables
because the expectations of the random variables
change directly with the context.
In one context, the expectation of A is one,
in another context, the expectation of A is minus one.
So the expectation changes the context.
This is not what happens in physics
because this type of contextuality
would violate the signaling condition,
but that's a way of representing the linguistic type
of contextuality in terms of random variables.
But once we have that idea of contextuality as changing
the expectations of random variables
in, with respect to the context,
we kind of like have a clue of how is it
that we can extend that to things
that might be more relevant to physics.
So let's take a more subtle case, right?
Let's take three, that column was random variables x, y, and z.
So plus or minus one random variables.
And let's just assume that their expectation is zero.
So we don't know, it's kind of like tossing a coin
for x, y, and z.
We don't know whether it's true or false
every time that we observe it,
but they're perfectly correlated among themselves.
So x is perfectly anti-correlated with y,
x is perfectly anti-correlated with z,
and perfectly anti-correlated with z and y.
Z and y are perfectly anti-correlated.
And it's a very straightforward to see that
if we have such cases where x, y, x, z, and y, z
are anti-correlated, that there is something funky going on.
And the way to see there is something funky going on
is just to notice that, for example, if x equals one,
this guy here implies that y equals minus one.
And if x equals one, this guy here implies
that z equals minus one.
But minus one times minus one is one.
However, this guy here tells us that y times z is minus one,
which is a contradiction.
So there is something wrong going on.
And what is wrong is that when we're using this reasoning
of x being implying y equals minus one here
in this first equation, and then we're using it
as implying z here in this second equation,
and then using to get the multiplication
in the third equation, we're assuming
that all three random variables are defined together.
That means that we're assuming there's a joint
probability distribution.
But it doesn't exist.
Assuming that x in the context of y is the same
as x in the context of z is what's leading
as your contradiction, assuming that all three of them
are observable at the same time
is leading as your contradictions.
And we know that this happens in quantum mechanics,
that there are things that we cannot absorb at the same time.
So can we see something like that in quantum mechanics?
And the answer is yes.
A very famous example is the GHZ states.
We have three particles, one, two, and three,
that are in an entangled state where either all three
of them have spin in the direction z being plus,
or all three of them have spin in the direction z being minus,
and then we have a superposition of those two states.
And then it's easy to see that these guys
have the following eigenstates.
For example, if we measure spin of particle one
in the direction x, measure spin in the particle two
in the direction y, and three in the direction y,
the product of those three spins is always gonna be one.
In the other hand, if we measure y, x, y,
it's also gonna be one, y, y, x is also one,
but x, x, x is minus one.
So we can try to look at it in terms of random variables.
Let's say we have an x-random variable
for spin in the direction x, a y-random variable
for spin in the direction y,
and then we assume them to be context independent.
And if we do that, we can multiply this guy here,
the random variables corresponding to this spin,
x, y, y, times the spin y, x, y, times y, y, x,
and each one of those guys in parentheses are equal to one.
And when we multiply them, we can now collect
all the y's together and all the x's together,
and the square of something that's closer
minus one is always one.
And when we do that, this guy here's one,
and we get a contradiction that one is equal to minus one
from this quantum mechanical computation.
Of course, it's not a contradiction
because what we're getting as contradiction
is based on the assumption that the y in the context of x
and y, y two in the context of x one and y three
is the same as the y two in the context of y one and x three.
So in other words,
we're assuming a joint probability distribution.
So when we have a set of random variables
that are contextual,
there is no joint probability distribution
for all the variables.
There is, of course, a joint probability distribution
for each one of those contexts,
but when you put all those six random variables together,
there is no joint probability distribution.
There's no single probability space,
omega fp such that the random variables xi and yi
models all the quantum outcomes.
That's not possible in standard probability theory.
Of course, we know that the quantum formalism
provides an answer.
We can use the quantum formalism as we shown here
to compute the expectations of actual observables
that we can measure together,
but are there alternatives to quantum model?
And more importantly,
are there alternatives to the quantum model useful for us
to think about what's going on with the quantum case?
We don't know whether those alternatives are useful,
but we feel that perhaps understanding them better
might be a way to at least understand
what is it that they can bring to this discussion
of what the quantum model or quantum,
of what the hub or space formula is telling us.
So the alternatives that have been tried,
as I mentioned, are upper and lower probabilities,
and of course, there are negative probabilities
that we wanna expand more today.
So let's go into defining negative probabilities.
To understand how we define negative probabilities,
it's useful to see how we define standard probability theory.
Right?
So the most accepted definition
of standard probability theory is by Komagorov.
He gave a set theoretic definition
of classic probability theory as being the triplets,
omega Fp, where omega is a sample space that you have.
F is an algebra over that sample space omega
that is actually a sigma algebra, a countable algebra,
and P is a function that takes elements of this algebra
to this interval zero and one.
And the restrictions that we have for P
is that the measure of omega or the whole set
of possibilities is equal to one.
In other words, the probability
that something happens is one.
And the other assumption is that the additivity rule
holds for this function.
In other words, for any denumerable
and this joint family of elements of this algebra F, Ai,
the probability of A, say A1 or A2
is the sum of the probability of A and the probability of two.
So that's something that's a requirement
of probability theory.
And because this is a number between zero and one,
this probability is monotonic.
So if you have a set A, the probability of adding something
more to that set A, that's this joint of A,
increases the probability of absorbing that event.
So if you add more possibilities,
this becomes more probable.
And in quantum mechanics, this is not quite the case.
It's non-monotonic, but this is something
that I'm not gonna talk too much about.
So how do we get negative probabilities?
We might be tempted to just violate this requirement
and we're done with it.
But the problem is that there are certain constraints
about negative probabilities
that at least seem to make sense,
which is that first and foremost,
we never directly observe a negative probability.
What the heck does it mean to even observe
a negative probability?
So we need to understand how exactly
does the negative probability show
in terms of directly observable quantities.
And observables are not parts of komagorov stock.
So we need to include that.
And furthermore, there's no connection
if we just like relax this requirement
that ties the marginals to the empirical observable contexts.
So what we're gonna do here is just try to show
how we can define negative probabilities
with those ideas in mind.
So that's our major theoretic definition
of negative probabilities.
It's gonna be a little bit abstract from now on,
but I'm gonna try to give the main ideas
of what is the outlining of this definition, right?
We start with a sample space, not a sample space.
We start with the concept of assigned measure.
So assigned measure is exactly the same as a measure,
except that instead of having a measure
going from a sigma algebra to the set
of non-negative real numbers,
assigned measure is something that takes the sets
of elements of the algebra into any real number,
including negative numbers.
And this is well known in measure theory.
So it's something that has been studied extensively.
And essentially, if we restrict the assigned measure
to non-negative numbers, we get a regular measure.
And if we restrict this measure
such that mu of omega for this measure is equal to one,
we get probabilities.
So that's the starting point.
We start with assigned measure space
that has the additivity rule as part of it.
And then once we have this assigned measure space,
we need to find a way to connect it
to the actual observations that we have.
And the starting point is to create an object
that's not quite a random variable,
but it's what we call an extended random variable
because instead of being defined over a probability space,
we're defining it into assigned measure space.
And the idea is just that if the assigned measure space
then becomes a probability space,
those random variables become regular random variables.
And the definition is just the standard definition
of random variable where you have a function
that goes from your sample space
to some real number M that's parts of a sets.
This should be this M here, something wrong here.
Anyway, and such that this function
is a measurable function in the sense
of this measure that we have.
It's a measure induced definition.
And then from this, we can kind of like start
to narrow down the idea of contexts.
And contexts are just proper subsets
of those random variables,
such that there is a sub sigma algebra
for each one of those subsets of the random variables.
That define a probability space
for those random variables there.
So intuitively we'll have this bunch
of extended random variables.
And we have this measure space
describing those extended random variables.
But then we get a subspace of that.
And this subspace has a measure space associated to it
such that we actually have proper random variables
in this subspace.
And this allows us to talk now
about families of signed probabilistic models
that have just those contexts defined for those families
and then measures in this context.
I'm gonna skip a little bit faster in this.
And then once we do that,
we can find a concept of a general context
that just has like the same idea of the broader one.
But now we're saying that those are just for those guys
that look like probabilities in the subspace.
So finally, we have like those families
of signed probability models.
And those families of signed probability models,
what you have are those like functions
that reproduce all the expectations
that you have on the contexts.
But it's kind of like the wild west here
because they can be as large or as small as you want.
And what we need to do then is tame a little bit
those set of functions by saying,
look, if we now take the sum of all the probabilities,
the extended probabilities that we have
over the sample space for each one of those elements
of this signed probability spaces, we get a value.
And what we want are the smallest possible values possible
because when that happens,
we're as close as we can be
to a proper probability distribution.
In fact, this is gonna come up in a theorem later on.
And when we do that,
when we ask for the minimum value of the sum,
then we call that a negative probability space.
It's when this minimum is obtained.
If we do that, then we have the following theorem.
Imagine that you have now this guy
as a minimum signed probability space
or as a negative probability space.
So if this M defined as the sum of the probabilities
of elements of the sample space is equal to one,
then this is a probability space.
It's very intuitive to see why that is.
It's because if you remove the absolute value of this,
this is just the probability of omega,
which is always one for a probability space.
And what you're saying is that nothing is pulling
that probability higher or lower
because the sum of the absolute values
is the same as the sum without absolute values.
There's nothing negative here.
It's all non-negative.
So this is also a probability space.
And alternatively, if this guy,
it's missing the both face omega here,
but if this is a probability space,
then it's also a signed probability space of M equals one.
Additionally, a collection of extended random variables
of this space is contextual,
if and only if M is not one.
So we have here criteria for contextuality
just showing up in the definition of negative probabilities.
I'm gonna skip this because I'm running out of time,
but this is how you can connect it to a quantum mechanics.
I gotta say there are lots of different ways
in which negative probabilities connect to a quantum mechanics.
For example, the previous speaker mentioned about PR boxes.
You can, as long as you have no signaling,
you can describe any correlations
in terms of negative probabilities.
So you can describe PR boxes in terms of negative probabilities.
You can describe GHC in terms of negative probabilities
or Bell in terms of negative probabilities.
But a natural question that often comes up
comes up in this discussion of negative probabilities
is what are the interpretations
that we can give to a negative probabilities?
I'm gonna talk about three possible interpretations.
Andrei Krenikov, he uses
a piatic matrix to give an interpretation
of negative probabilities as a violation
of von Mises principle of stability.
So as we know,
if you wanna have a frequentist interpretation
of probabilities, what are probabilities?
And von Mises defines probabilities
as being over infinite sequences
of possible outcomes on the sample space,
of possible values in the sample space.
And those probabilities are only defined
for those infinite sequences
if they converge to a number.
So that's what Mises principle of stability.
So what Krenikov noticed is that for certain sequences
that violate the principle of stability,
you can actually define probabilities for them
if you use a piatic metric.
And when you do that, the sequences that violate
von Mises principle of stability
actually take negative values.
So that's the interpretation that he gives,
is that negative probabilities are associated with numbers
that violate the principle of stability,
which in terms of like, if you are trying to understand,
for example, Bell correlations,
it means that those sequences
that you're assuming to be independent,
they're not actually independent.
Because...
You have 10 minutes including the question.
Oh yeah, I'm just finishing it.
Just finishing the rotations.
Thanks.
So another interpretation is proposed
by Bromsky and Brandenburger.
It's actually based on the composition theorem
that shows that if you have a signed measure,
you can always break it down into two measures,
one positive and one negative,
and each one of them independently are non-signed measures.
And what they do is they just attach a sign
to each one of the outcomes of an experiment,
and then those signs,
they lead to cancellation of certain events.
There's a third interpretation,
which is the one put forth by Derrack
and later on by Feynman,
which is that probabilities,
negative probabilities for that matter,
are just a bookkeeping device.
So in that sense,
negative probabilities are nothing more
than the same as we do with negative numbers.
So for example,
we never observe a negative number of apples.
In fact, it's nonsensical to talk about
a negative number of numbers as an actual observable,
but it doesn't mean that using negative numbers of apples
is not a useful book counting device.
We never observe a negative number of dollars
for that matter either.
It doesn't mean that one never has negative numbers
in an account as a bookkeeping device
for how much money we have in an account.
And the reason why I'm mentioning numbers here
is because the concept of even using negative numbers
has not been always non-controversial.
Consider, for example,
the following quote by De Morgan,
the famous mathematician and logician
in a book that he wrote in the 30s.
And I'll read the quote to you.
Above all, he, the student,
must reject the definition still sometimes given
of the quantity minus A, that it is less than nothing.
It is astonishing that the human intellect
should ever have tolerated such an absurdity
as the idea of a quantity less than nothing.
Above all, that the notion should have outlived the beliefs
in judicial astrology and the nonexistence of witches,
either of which is 10,000 times more possible.
So as late as like the 1800s,
negative numbers were still considered
like an absurd idea by some.
And it might be the case that negative probabilities
will in the future be considered the same
as we consider negative numbers,
something just natural, like natural numbers are
as a bookkeeping device.
So just to summarize what I talked today,
as we discussed, contextuality is a key characteristic
of quantum properties.
We often use hybrid space formalism to describe
those quantum properties and the hybrid space formalism
has built in that contextuality.
That contextuality cannot be represented
in probability theory unless you do something
like for example, the approach of contextuality by default
by increasing the number of random variables.
But that goes against a little bit the idea
that we do in physics because we have the same measurement
apparatus for some of those things.
But extended probability theories can be constructed.
And here we just presented a measure theoretic framework
for defining an extended probability
that we feel captures at least the gist
of quantum contextuality in the sense that we can model
quantum contextuality with that probability theory.
And we hope that such tool may be used
when describing quantum phenomena.
Like for example, we know that it implies
the no signaling condition, which is part of quantum systems.
But it's still unclear whether it's useful or not.
So I just wanna end with acknowledging my collaborators.
What I talked today is in essence part of a paper
that I did with Federico Halleck
who's organizing this conference.
But I also benefited from a lot of discussions
of many people in particular, those people listed here.
So thank you.
Okay, thank you, Jose, for your talk.
There are some questions in the chat.
The first one is from Shansen.
All right, thank you for that talk.
My question is about, has to do with sort of rhetoric
and pedagogy.
So, you know, like we, it sounds like we think very much
the same way about quantum mechanics is basically
like a new framework for handling probabilities.
And I wonder whether it's the smartest strategy
to convince people of our view
that you now like have to like get used to the idea
of negative probabilities.
I mean, if you start, I mean,
you're very nicely laid out the difference.
Like, you know, you have contextuality, right?
You don't, but quantum mechanics doesn't allow you
to have joint probabilities, right?
You can't assign values to all the different variables.
I wonder if there isn't like sort of a more natural way
of like ramming that down people's throats
than going this route of introducing
negative probabilities.
Yeah, thank you.
I confess that I would probably never teach
negative probabilities to my students.
Well, never, I mean, maybe in the future.
That's not the reason why I'm talking
about negative probabilities actually.
The reason why I'm talking about negative probabilities
is because I hope that by trying to understand
certain phenomena in terms of negative probabilities,
we might be able to get different insights
into those phenomena that we would not get
from the Hebert space formalism.
I can give you a specific example, right?
So when we look at different principles
that define quantum mechanics that people have tried,
like for example, Cabello have tried to define some principles
that give us quantum mechanics.
When you write them down in terms of negative probabilities,
they become clear.
Whether that's a useful tool for us to understand
what's going on with quantum mechanics or not,
I don't know because I didn't get any special insights
from that, but that's my hope, right?
That's what I'm trying to do.
I'm trying to see if we can understand
different types of quantum puzzles
in terms of negative probabilities
such that we can get better insight from them.
If we do, then perhaps in the future,
we might even try to teach our students that,
but we're not there yet.
Okay, thank you.
And there is another question from Alison Decini.
Hi, can you hear me?
Yes, I can hear you.
Okay, so thanks for the very nice question.
My question is about Cauchy-Speccher theorem.
So how do you avoid Cauchy-Speccher theorem
in this kind of framework?
Because Cauchy-Speccher theorem
rule out the existence of evaluations.
And if we represent all measurements
as random variables in the same,
sharing the same sample space,
a point of the sample space would be evaluation.
So how can you avoid Cauchy-Speccher theorem
within this approach?
That's my question.
If I understand your question correctly,
you're asking how I avoid Cauchy-Speccher and we don't.
Let me go back to the GHZ case,
which is a, so the difference
between GHZ and Cauchy-Speccher
that GHZ is for a specific state
and Cauchy-Speccher is for any state, right?
It's about the algebra of quantum observables.
But you could think of Cauchy-Speccher
as a series of measurements that prepare a state
and then later on you get a contradiction.
That's a different way to think about Cauchy-Speccher,
but it's all there together.
But this is the GHZ also has a contradiction, right?
And the contradiction comes here
from this guy in the bottom.
This first parenthesis being one times one times one
being equal to minus one.
It's the same type of contradiction
that you get on Cauchy-Speccher.
And you are using random variables
because each one of them individually are random.
But if you look at each one of those products,
if you thought of them as an individual random variable,
let's say we call this product A,
we call this product B and we call this product C.
What we have is that A times B times C is equal to D.
But this is what quantum mechanics tells us,
but A is equal to one, B is equal to one
and C is equal to one whereas D is equal to minus one.
That's the contradiction.
And the random variables that we have here
are deterministic random variables.
So I don't know if I'm answering your question
the way you thought about it,
but what I'm saying is we're not avoiding it.
We have the contradictions there,
but the contradictions come
from the assumptions of Cauchy-Speccher.
And the assumption of Cauchy-Speccher
is that this guy here, say Y2 in this context
is the same as the Y2 in this context.
And when we use negative probabilities,
that assumption is not true anymore
because negative probabilities
are washing out those requirements
of how is it that those random variables are related.
And then what you're asking about,
if you were to ask me how exactly
they are washing out those requirements,
I would tell, wow, I don't know.
That's how we book keep those probabilities.
If you wanna know how they are doing that,
then you would need to go to an interpretation
that is not a purely epistemic interpretation
as in bookkeeping.
And there's nothing that forbids you from doing that, right?
Does that make sense, what I'm saying?
Or am I being confused here, confusing?
Yeah, yeah, no, it makes sense.
Thanks, thank you.
Okay, we are done of the break.
So, but there is one more question.
It is very fast.
So perhaps I can make it, if it's okay with you.
So it is just about the notion of contextuality
that you seem to be using.
I'm just curious if you are familiar
with the work of Speckens and this idea
of using a different definition of contextuality
than the one that seems to be used in Gohan Specker
and in Bell's proof of non-contextuality.
And if that makes a difference in the kind of project
that you are using to accommodate all of contextuality
using negative probabilities.
Yeah, that's a good question.
One could give a whole talk about like different ways
of thinking about contextuality, right?
So I've been very casual about talking
about contextuality here and very specific about it.
But it's unclear exactly how those different notions
of contextuality relate to each other.
Sometimes, not all the time, right?
And to be very precise, one would need to prove
what is the relationship between one definition
of contextuality and another definition of contextuality.
And as far as I know, that has not been done
for all notions of contextuality.
But there is this intuition that they kind of like
are talking about the same thing.
Because at the end of the day,
they're all about commasurable properties
and how those commasurable properties
somehow are related to not having a possibility
of talking about all of those properties at the same time,
in terms of the intuition behind them.
I know I'm not answering your question
and the reason why I'm not answering your question
is because I'm trying to say that I don't think
there is a very clear answer to that question yet.
But we hope that once we give a definition
that's a measure theoretic definition,
that precise definition of negative probabilities
that correspond to also a definition of contextuality
in that setup, that you can use that to prove theorems
about how those things are related.
It might be really, really interesting to explore that
for two reasons, because the kind of definition
that the Speckens uses makes no explicit reference
to the idea of measuring two things at the same time.
It is kind of different to the general idea.
And two, because in the original paper,
I think that that's from 25,
he has a small derivation of showing that
from his definition, you can derive the traditional one,
the one that the Cohen Speckens used.
So there must be a connection.
Yeah, yeah, yeah, that's what I'm saying.
All those things are related,
but how exactly they are related, it's not.
Intuitively, yes, they should be related,
but it's one thing to think that they should be related
intuitively and to prove that they're related.
So that's what I was trying to say.
Thank you, Jose, and the people who have questions
would have some minutes to go to the bathroom
or take a coffee.
We will back.
