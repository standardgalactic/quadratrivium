1
00:00:00,000 --> 00:00:09,840
Okay, we'll start with the second part of the first session this morning.

2
00:00:09,840 --> 00:00:17,280
The first talk is in charge of Schubert Cercione from the University of Caglione.

3
00:00:18,560 --> 00:00:24,480
Thank you, thank you very much and thank you to all the organizers who invited me. For me it's

4
00:00:24,480 --> 00:00:31,520
very proud and very proud to be invited to this 10th conference of Quantum Foundation.

5
00:00:32,080 --> 00:00:41,280
I'm very happy to be part of this very prominent panel of scientists. I warmly thank all the

6
00:00:41,280 --> 00:00:49,600
organizers and especially Federico Olic and Martin Bosic, who are also a strict friend of

7
00:00:49,600 --> 00:00:57,600
Italian University and of Cagliari and also personal friends. So what I want to introduce

8
00:00:57,600 --> 00:01:04,800
here is Quantum Information Meet Smashing Learning. So basically I like to introduce

9
00:01:04,800 --> 00:01:13,280
generally this topic by starting from a provocative question. I ask you, can you show me a picture

10
00:01:13,280 --> 00:01:24,800
of your Christmas 2018? I am sure that you can and I am sure that if you try to find a picture

11
00:01:24,800 --> 00:01:30,960
of your Christmas, for instance in your mobile, you will find it but probably you need time to

12
00:01:30,960 --> 00:01:38,800
find it just because now we use to make a lot a lot of pics with our mobile because the memory

13
00:01:38,800 --> 00:01:48,720
that our mobile has now is very, very huge and many, many pics can be kept, can be considered,

14
00:01:48,720 --> 00:02:01,760
can be stored in a cell for instance in a mobile. So what we can say is that we can,

15
00:02:01,760 --> 00:02:10,000
can we say that many data corresponds to many information? Basically not because information

16
00:02:10,000 --> 00:02:19,200
means recover from the data what we really need. So the fact that now we have to deal with a very,

17
00:02:19,200 --> 00:02:26,800
very huge amount of information means that potentially we can have a lot of information

18
00:02:26,800 --> 00:02:34,960
but the problem is the process we need to do in order to extrapolate this information

19
00:02:34,960 --> 00:02:46,560
from this big amount of data. So this is the reason why in the last decades big data science

20
00:02:46,560 --> 00:02:54,880
corresponds to face with just this kind of process, this kind of problem and so the problem is how

21
00:02:54,880 --> 00:03:03,680
to manage huge amount of data in order to retrieve in the short time as possible the information

22
00:03:03,680 --> 00:03:11,680
we need, the information we need. Okay so this is why big data born in the in the in the last

23
00:03:11,680 --> 00:03:17,760
decade. So this is just a very brief summary of what I said in these five minutes. So in other

24
00:03:17,760 --> 00:03:24,000
words, enlarging the dimension of the data set is a good thing because it increases the potential

25
00:03:24,000 --> 00:03:30,000
information but it's also bad because it makes the extraction of the required information out.

26
00:03:31,040 --> 00:03:39,600
So from another side you know very well that you know very well how quantum information

27
00:03:40,320 --> 00:03:46,720
born, how quantum information became in the recent past a really new discipline of

28
00:03:50,720 --> 00:03:56,080
quantum theory in general, quantum computing mostly, and we know that the advantage of

29
00:03:56,080 --> 00:04:03,360
quantum computation are most of the the advantages are most of all based on the fact that we have

30
00:04:03,440 --> 00:04:07,600
a speed up of the computation. We can obtain, we can

31
00:04:10,000 --> 00:04:16,800
perform a computer and strong and hard computation in a very very shorter, a shorter time

32
00:04:16,800 --> 00:04:28,240
with respect to a classical, a classical one. Okay so so we know also that quantum computation

33
00:04:28,240 --> 00:04:36,480
is not so futuristic. We can say that till 10 maybe 20 years ago we were speaking about

34
00:04:36,480 --> 00:04:42,720
quantum computation as something that was phantasyntific, that was not really concrete,

35
00:04:42,720 --> 00:04:50,480
but we know that the application in quantum computation has a very very speed up in the

36
00:04:50,480 --> 00:04:56,880
last in the last decade and now we know that the quantum processor already exists and we know

37
00:04:56,880 --> 00:05:04,560
that also by using for instance our our computer in our house we can connect with the IBM quantum

38
00:05:04,560 --> 00:05:11,680
experience and we can just design a quantum algorithm and by a click we can send this

39
00:05:11,680 --> 00:05:19,600
quantum algorithm to a real quantum computer that will perform it, that will give us back the result

40
00:05:19,600 --> 00:05:28,160
as a as a statistical result. So basically the quantum computation is now real, is now real

41
00:05:28,160 --> 00:05:37,360
and also we know that this this topic is is going on and on and for instance just one year ago,

42
00:05:37,360 --> 00:05:43,600
just one year ago in in china that was there was was declared that the

43
00:05:43,600 --> 00:05:49,520
achievement of quantum supremacy because it was possible to perform a calculation in

44
00:05:50,560 --> 00:05:58,000
200 seconds, a kind of calculation that will take more than half a billion years on the

45
00:05:58,000 --> 00:06:06,320
world's faster, no quantum or classical computer. So now we really trust in quantum computation,

46
00:06:06,320 --> 00:06:13,440
we really trust in this. So we were speaking about the data and about quantum computation

47
00:06:13,840 --> 00:06:19,760
but because well summarizing the main problem of big data when machine learning with big data

48
00:06:19,760 --> 00:06:25,360
is the time complexity of the computation but the main advantage of quantum computation is

49
00:06:25,360 --> 00:06:32,640
the possibility to reproduce to use the time complexity. So in a certain sense,

50
00:06:32,640 --> 00:06:39,120
quantum computation and machine learning appears as a very natural and maybe unavoidable,

51
00:06:39,120 --> 00:06:47,840
unavoidable connection but the connection can be deeper and deeper with respect to what we can

52
00:06:47,840 --> 00:06:55,520
think. Indeed the question that they want to ask you in this in this talk is can quantum

53
00:06:55,520 --> 00:07:04,240
information theory help classical machine learning, classical by using classical computer only

54
00:07:04,240 --> 00:07:11,360
and we will provide a positive answer to this question. So this approach that we will speak

55
00:07:11,360 --> 00:07:19,440
is known as a quantum-inspired machine learning and the important thing is that well very often

56
00:07:20,720 --> 00:07:30,320
we know that in many disciplines we speak about the inspiration from quantum mechanics.

57
00:07:30,320 --> 00:07:37,600
Okay now what I want to focus on is the fact that our inspiration is not just an analogy,

58
00:07:37,600 --> 00:07:46,480
it's not a metaphor but it's just a really inspiration regarding a real inspiration to quantum

59
00:07:46,480 --> 00:07:55,920
work in order to have a real benefit in machine learning and in the next minutes I aim to show

60
00:07:55,920 --> 00:08:04,800
you what they mean by real inspiration. Okay in particular we focus on the classification problem.

61
00:08:04,800 --> 00:08:11,120
What is the classification problem? Okay the general idea is that the quantum classification

62
00:08:11,120 --> 00:08:17,440
in the idea that we want to perform is the idea of a quantum classifier where quantum

63
00:08:17,440 --> 00:08:21,760
classification is the idea of quantum classification is to formalize the standard

64
00:08:22,480 --> 00:08:28,000
classification problems in terms of the mathematical quantum object and then

65
00:08:28,640 --> 00:08:34,480
inspired by certain properties of quantum state discrimination we define a quantum

66
00:08:34,480 --> 00:08:42,000
classifier that will provide interesting and real good advantages. The procedure is

67
00:08:42,000 --> 00:08:48,640
based on three fundamental steps that we will describe not now but a few slides later encoding

68
00:08:48,640 --> 00:08:55,600
classification and the coding but we will meet this slide again in a bit. First of all

69
00:08:56,960 --> 00:09:00,000
I want to very very quickly summarize what is the

70
00:09:02,080 --> 00:09:08,400
sorry the general framework of classification problem. So when we have to classify an object

71
00:09:08,400 --> 00:09:17,280
we describe this object as a vector where each component of this vector is a feature of this

72
00:09:17,360 --> 00:09:25,520
object like this object could be a cat and this feature could be the length of the tail,

73
00:09:26,080 --> 00:09:32,640
the weight of the cat and so on and we have different classes of objects for instance

74
00:09:32,640 --> 00:09:40,240
dog and cats and what and so on and this so a pattern is represented by a pair where this is

75
00:09:40,240 --> 00:09:46,720
the vector the object represented by a vector and this is the label that characterized the class

76
00:09:46,720 --> 00:09:53,760
for instance class for instance label one means class of the cat, label two means class of the

77
00:09:53,760 --> 00:10:01,040
dogs for instance okay and the goal of the classification is to have a classifier

78
00:10:03,280 --> 00:10:15,360
I mean a function classifier is a function that allows us to study the vector and gives as an output

79
00:10:15,360 --> 00:10:23,760
the label so looking at the vector looking at the object I can say you if this object is a cat or is

80
00:10:23,760 --> 00:10:33,520
a dog basically okay okay okay so in a general supervised scenario I will be very very very

81
00:10:34,320 --> 00:10:41,920
I will skip many slides but but sometime I can focus on some technical data sometimes not

82
00:10:42,000 --> 00:10:51,600
generally generally in what we say supervised learning we mean that we consider a data set

83
00:10:52,160 --> 00:11:00,640
for instance a data set of dogs and a data set of cats for instance and we divide this data set

84
00:11:00,640 --> 00:11:07,520
at the beginning in two parts the first part is given by the training set this is the set

85
00:11:07,520 --> 00:11:15,520
useful in order to train our algorithm and this is the test set but the set is a set of

86
00:11:15,520 --> 00:11:23,600
object that we use in order to know whether our algorithm is good or not just in order to know

87
00:11:23,600 --> 00:11:33,600
just in order to measure the performance of our algorithm and so obviously then all of these

88
00:11:33,680 --> 00:11:40,240
objects are already labelled so we already know the level of the object so the goal is just to

89
00:11:40,240 --> 00:11:50,320
evaluate how the right if the classifier is good or not so we use a already labelled set of object

90
00:11:50,320 --> 00:11:56,640
we divide it in training and test and obviously also the training is divided with respect to the

91
00:11:56,640 --> 00:12:04,480
classes the classes will get the classes of dogs the classes of foxes and so on and so on okay so

92
00:12:05,200 --> 00:12:14,800
the the first idea of the quantum classification is just to translate formally translate each

93
00:12:14,800 --> 00:12:23,120
object each vector each cat that is represented by a vector in terms of a quantum object that we

94
00:12:23,120 --> 00:12:29,920
represent by a density operator row and there are many many ways to encode this is just encoding to

95
00:12:29,920 --> 00:12:38,720
encode a real vector in terms of the quantum object okay in terms of a density operator obviously

96
00:12:38,720 --> 00:12:47,440
it is only a formal translation we are not transforming a cat in a photon or a dog in

97
00:12:47,440 --> 00:12:58,160
an electron obviously okay so now once we have translated all the data set of real data in terms

98
00:12:58,160 --> 00:13:07,280
of the data set of quantum data or quantum data again we can consider the distinction between

99
00:13:07,920 --> 00:13:17,680
training and test training and test again and the idea is just to obtain a quantum classifier

100
00:13:17,680 --> 00:13:26,800
that is a function that has as input a quantum object of the quantum test set and has the

101
00:13:26,800 --> 00:13:35,360
output level so given this quantum object that is the density operator that represents an unknown

102
00:13:35,360 --> 00:13:44,400
object we can say that we can we can obtain by the classifier that this object is a dog okay so

103
00:13:45,040 --> 00:13:52,560
obviously what we have to show is that this translation from classical to quantum object

104
00:13:53,120 --> 00:13:58,960
has some benefit provides some benefit otherwise it's totally useless okay so okay

105
00:13:59,040 --> 00:14:09,040
now let us talk regarding the general setting in quantum state discrimination okay that probably

106
00:14:09,760 --> 00:14:18,080
most of you already know but I very briefly summarize here let R be a set of density operator

107
00:14:18,080 --> 00:14:24,000
and suppose that Thelis wishes to communicate information to Bobo by using a quantum system

108
00:14:24,000 --> 00:14:32,720
that is one of the system belonging to this state okay to the same Thelis prepares a quantum system

109
00:14:32,720 --> 00:14:42,800
in one of this state in one of this state and hands the system over to Bobo in principle Bobo has

110
00:14:42,800 --> 00:14:53,520
a complete knowledge about R so Bob knows in knows already knows which are row one row two row

111
00:14:53,520 --> 00:15:03,920
M but he does not know the actual state of the system that he have received by Thelis so in order to

112
00:15:03,920 --> 00:15:15,120
so that the Bob's task is to determine which one is among this state in one shot just by making a

113
00:15:15,120 --> 00:15:26,240
single von Neumann measurement over all possible sorry or possible physical observable so there

114
00:15:26,240 --> 00:15:34,400
and so the possibility is that the measure the outcome of the measurement could be just the

115
00:15:34,400 --> 00:15:41,840
row that Thelis sent to Bob or not or can be another one in the first case okay we say that Bob

116
00:15:42,400 --> 00:15:51,520
succeeds in another case we say that Bob fails okay so what is the probability for Bob to

117
00:15:51,520 --> 00:15:59,440
succeed okay given a set R or quantum state and a von Neumann measurement M the average

118
00:15:59,440 --> 00:16:07,520
probability to Bob for Bob to perform a correct discrimination so to correct individuate which

119
00:16:07,520 --> 00:16:15,760
is the state that was sent from Thelis is given by this quantity is given by this quantity and

120
00:16:16,720 --> 00:16:27,200
the aim of course is the maximize maximize this probability so to minimize the probability to

121
00:16:27,200 --> 00:16:36,160
have an error okay so the question is what what is the measure the measure that maximize the

122
00:16:36,160 --> 00:16:42,880
probability to have the correct discrimination and this measure this measure was obtained by

123
00:16:43,840 --> 00:16:52,560
by this strategy okay the strategy is is related only to the binary case so consider to have just

124
00:16:52,560 --> 00:17:01,520
row one and row two and if we consider this quantity p1 row one minus p2 row two where p1

125
00:17:01,520 --> 00:17:09,760
and p2 are a priori probability okay that can be obtained in some different but empirical way for

126
00:17:09,760 --> 00:17:16,880
instance we obtain this quantity we calculate the positive and negative eigenvalues of this

127
00:17:16,880 --> 00:17:25,760
quantity we consider the respective eigenvectors and we consider the sum of the projector built

128
00:17:25,760 --> 00:17:33,360
over all of each of these eigenvectors in this way we can obtain p plus and p minus

129
00:17:33,920 --> 00:17:41,520
and hence from showered that this p plus and p minus is a von Neumann measurement that is optimal

130
00:17:41,520 --> 00:17:48,000
that is the optimal measure for the discrimination problem for the binary discrimination problem

131
00:17:48,000 --> 00:17:55,200
that we have introduced at the moment so this is the this is the bound this is the bound and

132
00:17:56,160 --> 00:18:02,480
intuitively p plus and p minus represent the property for the system to be correctly

133
00:18:02,480 --> 00:18:09,760
identified as being the state row one or row two respectively okay and that's from bound can be

134
00:18:09,760 --> 00:18:17,840
seen as a measurement of distinguishability between row one and row two so now again with

135
00:18:17,840 --> 00:18:25,920
this light because now we have all the ingredients we need in order to put together this discrimination

136
00:18:25,920 --> 00:18:33,200
not to put together classification problem and quantum state discrimination the first stage

137
00:18:33,200 --> 00:18:41,760
is the encoding we say that we need to have real object and translate the real vector and translate

138
00:18:41,760 --> 00:18:48,880
them in terms of quantum object like a density operators like a pure states and for instance

139
00:18:48,880 --> 00:18:55,520
it is easy to see because there are several way several way to do this this is one simple way for

140
00:18:55,520 --> 00:19:06,160
instance by using the stereographic projection we map each object in this simple case two feature

141
00:19:06,160 --> 00:19:13,840
object in a point over a sphere and of a surface one sphere and that's correspond to the point of

142
00:19:13,840 --> 00:19:21,680
the block sphere so a pure density matrix a pure state a pure state but there are many other

143
00:19:21,680 --> 00:19:29,680
possibilities to encode real object in terms in terms of density matrix pure density matrix okay

144
00:19:29,680 --> 00:19:39,600
so once we do that once we have set of quantum states set of density operators

145
00:19:40,240 --> 00:19:48,240
for each training set we for each training set for each class of the training set we we represent

146
00:19:48,240 --> 00:19:55,280
each class by a quantum centroid that is defined in this way this is just the beginning of our

147
00:19:55,360 --> 00:20:03,040
orbit and it is no longer a pure state of course because it is just the sum of all of the state

148
00:20:03,040 --> 00:20:10,400
of all the encoded vector belonging to a to a given class to a given class okay obviously the

149
00:20:10,400 --> 00:20:16,480
quantum centroid are mixed states are mixed states and they are not the encoding of the

150
00:20:16,480 --> 00:20:22,160
respective classical centroid they are a totally new object a totally new object that

151
00:20:22,160 --> 00:20:29,920
has not any counterpart in the real world in the world of real object in the world of real vectors

152
00:20:29,920 --> 00:20:40,640
okay okay so now we use quantum state discrimination we use the Elstrom measurement

153
00:20:40,640 --> 00:20:48,720
that is optimal so we use p plus and p minus that we built we built by considering

154
00:20:52,160 --> 00:21:00,000
as the two states to discriminate we will consider the two centroids so let us consider

155
00:21:00,000 --> 00:21:06,960
a case where we want to discriminate between cats and dogs we consider the training set of the cats

156
00:21:07,600 --> 00:21:14,400
we are encoding this set in terms of quantum object we consider this you can see that the

157
00:21:14,400 --> 00:21:20,800
centroid of this class we do the same for the dogs and we have these two centroid these two

158
00:21:20,800 --> 00:21:28,160
quantum centroid these two quantum centroid define the finding are two density operator

159
00:21:28,160 --> 00:21:34,560
and from this from these two quantum centroids we can apply the Elstrom

160
00:21:37,360 --> 00:21:44,400
discrimination and we cannot take the Elstrom measurement the optimal Elstrom measurement

161
00:21:44,400 --> 00:21:55,520
p plus and p minus okay so once we pick another quantum object from the test set we have to do

162
00:21:55,520 --> 00:22:02,480
we have to say okay this quantum object is a set or is a dog in order to determine if this

163
00:22:03,280 --> 00:22:10,000
we consider this classifier that is called the Elstrom quantum classifier we consider

164
00:22:10,960 --> 00:22:22,080
these two values if the trace of p plus times rho is greater than p minus times rho we say that

165
00:22:23,040 --> 00:22:32,080
in a certain sense rho x is closer to the centroid of the cat otherwise rho x is closer to

166
00:22:32,080 --> 00:22:41,520
the centroid of of the dog and so we can make a really a proper classification a proper classification

167
00:22:41,520 --> 00:22:50,320
okay why we use quantum state discrimination what is the intuition that that is above this idea

168
00:22:50,880 --> 00:22:56,240
the application of quantum state discrimination for classification is inspired by the idea that

169
00:22:56,880 --> 00:23:05,040
better is the discrimination between two centroid and better I can distinguish between two classes

170
00:23:05,040 --> 00:23:13,120
and so more performance is the classifier in other words greater is the Elstrom bound that

171
00:23:13,120 --> 00:23:18,080
remind me I said it is that can be considered as a measurement of distinguishability between two

172
00:23:18,080 --> 00:23:25,360
centroid and greater would be for instance the accuracy of the classifier where the accuracy

173
00:23:25,360 --> 00:23:34,480
means just the number of times that the classification was correct over the number of total over the

174
00:23:34,480 --> 00:23:44,320
total number of experiments okay okay this idea is not only an intuitive idea but that was just

175
00:23:45,280 --> 00:23:56,000
exploited that was just proven by an experiment so we have provided an experiment where this

176
00:23:56,000 --> 00:24:07,760
classifier has been applied to 40 different data sets and compared with 11 different and

177
00:24:07,840 --> 00:24:14,560
general well-performing classifier standard classifier for instance what we can see is that

178
00:24:14,560 --> 00:24:21,680
generally generally once the Elstrom bound increase the Elstrom bound increases

179
00:24:22,400 --> 00:24:29,760
together with the increasing of the balance of accuracy so this idea seems to be correct

180
00:24:29,760 --> 00:24:36,240
that increasing the measurement of distinguishability between two centroid it

181
00:24:37,360 --> 00:24:44,320
is together with the increasing of the accuracy of the classifier so this intuition is corroborated

182
00:24:44,320 --> 00:24:51,040
by the experience is corroborated by the experience and okay we have many data when we

183
00:24:52,000 --> 00:25:00,000
perform it when we compare this classifier is Elstrom classifier with many standard classifier

184
00:25:00,000 --> 00:25:09,360
and we show that our classifier is generally better not always not always but okay the average

185
00:25:12,160 --> 00:25:20,960
performance of the quantum inspired classifier is very very high with respect to the others

186
00:25:20,960 --> 00:25:27,360
standard classical classifier okay now I have many data to show but okay this is

187
00:25:28,240 --> 00:25:35,440
just a comparison okay are we okay in these data in these tables you can realize these

188
00:25:38,320 --> 00:25:45,200
I don't say supremacy but something like this of our Elstrom classifier with respect to the other

189
00:25:45,280 --> 00:25:51,200
but that's not all that's not all indeed a sharp difference between classical and quantum

190
00:25:51,200 --> 00:25:58,480
information theory is also based on the fact that sometime made considering a tensor copy

191
00:25:58,480 --> 00:26:05,600
of an object can provide a benefit in our computation can provide let's say a kind of

192
00:26:05,600 --> 00:26:12,560
additional information with respect to the original information that is given by the

193
00:26:12,560 --> 00:26:21,120
initial single state so what we do is just repeat exactly the same algorithm that I have described

194
00:26:21,120 --> 00:26:29,440
you above before but by considering but off after the encoding after the encoding instead of the

195
00:26:29,440 --> 00:26:36,320
density operator raw the time obtained from the encoding by the encoding from the initial

196
00:26:36,320 --> 00:26:44,480
object instead of this row I consider all times itself and times I calculate a new centroid

197
00:26:44,480 --> 00:26:53,120
and I define again a new classifier in a new Elstrom of several in the new dimension has been a new

198
00:26:53,120 --> 00:27:01,920
k dimensional space in a new larger space and again I define a new Elstrom quantum classifier

199
00:27:02,800 --> 00:27:12,160
let's say k tensor product Elstrom quantum classifier what we see what we see is that by increasing

200
00:27:12,160 --> 00:27:19,680
the number of the copy also there's strong bound increase and so we see that the accuracy that

201
00:27:19,680 --> 00:27:26,960
we obtain after making this copy this copy in a just in a computational way just by making

202
00:27:27,360 --> 00:27:35,040
a in bi-mathematica okay or by fighting this copy but what we see is that by making this

203
00:27:35,040 --> 00:27:43,600
procedure we have benefit because we increase the accuracy of the of the process we increase

204
00:27:43,600 --> 00:27:52,000
again the accuracy of the process okay and again we have data to to show okay we have something but

205
00:27:52,000 --> 00:28:01,040
okay I want to use just the last five minutes talk maybe to show you a practical experiment

206
00:28:01,040 --> 00:28:09,760
a practical experiment that was provided together with the University of Cambridge and the Institute

207
00:28:09,760 --> 00:28:18,320
of Molecular Bioimaging of the University of Catania and the topic is a chronogenic essay

208
00:28:18,320 --> 00:28:25,600
the chronogenic essay is a quantification technique of the survival degree of an in vitro cell

209
00:28:25,600 --> 00:28:32,720
cultures which is based on the ability of a single cell to grow and form a colony of cell

210
00:28:32,720 --> 00:28:39,440
this colony is not a good thing that is a symbol of something with a of something

211
00:28:39,440 --> 00:28:46,480
disease something great disease the purpose is to count the number of their colonies so

212
00:28:46,560 --> 00:28:52,960
the picture that we are today like picture like this where this is a colony this is a colony

213
00:28:52,960 --> 00:29:02,320
this is a colony and so on okay recent results show how bi-classification between pixel x

214
00:29:03,920 --> 00:29:11,040
belongs to the colony or pixel x belongs to the belongs to the background by making this

215
00:29:11,040 --> 00:29:17,520
classification it is possible to have information about the number of of the colonies there is a

216
00:29:17,520 --> 00:29:26,080
correlation between this this is a biological result in a in a in a quantum imagine and biology

217
00:29:27,920 --> 00:29:36,160
fields that this result but it is it is an assist for us because it allows us to move

218
00:29:36,160 --> 00:29:43,200
the chronogenic essay in a binary classification context so for each pixel if we are able to

219
00:29:43,200 --> 00:29:50,800
classify if this pixel is in the colony or it is in the in the background we can have some result

220
00:29:50,800 --> 00:29:59,520
regarding the number of the code that is in the chronogenic essay and so you have 10 minutes

221
00:29:59,600 --> 00:30:07,040
including the question okay so just I thank you I just spend a few minutes to conclude

222
00:30:07,040 --> 00:30:16,240
and I and I leave a few minutes also for four questions okay okay so the experiment

223
00:30:16,240 --> 00:30:23,520
involved with the many many many data about 10 millions of data and the result that we had

224
00:30:23,520 --> 00:30:30,160
was against that the performance of our elstrom classifier with respect to other in this case

225
00:30:30,720 --> 00:30:38,080
18 well-performance classifier is very good because our classifier in most of the cases

226
00:30:38,080 --> 00:30:46,080
was one of the best one of the best with respect to to the other and that was very very nice for us

227
00:30:46,080 --> 00:30:53,600
so let me conclude with some open problem okay the first open problem is the is this

228
00:30:54,560 --> 00:31:01,440
the generalization of this quantum is pirate quantum is pirate and now we know why I say quantum

229
00:31:01,440 --> 00:31:07,040
is pirate and now we know why I really say that this this inspiration is useful it's not only

230
00:31:07,040 --> 00:31:15,520
a matter of metaphor but this quantum inspiration was based on only a binary quantum state discrimination

231
00:31:16,400 --> 00:31:24,400
so the challenge is to have a multiclass classifier a multiclass classifier and the second goal

232
00:31:24,400 --> 00:31:33,440
should be just to use this quantum inspired algorithm also in a quantum computer just in order

233
00:31:33,440 --> 00:31:42,960
to come from one to be spirited to one so we have some partial result regarding both of these

234
00:31:43,520 --> 00:31:51,120
this point and basically regarding the idea of multiclass classifier we are working on this but

235
00:31:52,080 --> 00:32:00,640
we are we are taking inspiration from the pretty good measurement that is not an optimal but

236
00:32:00,640 --> 00:32:10,080
sub the optimal measurement that allows us to have a multi a quantum multiclassifier

237
00:32:10,960 --> 00:32:17,680
and on the other hand well I am not time to show you the detail of the pretty good

238
00:32:17,680 --> 00:32:23,280
classifier but the idea is the same but instead of to have an optimal we have a sub optimal

239
00:32:23,280 --> 00:32:31,840
measurement but but also we this sub optimal measurement can be used for any classification

240
00:32:31,840 --> 00:32:39,760
instead of only binary classification and also the final challenge as I said is just to

241
00:32:41,040 --> 00:32:48,720
come from a quantum inspired to really quantum machine learning in order to put together the

242
00:32:48,720 --> 00:32:54,880
double benefit of quantum so increase the accuracy but also speed up the computation

243
00:32:55,520 --> 00:33:02,080
well by appealing to the neomarks deletion theorem is it possible in principle to reproduce

244
00:33:02,080 --> 00:33:08,560
and POVM measurement and for instance also the health strong of the pretty good measurement

245
00:33:08,640 --> 00:33:15,600
this is an this is an example but we are working on this and till now we have only only partial

246
00:33:15,600 --> 00:33:24,480
result regarding this but the works are still in progress and okay this is the the bibliography

247
00:33:24,480 --> 00:33:32,080
that the reference that you can refer to and okay thank you all for listening thank you

248
00:33:32,640 --> 00:33:42,480
okay thank you Shusepe for your interest in talk now we have some minutes

249
00:33:44,800 --> 00:33:49,840
questions and comments if you want to make a question please

250
00:33:49,840 --> 00:33:59,120
and I have a little question

251
00:34:03,040 --> 00:34:10,960
for your future but do you know if there is a mathematical relation between the

252
00:34:10,960 --> 00:34:22,880
hand strong bound and the accuracy of your classifier okay so yes are you asking if

253
00:34:23,680 --> 00:34:27,200
there is a connection between a strong bound and accuracy

254
00:34:29,840 --> 00:34:36,000
right okay well basically not basically not because it's very strange because

255
00:34:36,960 --> 00:34:44,400
there are really two topics that are that was never merged together between with within

256
00:34:45,680 --> 00:34:53,440
before now because in a certain sense health strong was not thinking about the classification

257
00:34:53,440 --> 00:34:59,280
problem so in a certain sense health strong well quantum state discrimination

258
00:35:00,000 --> 00:35:08,000
and has not the idea of what is the accuracy because the accuracy could be calculated empirically

259
00:35:08,000 --> 00:35:17,200
empirically only by making the experiment what the what the theoretically we have is this result

260
00:35:17,200 --> 00:35:24,160
is this result that is interesting regarding that strong bound and it is easy to misleading

261
00:35:24,240 --> 00:35:32,160
because easy to misleading because we are not saying that if okay we are not saying this

262
00:35:32,160 --> 00:35:38,880
that strong bound between two states is less of that strong bound between the states

263
00:35:38,880 --> 00:35:44,560
that you obtain by making the tensor copy of itself by itself because in this case it is

264
00:35:45,360 --> 00:35:52,400
an already well known result here we are saying this something different because this object

265
00:35:52,400 --> 00:36:05,600
our quantum centroid this tensor k row is not row times row times row k times k times it is

266
00:36:05,600 --> 00:36:16,000
something different is the centroid that we obtain by making in principle k copies of each

267
00:36:16,000 --> 00:36:24,800
object of the data set and after making the centroid so in a certain sense this is a first

268
00:36:24,800 --> 00:36:32,480
let me say that theoretical result that put together quantum state discrimination and

269
00:36:32,480 --> 00:36:38,000
classification problem because there is the idea of centroid and there is the idea of

270
00:36:38,000 --> 00:36:47,360
a strong bound put in together i hope to answer to your question thank you very much welcome

271
00:36:50,000 --> 00:36:51,600
another question or comment

272
00:36:54,160 --> 00:37:00,000
if not thank you again for your talk thank you all thank you

