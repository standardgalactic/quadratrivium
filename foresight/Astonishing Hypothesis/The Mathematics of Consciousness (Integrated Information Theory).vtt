WEBVTT

00:00.000 --> 00:16.040
On June 11th, 2022, a Washington Post published breaking news.

00:16.040 --> 00:20.600
A Google programmer thought that an AI he interacted with was conscious.

00:20.600 --> 00:24.960
Soon after, several news audits published statements from other AI researchers.

00:24.960 --> 00:30.280
They all said that just because it is acting like a conscious being, does not mean an AI

00:30.280 --> 00:32.040
is conscious.

00:32.040 --> 00:34.120
But one question remained unanswered.

00:34.120 --> 00:40.240
How do we know if an AI, or let's say a human being, is conscious?

00:40.240 --> 00:45.040
This is a serious problem, and there are many ethical implications.

00:45.040 --> 00:50.880
For example, we do not know for sure whether animals are conscious, and we do not know

00:50.880 --> 00:54.580
when consciousness arises to a human development.

00:54.580 --> 01:01.180
We can also never be entirely sure whether someone who seems unconscious really has no

01:01.180 --> 01:02.340
consciousness.

01:02.340 --> 01:07.100
Some patients have been shown to remain conscious during anesthesia.

01:07.100 --> 01:13.780
Other patients were found to be conscious after being diagnosed as comatose.

01:13.780 --> 01:17.940
And the problem is not just that we cannot rely on behavior.

01:17.940 --> 01:24.020
We also do not know for sure if someone is conscious by measuring their brain activity.

01:24.020 --> 01:28.860
We have not yet found a reliable biomarker of consciousness.

01:28.860 --> 01:33.200
What we lack is a rigorous scientific theory.

01:33.200 --> 01:38.260
The mathematical equations of such a scientific theory of consciousness should allow us to

01:38.260 --> 01:42.660
determine if someone or something is conscious.

01:42.660 --> 01:48.180
That is, a mathematical theory of consciousness should tell us if an artificial intelligence

01:48.180 --> 01:49.980
is conscious.

01:49.980 --> 01:54.980
Using this math, we should also be able to compute if someone is comatose anesthetized

01:54.980 --> 01:58.500
when dreamless sleep using their brain activity.

01:58.500 --> 02:03.700
More than that, such a theory should allow us to tell what someone is conscious of.

02:03.700 --> 02:08.160
In other words, we should be able to peek into someone's consciousness by applying this

02:08.160 --> 02:12.940
math to neural data measured from their brain.

02:12.940 --> 02:15.580
Does this sound too fantastical?

02:15.580 --> 02:18.740
Well it turns out that such a theory already exists.

02:18.740 --> 02:24.180
This theory is called Integrated Information Theory or IIT for short.

02:24.180 --> 02:29.820
IIT has originally been put forward by neuroscientist Giulia Tornoni in 2008.

02:29.820 --> 02:35.100
IIT has since gained wide support by other leading neuroscientists such as the director

02:35.100 --> 02:39.020
of the Allen Institute, Christoph Koch and many others.

02:39.020 --> 02:45.220
IIT can be expressed as equations, such as these, but IIT is much simpler than it may

02:45.220 --> 02:46.220
seem.

02:46.220 --> 02:49.740
If you are watching, at the end of this video, you will fully understand the mathematics

02:49.740 --> 02:52.700
of IIT and what we can do with it.

02:52.700 --> 02:56.740
IIT starts with a small set of axioms.

02:56.740 --> 03:01.060
Axioms are statements that seem reasonable to accept without proof.

03:01.060 --> 03:04.020
The first axiom is intrinsic existence.

03:04.020 --> 03:07.940
This simply means that we accept that our consciousness exists.

03:07.940 --> 03:13.340
Moreover, our consciousness exists independently of external observers.

03:13.340 --> 03:16.700
If this sounds too philosophical to you, do not worry.

03:16.700 --> 03:20.060
We just need a solid starting point for the theory.

03:20.060 --> 03:26.500
As you will see in a moment, this axiom nicely translates into mathematical equations.

03:26.500 --> 03:30.300
The second axiom of IIT is about information.

03:30.300 --> 03:33.620
Information here just means that consciousness differentiates.

03:33.620 --> 03:39.300
For example, whenever we do not see an elephant, we gain the information that we are not looking

03:39.300 --> 03:40.880
at an elephant.

03:40.880 --> 03:46.120
This definition of information is very different from classic information theory.

03:46.120 --> 03:51.720
As you will see in a moment, IIT does not measure information in bytes or bits.

03:51.720 --> 03:54.280
The third axiom is about integration.

03:54.280 --> 03:58.220
This just means that we have only one consciousness.

03:58.220 --> 04:02.280
We do not have many different fragments of consciousness at the same time.

04:02.280 --> 04:06.700
Our consciousness is unified into a cohesive whole.

04:06.700 --> 04:10.400
The fourth axiom of IIT is composition.

04:10.440 --> 04:12.320
This is as trivial as it sounds.

04:12.320 --> 04:15.040
It means that our consciousness is structured.

04:15.040 --> 04:18.320
We experience different things across space and time.

04:18.320 --> 04:20.620
There is one more axiom.

04:20.620 --> 04:22.920
This axiom is exclusion.

04:22.920 --> 04:26.320
This means that there are boundaries to a conscious experience.

04:26.320 --> 04:31.720
For example, we are only visually conscious of the world in front of us.

04:31.720 --> 04:34.560
If you question one or more of these axioms, do not worry.

04:34.560 --> 04:39.160
There are many axioms in mathematics that are contentious yet useful.

04:39.520 --> 04:42.480
How do we get from these axioms to the rest of the math?

04:42.480 --> 04:48.000
IIT builds on its axioms using hypotheses called postulates.

04:48.000 --> 04:53.200
Postulates translate from the axiomatic properties of consciousness to mathematical characteristics

04:53.200 --> 04:55.200
of a physical system.

04:55.200 --> 05:01.480
In other words, IIT tells us what a system must be like to produce consciousness.

05:01.480 --> 05:09.000
Now, what mathematical descriptions of a system might account for the properties of consciousness?

05:09.000 --> 05:12.520
Then let us first define what IIT means by system.

05:12.520 --> 05:18.760
Well, a system is just a set of things that interact.

05:18.760 --> 05:20.280
Interaction is key.

05:20.280 --> 05:25.080
Remember that the first axiom said that consciousness exists.

05:25.080 --> 05:29.720
Science can only assume the existence of things that interact with other things.

05:29.720 --> 05:34.040
Thus, interaction is the basis of existence.

05:34.040 --> 05:35.120
See how this works?

05:35.120 --> 05:40.440
If we accept that consciousness exists, we can assume that a physical system that produces

05:40.440 --> 05:44.400
consciousness must contain interacting elements.

05:44.400 --> 05:52.760
This is the kind of logic that IIT uses to translate from its axioms to its postulates.

05:52.760 --> 05:56.920
Now let us mathematically formalize what we just said.

05:56.920 --> 06:01.480
Each system consists of two or more elements.

06:01.480 --> 06:07.520
Typically an element can be represented as a discrete variable x sub i.

06:07.520 --> 06:09.560
Elements are defined by three properties.

06:09.560 --> 06:12.560
First, they can have at least two states.

06:12.560 --> 06:17.280
Second, they receive inputs from other elements that can change these states.

06:17.280 --> 06:21.600
And third, they have outputs that depend on these states.

06:21.600 --> 06:27.920
The interacting elements x sub i to sub n form system sub t, where t denotes the present

06:27.920 --> 06:29.440
moment in time.

06:29.440 --> 06:35.280
To keep things easy, let's assume that our four elements here are brain cells or neurons.

06:35.280 --> 06:39.760
As you might know, neurons can either be in an on or off state.

06:39.760 --> 06:43.400
We will indicate these states as unfilled or filled disks.

06:43.400 --> 06:50.320
As all disks are unfilled, all four neurons are off at the current time point t.

06:50.320 --> 06:55.960
Now we said that in order for consciousness to exist, there need to be causal interactions.

06:55.960 --> 06:58.760
These are visualized here by white lines.

06:58.760 --> 07:01.480
How do we define a causal interaction?

07:01.480 --> 07:08.120
Well, remember that each element must have an output that depends on its current state.

07:08.120 --> 07:11.920
These outputs become inputs to other elements.

07:11.920 --> 07:17.160
These inputs define whether or not elements change in state.

07:17.160 --> 07:24.000
So using outputs, single elements, or a combination of elements acting in concert can change the

07:24.000 --> 07:26.320
state of other elements.

07:26.320 --> 07:29.600
That is a causal interaction.

07:29.600 --> 07:35.560
If a subgroup of the system, such as a single element, or a combination thereof, influences

07:35.560 --> 07:41.000
the overall system state, we call that a causal mechanism.

07:41.000 --> 07:44.120
Let us denote mechanisms with y.

07:44.120 --> 07:51.880
And to keep things simple again, let us assume that each element of our model is also a mechanism.

07:51.880 --> 07:56.840
Of course, if we consider the brain, we can think of neurons as elements that are also

07:56.840 --> 07:57.840
mechanisms.

07:57.840 --> 08:03.160
But the mathematics of IIT is more general, as the same concept applies to other brain

08:03.160 --> 08:07.480
structures or even AI.

08:07.480 --> 08:14.000
Now IIT's general idea is to determine whether a system integrates information.

08:14.000 --> 08:19.600
Integrated information is quantified as how much a mechanism in a particular state makes

08:19.600 --> 08:22.720
a difference to the system as a whole.

08:22.720 --> 08:28.200
One way this can be done is by cutting a mechanism out of a system.

08:28.200 --> 08:33.840
If such a cut makes a difference, we can quantify how big that difference is.

08:33.840 --> 08:35.840
How is that done in practice?

08:35.840 --> 08:40.880
Remember, each element has a state, receives inputs, and produces an output.

08:40.880 --> 08:45.000
In our little system, each neuron receives three inputs.

08:45.000 --> 08:50.960
The inputs each neuron receives depend on the state of the other neurons that send their

08:50.960 --> 08:52.640
output to this neuron.

08:52.640 --> 08:59.560
So in our case, each neuron receives a zero or a one from three other neurons depending

08:59.560 --> 09:02.360
on their off or on state.

09:02.360 --> 09:07.560
Real-life neurons perform simple computations, such as summing up their inputs.

09:07.560 --> 09:12.720
If the sum is larger than a certain threshold, neurons change their state from off to on.

09:12.720 --> 09:18.040
So depending on its inputs, a neuron may or may not change its state.

09:18.040 --> 09:20.640
And this state becomes its output.

09:20.640 --> 09:25.840
In our model system, the output is split, so that each of the other neurons receives

09:25.840 --> 09:27.440
an input.

09:27.440 --> 09:31.200
But these are all just copies of the same output.

09:31.200 --> 09:36.960
This means we can describe all possible interactions of this neuron by creating a table.

09:36.960 --> 09:43.400
This table lists all possible combinations of three binary inputs and resulting states.

09:43.400 --> 09:48.040
The resulting state equals that neuron's output.

09:48.040 --> 09:51.600
As we said, real-life neurons sum up inputs.

09:51.600 --> 09:54.720
If the sum is large enough, they turn on.

09:54.720 --> 09:59.040
We can replicate this process in our system by assuming the neuron's threshold to be

09:59.040 --> 10:00.600
three.

10:00.600 --> 10:06.680
In other words, we define a neuron's state to be on, or one, if, and only if, all three

10:06.680 --> 10:08.680
of its inputs are one.

10:08.680 --> 10:13.720
But differently, our model neuron's output is always zero, except when all three of its

10:13.720 --> 10:15.840
inputs are one.

10:15.840 --> 10:20.880
An input-output relationship, such as this, is called an AND gate.

10:20.880 --> 10:22.600
You can probably see why.

10:22.600 --> 10:31.200
The output of this gate is only one if its input 1, and 2, and 3 are all one.

10:31.200 --> 10:37.320
Now let us assume our neuron is much more sensitive and has a much lower threshold for

10:37.320 --> 10:38.320
its summed inputs.

10:38.320 --> 10:40.400
Let's say it's set to one.

10:40.400 --> 10:43.440
In this case, we end up with a very different table.

10:43.440 --> 10:47.520
This kind of table resembles a different logic gate, called OR.

10:47.520 --> 10:48.680
You can see why.

10:48.680 --> 10:55.840
This gate is one if input 1, OR 2, OR 3, OR 1.

10:55.840 --> 10:58.840
Such logic gates are at the heart of computers.

10:58.840 --> 11:04.320
However, keep in mind that actual neurons are much more complicated than that.

11:04.320 --> 11:06.600
So here's our model system again.

11:06.600 --> 11:11.960
Except now we mark for each neuron what kind of input-output table or matrix we have defined

11:11.960 --> 11:12.960
for them.

11:12.960 --> 11:18.840
Now, looking at a single moment in time is not very helpful, since nothing is changing.

11:18.840 --> 11:24.440
And since we're after causal interactions, it is change that we're interested in.

11:24.440 --> 11:29.360
So instead of freezing the system at a single moment, let us look at how the system evolves

11:29.360 --> 11:30.360
over time.

11:30.360 --> 11:39.520
T0 is the current moment, minus 1 is the immediate past, and T plus 1 is one step in the future.

11:39.520 --> 11:46.480
The connections between neurons will be shown between one slice of time and the next one.

11:46.480 --> 11:50.720
In order to calculate the amount of integrated information of a system, we first need to

11:50.720 --> 11:55.200
evaluate the information generated by each single mechanism.

11:55.200 --> 12:01.800
This is done by computing how much each mechanism affects the system as a whole.

12:01.800 --> 12:08.280
This influence is quantifiable, since the current state of a mechanism constrains which

12:08.280 --> 12:10.920
system states can occur.

12:10.920 --> 12:13.280
And IIT goes a step further.

12:13.280 --> 12:18.640
By knowing that a mechanism is in its current state, we also obtain information about the

12:18.640 --> 12:25.560
past, since only a subset of all possible system states can lead up to the mechanism

12:25.560 --> 12:27.600
being in its current state.

12:27.600 --> 12:32.480
This means that a mechanism does not just constrain future possibilities, but in some

12:32.480 --> 12:36.360
sense also constrains the past.

12:36.360 --> 12:37.920
Let's start with the latter.

12:37.920 --> 12:42.680
Suppose we don't know anything about current or past system states.

12:42.680 --> 12:47.040
What would the probability of each possible past state be?

12:47.440 --> 12:52.760
IIT calls this the unconstrained probability or PUC.

12:52.760 --> 12:56.920
For our four neurons, there are 16 possible past states.

12:56.920 --> 13:02.040
We can symbolize these states using zeros and ones, for often on-states of these neurons

13:02.040 --> 13:03.040
respectively.

13:03.040 --> 13:09.400
If we do that, we get a table where each row represents a possible past state.

13:09.400 --> 13:15.080
Since each state is equally likely, our distribution PUC is uniform.

13:15.080 --> 13:20.200
All possible past states might have happened with a probability of 1 over 16.

13:20.200 --> 13:25.600
But remember, this unconstrained probability is due to the fact that we lack information

13:25.600 --> 13:29.520
about mechanism and system states in the present.

13:29.520 --> 13:34.400
Now let's suppose that we know that mechanism X1 is currently in an on state.

13:34.400 --> 13:39.800
Since mechanism X1 only turns on when all of its inputs were on, we can derive a very

13:39.800 --> 13:43.280
different probability distribution of past states.

13:43.280 --> 13:47.400
We now know that there can only have been two past states.

13:47.400 --> 13:49.120
Both states were equally likely.

13:49.120 --> 13:54.720
This means that the conditional probability of each of these states was 0.5.

13:54.720 --> 14:00.000
The resulting distribution is called the cos repertoire of mechanism X1.

14:00.000 --> 14:07.360
This example shows how one can gain information about a system's past by knowing the present.

14:07.360 --> 14:10.080
But how do we quantify that information?

14:10.080 --> 14:15.520
IIT does that by calculating the distance between the unconstrained and the conditional

14:15.520 --> 14:17.400
probability distributions.

14:17.400 --> 14:21.720
The result is called the cos information.

14:21.720 --> 14:27.400
Now that we have seen how we can quantify information about the past that is gained by considering

14:27.400 --> 14:32.440
a given mechanism in its current state, we can apply the same principle to future states.

14:32.440 --> 14:37.040
In other words, we can derive an unconstrained probability of future states as well as the

14:37.040 --> 14:42.200
conditional probabilities of future states given the current state of mechanism X1.

14:42.200 --> 14:45.760
The latter distribution is called the effect repertoire.

14:45.760 --> 14:50.040
Then we calculate the distance between these two distributions.

14:50.040 --> 14:55.840
The resulting number is the effect information of mechanism X1.

14:55.840 --> 14:56.840
Let's recap.

14:56.840 --> 15:02.040
We have seen how IIT quantifies information within a cos system that is contingent upon

15:02.040 --> 15:05.760
or constrained by a mechanism in its current state.

15:05.760 --> 15:11.960
The result of this process are two numbers, the cos information and the effect information.

15:11.960 --> 15:17.280
Of course, both these numbers were specific to a single mechanism in a specific state.

15:17.280 --> 15:21.520
In this case, we looked at mechanism X1, presently being on.

15:21.520 --> 15:27.920
If it were off, both the cos information and the effect information would change accordingly.

15:27.920 --> 15:29.880
And what about the other mechanisms?

15:29.880 --> 15:34.040
Well, if we know the present state of the system, we can simply repeat everything we

15:34.040 --> 15:36.280
have done so far for each of them.

15:36.280 --> 15:41.480
Each time, we obtain the cos information and effect information given that mechanism's

15:41.480 --> 15:43.280
current state.

15:43.280 --> 15:48.120
The final step is to decide whether the cos information or the effect information are

15:48.120 --> 15:50.680
more important to the system.

15:50.680 --> 15:57.000
Since IIT claims to characterize the capacity of a system, it sees the minimum of the two

15:57.000 --> 15:58.460
as a bottleneck.

15:58.460 --> 16:01.920
This bottleneck can be thought of as the weakest link that breaks the chain.

16:02.200 --> 16:07.160
Thus, whatever is the minimum between the cos information and the effect information

16:07.160 --> 16:13.440
is carried forward as the cos effect information of the mechanism being in its current state.

16:13.440 --> 16:20.680
Okay, this explains how IIT quantifies information, but how do we quantify integration?

16:20.680 --> 16:26.040
As it turns out, we can use the same algorithm of comparing probability distributions for

16:26.040 --> 16:28.440
this quantification as well.

16:28.440 --> 16:34.700
Specifically, we eliminate each connection between a mechanism and the rest of the system

16:34.700 --> 16:36.960
and see if that makes any difference.

16:36.960 --> 16:41.680
If the system's probability distributions do not change after we eliminate a connection,

16:41.680 --> 16:47.480
we can deduce that the connection was reducible and there was no integration.

16:47.480 --> 16:52.540
In practice, this is done by a process called statistical noising or marginalizing.

16:52.540 --> 16:58.980
This just means replacing the inputs provided by our connection with a random noise distribution.

16:58.980 --> 17:06.420
Then we compute the distance between the partitioned and unpartitioned probability distributions.

17:06.420 --> 17:11.100
Once we have done that, for all connections, we determine which elimination resulted in

17:11.100 --> 17:13.580
the smallest change.

17:13.580 --> 17:18.740
The partition that yielded the smallest distance is called the minimum information partition

17:18.740 --> 17:24.980
or MIP, and the result of that computation is the amount of integrated information, or

17:24.980 --> 17:29.980
small phi, that is generated by the mechanism.

17:29.980 --> 17:35.260
Now we can compute both the past integrated information as well as the future integrated

17:35.260 --> 17:36.420
information.

17:36.420 --> 17:41.700
As long as both are larger than zero, we can take the minimum to derive the cos effect

17:41.700 --> 17:45.380
integrated information for a mechanism.

17:45.380 --> 17:49.620
Of course, this computation only provided the integrated information on the level of

17:49.620 --> 17:53.100
mechanisms, such as individual neurons.

17:53.100 --> 17:56.260
How do we get from here to the whole brain and consciousness?

17:56.260 --> 18:01.380
Well, we can use the same algorithm of replacing connections with noise, not just for seeing

18:01.380 --> 18:05.780
on mechanisms, but also whole subsystems of a system.

18:05.780 --> 18:06.780
So there you are.

18:06.780 --> 18:10.340
There's a lot more to say of course, but this is the gist of it.

18:10.340 --> 18:14.940
You now understand that IIT provides a simple piece of math that links brain and mind.

18:14.980 --> 18:19.900
By computing the integrated information of a neural system, we can finally start to work

18:19.900 --> 18:22.380
on finding out more about consciousness.

