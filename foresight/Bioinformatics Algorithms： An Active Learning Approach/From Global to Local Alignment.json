{"text": " There are many different phases of sequence comparison in biology and we will now learn some of them. We'll ask first the question what is wrong with the naive scoring model that we used for the longest common subsequence problem when we scored all matches by one. We saw the biologically adequate alignment of adenylation demands constructed by Mara Hales but this alignment is not the optimal longest common subsequence of this adenylation demand. At the bottom of this slide there is actually longest common subsequence, it has more matches but it is biologically completely wrong. So the question arises how can we modify the scoring for computing alignments in such a way that we avoid frivolous matches that you can see in the bottom alignment on this slide. In the current primitive scoring we simply compute score as the number of matches. Let's change it and let's also take into account penalties for mismatches and insertion in deletions. So we give premium 1 to every match and in our alignment game we now give penalty mu to every mismatch and penalty sigma to every indel. And as a result the score in our alignment game changes. Before it was 4, now it is minus 7. How to find optimal solution of the alignment gain and optimal alignment under this model? In this case we essentially constructed the scoring matrix which is 5 by 5 matrix which describes the score for matching every two symbols in the extended alphabet which consists of nucleotides plus the space symbol. And we can design whatever arbitrary scoring matrices, for example I design an arbitrary matrix here and we can use it to play the alignment game. In fact biologists invest a lot of efforts into designing adequate scoring matrices, particularly scoring matrices for amino acid sequences. And the goal of the scoring matrices is to reflect the mutation propensity of different amino acid. For example amino acid Y often mutates into F and that's why it gets high score plus 7 but rarely mutates in some other amino acids, for example proline and in this case it gets actually penalty minus 5. And this is an example of scoring matrix that biologists use. Now in the case we work with scoring matrices how our dynamic programming currency change? Instead of the currency shown on the slide we simply have the following currency, Sij equal to four different possibilities depending on whether we are computing score for insertion, deletion, match or mismatch as shown on the slide and the scores of edges in the alignment graph change accordingly as shown on the slide. Or alternatively we can, for very general scoring matrix, we simply can write three terms of currency where green, blue and red alternatives correspond to vertical, horizontal and diagonal edges. And global alignment problem that we want to solve is the following one. Given strings V and W and a matrix score we want to find an alignment of this string whose alignment score as defined by the scoring matrix is maximum among all possible alignments of these strings. Global alignment is a good model for some biological sequence comparison problems but bad model for some others. And I'll give you an example of how we unbox genes to illustrate the challenges of biological sequence comparison. Two genes in different species may be similar over short conserved regions and dissimilar over remaining regions. For example, homeobog genes have short regions called the homeo domain that is highly conserved among species varying from human to fly. But global alignment of homeobog genes would not reveal homeo domain because it would most likely pass through completely arbitrary regions of the sequences since homeo domains are short sub-segments of homeobog genes. How can we find this important biological similarity that however do not extend over the entire length of sequences and thus in the case of search for these short sequences the global alignment fails? At this slide you see two alignments and the question arises which alignment is better. The alignment on the top actually has a higher score but the alignment at the bottom has lower score but more biologically relevant because it shows a very strong match of short sequences. How can we find this alignment despite the fact that global alignment may miss it? And search for such short segments within sequences that exhibit similarity is called the local alignment problem. So in this case there are two possible alignments in the alignment graphs. The alignment on the top is biologically correct but the alignment in the middle is actually a random alignment that however has a higher score from the perspective of global alignment and therefore hides from us the biologically relevant alignment. So what I want to do is to somehow find these short sub-strengths of the entire strength that exhibit high similarity. How do I do this? There is a very simple way to search for short similar strengths within longer strengths. We can simply try all possible pair of strengths from two sequences and each such pair corresponds to a rectangle in the alignment graph. Here is one of the rectangles. But there are so many such rectangles that this approach of course becomes impractical since search for optimal global alignment within each smaller rectangle requires quadratic time and therefore overall the running time will become very large. What can we do to come up with a practical local alignment algorithm? The first thing we need to do is to formulate the local alignment problem. The input is strengths v and w and a scoring matrix score and output as is sub-strengths of the entire strengths v and w whose global alignment as defined by the score is maximum in one all global alignments of all sub-strengths of v and w. My proposal for solving this problem let's introduce free taxi rides through the alignment graph. Indeed, if we were able to start in the source and travel freely to the start of the concert fragment and then take another free taxi ride from the end of concert fragment to the destination final note of the alignment graph, then we will be able to score these interesting segments by taking zero cost of taxi ride to the beginning of this fragment, then real cost of the alignment of the fragment and then plus another zero which is the cost of another taxi ride. You may ask how in the world we can take taxi ride through the alignment graph? The whole point of introducing this concept of Manhattan kind cities and traveling in them is that we are free to build whatever Manhattan like grids for solving our biological problems and in this case what is a free taxi ride? It's simply adding extra edges of weight zero to our alignment graph and since we are free to build whatever Manhattan we want, we can of course, we are at liberty of introducing this taxi ride. So let's see how our graph change. What we need to do to implement this free taxi ride? We need to add edges from the source to any other note and it will be roughly quadratic number of edges. We also need to add edges from every note to the same once again quadratic number of edges so the number of edges in the graph remains quadratic and therefore our algorithm will be fast. And in that how our dynamic programming currency change for the local alignment? Before we had three possibilities corresponding to three ways to enter a note. By vertical edge, by horizontal edge and by diagonal edge. Now there is one more possibility we can take a free taxi ride to the note. So now there are four possibilities for entering every note which means that we need to add the fourth term in this currency which is the weight of edge from zero zero to ij. And the weight of this edge since our taxi rides are free is zero and that's the only change that we need to implement to make our local alignment algorithms practical and fast. And we now move to the problem of defining adequate insertion and deletion penalties in sequence alignment.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.72, "text": " There are many different phases of sequence comparison in biology and we will now learn", "tokens": [50364, 821, 366, 867, 819, 18764, 295, 8310, 9660, 294, 14956, 293, 321, 486, 586, 1466, 51000], "temperature": 0.0, "avg_logprob": -0.25417881324642994, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.27736562490463257}, {"id": 1, "seek": 0, "start": 12.72, "end": 14.22, "text": " some of them.", "tokens": [51000, 512, 295, 552, 13, 51075], "temperature": 0.0, "avg_logprob": -0.25417881324642994, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.27736562490463257}, {"id": 2, "seek": 0, "start": 14.22, "end": 21.64, "text": " We'll ask first the question what is wrong with the naive scoring model that we used", "tokens": [51075, 492, 603, 1029, 700, 264, 1168, 437, 307, 2085, 365, 264, 29052, 22358, 2316, 300, 321, 1143, 51446], "temperature": 0.0, "avg_logprob": -0.25417881324642994, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.27736562490463257}, {"id": 3, "seek": 0, "start": 21.64, "end": 29.560000000000002, "text": " for the longest common subsequence problem when we scored all matches by one.", "tokens": [51446, 337, 264, 15438, 2689, 13924, 655, 1154, 562, 321, 18139, 439, 10676, 538, 472, 13, 51842], "temperature": 0.0, "avg_logprob": -0.25417881324642994, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.27736562490463257}, {"id": 4, "seek": 2956, "start": 29.56, "end": 36.72, "text": " We saw the biologically adequate alignment of adenylation demands constructed by Mara", "tokens": [50364, 492, 1866, 264, 3228, 17157, 20927, 18515, 295, 614, 268, 5088, 399, 15107, 17083, 538, 2039, 64, 50722], "temperature": 0.0, "avg_logprob": -0.22532545892815842, "compression_ratio": 1.7431693989071038, "no_speech_prob": 0.006288308650255203}, {"id": 5, "seek": 2956, "start": 36.72, "end": 44.76, "text": " Hales but this alignment is not the optimal longest common subsequence of this adenylation", "tokens": [50722, 389, 4229, 457, 341, 18515, 307, 406, 264, 16252, 15438, 2689, 13924, 655, 295, 341, 614, 268, 5088, 399, 51124], "temperature": 0.0, "avg_logprob": -0.22532545892815842, "compression_ratio": 1.7431693989071038, "no_speech_prob": 0.006288308650255203}, {"id": 6, "seek": 2956, "start": 44.76, "end": 45.86, "text": " demand.", "tokens": [51124, 4733, 13, 51179], "temperature": 0.0, "avg_logprob": -0.22532545892815842, "compression_ratio": 1.7431693989071038, "no_speech_prob": 0.006288308650255203}, {"id": 7, "seek": 2956, "start": 45.86, "end": 52.16, "text": " At the bottom of this slide there is actually longest common subsequence, it has more matches", "tokens": [51179, 1711, 264, 2767, 295, 341, 4137, 456, 307, 767, 15438, 2689, 13924, 655, 11, 309, 575, 544, 10676, 51494], "temperature": 0.0, "avg_logprob": -0.22532545892815842, "compression_ratio": 1.7431693989071038, "no_speech_prob": 0.006288308650255203}, {"id": 8, "seek": 2956, "start": 52.16, "end": 56.519999999999996, "text": " but it is biologically completely wrong.", "tokens": [51494, 457, 309, 307, 3228, 17157, 2584, 2085, 13, 51712], "temperature": 0.0, "avg_logprob": -0.22532545892815842, "compression_ratio": 1.7431693989071038, "no_speech_prob": 0.006288308650255203}, {"id": 9, "seek": 5652, "start": 56.52, "end": 63.760000000000005, "text": " So the question arises how can we modify the scoring for computing alignments in such", "tokens": [50364, 407, 264, 1168, 27388, 577, 393, 321, 16927, 264, 22358, 337, 15866, 7975, 1117, 294, 1270, 50726], "temperature": 0.0, "avg_logprob": -0.17453910643795886, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.001585520920343697}, {"id": 10, "seek": 5652, "start": 63.760000000000005, "end": 70.56, "text": " a way that we avoid frivolous matches that you can see in the bottom alignment on this", "tokens": [50726, 257, 636, 300, 321, 5042, 431, 21356, 563, 10676, 300, 291, 393, 536, 294, 264, 2767, 18515, 322, 341, 51066], "temperature": 0.0, "avg_logprob": -0.17453910643795886, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.001585520920343697}, {"id": 11, "seek": 5652, "start": 70.56, "end": 72.56, "text": " slide.", "tokens": [51066, 4137, 13, 51166], "temperature": 0.0, "avg_logprob": -0.17453910643795886, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.001585520920343697}, {"id": 12, "seek": 5652, "start": 72.56, "end": 78.76, "text": " In the current primitive scoring we simply compute score as the number of matches.", "tokens": [51166, 682, 264, 2190, 28540, 22358, 321, 2935, 14722, 6175, 382, 264, 1230, 295, 10676, 13, 51476], "temperature": 0.0, "avg_logprob": -0.17453910643795886, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.001585520920343697}, {"id": 13, "seek": 5652, "start": 78.76, "end": 85.84, "text": " Let's change it and let's also take into account penalties for mismatches and insertion", "tokens": [51476, 961, 311, 1319, 309, 293, 718, 311, 611, 747, 666, 2696, 35389, 337, 23220, 852, 279, 293, 8969, 313, 51830], "temperature": 0.0, "avg_logprob": -0.17453910643795886, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.001585520920343697}, {"id": 14, "seek": 8584, "start": 85.84, "end": 86.84, "text": " in deletions.", "tokens": [50364, 294, 1103, 302, 626, 13, 50414], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 15, "seek": 8584, "start": 86.84, "end": 94.60000000000001, "text": " So we give premium 1 to every match and in our alignment game we now give penalty mu", "tokens": [50414, 407, 321, 976, 12049, 502, 281, 633, 2995, 293, 294, 527, 18515, 1216, 321, 586, 976, 16263, 2992, 50802], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 16, "seek": 8584, "start": 94.60000000000001, "end": 100.16, "text": " to every mismatch and penalty sigma to every indel.", "tokens": [50802, 281, 633, 23220, 852, 293, 16263, 12771, 281, 633, 1016, 338, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 17, "seek": 8584, "start": 100.16, "end": 104.80000000000001, "text": " And as a result the score in our alignment game changes.", "tokens": [51080, 400, 382, 257, 1874, 264, 6175, 294, 527, 18515, 1216, 2962, 13, 51312], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 18, "seek": 8584, "start": 104.80000000000001, "end": 108.36, "text": " Before it was 4, now it is minus 7.", "tokens": [51312, 4546, 309, 390, 1017, 11, 586, 309, 307, 3175, 1614, 13, 51490], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 19, "seek": 8584, "start": 108.36, "end": 114.68, "text": " How to find optimal solution of the alignment gain and optimal alignment under this model?", "tokens": [51490, 1012, 281, 915, 16252, 3827, 295, 264, 18515, 6052, 293, 16252, 18515, 833, 341, 2316, 30, 51806], "temperature": 0.0, "avg_logprob": -0.17857761816544968, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.01701122522354126}, {"id": 20, "seek": 11468, "start": 114.68, "end": 122.04, "text": " In this case we essentially constructed the scoring matrix which is 5 by 5 matrix which", "tokens": [50364, 682, 341, 1389, 321, 4476, 17083, 264, 22358, 8141, 597, 307, 1025, 538, 1025, 8141, 597, 50732], "temperature": 0.0, "avg_logprob": -0.21738512814044952, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.06547068059444427}, {"id": 21, "seek": 11468, "start": 122.04, "end": 129.72, "text": " describes the score for matching every two symbols in the extended alphabet which consists", "tokens": [50732, 15626, 264, 6175, 337, 14324, 633, 732, 16944, 294, 264, 10913, 23339, 597, 14689, 51116], "temperature": 0.0, "avg_logprob": -0.21738512814044952, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.06547068059444427}, {"id": 22, "seek": 11468, "start": 129.72, "end": 133.88, "text": " of nucleotides plus the space symbol.", "tokens": [51116, 295, 14962, 310, 1875, 1804, 264, 1901, 5986, 13, 51324], "temperature": 0.0, "avg_logprob": -0.21738512814044952, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.06547068059444427}, {"id": 23, "seek": 11468, "start": 133.88, "end": 140.36, "text": " And we can design whatever arbitrary scoring matrices, for example I design an arbitrary", "tokens": [51324, 400, 321, 393, 1715, 2035, 23211, 22358, 32284, 11, 337, 1365, 286, 1715, 364, 23211, 51648], "temperature": 0.0, "avg_logprob": -0.21738512814044952, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.06547068059444427}, {"id": 24, "seek": 14036, "start": 140.36, "end": 146.24, "text": " matrix here and we can use it to play the alignment game.", "tokens": [50364, 8141, 510, 293, 321, 393, 764, 309, 281, 862, 264, 18515, 1216, 13, 50658], "temperature": 0.0, "avg_logprob": -0.18264297940837804, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.15765313804149628}, {"id": 25, "seek": 14036, "start": 146.24, "end": 155.0, "text": " In fact biologists invest a lot of efforts into designing adequate scoring matrices,", "tokens": [50658, 682, 1186, 3228, 12256, 1963, 257, 688, 295, 6484, 666, 14685, 20927, 22358, 32284, 11, 51096], "temperature": 0.0, "avg_logprob": -0.18264297940837804, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.15765313804149628}, {"id": 26, "seek": 14036, "start": 155.0, "end": 159.12, "text": " particularly scoring matrices for amino acid sequences.", "tokens": [51096, 4098, 22358, 32284, 337, 24674, 8258, 22978, 13, 51302], "temperature": 0.0, "avg_logprob": -0.18264297940837804, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.15765313804149628}, {"id": 27, "seek": 14036, "start": 159.12, "end": 166.32000000000002, "text": " And the goal of the scoring matrices is to reflect the mutation propensity of different", "tokens": [51302, 400, 264, 3387, 295, 264, 22358, 32284, 307, 281, 5031, 264, 27960, 2365, 6859, 295, 819, 51662], "temperature": 0.0, "avg_logprob": -0.18264297940837804, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.15765313804149628}, {"id": 28, "seek": 14036, "start": 166.32000000000002, "end": 167.60000000000002, "text": " amino acid.", "tokens": [51662, 24674, 8258, 13, 51726], "temperature": 0.0, "avg_logprob": -0.18264297940837804, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.15765313804149628}, {"id": 29, "seek": 16760, "start": 167.6, "end": 176.51999999999998, "text": " For example amino acid Y often mutates into F and that's why it gets high score plus", "tokens": [50364, 1171, 1365, 24674, 8258, 398, 2049, 5839, 1024, 666, 479, 293, 300, 311, 983, 309, 2170, 1090, 6175, 1804, 50810], "temperature": 0.0, "avg_logprob": -0.21184670223909266, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02724640443921089}, {"id": 30, "seek": 16760, "start": 176.51999999999998, "end": 183.32, "text": " 7 but rarely mutates in some other amino acids, for example proline and in this case it gets", "tokens": [50810, 1614, 457, 13752, 5839, 1024, 294, 512, 661, 24674, 21667, 11, 337, 1365, 447, 1889, 293, 294, 341, 1389, 309, 2170, 51150], "temperature": 0.0, "avg_logprob": -0.21184670223909266, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02724640443921089}, {"id": 31, "seek": 16760, "start": 183.32, "end": 186.51999999999998, "text": " actually penalty minus 5.", "tokens": [51150, 767, 16263, 3175, 1025, 13, 51310], "temperature": 0.0, "avg_logprob": -0.21184670223909266, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02724640443921089}, {"id": 32, "seek": 16760, "start": 186.51999999999998, "end": 192.2, "text": " And this is an example of scoring matrix that biologists use.", "tokens": [51310, 400, 341, 307, 364, 1365, 295, 22358, 8141, 300, 3228, 12256, 764, 13, 51594], "temperature": 0.0, "avg_logprob": -0.21184670223909266, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02724640443921089}, {"id": 33, "seek": 19220, "start": 192.2, "end": 198.83999999999997, "text": " Now in the case we work with scoring matrices how our dynamic programming currency change?", "tokens": [50364, 823, 294, 264, 1389, 321, 589, 365, 22358, 32284, 577, 527, 8546, 9410, 13346, 1319, 30, 50696], "temperature": 0.0, "avg_logprob": -0.37083709449098823, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.01503218524158001}, {"id": 34, "seek": 19220, "start": 198.83999999999997, "end": 206.11999999999998, "text": " Instead of the currency shown on the slide we simply have the following currency, Sij", "tokens": [50696, 7156, 295, 264, 13346, 4898, 322, 264, 4137, 321, 2935, 362, 264, 3480, 13346, 11, 318, 1718, 51060], "temperature": 0.0, "avg_logprob": -0.37083709449098823, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.01503218524158001}, {"id": 35, "seek": 19220, "start": 206.11999999999998, "end": 215.79999999999998, "text": " equal to four different possibilities depending on whether we are computing score for insertion,", "tokens": [51060, 2681, 281, 1451, 819, 12178, 5413, 322, 1968, 321, 366, 15866, 6175, 337, 8969, 313, 11, 51544], "temperature": 0.0, "avg_logprob": -0.37083709449098823, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.01503218524158001}, {"id": 36, "seek": 21580, "start": 215.8, "end": 223.44, "text": " deletion, match or mismatch as shown on the slide and the scores of edges in the alignment", "tokens": [50364, 1103, 302, 313, 11, 2995, 420, 23220, 852, 382, 4898, 322, 264, 4137, 293, 264, 13444, 295, 8819, 294, 264, 18515, 50746], "temperature": 0.0, "avg_logprob": -0.23582515716552735, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.39267122745513916}, {"id": 37, "seek": 21580, "start": 223.44, "end": 228.24, "text": " graph change accordingly as shown on the slide.", "tokens": [50746, 4295, 1319, 19717, 382, 4898, 322, 264, 4137, 13, 50986], "temperature": 0.0, "avg_logprob": -0.23582515716552735, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.39267122745513916}, {"id": 38, "seek": 21580, "start": 228.24, "end": 234.60000000000002, "text": " Or alternatively we can, for very general scoring matrix, we simply can write three", "tokens": [50986, 1610, 8535, 356, 321, 393, 11, 337, 588, 2674, 22358, 8141, 11, 321, 2935, 393, 2464, 1045, 51304], "temperature": 0.0, "avg_logprob": -0.23582515716552735, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.39267122745513916}, {"id": 39, "seek": 23460, "start": 234.6, "end": 245.68, "text": " terms of currency where green, blue and red alternatives correspond to vertical, horizontal", "tokens": [50364, 2115, 295, 13346, 689, 3092, 11, 3344, 293, 2182, 20478, 6805, 281, 9429, 11, 12750, 50918], "temperature": 0.0, "avg_logprob": -0.1793470226350378, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.4333426356315613}, {"id": 40, "seek": 23460, "start": 245.68, "end": 248.4, "text": " and diagonal edges.", "tokens": [50918, 293, 21539, 8819, 13, 51054], "temperature": 0.0, "avg_logprob": -0.1793470226350378, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.4333426356315613}, {"id": 41, "seek": 23460, "start": 248.4, "end": 254.76, "text": " And global alignment problem that we want to solve is the following one.", "tokens": [51054, 400, 4338, 18515, 1154, 300, 321, 528, 281, 5039, 307, 264, 3480, 472, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1793470226350378, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.4333426356315613}, {"id": 42, "seek": 23460, "start": 254.76, "end": 261.71999999999997, "text": " Given strings V and W and a matrix score we want to find an alignment of this string", "tokens": [51372, 18600, 13985, 691, 293, 343, 293, 257, 8141, 6175, 321, 528, 281, 915, 364, 18515, 295, 341, 6798, 51720], "temperature": 0.0, "avg_logprob": -0.1793470226350378, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.4333426356315613}, {"id": 43, "seek": 26172, "start": 261.72, "end": 270.84000000000003, "text": " whose alignment score as defined by the scoring matrix is maximum among all possible alignments", "tokens": [50364, 6104, 18515, 6175, 382, 7642, 538, 264, 22358, 8141, 307, 6674, 3654, 439, 1944, 7975, 1117, 50820], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 44, "seek": 26172, "start": 270.84000000000003, "end": 273.88000000000005, "text": " of these strings.", "tokens": [50820, 295, 613, 13985, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 45, "seek": 26172, "start": 273.88000000000005, "end": 280.24, "text": " Global alignment is a good model for some biological sequence comparison problems but", "tokens": [50972, 14465, 18515, 307, 257, 665, 2316, 337, 512, 13910, 8310, 9660, 2740, 457, 51290], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 46, "seek": 26172, "start": 280.24, "end": 283.64000000000004, "text": " bad model for some others.", "tokens": [51290, 1578, 2316, 337, 512, 2357, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 47, "seek": 26172, "start": 283.64000000000004, "end": 288.28000000000003, "text": " And I'll give you an example of how we unbox genes to illustrate the challenges of biological", "tokens": [51460, 400, 286, 603, 976, 291, 364, 1365, 295, 577, 321, 20242, 14424, 281, 23221, 264, 4759, 295, 13910, 51692], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 48, "seek": 26172, "start": 288.28000000000003, "end": 290.20000000000005, "text": " sequence comparison.", "tokens": [51692, 8310, 9660, 13, 51788], "temperature": 0.0, "avg_logprob": -0.2630696167816987, "compression_ratio": 1.705, "no_speech_prob": 0.04036683216691017}, {"id": 49, "seek": 29020, "start": 290.2, "end": 298.15999999999997, "text": " Two genes in different species may be similar over short conserved regions and dissimilar", "tokens": [50364, 4453, 14424, 294, 819, 6172, 815, 312, 2531, 670, 2099, 1014, 6913, 10682, 293, 7802, 332, 2202, 50762], "temperature": 0.0, "avg_logprob": -0.1631659737116174, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.17990076541900635}, {"id": 50, "seek": 29020, "start": 298.15999999999997, "end": 300.59999999999997, "text": " over remaining regions.", "tokens": [50762, 670, 8877, 10682, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1631659737116174, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.17990076541900635}, {"id": 51, "seek": 29020, "start": 300.59999999999997, "end": 308.24, "text": " For example, homeobog genes have short regions called the homeo domain that is highly conserved", "tokens": [50884, 1171, 1365, 11, 1280, 996, 664, 14424, 362, 2099, 10682, 1219, 264, 1280, 78, 9274, 300, 307, 5405, 1014, 6913, 51266], "temperature": 0.0, "avg_logprob": -0.1631659737116174, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.17990076541900635}, {"id": 52, "seek": 29020, "start": 308.24, "end": 312.32, "text": " among species varying from human to fly.", "tokens": [51266, 3654, 6172, 22984, 490, 1952, 281, 3603, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1631659737116174, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.17990076541900635}, {"id": 53, "seek": 29020, "start": 312.32, "end": 318.91999999999996, "text": " But global alignment of homeobog genes would not reveal homeo domain because it would most", "tokens": [51470, 583, 4338, 18515, 295, 1280, 996, 664, 14424, 576, 406, 10658, 1280, 78, 9274, 570, 309, 576, 881, 51800], "temperature": 0.0, "avg_logprob": -0.1631659737116174, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.17990076541900635}, {"id": 54, "seek": 31892, "start": 318.92, "end": 324.88, "text": " likely pass through completely arbitrary regions of the sequences since homeo domains", "tokens": [50364, 3700, 1320, 807, 2584, 23211, 10682, 295, 264, 22978, 1670, 1280, 78, 25514, 50662], "temperature": 0.0, "avg_logprob": -0.19681456353929308, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04283414036035538}, {"id": 55, "seek": 31892, "start": 324.88, "end": 329.0, "text": " are short sub-segments of homeobog genes.", "tokens": [50662, 366, 2099, 1422, 12, 405, 37148, 295, 1280, 996, 664, 14424, 13, 50868], "temperature": 0.0, "avg_logprob": -0.19681456353929308, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04283414036035538}, {"id": 56, "seek": 31892, "start": 329.0, "end": 336.04, "text": " How can we find this important biological similarity that however do not extend over", "tokens": [50868, 1012, 393, 321, 915, 341, 1021, 13910, 32194, 300, 4461, 360, 406, 10101, 670, 51220], "temperature": 0.0, "avg_logprob": -0.19681456353929308, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04283414036035538}, {"id": 57, "seek": 31892, "start": 336.04, "end": 343.12, "text": " the entire length of sequences and thus in the case of search for these short sequences", "tokens": [51220, 264, 2302, 4641, 295, 22978, 293, 8807, 294, 264, 1389, 295, 3164, 337, 613, 2099, 22978, 51574], "temperature": 0.0, "avg_logprob": -0.19681456353929308, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04283414036035538}, {"id": 58, "seek": 31892, "start": 343.12, "end": 346.20000000000005, "text": " the global alignment fails?", "tokens": [51574, 264, 4338, 18515, 18199, 30, 51728], "temperature": 0.0, "avg_logprob": -0.19681456353929308, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04283414036035538}, {"id": 59, "seek": 34620, "start": 346.2, "end": 352.08, "text": " At this slide you see two alignments and the question arises which alignment is better.", "tokens": [50364, 1711, 341, 4137, 291, 536, 732, 7975, 1117, 293, 264, 1168, 27388, 597, 18515, 307, 1101, 13, 50658], "temperature": 0.0, "avg_logprob": -0.13918904066085816, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.16735953092575073}, {"id": 60, "seek": 34620, "start": 352.08, "end": 358.64, "text": " The alignment on the top actually has a higher score but the alignment at the bottom has", "tokens": [50658, 440, 18515, 322, 264, 1192, 767, 575, 257, 2946, 6175, 457, 264, 18515, 412, 264, 2767, 575, 50986], "temperature": 0.0, "avg_logprob": -0.13918904066085816, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.16735953092575073}, {"id": 61, "seek": 34620, "start": 358.64, "end": 366.84, "text": " lower score but more biologically relevant because it shows a very strong match of short", "tokens": [50986, 3126, 6175, 457, 544, 3228, 17157, 7340, 570, 309, 3110, 257, 588, 2068, 2995, 295, 2099, 51396], "temperature": 0.0, "avg_logprob": -0.13918904066085816, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.16735953092575073}, {"id": 62, "seek": 34620, "start": 366.84, "end": 367.84, "text": " sequences.", "tokens": [51396, 22978, 13, 51446], "temperature": 0.0, "avg_logprob": -0.13918904066085816, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.16735953092575073}, {"id": 63, "seek": 34620, "start": 367.84, "end": 374.2, "text": " How can we find this alignment despite the fact that global alignment may miss it?", "tokens": [51446, 1012, 393, 321, 915, 341, 18515, 7228, 264, 1186, 300, 4338, 18515, 815, 1713, 309, 30, 51764], "temperature": 0.0, "avg_logprob": -0.13918904066085816, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.16735953092575073}, {"id": 64, "seek": 37420, "start": 374.2, "end": 380.8, "text": " And search for such short segments within sequences that exhibit similarity is called", "tokens": [50364, 400, 3164, 337, 1270, 2099, 19904, 1951, 22978, 300, 20487, 32194, 307, 1219, 50694], "temperature": 0.0, "avg_logprob": -0.10922638781659015, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.14951381087303162}, {"id": 65, "seek": 37420, "start": 380.8, "end": 383.2, "text": " the local alignment problem.", "tokens": [50694, 264, 2654, 18515, 1154, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10922638781659015, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.14951381087303162}, {"id": 66, "seek": 37420, "start": 383.2, "end": 389.88, "text": " So in this case there are two possible alignments in the alignment graphs.", "tokens": [50814, 407, 294, 341, 1389, 456, 366, 732, 1944, 7975, 1117, 294, 264, 18515, 24877, 13, 51148], "temperature": 0.0, "avg_logprob": -0.10922638781659015, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.14951381087303162}, {"id": 67, "seek": 37420, "start": 389.88, "end": 397.08, "text": " The alignment on the top is biologically correct but the alignment in the middle is actually", "tokens": [51148, 440, 18515, 322, 264, 1192, 307, 3228, 17157, 3006, 457, 264, 18515, 294, 264, 2808, 307, 767, 51508], "temperature": 0.0, "avg_logprob": -0.10922638781659015, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.14951381087303162}, {"id": 68, "seek": 37420, "start": 397.08, "end": 401.91999999999996, "text": " a random alignment that however has a higher score from the perspective of global alignment", "tokens": [51508, 257, 4974, 18515, 300, 4461, 575, 257, 2946, 6175, 490, 264, 4585, 295, 4338, 18515, 51750], "temperature": 0.0, "avg_logprob": -0.10922638781659015, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.14951381087303162}, {"id": 69, "seek": 40192, "start": 401.92, "end": 408.12, "text": " and therefore hides from us the biologically relevant alignment.", "tokens": [50364, 293, 4412, 35953, 490, 505, 264, 3228, 17157, 7340, 18515, 13, 50674], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 70, "seek": 40192, "start": 408.12, "end": 414.6, "text": " So what I want to do is to somehow find these short sub-strengths of the entire strength", "tokens": [50674, 407, 437, 286, 528, 281, 360, 307, 281, 6063, 915, 613, 2099, 1422, 12, 26421, 3674, 82, 295, 264, 2302, 3800, 50998], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 71, "seek": 40192, "start": 414.6, "end": 416.64000000000004, "text": " that exhibit high similarity.", "tokens": [50998, 300, 20487, 1090, 32194, 13, 51100], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 72, "seek": 40192, "start": 416.64000000000004, "end": 417.64000000000004, "text": " How do I do this?", "tokens": [51100, 1012, 360, 286, 360, 341, 30, 51150], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 73, "seek": 40192, "start": 417.64000000000004, "end": 424.68, "text": " There is a very simple way to search for short similar strengths within longer strengths.", "tokens": [51150, 821, 307, 257, 588, 2199, 636, 281, 3164, 337, 2099, 2531, 16986, 1951, 2854, 16986, 13, 51502], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 74, "seek": 40192, "start": 424.68, "end": 431.88, "text": " We can simply try all possible pair of strengths from two sequences and each such pair corresponds", "tokens": [51502, 492, 393, 2935, 853, 439, 1944, 6119, 295, 16986, 490, 732, 22978, 293, 1184, 1270, 6119, 23249, 51862], "temperature": 0.0, "avg_logprob": -0.16708852979871963, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.026861796155571938}, {"id": 75, "seek": 43188, "start": 431.88, "end": 435.2, "text": " to a rectangle in the alignment graph.", "tokens": [50364, 281, 257, 21930, 294, 264, 18515, 4295, 13, 50530], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 76, "seek": 43188, "start": 435.2, "end": 437.28, "text": " Here is one of the rectangles.", "tokens": [50530, 1692, 307, 472, 295, 264, 24077, 904, 13, 50634], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 77, "seek": 43188, "start": 437.28, "end": 442.48, "text": " But there are so many such rectangles that this approach of course becomes impractical", "tokens": [50634, 583, 456, 366, 370, 867, 1270, 24077, 904, 300, 341, 3109, 295, 1164, 3643, 704, 1897, 804, 50894], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 78, "seek": 43188, "start": 442.48, "end": 449.52, "text": " since search for optimal global alignment within each smaller rectangle requires quadratic", "tokens": [50894, 1670, 3164, 337, 16252, 4338, 18515, 1951, 1184, 4356, 21930, 7029, 37262, 51246], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 79, "seek": 43188, "start": 449.52, "end": 455.12, "text": " time and therefore overall the running time will become very large.", "tokens": [51246, 565, 293, 4412, 4787, 264, 2614, 565, 486, 1813, 588, 2416, 13, 51526], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 80, "seek": 43188, "start": 455.12, "end": 460.96, "text": " What can we do to come up with a practical local alignment algorithm?", "tokens": [51526, 708, 393, 321, 360, 281, 808, 493, 365, 257, 8496, 2654, 18515, 9284, 30, 51818], "temperature": 0.0, "avg_logprob": -0.18768013224882238, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.02870994247496128}, {"id": 81, "seek": 46096, "start": 460.96, "end": 465.32, "text": " The first thing we need to do is to formulate the local alignment problem.", "tokens": [50364, 440, 700, 551, 321, 643, 281, 360, 307, 281, 47881, 264, 2654, 18515, 1154, 13, 50582], "temperature": 0.0, "avg_logprob": -0.19377159777982736, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.1582687646150589}, {"id": 82, "seek": 46096, "start": 465.32, "end": 475.2, "text": " The input is strengths v and w and a scoring matrix score and output as is sub-strengths", "tokens": [50582, 440, 4846, 307, 16986, 371, 293, 261, 293, 257, 22358, 8141, 6175, 293, 5598, 382, 307, 1422, 12, 26421, 3674, 82, 51076], "temperature": 0.0, "avg_logprob": -0.19377159777982736, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.1582687646150589}, {"id": 83, "seek": 46096, "start": 475.2, "end": 482.12, "text": " of the entire strengths v and w whose global alignment as defined by the score is maximum", "tokens": [51076, 295, 264, 2302, 16986, 371, 293, 261, 6104, 4338, 18515, 382, 7642, 538, 264, 6175, 307, 6674, 51422], "temperature": 0.0, "avg_logprob": -0.19377159777982736, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.1582687646150589}, {"id": 84, "seek": 46096, "start": 482.12, "end": 488.76, "text": " in one all global alignments of all sub-strengths of v and w.", "tokens": [51422, 294, 472, 439, 4338, 7975, 1117, 295, 439, 1422, 12, 26421, 3674, 82, 295, 371, 293, 261, 13, 51754], "temperature": 0.0, "avg_logprob": -0.19377159777982736, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.1582687646150589}, {"id": 85, "seek": 48876, "start": 488.76, "end": 497.0, "text": " My proposal for solving this problem let's introduce free taxi rides through the alignment", "tokens": [50364, 1222, 11494, 337, 12606, 341, 1154, 718, 311, 5366, 1737, 18984, 20773, 807, 264, 18515, 50776], "temperature": 0.0, "avg_logprob": -0.15270262956619263, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.10548931360244751}, {"id": 86, "seek": 48876, "start": 497.0, "end": 498.0, "text": " graph.", "tokens": [50776, 4295, 13, 50826], "temperature": 0.0, "avg_logprob": -0.15270262956619263, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.10548931360244751}, {"id": 87, "seek": 48876, "start": 498.0, "end": 508.15999999999997, "text": " Indeed, if we were able to start in the source and travel freely to the start of the concert", "tokens": [50826, 15061, 11, 498, 321, 645, 1075, 281, 722, 294, 264, 4009, 293, 3147, 16433, 281, 264, 722, 295, 264, 8543, 51334], "temperature": 0.0, "avg_logprob": -0.15270262956619263, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.10548931360244751}, {"id": 88, "seek": 48876, "start": 508.15999999999997, "end": 516.3199999999999, "text": " fragment and then take another free taxi ride from the end of concert fragment to the destination", "tokens": [51334, 26424, 293, 550, 747, 1071, 1737, 18984, 5077, 490, 264, 917, 295, 8543, 26424, 281, 264, 12236, 51742], "temperature": 0.0, "avg_logprob": -0.15270262956619263, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.10548931360244751}, {"id": 89, "seek": 51632, "start": 516.32, "end": 524.0400000000001, "text": " final note of the alignment graph, then we will be able to score these interesting segments", "tokens": [50364, 2572, 3637, 295, 264, 18515, 4295, 11, 550, 321, 486, 312, 1075, 281, 6175, 613, 1880, 19904, 50750], "temperature": 0.0, "avg_logprob": -0.15296721752778983, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.10900163650512695}, {"id": 90, "seek": 51632, "start": 524.0400000000001, "end": 532.12, "text": " by taking zero cost of taxi ride to the beginning of this fragment, then real cost of the alignment", "tokens": [50750, 538, 1940, 4018, 2063, 295, 18984, 5077, 281, 264, 2863, 295, 341, 26424, 11, 550, 957, 2063, 295, 264, 18515, 51154], "temperature": 0.0, "avg_logprob": -0.15296721752778983, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.10900163650512695}, {"id": 91, "seek": 51632, "start": 532.12, "end": 537.12, "text": " of the fragment and then plus another zero which is the cost of another taxi ride.", "tokens": [51154, 295, 264, 26424, 293, 550, 1804, 1071, 4018, 597, 307, 264, 2063, 295, 1071, 18984, 5077, 13, 51404], "temperature": 0.0, "avg_logprob": -0.15296721752778983, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.10900163650512695}, {"id": 92, "seek": 51632, "start": 537.12, "end": 541.0400000000001, "text": " You may ask how in the world we can take taxi ride through the alignment graph?", "tokens": [51404, 509, 815, 1029, 577, 294, 264, 1002, 321, 393, 747, 18984, 5077, 807, 264, 18515, 4295, 30, 51600], "temperature": 0.0, "avg_logprob": -0.15296721752778983, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.10900163650512695}, {"id": 93, "seek": 54104, "start": 541.68, "end": 547.8, "text": " The whole point of introducing this concept of Manhattan kind cities and traveling in", "tokens": [50396, 440, 1379, 935, 295, 15424, 341, 3410, 295, 23633, 733, 6486, 293, 9712, 294, 50702], "temperature": 0.0, "avg_logprob": -0.20425580622075679, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.051985930651426315}, {"id": 94, "seek": 54104, "start": 547.8, "end": 555.4, "text": " them is that we are free to build whatever Manhattan like grids for solving our biological", "tokens": [50702, 552, 307, 300, 321, 366, 1737, 281, 1322, 2035, 23633, 411, 677, 3742, 337, 12606, 527, 13910, 51082], "temperature": 0.0, "avg_logprob": -0.20425580622075679, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.051985930651426315}, {"id": 95, "seek": 54104, "start": 555.4, "end": 559.56, "text": " problems and in this case what is a free taxi ride?", "tokens": [51082, 2740, 293, 294, 341, 1389, 437, 307, 257, 1737, 18984, 5077, 30, 51290], "temperature": 0.0, "avg_logprob": -0.20425580622075679, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.051985930651426315}, {"id": 96, "seek": 54104, "start": 559.56, "end": 565.8399999999999, "text": " It's simply adding extra edges of weight zero to our alignment graph and since we are free", "tokens": [51290, 467, 311, 2935, 5127, 2857, 8819, 295, 3364, 4018, 281, 527, 18515, 4295, 293, 1670, 321, 366, 1737, 51604], "temperature": 0.0, "avg_logprob": -0.20425580622075679, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.051985930651426315}, {"id": 97, "seek": 54104, "start": 565.8399999999999, "end": 571.0, "text": " to build whatever Manhattan we want, we can of course, we are at liberty of introducing", "tokens": [51604, 281, 1322, 2035, 23633, 321, 528, 11, 321, 393, 295, 1164, 11, 321, 366, 412, 22849, 295, 15424, 51862], "temperature": 0.0, "avg_logprob": -0.20425580622075679, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.051985930651426315}, {"id": 98, "seek": 57100, "start": 571.0, "end": 572.32, "text": " this taxi ride.", "tokens": [50364, 341, 18984, 5077, 13, 50430], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 99, "seek": 57100, "start": 572.32, "end": 575.2, "text": " So let's see how our graph change.", "tokens": [50430, 407, 718, 311, 536, 577, 527, 4295, 1319, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 100, "seek": 57100, "start": 575.2, "end": 578.12, "text": " What we need to do to implement this free taxi ride?", "tokens": [50574, 708, 321, 643, 281, 360, 281, 4445, 341, 1737, 18984, 5077, 30, 50720], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 101, "seek": 57100, "start": 578.12, "end": 585.32, "text": " We need to add edges from the source to any other note and it will be roughly quadratic", "tokens": [50720, 492, 643, 281, 909, 8819, 490, 264, 4009, 281, 604, 661, 3637, 293, 309, 486, 312, 9810, 37262, 51080], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 102, "seek": 57100, "start": 585.32, "end": 587.2, "text": " number of edges.", "tokens": [51080, 1230, 295, 8819, 13, 51174], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 103, "seek": 57100, "start": 587.2, "end": 592.44, "text": " We also need to add edges from every note to the same once again quadratic number of", "tokens": [51174, 492, 611, 643, 281, 909, 8819, 490, 633, 3637, 281, 264, 912, 1564, 797, 37262, 1230, 295, 51436], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 104, "seek": 57100, "start": 592.44, "end": 597.92, "text": " edges so the number of edges in the graph remains quadratic and therefore our algorithm", "tokens": [51436, 8819, 370, 264, 1230, 295, 8819, 294, 264, 4295, 7023, 37262, 293, 4412, 527, 9284, 51710], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 105, "seek": 57100, "start": 597.92, "end": 600.36, "text": " will be fast.", "tokens": [51710, 486, 312, 2370, 13, 51832], "temperature": 0.0, "avg_logprob": -0.17240465749608408, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.007089813239872456}, {"id": 106, "seek": 60036, "start": 600.36, "end": 606.44, "text": " And in that how our dynamic programming currency change for the local alignment?", "tokens": [50364, 400, 294, 300, 577, 527, 8546, 9410, 13346, 1319, 337, 264, 2654, 18515, 30, 50668], "temperature": 0.0, "avg_logprob": -0.26093468737246384, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.015552587807178497}, {"id": 107, "seek": 60036, "start": 606.44, "end": 614.8000000000001, "text": " Before we had three possibilities corresponding to three ways to enter a note.", "tokens": [50668, 4546, 321, 632, 1045, 12178, 11760, 281, 1045, 2098, 281, 3242, 257, 3637, 13, 51086], "temperature": 0.0, "avg_logprob": -0.26093468737246384, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.015552587807178497}, {"id": 108, "seek": 60036, "start": 614.8000000000001, "end": 619.04, "text": " By vertical edge, by horizontal edge and by diagonal edge.", "tokens": [51086, 3146, 9429, 4691, 11, 538, 12750, 4691, 293, 538, 21539, 4691, 13, 51298], "temperature": 0.0, "avg_logprob": -0.26093468737246384, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.015552587807178497}, {"id": 109, "seek": 60036, "start": 619.04, "end": 623.6800000000001, "text": " Now there is one more possibility we can take a free taxi ride to the note.", "tokens": [51298, 823, 456, 307, 472, 544, 7959, 321, 393, 747, 257, 1737, 18984, 5077, 281, 264, 3637, 13, 51530], "temperature": 0.0, "avg_logprob": -0.26093468737246384, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.015552587807178497}, {"id": 110, "seek": 62368, "start": 623.68, "end": 631.52, "text": " So now there are four possibilities for entering every note which means that we need to add", "tokens": [50364, 407, 586, 456, 366, 1451, 12178, 337, 11104, 633, 3637, 597, 1355, 300, 321, 643, 281, 909, 50756], "temperature": 0.0, "avg_logprob": -0.17978085411919487, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.009780732914805412}, {"id": 111, "seek": 62368, "start": 631.52, "end": 639.4799999999999, "text": " the fourth term in this currency which is the weight of edge from zero zero to ij.", "tokens": [50756, 264, 6409, 1433, 294, 341, 13346, 597, 307, 264, 3364, 295, 4691, 490, 4018, 4018, 281, 741, 73, 13, 51154], "temperature": 0.0, "avg_logprob": -0.17978085411919487, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.009780732914805412}, {"id": 112, "seek": 62368, "start": 639.4799999999999, "end": 644.7199999999999, "text": " And the weight of this edge since our taxi rides are free is zero and that's the only", "tokens": [51154, 400, 264, 3364, 295, 341, 4691, 1670, 527, 18984, 20773, 366, 1737, 307, 4018, 293, 300, 311, 264, 787, 51416], "temperature": 0.0, "avg_logprob": -0.17978085411919487, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.009780732914805412}, {"id": 113, "seek": 64472, "start": 644.76, "end": 654.8000000000001, "text": " change that we need to implement to make our local alignment algorithms practical and fast.", "tokens": [50366, 1319, 300, 321, 643, 281, 4445, 281, 652, 527, 2654, 18515, 14642, 8496, 293, 2370, 13, 50868], "temperature": 0.0, "avg_logprob": -0.17357789553128755, "compression_ratio": 1.4308943089430894, "no_speech_prob": 0.8033134341239929}, {"id": 114, "seek": 64472, "start": 654.8000000000001, "end": 661.9200000000001, "text": " And we now move to the problem of defining adequate insertion and deletion penalties", "tokens": [50868, 400, 321, 586, 1286, 281, 264, 1154, 295, 17827, 20927, 8969, 313, 293, 1103, 302, 313, 35389, 51224], "temperature": 0.0, "avg_logprob": -0.17357789553128755, "compression_ratio": 1.4308943089430894, "no_speech_prob": 0.8033134341239929}, {"id": 115, "seek": 66192, "start": 661.92, "end": 663.4399999999999, "text": " in sequence alignment.", "tokens": [50364, 294, 8310, 18515, 13, 50440], "temperature": 0.0, "avg_logprob": -0.6376346179417202, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.6691659092903137}], "language": "en"}