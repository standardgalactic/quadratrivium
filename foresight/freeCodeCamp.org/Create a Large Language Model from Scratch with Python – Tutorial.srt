1
00:00:00,000 --> 00:00:04,000
Learn how to build your own large language model from scratch.

2
00:00:04,360 --> 00:00:09,840
This course goes into the data handling, math and transformers behind large language models.

3
00:00:09,960 --> 00:00:11,800
Elliot Arledge created this course.

4
00:00:12,040 --> 00:00:18,640
He will help you gain a deep understanding of how LLMs work and how they can be used in various applications.

5
00:00:19,040 --> 00:00:20,600
So let's get started.

6
00:00:21,000 --> 00:00:22,880
Welcome to Intro to Language Modeling.

7
00:00:22,920 --> 00:00:25,280
In this course, you're going to learn a lot of crazy stuff.

8
00:00:25,320 --> 00:00:27,080
Okay, I'm just going to give you a heads up.

9
00:00:27,120 --> 00:00:28,960
It's going to be a lot of crazy stuff we learn here.

10
00:00:29,240 --> 00:00:31,400
However, it will not be insanely hard.

11
00:00:31,640 --> 00:00:35,720
I don't expect you have any any experience in calculus or linear algebra.

12
00:00:36,640 --> 00:00:39,040
A lot of courses out there do assume that, but I will not.

13
00:00:39,920 --> 00:00:41,200
We're going to build up from square one.

14
00:00:41,320 --> 00:00:46,240
We're going to take baby steps when it comes to new fundamental concepts in math and machine learning.

15
00:00:46,600 --> 00:00:51,240
And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out.

16
00:00:51,920 --> 00:00:55,520
That way we don't take forever just taking baby steps through every little concept.

17
00:00:56,000 --> 00:00:58,560
This course is inspired by Andre Karpathy's.

18
00:00:59,400 --> 00:01:01,320
Building a GPT from scratch lecture.

19
00:01:02,240 --> 00:01:03,240
So shout out to him.

20
00:01:03,880 --> 00:01:09,680
And yeah, we don't assume you have any experience, maybe three months of Python experience.

21
00:01:10,480 --> 00:01:13,000
Just so the syntax is sort of familiar and you can.

22
00:01:14,080 --> 00:01:19,120
You're able to follow along that way, but no matter how smart you are, how quick you learn.

23
00:01:20,080 --> 00:01:25,240
The willingness to put in the hours is the most important because this is material that you won't normally come across.

24
00:01:26,200 --> 00:01:35,800
So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water.

25
00:01:36,240 --> 00:01:37,280
Water is very important.

26
00:01:37,760 --> 00:01:41,440
And yeah, hopefully you can make it to the end of this.

27
00:01:42,400 --> 00:01:43,040
You can do it.

28
00:01:44,280 --> 00:01:51,640
Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing.

29
00:01:52,280 --> 00:01:57,600
We'll be scaling the data to about 45 gigabytes for the entire training data set.

30
00:01:58,160 --> 00:02:03,560
So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45.

31
00:02:04,440 --> 00:02:09,240
So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine.

32
00:02:09,240 --> 00:02:16,640
You can just download a different data set and sort of follow the same data pipeline that I do in this video.

33
00:02:17,640 --> 00:02:21,040
Through the course, you may see me switch between Mac OS and Windows.

34
00:02:21,440 --> 00:02:26,640
The code still works all the same, both operating systems, and I'll be using a tool called SSH.

35
00:02:26,640 --> 00:02:42,640
It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook.

36
00:02:43,640 --> 00:02:49,640
So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording.

37
00:02:50,640 --> 00:02:53,640
So why am I creating this course?

38
00:02:53,640 --> 00:03:03,640
Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space.

39
00:03:03,640 --> 00:03:09,640
So I intend to build up from baby steps and then larger steps when things are fairly simple to work with.

40
00:03:10,640 --> 00:03:19,640
And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you.

41
00:03:19,640 --> 00:03:24,640
So with that being said, let's go ahead and jump in to the good stuff.

42
00:03:24,640 --> 00:03:29,640
So in order to develop this project step by step, we're going to use something called Jupyter notebooks.

43
00:03:29,640 --> 00:03:34,640
And you can sort of play with these in the Anaconda prompt or at least launch them from here.

44
00:03:34,640 --> 00:03:38,640
So Anaconda prompt is just great for anything machine learning related.

45
00:03:38,640 --> 00:03:40,640
So make sure to have this installed.

46
00:03:40,640 --> 00:03:47,640
I will link a video in the description so that you can sort of set this up and install it step by step guide in there.

47
00:03:47,640 --> 00:03:53,640
So we can do from this point is sort of just set up our project and initialize everything.

48
00:03:53,640 --> 00:04:03,640
So I'm going to do is just head over into my directory that I want to be Python testing.

49
00:04:03,640 --> 00:04:10,640
We're going to make a directory free code camp GPT course.

50
00:04:10,640 --> 00:04:14,640
And then from this point, we're going to go and make a virtual environment.

51
00:04:14,640 --> 00:04:26,640
So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around.

52
00:04:26,640 --> 00:04:29,640
And what the virtual environment does is it sort of separates that.

53
00:04:29,640 --> 00:04:34,640
So you have this isolated environment over here, and you can just play around with this however you want.

54
00:04:34,640 --> 00:04:43,640
And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system.

55
00:04:43,640 --> 00:04:46,640
When you're not in a virtual environment, if that makes sense.

56
00:04:46,640 --> 00:04:51,640
So we're going to go ahead and set that up right now by using Python dash M.

57
00:04:51,640 --> 00:04:55,640
And then we're going to go V and V for virtual V and V and then CUDA.

58
00:04:55,640 --> 00:05:06,640
So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs.

59
00:05:06,640 --> 00:05:09,640
GPUs are going to accelerate this a ton.

60
00:05:09,640 --> 00:05:13,640
And basically CUDA is just that little feature in the GPU that lets us do that.

61
00:05:13,640 --> 00:05:16,640
So we're going to make an environment called CUDA.

62
00:05:16,640 --> 00:05:17,640
I'm going to go and press enter.

63
00:05:17,640 --> 00:05:20,640
It's going to do that for us going to take a few seconds.

64
00:05:20,640 --> 00:05:23,640
So now that's done, we can go ahead and do CUDA.

65
00:05:23,640 --> 00:05:28,640
And we're just going to basically activate this environment so we can start developing in it.

66
00:05:28,640 --> 00:05:33,640
I'm going to go backslash, we're going to go scripts, and then activate.

67
00:05:33,640 --> 00:05:35,640
So now you can see it says CUDA base.

68
00:05:35,640 --> 00:05:39,640
So we're in CUDA and then secondary base.

69
00:05:39,640 --> 00:05:41,640
So it's going to prioritize CUDA.

70
00:05:41,640 --> 00:05:46,640
So from this point, we can actually start installing some stuff, some libraries here.

71
00:05:46,640 --> 00:05:53,640
So we can go pip three install Matt plot lib numpy.

72
00:05:54,640 --> 00:06:01,640
We're going to use p y l m z a l z m a.

73
00:06:01,640 --> 00:06:04,640
And then what are some other ones?

74
00:06:04,640 --> 00:06:07,640
We're going to do IPY kernel.

75
00:06:07,640 --> 00:06:14,640
This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks.

76
00:06:14,640 --> 00:06:16,640
So that's why that's important.

77
00:06:16,640 --> 00:06:19,640
And then just the actual Jupyter notebook feature.

78
00:06:19,640 --> 00:06:21,640
So go and press enter.

79
00:06:21,640 --> 00:06:22,640
Those are going to install.

80
00:06:22,640 --> 00:06:25,640
That's going to take a few seconds to do.

81
00:06:25,640 --> 00:06:33,640
So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm.

82
00:06:33,640 --> 00:06:37,640
And don't quote me on this, but I'm pretty sure it's based in C plus plus.

83
00:06:37,640 --> 00:06:40,640
So you actually need some build tools for this.

84
00:06:40,640 --> 00:06:45,640
And you can get that with visual studio build tools.

85
00:06:45,640 --> 00:06:50,640
So what you're you might see, you might see a little error and basically go to that website.

86
00:06:50,640 --> 00:06:51,640
You're going to get this right here.

87
00:06:51,640 --> 00:06:54,640
So just go ahead and download build tools.

88
00:06:54,640 --> 00:06:57,640
What's going to download here, you're going to click on that.

89
00:06:57,640 --> 00:06:59,640
It's going to, it's going to set up.

90
00:06:59,640 --> 00:07:05,640
And then you're going to go ahead and click continue.

91
00:07:05,640 --> 00:07:09,640
And then at this point, you can go ahead and click modify if you see this here.

92
00:07:09,640 --> 00:07:14,640
And then you might get to a little workloads section here.

93
00:07:14,640 --> 00:07:16,640
So once you're at workloads, that's good.

94
00:07:16,640 --> 00:07:21,640
What you're going to make sure is that you have these two checked off right here.

95
00:07:21,640 --> 00:07:23,640
Just make sure that you have these two.

96
00:07:23,640 --> 00:07:25,640
I'm not sure what desktop particularly does.

97
00:07:25,640 --> 00:07:32,640
It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects.

98
00:07:32,640 --> 00:07:35,640
So just get these two for now.

99
00:07:35,640 --> 00:07:36,640
That'll be good.

100
00:07:36,640 --> 00:07:40,640
And then you can click modify over here if you wanted to modify just like that.

101
00:07:40,640 --> 00:07:45,640
And then you should be good to rerun that command.

102
00:07:45,640 --> 00:07:57,640
So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch.

103
00:07:57,640 --> 00:07:59,640
We're not going to do it like this.

104
00:07:59,640 --> 00:08:07,640
What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch.

105
00:08:07,640 --> 00:08:12,640
So it's going to install the CUDA extension, which will allow us to utilize the GPU.

106
00:08:12,640 --> 00:08:14,640
So it's just this command right here.

107
00:08:14,640 --> 00:08:25,640
And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started.

108
00:08:25,640 --> 00:08:29,640
And then you'll be able to see this right here.

109
00:08:29,640 --> 00:08:35,640
So we have stable windows pip Python and CUDA 11.7 or 11.8.

110
00:08:35,640 --> 00:08:36,640
So I just clicked on this.

111
00:08:36,640 --> 00:08:43,640
And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch.

112
00:08:43,640 --> 00:08:49,640
And then with this index URL for the CUDA 11.8.

113
00:08:49,640 --> 00:08:53,640
So that's pretty much all we're doing there to install CUDA.

114
00:08:53,640 --> 00:08:54,640
That's part of our torch.

115
00:08:54,640 --> 00:08:58,640
So we can go ahead and click enter on this.

116
00:08:58,640 --> 00:08:59,640
So great.

117
00:08:59,640 --> 00:09:04,640
We've installed a lot of things, a lot of libraries, a lot of setup has been done already.

118
00:09:04,640 --> 00:09:08,640
What I want to check now is just to make sure that our Python version is what we want.

119
00:09:08,640 --> 00:09:11,640
So I've done version 3.10.9.

120
00:09:11,640 --> 00:09:12,640
That's great.

121
00:09:12,640 --> 00:09:17,640
If you're between 3.9, 3.10, 3.11, that's perfect.

122
00:09:17,640 --> 00:09:20,640
So if you're in between those, it should be fine.

123
00:09:20,640 --> 00:09:24,640
At this point, we can just jump right into our Jupyter Notebook.

124
00:09:24,640 --> 00:09:27,640
So the command for that is just Jupyter Notebook.

125
00:09:27,640 --> 00:09:28,640
It's about like that.

126
00:09:28,640 --> 00:09:31,640
Click enter.

127
00:09:31,640 --> 00:09:34,640
It's going to send us into here.

128
00:09:35,640 --> 00:09:42,640
And I've created this little biogram.ipynb here in my VS Code.

129
00:09:42,640 --> 00:09:46,640
So pretty much you need to actually type some stuff in it.

130
00:09:46,640 --> 00:09:52,640
And you need to make sure that it has the ipynb extension or else it won't work.

131
00:09:52,640 --> 00:09:58,640
So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason.

132
00:09:58,640 --> 00:10:00,640
So just make sure you type some stuff in it.

133
00:10:00,640 --> 00:10:01,640
Open that in VS Code.

134
00:10:01,640 --> 00:10:05,640
Type, I don't know, a equals 3 or str equals banana.

135
00:10:05,640 --> 00:10:07,640
I don't care.

136
00:10:07,640 --> 00:10:12,640
At this point, let's go ahead and pop into here.

137
00:10:12,640 --> 00:10:14,640
So this is what our notebook is going to look like.

138
00:10:14,640 --> 00:10:19,640
And we're going to be working with this quite a bit throughout this course.

139
00:10:19,640 --> 00:10:27,640
So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook.

140
00:10:27,640 --> 00:10:33,640
And make sure that we can interact with it from this kernel rather than just through the command prompt.

141
00:10:33,640 --> 00:10:35,640
So we're going to go ahead and check here.

142
00:10:35,640 --> 00:10:37,640
And I have a virtual environment here.

143
00:10:37,640 --> 00:10:42,640
You may not, but all we're going to do is basically go into here.

144
00:10:42,640 --> 00:10:44,640
We're going to end this.

145
00:10:44,640 --> 00:10:56,640
And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install.

146
00:10:57,640 --> 00:11:04,640
User, you'll see why we're doing this in the second user name equals CUDA.

147
00:11:04,640 --> 00:11:07,640
This is from the virtual environment we initialized before.

148
00:11:07,640 --> 00:11:09,640
So that's the name of the virtual environment.

149
00:11:09,640 --> 00:11:20,640
And then the display name, how it's actually going to look in the terminal is going to be display name.

150
00:11:21,640 --> 00:11:28,640
We'll just call it CUDA GPT.

151
00:11:28,640 --> 00:11:29,640
I don't know.

152
00:11:29,640 --> 00:11:30,640
That sounds like a cool name.

153
00:11:30,640 --> 00:11:32,640
And I'm going to press enter.

154
00:11:32,640 --> 00:11:37,640
It's going to make this environment for us great installed.

155
00:11:37,640 --> 00:11:38,640
Good.

156
00:11:38,640 --> 00:11:45,640
So we can go and run our notebook again and we'll see if this changes.

157
00:11:45,640 --> 00:11:52,640
So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT.

158
00:11:52,640 --> 00:11:54,640
Let's click that.

159
00:11:54,640 --> 00:11:55,640
Sweet.

160
00:11:55,640 --> 00:12:01,640
So now we can actually start doing more and just sort of experimenting with how the notebooks

161
00:12:01,640 --> 00:12:08,640
work and actually how we can build up this bi-gram model and sort of learning how language

162
00:12:08,640 --> 00:12:10,640
models work from scratch.

163
00:12:10,640 --> 00:12:11,640
So let's go ahead and do that.

164
00:12:11,640 --> 00:12:19,640
Now that we jump into this actual code here, what I want to do is delete all of these.

165
00:12:19,640 --> 00:12:20,640
Good.

166
00:12:20,640 --> 00:12:25,640
So now what I'm going to do is just get a small little data set, just very small for us to

167
00:12:25,640 --> 00:12:30,640
work with that we can sort of try to make a bi-gram out of, something very small.

168
00:12:30,640 --> 00:12:36,640
So what we can do is go to this website called Project Gutenberg and they basically just

169
00:12:36,640 --> 00:12:41,640
have a bunch of free books that are licensed under Creative Commons.

170
00:12:41,640 --> 00:12:45,640
So we can use all of these for free.

171
00:12:45,640 --> 00:12:49,640
So let's use the Wizard of Oz.

172
00:12:49,640 --> 00:12:53,640
Put it at the end of Wizard of Oz.

173
00:12:53,640 --> 00:12:55,640
Great.

174
00:12:55,640 --> 00:12:58,640
So what we're going to want to do is just click on plain text here.

175
00:12:58,640 --> 00:12:59,640
Great.

176
00:12:59,640 --> 00:13:09,640
So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore

177
00:13:09,640 --> 00:13:11,640
of underscore Oz.

178
00:13:11,640 --> 00:13:12,640
Good.

179
00:13:12,640 --> 00:13:24,640
So now what I'm going to do is we should probably drag this into, we should drag this into our

180
00:13:24,640 --> 00:13:25,640
folder here.

181
00:13:25,640 --> 00:13:30,640
I'm just going to pop that into there.

182
00:13:30,640 --> 00:13:31,640
Good stuff.

183
00:13:31,640 --> 00:13:33,640
Did that work?

184
00:13:33,640 --> 00:13:34,640
Sweet.

185
00:13:34,640 --> 00:13:37,640
So now we have our Wizard of Oz text in here, we can open that.

186
00:13:37,640 --> 00:13:41,640
What we can do is start of this book.

187
00:13:41,640 --> 00:13:42,640
Okay.

188
00:13:42,640 --> 00:13:49,640
So we can go ahead and go down to when it starts.

189
00:13:49,640 --> 00:13:58,640
Sweet.

190
00:13:58,640 --> 00:14:01,640
So maybe we'll just cut it here.

191
00:14:01,640 --> 00:14:03,640
That'd be a good place to start.

192
00:14:03,640 --> 00:14:04,640
Just like that.

193
00:14:04,640 --> 00:14:07,640
I'll put a few spaces.

194
00:14:07,640 --> 00:14:08,640
Good.

195
00:14:08,640 --> 00:14:10,640
So now we have this book.

196
00:14:10,640 --> 00:14:17,640
We go to the bottom here just to get rid of some of this other licensing stuff, which

197
00:14:17,640 --> 00:14:23,640
might get in the way with our predictions in the context of the entire book.

198
00:14:23,640 --> 00:14:26,640
So let's just go down to when that starts.

199
00:14:26,640 --> 00:14:29,640
End of the book.

200
00:14:29,640 --> 00:14:31,640
Okay.

201
00:14:31,640 --> 00:14:37,640
So we've gotten all that.

202
00:14:37,640 --> 00:14:38,640
That is done.

203
00:14:38,640 --> 00:14:41,640
Get rid of the illustration there.

204
00:14:41,640 --> 00:14:42,640
Perfect.

205
00:14:42,640 --> 00:14:45,640
So now we have this Wizard of Oz text that we can work with.

206
00:14:45,640 --> 00:14:47,640
Let's close that up.

207
00:14:47,640 --> 00:14:48,640
233 kilobytes.

208
00:14:48,640 --> 00:14:49,640
Awesome.

209
00:14:49,640 --> 00:14:50,640
Very small size.

210
00:14:50,640 --> 00:14:51,640
We can work with this.

211
00:14:51,640 --> 00:14:52,640
This is great.

212
00:14:52,640 --> 00:14:54,640
So we have this wizard of Oz dot txt file.

213
00:14:54,640 --> 00:14:55,640
And what are we going to do with that?

214
00:14:55,640 --> 00:15:00,640
Well, we're going to try to train a transformer or at least a background language model on

215
00:15:00,640 --> 00:15:01,640
this text.

216
00:15:01,640 --> 00:15:05,640
So in order to do that, we need to sort of learn how to manage this text file, how to

217
00:15:05,640 --> 00:15:07,640
open it, et cetera.

218
00:15:07,640 --> 00:15:15,640
So we're going to go ahead and open this and do wizard of Oz.

219
00:15:15,640 --> 00:15:16,640
Like that.

220
00:15:16,640 --> 00:15:17,640
And we're going to open in read mode.

221
00:15:17,640 --> 00:15:22,640
And then we're going to use the encoding utf 8 just like that.

222
00:15:22,640 --> 00:15:25,640
So this is the file mode that you're going to open in.

223
00:15:25,640 --> 00:15:26,640
There's read mode.

224
00:15:26,640 --> 00:15:27,640
There's write mode.

225
00:15:27,640 --> 00:15:28,640
There's read binary.

226
00:15:28,640 --> 00:15:29,640
There's write binary.

227
00:15:29,640 --> 00:15:36,640
And those are really the only ones we're going to be worrying about for this video.

228
00:15:36,640 --> 00:15:40,640
The other ones you can look into in your spare time if you'd like to.

229
00:15:40,640 --> 00:15:43,640
I've already seen using those four for now.

230
00:15:43,640 --> 00:15:47,640
And then the encoding is just what type of character coding are we using?

231
00:15:47,640 --> 00:15:48,640
That's pretty much it.

232
00:15:48,640 --> 00:15:51,640
We can just open this as F short for file.

233
00:15:51,640 --> 00:15:54,640
I'm going to go text equals f dot read.

234
00:15:55,640 --> 00:15:58,640
I'm going to read this file stored in a string variable.

235
00:15:58,640 --> 00:16:01,640
And then we can print some stuff about it.

236
00:16:01,640 --> 00:16:05,640
So we can go print the length of this text.

237
00:16:05,640 --> 00:16:06,640
Run that.

238
00:16:06,640 --> 00:16:10,640
We get the length of the text.

239
00:16:10,640 --> 00:16:16,640
We could print the first 200 characters of the text.

240
00:16:16,640 --> 00:16:17,640
Sure.

241
00:16:17,640 --> 00:16:19,640
So you have the first 200 characters.

242
00:16:19,640 --> 00:16:20,640
Great.

243
00:16:21,640 --> 00:16:24,640
So now we know how to, you know, just play with characters.

244
00:16:24,640 --> 00:16:28,640
At least just see what the characters actually look like.

245
00:16:28,640 --> 00:16:33,640
So now we can do a little bit more from this point, which is going to be encoders.

246
00:16:33,640 --> 00:16:40,640
And before we get into that, what I'm going to do is put these into a little vocabulary

247
00:16:40,640 --> 00:16:42,640
list that we can work with.

248
00:16:42,640 --> 00:16:48,640
So all I'm going to do is I'm going to say we're going to make a charge variable.

249
00:16:48,640 --> 00:16:54,640
So the charge is going to be all the charge or all the characters in this text piece.

250
00:16:54,640 --> 00:17:08,640
So we're going to make a sorted set of text here, and we're going to just print out charge.

251
00:17:08,640 --> 00:17:09,640
So look at that.

252
00:17:09,640 --> 00:17:13,640
We have a giant array of all these characters.

253
00:17:13,640 --> 00:17:19,640
So now we can, what we can do is we can use something called a tokenizer and a tokenizer

254
00:17:19,640 --> 00:17:22,640
consists of an encoder and a decoder.

255
00:17:22,640 --> 00:17:28,640
What an encoder does is it's actually going to convert each character or sorry, each element

256
00:17:28,640 --> 00:17:31,640
of this array to an integer.

257
00:17:31,640 --> 00:17:34,640
So maybe this would be a zero.

258
00:17:34,640 --> 00:17:36,640
This would be a one, right?

259
00:17:36,640 --> 00:17:42,640
So a new, a new line or an enter would be a zero, a space would be a one exclamation

260
00:17:42,640 --> 00:17:44,640
mark would be a two, et cetera, right?

261
00:17:44,640 --> 00:17:46,640
All the way to the length of them.

262
00:17:46,640 --> 00:17:51,640
And then what we could do is we could even, we could even print the length of these characters.

263
00:17:51,640 --> 00:17:53,640
So you can see how many there actually are.

264
00:17:53,640 --> 00:17:58,640
So there's 81 characters in the entire, in the entire Wizard of Oz book.

265
00:17:58,640 --> 00:18:02,640
So I've written some code here that is going to do that job for us, the job of tokenizers.

266
00:18:02,640 --> 00:18:07,640
So what we do is we just use a little generator, some generator for loops here,

267
00:18:07,640 --> 00:18:13,640
a generator for loops rather, and we make a little mapping from strings to integers

268
00:18:13,640 --> 00:18:16,640
and integers to strings, given the vocabulary.

269
00:18:16,640 --> 00:18:19,640
So we just enumerate through each of these.

270
00:18:19,640 --> 00:18:24,640
We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right?

271
00:18:24,640 --> 00:18:26,640
That's basically all we're doing here.

272
00:18:26,640 --> 00:18:28,640
And we have an encoder and a decoder.

273
00:18:28,640 --> 00:18:34,640
So let's say we wanted to convert the string hello to integers.

274
00:18:34,640 --> 00:18:40,640
So we go encode, and we could do hello, just like that.

275
00:18:40,640 --> 00:18:45,640
And then we could go ahead and print this out.

276
00:18:45,640 --> 00:18:46,640
Perfect.

277
00:18:46,640 --> 00:18:48,640
Let's go ahead and run that.

278
00:18:48,640 --> 00:18:49,640
Boom.

279
00:18:49,640 --> 00:18:53,640
So now we have a conversion from characters to integers.

280
00:18:53,640 --> 00:18:59,640
And then if we wanted to maybe convert this back, so decode it,

281
00:18:59,640 --> 00:19:05,640
sort this in a little, maybe decoded hello equals that.

282
00:19:05,640 --> 00:19:13,640
And then we could go or encoded rather encoded hello.

283
00:19:13,640 --> 00:19:18,640
And then we could go decoded.

284
00:19:18,640 --> 00:19:24,640
Hello is equal to we go decode and we can use the encoded hello.

285
00:19:24,640 --> 00:19:28,640
So we're going to go ahead and encode this into integers.

286
00:19:28,640 --> 00:19:33,640
And then we're going to decode the integers back to a character format.

287
00:19:33,640 --> 00:19:36,640
So let's go ahead and print that out.

288
00:19:36,640 --> 00:19:41,640
We're going to go ahead and print the decoded hello.

289
00:19:41,640 --> 00:19:42,640
Perfect.

290
00:19:42,640 --> 00:19:44,640
So now we get that.

291
00:19:44,640 --> 00:19:48,640
So I'm going to fill you in on a little background information about these tokenizers.

292
00:19:48,640 --> 00:19:53,640
So right now we're using the character level tokenizer, which takes basically each character

293
00:19:53,640 --> 00:19:56,640
and converts it to an integer equivalent.

294
00:19:56,640 --> 00:20:03,640
So we have a very small vocabulary and a very large amount of tokens to convert.

295
00:20:03,640 --> 00:20:08,640
So if we have 40,000 individual characters, it means we have a small vocabulary to work

296
00:20:08,640 --> 00:20:12,640
with, but a lot of characters to encode and decode, right?

297
00:20:12,640 --> 00:20:19,640
If we have, if we work with maybe a word level tokenizer, that means we have a ton, like

298
00:20:19,640 --> 00:20:24,640
every single word in the English language, I mean, if you're working with multiple languages,

299
00:20:24,640 --> 00:20:30,640
this could be like, you know, a lot, very large amount of tokens.

300
00:20:30,640 --> 00:20:35,640
So you're going to have like maybe millions or billions or trillions if you're, if you're

301
00:20:35,640 --> 00:20:36,640
doing something weird.

302
00:20:36,640 --> 00:20:42,640
But in that case, you're going to have a way smaller set to work with.

303
00:20:42,640 --> 00:20:48,640
So you're going to have very large vocabulary, but a very small amount to encode and decode.

304
00:20:48,640 --> 00:20:54,640
So if you have a subword tokenizer, that means you're going to be somewhere in between a character

305
00:20:54,640 --> 00:20:57,640
level and a word level tokenizer, if that makes sense.

306
00:20:57,640 --> 00:21:02,640
So in the context of language models, it's really important that we're efficient with our data

307
00:21:02,640 --> 00:21:05,640
and just having a giant string might not work the best.

308
00:21:05,640 --> 00:21:10,640
And we're going to be using a machine learning framework called pi torch or torch.

309
00:21:10,640 --> 00:21:13,640
So I've imported this right here.

310
00:21:14,640 --> 00:21:21,640
And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well.

311
00:21:21,640 --> 00:21:28,640
A lot of the linear algebra, which involves a type of data structure called tensors.

312
00:21:28,640 --> 00:21:30,640
So tensors are pretty much matrices.

313
00:21:30,640 --> 00:21:32,640
If you're not familiar with those, that's fine.

314
00:21:32,640 --> 00:21:34,640
We'll go over them more in the course.

315
00:21:34,640 --> 00:21:41,640
But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with.

316
00:21:41,640 --> 00:21:43,640
So I'm going to go ahead and delete these here.

317
00:21:43,640 --> 00:21:49,640
And all we can do is just make our data element.

318
00:21:49,640 --> 00:21:53,640
We could this is going to be the entire text data of the entire Wizard of Oz.

319
00:21:53,640 --> 00:22:03,640
So we could go ahead and make this data equals and we're going to go torch tensor.

320
00:22:03,640 --> 00:22:06,640
And then we're going to go and code.

321
00:22:06,640 --> 00:22:08,640
We're going to put the text inside of that.

322
00:22:08,640 --> 00:22:11,640
So we're going to go ahead and encode this text right here.

323
00:22:11,640 --> 00:22:24,640
And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long.

324
00:22:24,640 --> 00:22:30,640
This basically means we're just going to have this as a super long sequence of integers.

325
00:22:30,640 --> 00:22:37,640
And yeah, let's go see what we can do with this torch tensor element right here.

326
00:22:37,640 --> 00:22:44,640
So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data.

327
00:22:44,640 --> 00:22:48,640
So it's pretty much the same thing in terms of working with arrays.

328
00:22:48,640 --> 00:22:59,640
It's just a different type of data structure in the context of pi torch sort of easier to work within that way.

329
00:22:59,640 --> 00:23:11,640
Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot.

330
00:23:11,640 --> 00:23:16,640
But we're going to go over some of this stuff later in the course just about how to do all this math.

331
00:23:16,640 --> 00:23:28,640
We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff.

332
00:23:28,640 --> 00:23:33,640
So next I'm going to talk about is something called validation and training splits.

333
00:23:33,640 --> 00:23:40,640
So why don't we just, you know, use the entire text document and only train on that entire text corpus?

334
00:23:40,640 --> 00:23:41,640
Why don't we train on that?

335
00:23:41,640 --> 00:23:47,640
Well, the reason we actually split into training and validation sets, I'm going to show you right here.

336
00:23:47,640 --> 00:23:49,640
So we have this giant text corpus.

337
00:23:49,640 --> 00:23:50,640
It's a super long text file.

338
00:23:50,640 --> 00:23:54,640
Think of it as a, you know, an essay, but a lot of pages.

339
00:23:54,640 --> 00:24:00,640
So this is our entire corpus and we make our training set, you know, 80% of it.

340
00:24:00,640 --> 00:24:01,640
So maybe this much.

341
00:24:01,640 --> 00:24:05,640
And then the other validation is this 20% right here.

342
00:24:05,640 --> 00:24:06,640
Okay.

343
00:24:06,640 --> 00:24:17,640
So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out.

344
00:24:17,640 --> 00:24:19,640
It would have it in the entire thing memorized.

345
00:24:19,640 --> 00:24:21,640
It wouldn't really get anything useful out of that.

346
00:24:21,640 --> 00:24:24,640
You would only know this document.

347
00:24:24,640 --> 00:24:29,640
But what the purpose of language modeling is, is to generate text that's like the training data.

348
00:24:29,640 --> 00:24:32,640
And this is exactly why we put into splits.

349
00:24:32,640 --> 00:24:40,640
So if we, if we run our training split right here, it's only going to know 80% of that entire corpus.

350
00:24:40,640 --> 00:24:44,640
And it's only going to generate on that 80% instead of the entire thing.

351
00:24:44,640 --> 00:24:49,640
And then we have our other 20%, which only knows 20% of the entire corpus.

352
00:24:49,640 --> 00:24:56,640
So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document.

353
00:24:56,640 --> 00:25:00,640
We're trying to generate text that's like the document.

354
00:25:00,640 --> 00:25:08,640
Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare.

355
00:25:08,640 --> 00:25:15,640
And the point is to generate Shakespearean like text, but not exactly what it looked like.

356
00:25:15,640 --> 00:25:21,640
Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right?

357
00:25:21,640 --> 00:25:23,640
We're trying to generate text that's like it.

358
00:25:23,640 --> 00:25:29,640
So that's the entire reason, or at least that's most of the reason why we use train and vowel splits.

359
00:25:29,640 --> 00:25:33,640
So you might be wondering, you know, like, why is this even called the bi-gram language model?

360
00:25:33,640 --> 00:25:36,640
I'm actually going to show you how that works right now.

361
00:25:36,640 --> 00:25:40,640
So if we go back to our whiteboard here, I've drawn a little sketch.

362
00:25:40,640 --> 00:25:47,640
So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now.

363
00:25:47,640 --> 00:25:49,640
We're just working with characters.

364
00:25:49,640 --> 00:25:52,640
Pretty much we have two, right?

365
00:25:52,640 --> 00:25:55,640
So by means to the by prefix means two.

366
00:25:55,640 --> 00:25:58,640
So we're going to, we're going to have a bi-gram.

367
00:25:58,640 --> 00:26:03,640
So given maybe, I mean, there's nothing before an H in this content.

368
00:26:03,640 --> 00:26:08,640
So we just assume that's the start of content, and then that's going to point to an H.

369
00:26:08,640 --> 00:26:12,640
So H is the most likely to come after the start.

370
00:26:12,640 --> 00:26:19,640
And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L,

371
00:26:19,640 --> 00:26:22,640
we're going to have another L, and then L leads to O, right?

372
00:26:22,640 --> 00:26:28,640
So maybe there's going to be some probabilities associated with these.

373
00:26:28,640 --> 00:26:31,640
So that's pretty much how it's how it's going to predict right now.

374
00:26:31,640 --> 00:26:35,640
It's only going to consider the previous character to predict the next.

375
00:26:35,640 --> 00:26:38,640
So we have given this one, we predict the next.

376
00:26:38,640 --> 00:26:41,640
So there's two, which is why it's called bi-gram language model.

377
00:26:41,640 --> 00:26:48,640
So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want,

378
00:26:48,640 --> 00:26:55,640
how we can actually implement this into a neural network, an artificial neural network, and train it.

379
00:26:55,640 --> 00:27:04,640
So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here,

380
00:27:04,640 --> 00:27:11,640
just a small snippet, and we're going to make some predictions and we're going to make some targets out of that.

381
00:27:11,640 --> 00:27:18,640
So our block size is just a bunch of encoded characters or integers that we have predictions and targets.

382
00:27:18,640 --> 00:27:24,640
So let's say we take a small little size of maybe block size of five, okay?

383
00:27:24,640 --> 00:27:30,640
So we have this tiny little tensor of five integers and these are our predictions.

384
00:27:30,640 --> 00:27:42,640
So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one.

385
00:27:42,640 --> 00:27:48,640
So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside.

386
00:27:48,640 --> 00:27:56,640
So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one.

387
00:27:56,640 --> 00:27:59,640
So we're going to be accessing the same indices.

388
00:27:59,640 --> 00:28:03,640
So at index zero, it's going to be five, index zero is going to be 67, right?

389
00:28:03,640 --> 00:28:07,640
So 67 is following five in the bi-gram language model.

390
00:28:07,640 --> 00:28:10,640
So that's pretty much all we do.

391
00:28:10,640 --> 00:28:20,640
So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target.

392
00:28:20,640 --> 00:28:25,640
And then we can optimize for reducing that error.

393
00:28:25,640 --> 00:28:39,640
So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here.

394
00:28:39,640 --> 00:28:43,640
We would take a little snippet random.

395
00:28:43,640 --> 00:28:58,640
It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size.

396
00:28:58,640 --> 00:29:03,640
So five.

397
00:29:03,640 --> 00:29:06,640
Ignore my terrible writing again.

398
00:29:06,640 --> 00:29:22,640
And then this one would just be it would just be one up to block size or five plus one.

399
00:29:22,640 --> 00:29:24,640
So we'll be up to six, right?

400
00:29:24,640 --> 00:29:26,640
And that's that's pretty much all we do.

401
00:29:26,640 --> 00:29:28,640
This is exactly what it's going to look like in the code.

402
00:29:28,640 --> 00:29:33,640
So I've written some code here that does exactly what we just talked about in Python.

403
00:29:33,640 --> 00:29:41,640
So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger.

404
00:29:41,640 --> 00:29:51,640
And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one.

405
00:29:51,640 --> 00:29:56,640
So we make it position one up to block size plus one little offset there.

406
00:29:56,640 --> 00:30:08,640
And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are.

407
00:30:08,640 --> 00:30:12,640
So this is what this looks like in Python, right, we can do predictions.

408
00:30:12,640 --> 00:30:15,640
But this isn't really scalable yet.

409
00:30:15,640 --> 00:30:18,640
This is sequential right sequential.

410
00:30:18,640 --> 00:30:25,640
It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly.

411
00:30:25,640 --> 00:30:30,640
That only happens sequentially it's this one and this task and this task and this task, right.

412
00:30:30,640 --> 00:30:37,640
But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel.

413
00:30:37,640 --> 00:30:52,640
So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them.

414
00:30:52,640 --> 00:31:05,640
So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot.

415
00:31:05,640 --> 00:31:08,640
So I'm going to illustrate that for you right now.

416
00:31:08,640 --> 00:31:10,640
So let's just say we have a block.

417
00:31:10,640 --> 00:31:12,640
Okay, block looks like this.

418
00:31:12,640 --> 00:31:19,640
And we have some we have some integers in between here.

419
00:31:19,640 --> 00:31:22,640
So this is a block.

420
00:31:22,640 --> 00:31:24,640
Okay.

421
00:31:24,640 --> 00:31:28,640
Now, if we want to make multiple of these, we're just going to stack them.

422
00:31:28,640 --> 00:31:32,640
So we're going to make another one.

423
00:31:32,640 --> 00:31:35,640
Another one.

424
00:31:35,640 --> 00:31:36,640
Another one.

425
00:31:36,640 --> 00:31:38,640
So let's say we have four batches.

426
00:31:38,640 --> 00:31:39,640
Okay.

427
00:31:39,640 --> 00:31:40,640
Or sorry, four blocks.

428
00:31:40,640 --> 00:31:45,640
So we have four different blocks that are just stacked on top of each other.

429
00:31:45,640 --> 00:31:50,640
And we can represent this as a new hyper parameter called batch size.

430
00:31:50,640 --> 00:31:55,640
This is going to tell us how many of these sequences can we actually process in parallel.

431
00:31:55,640 --> 00:32:03,640
So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time.

432
00:32:03,640 --> 00:32:07,640
So this is a really good way to scale language models.

433
00:32:07,640 --> 00:32:12,640
And without these, you can't really expect any fast training or good performance at all.

434
00:32:12,640 --> 00:32:19,640
So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process.

435
00:32:19,640 --> 00:32:24,640
And we can, it just takes one line to do this actually.

436
00:32:24,640 --> 00:32:37,640
So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation.

437
00:32:38,640 --> 00:32:44,640
And if it's available, like it says it's available, we'll set the device to CUDA else CPU.

438
00:32:44,640 --> 00:32:47,640
So we're going to go and print out the device here.

439
00:32:47,640 --> 00:32:50,640
So that's going to run and we get CUDA.

440
00:32:50,640 --> 00:32:55,640
So that means we can use the GPU for a lot of our processing here.

441
00:32:55,640 --> 00:33:02,640
And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size.

442
00:33:02,640 --> 00:33:07,640
And then we're going to use batch size, which is how many blocks we're doing in parallel.

443
00:33:07,640 --> 00:33:10,640
And we're just going to make this four for now.

444
00:33:10,640 --> 00:33:14,640
So these are our two hyper parameters that are very, very important for training.

445
00:33:14,640 --> 00:33:28,640
And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it.

446
00:33:28,640 --> 00:33:39,640
And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first.

447
00:33:39,640 --> 00:33:43,640
It'll shut down and you just go up Jupyter notebook again and then enter.

448
00:33:43,640 --> 00:33:52,640
And then after this is done, you should be able to just restart that and it will work.

449
00:33:52,640 --> 00:33:53,640
Hopefully.

450
00:33:53,640 --> 00:33:55,640
There we go.

451
00:33:55,640 --> 00:34:00,640
So I go in restart and clear outputs.

452
00:34:00,640 --> 00:34:02,640
And we can run that.

453
00:34:02,640 --> 00:34:03,640
See, we get boo.

454
00:34:03,640 --> 00:34:05,640
So awesome.

455
00:34:05,640 --> 00:34:10,640
Now, let's try to do some actual cool pie torch stop.

456
00:34:10,640 --> 00:34:14,640
So we're going to go in and import torch here.

457
00:34:14,640 --> 00:34:19,640
And then let's go ahead and try this random feature.

458
00:34:19,640 --> 00:34:21,640
So you go random.

459
00:34:21,640 --> 00:34:24,640
We'll do equals torch dot random.

460
00:34:24,640 --> 00:34:30,640
And then let's say we go minus 100 to 100.

461
00:34:30,640 --> 00:34:34,640
And then in brackets, we go six, just like that.

462
00:34:34,640 --> 00:34:40,640
So if we want to print this out here, or we could just go random like that.

463
00:34:40,640 --> 00:34:42,640
Run this block first.

464
00:34:42,640 --> 00:34:43,640
Good.

465
00:34:43,640 --> 00:34:44,640
And boom.

466
00:34:44,640 --> 00:34:47,640
So we get a tensor type.

467
00:34:47,640 --> 00:34:51,640
And all these numbers are we have we have six of them.

468
00:34:51,640 --> 00:34:53,640
So 123456.

469
00:34:53,640 --> 00:34:56,640
And they're between negative 100 and 100.

470
00:34:56,640 --> 00:35:02,640
So we're going to have to keep this in mind right here when we're getting our random

471
00:35:02,640 --> 00:35:05,640
batches from this giant text corpus.

472
00:35:05,640 --> 00:35:07,640
So let's try out a new one.

473
00:35:07,640 --> 00:35:09,640
Let's just try.

474
00:35:09,640 --> 00:35:11,640
We can make we can make tensors.

475
00:35:11,640 --> 00:35:12,640
We've done this before.

476
00:35:12,640 --> 00:35:17,640
So you do tensor equals torch dot tensor.

477
00:35:17,640 --> 00:35:30,640
So if you go 0.1, 1.2, here, I'll just copy and paste one right here.

478
00:35:30,640 --> 00:35:32,640
So we do this.

479
00:35:32,640 --> 00:35:38,640
And we can just do tensor and we'll get exactly this.

480
00:35:38,640 --> 00:35:45,640
So we get a three by two matrix.

481
00:35:45,640 --> 00:35:48,640
Now we're going to try a different one called zeros.

482
00:35:48,640 --> 00:35:51,640
So zeros is just torch dot zeros.

483
00:35:51,640 --> 00:35:56,640
And then inside of here, we could just do the dimensions or the shape of this.

484
00:35:56,640 --> 00:36:00,640
So two by three, and then we could just do zeros.

485
00:36:00,640 --> 00:36:02,640
And then go ahead and run that.

486
00:36:02,640 --> 00:36:06,640
So we get a two by three of zeros.

487
00:36:06,640 --> 00:36:10,640
And these are all floating point numbers, by the way.

488
00:36:10,640 --> 00:36:11,640
Maybe we could try ones.

489
00:36:11,640 --> 00:36:13,640
Now I know ones is pretty fun ones.

490
00:36:13,640 --> 00:36:17,640
So we both torch dot ones.

491
00:36:17,640 --> 00:36:19,640
It's pretty much the same as zeros.

492
00:36:19,640 --> 00:36:25,640
We could just do like maybe three by four and then print that ones out.

493
00:36:25,640 --> 00:36:28,640
So we have a three by four of ones.

494
00:36:28,640 --> 00:36:29,640
Sweet.

495
00:36:29,640 --> 00:36:37,640
So what if we do input equals torch dot empty.

496
00:36:37,640 --> 00:36:45,640
We can make this two by three.

497
00:36:45,640 --> 00:36:48,640
So these are interesting.

498
00:36:48,640 --> 00:36:55,640
These are pretty much a bunch of very either very large or very small numbers.

499
00:36:55,640 --> 00:36:58,640
I haven't particularly found a use case for this yet,

500
00:36:58,640 --> 00:37:01,640
but just another feature that PyTorch has.

501
00:37:01,640 --> 00:37:03,640
We have a range.

502
00:37:03,640 --> 00:37:09,640
So we go arrange equals torch dot arrange.

503
00:37:09,640 --> 00:37:14,640
I could do like five, for example, just do range.

504
00:37:14,640 --> 00:37:22,640
So now we have a tensor just sorted zero or rather starting at zero up to four.

505
00:37:22,640 --> 00:37:27,640
So five, just just like that.

506
00:37:27,640 --> 00:37:36,640
Line space equals torch dot line line space.

507
00:37:36,640 --> 00:37:44,640
Spelling is weird to three, 10, and then steps, for example, equals five.

508
00:37:44,640 --> 00:37:47,640
So it makes sense in a second here, go run.

509
00:37:47,640 --> 00:37:51,640
And we got a line space of steps equals five.

510
00:37:51,640 --> 00:37:54,640
So we have five different ones, boom, boom, boom, boom, boom.

511
00:37:54,640 --> 00:37:56,640
And we go all the way from three to 10.

512
00:37:56,640 --> 00:38:03,640
So pretty much getting all of the constant increments from three all the way up to 10 over five steps.

513
00:38:03,640 --> 00:38:07,640
So you're doing, you're basically adding the same amount every time.

514
00:38:07,640 --> 00:38:15,640
So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right?

515
00:38:15,640 --> 00:38:19,640
So just over five steps, we want to find what that constant increment is.

516
00:38:19,640 --> 00:38:22,640
So that's a pretty cool one.

517
00:38:22,640 --> 00:38:27,640
And then we have, we'll do log space, which is interesting.

518
00:38:27,640 --> 00:38:31,640
Log space equals torch dot log space.

519
00:38:31,640 --> 00:38:48,640
And then we'll go start, start equals negative 10 and equals 10.

520
00:38:48,640 --> 00:38:50,640
These are both start and end.

521
00:38:50,640 --> 00:38:51,640
You can either put these here.

522
00:38:51,640 --> 00:38:55,640
You can either put the start with them, start equals, or you don't have to.

523
00:38:55,640 --> 00:38:57,640
It's honestly up to you.

524
00:38:57,640 --> 00:39:00,640
And then we can put our steps again.

525
00:39:00,640 --> 00:39:03,640
So steps equals every five.

526
00:39:03,640 --> 00:39:05,640
Let's go ahead and run that.

527
00:39:05,640 --> 00:39:10,640
Oops, need to put log space there.

528
00:39:10,640 --> 00:39:11,640
So we get that.

529
00:39:11,640 --> 00:39:15,640
So we start at one of the negative 10.

530
00:39:15,640 --> 00:39:17,640
And then we just do this little increments here.

531
00:39:17,640 --> 00:39:19,640
So it goes 10, negative five, zero plus five times.

532
00:39:19,640 --> 00:39:20,640
Just over five steps.

533
00:39:20,640 --> 00:39:22,640
So that's pretty cool.

534
00:39:22,640 --> 00:39:24,640
What else do we have here?

535
00:39:24,640 --> 00:39:28,640
So we have I, torch dot I.

536
00:39:28,640 --> 00:39:30,640
I just have all these on my second screen here.

537
00:39:30,640 --> 00:39:36,640
So a bunch of examples just written out and we're just kind of visualizing what these can do.

538
00:39:36,640 --> 00:39:47,640
And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do.

539
00:39:47,640 --> 00:39:50,640
So we're just kind of experimenting with these.

540
00:39:50,640 --> 00:39:55,640
What we can do with the basics of pytorch and some of the very basic functions.

541
00:39:55,640 --> 00:40:01,640
So first I will print this out here.

542
00:40:01,640 --> 00:40:09,640
So we get pretty much just a diagonal line and it's in five.

543
00:40:09,640 --> 00:40:19,640
So you get a five by five matrix and pretty much just reduced row each long form.

544
00:40:19,640 --> 00:40:23,640
I don't know how to pronounce it, but that's pretty much what it looks like.

545
00:40:23,640 --> 00:40:26,640
So pretty cool stuff.

546
00:40:26,640 --> 00:40:29,640
Let's see what else we have.

547
00:40:29,640 --> 00:40:35,640
We have empty like.

548
00:40:35,640 --> 00:40:53,640
We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty.

549
00:40:53,640 --> 00:41:09,640
And then we can go two by three and then data type torch dot int 64 64 bit integers.

550
00:41:09,640 --> 00:41:17,640
And then let's see what happens here empty.

551
00:41:17,640 --> 00:41:20,640
So that's pretty cool.

552
00:41:20,640 --> 00:41:21,640
What else do we have?

553
00:41:21,640 --> 00:41:23,640
Yes, we can do timing as well.

554
00:41:23,640 --> 00:41:27,640
So I'm just going to erase all of these.

555
00:41:27,640 --> 00:41:38,640
You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit.

556
00:41:38,640 --> 00:41:41,640
But yeah, I'm actually going to delete all of these here.

557
00:41:41,640 --> 00:41:57,640
And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment.

558
00:41:57,640 --> 00:42:16,640
Cuda if torch dot Cuda underscore is dot Cuda is available.

559
00:42:16,640 --> 00:42:28,640
And then else you print out our device here and run this Cuda suite.

560
00:42:28,640 --> 00:42:44,640
So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality.

561
00:42:44,640 --> 00:42:49,640
We're just doing different experiments with a bunch of different tensors.

562
00:42:49,640 --> 00:42:56,640
So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time.

563
00:42:56,640 --> 00:43:00,640
So this comes with the operating system or sorry with with Python.

564
00:43:00,640 --> 00:43:03,640
You don't have to actually install this manually.

565
00:43:03,640 --> 00:43:13,640
So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now.

566
00:43:13,640 --> 00:43:20,640
So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds.

567
00:43:20,640 --> 00:43:26,640
So if we subtract end time, start time will get a three second difference and that would be the total elapsed time.

568
00:43:26,640 --> 00:43:34,640
And then this little number here, this four will be just how many decimal places we have.

569
00:43:34,640 --> 00:43:36,640
So I can go ahead and run this here.

570
00:43:36,640 --> 00:43:40,640
Time is not defined. Let's run that first.

571
00:43:40,640 --> 00:43:45,640
It's going to take, you know, almost no time at all.

572
00:43:45,640 --> 00:43:49,640
So we can actually increase this if we want to 10 and then run that again.

573
00:43:49,640 --> 00:43:53,640
Again, it's, you know, we're making up pretty much a one by one matrix.

574
00:43:53,640 --> 00:43:55,640
So just a just a zero.

575
00:43:55,640 --> 00:44:01,640
So we're not really going to get anything significant from that.

576
00:44:01,640 --> 00:44:17,640
But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network.

577
00:44:17,640 --> 00:44:27,640
That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem.

578
00:44:27,640 --> 00:44:31,640
It'll take maybe a few seconds at most like maybe 30 seconds to do that.

579
00:44:31,640 --> 00:44:35,640
And that's not going to be any time at all in the entire training process.

580
00:44:35,640 --> 00:44:46,640
So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU.

581
00:44:46,640 --> 00:44:48,640
So I have some code for that right here.

582
00:44:48,640 --> 00:44:52,640
So we're going to initialize a bunch of matrices here.

583
00:44:52,640 --> 00:44:58,640
So our sorry, tensors, and we have just basically random ones.

584
00:44:58,640 --> 00:45:03,640
So we have a 10,000 by 10,000, all random, all random floating point numbers.

585
00:45:03,640 --> 00:45:05,640
And then we're going to push these to the GPU.

586
00:45:05,640 --> 00:45:09,640
And we have two of these and then same thing for NumPy.

587
00:45:10,640 --> 00:45:17,640
So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.

588
00:45:17,640 --> 00:45:29,640
So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply.

589
00:45:29,640 --> 00:45:37,640
So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds.

590
00:45:37,640 --> 00:45:40,640
And then we have, see, look at that.

591
00:45:40,640 --> 00:45:45,640
So for the GPU, it took a little while to do that.

592
00:45:45,640 --> 00:45:49,640
And then for the CPU, it didn't take as long.

593
00:45:49,640 --> 00:45:56,640
So this is because there's the shape of these matrices are not really that big.

594
00:45:56,640 --> 00:45:58,640
They're just two dimensional, right?

595
00:45:58,640 --> 00:46:04,640
So it's see, this is something that the CPU can do very quickly because there's not that much to do.

596
00:46:04,640 --> 00:46:07,640
But let's say we want to bump it up a notch.

597
00:46:07,640 --> 00:46:15,640
So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.

598
00:46:15,640 --> 00:46:16,640
Hopefully that works.

599
00:46:16,640 --> 00:46:19,640
And then we can do, we'll just do the same thing.

600
00:46:19,640 --> 00:46:21,640
So just paste this.

601
00:46:22,640 --> 00:46:38,640
Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did.

602
00:46:38,640 --> 00:46:43,640
And this is because there's, you know, a lot more going on here.

603
00:46:43,640 --> 00:46:47,640
There's a lot more simple multiplication to do.

604
00:46:47,640 --> 00:46:55,640
So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model,

605
00:46:55,640 --> 00:47:01,640
we're not going to be doing very complex operations between all these tensors.

606
00:47:01,640 --> 00:47:04,640
They're going to be very similar to what we saw in here.

607
00:47:04,640 --> 00:47:09,640
The dimensionality and shape is going to be very similar to what we're seeing right now.

608
00:47:09,640 --> 00:47:11,640
You know, maybe three or four dimensions.

609
00:47:11,640 --> 00:47:15,640
And it's going to be very easy for a GPU to do this.

610
00:47:15,640 --> 00:47:18,640
They're not complex tasks that we need the CPU to do.

611
00:47:18,640 --> 00:47:20,640
They're not very hard at all.

612
00:47:20,640 --> 00:47:28,640
So when we give this task to parallel processing, it's going to be a ton quicker.

613
00:47:28,640 --> 00:47:31,640
So you're going to see why this matters later in the course.

614
00:47:31,640 --> 00:47:35,640
You're going to see this with some of the hyper parameters we're going to use,

615
00:47:35,640 --> 00:47:40,640
which I'm not going to get into quite yet, but over the next little bit,

616
00:47:40,640 --> 00:47:48,640
you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process.

617
00:47:48,640 --> 00:47:49,640
So this is great.

618
00:47:49,640 --> 00:47:57,640
Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency.

619
00:47:57,640 --> 00:48:03,640
So there's actually another term that we can use called a percentage percentage time.

620
00:48:03,640 --> 00:48:07,640
I don't know if that's exactly how you're supposed to call it, but that's what it is.

621
00:48:07,640 --> 00:48:12,640
And pretty much what it'll do is time how long it takes to execute a block.

622
00:48:12,640 --> 00:48:17,640
So we can see here there's CPU times zero nanoseconds.

623
00:48:17,640 --> 00:48:22,640
The end is for nano billionth of a second is a nanosecond and then wall time.

624
00:48:22,640 --> 00:48:28,640
So CPU time is how long it takes to execute on the CPU.

625
00:48:28,640 --> 00:48:36,640
The time that it's doing operations for and then the wall time would be how long it actually takes like in real time.

626
00:48:36,640 --> 00:48:40,640
How long do you have to wait? Do you have to wait until it's finished?

627
00:48:40,640 --> 00:48:45,640
So the only thing that the CPU CPU time doesn't include is waiting.

628
00:48:45,640 --> 00:48:50,640
So in an entire process, there's going to be some operations and there's going to be some waiting.

629
00:48:50,640 --> 00:48:56,640
Wall time is going to have both of those and CPU time is just the execution.

630
00:48:56,640 --> 00:49:02,640
So let's go ahead and continue with some of the basic PyTorch functions.

631
00:49:02,640 --> 00:49:06,640
So I've written some stuff down here.

632
00:49:06,640 --> 00:49:14,640
So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu.

633
00:49:14,640 --> 00:49:17,640
I don't think that's how you pronounce it, but we'll get into that more.

634
00:49:17,640 --> 00:49:24,640
Transposing, linear, concatenating, and the softmax function.

635
00:49:24,640 --> 00:49:28,640
So let's first start off here with the Torch.multinomial.

636
00:49:28,640 --> 00:49:34,640
So this is essentially a probability distribution based on the index that you give it.

637
00:49:34,640 --> 00:49:36,640
So we have probabilities here.

638
00:49:36,640 --> 00:49:39,640
We say 0.1 and 0.9.

639
00:49:39,640 --> 00:49:42,640
These numbers have to add up to one to make 100%.

640
00:49:42,640 --> 00:49:44,640
100% is one, one whole.

641
00:49:44,640 --> 00:49:47,640
So I have 10% and 90%.

642
00:49:47,640 --> 00:49:49,640
This is an index zero.

643
00:49:49,640 --> 00:49:55,640
So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one.

644
00:49:55,640 --> 00:50:05,640
So if I go ahead and run these up here.

645
00:50:05,640 --> 00:50:07,640
Give this a second to do its thing.

646
00:50:07,640 --> 00:50:11,640
So you can see that in the end we have our numSample set to 10.

647
00:50:11,640 --> 00:50:13,640
So it's going to give us 10 of these.

648
00:50:13,640 --> 00:50:15,640
1, 2, 3, 4, 5, 6, 7, 9, 10.

649
00:50:15,640 --> 00:50:17,640
And all of them are ones.

650
00:50:17,640 --> 00:50:20,640
If we run it again, we make it slightly different results.

651
00:50:20,640 --> 00:50:22,640
So now we have some zeros in there.

652
00:50:22,640 --> 00:50:25,640
But the zeros have very low probability of happening.

653
00:50:25,640 --> 00:50:29,640
As a matter of fact, exactly a 10% probability of happening.

654
00:50:29,640 --> 00:50:36,640
So we're going to use this later in predicting what word is going to come next.

655
00:50:36,640 --> 00:50:42,640
Let's move on to Torch.cat or short for Torch.concatenate.

656
00:50:42,640 --> 00:50:46,640
So this will essentially concatenate two tensors into one.

657
00:50:46,640 --> 00:50:50,640
So I initialize this tensor here, torch.tensor, 1, 2, 3, 4.

658
00:50:50,640 --> 00:50:52,640
It's one dimensional.

659
00:50:52,640 --> 00:50:56,640
And we have another tensor here that just contains five.

660
00:50:56,640 --> 00:51:03,640
So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5.

661
00:51:03,640 --> 00:51:08,640
We just combine them together and this is what will come out in the end.

662
00:51:08,640 --> 00:51:10,640
So we run that 1, 2, 3, 4, 5.

663
00:51:10,640 --> 00:51:12,640
Perfect.

664
00:51:12,640 --> 00:51:15,640
So we're going to actually use this when we're generating.

665
00:51:15,640 --> 00:51:18,640
When we're generating text given a context.

666
00:51:18,640 --> 00:51:22,640
So it's going to start from zero.

667
00:51:22,640 --> 00:51:25,640
We're going to use our probability distribution to pick the first one.

668
00:51:25,640 --> 00:51:33,640
And then based on the first one, we're going to, you know, we're going to predict the next character.

669
00:51:33,640 --> 00:51:41,640
And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted.

670
00:51:41,640 --> 00:51:44,640
So we have this, maybe like 100 characters over here.

671
00:51:44,640 --> 00:51:46,640
And then the next character that we're predicting is over here.

672
00:51:46,640 --> 00:51:48,640
We just concatenate these.

673
00:51:48,640 --> 00:51:54,640
And by the end, we will have all of the integers that we've predicted.

674
00:51:54,640 --> 00:51:57,640
So next up, we have torch.trill.

675
00:51:57,640 --> 00:52:03,640
And what this stands for, what the trail stands for is a triangle lower.

676
00:52:03,640 --> 00:52:07,640
So it's going to be in a sort of a triangle formation like this diagonal.

677
00:52:07,640 --> 00:52:12,640
It's going to be going from top left to bottom right.

678
00:52:12,640 --> 00:52:16,640
And so you're going to see a little bit more why later in this course.

679
00:52:16,640 --> 00:52:30,640
But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history.

680
00:52:30,640 --> 00:52:32,640
We're trying to predict the future.

681
00:52:32,640 --> 00:52:36,640
So giving the answers in the future isn't what we want to do at all.

682
00:52:36,640 --> 00:52:40,640
So maybe we've just predicted one and the rest of them we haven't predicted yet.

683
00:52:40,640 --> 00:52:42,640
So we set all these to zero.

684
00:52:42,640 --> 00:52:44,640
And then we predicted another one.

685
00:52:44,640 --> 00:52:45,640
And these are still zero.

686
00:52:45,640 --> 00:52:47,640
So these are talking to each other in history.

687
00:52:47,640 --> 00:52:58,640
And as and as our predictions add up, we have more and more history to look back to and less future, right?

688
00:52:58,640 --> 00:53:04,640
Basically, the premise of this is just making sure we can't communicate with the answer.

689
00:53:04,640 --> 00:53:11,640
We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet.

690
00:53:11,640 --> 00:53:13,640
They don't give you the answer sheet.

691
00:53:13,640 --> 00:53:19,640
So you have to know based on your history of knowledge, which answers to predict.

692
00:53:19,640 --> 00:53:22,640
And that's all that's going on here.

693
00:53:22,640 --> 00:53:26,640
And we have, I mean, you could probably guess this triangle upper.

694
00:53:26,640 --> 00:53:28,640
So we have all the upper ones.

695
00:53:28,640 --> 00:53:32,640
These are, you know, lower on the lower side and then these are on the upper side.

696
00:53:32,640 --> 00:53:34,640
So same concept there.

697
00:53:34,640 --> 00:53:37,640
And then we have a masked fill.

698
00:53:37,640 --> 00:53:48,640
So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here.

699
00:53:48,640 --> 00:53:53,640
So if you exponentiate zero, if you exponentiate zero, it'll become one.

700
00:53:53,640 --> 00:53:57,640
If you exponentiate negative infinity, it'll become zero.

701
00:53:57,640 --> 00:54:03,640
All that's going on here is we're doing approximately 2.71.

702
00:54:03,640 --> 00:54:08,640
And this is a constant that we use in the dot exp function.

703
00:54:08,640 --> 00:54:14,640
And then we're putting this to whatever power is in that current slot.

704
00:54:14,640 --> 00:54:16,640
So we have a zero here.

705
00:54:16,640 --> 00:54:26,640
So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71.

706
00:54:27,640 --> 00:54:40,640
And then 2.71 to the negative infinity is, of course, zero.

707
00:54:40,640 --> 00:54:43,640
So that's pretty much how we get from this to this.

708
00:54:43,640 --> 00:54:48,640
And we're just, we're simply just masking these over.

709
00:54:48,640 --> 00:54:50,640
So that's great.

710
00:54:50,640 --> 00:54:54,640
And I sort of showcase what the exp does.

711
00:54:54,640 --> 00:54:56,640
And we're just using this one right here.

712
00:54:56,640 --> 00:55:00,640
We're using this output and we're just plugging it into here.

713
00:55:00,640 --> 00:55:05,640
So it'll go from negative infinity to zero and then zero to one.

714
00:55:05,640 --> 00:55:08,640
So that's how we get from here to here.

715
00:55:08,640 --> 00:55:11,640
Now we have transposing.

716
00:55:11,640 --> 00:55:16,640
So transposing is when we sort of flip or swap the dimensions of a tensor.

717
00:55:16,640 --> 00:55:23,640
So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four.

718
00:55:23,640 --> 00:55:30,640
And we can use the transpose function to essentially flip any dimensions that we want.

719
00:55:30,640 --> 00:55:37,640
So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension,

720
00:55:37,640 --> 00:55:41,640
but we're pretty much swapping the zero with position with the second.

721
00:55:41,640 --> 00:55:45,640
So zero, one, two, we're swapping this one with this one.

722
00:55:45,640 --> 00:55:51,640
So the end result, like you would probably guess the shape of this is going to be 432 instead of 234.

723
00:55:51,640 --> 00:55:55,640
So you kind of just take a look at this and see, you know, which ones are being flipped.

724
00:55:55,640 --> 00:55:59,640
And those are the dimensions and that's the output.

725
00:55:59,640 --> 00:56:01,640
So hopefully that makes sense.

726
00:56:01,640 --> 00:56:03,640
Next up, we have torch dot stack.

727
00:56:03,640 --> 00:56:06,640
And this is where we're actually going to go.

728
00:56:06,640 --> 00:56:08,640
We're going to we're going to do more of this.

729
00:56:08,640 --> 00:56:15,640
We're actually going to use torch dot stack stack very shortly here when we're getting our batches.

730
00:56:15,640 --> 00:56:23,640
So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant,

731
00:56:23,640 --> 00:56:28,640
a giant length of integers or tokens.

732
00:56:28,640 --> 00:56:34,640
And all we're doing is we're just stacking them together in blocks or to make a batch.

733
00:56:34,640 --> 00:56:37,640
So that's pretty much what we're going to end up doing.

734
00:56:37,640 --> 00:56:39,640
And that's what torch dot stack does.

735
00:56:39,640 --> 00:56:46,640
We can take something that's one dimensional and then we can stack it to make it two dimensional.

736
00:56:46,640 --> 00:56:52,640
We can take something that's two dimensional and stack it a bunch of times to make it three dimensional.

737
00:56:52,640 --> 00:56:58,640
Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other.

738
00:56:58,640 --> 00:56:59,640
Now it's four dimensional.

739
00:56:59,640 --> 00:57:01,640
So hopefully that makes sense.

740
00:57:01,640 --> 00:57:05,640
All we're doing is we're just passing in each tensor that we're going to stack in order.

741
00:57:05,640 --> 00:57:09,640
So this is our little output here and that's pretty much all it is.

742
00:57:09,640 --> 00:57:17,640
The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish.

743
00:57:17,640 --> 00:57:18,640
It's really important.

744
00:57:18,640 --> 00:57:20,640
It's called the nn dot linear function.

745
00:57:20,640 --> 00:57:25,640
So it is a pretty much a function of the nn dot module.

746
00:57:25,640 --> 00:57:35,640
And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters.

747
00:57:35,640 --> 00:57:41,640
So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false.

748
00:57:41,640 --> 00:57:50,640
But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better.

749
00:57:50,640 --> 00:57:59,640
And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output.

750
00:57:59,640 --> 00:58:06,640
So pretty much anything with nn dot linear is going to be very important and it's going to be learnable.

751
00:58:06,640 --> 00:58:08,640
So we can see over here.

752
00:58:08,640 --> 00:58:12,640
This is the tors.nn little site here on the docs.

753
00:58:12,640 --> 00:58:19,640
So we have containers, a bunch of different layers like activations, layers, pretty much just layers.

754
00:58:19,640 --> 00:58:20,640
That's all it is.

755
00:58:20,640 --> 00:58:23,640
And so these are these are important.

756
00:58:23,640 --> 00:58:26,640
We're going to, we're basically going to learn from these.

757
00:58:26,640 --> 00:58:32,640
And you're going to see why we're going to use something called keys and values, keys, values and queers later on.

758
00:58:32,640 --> 00:58:33,640
You'll see why those are important.

759
00:58:33,640 --> 00:58:38,640
But if that doesn't make sense yet, help me, let me illustrate value for you right now.

760
00:58:38,640 --> 00:58:40,640
So I drew this out here.

761
00:58:40,640 --> 00:58:48,640
So if we look back at our examples, we have a, we make, we initialize a term.

762
00:58:48,640 --> 00:58:51,640
We make, we initialize a tensor.

763
00:58:51,640 --> 00:58:53,640
It's 10, 10 and 10.

764
00:58:53,640 --> 00:58:56,640
What we're going to do is we're going to do a linear transformation.

765
00:58:56,640 --> 00:58:58,640
This linear stands for linear transformation.

766
00:58:58,640 --> 00:59:04,640
So pretty much we're just going to apply a weight and a bias through each of these layers here.

767
00:59:04,640 --> 00:59:09,640
So we have an input and we have an output x is our input, y is our output.

768
00:59:09,640 --> 00:59:13,640
And this is of size three and this is of size three.

769
00:59:13,640 --> 00:59:16,640
So pretty much we just need to make sure that these are lining up.

770
00:59:16,640 --> 00:59:26,640
And for more context, the nn.sequential is sort of built off nn.linear.

771
00:59:26,640 --> 00:59:32,640
So if we go ahead and search that up right now, this will make sense in a second here.

772
00:59:32,640 --> 00:59:37,640
This is also some good prerequisite knowledge in general for machine learning.

773
00:59:37,640 --> 00:59:45,640
So let's see nn.sequential doesn't show it here, but pretty much.

774
00:59:45,640 --> 00:59:53,640
If you have, let's say, two, you have two input neurons and maybe you have one output neuron.

775
00:59:53,640 --> 00:59:55,640
Okay, you have a bunch of hidden layers in between here.

776
00:59:55,640 --> 01:00:02,640
Let's say we have one, two, three, four, and then one, two, three.

777
01:00:02,640 --> 01:00:09,640
So pretty much you need to make sure that the inputs aligns with this hidden layer.

778
01:00:09,640 --> 01:00:12,640
This hidden layer aligns with this one and this one aligns with this one.

779
01:00:12,640 --> 01:00:17,640
So you're going to have a transformation of two to four.

780
01:00:17,640 --> 01:00:25,640
So two, four, and then this one's going to be four to three, four to three,

781
01:00:25,640 --> 01:00:27,640
and then you're going to have a final one.

782
01:00:27,640 --> 01:00:31,640
This is two to four right here, four to three here, and then this final one.

783
01:00:31,640 --> 01:00:33,640
It's going to be three to one.

784
01:00:33,640 --> 01:00:37,640
So you pretty much just need to make sure that these are lining up.

785
01:00:37,640 --> 01:00:43,640
So we can see that we have two, four, and then this four is carried on from this output here.

786
01:00:43,640 --> 01:00:47,640
And pretty much this will just make sure that our shapes are consistent.

787
01:00:47,640 --> 01:00:52,640
And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work.

788
01:00:52,640 --> 01:00:54,640
So we need to make sure that our shapes are consistent.

789
01:00:54,640 --> 01:00:59,640
If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets,

790
01:00:59,640 --> 01:01:04,640
but if you're really interested, you could use chatGPT, of course.

791
01:01:04,640 --> 01:01:09,640
And that's a really good learning resource, chatGPT, going on to get up discussions, maybe,

792
01:01:09,640 --> 01:01:12,640
or just looking at documentation.

793
01:01:12,640 --> 01:01:19,640
And if you're not good at reading documentation, then you could take maybe some little keywords from here,

794
01:01:19,640 --> 01:01:22,640
like a sequential container.

795
01:01:22,640 --> 01:01:24,640
Well, what is a sequential container?

796
01:01:24,640 --> 01:01:29,640
You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation

797
01:01:29,640 --> 01:01:31,640
and figure things out step by step.

798
01:01:31,640 --> 01:01:38,640
It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on.

799
01:01:38,640 --> 01:01:40,640
You don't need to memorize them.

800
01:01:40,640 --> 01:01:44,640
But while you're working with them, it's important to understand what they're really doing behind the scenes,

801
01:01:44,640 --> 01:01:50,640
especially if you want to make an efficient and popular working neural net.

802
01:01:50,640 --> 01:01:53,640
So that's that.

803
01:01:53,640 --> 01:02:00,640
And pretty much what's going to happen here with these linear layers is we're just going to simply transform

804
01:02:00,640 --> 01:02:03,640
from one to the other input to output, no hidden layers.

805
01:02:03,640 --> 01:02:06,640
And we're just going to be able to learn best parameters for doing that.

806
01:02:06,640 --> 01:02:10,640
You're going to see why that's useful later.

807
01:02:10,640 --> 01:02:12,640
Now we have the softmax function.

808
01:02:12,640 --> 01:02:14,640
So that sounds scary.

809
01:02:14,640 --> 01:02:18,640
And the softmax function isn't actually what it sounds like at all.

810
01:02:18,640 --> 01:02:20,640
Let me illustrate that for you right now.

811
01:02:20,640 --> 01:02:25,640
So let's go ahead and change the color here.

812
01:02:25,640 --> 01:02:35,640
So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc.

813
01:02:35,640 --> 01:02:37,640
Right, floating points, whatever.

814
01:02:37,640 --> 01:02:48,640
So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these.

815
01:02:48,640 --> 01:02:53,640
And we're going to divide them by the sum of all of these exponentiated.

816
01:02:53,640 --> 01:02:57,640
So pretty much what's going to happen, let's say we exponentiate one.

817
01:02:57,640 --> 01:03:05,640
So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp.

818
01:03:05,640 --> 01:03:08,640
And I think I talked about this up here.

819
01:03:08,640 --> 01:03:15,640
This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating.

820
01:03:15,640 --> 01:03:23,640
So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71.

821
01:03:23,640 --> 01:03:35,640
And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two.

822
01:03:35,640 --> 01:03:37,640
Okay, so we're going to get 7.34.

823
01:03:38,640 --> 01:03:40,640
I'm going to get 7.34.

824
01:03:40,640 --> 01:03:42,640
Gorg my writing, it's terrible.

825
01:03:42,640 --> 01:03:47,640
2.71 to 3 cubed.

826
01:03:47,640 --> 01:03:48,640
So 19.9.

827
01:03:51,640 --> 01:03:55,640
So pretty much what's going to happen is we can rearrange this in a new array.

828
01:03:55,640 --> 01:04:00,640
7.34 and 19.9.

829
01:04:00,640 --> 01:04:06,640
So if we add all these up together, we add all these up together, we're going to get 2.71 plus this.

830
01:04:06,640 --> 01:04:08,640
Let's do this math real quick.

831
01:04:08,640 --> 01:04:12,640
I'm just going to walk you through this to help you understand what the softmax function is doing.

832
01:04:12,640 --> 01:04:20,640
7.34 plus 19.9.

833
01:04:20,640 --> 01:04:23,640
That's going to give us a total of 29.95.

834
01:04:23,640 --> 01:04:24,640
Great.

835
01:04:24,640 --> 01:04:29,640
29.95.

836
01:04:29,640 --> 01:04:36,640
So all we do is we just divide each of these elements by the total.

837
01:04:36,640 --> 01:04:40,640
So 2.71 divided by this is going to give us maybe x.

838
01:04:40,640 --> 01:04:44,640
And we do 7.34 divided by this is going to give us y.

839
01:04:44,640 --> 01:04:49,640
And then we have 19.9 divided by this is going to give us z.

840
01:04:49,640 --> 01:04:54,640
So pretty much you're going to exponentiate all of these.

841
01:04:54,640 --> 01:04:57,640
You're going to add them together to create a total.

842
01:04:57,640 --> 01:05:02,640
And then you're going to divide each of those exponentiate elements by the exponentiated total.

843
01:05:02,640 --> 01:05:09,640
So after that, this x right here is just, we're just going to wrap these again.

844
01:05:09,640 --> 01:05:17,640
And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z.

845
01:05:17,640 --> 01:05:19,640
That's all it's doing.

846
01:05:19,640 --> 01:05:22,640
And yeah, it's not really crazy.

847
01:05:22,640 --> 01:05:25,640
There's a weird formula for it.

848
01:05:25,640 --> 01:05:30,640
Softmax, softmax function.

849
01:05:30,640 --> 01:05:35,640
So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here

850
01:05:35,640 --> 01:05:39,640
and a lot of math that's above the high school level.

851
01:05:39,640 --> 01:05:43,640
But yeah, like this formula here, I believe this is what it is.

852
01:05:43,640 --> 01:05:46,640
Or standard unit, softmax function, there you go.

853
01:05:46,640 --> 01:05:48,640
So pretty much this is what it does.

854
01:05:48,640 --> 01:05:51,640
And there's your easy explanation of what it does.

855
01:05:51,640 --> 01:05:56,640
So you're going to see why this is useful later, but it's just important to know what's going on

856
01:05:56,640 --> 01:06:02,640
so that you won't lag behind later in the course when this background knowledge becomes important.

857
01:06:02,640 --> 01:06:08,640
So if we go over a little example of that, of the softmax function in code, it looks like this right here.

858
01:06:08,640 --> 01:06:13,640
So we import torsha and n dot functional as f, f short for functional.

859
01:06:13,640 --> 01:06:17,640
And we pretty much just do f dot softmax and then plug in a tensor.

860
01:06:17,640 --> 01:06:23,640
And what we want the dimension to be the output dimension.

861
01:06:23,640 --> 01:06:29,640
So if we plug this into here and we print it out, we go and print it out.

862
01:06:29,640 --> 01:06:35,640
It's going to take a second.

863
01:06:35,640 --> 01:06:39,640
Torch is not defined. So let's run this from the top here.

864
01:06:39,640 --> 01:06:42,640
Boom.

865
01:06:42,640 --> 01:06:44,640
And let's try that again. Boom. There we go.

866
01:06:44,640 --> 01:06:48,640
So if you took all those values, let's actually do this again from scratch.

867
01:06:48,640 --> 01:06:56,640
So we do 2.71, 2.71 divided by 29.95.

868
01:06:56,640 --> 01:07:02,640
We get 0.09, 0.09. Good.

869
01:07:02,640 --> 01:07:12,640
And then if we do 7.34 divided by 29.95, we get 0.245.

870
01:07:12,640 --> 01:07:16,640
So 0.245. Well, it's kind of close.

871
01:07:16,640 --> 01:07:22,640
Really close actually. And then 66.52. So if we go, what was that last one there?

872
01:07:22,640 --> 01:07:30,640
19.9. So we do 19.9 divided by 29.95.

873
01:07:30,640 --> 01:07:35,640
66.4. So 66.5. It's pretty close.

874
01:07:35,640 --> 01:07:41,640
Again, we're rounding, so it's not perfectly accurate.

875
01:07:41,640 --> 01:07:47,640
As you can see, they're very close and for only having two decimal places, we did pretty good.

876
01:07:47,640 --> 01:07:52,640
So that's just sort of illustrating what the softmax function does and what it looks like in code.

877
01:07:52,640 --> 01:08:01,640
We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line.

878
01:08:01,640 --> 01:08:04,640
It's just like that.

879
01:08:04,640 --> 01:08:07,640
So now we're going to go over embeddings.

880
01:08:07,640 --> 01:08:10,640
And I'm not actually, I don't have any code for this yet.

881
01:08:10,640 --> 01:08:16,640
We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills

882
01:08:16,640 --> 01:08:23,640
and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning.

883
01:08:23,640 --> 01:08:33,640
So if we pop in a chat GPT here, we say, what is an end dot embedding?

884
01:08:33,640 --> 01:08:45,640
And then dots. Let me type in a non-bedding class in the PyTorch library.

885
01:08:45,640 --> 01:08:51,640
Okay, actual language processing max maps each discrete input to a dense vector representation.

886
01:08:51,640 --> 01:08:54,640
Okay, how does this work? Let's see.

887
01:08:54,640 --> 01:08:58,640
So we have some vocab. So that's probably our vocabulary size.

888
01:08:58,640 --> 01:09:05,640
I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set.

889
01:09:05,640 --> 01:09:11,640
That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter.

890
01:09:11,640 --> 01:09:16,640
So let's see. This doesn't quite make sense to me yet.

891
01:09:16,640 --> 01:09:19,640
So maybe I want to learn what does this actually look like?

892
01:09:19,640 --> 01:09:33,640
Can you explain this to a, maybe an eighth grader and provide a visualization?

893
01:09:33,640 --> 01:09:37,640
Certainly. Okay.

894
01:09:37,640 --> 01:09:42,640
Little secret codes that represent the meaning of the words. Okay, that helps.

895
01:09:42,640 --> 01:09:47,640
So if we have cat, okay, so cat, cat's a word.

896
01:09:47,640 --> 01:09:53,640
So maybe we want to know what it would look like on a character level.

897
01:09:53,640 --> 01:10:03,640
What about on a character level instead of word level?

898
01:10:03,640 --> 01:10:05,640
So it's probably going to look very similar.

899
01:10:05,640 --> 01:10:10,640
We have this little vector here storing some information about whatever this is.

900
01:10:10,640 --> 01:10:18,640
So a, it means this here. Okay, so as your point to, and this is really useful.

901
01:10:18,640 --> 01:10:21,640
So we've pretty much just learned what embedding vectors does.

902
01:10:21,640 --> 01:10:30,640
And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character.

903
01:10:30,640 --> 01:10:34,640
And we don't even know what each of these elements mean.

904
01:10:34,640 --> 01:10:35,640
We don't know what they mean.

905
01:10:35,640 --> 01:10:41,640
This could be maybe positivity or should be the start of a word or it could be any piece of information,

906
01:10:41,640 --> 01:10:44,640
maybe something we can't even comprehend yet.

907
01:10:44,640 --> 01:10:55,640
But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before,

908
01:10:55,640 --> 01:11:02,640
nn.embedding right here is a part of the nn.module.

909
01:11:02,640 --> 01:11:05,640
So these are learnable parameters, which is great.

910
01:11:05,640 --> 01:11:08,640
So it's actually going to learn the importance of each letter,

911
01:11:08,640 --> 01:11:11,640
and it's going to be able to produce some amazing results.

912
01:11:11,640 --> 01:11:21,640
So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter.

913
01:11:21,640 --> 01:11:25,640
In our case, it's character level, not subword, not word, it's character level.

914
01:11:25,640 --> 01:11:28,640
So it's going to represent some meaning about those.

915
01:11:28,640 --> 01:11:30,640
So that's what embedding vectors are.

916
01:11:30,640 --> 01:11:32,640
Let's go figure out how they work in code.

917
01:11:32,640 --> 01:11:38,640
We have this little character level embedding vector and it contains a list.

918
01:11:38,640 --> 01:11:45,640
There's five elements in here, one, two, three, four, five, and it's by the vocab size.

919
01:11:45,640 --> 01:11:51,640
So we have all of our vocabulary by the length of each embedding vector.

920
01:11:51,640 --> 01:11:56,640
So this actually makes sense because our vocab size by the embedding dimension,

921
01:11:56,640 --> 01:12:02,640
which is how much information is actually being stored in each of these characters.

922
01:12:02,640 --> 01:12:04,640
So this now is very easy to understand.

923
01:12:04,640 --> 01:12:09,640
I'm just going to copy this code from here and I'm going to paste it down here.

924
01:12:09,640 --> 01:12:16,640
And let's just get rid of the torch because we already initialized that above.

925
01:12:17,640 --> 01:12:22,640
So if we just run this, actually, let's turn that down to maybe a thousand characters.

926
01:12:22,640 --> 01:12:24,640
Let's try that out.

927
01:12:25,640 --> 01:12:27,640
And it's not defined.

928
01:12:27,640 --> 01:12:29,640
We did not initialize it.

929
01:12:40,640 --> 01:12:43,640
So let's go back down here and look at that.

930
01:12:43,640 --> 01:12:50,640
So this dot shape is going to essentially show the shape of it this much by this much.

931
01:12:50,640 --> 01:12:52,640
So it's four by a hundred.

932
01:12:52,640 --> 01:12:59,640
And yeah, so we can we can work with these and we can store stuff about characters in them.

933
01:12:59,640 --> 01:13:05,640
And you're going to see this in the next lecture, how we actually use embedding vectors.

934
01:13:05,640 --> 01:13:09,640
So no need to worry if a lot of this doesn't make sense yet.

935
01:13:09,640 --> 01:13:10,640
That's fine.

936
01:13:10,640 --> 01:13:13,640
You're going to learn a little bit more about how we use these over the course.

937
01:13:13,640 --> 01:13:17,640
You're going to get more confident with using them even in your own projects.

938
01:13:17,640 --> 01:13:19,640
So don't don't stress about it too much right now.

939
01:13:19,640 --> 01:13:23,640
Embeddings are pretty tricky at first to learn.

940
01:13:23,640 --> 01:13:25,640
So don't worry about that too much.

941
01:13:25,640 --> 01:13:37,640
But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks.

942
01:13:37,640 --> 01:13:49,640
So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say.

943
01:13:49,640 --> 01:13:53,640
But that's the architecture of this multilayer perceptron.

944
01:13:53,640 --> 01:13:58,640
But pretty much what's happening is we have a little input here and we have a white matrix.

945
01:13:58,640 --> 01:14:01,640
So white matrix is looks like this.

946
01:14:01,640 --> 01:14:11,640
It's like this and we have some, we have some values in between X1, Y1 and maybe Z1.

947
01:14:11,640 --> 01:14:17,640
So a bunch of weights and maybe biases to that we add to it.

948
01:14:17,640 --> 01:14:23,640
So the tricky part is how do we actually multiply our input by this white matrix?

949
01:14:23,640 --> 01:14:25,640
We're just doing one matrix times another.

950
01:14:25,640 --> 01:14:27,640
Well, that's called matrix multiplication.

951
01:14:27,640 --> 01:14:30,640
And I'm going to show you how to do that right now.

952
01:14:30,640 --> 01:14:34,640
So first off, we have to learn something called dot products.

953
01:14:34,640 --> 01:14:40,640
So dot products are actually pretty easy and you might have actually done them before.

954
01:14:40,640 --> 01:14:46,640
So let's say we go ahead and take, we go ahead and take this right here we go.

955
01:14:47,640 --> 01:14:49,640
One, two, three.

956
01:14:49,640 --> 01:14:51,640
That's going to be what A is.

957
01:14:51,640 --> 01:14:55,640
And then we have four, five, six.

958
01:14:55,640 --> 01:15:04,640
So if we want to find the dot product between these two, all we have to do is simply take the index of both of these,

959
01:15:04,640 --> 01:15:08,640
the first ones and the second ones and third ones, multiply them together and then add.

960
01:15:08,640 --> 01:15:20,640
So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five,

961
01:15:20,640 --> 01:15:24,640
and then add it to three times six.

962
01:15:26,640 --> 01:15:32,640
So one times four is four, two times five is ten, three times six is eighteen.

963
01:15:32,640 --> 01:15:36,640
So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two.

964
01:15:36,640 --> 01:15:42,640
So the dot product of this is going to be thirty-two.

965
01:15:42,640 --> 01:15:45,640
And that's pretty much how simple dot products are.

966
01:15:45,640 --> 01:15:54,640
It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up.

967
01:15:54,640 --> 01:15:55,640
That's a dot product.

968
01:15:55,640 --> 01:15:59,640
So we actually need dot products for matrix multiplication.

969
01:15:59,640 --> 01:16:01,640
So let's go ahead and jump into that right now.

970
01:16:01,640 --> 01:16:06,640
So I'm just going to create two matrices that are going to be pretty easy to work with.

971
01:16:06,640 --> 01:16:12,640
So let's say we have A and have one matrix over here.

972
01:16:12,640 --> 01:16:20,640
It's going to be one, two, three, four, five and six.

973
01:16:20,640 --> 01:16:26,640
This is going to be equal to A and then B is going to be another matrix.

974
01:16:26,640 --> 01:16:35,640
So we're going to have seven, eight, nine, ten, eleven, twelve.

975
01:16:35,640 --> 01:16:37,640
Ignore my terrible writing.

976
01:16:37,640 --> 01:16:41,640
Pretty much what we do is to multiply these together.

977
01:16:41,640 --> 01:16:45,640
First we need to make sure that they can multiply together.

978
01:16:45,640 --> 01:16:49,640
So we need to take a look at the amount of rows and columns at this half.

979
01:16:49,640 --> 01:16:52,640
So this one right here is three rows, one, two, three.

980
01:16:52,640 --> 01:16:54,640
Three rows and two columns.

981
01:16:54,640 --> 01:16:57,640
So this is going to be a three by two matrix.

982
01:16:57,640 --> 01:17:01,640
And this one has two rows and three columns.

983
01:17:01,640 --> 01:17:04,640
So it's a two by three matrix.

984
01:17:04,640 --> 01:17:11,640
So all we have to make sure that if we're multiplying A dot product with B,

985
01:17:11,640 --> 01:17:15,640
and this is the PyTorch syntax for multiplying matrices,

986
01:17:15,640 --> 01:17:21,640
if we're multiplying A by B, then we have to make sure the following is true.

987
01:17:21,640 --> 01:17:29,640
So if we use three by two and then dot product with two times three,

988
01:17:29,640 --> 01:17:34,640
we have to make sure that these two inner values are the same.

989
01:17:34,640 --> 01:17:37,640
So two is equal to two, so we cross these out,

990
01:17:37,640 --> 01:17:40,640
and then the ones that we have left over are three by three.

991
01:17:40,640 --> 01:17:44,640
So the resulting matrix would be A three by three.

992
01:17:44,640 --> 01:17:52,640
Or if you had like a three by four times A five by five by one,

993
01:17:52,640 --> 01:17:55,640
that doesn't work because these values aren't the same.

994
01:17:55,640 --> 01:17:58,640
So these two matrices couldn't multiply.

995
01:17:58,640 --> 01:18:02,640
And sometimes you actually have to flip these to make them work.

996
01:18:02,640 --> 01:18:07,640
So maybe we change this value here to A three.

997
01:18:07,640 --> 01:18:09,640
We change this value to a three.

998
01:18:09,640 --> 01:18:12,640
In this order, they do not multiply.

999
01:18:12,640 --> 01:18:25,640
But if we switch them around, we have a three by five with A five by three,

1000
01:18:25,640 --> 01:18:29,640
sorry, five by three with A three by four.

1001
01:18:29,640 --> 01:18:31,640
So these two numbers are the same.

1002
01:18:31,640 --> 01:18:32,640
That works.

1003
01:18:32,640 --> 01:18:34,640
The resulting matrix is a five by four.

1004
01:18:34,640 --> 01:18:38,640
So that's how you make sure that two matrices are compatible.

1005
01:18:38,640 --> 01:18:41,640
So now to actually multiply these together,

1006
01:18:41,640 --> 01:18:43,640
what we're going to do, I'm going to make a new line here.

1007
01:18:43,640 --> 01:18:47,640
So we're going to rewrite these.

1008
01:18:47,640 --> 01:18:49,640
Now we don't have to rewrite them.

1009
01:18:49,640 --> 01:18:51,640
Let's just cross that out here.

1010
01:18:51,640 --> 01:19:01,640
So pretty much what we have to do is we have to take these two and dot product with these two.

1011
01:19:01,640 --> 01:19:08,640
And then once we're done that, we do the same with these and these, these and these.

1012
01:19:08,640 --> 01:19:14,640
So we start with the first, the first row in the A matrix.

1013
01:19:14,640 --> 01:19:18,640
And we iterate through all of the columns in the B matrix.

1014
01:19:18,640 --> 01:19:24,640
And then after we're done that, we just go to the next row in the A matrix and then et cetera, right?

1015
01:19:24,640 --> 01:19:26,640
So let's go ahead and do this right now.

1016
01:19:26,640 --> 01:19:30,640
That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here.

1017
01:19:30,640 --> 01:19:40,640
So we have our one times, our one times seven plus two times 10.

1018
01:19:40,640 --> 01:19:47,640
So one times seven plus two times 10.

1019
01:19:47,640 --> 01:19:50,640
And this is equal to 27.

1020
01:19:50,640 --> 01:19:56,640
So that's the first dot product of one and two and seven and 10.

1021
01:19:56,640 --> 01:20:01,640
So what this is actually going to look like in our new matrix, I'm going to write this out here.

1022
01:20:01,640 --> 01:20:04,640
So this is our new matrix here.

1023
01:20:04,640 --> 01:20:08,640
This 27 is going to go right here.

1024
01:20:08,640 --> 01:20:10,640
Let's continue.

1025
01:20:10,640 --> 01:20:16,640
So next up, we're going to do one and two and then eight and 11.

1026
01:20:16,640 --> 01:20:32,640
So we're going to go one, one times eight plus two, or sorry, two and 11.

1027
01:20:32,640 --> 01:20:35,640
So one times eight is eight and then two times 11 is 22.

1028
01:20:35,640 --> 01:20:41,640
So our result here is 30 and 30 is just going to go right here.

1029
01:20:41,640 --> 01:20:45,640
So 27, 30, and you can see how this is going to work, right?

1030
01:20:45,640 --> 01:20:53,640
So in our first row of A, we're going to get the first row of this resulting matrix.

1031
01:20:53,640 --> 01:20:58,640
So let's go ahead and do the rest here.

1032
01:20:58,640 --> 01:21:03,640
So we have one and two and then nine and 12.

1033
01:21:03,640 --> 01:21:10,640
One times nine, two times 12.

1034
01:21:10,640 --> 01:21:13,640
One times nine is nine, two times 12 is 24.

1035
01:21:13,640 --> 01:21:17,640
So if we do, that's like 33, I believe.

1036
01:21:17,640 --> 01:21:22,640
So 33 and we can go ahead and write that here.

1037
01:21:22,640 --> 01:21:25,640
So now let's move on to the next.

1038
01:21:25,640 --> 01:21:34,640
We have three and four, three, three and four dot product with seven and 10.

1039
01:21:34,640 --> 01:21:39,640
So three will multiply seven.

1040
01:21:39,640 --> 01:21:48,640
And then we're going to go ahead and add that to four times 10.

1041
01:21:48,640 --> 01:21:54,640
Three times seven, three times seven is 21, and then four times 10 is 40.

1042
01:21:54,640 --> 01:21:56,640
So we're going to get 47.

1043
01:21:56,640 --> 01:22:02,640
So I'll put there so we can go in and write 47 right there.

1044
01:22:02,640 --> 01:22:09,640
And our next one is going to be three and four dot product with eight and 11.

1045
01:22:09,640 --> 01:22:20,640
So eight plus four times 11.

1046
01:22:20,640 --> 01:22:21,640
Perfect.

1047
01:22:21,640 --> 01:22:27,640
So we get three times eight is 24 and then plus 44.

1048
01:22:27,640 --> 01:22:32,640
So 24 plus 44, that's 68.

1049
01:22:32,640 --> 01:22:39,640
So we get 68 and we can go in and write that here.

1050
01:22:39,640 --> 01:22:53,640
So next up, we have three and four and nine and 12.

1051
01:22:53,640 --> 01:22:56,640
So three times nine is 27.

1052
01:22:56,640 --> 01:22:57,640
And then four times 12.

1053
01:22:57,640 --> 01:22:59,640
So let's just, let's just do that.

1054
01:22:59,640 --> 01:23:01,640
I'm not doing that in my head.

1055
01:23:01,640 --> 01:23:03,640
27 plus was four times 12.

1056
01:23:03,640 --> 01:23:04,640
So that's 48.

1057
01:23:04,640 --> 01:23:10,640
27 plus 48 gives us 75.

1058
01:23:10,640 --> 01:23:14,640
Let's go ahead and write our 75 here.

1059
01:23:14,640 --> 01:23:18,640
Then we can go ahead and slide down to this row since we're done, since we're done that.

1060
01:23:18,640 --> 01:23:36,640
And then we go five, five and six dot product was seven and 10.

1061
01:23:36,640 --> 01:23:42,640
So our result from this five times seven is 35 and then six times 10 is 60.

1062
01:23:42,640 --> 01:23:44,640
So we're going to get 95.

1063
01:23:44,640 --> 01:23:49,640
We can go in and write our 95 here.

1064
01:23:49,640 --> 01:24:06,640
And then five and six dot product with eight and 11.

1065
01:24:06,640 --> 01:24:10,640
So five times eight is 40 and then six times 11 is 66.

1066
01:24:10,640 --> 01:24:19,640
So we get 104.

1067
01:24:19,640 --> 01:24:35,640
And then the last one, so five and six dot product with nine and 12.

1068
01:24:35,640 --> 01:24:38,640
So five, five times nine is 45.

1069
01:24:38,640 --> 01:24:44,640
And then six times 12 is what six times 12, 72, I think.

1070
01:24:44,640 --> 01:24:47,640
So six times 12, 72.

1071
01:24:47,640 --> 01:24:48,640
Yeah.

1072
01:24:48,640 --> 01:24:56,640
So 45 plus 72, 117.

1073
01:24:56,640 --> 01:25:04,640
And that is how you do a three by two matrix and a two by three matrix multiplying them together.

1074
01:25:04,640 --> 01:25:12,640
So the result would be C equals that.

1075
01:25:12,640 --> 01:25:21,640
So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far.

1076
01:25:21,640 --> 01:25:30,640
So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU.

1077
01:25:30,640 --> 01:25:36,640
So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training.

1078
01:25:36,640 --> 01:25:41,640
But pretty much that's how you do dot products and matrix multiplication.

1079
01:25:41,640 --> 01:25:44,640
So I actually realized I messed up a little bit on the math there.

1080
01:25:44,640 --> 01:25:48,640
So this 104, that's actually 106.

1081
01:25:48,640 --> 01:25:52,640
So I messed up there if you caught that.

1082
01:25:52,640 --> 01:25:53,640
Good job.

1083
01:25:53,640 --> 01:25:58,640
But pretty much this is what this looks like in three lines of code.

1084
01:25:58,640 --> 01:26:05,640
So all of this up here that we just covered all of this is in three lines.

1085
01:26:05,640 --> 01:26:09,640
So we initialize an A tensor and a B tensor.

1086
01:26:09,640 --> 01:26:11,640
Each one of these is a row.

1087
01:26:11,640 --> 01:26:16,640
Each one of these is a row and it'll pretty much multiply these together.

1088
01:26:16,640 --> 01:26:23,640
So this at symbol, this is a shorthand how you multiply two matrices in pytorch together.

1089
01:26:23,640 --> 01:26:31,640
Another way to do this is to use the torch dot matrix multiply function or math mall for short.

1090
01:26:31,640 --> 01:26:34,640
And then you can do A and B.

1091
01:26:34,640 --> 01:26:38,640
So these will print literally the same thing.

1092
01:26:38,640 --> 01:26:39,640
Look at that.

1093
01:26:39,640 --> 01:26:43,640
So I'm not too sure on the differences between them.

1094
01:26:43,640 --> 01:26:47,640
I use A at B for short.

1095
01:26:47,640 --> 01:26:56,640
But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that.

1096
01:26:56,640 --> 01:27:06,640
But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks.

1097
01:27:06,640 --> 01:27:09,640
So there's our network here if I go up.

1098
01:27:09,640 --> 01:27:19,640
Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number.

1099
01:27:19,640 --> 01:27:25,640
So if it's like a one, it would be like one dot zero or something or just like a one dot.

1100
01:27:25,640 --> 01:27:27,640
That's what it would look like as a floating point number.

1101
01:27:27,640 --> 01:27:32,640
But if it were an integer, say B is full of ones with integers, it would just be a one.

1102
01:27:32,640 --> 01:27:35,640
There wouldn't be any decimal zero zero center, right?

1103
01:27:35,640 --> 01:27:37,640
It would just be one.

1104
01:27:37,640 --> 01:27:45,640
So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type.

1105
01:27:45,640 --> 01:27:48,640
So I showcase this right here.

1106
01:27:48,640 --> 01:27:50,640
We have an int 64.

1107
01:27:50,640 --> 01:27:56,640
So type of it is an integer and a float 32, 64 and 32 don't mean anything.

1108
01:27:56,640 --> 01:27:59,640
All we have to know is an integer and floating point number.

1109
01:27:59,640 --> 01:28:12,640
So I've initialized a torch.randint, I covered above and set above here.

1110
01:28:12,640 --> 01:28:19,640
And maybe not.

1111
01:28:19,640 --> 01:28:27,640
Anyways, this pretty much does torch.randint is going the first parameter here is anything.

1112
01:28:27,640 --> 01:28:29,640
It's pretty much your range.

1113
01:28:29,640 --> 01:28:34,640
So I could do like zero to five, or I could just do like one.

1114
01:28:34,640 --> 01:28:40,640
So it'll do zero up to one, and then your shape of the matrix that it generates.

1115
01:28:40,640 --> 01:28:43,640
So I said it's a random int.

1116
01:28:43,640 --> 01:28:49,640
So that means it's going to generate a tensor with the data type integer 64.

1117
01:28:49,640 --> 01:28:58,640
So we have a three by two, and then I initialize another random key detail here.

1118
01:28:58,640 --> 01:29:01,640
We don't have the int suffix.

1119
01:29:01,640 --> 01:29:05,640
So this just generates floating point numbers.

1120
01:29:05,640 --> 01:29:08,640
And if we actually return the types of each of these.

1121
01:29:08,640 --> 01:29:20,640
So five print int 64 dot d type, and then float 32 dot d type.

1122
01:29:20,640 --> 01:29:21,640
Save that.

1123
01:29:21,640 --> 01:29:25,640
I'm going to comment this out for now.

1124
01:29:25,640 --> 01:29:28,640
We get a in 64 and float 32.

1125
01:29:28,640 --> 01:29:37,640
So if we just try to multiply these together, try to multiply these together.

1126
01:29:37,640 --> 01:29:40,640
Expected scalar type long above found float.

1127
01:29:40,640 --> 01:29:43,640
So long is pretty much when you have a sequence of integers.

1128
01:29:43,640 --> 01:29:47,640
And float is, of course, you have the decimal place.

1129
01:29:47,640 --> 01:29:49,640
So you can actually multiply this together.

1130
01:29:49,640 --> 01:29:57,640
So pretty much what you can do is cast the float method on this.

1131
01:29:57,640 --> 01:30:04,640
If you just do dot float, and then parentheses, and then run this, it'll actually work.

1132
01:30:04,640 --> 01:30:07,640
So you can cast integers to floats.

1133
01:30:07,640 --> 01:30:11,640
And then I think there's a way you can cast floats to integers, but it has some rounding in there.

1134
01:30:11,640 --> 01:30:17,640
So probably not the best for input and weights, matrix multiplication.

1135
01:30:17,640 --> 01:30:26,640
But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise.

1136
01:30:26,640 --> 01:30:30,640
So you want to make sure that they have sort of room to float around.

1137
01:30:30,640 --> 01:30:33,640
So that's pretty much how you avoid that error.

1138
01:30:33,640 --> 01:30:34,640
Let's move on.

1139
01:30:34,640 --> 01:30:35,640
So congratulations.

1140
01:30:35,640 --> 01:30:38,640
You probably made it further than quite a few people already.

1141
01:30:38,640 --> 01:30:40,640
So congratulations on that.

1142
01:30:40,640 --> 01:30:44,640
That was one of the most comprehensive parts of this entire course.

1143
01:30:44,640 --> 01:30:48,640
Understanding the math is going on behind the scenes.

1144
01:30:48,640 --> 01:30:52,640
For some people, it's very hard to grasp if you're not very fluent with math.

1145
01:30:52,640 --> 01:30:58,640
But yeah, let's continue the biogram language model and let's pump out some code here.

1146
01:30:58,640 --> 01:31:03,640
So to recap, we're using CUDA to accelerate the training process.

1147
01:31:03,640 --> 01:31:10,640
We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel.

1148
01:31:10,640 --> 01:31:12,640
Two hyperparameters.

1149
01:31:12,640 --> 01:31:14,640
We open our text.

1150
01:31:14,640 --> 01:31:16,640
We make a vocabulary out of it.

1151
01:31:16,640 --> 01:31:19,640
We initialize our encoder and decoder.

1152
01:31:19,640 --> 01:31:26,640
We get our data encoding all this text, and then we get our train and bow splits.

1153
01:31:26,640 --> 01:31:28,640
And then this next function here, get batch.

1154
01:31:28,640 --> 01:31:34,640
So before I jump into this, go ahead and run this here.

1155
01:31:34,640 --> 01:31:42,640
So this is pretty much just taking the first little, I don't know, we have eight characters.

1156
01:31:42,640 --> 01:31:49,640
So it's taking the first eight characters and then index one all the way to index nine.

1157
01:31:50,640 --> 01:31:58,640
And we can pretty much use this to show what the current input is and then what the target would be.

1158
01:31:58,640 --> 01:32:06,640
So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right?

1159
01:32:06,640 --> 01:32:09,640
So this is the premise of the biogram language model.

1160
01:32:09,640 --> 01:32:11,640
Given this character, we're going to predict the next.

1161
01:32:11,640 --> 01:32:14,640
It doesn't know anything else in the entire history.

1162
01:32:14,640 --> 01:32:18,640
It just knows what's before it or just knows what the current character is.

1163
01:32:18,640 --> 01:32:22,640
And based on that, we're going to predict the next one.

1164
01:32:22,640 --> 01:32:28,640
So we have this get batch function here, and this part right here is the most important piece of code.

1165
01:32:28,640 --> 01:32:35,640
This is going to work a little bit more later with our train and bow splits, making sure that, you know,

1166
01:32:35,640 --> 01:32:38,640
I'll try to explain this in a different way with our training bow splits.

1167
01:32:38,640 --> 01:32:42,640
So imagine you take a course, as you take a math course, okay?

1168
01:32:42,640 --> 01:32:49,640
And 90% of all your work is done just learning how the course works, learning all about the math.

1169
01:32:49,640 --> 01:32:52,640
So that's like 90% of data you get from it.

1170
01:32:52,640 --> 01:32:54,640
And then maybe another 10%.

1171
01:32:54,640 --> 01:33:00,640
Another 10% at the end is that final exam, which might have some questions you've never seen before.

1172
01:33:00,640 --> 01:33:05,640
So the point is in that first 90%, you're tested on based on what you know.

1173
01:33:05,640 --> 01:33:09,640
And then this other 10% is what you don't know.

1174
01:33:09,640 --> 01:33:15,640
And this pretty much means you can't memorize everything and then just start generating based on your memory.

1175
01:33:15,640 --> 01:33:21,640
You generate something that's alike or something that's close based on what you already know and the patterns you captured

1176
01:33:21,640 --> 01:33:24,640
in that 90% of the course.

1177
01:33:24,640 --> 01:33:26,640
So you can write your final exam successfully.

1178
01:33:26,640 --> 01:33:28,640
So that's pretty much what's going on here.

1179
01:33:28,640 --> 01:33:36,640
The training is the course, learning everything about it and then validation is validating the final exam.

1180
01:33:36,640 --> 01:33:49,640
So pretty much what we're doing here is initialize IX and that'll take a random manager between zero

1181
01:33:49,640 --> 01:33:54,640
and then length of the length of the entire text minus block size.

1182
01:33:54,640 --> 01:34:04,640
So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data.

1183
01:34:04,640 --> 01:34:06,640
So that's kind of how that works.

1184
01:34:06,640 --> 01:34:10,640
And if we print this out here, it'll just give us this right here.

1185
01:34:10,640 --> 01:34:11,640
So we get some random integers.

1186
01:34:11,640 --> 01:34:20,640
These are some random indices in the entire text that we can start generating from.

1187
01:34:20,640 --> 01:34:24,640
So print this out and then torch.stack.

1188
01:34:24,640 --> 01:34:25,640
We covered this before.

1189
01:34:25,640 --> 01:34:28,640
Pretty much what this does, it's going to stack them in batches.

1190
01:34:28,640 --> 01:34:30,640
This is the entire point of batches.

1191
01:34:30,640 --> 01:34:33,640
So that's what we do there.

1192
01:34:33,640 --> 01:34:39,640
We get X and then Y is just the same thing, but offset by one like this.

1193
01:34:39,640 --> 01:34:42,640
So that's what happens there.

1194
01:34:42,640 --> 01:34:47,640
And let's get into actually, I'm going to add something here.

1195
01:34:47,640 --> 01:34:48,640
This is going to be very important.

1196
01:34:48,640 --> 01:34:56,640
We're going to go X and Y is equal to model dot.

1197
01:34:56,640 --> 01:35:02,640
We're going to go X dot to device.

1198
01:35:02,640 --> 01:35:06,640
So notice how, no, we didn't do it up here.

1199
01:35:06,640 --> 01:35:15,640
Okay, we'll cover this later, but pretty much you're going to see what this does in a second here.

1200
01:35:16,640 --> 01:35:23,640
We return these and you can see that the device changed.

1201
01:35:23,640 --> 01:35:24,640
So now we're actually on CUDA.

1202
01:35:24,640 --> 01:35:33,640
And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU.

1203
01:35:33,640 --> 01:35:39,640
They're no longer going to be processed sequentially, but rather in our batches in parallel.

1204
01:35:39,640 --> 01:35:48,640
So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here.

1205
01:35:48,640 --> 01:35:52,640
So now we can go ahead and actually initialize our neural net.

1206
01:35:52,640 --> 01:35:59,640
So what I'm going to do is I'm going to go back up here and we're going to import some more stuff.

1207
01:35:59,640 --> 01:36:07,640
So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second.

1208
01:36:07,640 --> 01:36:09,640
I'm going to explain this here.

1209
01:36:09,640 --> 01:36:20,640
I just want to get some code out first.

1210
01:36:20,640 --> 01:36:22,640
And down here we can initialize this.

1211
01:36:22,640 --> 01:36:23,640
So it's a class.

1212
01:36:23,640 --> 01:36:33,640
We're going to make it a by-gram language model subclass of nn.module.

1213
01:36:33,640 --> 01:36:42,640
And the reason why we do nn.module here is because it's going to take an nn.module.

1214
01:36:42,640 --> 01:36:57,640
I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters.

1215
01:36:57,640 --> 01:37:02,640
So I'm going to go ahead and look at the documentation here so you can sort of understand this better.

1216
01:37:03,640 --> 01:37:05,640
We go to nn.

1217
01:37:05,640 --> 01:37:16,640
So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before.

1218
01:37:16,640 --> 01:37:18,640
So we have nn.linear.

1219
01:37:18,640 --> 01:37:26,640
So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable.

1220
01:37:26,640 --> 01:37:31,640
So that weight matrix will be changed through gradient descent.

1221
01:37:31,640 --> 01:37:34,640
And actually, I think I should probably cover gradient descent right now.

1222
01:37:34,640 --> 01:37:42,640
So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better.

1223
01:37:42,640 --> 01:37:46,640
So I'm going to go ahead and set up a little graph for that right now.

1224
01:37:46,640 --> 01:37:49,640
So I'm going to be using a little tool called Desmos.

1225
01:37:49,640 --> 01:37:51,640
Desmos is actually great.

1226
01:37:51,640 --> 01:37:53,640
It acts as a graphing calculator.

1227
01:37:53,640 --> 01:37:56,640
So you can plug in formulas and move things around.

1228
01:37:56,640 --> 01:37:59,640
You sort of visualize how math functions work.

1229
01:37:59,640 --> 01:38:06,640
So I've written some functions out here that will basically calculate the derivative of a sine wave.

1230
01:38:06,640 --> 01:38:10,640
So if I move A around, you'll see that changes.

1231
01:38:10,640 --> 01:38:17,640
So before I get into what's really going on here, I need to first tell you what the loss actually is.

1232
01:38:17,640 --> 01:38:23,640
If you're not familiar with the loss, let's say we have 80 characters in our vocabulary.

1233
01:38:23,640 --> 01:38:28,640
And we have just started our model, no training at all, completely random weights.

1234
01:38:28,640 --> 01:38:34,640
And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully.

1235
01:38:34,640 --> 01:38:42,640
So how we can measure the loss of this is by taking the negative log likelihood.

1236
01:38:42,640 --> 01:38:44,640
So the likelihood is one out of 80.

1237
01:38:44,640 --> 01:38:47,640
We take the log of that and then negative.

1238
01:38:47,640 --> 01:38:51,640
So if we plug this in here, we'll get 4.38.

1239
01:38:51,640 --> 01:38:53,640
So that's a terrible loss.

1240
01:38:53,640 --> 01:38:55,640
Obviously, that's one out of 80.

1241
01:38:55,640 --> 01:38:59,640
So it's like, you know, not even 2% chance.

1242
01:38:59,640 --> 01:39:01,640
So that's not great.

1243
01:39:01,640 --> 01:39:08,640
So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss.

1244
01:39:08,640 --> 01:39:10,640
And that's how we train our network.

1245
01:39:10,640 --> 01:39:11,640
So how does this actually work?

1246
01:39:11,640 --> 01:39:13,640
How does this actually work out in code, you ask?

1247
01:39:13,640 --> 01:39:17,640
So pretty much, let's say we have a loss here, okay?

1248
01:39:17,640 --> 01:39:20,640
Start off with a loss of 2, just arbitrary loss, whatever.

1249
01:39:20,640 --> 01:39:24,640
And what we're trying to do is decrease it.

1250
01:39:24,640 --> 01:39:29,640
So over time, it's going to become smaller and smaller if we move in this direction.

1251
01:39:29,640 --> 01:39:32,640
So how do we know if we're moving in the right direction?

1252
01:39:32,640 --> 01:39:37,640
Well, we take the derivative of what the current point is at right now,

1253
01:39:37,640 --> 01:39:39,640
and then we try moving it in a different direction.

1254
01:39:39,640 --> 01:39:42,640
So if we move it this way, sure, it'll go down.

1255
01:39:42,640 --> 01:39:43,640
That's great.

1256
01:39:43,640 --> 01:39:47,640
We can hit the local bottom over there, or we can move to this side.

1257
01:39:47,640 --> 01:39:51,640
And then we can see that the slope is increasing in a negative direction.

1258
01:39:51,640 --> 01:39:56,640
So we're going to keep adjusting the parameters in favor of this direction.

1259
01:39:56,640 --> 01:39:59,640
So that's pretty much what gradient descent is.

1260
01:39:59,640 --> 01:40:03,640
We're descending with the gradient.

1261
01:40:03,640 --> 01:40:05,640
So pretty self-explanatory.

1262
01:40:05,640 --> 01:40:07,640
That's what the loss function does.

1263
01:40:07,640 --> 01:40:11,640
And gradient descent is an optimizer.

1264
01:40:11,640 --> 01:40:13,640
So it's an optimizer for the network.

1265
01:40:13,640 --> 01:40:17,640
Optimizes our parameters, our weight, matrices, etc.

1266
01:40:17,640 --> 01:40:20,640
So these are some common optimizers that are used.

1267
01:40:20,640 --> 01:40:25,640
And this is just by going to torch.optim, short for optimizer.

1268
01:40:25,640 --> 01:40:29,640
And these are just a list of a bunch of optimizers that PyTorch provides.

1269
01:40:29,640 --> 01:40:34,640
So what we're going to be using is something called AdamW.

1270
01:40:34,640 --> 01:40:40,640
And what AdamW is, is it pretty much...

1271
01:40:40,640 --> 01:40:42,640
I'm just going to read off my little script here,

1272
01:40:42,640 --> 01:40:46,640
because I can't memorize every optimizer that exists.

1273
01:40:46,640 --> 01:40:52,640
So Adam, without Adam, just Adam, not AdamW,

1274
01:40:52,640 --> 01:40:57,640
Adam is a popular optimization algorithm that combines ideas of momentum.

1275
01:40:57,640 --> 01:41:03,640
And it uses a moving average of both the gradient and its squared value

1276
01:41:03,640 --> 01:41:06,640
to adapt the learning rate of each parameter.

1277
01:41:06,640 --> 01:41:10,640
And the learning rate is something that we should also go over.

1278
01:41:10,640 --> 01:41:15,640
So let's say I figure out I need to move in this direction.

1279
01:41:15,640 --> 01:41:17,640
I move, I take a step like that.

1280
01:41:17,640 --> 01:41:20,640
Okay, that's a very big step that I say,

1281
01:41:20,640 --> 01:41:22,640
okay, we need to keep moving in that direction.

1282
01:41:22,640 --> 01:41:26,640
So what happens is I go like this, and then I end up there.

1283
01:41:26,640 --> 01:41:29,640
And it's like, whoa, we're going up now, what happened?

1284
01:41:29,640 --> 01:41:31,640
So that's because you have a very high learning rate.

1285
01:41:31,640 --> 01:41:35,640
If you have a lower learning rate, what will happen is you'll start here.

1286
01:41:35,640 --> 01:41:38,640
It'll take little one-pixel steps or very, very small steps.

1287
01:41:38,640 --> 01:41:41,640
Okay, that's good. That's better. It's even better.

1288
01:41:41,640 --> 01:41:44,640
Keep going in this direction. This is great.

1289
01:41:44,640 --> 01:41:47,640
And you keep going down. You're like, okay, this is good.

1290
01:41:47,640 --> 01:41:50,640
We're descending. And it's starting to flatten out.

1291
01:41:50,640 --> 01:41:53,640
So we know that we're hitting a local bottom here.

1292
01:41:53,640 --> 01:41:56,640
And then we stop because it starts ascending again.

1293
01:41:56,640 --> 01:42:03,640
So that means this is our best set of parameters because of what that loss is

1294
01:42:03,640 --> 01:42:08,640
or what the derivative is of that particular point.

1295
01:42:08,640 --> 01:42:12,640
So pretty much this is what the learning rate is.

1296
01:42:12,640 --> 01:42:17,640
So you want to have a small learning rate so that you don't take too large steps

1297
01:42:17,640 --> 01:42:21,640
so that the parameters don't change dramatically and end up messing you up.

1298
01:42:21,640 --> 01:42:24,640
So you want to make them small enough so that you can still have efficient training.

1299
01:42:24,640 --> 01:42:33,640
You don't want to be moving in a millionth of one or something.

1300
01:42:33,640 --> 01:42:38,640
That would be ridiculous. You'd have to do so many iterations to even get this far.

1301
01:42:38,640 --> 01:42:45,640
So maybe you'd make it decently high but not too high that it'll go like that, right?

1302
01:42:45,640 --> 01:42:50,640
So that's what the learning rate is, just how fast it learns pretty much.

1303
01:42:50,640 --> 01:42:59,640
And yeah, so AtomW is a modification of the Atom Optimizer.

1304
01:42:59,640 --> 01:43:05,640
And it adds weight to K. So pretty much there's just some features that you add on to gradient descent

1305
01:43:05,640 --> 01:43:09,640
and then AtomW is the same thing except that has weight to K.

1306
01:43:09,640 --> 01:43:13,640
And what this pretty much means is it generalizes the parameters more.

1307
01:43:13,640 --> 01:43:17,640
So instead of having very high level of performance or very low level,

1308
01:43:17,640 --> 01:43:20,640
it takes a little generalized in between.

1309
01:43:20,640 --> 01:43:27,640
So the weight significance will actually shrink as it flans out.

1310
01:43:27,640 --> 01:43:32,640
So this will pretty much make sure that certain parameters in your network,

1311
01:43:32,640 --> 01:43:39,640
certain parameters in your weight matrices aren't affecting the output of this model drastically.

1312
01:43:39,640 --> 01:43:42,640
That could be in a positive or negative direction.

1313
01:43:42,640 --> 01:43:47,640
You can have insanely high performance from some lucky parameters in your weight matrices.

1314
01:43:47,640 --> 01:43:53,640
So pretty much the point is to minimize those, to decay those values.

1315
01:43:53,640 --> 01:44:00,640
That's what weight to K is, to prevent it from having that insane or super low performance.

1316
01:44:00,640 --> 01:44:02,640
That's what weight to K is.

1317
01:44:02,640 --> 01:44:07,640
So that's a little background on gradient descent and optimizers.

1318
01:44:07,640 --> 01:44:10,640
Let's go ahead and finish typing this out.

1319
01:44:10,640 --> 01:44:17,640
So next up, we actually, we need to initialize some things.

1320
01:44:17,640 --> 01:44:27,640
So we have our init self, of course, since it's a class, vocab size.

1321
01:44:27,640 --> 01:44:32,640
I want to make sure that's correct, vocabulary size.

1322
01:44:32,640 --> 01:44:47,640
I might actually shrink this just a vocab size because it sounds way easier to type out.

1323
01:44:47,640 --> 01:44:49,640
And vocab size, good.

1324
01:44:49,640 --> 01:44:53,640
So we're going to pump out some R code here.

1325
01:44:53,640 --> 01:44:57,640
And this is just assuming that you have some sort of a background in Python.

1326
01:44:57,640 --> 01:45:01,640
If not, it's all good.

1327
01:45:01,640 --> 01:45:04,640
Just understanding the premise of what's going on here.

1328
01:45:04,640 --> 01:45:09,640
So we're going to make something called an embedding table.

1329
01:45:09,640 --> 01:45:19,640
And I'm going to explain this to you in a second here, why the embedding table is really important.

1330
01:45:19,640 --> 01:45:23,640
Notice that we use the nn.

1331
01:45:23,640 --> 01:45:26,640
We use the nn module in this.

1332
01:45:26,640 --> 01:45:30,640
So that means this is going to be a learnable parameter, the init.embedding.

1333
01:45:30,640 --> 01:45:36,640
So we're going to make this vocab size by vocab size.

1334
01:45:36,640 --> 01:45:41,640
So let's say you have all eight characters here and you have all eight characters here.

1335
01:45:41,640 --> 01:45:46,640
I'm going to actually show you what this looks like in a second here and why this is really important.

1336
01:45:46,640 --> 01:45:51,640
So first off, we're going to finish typing out this background language model.

1337
01:45:51,640 --> 01:45:55,640
So we're going to define our forward pass here.

1338
01:45:55,640 --> 01:46:02,640
So the reason why we type this forward pass out, instead of just using what it offers by default,

1339
01:46:02,640 --> 01:46:12,640
is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task.

1340
01:46:12,640 --> 01:46:18,640
This is a really good practice because we want to actually know what's going on behind the scenes in our model.

1341
01:46:18,640 --> 01:46:20,640
We want to know exactly what's going on.

1342
01:46:20,640 --> 01:46:29,640
We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug.

1343
01:46:29,640 --> 01:46:37,640
So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch?

1344
01:46:37,640 --> 01:46:42,640
Well, like I said, understanding the process, what are all the transformations that are actually going on,

1345
01:46:42,640 --> 01:46:50,640
all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output?

1346
01:46:50,640 --> 01:47:03,640
Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices

1347
01:47:03,640 --> 01:47:12,640
If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that.

1348
01:47:12,640 --> 01:47:21,640
So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up.

1349
01:47:21,640 --> 01:47:28,640
So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here.

1350
01:47:28,640 --> 01:47:34,640
Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right?

1351
01:47:34,640 --> 01:47:41,640
So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right?

1352
01:47:41,640 --> 01:47:48,640
And optimization, of course. So that's pretty much why we write out the forward pass from scratch.

1353
01:47:48,640 --> 01:47:55,640
It's also just best practice. So it's never really a good idea to not write this.

1354
01:47:55,640 --> 01:48:03,640
But let's continue. So self, and it will do index and targets.

1355
01:48:03,640 --> 01:48:12,640
So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here.

1356
01:48:12,640 --> 01:48:17,640
Before we do logits, I'm going to explain to you this embedding table here.

1357
01:48:18,640 --> 01:48:23,640
Paste that in.

1358
01:48:29,640 --> 01:48:35,640
Return logits. You're going to see why we return logits in a second here.

1359
01:48:35,640 --> 01:48:41,640
So this an end on embedding here is pretty much just a lookup table.

1360
01:48:41,640 --> 01:48:45,640
So what we're going to have, I'm actually going to pull up my notebook here.

1361
01:48:45,640 --> 01:48:53,640
So we have a giant sort of grid of what the predictions are going to look like.

1362
01:48:53,640 --> 01:48:57,640
It's going to look, can I drag it in here? No.

1363
01:48:57,640 --> 01:49:03,640
So go ahead and download this full screen. Boom.

1364
01:49:03,640 --> 01:49:06,640
This is my notion here, but pretty much this is what it looks like.

1365
01:49:06,640 --> 01:49:09,640
And I took this picture from Andrei Karpathy's lecture.

1366
01:49:09,640 --> 01:49:14,640
But what this is, is it has start tokens and end tokens.

1367
01:49:14,640 --> 01:49:18,640
So start is at the start of the block, and end tokens are at the end of the block.

1368
01:49:18,640 --> 01:49:29,640
And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character.

1369
01:49:29,640 --> 01:49:45,640
So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.

1370
01:49:45,640 --> 01:49:51,640
So if we just add up all these, if we normalize them, and we get a little probability of this happening,

1371
01:49:51,640 --> 01:49:55,640
I don't know, if we add up all these together, I don't know what that is.

1372
01:49:55,640 --> 01:49:59,640
It's some crazy number, maybe 20,000 or something, something crazy.

1373
01:49:59,640 --> 01:50:07,640
Pretty much that percentage is the percentage of the end token coming after the character A.

1374
01:50:07,640 --> 01:50:15,640
And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind.

1375
01:50:15,640 --> 01:50:24,640
That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that.

1376
01:50:24,640 --> 01:50:30,640
To that entire row. So this one's pretty significant in proportion to the others.

1377
01:50:30,640 --> 01:50:33,640
So this one's going to be a fairly high probability of coming next.

1378
01:50:33,640 --> 01:50:37,640
A lot of the times you're going to have an I coming after an R.

1379
01:50:37,640 --> 01:50:40,640
And that's pretty much what that is. That's the embedding table.

1380
01:50:40,640 --> 01:50:43,640
So that's why we make it vocab size by vocab size.

1381
01:50:43,640 --> 01:50:48,640
So that's a little background on what we're doing here.

1382
01:50:48,640 --> 01:50:53,640
So let's continue with the term logits.

1383
01:50:53,640 --> 01:50:57,640
So what exactly are the logits? You're probably asking that.

1384
01:50:57,640 --> 01:51:04,640
So let's actually go back to a little notebook I had over here.

1385
01:51:04,640 --> 01:51:10,640
So remember our softmax function, right? Our softmax right here.

1386
01:51:10,640 --> 01:51:15,640
So we exponentiated each of these values and then we normalized them.

1387
01:51:15,640 --> 01:51:21,640
Normalized. We took its contribution to the sum of everything. That's what normalizing is.

1388
01:51:21,640 --> 01:51:28,640
So you can think of logits as just a bunch of floating point numbers that are normalized, right?

1389
01:51:28,640 --> 01:51:33,640
So you have a total, I'll write this out.

1390
01:51:33,640 --> 01:51:43,640
So let's say we have, that's a terrible line. Let's draw a new one.

1391
01:51:44,640 --> 01:52:03,640
So let's say we have 2, 4, and 6. And we want to normalize these.

1392
01:52:03,640 --> 01:52:09,640
So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12.

1393
01:52:09,640 --> 01:52:15,640
So 2 divided by 12. We take the percentage of that.

1394
01:52:15,640 --> 01:52:21,640
2 out of 12 is 0.16 something, okay?

1395
01:52:21,640 --> 01:52:27,640
So 0.16, we'll just do 1.167.

1396
01:52:27,640 --> 01:52:31,640
And then 4 out of 12 would be double that.

1397
01:52:31,640 --> 01:52:38,640
So 4 out of 12 would be 33, 33%.

1398
01:52:38,640 --> 01:52:42,640
And then 6 out of 12, that's 50. So 0.5.

1399
01:52:42,640 --> 01:52:50,640
So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution.

1400
01:52:50,640 --> 01:53:04,640
So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d.

1401
01:53:04,640 --> 01:53:08,640
We know that from this distribution, a followed by d is most likely to come next.

1402
01:53:08,640 --> 01:53:15,640
So this is what the logits are. They're pretty much a probability distribution of what we want to predict.

1403
01:53:15,640 --> 01:53:21,640
So given that, let's hop back into here. We're going to mess around with these a little bit.

1404
01:53:21,640 --> 01:53:27,640
So we have this embedding table, and I already showed you what that looked like.

1405
01:53:27,640 --> 01:53:30,640
It looked like this right here. This is our embedding table.

1406
01:53:30,640 --> 01:53:38,640
So let's use something called, we're going to use a function called dot view.

1407
01:53:38,640 --> 01:53:43,640
So this is going to help us sort of reshape what our logits look like.

1408
01:53:43,640 --> 01:53:46,640
And I'm going to go over an example of what this looks like in a second here.

1409
01:53:46,640 --> 01:53:51,640
I'm just going to pump out some code. So we have our batch by our time.

1410
01:53:51,640 --> 01:53:56,640
So the time is, you can think of time as that sequence of integers.

1411
01:53:56,640 --> 01:53:59,640
That's the time dimension, right? You start from here.

1412
01:53:59,640 --> 01:54:03,640
Maybe through the generating process, we don't know what's here next.

1413
01:54:03,640 --> 01:54:06,640
We don't know what's on the, we don't know what the next token is.

1414
01:54:06,640 --> 01:54:10,640
So that's why we say it's time because there's some we don't know yet and there's some that we already do know.

1415
01:54:10,640 --> 01:54:12,640
That's what we call the time dimension.

1416
01:54:12,640 --> 01:54:18,640
And then channels would just be, how many different channels are, what's the vocabulary size?

1417
01:54:18,640 --> 01:54:20,640
Channels is the vocabulary size.

1418
01:54:20,640 --> 01:54:24,640
So we can make this the logits dot shape.

1419
01:54:24,640 --> 01:54:27,640
This is what logits going to return here is B by T by C.

1420
01:54:27,640 --> 01:54:29,640
That's the shape of it.

1421
01:54:29,640 --> 01:54:38,640
And then our targets do, actually, no, we won't do that yet.

1422
01:54:38,640 --> 01:54:46,640
We'll do do logits equals logits dot view.

1423
01:54:46,640 --> 01:54:52,640
And then we'll, this is very important, B by T.

1424
01:54:52,640 --> 01:55:02,640
So because we're particularly paying attention to the channels, the vocabulary, the batch and time,

1425
01:55:02,640 --> 01:55:05,640
they, I mean, they're not as important here.

1426
01:55:05,640 --> 01:55:07,640
So we can sort of blend these together.

1427
01:55:07,640 --> 01:55:15,640
And as long as the logits and the targets have the same batch and time, we should be all right.

1428
01:55:15,640 --> 01:55:21,640
So we're going to do B, B times T by C.

1429
01:55:21,640 --> 01:55:26,640
And then we can go to initialize our targets.

1430
01:55:26,640 --> 01:55:33,640
It's going to be targets dot view.

1431
01:55:33,640 --> 01:55:39,640
And it's going to be just a B by T.

1432
01:55:39,640 --> 01:55:43,640
And then we can make our loss, remember the loss function, right?

1433
01:55:43,640 --> 01:55:49,640
We do the functional of cross entropy, just a way of measuring the loss.

1434
01:55:49,640 --> 01:55:52,640
And we basically take where there's two parameters here.

1435
01:55:52,640 --> 01:55:58,640
So we have the logits and the targets.

1436
01:55:58,640 --> 01:56:02,640
So I'm going to go over exactly what's going on here in a second.

1437
01:56:02,640 --> 01:56:05,640
But first, you might be asking, what does this view mean?

1438
01:56:05,640 --> 01:56:06,640
What exactly does this do?

1439
01:56:06,640 --> 01:56:08,640
So I'm going to show you that right now.

1440
01:56:08,640 --> 01:56:14,640
There's some code here that initializes a random tensor of shape 2 by 3 by 5.

1441
01:56:14,640 --> 01:56:22,640
And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape.

1442
01:56:22,640 --> 01:56:27,640
So shape takes the, you know, it takes the 2 by 3 by 5.

1443
01:56:27,640 --> 01:56:31,640
We get x equals 2, y equals 3, and z equals 5.

1444
01:56:31,640 --> 01:56:42,640
So then we can do dot view, and that'll pretty much make that tensor again with those dimensions.

1445
01:56:42,640 --> 01:56:45,640
So then we can just print that out afterwards.

1446
01:56:45,640 --> 01:56:53,640
We go, we could print out, I don't know, print x, y, z.

1447
01:56:53,640 --> 01:56:56,640
We have 2, 3, 5.

1448
01:56:56,640 --> 01:57:00,640
Print, print a dot shape.

1449
01:57:00,640 --> 01:57:08,640
And actually, I'll print out a dot shape right here first so you can see that this actually does line up.

1450
01:57:08,640 --> 01:57:11,640
A dot shape.

1451
01:57:11,640 --> 01:57:13,640
And then down here as well.

1452
01:57:13,640 --> 01:57:15,640
Same exact thing.

1453
01:57:15,640 --> 01:57:20,640
This also view does, basically allows us to unpack with the dot shape,

1454
01:57:20,640 --> 01:57:24,640
and then we can use view to put them back together into a tensor.

1455
01:57:24,640 --> 01:57:30,640
So you might be asking, why in this notebook did we, did we have to reshape these?

1456
01:57:30,640 --> 01:57:32,640
Why do we do that?

1457
01:57:32,640 --> 01:57:38,640
Well, the answer sort of falls into what the shape needs to be here with cross entropy.

1458
01:57:38,640 --> 01:57:40,640
What does it expect?

1459
01:57:40,640 --> 01:57:43,640
What does PyTorch expect the actual shape to be?

1460
01:57:43,640 --> 01:57:51,640
So I looked at the documentation here, and it pretty much says that we want either one dimension,

1461
01:57:51,640 --> 01:57:58,640
which is channels, or 2, which is n, which I believe n is also the batch.

1462
01:57:58,640 --> 01:58:02,640
So you have n, n different blocks or batches.

1463
01:58:02,640 --> 01:58:06,640
And then you have some other dimensions here.

1464
01:58:06,640 --> 01:58:15,640
So pretty much what it's expecting is a b by c by t instead of a b by t by c,

1465
01:58:15,640 --> 01:58:19,640
which is precisely what we get out of here.

1466
01:58:19,640 --> 01:58:22,640
It's the logits dot shape is b by t by c.

1467
01:58:22,640 --> 01:58:25,640
We want it in a b by c by t.

1468
01:58:25,640 --> 01:58:30,640
So pretty much what we're doing is we're just putting this into,

1469
01:58:30,640 --> 01:58:33,640
we're just making this one parameter by multiplying those.

1470
01:58:33,640 --> 01:58:34,640
That's what's going on here.

1471
01:58:34,640 --> 01:58:37,640
And then that means the second one is going to be c.

1472
01:58:37,640 --> 01:58:43,640
So you get like a b times t equals n, and then c, just the way that it expects it, right?

1473
01:58:43,640 --> 01:58:44,640
Just like that.

1474
01:58:44,640 --> 01:58:47,640
So that's pretty much what we're doing there.

1475
01:58:47,640 --> 01:58:57,640
And a lot of the times you might get errors from passing it into a functional function in PyTorch.

1476
01:58:57,640 --> 01:59:02,640
So it's important to pay attention to how PyTorch expects the shapes to be,

1477
01:59:02,640 --> 01:59:04,640
because you're going to get errors from that.

1478
01:59:04,640 --> 01:59:07,640
And I mean, it's not very hard to reshape them.

1479
01:59:07,640 --> 01:59:12,640
You just use the dot view and dot shape and you unpack them, reshape them together.

1480
01:59:12,640 --> 01:59:19,640
It's overall pretty simple for a beginner to intermediate level projects.

1481
01:59:19,640 --> 01:59:22,640
So it shouldn't really be a trouble there, but just watch out for that,

1482
01:59:22,640 --> 01:59:27,640
because it will come back and get you if you're not aware at some point.

1483
01:59:27,640 --> 01:59:30,640
So I've added a new function here called generate,

1484
01:59:30,640 --> 01:59:33,640
and this is pretty much going to generate tokens for us.

1485
01:59:33,640 --> 01:59:39,640
So we pass an index, which is the current index or the context,

1486
01:59:39,640 --> 01:59:43,640
and then we have max new tokens, and this is passed in through here.

1487
01:59:43,640 --> 01:59:51,640
So we have our context, we make it a single zero, just the next line character.

1488
01:59:51,640 --> 01:59:56,640
And then we generate based on that, and then our max new tokens, second parameter,

1489
01:59:56,640 --> 01:59:59,640
we just make it 500 second parameter.

1490
01:59:59,640 --> 02:00:00,640
So cool.

1491
02:00:00,640 --> 02:00:02,640
What do we do inside of here?

1492
02:00:02,640 --> 02:00:15,640
We have a little loop that pretty much it generates based on the range of the max new tokens.

1493
02:00:15,640 --> 02:00:21,640
So we're going to generate max new tokens, tokens, if that makes sense.

1494
02:00:21,640 --> 02:00:28,640
Pretty much what we do is we call forward pass based on the current state of the model parameters.

1495
02:00:28,640 --> 02:00:33,640
And I want to be explicit here and say self dot forward, rather than just self index,

1496
02:00:33,640 --> 02:00:35,640
it will call self dot forward when we do this.

1497
02:00:35,640 --> 02:00:39,640
But let's just be explicit and say self dot forward here.

1498
02:00:39,640 --> 02:00:42,640
So we get the logic and the loss from this.

1499
02:00:42,640 --> 02:00:44,640
We focus on the last time step.

1500
02:00:44,640 --> 02:00:47,640
That's the only one we care about diagram language model.

1501
02:00:47,640 --> 02:00:52,640
We only care about the single previous character, only one doesn't have context before.

1502
02:00:52,640 --> 02:00:57,640
And then we apply the softmax to get probability distribution.

1503
02:00:57,640 --> 02:01:00,640
And we already went over the softmax function before.

1504
02:01:00,640 --> 02:01:07,640
The reason why we use negative one here is because we're focusing on the last dimension.

1505
02:01:07,640 --> 02:01:13,640
And in case you aren't familiar with negative indexing, which is what this is here and same with here,

1506
02:01:13,640 --> 02:01:16,640
is imagine you have a little number line.

1507
02:01:16,640 --> 02:01:21,640
It starts at index zero, one, two, three, four, five, et cetera.

1508
02:01:21,640 --> 02:01:29,640
So if you go before zero, it's just going to loop to the very end of that array.

1509
02:01:29,640 --> 02:01:34,640
So when we call negative one, it's going to do the last element, negative two,

1510
02:01:34,640 --> 02:01:37,640
second last element, negative three, third last element, et cetera.

1511
02:01:37,640 --> 02:01:39,640
So that's pretty much all this is here.

1512
02:01:39,640 --> 02:01:41,640
And you can do this for anything in Python.

1513
02:01:41,640 --> 02:01:44,640
Negative indexing is quite common.

1514
02:01:44,640 --> 02:01:47,640
So that's what we do here.

1515
02:01:47,640 --> 02:01:51,640
We apply softmax to the last dimension.

1516
02:01:51,640 --> 02:01:54,640
And then we sample from the distribution.

1517
02:01:54,640 --> 02:01:59,640
So we already went over torch dot monomial, we get one sample.

1518
02:01:59,640 --> 02:02:09,640
And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate.

1519
02:02:09,640 --> 02:02:17,640
It concatenates the previous context or the previous tokens with the newly generated one.

1520
02:02:17,640 --> 02:02:19,640
And then we just combine them together.

1521
02:02:19,640 --> 02:02:21,640
So they're one thing.

1522
02:02:21,640 --> 02:02:26,640
And we do this on a B by T plus one.

1523
02:02:26,640 --> 02:02:29,640
And if that doesn't make sense, let me help you out here.

1524
02:02:29,640 --> 02:02:34,640
So we have this time dimension, let's say we have, you know, maybe just one element here.

1525
02:02:34,640 --> 02:02:37,640
So we have something in the zero position.

1526
02:02:37,640 --> 02:02:43,640
And then whenever we generate a token, we're going to take the information from the zero position.

1527
02:02:43,640 --> 02:02:45,640
And then we're going to add one to it.

1528
02:02:45,640 --> 02:02:47,640
So it becomes a B by T.

1529
02:02:47,640 --> 02:02:51,640
Since there was only one element, the length of that was one, it is now two.

1530
02:02:51,640 --> 02:02:54,640
Then we have this two, we make it three.

1531
02:02:54,640 --> 02:02:57,640
And then we have this three, we make it four.

1532
02:02:57,640 --> 02:03:04,640
So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it.

1533
02:03:04,640 --> 02:03:08,640
And then we, you know, after this loop, we just return the index.

1534
02:03:08,640 --> 02:03:12,640
So this is all the generated tokens for max new tokens.

1535
02:03:12,640 --> 02:03:16,640
And that's pretty much what that does.

1536
02:03:16,640 --> 02:03:24,640
Model up to device here, this is just going to push our parameters to the GPU for more efficient training.

1537
02:03:24,640 --> 02:03:29,640
I'm not sure if this makes a huge difference right now because we're only doing background language modeling.

1538
02:03:29,640 --> 02:03:33,640
But yeah, it's handy to have this here.

1539
02:03:33,640 --> 02:03:37,640
And then, I mean, this is, this is pretty self explanatory here.

1540
02:03:37,640 --> 02:03:40,640
We generate based on a context.

1541
02:03:40,640 --> 02:03:44,640
This is the context, which is a single zero or a next line character.

1542
02:03:44,640 --> 02:03:47,640
We pass in our max new tokens.

1543
02:03:47,640 --> 02:03:49,640
And then we pretty much just decode this.

1544
02:03:49,640 --> 02:03:52,640
So that's how that works.

1545
02:03:52,640 --> 02:03:58,640
Let's move on to the optimizer and the training loop, the actual training process.

1546
02:03:58,640 --> 02:04:02,640
So I actually skipped something and probably left you a little bit confused.

1547
02:04:02,640 --> 02:04:12,640
But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here?

1548
02:04:12,640 --> 02:04:18,640
Because the logits only returns two dimensions, right?

1549
02:04:18,640 --> 02:04:23,640
You have a B by T, or you have a B times T by C.

1550
02:04:23,640 --> 02:04:26,640
So how exactly does this work?

1551
02:04:26,640 --> 02:04:31,640
Well, when we call this forward pass, all we're passing in is the index here.

1552
02:04:31,640 --> 02:04:35,640
So that means targets defaults to none.

1553
02:04:35,640 --> 02:04:41,640
So because targets is none, the loss is none, and this code does not execute.

1554
02:04:41,640 --> 02:04:46,640
And it just uses this logits here, which is three dimensional.

1555
02:04:46,640 --> 02:04:48,640
So that's how that works.

1556
02:04:48,640 --> 02:05:00,640
And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there.

1557
02:05:00,640 --> 02:05:04,640
And that will make sure targets is not none.

1558
02:05:04,640 --> 02:05:11,640
So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits.

1559
02:05:11,640 --> 02:05:16,640
So that's just a little clarification there, if that was confusing to anybody.

1560
02:05:16,640 --> 02:05:24,640
Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type.

1561
02:05:24,640 --> 02:05:35,640
So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes.

1562
02:05:35,640 --> 02:05:47,640
So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference

1563
02:05:47,640 --> 02:05:51,640
between float and int is float has decimals, it's a floating point number.

1564
02:05:51,640 --> 02:05:56,640
And then integers just, just a single integer doesn't, it's not really anything more than that.

1565
02:05:56,640 --> 02:06:01,640
It can just be bigger based on the amount of bits that occupies.

1566
02:06:01,640 --> 02:06:05,640
So that's just a overview on tors dot long.

1567
02:06:05,640 --> 02:06:08,640
It's the exact same thing as in 64.

1568
02:06:08,640 --> 02:06:11,640
So that's that.

1569
02:06:11,640 --> 02:06:14,640
Now we have this, we have this training loop here.

1570
02:06:14,640 --> 02:06:18,640
So we define our optimizer.

1571
02:06:18,640 --> 02:06:26,640
And I already meant over optimizers previously, Adam W, which is Adam weight decay.

1572
02:06:26,640 --> 02:06:28,640
So we have weight decay in here.

1573
02:06:28,640 --> 02:06:33,640
And then all of our model parameters, and then our learning rates.

1574
02:06:33,640 --> 02:06:36,640
So I actually wrote to learning rate up here.

1575
02:06:36,640 --> 02:06:41,640
So I would add this and then just rerun this part of the code here if you're typing along.

1576
02:06:41,640 --> 02:06:48,640
So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop.

1577
02:06:48,640 --> 02:06:55,640
And the learning rate is special, because sometimes you're learning what will be too high.

1578
02:06:55,640 --> 02:06:58,640
And some said, sometimes it'll be too low.

1579
02:06:58,640 --> 02:07:09,640
So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time.

1580
02:07:09,640 --> 02:07:15,640
So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip.

1581
02:07:15,640 --> 02:07:22,640
So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly.

1582
02:07:22,640 --> 02:07:32,640
You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that.

1583
02:07:33,640 --> 02:07:42,640
So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs.

1584
02:07:42,640 --> 02:07:47,640
So now we have this training loop here, which is going to iterate over the max iterations.

1585
02:07:47,640 --> 02:07:50,640
We just give each iteration the term iter.

1586
02:07:50,640 --> 02:07:57,640
And I don't think we use this yet, but we will later for just reporting on the loss over time.

1587
02:07:58,640 --> 02:08:06,640
But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training.

1588
02:08:06,640 --> 02:08:08,640
This is the training loop, we don't care about validation.

1589
02:08:08,640 --> 02:08:16,640
So we're going to call train on this, we're going to get some x inputs and some y targets.

1590
02:08:16,640 --> 02:08:24,640
So we go in and do a model dot forward here, we got our logits and our loss.

1591
02:08:24,640 --> 02:08:30,640
And then we're going to do our optimizer dot zero grad and I'll explain this in the second here.

1592
02:08:30,640 --> 02:08:32,640
It's a little bit confusing.

1593
02:08:32,640 --> 02:08:42,640
But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops.

1594
02:08:42,640 --> 02:08:44,640
I know I can go by this a little bit quickly.

1595
02:08:44,640 --> 02:08:50,640
But this is the standard training loop architecture for basic models.

1596
02:08:50,640 --> 02:08:53,640
And this is what it'll usually look like.

1597
02:08:53,640 --> 02:08:58,640
So you'll, you know, you'll get your data, get your inputs or outputs, whatever.

1598
02:08:58,640 --> 02:09:00,640
You'll do a forward pass.

1599
02:09:00,640 --> 02:09:03,640
You'll define some thing about the optimizer here.

1600
02:09:03,640 --> 02:09:04,640
In our case, it's your grad.

1601
02:09:04,640 --> 02:09:10,640
And then you'll have a loss dot backward, which is backward pass.

1602
02:09:10,640 --> 02:09:15,640
And the optimizer dot step, which lets gradient descent work its magic.

1603
02:09:15,640 --> 02:09:19,640
So back to optimizer dot zero grad.

1604
02:09:19,640 --> 02:09:27,640
So by default, PyTorch will accumulate the gradients over time via adding them.

1605
02:09:27,640 --> 02:09:34,640
And what we do by by putting a zero grad is we make sure that they do not add over time.

1606
02:09:34,640 --> 02:09:38,640
So the previous gradients do not affect the current one.

1607
02:09:38,640 --> 02:09:43,640
And the reason we don't want this is because previous gradients are from previous data.

1608
02:09:43,640 --> 02:09:48,640
And the data is, you know, kind of weird sometimes, sometimes it's biased.

1609
02:09:48,640 --> 02:09:55,640
And we don't want that determining, you know, how much like what our error is, right?

1610
02:09:55,640 --> 02:10:02,640
So we only want to decide, we only want to optimize based on the current gradient of our current data.

1611
02:10:02,640 --> 02:10:05,640
And this little parameter in here, we go set to none.

1612
02:10:05,640 --> 02:10:11,640
This pretty much means we're going to set, we're going to set the gradients instead of zero,

1613
02:10:11,640 --> 02:10:15,640
instead of zero gradient, we're going to set it to none.

1614
02:10:15,640 --> 02:10:20,640
And the reason why we set it to none is because none occupies a lot less space.

1615
02:10:20,640 --> 02:10:24,640
It just, yeah, just occupies a lot less space when you have a zero.

1616
02:10:24,640 --> 02:10:28,640
That's, that's probably an int 64 or something that's going to take up space.

1617
02:10:28,640 --> 02:10:34,640
And because, you know, we might have a lot of these accumulating that takes up space over time.

1618
02:10:34,640 --> 02:10:41,640
So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to.

1619
02:10:41,640 --> 02:10:46,640
And that's pretty much what that does.

1620
02:10:46,640 --> 02:10:56,640
It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets,

1621
02:10:56,640 --> 02:11:01,640
which need to understand previous context because they're recurrent.

1622
02:11:01,640 --> 02:11:10,640
I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation.

1623
02:11:10,640 --> 02:11:17,640
We'll simply take an average of all the accumulation steps and just averages the gradients together.

1624
02:11:17,640 --> 02:11:21,640
So you get a more effective, maybe block size, right?

1625
02:11:21,640 --> 02:11:23,640
You get more context that way.

1626
02:11:23,640 --> 02:11:25,640
And you can have the same batch size.

1627
02:11:25,640 --> 02:11:27,640
So just little neat tricks like that.

1628
02:11:27,640 --> 02:11:34,640
We'll talk about gradient accumulation more later in the course, but pretty much what's going on here.

1629
02:11:34,640 --> 02:11:37,640
We define an optimizer, Adam W.

1630
02:11:37,640 --> 02:11:39,640
We iterate over max editors.

1631
02:11:39,640 --> 02:11:41,640
We get a batch training split.

1632
02:11:41,640 --> 02:11:47,640
We do a forward pass, zero grad, backward pass, and then we get a step in the right direction.

1633
02:11:47,640 --> 02:11:49,640
So we're gradient descent works as magic.

1634
02:11:49,640 --> 02:11:52,640
And at the end, we could just print out the loss here.

1635
02:11:52,640 --> 02:11:54,640
So I've run this a few times.

1636
02:11:54,640 --> 02:12:00,640
And over time, I've gotten the loss of 2.55, which is okay.

1637
02:12:00,640 --> 02:12:10,640
And if we generate based on that loss, we get still pretty garbage tokens.

1638
02:12:10,640 --> 02:12:14,640
But then again, this is a background language model.

1639
02:12:14,640 --> 02:12:17,640
So actually, I might need to retrain this here.

1640
02:12:17,640 --> 02:12:19,640
It's not trained yet.

1641
02:12:19,640 --> 02:12:26,640
So what I'm actually going to do is run this, run this, run this, boom.

1642
02:12:26,640 --> 02:12:30,640
And then what I'll do, oh, it looks like we're printing out a lot of stuff here.

1643
02:12:30,640 --> 02:12:33,640
So that's coming from our get batch.

1644
02:12:33,640 --> 02:12:35,640
So I'll just comment that.

1645
02:12:35,640 --> 02:12:37,640
Or we can just delete it overall.

1646
02:12:37,640 --> 02:12:39,640
Cool.

1647
02:12:39,640 --> 02:12:48,640
And now if we run this again.

1648
02:12:48,640 --> 02:12:53,640
Give it a second.

1649
02:12:53,640 --> 02:12:56,640
Perfect.

1650
02:12:56,640 --> 02:13:01,640
So I don't know why it's still doing that.

1651
02:13:01,640 --> 02:13:06,640
If we run it again, let's see.

1652
02:13:06,640 --> 02:13:15,640
Where are we printing stuff?

1653
02:13:16,640 --> 02:13:18,640
No.

1654
02:13:18,640 --> 02:13:20,640
Ah, yes.

1655
02:13:20,640 --> 02:13:22,640
We have to run this again after changing it.

1656
02:13:22,640 --> 02:13:26,640
Silly me.

1657
02:13:26,640 --> 02:13:30,640
And of course, 10,000 steps is a lot.

1658
02:13:30,640 --> 02:13:32,640
So it takes a little while.

1659
02:13:32,640 --> 02:13:34,640
It takes a few seconds, which is actually quite quick.

1660
02:13:34,640 --> 02:13:38,640
So after the first one, we get a loss of 3.15.

1661
02:13:38,640 --> 02:13:40,640
We can generate from that.

1662
02:13:40,640 --> 02:13:42,640
And we get something that is less garbage.

1663
02:13:42,640 --> 02:13:44,640
You know, it has some next line characters.

1664
02:13:44,640 --> 02:13:46,640
It understands a little bit more to, you know,

1665
02:13:46,640 --> 02:13:48,640
space things out and whatnot.

1666
02:13:48,640 --> 02:13:52,640
So that's like slightly less garbage than before.

1667
02:13:52,640 --> 02:13:56,640
But yeah, this, this is pretty good.

1668
02:13:56,640 --> 02:13:58,640
So I lied.

1669
02:13:58,640 --> 02:14:02,640
There aren't actually any lectures previously where I talked about optimizers.

1670
02:14:02,640 --> 02:14:05,640
So might as well talk about it now.

1671
02:14:05,640 --> 02:14:07,640
So a bunch of common ones.

1672
02:14:07,640 --> 02:14:12,640
And honestly, you don't really need to know anything more than the common ones

1673
02:14:12,640 --> 02:14:16,640
because most of them are just built off of these.

1674
02:14:16,640 --> 02:14:19,640
So you have your mean squared error,

1675
02:14:19,640 --> 02:14:22,640
common loss function using regression, regression problems,

1676
02:14:22,640 --> 02:14:25,640
where it's like, you know, you have a bunch of data points,

1677
02:14:25,640 --> 02:14:27,640
find the best fit line, right?

1678
02:14:27,640 --> 02:14:29,640
That's a common regression problem.

1679
02:14:29,640 --> 02:14:31,640
Goals to predict a continuous output

1680
02:14:31,640 --> 02:14:33,640
and measures the average squared difference

1681
02:14:33,640 --> 02:14:36,640
between the predicted and actual values,

1682
02:14:36,640 --> 02:14:39,640
often used to train neural networks for regression tasks.

1683
02:14:39,640 --> 02:14:41,640
So cool.

1684
02:14:41,640 --> 02:14:43,640
That's the most basic one.

1685
02:14:43,640 --> 02:14:45,640
You can look into that more if you'd like,

1686
02:14:45,640 --> 02:14:48,640
but that's our most basic optimizer.

1687
02:14:48,640 --> 02:14:50,640
Gradient descent is a step up from that.

1688
02:14:50,640 --> 02:14:53,640
It's used to minimize the loss function in a model,

1689
02:14:53,640 --> 02:14:55,640
measures how well the model,

1690
02:14:55,640 --> 02:14:59,640
the gradient measures how well the model is able to predict

1691
02:14:59,640 --> 02:15:02,640
the target variable based on the input features.

1692
02:15:02,640 --> 02:15:04,640
So we have some input X,

1693
02:15:04,640 --> 02:15:08,640
we have some weights and biases maybe, WX plus B.

1694
02:15:08,640 --> 02:15:14,640
And all we're trying to do is make sure that the inputs

1695
02:15:14,640 --> 02:15:21,640
or make sure that we make the inputs become the desired outputs

1696
02:15:21,640 --> 02:15:25,640
and based on how far it is away from the desired outputs,

1697
02:15:25,640 --> 02:15:28,640
we can change the parameters of the model.

1698
02:15:28,640 --> 02:15:32,640
So we went over gradient descent recently or previously,

1699
02:15:32,640 --> 02:15:35,640
but that's pretty much what's going on here.

1700
02:15:35,640 --> 02:15:40,640
And momentum is just a little extension of gradient descent

1701
02:15:40,640 --> 02:15:43,640
that adds the momentum term.

1702
02:15:43,640 --> 02:15:46,640
So it helps smooth out the training

1703
02:15:46,640 --> 02:15:51,640
and allows it to continue moving in the right direction,

1704
02:15:51,640 --> 02:15:54,640
even if the gradient changes direction or varies in magnitude.

1705
02:15:54,640 --> 02:15:57,640
It's particularly useful for training deep neural nets.

1706
02:15:57,640 --> 02:16:00,640
So momentum is when you have, you know,

1707
02:16:00,640 --> 02:16:04,640
you consider some of the other gradients.

1708
02:16:04,640 --> 02:16:07,640
So you have something that's like maybe passed on from here

1709
02:16:07,640 --> 02:16:10,640
and then it might include a little bit of the current one.

1710
02:16:10,640 --> 02:16:13,640
So like 90%, like a good momentum coefficient

1711
02:16:13,640 --> 02:16:16,640
would be like 90% previous gradients

1712
02:16:16,640 --> 02:16:18,640
and then 10% of the current one.

1713
02:16:18,640 --> 02:16:23,640
So it kind of like lags behind and makes it converge sort of smoothly.

1714
02:16:23,640 --> 02:16:25,640
That makes sense.

1715
02:16:25,640 --> 02:16:27,640
Arm as prop, I've never used this,

1716
02:16:27,640 --> 02:16:31,640
but it's an algorithm that use the moving average of the squared gradient

1717
02:16:31,640 --> 02:16:33,640
to adapt learning rates of each parameter,

1718
02:16:33,640 --> 02:16:36,640
helps to avoid oscillations in the parameter updates

1719
02:16:36,640 --> 02:16:39,640
and can move and can improve convergence in some cases.

1720
02:16:39,640 --> 02:16:42,640
So you can look more into that if you'd like.

1721
02:16:42,640 --> 02:16:44,640
Adam, very popular,

1722
02:16:44,640 --> 02:16:48,640
combines the ideas of momentum and arm as prop.

1723
02:16:48,640 --> 02:16:50,640
He uses a moving average,

1724
02:16:50,640 --> 02:16:52,640
both the gradient and its squared value

1725
02:16:52,640 --> 02:16:54,640
to adapt learning rate of each parameter.

1726
02:16:54,640 --> 02:16:58,640
So often uses the default optimizer for deep learning models.

1727
02:16:58,640 --> 02:17:01,640
And in our case, when we continue to build this out,

1728
02:17:01,640 --> 02:17:04,640
it's going to be quite a deep net.

1729
02:17:04,640 --> 02:17:08,640
And Adam W is just a modification of the item optimizer

1730
02:17:08,640 --> 02:17:10,640
that adds weight decay to the parameter updates.

1731
02:17:10,640 --> 02:17:15,640
So helps to regularize and improve generalization performance.

1732
02:17:15,640 --> 02:17:19,640
Using this optimizer as it best suits the properties of the model

1733
02:17:19,640 --> 02:17:21,640
we'll train in this video.

1734
02:17:21,640 --> 02:17:24,640
So, of course, I'm reading off the script here.

1735
02:17:24,640 --> 02:17:28,640
There's no really other better way to say how these optimizers work.

1736
02:17:28,640 --> 02:17:31,640
But, yeah, if you want to look more into, you know,

1737
02:17:31,640 --> 02:17:34,640
concepts like momentum or weight decay

1738
02:17:34,640 --> 02:17:39,640
or, you know, oscillations and just some statistic stuff, you can.

1739
02:17:39,640 --> 02:17:43,640
But honestly, the only thing that really matters

1740
02:17:43,640 --> 02:17:47,640
is just knowing which optimizers are used for certain things.

1741
02:17:47,640 --> 02:17:50,640
So, like, what is the momentum used for?

1742
02:17:50,640 --> 02:17:53,640
What is Adam W great for?

1743
02:17:53,640 --> 02:17:56,640
What is MSC good for, right?

1744
02:17:56,640 --> 02:18:00,640
Just knowing what the differences and similarities are,

1745
02:18:00,640 --> 02:18:05,640
as well as when is the best case to use the optimizer.

1746
02:18:05,640 --> 02:18:10,640
So, yeah, you can find more information about that at torch.optim.

1747
02:18:10,640 --> 02:18:12,640
So when we develop language models,

1748
02:18:12,640 --> 02:18:15,640
something really important in language modeling,

1749
02:18:15,640 --> 02:18:18,640
data science, machine learning, at all,

1750
02:18:18,640 --> 02:18:20,640
is just being able to report a loss

1751
02:18:20,640 --> 02:18:23,640
or get an idea of how well our model is performing

1752
02:18:23,640 --> 02:18:25,640
over, you know, the first 1,000 iterations

1753
02:18:25,640 --> 02:18:27,640
and then the first 2,000 iterations

1754
02:18:27,640 --> 02:18:29,640
and 4,000 iterations, right?

1755
02:18:29,640 --> 02:18:31,640
So we want to get a general idea

1756
02:18:31,640 --> 02:18:33,640
of how our model is converging over time.

1757
02:18:33,640 --> 02:18:36,640
But we don't want to just print every single step of this.

1758
02:18:36,640 --> 02:18:37,640
That wouldn't make sense.

1759
02:18:37,640 --> 02:18:41,640
So what we actually could do is print every, you know,

1760
02:18:41,640 --> 02:18:43,640
200 iterations, 500.

1761
02:18:43,640 --> 02:18:45,640
We could print every 10,000 iterations

1762
02:18:45,640 --> 02:18:48,640
if you're running a crazy big language model if you wanted to.

1763
02:18:49,640 --> 02:18:51,640
And that's exactly what we're going to implement right here.

1764
02:18:51,640 --> 02:18:57,640
So, actually, this doesn't require an insane amount of Python syntax.

1765
02:18:57,640 --> 02:19:01,640
This is just, I'm actually just going to add it into our for loop here.

1766
02:19:01,640 --> 02:19:05,640
And what this is going to do is it's going to do what I just said,

1767
02:19:05,640 --> 02:19:09,640
is print every, you know, every certain number of iterations.

1768
02:19:09,640 --> 02:19:16,640
So we can add a new hyper parameter up here called eval-itters.

1769
02:19:16,640 --> 02:19:21,640
And I'm going to make this 250 just for,

1770
02:19:21,640 --> 02:19:24,640
just to make things sort of easy here.

1771
02:19:24,640 --> 02:19:28,640
And we're going to go ahead and add this in here.

1772
02:19:28,640 --> 02:19:33,640
So I'm going to go if-iter.

1773
02:19:33,640 --> 02:19:35,640
And we're going to do the module operator.

1774
02:19:35,640 --> 02:19:38,640
You can look more into this if you want later.

1775
02:19:38,640 --> 02:19:43,640
And we're going to do eval-itters equals equals zero.

1776
02:19:43,640 --> 02:19:50,640
What this is going to do is it's going to check if the current iteration

1777
02:19:50,640 --> 02:19:57,640
divided by, or sorry, if the remainder of the current iteration

1778
02:19:57,640 --> 02:20:00,640
divided by our eval-itters parameter,

1779
02:20:00,640 --> 02:20:05,640
if the remainder of that is zero, then we continue with it.

1780
02:20:05,640 --> 02:20:07,640
So hopefully that made sense.

1781
02:20:07,640 --> 02:20:12,640
If you want to, you could just ask GPT4

1782
02:20:12,640 --> 02:20:16,640
or GPT3.5, whatever you have, just this module operator,

1783
02:20:16,640 --> 02:20:19,640
and you should get a good general understanding of what it does.

1784
02:20:19,640 --> 02:20:21,640
Cool.

1785
02:20:21,640 --> 02:20:24,640
So all we can do now is we'll just say,

1786
02:20:24,640 --> 02:20:26,640
we'll just have a filler statement here.

1787
02:20:26,640 --> 02:20:31,640
We'll just do print, we've been f-string,

1788
02:20:31,640 --> 02:20:40,640
and then we'll go losses, losses, maybe that.

1789
02:20:40,640 --> 02:20:42,640
Or actually, I'm going to change this here.

1790
02:20:42,640 --> 02:20:49,640
We can go step-iter.

1791
02:20:49,640 --> 02:20:53,640
Add a little colon in there.

1792
02:20:53,640 --> 02:21:01,640
And then I'll go split.

1793
02:21:01,640 --> 02:21:07,640
Actually, I'll just go loss, and then losses like that.

1794
02:21:07,640 --> 02:21:11,640
And then we'll have some sort of put in here.

1795
02:21:11,640 --> 02:21:14,640
Something soon.

1796
02:21:14,640 --> 02:21:16,640
I don't know.

1797
02:21:16,640 --> 02:21:20,640
And all I've done is I've actually added a little function in here

1798
02:21:20,640 --> 02:21:21,640
behind the scenes.

1799
02:21:21,640 --> 02:21:23,640
You guys didn't see me do this yet.

1800
02:21:23,640 --> 02:21:28,640
But pretty much, I'm not going to go through the actual function itself,

1801
02:21:28,640 --> 02:21:32,640
but what is important is that you know this decorator right here.

1802
02:21:32,640 --> 02:21:34,640
This probably isn't very common to you.

1803
02:21:34,640 --> 02:21:36,640
This is torch.nograt.

1804
02:21:36,640 --> 02:21:39,640
And what this is going to do is it's going to make sure that

1805
02:21:39,640 --> 02:21:41,640
PyTorch doesn't use gradients at all in here.

1806
02:21:41,640 --> 02:21:43,640
That'll reduce computation.

1807
02:21:43,640 --> 02:21:45,640
It'll reduce memory usage.

1808
02:21:45,640 --> 02:21:47,640
It's just overall better for performance.

1809
02:21:47,640 --> 02:21:49,640
And because we're just reporting a loss,

1810
02:21:49,640 --> 02:21:53,640
we don't really need to do any optimizing or gradient computation here.

1811
02:21:53,640 --> 02:21:55,640
We're just getting losses.

1812
02:21:55,640 --> 02:21:57,640
We're feeding some stuff into the model.

1813
02:21:57,640 --> 02:22:01,640
We're getting a loss out of it, and we're going from there.

1814
02:22:01,640 --> 02:22:06,640
So that's pretty much what's happening with this torch.nograt.

1815
02:22:06,640 --> 02:22:11,640
And, you know, for things like, I don't know,

1816
02:22:11,640 --> 02:22:14,640
if you have other classes or other outside functions,

1817
02:22:14,640 --> 02:22:17,640
like, I mean, get batched by default isn't using this

1818
02:22:17,640 --> 02:22:20,640
because it doesn't have the model thing passed into it.

1819
02:22:20,640 --> 02:22:25,640
But estimate loss does have model pass into it right here.

1820
02:22:25,640 --> 02:22:30,640
So we just kind of want to make sure that it's not using any gradients.

1821
02:22:30,640 --> 02:22:32,640
We're going to reduce computation that way.

1822
02:22:32,640 --> 02:22:34,640
So anyways, if you want,

1823
02:22:34,640 --> 02:22:37,640
you can just take a quick readover of this,

1824
02:22:37,640 --> 02:22:40,640
and it should overall make sense.

1825
02:22:40,640 --> 02:22:44,640
Terms like .item.me are pretty common.

1826
02:22:44,640 --> 02:22:47,640
A lot of the other things here, like model, X and Y,

1827
02:22:47,640 --> 02:22:49,640
we get our logits and our loss.

1828
02:22:49,640 --> 02:22:51,640
This stuff should make sense.

1829
02:22:51,640 --> 02:22:53,640
It should be pretty straightforward.

1830
02:22:53,640 --> 02:22:56,640
And only two other things I want to touch on

1831
02:22:56,640 --> 02:22:58,640
is model.eval and model.train,

1832
02:22:58,640 --> 02:23:01,640
because you probably have not seen these yet.

1833
02:23:01,640 --> 02:23:08,640
So model.train essentially puts the model in the training mode.

1834
02:23:08,640 --> 02:23:10,640
The model learns from the data,

1835
02:23:10,640 --> 02:23:12,640
meaning the weights and biases,

1836
02:23:12,640 --> 02:23:14,640
if we have, well, sometimes you only have weights,

1837
02:23:14,640 --> 02:23:16,640
sometimes you, you know,

1838
02:23:16,640 --> 02:23:19,640
sometimes you have weights and biases, whatever it is,

1839
02:23:19,640 --> 02:23:21,640
those are updated during this phase.

1840
02:23:21,640 --> 02:23:23,640
And then some layers of the model,

1841
02:23:23,640 --> 02:23:25,640
like dropout and batch normalization,

1842
02:23:25,640 --> 02:23:28,640
which you may not be familiar with yet,

1843
02:23:28,640 --> 02:23:30,640
operate differently in training mode.

1844
02:23:30,640 --> 02:23:32,640
For example, dropout is active,

1845
02:23:32,640 --> 02:23:35,640
and what dropout does is this little hyperparameter

1846
02:23:35,640 --> 02:23:37,640
that we add up here.

1847
02:23:37,640 --> 02:23:39,640
It'll look like this.

1848
02:23:39,640 --> 02:23:41,640
Dropout would be like 0.2.

1849
02:23:41,640 --> 02:23:43,640
So pretty much what dropout does

1850
02:23:43,640 --> 02:23:46,640
is it's going to drop out random neurons in the network

1851
02:23:46,640 --> 02:23:48,640
so that we don't overfit.

1852
02:23:48,640 --> 02:23:52,640
And this is actually disabled in validation mode,

1853
02:23:52,640 --> 02:23:54,640
or eval mode.

1854
02:23:54,640 --> 02:23:58,640
So this will just help our model sort of learn better

1855
02:23:58,640 --> 02:24:00,640
when it has little, like, pieces of noise

1856
02:24:00,640 --> 02:24:03,640
and when things aren't in quite the right place

1857
02:24:03,640 --> 02:24:06,640
so that you don't have, you know, certain neurons in the network

1858
02:24:06,640 --> 02:24:10,640
taking priority and just making a lot of the heavy decisions.

1859
02:24:10,640 --> 02:24:11,640
We don't want that.

1860
02:24:11,640 --> 02:24:14,640
So dropout will just sort of help our model train better

1861
02:24:14,640 --> 02:24:18,640
by taking 20% of the neurons out, 0.2, at random.

1862
02:24:18,640 --> 02:24:20,640
And that's all dropout does.

1863
02:24:20,640 --> 02:24:23,640
So I'm just going to delete that for now.

1864
02:24:24,640 --> 02:24:27,640
And then, yeah, model that train.

1865
02:24:27,640 --> 02:24:30,640
Well, dropout is active during this phase,

1866
02:24:30,640 --> 02:24:33,640
during training, randomly turning off,

1867
02:24:33,640 --> 02:24:35,640
random neurons in the network.

1868
02:24:35,640 --> 02:24:37,640
And this is to prevent overfitting.

1869
02:24:37,640 --> 02:24:39,640
We went over overfitting earlier, I believe.

1870
02:24:39,640 --> 02:24:42,640
And as for evaluation mode,

1871
02:24:42,640 --> 02:24:46,640
evaluation mode is used when the model's being evaluated

1872
02:24:46,640 --> 02:24:48,640
or tested just like it sounds.

1873
02:24:48,640 --> 02:24:49,640
It's being trained.

1874
02:24:49,640 --> 02:24:52,640
What the other mode is being validated or tested.

1875
02:24:52,640 --> 02:24:56,640
And layers like dropout and batch normalization

1876
02:24:56,640 --> 02:24:58,640
behave differently in this mode.

1877
02:24:58,640 --> 02:25:00,640
Like dropout is turned off in the evaluation, right?

1878
02:25:00,640 --> 02:25:02,640
Because what we're actually doing

1879
02:25:02,640 --> 02:25:04,640
is we're using the entire network.

1880
02:25:04,640 --> 02:25:07,640
We want everything to be working sort of together.

1881
02:25:07,640 --> 02:25:10,640
And we want to actually see how well does it perform.

1882
02:25:10,640 --> 02:25:12,640
Training mode is when we're just, you know,

1883
02:25:12,640 --> 02:25:15,640
sampling, doing weird things to try to challenge the network

1884
02:25:15,640 --> 02:25:16,640
as we're training it.

1885
02:25:16,640 --> 02:25:19,640
And then evaluating or validation would be

1886
02:25:19,640 --> 02:25:22,640
when we just get the network in its optimal form

1887
02:25:22,640 --> 02:25:25,640
and we're trying to see how good of results it produces.

1888
02:25:25,640 --> 02:25:27,640
So that's what a val is.

1889
02:25:27,640 --> 02:25:29,640
And the reason we switched into a val here

1890
02:25:29,640 --> 02:25:32,640
is just because, well, we are testing the model.

1891
02:25:32,640 --> 02:25:34,640
We want to see, you know, how well it does

1892
02:25:34,640 --> 02:25:38,640
with any given set of data from a get batch.

1893
02:25:38,640 --> 02:25:40,640
And we don't actually need to train here.

1894
02:25:40,640 --> 02:25:43,640
If there was no training, this would not be here

1895
02:25:43,640 --> 02:25:46,640
because we would not be using any gradients.

1896
02:25:47,640 --> 02:25:50,640
So we would be using gradients if training was on.

1897
02:25:50,640 --> 02:25:53,640
Anyways, that's estimate loss for you.

1898
02:25:53,640 --> 02:25:57,640
This function is, you know, just generally good

1899
02:25:57,640 --> 02:25:59,640
to have a data science.

1900
02:25:59,640 --> 02:26:02,640
Your train and validation splits, whatnot.

1901
02:26:02,640 --> 02:26:05,640
And yeah, good for reporting.

1902
02:26:05,640 --> 02:26:07,640
You know how it is.

1903
02:26:07,640 --> 02:26:09,640
And we can go ahead and add this down here.

1904
02:26:09,640 --> 02:26:11,640
So there's something soon.

1905
02:26:11,640 --> 02:26:18,640
We'll go losses is equal to estimates loss.

1906
02:26:18,640 --> 02:26:24,640
And then we can go ahead and put a...

1907
02:26:24,640 --> 02:26:28,640
Yeah, we don't actually have to put anything in here.

1908
02:26:28,640 --> 02:26:29,640
Cool.

1909
02:26:29,640 --> 02:26:32,640
So now let's go ahead and run this.

1910
02:26:32,640 --> 02:26:34,640
Let me run from the start here.

1911
02:26:34,640 --> 02:26:38,640
Boom, boom, boom, boom, boom, boom.

1912
02:26:42,640 --> 02:26:44,640
Perfect.

1913
02:26:51,640 --> 02:26:53,640
I'm running for 10,000 iterations.

1914
02:26:53,640 --> 02:26:55,640
That's interesting.

1915
02:26:55,640 --> 02:26:57,640
Okay.

1916
02:26:57,640 --> 02:27:01,640
So, yes.

1917
02:27:01,640 --> 02:27:03,640
So what I'm going to do actually here

1918
02:27:03,640 --> 02:27:05,640
is you can see this loss part is weird.

1919
02:27:05,640 --> 02:27:08,640
So I'm actually going to change this up.

1920
02:27:08,640 --> 02:27:12,640
And I'm just going to switch it to...

1921
02:27:12,640 --> 02:27:15,640
We're going to go train loss.

1922
02:27:15,640 --> 02:27:17,640
And we're going to go losses.

1923
02:27:17,640 --> 02:27:20,640
And we're going to do the train split.

1924
02:27:20,640 --> 02:27:23,640
And then we're going to go over here

1925
02:27:23,640 --> 02:27:27,640
and just do the validation loss.

1926
02:27:27,640 --> 02:27:30,640
We can do validation or just val for short.

1927
02:27:30,640 --> 02:27:35,640
And I'm going to make it consistent here.

1928
02:27:35,640 --> 02:27:38,640
So we have a colon there, a colon here.

1929
02:27:38,640 --> 02:27:44,640
And then we just go losses and do val.

1930
02:27:44,640 --> 02:27:45,640
Cool.

1931
02:27:45,640 --> 02:27:50,640
So I'm going to reduce these max editors up here to only 1,000.

1932
02:27:50,640 --> 02:27:52,640
Run that.

1933
02:27:52,640 --> 02:27:54,640
Run this.

1934
02:27:54,640 --> 02:27:57,640
Oh, somebody did a match.

1935
02:28:06,640 --> 02:28:08,640
Okay.

1936
02:28:08,640 --> 02:28:10,640
Okay.

1937
02:28:10,640 --> 02:28:12,640
Okay.

1938
02:28:34,640 --> 02:28:36,640
Yes.

1939
02:28:36,640 --> 02:28:38,640
So what actually happened here was

1940
02:28:38,640 --> 02:28:41,640
when we were doing these little ticks,

1941
02:28:41,640 --> 02:28:44,640
what was happening is these were matching up with these.

1942
02:28:44,640 --> 02:28:47,640
And it was telling us, oh, you can't do that.

1943
02:28:47,640 --> 02:28:49,640
You can't start here and then end there

1944
02:28:49,640 --> 02:28:51,640
and have all this weird stuff.

1945
02:28:51,640 --> 02:28:53,640
Like, you can't do that.

1946
02:28:53,640 --> 02:28:56,640
So pretty much we just need to make sure that these are different.

1947
02:28:56,640 --> 02:28:58,640
So I'm going to do a double quote instead of single

1948
02:28:58,640 --> 02:29:00,640
and then double quote to finish it off.

1949
02:29:00,640 --> 02:29:03,640
And as you can see, this worked out here.

1950
02:29:03,640 --> 02:29:05,640
So I'll just run that again

1951
02:29:05,640 --> 02:29:07,640
so you guys can see what this looks like.

1952
02:29:07,640 --> 02:29:08,640
Okay.

1953
02:29:08,640 --> 02:29:11,640
Because we have, you know, a lot of decimal places.

1954
02:29:11,640 --> 02:29:16,640
So what we can actually do here is we can add in a little format

1955
02:29:16,640 --> 02:29:19,640
or a little decimal place reducer, if you call it,

1956
02:29:19,640 --> 02:29:23,640
just for, you know, so you can read it.

1957
02:29:23,640 --> 02:29:25,640
So it's not like some weird decimal number

1958
02:29:25,640 --> 02:29:27,640
and you're like, oh, does this eight matter?

1959
02:29:27,640 --> 02:29:28,640
Probably not.

1960
02:29:28,640 --> 02:29:30,640
Just like the first three digits, maybe.

1961
02:29:30,640 --> 02:29:33,640
So all we can do here is just add in,

1962
02:29:33,640 --> 02:29:35,640
I believe this is how it goes.

1963
02:29:37,640 --> 02:29:40,640
I don't think it's the other way.

1964
02:29:40,640 --> 02:29:42,640
We'll find out.

1965
02:29:42,640 --> 02:29:46,640
Some stuff in Python is extremely confusing to me.

1966
02:29:46,640 --> 02:29:49,640
But there we go.

1967
02:29:49,640 --> 02:29:50,640
So I got it right.

1968
02:29:50,640 --> 02:29:52,640
Go on and then period.

1969
02:29:52,640 --> 02:29:54,640
And as you can see, we have those digits reduced.

1970
02:29:54,640 --> 02:29:57,640
So I can actually put this down to 3F.

1971
02:30:03,640 --> 02:30:04,640
Wonderful.

1972
02:30:04,640 --> 02:30:09,640
So we have our train loss and our validation loss.

1973
02:30:09,640 --> 02:30:11,640
Great job you made it this far.

1974
02:30:11,640 --> 02:30:13,640
This is absolutely amazing.

1975
02:30:13,640 --> 02:30:14,640
This is insane.

1976
02:30:14,640 --> 02:30:16,640
You've gotten this far in the video.

1977
02:30:16,640 --> 02:30:19,640
We've covered all the basics, everything you need to know

1978
02:30:19,640 --> 02:30:22,640
about background language models, optimizers,

1979
02:30:22,640 --> 02:30:25,640
training loops, reporting losses.

1980
02:30:25,640 --> 02:30:28,640
I can't even name everything we've done because it's so much.

1981
02:30:28,640 --> 02:30:31,640
So congratulations that you made it this far.

1982
02:30:31,640 --> 02:30:33,640
You should go take a quick break.

1983
02:30:33,640 --> 02:30:37,640
Give yourself a pat on the back and get ready for the next part

1984
02:30:37,640 --> 02:30:39,640
here because it's going to be absolutely insane.

1985
02:30:39,640 --> 02:30:43,640
We're going to dig into literally state of the art language

1986
02:30:43,640 --> 02:30:47,640
models and how we can build them from scratch,

1987
02:30:47,640 --> 02:30:49,640
or at least how we can pre-train them.

1988
02:30:49,640 --> 02:30:52,640
And some of these terms are going to seem a little bit out

1989
02:30:52,640 --> 02:30:56,640
there, but I can ensure you by the end of this next section

1990
02:30:56,640 --> 02:31:00,640
here, you're going to have a pretty good understanding

1991
02:31:00,640 --> 02:31:03,640
about the state of language models right now.

1992
02:31:03,640 --> 02:31:07,640
So go take a quick break and I'll see you back in a little bit.

1993
02:31:07,640 --> 02:31:11,640
So there's something I'd like to clear up and actually sort of

1994
02:31:11,640 --> 02:31:15,640
lied to you a little bit, a little while back in this course

1995
02:31:15,640 --> 02:31:17,640
about what normalizing is.

1996
02:31:17,640 --> 02:31:21,640
So I recall we were talking about the softmax function

1997
02:31:21,640 --> 02:31:24,640
and normalizing vectors.

1998
02:31:24,640 --> 02:31:29,640
So the softmax is definitely a form of normalization,

1999
02:31:29,640 --> 02:31:31,640
but there are many forms.

2000
02:31:31,640 --> 02:31:35,640
There are not just a few or like there's not just one or two normalizations.

2001
02:31:35,640 --> 02:31:40,640
There are actually many of them and I have them on my second monitor here,

2002
02:31:40,640 --> 02:31:44,640
but I don't want to just dump that library of information on your head

2003
02:31:44,640 --> 02:31:46,640
because that's not how you learn.

2004
02:31:46,640 --> 02:31:49,640
So what we're going to do is we're going to plug this into GPT-4.

2005
02:31:50,640 --> 02:32:06,640
I'm going to say, can you list all the forms of normalizing in machine learning?

2006
02:32:06,640 --> 02:32:16,640
And how are they different from one another?

2007
02:32:16,640 --> 02:32:18,640
GPT-4 is a great tool.

2008
02:32:18,640 --> 02:32:22,640
If you don't already use it, I highly suggest you use it,

2009
02:32:22,640 --> 02:32:26,640
or even GPT-3.5, which is the free version.

2010
02:32:26,640 --> 02:32:30,640
But yeah, it's a great tool for just quickly learning anything

2011
02:32:30,640 --> 02:32:35,640
and then have it give you example practice questions with answers

2012
02:32:35,640 --> 02:32:38,640
so you can learn topics in literally minutes

2013
02:32:38,640 --> 02:32:43,640
that would take you several lectures to learn in a university course.

2014
02:32:43,640 --> 02:32:47,640
But anyways, there's a few here.

2015
02:32:47,640 --> 02:32:51,640
So min-max normalization, yep.

2016
02:32:51,640 --> 02:32:54,640
z-score, decimal scaling, mean normalization,

2017
02:32:54,640 --> 02:32:59,640
unit vector, or layer 2, robust scaling, power transformations.

2018
02:32:59,640 --> 02:33:00,640
Okay.

2019
02:33:00,640 --> 02:33:03,640
So yeah, and then softmax would be another one.

2020
02:33:03,640 --> 02:33:09,640
What about softmax?

2021
02:33:09,640 --> 02:33:11,640
It is in data type normalization,

2022
02:33:11,640 --> 02:33:19,640
but it's not typically using from normalizing input data.

2023
02:33:19,640 --> 02:33:21,640
It's commonly used in the output layer.

2024
02:33:21,640 --> 02:33:24,640
So softmax is a type of normalization,

2025
02:33:24,640 --> 02:33:29,640
but it's not used for normalizing input data.

2026
02:33:29,640 --> 02:33:36,640
And honestly, we've proved that here by actually producing some probabilities.

2027
02:33:36,640 --> 02:33:39,640
So this isn't something we used in our forward pass.

2028
02:33:39,640 --> 02:33:41,640
This is something we used in our generate function

2029
02:33:41,640 --> 02:33:44,640
to get a bunch of probabilities from our logits.

2030
02:33:44,640 --> 02:33:47,640
So this is, yeah, interesting.

2031
02:33:47,640 --> 02:33:50,640
It's good to just figure little things like these out for, you know,

2032
02:33:50,640 --> 02:33:54,640
just to be, put you on the edge a little bit more

2033
02:33:54,640 --> 02:33:57,640
for the future when it comes to engineering these kind of things.

2034
02:33:57,640 --> 02:33:59,640
All right, great.

2035
02:33:59,640 --> 02:34:02,640
So the next thing I want to touch on is activation functions.

2036
02:34:02,640 --> 02:34:06,640
And activation functions are extremely important

2037
02:34:06,640 --> 02:34:12,640
in offering new ways of changing our inputs that are not linear.

2038
02:34:12,640 --> 02:34:16,640
So, for example, if we were to have a bunch of linear layers,

2039
02:34:16,640 --> 02:34:19,640
a bunch of, let me erase this,

2040
02:34:19,640 --> 02:34:23,640
if we were to have a bunch of, you know,

2041
02:34:23,640 --> 02:34:26,640
nn.linears in a row,

2042
02:34:26,640 --> 02:34:30,640
what would actually happen is they would all just, you know,

2043
02:34:30,640 --> 02:34:32,640
they would all squeeze together

2044
02:34:32,640 --> 02:34:37,640
and essentially apply one transformation that sums up all of them kind of.

2045
02:34:37,640 --> 02:34:41,640
They all sort of multiply together and it gives us one transformation

2046
02:34:41,640 --> 02:34:45,640
that is kind of just a waste of computation

2047
02:34:45,640 --> 02:34:49,640
because let's say you have 100 of these nn.linear layers

2048
02:34:49,640 --> 02:34:52,640
and nothing else.

2049
02:34:52,640 --> 02:34:54,640
You're essentially going from inputs to outputs,

2050
02:34:54,640 --> 02:34:59,640
but you're doing 100 times the computation for just one multiplication.

2051
02:34:59,640 --> 02:35:01,640
That doesn't really make sense.

2052
02:35:01,640 --> 02:35:06,640
So what can we do to actually make these deep neural networks important

2053
02:35:06,640 --> 02:35:10,640
and what can we offer that's more than just linear transformations?

2054
02:35:10,640 --> 02:35:13,640
Well, that's where activation functions come in

2055
02:35:13,640 --> 02:35:16,640
and I'm going to go over these in a quick second here.

2056
02:35:16,640 --> 02:35:19,640
So let's go navigate over to the PyTorch docs.

2057
02:35:19,640 --> 02:35:23,640
So the three activation functions I'm going to cover

2058
02:35:23,640 --> 02:35:27,640
in this little part of the video are the relu, the sigmoid,

2059
02:35:27,640 --> 02:35:29,640
and the tanh activation functions.

2060
02:35:29,640 --> 02:35:35,640
So let's start off with the relu or rectified linear unit.

2061
02:35:35,640 --> 02:35:37,640
So we're going to use functional relu

2062
02:35:37,640 --> 02:35:40,640
and the reason why we're not just going to use torch.n

2063
02:35:40,640 --> 02:35:43,640
is because we're not doing any forward passes here.

2064
02:35:43,640 --> 02:35:47,640
I'm just going to add these into our,

2065
02:35:47,640 --> 02:35:52,640
I'm going to add these, let me clear this, clear this output.

2066
02:35:52,640 --> 02:35:53,640
That's fine.

2067
02:35:53,640 --> 02:35:56,640
I'm actually going to add these into here and there's no forward pass.

2068
02:35:56,640 --> 02:35:58,640
We're just going to simply run them through a function

2069
02:35:58,640 --> 02:36:01,640
and get an output just so we can see what it looks like.

2070
02:36:01,640 --> 02:36:06,640
So I've actually added this up here from torch.n

2071
02:36:06,640 --> 02:36:08,640
and import functional as capital F.

2072
02:36:08,640 --> 02:36:12,640
It's just kind of a common PyTorch practice, capital F.

2073
02:36:12,640 --> 02:36:17,640
And let's go ahead and start off with the relu here.

2074
02:36:17,640 --> 02:36:25,640
So we can go, I don't know, x equals torch.tensor

2075
02:36:25,640 --> 02:36:31,640
and then we'll make it a negative 0.05, for example.

2076
02:36:31,640 --> 02:36:38,640
And then we'll go dtype equals torch.flurp32

2077
02:36:38,640 --> 02:36:45,640
and we can go y equals f.relu of x.

2078
02:36:45,640 --> 02:36:52,640
And then we'll go ahead and print y.

2079
02:36:52,640 --> 02:36:54,640
It has no attribute relu.

2080
02:36:54,640 --> 02:36:56,640
Okay, let's try nn then.

2081
02:36:56,640 --> 02:37:01,640
Let's try nn and see if that works.

2082
02:37:01,640 --> 02:37:04,640
Okay, well that didn't work and that's fine

2083
02:37:04,640 --> 02:37:07,640
because we can simply take a look at this

2084
02:37:07,640 --> 02:37:10,640
and it'll help us understand.

2085
02:37:10,640 --> 02:37:12,640
We don't actually need to,

2086
02:37:12,640 --> 02:37:13,640
we don't need to write this out in code

2087
02:37:13,640 --> 02:37:15,640
as long as it sort of makes sense.

2088
02:37:15,640 --> 02:37:17,640
We don't need to write this in the forward pass, really.

2089
02:37:17,640 --> 02:37:19,640
You're not going to use it anywhere else.

2090
02:37:19,640 --> 02:37:22,640
So yeah, I'm not going to be too discouraged

2091
02:37:22,640 --> 02:37:26,640
that that does not work in the functional library.

2092
02:37:26,640 --> 02:37:29,640
But yeah, so pretty much what this does

2093
02:37:29,640 --> 02:37:31,640
is if a number is below,

2094
02:37:31,640 --> 02:37:34,640
if a number is 0 or below 0,

2095
02:37:34,640 --> 02:37:36,640
it will turn that number into 0.

2096
02:37:36,640 --> 02:37:40,640
And then if it's above 0, it'll stay the same.

2097
02:37:40,640 --> 02:37:43,640
So this graph sort of helps you visualize that.

2098
02:37:43,640 --> 02:37:45,640
There's a little function here.

2099
02:37:45,640 --> 02:37:47,640
That might make sense to some people.

2100
02:37:47,640 --> 02:37:49,640
I don't really care about the functions too much

2101
02:37:49,640 --> 02:37:52,640
as long as I can sort of visualize what the function means,

2102
02:37:52,640 --> 02:37:55,640
what it does, what are some applications it can be used.

2103
02:37:55,640 --> 02:37:59,640
That usually covers enough for like any function at all.

2104
02:37:59,640 --> 02:38:02,640
So that's the Relu function.

2105
02:38:02,640 --> 02:38:03,640
Pretty cool.

2106
02:38:03,640 --> 02:38:07,640
It simply offers a non-linearity to our linear networks.

2107
02:38:07,640 --> 02:38:09,640
So if you have 100 layers deep

2108
02:38:09,640 --> 02:38:11,640
and every, I don't know,

2109
02:38:11,640 --> 02:38:14,640
every second step you put a Relu,

2110
02:38:14,640 --> 02:38:16,640
that network is going to learn a lot more things.

2111
02:38:16,640 --> 02:38:19,640
It's going to learn a lot more linearity, non-linearity.

2112
02:38:19,640 --> 02:38:22,640
Then if you were to just have 100 layers

2113
02:38:22,640 --> 02:38:25,640
multiplying all into one transformation.

2114
02:38:25,640 --> 02:38:27,640
So that's what that is.

2115
02:38:27,640 --> 02:38:28,640
That's the Relu.

2116
02:38:28,640 --> 02:38:30,640
Now let's go over to Sigmoid.

2117
02:38:30,640 --> 02:38:34,640
So here we can actually use the functional library.

2118
02:38:34,640 --> 02:38:38,640
And all Sigmoid does is we go 1 over 1 plus

2119
02:38:38,640 --> 02:38:41,640
exponentiated of negative x.

2120
02:38:41,640 --> 02:38:44,640
So I'm going to add that here.

2121
02:38:44,640 --> 02:38:47,640
We could, yeah, why not do that?

2122
02:38:47,640 --> 02:38:51,640
Negative 0.05 float 32.

2123
02:38:51,640 --> 02:38:52,640
Sure.

2124
02:38:52,640 --> 02:38:56,640
We'll go f dot Sigmoid.

2125
02:38:56,640 --> 02:38:59,640
And then we'll just go x and then we'll print y.

2126
02:38:59,640 --> 02:39:00,640
Cool.

2127
02:39:00,640 --> 02:39:04,640
So we get a tensor 0.4875.

2128
02:39:04,640 --> 02:39:05,640
Interesting.

2129
02:39:05,640 --> 02:39:09,640
So this little negative 0.05 here

2130
02:39:09,640 --> 02:39:13,640
is essentially being plugged into this negative x.

2131
02:39:13,640 --> 02:39:21,640
So 1 over 1 plus 2.71 to the power of negative 0.05.

2132
02:39:21,640 --> 02:39:23,640
So it's essentially,

2133
02:39:23,640 --> 02:39:33,640
if we do 2.71, 2.71 to the power of negative 0.05,

2134
02:39:33,640 --> 02:39:35,640
we're just going to get positive.

2135
02:39:35,640 --> 02:39:40,640
So 1.05 and then 1 plus that.

2136
02:39:40,640 --> 02:39:44,640
So that's 2.05.

2137
02:39:44,640 --> 02:39:46,640
We just do 1 over that.

2138
02:39:46,640 --> 02:39:47,640
2.05.

2139
02:39:47,640 --> 02:39:50,640
So we get about 0.487.

2140
02:39:50,640 --> 02:39:53,640
And what do we get here?

2141
02:39:53,640 --> 02:39:54,640
0.4 at 7.

2142
02:39:54,640 --> 02:39:56,640
Cool.

2143
02:39:56,640 --> 02:39:57,640
So that's interesting.

2144
02:39:57,640 --> 02:40:00,640
And let's actually look, is there a graph here?

2145
02:40:00,640 --> 02:40:05,640
Let's look at the Sigmoid activation function.

2146
02:40:05,640 --> 02:40:06,640
Wikipedia.

2147
02:40:06,640 --> 02:40:08,640
Don't get too scared by this math here.

2148
02:40:08,640 --> 02:40:09,640
I don't like it either,

2149
02:40:09,640 --> 02:40:13,640
but I like the graphs they're cool to look at.

2150
02:40:13,640 --> 02:40:15,640
So this is pretty much what it's doing here.

2151
02:40:15,640 --> 02:40:19,640
So yeah, it's just a little curve.

2152
02:40:19,640 --> 02:40:23,640
Kind of looks like a, it's kind of just like a wave,

2153
02:40:23,640 --> 02:40:26,640
but it's cool looking.

2154
02:40:26,640 --> 02:40:28,640
That's what the Sigmoid function does.

2155
02:40:28,640 --> 02:40:31,640
It's used to just generalize over this line.

2156
02:40:31,640 --> 02:40:36,640
And yeah, Sigmoid function is pretty cool.

2157
02:40:36,640 --> 02:40:38,640
So now let's move on to the tanh.

2158
02:40:38,640 --> 02:40:40,640
The tanh function.

2159
02:40:40,640 --> 02:40:43,640
Google Bing is, or Microsoft Bing is giving me

2160
02:40:43,640 --> 02:40:44,640
a nice description of that.

2161
02:40:44,640 --> 02:40:46,640
Cool.

2162
02:40:46,640 --> 02:40:47,640
Perfect.

2163
02:40:47,640 --> 02:40:48,640
E to the negative x.

2164
02:40:48,640 --> 02:40:51,640
I like that.

2165
02:40:51,640 --> 02:40:53,640
So tanh is a little bit different.

2166
02:40:53,640 --> 02:40:56,640
There's a lot more exponentiating going on here.

2167
02:40:56,640 --> 02:41:01,640
So you have, I'll just say expo or exp of x

2168
02:41:01,640 --> 02:41:03,640
minus exp of negative x

2169
02:41:03,640 --> 02:41:06,640
divided by exp of x plus exp of negative x.

2170
02:41:06,640 --> 02:41:09,640
There's a lot of positives and negatives in here.

2171
02:41:09,640 --> 02:41:13,640
Positive, positive, negative, negative, negative, positive.

2172
02:41:13,640 --> 02:41:16,640
So that's interesting.

2173
02:41:16,640 --> 02:41:19,640
Let's go ahead and put this into code here.

2174
02:41:19,640 --> 02:41:23,640
So I'll go torch dot examples, or torch examples.

2175
02:41:23,640 --> 02:41:25,640
This is our file here.

2176
02:41:25,640 --> 02:41:30,640
And I'll just go tanh.

2177
02:41:30,640 --> 02:41:32,640
Cool.

2178
02:41:32,640 --> 02:41:35,640
So negative 0.05.

2179
02:41:35,640 --> 02:41:36,640
Cool.

2180
02:41:36,640 --> 02:41:37,640
What if we do a one?

2181
02:41:37,640 --> 02:41:41,640
What will that produce?

2182
02:41:41,640 --> 02:41:45,640
Oh, 0.76.

2183
02:41:45,640 --> 02:41:50,640
What if we do a 10?

2184
02:41:50,640 --> 02:41:51,640
1.0.

2185
02:41:51,640 --> 02:41:53,640
Interesting.

2186
02:41:53,640 --> 02:41:56,640
So this is sort of similar to the sigmoid,

2187
02:41:56,640 --> 02:41:58,640
except it's, you know,

2188
02:41:58,640 --> 02:42:05,640
it's actually asked to attach a BT what the difference is.

2189
02:42:06,640 --> 02:42:13,640
When would you use tanh over sigmoid?

2190
02:42:13,640 --> 02:42:16,640
Let's see here.

2191
02:42:16,640 --> 02:42:20,640
Sigmoid function and hyperbolic tangent or tanh function

2192
02:42:20,640 --> 02:42:23,640
are activations functions used in neural networks.

2193
02:42:23,640 --> 02:42:25,640
They have a similar s-shaped curve,

2194
02:42:25,640 --> 02:42:27,640
but have different ranges.

2195
02:42:27,640 --> 02:42:30,640
So sigmoid output values between a 0 and a 1

2196
02:42:30,640 --> 02:42:33,640
while tanh is between a negative 1 and a 1.

2197
02:42:33,640 --> 02:42:35,640
So if you're, you know,

2198
02:42:35,640 --> 02:42:38,640
if you're rating maybe the,

2199
02:42:38,640 --> 02:42:42,640
maybe if you're getting a probability distribution,

2200
02:42:42,640 --> 02:42:46,640
for example, you want it to be between 0 and 1,

2201
02:42:46,640 --> 02:42:48,640
meaning percentages or decimal places.

2202
02:42:48,640 --> 02:42:53,640
So like a 0.5 would be 50%, 0.87 would be 87%.

2203
02:42:53,640 --> 02:42:56,640
And that's what the sigmoid function does.

2204
02:42:56,640 --> 02:42:59,640
It's quite close to the softmax function, actually.

2205
02:42:59,640 --> 02:43:02,640
Except the softmax just, you know,

2206
02:43:02,640 --> 02:43:05,640
it prioritizes the bigger values

2207
02:43:05,640 --> 02:43:08,640
and puts the smaller values to our priority.

2208
02:43:08,640 --> 02:43:10,640
That's all the softmax says.

2209
02:43:10,640 --> 02:43:12,640
It's kind of a sigmoid on steroids.

2210
02:43:12,640 --> 02:43:16,640
And the tanh outputs between negative 1 and 1.

2211
02:43:16,640 --> 02:43:20,640
So, yeah, you could maybe even start theorycrafting

2212
02:43:20,640 --> 02:43:23,640
and thinking of some ways you could use

2213
02:43:23,640 --> 02:43:26,640
even the tanh function and sigmoid in different use cases.

2214
02:43:26,640 --> 02:43:29,640
So that's kind of a general overview on those.

2215
02:43:29,640 --> 02:43:31,640
So biogram language models are finished.

2216
02:43:31,640 --> 02:43:33,640
All of this we finished here is now done.

2217
02:43:33,640 --> 02:43:35,640
You're back from your break.

2218
02:43:35,640 --> 02:43:38,640
If you took one, if you didn't, that's fine too.

2219
02:43:38,640 --> 02:43:41,640
But pretty much we're going to dig into

2220
02:43:41,640 --> 02:43:44,640
the transformer architecture now.

2221
02:43:44,640 --> 02:43:47,640
And we're actually going to build it from scratch.

2222
02:43:47,640 --> 02:43:50,640
So there was recently a paper proposed

2223
02:43:50,640 --> 02:43:53,640
called the transformer model.

2224
02:43:53,640 --> 02:43:57,640
And this uses a mechanism called self-attention.

2225
02:43:57,640 --> 02:44:00,640
Self-attention is used in these multi-head attention,

2226
02:44:00,640 --> 02:44:02,640
little bricks here.

2227
02:44:02,640 --> 02:44:05,640
And there's a lot that happens.

2228
02:44:05,640 --> 02:44:07,640
So there's something I want to clarify

2229
02:44:07,640 --> 02:44:09,640
before we jump right into this architecture

2230
02:44:09,640 --> 02:44:12,640
and just dump a bunch of information

2231
02:44:12,640 --> 02:44:15,640
on your poor little brain right now.

2232
02:44:15,640 --> 02:44:18,640
But a lot of these networks, at first,

2233
02:44:18,640 --> 02:44:21,640
can be extremely confusing to beginners.

2234
02:44:21,640 --> 02:44:23,640
So I want to make it clear.

2235
02:44:23,640 --> 02:44:26,640
It's perfectly okay if you don't understand this at first.

2236
02:44:26,640 --> 02:44:29,640
I'm going to try to explain this in the best way possible.

2237
02:44:29,640 --> 02:44:31,640
Believe me, I've seen tons of videos

2238
02:44:31,640 --> 02:44:34,640
on people explaining the transformer architecture.

2239
02:44:34,640 --> 02:44:37,640
And all of them have been, to some degree,

2240
02:44:37,640 --> 02:44:39,640
a bit confusing to me as well.

2241
02:44:39,640 --> 02:44:41,640
So I'm going to try to clarify

2242
02:44:41,640 --> 02:44:46,640
all those little pieces of confusion.

2243
02:44:46,640 --> 02:44:48,640
Like what does that mean?

2244
02:44:48,640 --> 02:44:49,640
You didn't cover that piece.

2245
02:44:49,640 --> 02:44:51,640
I don't know what's going on here.

2246
02:44:51,640 --> 02:44:53,640
I'm going to cover all those little bits

2247
02:44:53,640 --> 02:44:56,640
and make sure that nothing is left behind.

2248
02:44:56,640 --> 02:44:58,640
So you're going to want to sit tight

2249
02:44:58,640 --> 02:45:01,640
and pay attention for this next part here.

2250
02:45:01,640 --> 02:45:04,640
So yeah, let's go ahead and dive into

2251
02:45:04,640 --> 02:45:07,640
just the general transformer architecture

2252
02:45:07,640 --> 02:45:09,640
and why it's important.

2253
02:45:09,640 --> 02:45:11,640
So in the transformer network,

2254
02:45:11,640 --> 02:45:14,640
you have a lot of computation going on.

2255
02:45:14,640 --> 02:45:17,640
You have some adding and normalizing.

2256
02:45:17,640 --> 02:45:19,640
You have some multi-hat attention.

2257
02:45:19,640 --> 02:45:21,640
You have some feed forward networks.

2258
02:45:21,640 --> 02:45:23,640
There's a lot going on here.

2259
02:45:23,640 --> 02:45:25,640
There's a lot of computation, a lot of multiplying,

2260
02:45:25,640 --> 02:45:27,640
there's a lot going on.

2261
02:45:27,640 --> 02:45:29,640
So the question I actually had at first was,

2262
02:45:29,640 --> 02:45:32,640
well, if you're just multiplying these inputs

2263
02:45:32,640 --> 02:45:34,640
by a bunch of different things along,

2264
02:45:34,640 --> 02:45:37,640
you should just end up with some random value at the end

2265
02:45:37,640 --> 02:45:40,640
that maybe doesn't really mean that much

2266
02:45:40,640 --> 02:45:42,640
of the initial input.

2267
02:45:42,640 --> 02:45:44,640
And that's actually correct.

2268
02:45:44,640 --> 02:45:47,640
For the first few iterations,

2269
02:45:47,640 --> 02:45:49,640
the model has absolutely no context

2270
02:45:49,640 --> 02:45:50,640
as to what's going on.

2271
02:45:50,640 --> 02:45:51,640
It is clueless.

2272
02:45:51,640 --> 02:45:53,640
It is going in random directions

2273
02:45:53,640 --> 02:45:57,640
and it's just trying to find the best way to converge.

2274
02:45:57,640 --> 02:45:59,640
So this is what machine learning and deep learning

2275
02:45:59,640 --> 02:46:01,640
is actually all about,

2276
02:46:01,640 --> 02:46:04,640
is having all these little parameters in,

2277
02:46:04,640 --> 02:46:06,640
you know, the adding and normalizing,

2278
02:46:06,640 --> 02:46:09,640
the feed forward networks, even multi-hat attention.

2279
02:46:09,640 --> 02:46:12,640
We're trying to optimize the parameters

2280
02:46:12,640 --> 02:46:15,640
for producing an output that is meaningful

2281
02:46:15,640 --> 02:46:17,640
that will actually help us produce

2282
02:46:17,640 --> 02:46:21,640
almost perfectly like English text.

2283
02:46:21,640 --> 02:46:23,640
And so this is the entire process of pre-training.

2284
02:46:23,640 --> 02:46:26,640
You send a bunch of inputs into a transformer

2285
02:46:26,640 --> 02:46:29,640
and you get some output probabilities

2286
02:46:29,640 --> 02:46:31,640
that you used to generate from.

2287
02:46:31,640 --> 02:46:34,640
And what attention does

2288
02:46:34,640 --> 02:46:37,640
is it sets little different scores

2289
02:46:37,640 --> 02:46:41,640
to, you know, each little token in a sentence.

2290
02:46:41,640 --> 02:46:44,640
For tokens you have character, subword,

2291
02:46:44,640 --> 02:46:46,640
and word-level tokens.

2292
02:46:46,640 --> 02:46:49,640
So you're pretty much just mapping

2293
02:46:49,640 --> 02:46:51,640
bits of attention to each of these,

2294
02:46:51,640 --> 02:46:53,640
as well as, you know,

2295
02:46:53,640 --> 02:46:56,640
what is the position also mean as well.

2296
02:46:56,640 --> 02:47:00,640
So you could have two words that are right next to each other,

2297
02:47:00,640 --> 02:47:02,640
but then if you don't actually, you know,

2298
02:47:02,640 --> 02:47:04,640
positionally encode them,

2299
02:47:04,640 --> 02:47:06,640
it doesn't really mean much,

2300
02:47:06,640 --> 02:47:09,640
because it's like, oh, these could be like 4,000 characters apart.

2301
02:47:09,640 --> 02:47:11,640
So that's why you need both

2302
02:47:11,640 --> 02:47:14,640
to put attention scores on these tokens

2303
02:47:14,640 --> 02:47:17,640
and to positionally encode them.

2304
02:47:17,640 --> 02:47:19,640
And that's what's happening here.

2305
02:47:19,640 --> 02:47:24,640
So what we do is we get to our inputs.

2306
02:47:24,640 --> 02:47:26,640
We got our inputs.

2307
02:47:26,640 --> 02:47:28,640
So, I mean, we went over this with

2308
02:47:28,640 --> 02:47:30,640
diagram language models.

2309
02:47:30,640 --> 02:47:33,640
We feed our X and Y,

2310
02:47:33,640 --> 02:47:35,640
so X would be our inputs,

2311
02:47:35,640 --> 02:47:38,640
Y would be our targets or outputs.

2312
02:47:38,640 --> 02:47:41,640
And what we're going to do

2313
02:47:41,640 --> 02:47:44,640
is give these little embeddings.

2314
02:47:44,640 --> 02:47:47,640
So I believe we went over embeddings a little while ago,

2315
02:47:47,640 --> 02:47:49,640
and pretty much what those mean

2316
02:47:49,640 --> 02:47:51,640
is it's going to have a little row

2317
02:47:51,640 --> 02:47:53,640
for each token on that table,

2318
02:47:53,640 --> 02:47:55,640
and that's going to store, you know,

2319
02:47:55,640 --> 02:47:58,640
some vector as to what that token means.

2320
02:47:58,640 --> 02:48:01,640
So let's say you had, like, you know,

2321
02:48:01,640 --> 02:48:04,640
the character E, for example,

2322
02:48:04,640 --> 02:48:09,640
the sentiment or the vector of the character E

2323
02:48:09,640 --> 02:48:11,640
is probably going to be vastly different

2324
02:48:11,640 --> 02:48:13,640
than the sentiment of Z, right?

2325
02:48:13,640 --> 02:48:15,640
Because E is a very common vowel,

2326
02:48:15,640 --> 02:48:17,640
and Z is one of the most uncommon,

2327
02:48:17,640 --> 02:48:21,640
if not the most uncommon letter in the English language.

2328
02:48:21,640 --> 02:48:24,640
So these embeddings are learned.

2329
02:48:24,640 --> 02:48:27,640
We have these both for our inputs and our outputs.

2330
02:48:27,640 --> 02:48:29,640
We give them positional encodings

2331
02:48:29,640 --> 02:48:31,640
like I was talking about,

2332
02:48:31,640 --> 02:48:33,640
and there's ways we can do that.

2333
02:48:33,640 --> 02:48:36,640
We can actually use learnable parameters

2334
02:48:36,640 --> 02:48:38,640
to assign these encodings.

2335
02:48:38,640 --> 02:48:41,640
A lot of these are learnable parameters, by the way,

2336
02:48:41,640 --> 02:48:43,640
and you'll see that as you, you know,

2337
02:48:43,640 --> 02:48:45,640
delve more and more into transformers.

2338
02:48:45,640 --> 02:48:50,640
But, yeah, so after we've given these inputs,

2339
02:48:50,640 --> 02:48:52,640
embeddings, and positional encodings,

2340
02:48:52,640 --> 02:48:54,640
and same thing with the outputs,

2341
02:48:54,640 --> 02:48:56,640
which are essentially just shifted right,

2342
02:48:56,640 --> 02:48:59,640
you have, you know, I up to block size for inputs,

2343
02:48:59,640 --> 02:49:04,640
and then I plus one up to block size plus one, right?

2344
02:49:04,640 --> 02:49:08,640
Or whatever little thing we employed here

2345
02:49:08,640 --> 02:49:10,640
in our background language models.

2346
02:49:10,640 --> 02:49:12,640
Quite what it was.

2347
02:49:12,640 --> 02:49:14,640
Or even if we did that at all.

2348
02:49:18,640 --> 02:49:20,640
No.

2349
02:49:20,640 --> 02:49:22,640
I'm just speaking gibberish right now,

2350
02:49:22,640 --> 02:49:24,640
but that's fine because it's going to make sense

2351
02:49:24,640 --> 02:49:26,640
in a little bit here.

2352
02:49:28,640 --> 02:49:31,640
So what I'm going to actually do

2353
02:49:31,640 --> 02:49:33,640
is I'm not going to read off of this right here

2354
02:49:33,640 --> 02:49:35,640
because this is really confusing.

2355
02:49:35,640 --> 02:49:38,640
So I'm going to switch over to a little,

2356
02:49:39,640 --> 02:49:42,640
like a little sketch that I drew out.

2357
02:49:42,640 --> 02:49:44,640
And this is pretty much the entire transformer

2358
02:49:44,640 --> 02:49:46,640
with a lot of other things considered

2359
02:49:46,640 --> 02:49:50,640
that this initial image does not really put into perspective.

2360
02:49:50,640 --> 02:49:53,640
So let's go ahead and jump into

2361
02:49:53,640 --> 02:49:57,640
sort of what's going on in here from the ground up.

2362
02:49:57,640 --> 02:49:59,640
So like I was talking about before,

2363
02:49:59,640 --> 02:50:01,640
we have some inputs and we have some outputs

2364
02:50:01,640 --> 02:50:03,640
which are shifted right,

2365
02:50:03,640 --> 02:50:07,640
and we give each of them some embedding vectors

2366
02:50:07,640 --> 02:50:09,640
and positional encodings.

2367
02:50:09,640 --> 02:50:12,640
So from here, let's say we have n layers.

2368
02:50:12,640 --> 02:50:14,640
This is going to make sense in a second.

2369
02:50:14,640 --> 02:50:16,640
n layers is set to four.

2370
02:50:16,640 --> 02:50:18,640
So the amount of layers we have is set to four.

2371
02:50:18,640 --> 02:50:21,640
So you can see we have an encoder, encoder.

2372
02:50:21,640 --> 02:50:24,640
Like we have four of these and we have four decoders.

2373
02:50:24,640 --> 02:50:27,640
So four is actually the amount of encoders

2374
02:50:27,640 --> 02:50:29,640
and decoders we have.

2375
02:50:29,640 --> 02:50:31,640
We always have the same amount of each.

2376
02:50:31,640 --> 02:50:34,640
So if we have, you know, ten layers,

2377
02:50:34,640 --> 02:50:37,640
that means we have ten encoders and ten decoders.

2378
02:50:37,640 --> 02:50:39,640
And pretty much what would happen

2379
02:50:39,640 --> 02:50:42,640
is after this input,

2380
02:50:42,640 --> 02:50:44,640
embedding and positional embedding,

2381
02:50:44,640 --> 02:50:47,640
we feed that into the first encoder layer

2382
02:50:47,640 --> 02:50:49,640
and then the next, and then next,

2383
02:50:49,640 --> 02:50:51,640
and then right as soon as we hit the last one,

2384
02:50:51,640 --> 02:50:56,640
we feed these into each of these decoders here,

2385
02:50:56,640 --> 02:50:58,640
each of these decoder layers.

2386
02:50:58,640 --> 02:51:03,640
So only the last encoder will feed into these decoders.

2387
02:51:03,640 --> 02:51:08,640
And pretty much these decoders will all run.

2388
02:51:08,640 --> 02:51:10,640
They'll all learn different things.

2389
02:51:10,640 --> 02:51:12,640
And then they'll turn what they learned.

2390
02:51:12,640 --> 02:51:15,640
They'll do, they'll apply a linear transformation

2391
02:51:15,640 --> 02:51:16,640
at the end of it.

2392
02:51:16,640 --> 02:51:18,640
This is not in the decoder function.

2393
02:51:18,640 --> 02:51:20,640
This is actually after the last decoder.

2394
02:51:20,640 --> 02:51:22,640
It'll apply a linear transformation

2395
02:51:22,640 --> 02:51:26,640
to pretty much sort of simplify

2396
02:51:26,640 --> 02:51:28,640
or give a summary of what it learned.

2397
02:51:28,640 --> 02:51:33,640
And then we apply a softmax on that new, you know, tensor

2398
02:51:33,640 --> 02:51:36,640
to get some probabilities to sample from,

2399
02:51:36,640 --> 02:51:38,640
like we talked about in the generate function

2400
02:51:38,640 --> 02:51:39,640
in our biogram.

2401
02:51:39,640 --> 02:51:42,640
And then once we get these probabilities,

2402
02:51:42,640 --> 02:51:47,640
we can then sample from them and generate tokens.

2403
02:51:47,640 --> 02:51:51,640
And that's kind of like the first little step here.

2404
02:51:51,640 --> 02:51:52,640
That's what's going on.

2405
02:51:52,640 --> 02:51:53,640
We have some encoders.

2406
02:51:53,640 --> 02:51:55,640
We have some decoders.

2407
02:51:55,640 --> 02:51:57,640
We do a transformation to summarize.

2408
02:51:57,640 --> 02:51:59,640
We have a softmax to get probabilities.

2409
02:51:59,640 --> 02:52:01,640
And then we generate based on those probabilities.

2410
02:52:01,640 --> 02:52:04,640
Cool.

2411
02:52:04,640 --> 02:52:06,640
Next up, in the encoder,

2412
02:52:06,640 --> 02:52:09,640
in each of these encoders, this is what it's going to look like.

2413
02:52:09,640 --> 02:52:11,640
So we have multi-hat attention,

2414
02:52:11,640 --> 02:52:14,640
which I'm going to dub into a second here.

2415
02:52:14,640 --> 02:52:17,640
So after this multi-hat attention,

2416
02:52:17,640 --> 02:52:19,640
we have a residual connection.

2417
02:52:19,640 --> 02:52:22,640
So in case you aren't familiar with residual connections,

2418
02:52:22,640 --> 02:52:24,640
I might have went over this before.

2419
02:52:24,640 --> 02:52:26,640
But pretty much what they do is

2420
02:52:26,640 --> 02:52:28,640
it's a little connector.

2421
02:52:28,640 --> 02:52:30,640
So I don't know.

2422
02:52:30,640 --> 02:52:32,640
Let's say you get some inputs X,

2423
02:52:32,640 --> 02:52:34,640
you have some inputs X down here,

2424
02:52:34,640 --> 02:52:38,640
and you put them into some sort of function here,

2425
02:52:38,640 --> 02:52:40,640
some sort of like feedforward network, whatever it is.

2426
02:52:40,640 --> 02:52:43,640
A feedforward network is essentially just a linear,

2427
02:52:43,640 --> 02:52:45,640
a RELU, and then a linear.

2428
02:52:45,640 --> 02:52:47,640
That's all feedforward network is right here.

2429
02:52:47,640 --> 02:52:49,640
Linear, really, really linear.

2430
02:52:49,640 --> 02:52:55,640
And all you do is you wrap those inputs

2431
02:52:55,640 --> 02:52:58,640
around so you don't actually put them

2432
02:52:58,640 --> 02:53:00,640
into that feedforward network.

2433
02:53:00,640 --> 02:53:02,640
You actually wrap them around,

2434
02:53:02,640 --> 02:53:05,640
and then you can add them to the output.

2435
02:53:05,640 --> 02:53:07,640
So you had some X values here,

2436
02:53:07,640 --> 02:53:10,640
go through the RELU, and then you had some wrap around.

2437
02:53:10,640 --> 02:53:14,640
And then right here, you simply add them together

2438
02:53:14,640 --> 02:53:17,640
and you normalize them using some encod layer norm,

2439
02:53:17,640 --> 02:53:19,640
which we're going to cover in a little bit.

2440
02:53:19,640 --> 02:53:23,640
And the reason why residual connections

2441
02:53:23,640 --> 02:53:26,640
are so useful in transformers

2442
02:53:26,640 --> 02:53:29,640
is because when you have a really deep neural network,

2443
02:53:29,640 --> 02:53:32,640
a lot of the information is actually forgotten

2444
02:53:32,640 --> 02:53:34,640
in the first steps.

2445
02:53:34,640 --> 02:53:37,640
So if you have your first view encoder layers

2446
02:53:37,640 --> 02:53:39,640
and your first view decoder layers,

2447
02:53:39,640 --> 02:53:42,640
a lot of the information here is going to be forgotten

2448
02:53:42,640 --> 02:53:44,640
because it's not being carried through.

2449
02:53:44,640 --> 02:53:48,640
The first steps of it aren't explicitly being carried through

2450
02:53:48,640 --> 02:53:52,640
and sort of skipped through the functions.

2451
02:53:52,640 --> 02:53:56,640
And yeah, you can sort of see how they would just be forgotten.

2452
02:53:56,640 --> 02:53:59,640
So residual connections are sort of just a cheat

2453
02:53:59,640 --> 02:54:01,640
for getting around that,

2454
02:54:01,640 --> 02:54:03,640
for not having deep neural networks forget things

2455
02:54:03,640 --> 02:54:05,640
from the beginning,

2456
02:54:05,640 --> 02:54:07,640
and having them all sort of work together to the same degree.

2457
02:54:07,640 --> 02:54:10,640
So residual connections are great that way.

2458
02:54:10,640 --> 02:54:13,640
And then, you know, at the end there,

2459
02:54:13,640 --> 02:54:16,640
you would add them together and then normalize.

2460
02:54:16,640 --> 02:54:19,640
And there's two different ways that you can do this add a norm.

2461
02:54:19,640 --> 02:54:22,640
There's add a norm and then norm and add.

2462
02:54:22,640 --> 02:54:27,640
So these are two different separate architectures

2463
02:54:27,640 --> 02:54:30,640
that you can do in transformers.

2464
02:54:30,640 --> 02:54:34,640
And both of these are sort of like meta architectures.

2465
02:54:34,640 --> 02:54:40,640
But pretty much pre-norm is the normalize then add,

2466
02:54:40,640 --> 02:54:42,640
and then post-norm is add then normalize.

2467
02:54:42,640 --> 02:54:46,640
So in this attention is all you need paper

2468
02:54:46,640 --> 02:54:51,640
proposed by a bunch of research scientists was

2469
02:54:51,640 --> 02:54:55,640
initially you want to add these,

2470
02:54:55,640 --> 02:54:59,640
you want to add these together and then normalize them.

2471
02:54:59,640 --> 02:55:05,640
So that is what we call the post-norm architecture.

2472
02:55:05,640 --> 02:55:08,640
And then pre-norm is just flip them around.

2473
02:55:08,640 --> 02:55:13,640
So I've actually done some testing with pre-norm and post-norm

2474
02:55:13,640 --> 02:55:17,640
and the original transformer paper

2475
02:55:17,640 --> 02:55:21,640
turned out to be quite actually a lot better,

2476
02:55:21,640 --> 02:55:24,640
at least for training very small language models.

2477
02:55:24,640 --> 02:55:26,640
If you're training bigger ones, it might be different,

2478
02:55:26,640 --> 02:55:31,640
but essentially we're just going to go by the rules that we use in here.

2479
02:55:31,640 --> 02:55:32,640
So add a norm.

2480
02:55:32,640 --> 02:55:33,640
We're not going to do norm and add.

2481
02:55:33,640 --> 02:55:37,640
Add a norm in this video specifically because it works better

2482
02:55:37,640 --> 02:55:41,640
and we just don't want to break any of the rules and go outside of it

2483
02:55:41,640 --> 02:55:42,640
because then that starts to get confusing.

2484
02:55:42,640 --> 02:55:45,640
And actually if you watch the Andre Carpathi lecture

2485
02:55:45,640 --> 02:55:47,640
on building GPTs from scratch,

2486
02:55:47,640 --> 02:55:53,640
he actually implemented it in the pre-norm way.

2487
02:55:53,640 --> 02:55:55,640
So normalize then add.

2488
02:55:55,640 --> 02:55:58,640
So yeah, based on my experience,

2489
02:55:58,640 --> 02:56:05,640
what I've done on my computer here is the post-norm architecture works quite better.

2490
02:56:05,640 --> 02:56:07,640
So that's why we're going to use it.

2491
02:56:07,640 --> 02:56:10,640
We're going to do add then normalize.

2492
02:56:10,640 --> 02:56:15,640
So then we essentially feed this into a feedforward network

2493
02:56:15,640 --> 02:56:16,640
which we covered earlier.

2494
02:56:16,640 --> 02:56:20,640
And then how did it go?

2495
02:56:20,640 --> 02:56:23,640
So we're encoder.

2496
02:56:23,640 --> 02:56:28,640
We do a residual connection from here to here

2497
02:56:28,640 --> 02:56:35,640
and then another residual connection from outside of our feedforward network.

2498
02:56:35,640 --> 02:56:39,640
So each time we're doing some other things like some, you know,

2499
02:56:39,640 --> 02:56:41,640
some computation blocks in here,

2500
02:56:41,640 --> 02:56:43,640
we're going to have a rest connection.

2501
02:56:43,640 --> 02:56:46,640
Same with our feedforward rest connection.

2502
02:56:46,640 --> 02:56:49,640
And then of course the output from here,

2503
02:56:49,640 --> 02:56:51,640
just when it exits,

2504
02:56:51,640 --> 02:56:53,640
it's going to feed into the next encoder block

2505
02:56:53,640 --> 02:56:55,640
if it's not the last encoder.

2506
02:56:55,640 --> 02:56:57,640
So this one is going to do all this.

2507
02:56:57,640 --> 02:56:58,640
It's going to feed into that one.

2508
02:56:58,640 --> 02:56:59,640
It's going to do the same thing.

2509
02:56:59,640 --> 02:57:00,640
Feed into this one.

2510
02:57:00,640 --> 02:57:02,640
Going to feed into that one.

2511
02:57:02,640 --> 02:57:06,640
And then the output of this is going to feed into each of these decoders,

2512
02:57:06,640 --> 02:57:09,640
all the same information.

2513
02:57:09,640 --> 02:57:15,640
And yeah, so that's a little bit scoped in as to what these encoders look like.

2514
02:57:15,640 --> 02:57:18,640
So now that you know what the encoder looks like,

2515
02:57:18,640 --> 02:57:19,640
what the feedforward looks like,

2516
02:57:19,640 --> 02:57:21,640
we're going to go into multi-head attention,

2517
02:57:21,640 --> 02:57:23,640
sort of the premise,

2518
02:57:23,640 --> 02:57:26,640
sort of the highlight of the transformer architecture

2519
02:57:26,640 --> 02:57:28,640
and why it's so important.

2520
02:57:28,640 --> 02:57:31,640
So multi-head attention,

2521
02:57:31,640 --> 02:57:33,640
we call it multi-head attention

2522
02:57:33,640 --> 02:57:35,640
because there are a bunch of these different heads

2523
02:57:35,640 --> 02:57:37,640
learning different semantic info

2524
02:57:37,640 --> 02:57:39,640
from a unique perspective.

2525
02:57:39,640 --> 02:57:42,640
So let's say you have 10 different people

2526
02:57:42,640 --> 02:57:45,640
looking at the same book.

2527
02:57:45,640 --> 02:57:47,640
If you have 10 different people,

2528
02:57:47,640 --> 02:57:52,640
let's say they're all reading the same Harry Potter book.

2529
02:57:52,640 --> 02:57:54,640
These different people,

2530
02:57:54,640 --> 02:57:57,640
they might have different cognitive abilities.

2531
02:57:57,640 --> 02:57:59,640
They might have different IQs.

2532
02:57:59,640 --> 02:58:01,640
They might have been raised in different ways.

2533
02:58:01,640 --> 02:58:03,640
So they might interpret things differently.

2534
02:58:03,640 --> 02:58:06,640
They might look at little things in that book

2535
02:58:06,640 --> 02:58:08,640
and their mind will,

2536
02:58:08,640 --> 02:58:10,640
they'll imagine different scenarios,

2537
02:58:10,640 --> 02:58:12,640
different environments from the book.

2538
02:58:12,640 --> 02:58:16,640
And essentially why this is so valuable

2539
02:58:16,640 --> 02:58:19,640
is because we don't just want to have one person,

2540
02:58:19,640 --> 02:58:21,640
just one perspective on this.

2541
02:58:21,640 --> 02:58:24,640
We want to have a bunch of different heads in parallel

2542
02:58:24,640 --> 02:58:30,640
looking at this same piece of data

2543
02:58:30,640 --> 02:58:33,640
because they're all going to capture different things about it.

2544
02:58:33,640 --> 02:58:36,640
And keep in mind each of these heads,

2545
02:58:36,640 --> 02:58:38,640
each of these heads in parallel,

2546
02:58:38,640 --> 02:58:40,640
these different perspectives,

2547
02:58:40,640 --> 02:58:42,640
they have different learnable parameters.

2548
02:58:42,640 --> 02:58:44,640
So they're not all the same one

2549
02:58:44,640 --> 02:58:46,640
looking at this piece of data.

2550
02:58:46,640 --> 02:58:49,640
They're actually,

2551
02:58:49,640 --> 02:58:51,640
they all have different learnable parameters.

2552
02:58:51,640 --> 02:58:54,640
So you have a bunch of these

2553
02:58:54,640 --> 02:58:56,640
at the same time learning different things

2554
02:58:56,640 --> 02:58:58,640
and that's why it's so powerful.

2555
02:58:58,640 --> 02:59:04,640
So this scale.product attention runs in parallel,

2556
02:59:04,640 --> 02:59:06,640
which means we can scale that to the GPU,

2557
02:59:06,640 --> 02:59:08,640
which is very useful.

2558
02:59:08,640 --> 02:59:10,640
It's good to touch on that.

2559
02:59:10,640 --> 02:59:12,640
Anything with the GPU that you can accelerate

2560
02:59:12,640 --> 02:59:14,640
is just an automatic win

2561
02:59:14,640 --> 02:59:19,640
because parallelism is great in machine learning.

2562
02:59:19,640 --> 02:59:21,640
Why not have parallelism, right?

2563
02:59:21,640 --> 02:59:23,640
If it's just going to be running the CPU, what's the point?

2564
02:59:23,640 --> 02:59:25,640
That's why we love GPUs.

2565
02:59:25,640 --> 02:59:27,640
Anyways, yeah.

2566
02:59:27,640 --> 02:59:29,640
So you're going to have these different,

2567
02:59:29,640 --> 02:59:31,640
you're going to have these things that are called keys,

2568
02:59:31,640 --> 02:59:33,640
queries and values.

2569
02:59:33,640 --> 02:59:35,640
I'll touch on those in a second here

2570
02:59:35,640 --> 02:59:37,640
because keys, queries and values

2571
02:59:37,640 --> 02:59:39,640
sort of point to self-attention,

2572
02:59:39,640 --> 02:59:41,640
which is literally the entire point of the transformer.

2573
02:59:41,640 --> 02:59:43,640
Transformer wouldn't really mean anything

2574
02:59:43,640 --> 02:59:45,640
without self-attention.

2575
02:59:45,640 --> 02:59:47,640
So I'll touch on those in a second here

2576
02:59:47,640 --> 02:59:49,640
and we'll actually delve deeper

2577
02:59:49,640 --> 02:59:51,640
as we hit this sort of block.

2578
02:59:51,640 --> 02:59:53,640
But yeah, you have these keys, queries and values.

2579
02:59:53,640 --> 02:59:55,640
They go into scale.product attention.

2580
02:59:55,640 --> 02:59:57,640
So a bunch of these running in parallel

2581
02:59:57,640 --> 02:59:59,640
and then you concatenate the results

2582
02:59:59,640 --> 03:00:01,640
from all these different heads running in parallel.

2583
03:00:01,640 --> 03:00:03,640
You have all these different people.

2584
03:00:03,640 --> 03:00:05,640
You concatenate all of them,

2585
03:00:05,640 --> 03:00:07,640
you generalize it,

2586
03:00:07,640 --> 03:00:09,640
and then you apply a transformation

2587
03:00:09,640 --> 03:00:11,640
to a linear transformation

2588
03:00:11,640 --> 03:00:13,640
to pretty much summarize that

2589
03:00:13,640 --> 03:00:16,640
and then do your add a norm,

2590
03:00:16,640 --> 03:00:18,640
then pay for a network.

2591
03:00:18,640 --> 03:00:20,640
So that's what's going on in multi-head attention.

2592
03:00:20,640 --> 03:00:22,640
You're just doing a bunch of self-attentions

2593
03:00:22,640 --> 03:00:24,640
in parallel, concatenating,

2594
03:00:24,640 --> 03:00:26,640
and then continuing on with this part.

2595
03:00:26,640 --> 03:00:28,640
So scale.product attention.

2596
03:00:28,640 --> 03:00:30,640
What is that?

2597
03:00:30,640 --> 03:00:32,640
So let's just start from the ground up here.

2598
03:00:32,640 --> 03:00:34,640
We'll just go from left to right.

2599
03:00:34,640 --> 03:00:36,640
So you have your keys, queries and values.

2600
03:00:36,640 --> 03:00:38,640
What do your keys do?

2601
03:00:38,640 --> 03:00:40,640
Well, a key is

2602
03:00:40,640 --> 03:00:42,640
let's just say you have a token and a sentence.

2603
03:00:42,640 --> 03:00:44,640
Okay?

2604
03:00:44,640 --> 03:00:46,640
So if you have

2605
03:00:46,640 --> 03:00:48,640
let me just

2606
03:00:48,640 --> 03:00:50,640
roll down here to a good example.

2607
03:00:50,640 --> 03:00:52,640
So

2608
03:00:52,640 --> 03:00:54,640
self-attention

2609
03:00:54,640 --> 03:00:56,640
uses

2610
03:00:56,640 --> 03:00:58,640
keys, queries and values.

2611
03:00:58,640 --> 03:01:00,640
Self-attention helps

2612
03:01:00,640 --> 03:01:02,640
identify

2613
03:01:02,640 --> 03:01:04,640
which of these tokens in a sentence

2614
03:01:04,640 --> 03:01:06,640
in any given sentence are more important

2615
03:01:06,640 --> 03:01:08,640
and how much attention

2616
03:01:08,640 --> 03:01:10,640
you should pay

2617
03:01:10,640 --> 03:01:12,640
to each of those characters or words, whatever you're using.

2618
03:01:12,640 --> 03:01:14,640
We'll just use words

2619
03:01:14,640 --> 03:01:16,640
to

2620
03:01:16,640 --> 03:01:18,640
make it easier to understand for the purpose of this video.

2621
03:01:18,640 --> 03:01:20,640
But

2622
03:01:20,640 --> 03:01:22,640
essentially imagine you have

2623
03:01:24,640 --> 03:01:26,640
these two sentences here.

2624
03:01:26,640 --> 03:01:28,640
So you have

2625
03:01:28,640 --> 03:01:30,640
let me bring out my little piece of text.

2626
03:01:30,640 --> 03:01:32,640
So you have

2627
03:01:34,640 --> 03:01:36,640
that didn't work.

2628
03:01:38,640 --> 03:01:40,640
So imagine you have

2629
03:01:42,640 --> 03:01:44,640
server, can I have the check?

2630
03:01:44,640 --> 03:01:46,640
And then you have

2631
03:01:48,640 --> 03:01:50,640
and you have

2632
03:01:50,640 --> 03:01:52,640
looks like I crashed the server.

2633
03:01:52,640 --> 03:01:54,640
So

2634
03:01:54,640 --> 03:01:56,640
I mean, both of these have

2635
03:01:56,640 --> 03:01:58,640
the word server in them, but they mean different things.

2636
03:01:58,640 --> 03:02:00,640
Server meaning like the waiter

2637
03:02:00,640 --> 03:02:02,640
or the waitress or whoever

2638
03:02:02,640 --> 03:02:04,640
is billing

2639
03:02:04,640 --> 03:02:06,640
you at the end of your restaurant visit.

2640
03:02:06,640 --> 03:02:08,640
And then looks like I crashed the server

2641
03:02:08,640 --> 03:02:10,640
is like, oh, there's actually a server running

2642
03:02:10,640 --> 03:02:12,640
in the cloud, not like a person

2643
03:02:12,640 --> 03:02:14,640
that's billing me, but an actual server.

2644
03:02:14,640 --> 03:02:16,640
That's maybe running a video game.

2645
03:02:16,640 --> 03:02:18,640
And

2646
03:02:18,640 --> 03:02:20,640
these are two different things. So what attention can do

2647
03:02:20,640 --> 03:02:22,640
is it can actually identify

2648
03:02:22,640 --> 03:02:24,640
which words would get attention here.

2649
03:02:24,640 --> 03:02:26,640
So it can say

2650
03:02:26,640 --> 03:02:28,640
server, can I have the check?

2651
03:02:28,640 --> 03:02:30,640
Can I have?

2652
03:02:30,640 --> 03:02:32,640
So it's maybe you're looking

2653
03:02:32,640 --> 03:02:34,640
for something you're looking for the check

2654
03:02:34,640 --> 03:02:36,640
and then server

2655
03:02:36,640 --> 03:02:38,640
is like, oh, well in this

2656
03:02:38,640 --> 03:02:40,640
in this particular sequence or in this

2657
03:02:40,640 --> 03:02:42,640
in the sentiment of this sentence here

2658
03:02:42,640 --> 03:02:44,640
server

2659
03:02:44,640 --> 03:02:46,640
is specifically tied to

2660
03:02:46,640 --> 03:02:48,640
this one meaning, maybe a human

2661
03:02:48,640 --> 03:02:50,640
someone at a restaurant

2662
03:02:50,640 --> 03:02:52,640
and then crash

2663
03:02:52,640 --> 03:02:54,640
the server

2664
03:02:54,640 --> 03:02:56,640
crash is going to get a very high attention

2665
03:02:56,640 --> 03:02:58,640
score because

2666
03:02:58,640 --> 03:03:00,640
you don't normally

2667
03:03:00,640 --> 03:03:02,640
crash a server at a restaurant

2668
03:03:02,640 --> 03:03:04,640
that doesn't particularly make sense.

2669
03:03:04,640 --> 03:03:06,640
So

2670
03:03:06,640 --> 03:03:08,640
if you have different words like this

2671
03:03:08,640 --> 03:03:10,640
what self-attention will do

2672
03:03:10,640 --> 03:03:12,640
is it will learn

2673
03:03:12,640 --> 03:03:14,640
which words in the sentence

2674
03:03:14,640 --> 03:03:16,640
are actually more important

2675
03:03:16,640 --> 03:03:18,640
and which words should

2676
03:03:18,640 --> 03:03:20,640
pay more attention to.

2677
03:03:20,640 --> 03:03:22,640
So that's really all that's going on here

2678
03:03:22,640 --> 03:03:24,640
and

2679
03:03:24,640 --> 03:03:26,640
the key

2680
03:03:26,640 --> 03:03:28,640
is essentially going to emit

2681
03:03:28,640 --> 03:03:30,640
a different

2682
03:03:30,640 --> 03:03:32,640
it's going to emit

2683
03:03:32,640 --> 03:03:34,640
a little tensor

2684
03:03:34,640 --> 03:03:36,640
here saying

2685
03:03:36,640 --> 03:03:38,640
what do I contain

2686
03:03:38,640 --> 03:03:40,640
and then query

2687
03:03:40,640 --> 03:03:42,640
is going to say

2688
03:03:42,640 --> 03:03:44,640
what am I looking for?

2689
03:03:44,640 --> 03:03:46,640
So what's going to happen

2690
03:03:46,640 --> 03:03:48,640
is if these, let's say

2691
03:03:48,640 --> 03:03:50,640
server, it's going to look for things like

2692
03:03:50,640 --> 03:03:52,640
check or crashed

2693
03:03:52,640 --> 03:03:54,640
so if it sees crashed

2694
03:03:54,640 --> 03:03:56,640
then that means the key and the query

2695
03:03:56,640 --> 03:03:58,640
are going to multiply

2696
03:03:58,640 --> 03:04:00,640
and it's going to get a very high attention score

2697
03:04:00,640 --> 03:04:02,640
but if you have something

2698
03:04:02,640 --> 03:04:04,640
like

2699
03:04:04,640 --> 03:04:06,640
it's like

2700
03:04:06,640 --> 03:04:08,640
there's literally almost any sentence

2701
03:04:08,640 --> 03:04:10,640
so that doesn't mean much.

2702
03:04:10,640 --> 03:04:12,640
We're not going to pay attention to those words

2703
03:04:12,640 --> 03:04:14,640
so that's going to get a very low attention score

2704
03:04:14,640 --> 03:04:16,640
and all attention

2705
03:04:16,640 --> 03:04:18,640
is you're just dot-producting

2706
03:04:18,640 --> 03:04:20,640
these vectors together.

2707
03:04:20,640 --> 03:04:22,640
So you get a key

2708
03:04:22,640 --> 03:04:24,640
and a query, you dot-product them

2709
03:04:24,640 --> 03:04:26,640
we already went over dot-products

2710
03:04:26,640 --> 03:04:28,640
in this course before

2711
03:04:28,640 --> 03:04:30,640
and then

2712
03:04:30,640 --> 03:04:32,640
this is a little bit of a confusing part

2713
03:04:32,640 --> 03:04:34,640
is you just scale

2714
03:04:34,640 --> 03:04:36,640
by one over the

2715
03:04:36,640 --> 03:04:38,640
square root

2716
03:04:38,640 --> 03:04:40,640
of the length of a row

2717
03:04:40,640 --> 03:04:42,640
in the keys or queries matrix

2718
03:04:42,640 --> 03:04:44,640
otherwise known as

2719
03:04:44,640 --> 03:04:46,640
DK.

2720
03:04:46,640 --> 03:04:48,640
So let's say we have

2721
03:04:48,640 --> 03:04:50,640
our key and our query

2722
03:04:50,640 --> 03:04:52,640
these are all going to be the same length by the way.

2723
03:04:52,640 --> 03:04:54,640
Let's say our keys

2724
03:04:54,640 --> 03:04:56,640
is

2725
03:04:56,640 --> 03:04:58,640
maybe our keys is going to be like

2726
03:04:58,640 --> 03:05:00,640
10 characters long

2727
03:05:00,640 --> 03:05:02,640
our keys are going to be 10 characters long as well

2728
03:05:02,640 --> 03:05:04,640
so it's going to do

2729
03:05:04,640 --> 03:05:06,640
one over the square root of 10

2730
03:05:06,640 --> 03:05:08,640
if that makes sense

2731
03:05:08,640 --> 03:05:10,640
and so

2732
03:05:10,640 --> 03:05:12,640
that's just

2733
03:05:12,640 --> 03:05:14,640
essentially a way of preventing

2734
03:05:14,640 --> 03:05:16,640
these dot-products

2735
03:05:16,640 --> 03:05:18,640
from exploding

2736
03:05:18,640 --> 03:05:20,640
we want to scale them because

2737
03:05:20,640 --> 03:05:22,640
as we have

2738
03:05:22,640 --> 03:05:24,640
as the length of it increases

2739
03:05:24,640 --> 03:05:26,640
so will the

2740
03:05:26,640 --> 03:05:28,640
ending dot-product

2741
03:05:28,640 --> 03:05:30,640
because there's more of these to multiply

2742
03:05:30,640 --> 03:05:32,640
so we pretty much just want to

2743
03:05:32,640 --> 03:05:34,640
scale it by using

2744
03:05:34,640 --> 03:05:36,640
an inverse square root

2745
03:05:36,640 --> 03:05:38,640
and that will just help us with

2746
03:05:38,640 --> 03:05:40,640
scaling make sure nothing explodes

2747
03:05:40,640 --> 03:05:42,640
in unnecessary ways

2748
03:05:42,640 --> 03:05:44,640
and then

2749
03:05:44,640 --> 03:05:46,640
the next little important part

2750
03:05:46,640 --> 03:05:48,640
is using tort.trill

2751
03:05:48,640 --> 03:05:50,640
which I imagine we went over in our examples here

2752
03:05:50,640 --> 03:05:52,640
trill

2753
03:05:52,640 --> 03:05:54,640
yeah

2754
03:05:54,640 --> 03:05:56,640
so

2755
03:05:56,640 --> 03:05:58,640
you can see that

2756
03:05:58,640 --> 03:06:00,640
it's a diagonal

2757
03:06:00,640 --> 03:06:02,640
it's a left triangular

2758
03:06:02,640 --> 03:06:04,640
matrix of ones

2759
03:06:04,640 --> 03:06:06,640
and these aren't going to be ones

2760
03:06:06,640 --> 03:06:08,640
in our self-attention here

2761
03:06:08,640 --> 03:06:10,640
in our tort.trill or masking

2762
03:06:10,640 --> 03:06:12,640
what this is going to be

2763
03:06:12,640 --> 03:06:14,640
is

2764
03:06:14,640 --> 03:06:16,640
the scores at each time step

2765
03:06:16,640 --> 03:06:18,640
the combination of scores

2766
03:06:18,640 --> 03:06:20,640
at each time step

2767
03:06:20,640 --> 03:06:22,640
so

2768
03:06:22,640 --> 03:06:24,640
if we've only gone

2769
03:06:24,640 --> 03:06:26,640
if we're only looking at the first

2770
03:06:26,640 --> 03:06:28,640
time step

2771
03:06:28,640 --> 03:06:30,640
we should not have access to the rest of things

2772
03:06:30,640 --> 03:06:32,640
or else that would be cheating

2773
03:06:32,640 --> 03:06:34,640
we shouldn't be allowed to look ahead

2774
03:06:34,640 --> 03:06:36,640
because we haven't actually produced these yet

2775
03:06:36,640 --> 03:06:38,640
we need to produce these before we can

2776
03:06:38,640 --> 03:06:40,640
put them into perspective

2777
03:06:40,640 --> 03:06:42,640
and put a weight on them

2778
03:06:42,640 --> 03:06:44,640
so we're going to set all these to zero

2779
03:06:44,640 --> 03:06:46,640
and then we go to the next time step

2780
03:06:46,640 --> 03:06:48,640
so now we've just generated this

2781
03:06:48,640 --> 03:06:50,640
one we haven't generated these yet

2782
03:06:50,640 --> 03:06:52,640
so we can't look at them

2783
03:06:52,640 --> 03:06:54,640
and then as we go more and more

2784
03:06:54,640 --> 03:06:56,640
as the time step increases

2785
03:06:56,640 --> 03:06:58,640
we know more and more context

2786
03:06:58,640 --> 03:07:00,640
about all of these tokens

2787
03:07:00,640 --> 03:07:02,640
so

2788
03:07:02,640 --> 03:07:04,640
that's all that's doing

2789
03:07:04,640 --> 03:07:06,640
mask attention is pretty much just saying

2790
03:07:06,640 --> 03:07:08,640
we don't want to look into the future

2791
03:07:08,640 --> 03:07:10,640
we want to only guess with what we currently know

2792
03:07:10,640 --> 03:07:12,640
in our current time step

2793
03:07:12,640 --> 03:07:14,640
and everything before it

2794
03:07:14,640 --> 03:07:16,640
you can't jump into the future

2795
03:07:16,640 --> 03:07:18,640
look at what happened in the past

2796
03:07:18,640 --> 03:07:20,640
and do stuff based on that

2797
03:07:20,640 --> 03:07:22,640
same thing applies to life

2798
03:07:22,640 --> 03:07:24,640
you can't really skip to the future and say

2799
03:07:24,640 --> 03:07:26,640
hey if you do this you're going to be a billionaire

2800
03:07:26,640 --> 03:07:28,640
no that would be cheating

2801
03:07:28,640 --> 03:07:30,640
you're not allowed to do that

2802
03:07:30,640 --> 03:07:32,640
you can only look at the mistakes you made

2803
03:07:32,640 --> 03:07:34,640
and say how can I become a billionaire

2804
03:07:34,640 --> 03:07:36,640
based on all these other mistakes that I made

2805
03:07:36,640 --> 03:07:38,640
how can I become as close to perfect as possible

2806
03:07:38,640 --> 03:07:40,640
which no one I can ever be perfect

2807
03:07:40,640 --> 03:07:42,640
but that's my little analogy for the day

2808
03:07:42,640 --> 03:07:44,640
so that's mask attention

2809
03:07:44,640 --> 03:07:46,640
pretty much just not letting us skip time steps

2810
03:07:46,640 --> 03:07:48,640
so that's fun

2811
03:07:48,640 --> 03:07:50,640
let's continue

2812
03:07:50,640 --> 03:07:52,640
two more little things I want to touch on before I jump forward here

2813
03:07:52,640 --> 03:07:54,640
so

2814
03:07:54,640 --> 03:07:56,640
these keys, queries and values

2815
03:07:56,640 --> 03:07:58,640
each of these are learned through a linear transformation

2816
03:07:58,640 --> 03:08:00,640
just an end dot linear

2817
03:08:00,640 --> 03:08:02,640
is applied

2818
03:08:02,640 --> 03:08:04,640
and that's how we get our keys, queries and values

2819
03:08:04,640 --> 03:08:06,640
so that's just a little

2820
03:08:06,640 --> 03:08:08,640
touching there if you're wondering how do we get those

2821
03:08:08,640 --> 03:08:10,640
it's just an end dot linear transformation

2822
03:08:10,640 --> 03:08:12,640
and then as for our

2823
03:08:12,640 --> 03:08:14,640
masking we don't actually apply this all the time

2824
03:08:14,640 --> 03:08:16,640
you might have seen right here

2825
03:08:16,640 --> 03:08:18,640
we have

2826
03:08:18,640 --> 03:08:20,640
multi-head attention

2827
03:08:20,640 --> 03:08:22,640
multi-head attention and then mask

2828
03:08:22,640 --> 03:08:24,640
multi-head attention

2829
03:08:24,640 --> 03:08:26,640
so this masked attention isn't used all the time

2830
03:08:26,640 --> 03:08:28,640
it's only used

2831
03:08:28,640 --> 03:08:30,640
actually one out of the three attentions

2832
03:08:30,640 --> 03:08:32,640
we have per layer

2833
03:08:32,640 --> 03:08:34,640
so

2834
03:08:34,640 --> 03:08:36,640
I'll give you a little bit more information

2835
03:08:36,640 --> 03:08:38,640
about that as we

2836
03:08:38,640 --> 03:08:40,640
progress more and more into the architecture

2837
03:08:40,640 --> 03:08:42,640
as we learn more about it

2838
03:08:42,640 --> 03:08:44,640
I'm not going to dive into that

2839
03:08:44,640 --> 03:08:46,640
quite yet though

2840
03:08:46,640 --> 03:08:48,640
so let's just continue on with what's going on

2841
03:08:48,640 --> 03:08:50,640
so we have a softmax

2842
03:08:50,640 --> 03:08:52,640
and why softmax important

2843
03:08:52,640 --> 03:08:54,640
well

2844
03:08:54,640 --> 03:08:56,640
I actually mentioned earlier

2845
03:08:56,640 --> 03:08:58,640
softmax is not commonly used

2846
03:08:58,640 --> 03:09:00,640
as a normalization method

2847
03:09:00,640 --> 03:09:02,640
but here we're actually using

2848
03:09:02,640 --> 03:09:04,640
softmax to normalize

2849
03:09:04,640 --> 03:09:06,640
so when you have all of these

2850
03:09:06,640 --> 03:09:08,640
when you have all of these

2851
03:09:08,640 --> 03:09:10,640
attention scores

2852
03:09:10,640 --> 03:09:12,640
essentially what the softmax is doing

2853
03:09:12,640 --> 03:09:14,640
is it's going to

2854
03:09:14,640 --> 03:09:16,640
exponentiate and normalize all of these

2855
03:09:16,640 --> 03:09:18,640
so

2856
03:09:18,640 --> 03:09:20,640
all of the attention scores that have scored

2857
03:09:20,640 --> 03:09:22,640
high like maybe 50 to

2858
03:09:22,640 --> 03:09:24,640
90% or whatever it is

2859
03:09:24,640 --> 03:09:26,640
those are going to take a massive effect

2860
03:09:26,640 --> 03:09:28,640
in that entire

2861
03:09:28,640 --> 03:09:30,640
attention

2862
03:09:30,640 --> 03:09:32,640
I guess tensor if you want to call it that

2863
03:09:34,640 --> 03:09:36,640
and that's important

2864
03:09:36,640 --> 03:09:38,640
it might not seem important

2865
03:09:38,640 --> 03:09:40,640
but it's essentially just giving the model

2866
03:09:40,640 --> 03:09:42,640
more confidence

2867
03:09:42,640 --> 03:09:44,640
as to which tokens matter more

2868
03:09:44,640 --> 03:09:46,640
so for example

2869
03:09:46,640 --> 03:09:48,640
if we just

2870
03:09:48,640 --> 03:09:50,640
did a normalization

2871
03:09:50,640 --> 03:09:52,640
we would

2872
03:09:52,640 --> 03:09:54,640
have words like server and crash

2873
03:09:54,640 --> 03:09:56,640
and then server and check

2874
03:09:56,640 --> 03:09:58,640
and then

2875
03:09:58,640 --> 03:10:00,640
you would just know

2876
03:10:00,640 --> 03:10:02,640
a decent amount about those

2877
03:10:02,640 --> 03:10:04,640
those would pay attention to a decent amount

2878
03:10:04,640 --> 03:10:06,640
because they multiply together quite well

2879
03:10:06,640 --> 03:10:08,640
but if you softmax those

2880
03:10:08,640 --> 03:10:10,640
then it's like

2881
03:10:10,640 --> 03:10:12,640
those are almost the only characters that matter

2882
03:10:12,640 --> 03:10:14,640
so it's looking at the context

2883
03:10:14,640 --> 03:10:16,640
of those two

2884
03:10:16,640 --> 03:10:18,640
and then we're sort of filling in

2885
03:10:18,640 --> 03:10:20,640
like we're learning about the rest of the sentence

2886
03:10:20,640 --> 03:10:22,640
based on just the

2887
03:10:22,640 --> 03:10:24,640
sentiment of those attention scores

2888
03:10:24,640 --> 03:10:26,640
because they're so high priority

2889
03:10:26,640 --> 03:10:28,640
because they multiply together

2890
03:10:28,640 --> 03:10:30,640
to such a high degree

2891
03:10:30,640 --> 03:10:32,640
we want to emphasize them

2892
03:10:32,640 --> 03:10:34,640
basically let the model learn more

2893
03:10:34,640 --> 03:10:36,640
about which words matter more together

2894
03:10:36,640 --> 03:10:38,640
so

2895
03:10:38,640 --> 03:10:40,640
that's pretty much just what the softmax does

2896
03:10:40,640 --> 03:10:42,640
it increases our confidence in

2897
03:10:42,640 --> 03:10:44,640
attention

2898
03:10:44,640 --> 03:10:46,640
and then a matrix multiply

2899
03:10:46,640 --> 03:10:48,640
we go back to our V here

2900
03:10:48,640 --> 03:10:50,640
and this is a value

2901
03:10:50,640 --> 03:10:52,640
so essentially what this is

2902
03:10:52,640 --> 03:10:54,640
is just a linear transformation

2903
03:10:54,640 --> 03:10:56,640
and we apply this on our

2904
03:10:56,640 --> 03:10:58,640
we apply this on our inputs

2905
03:10:58,640 --> 03:11:00,640
and

2906
03:11:00,640 --> 03:11:02,640
we have some value about

2907
03:11:02,640 --> 03:11:04,640
you know

2908
03:11:04,640 --> 03:11:06,640
what exactly those tokens are

2909
03:11:06,640 --> 03:11:08,640
and after we've gotten all of our attention

2910
03:11:08,640 --> 03:11:10,640
our softmax everything done

2911
03:11:10,640 --> 03:11:12,640
it's just going to multiply

2912
03:11:12,640 --> 03:11:14,640
the original values

2913
03:11:14,640 --> 03:11:16,640
by everything we've gotten so far

2914
03:11:16,640 --> 03:11:18,640
just so that you don't have any information

2915
03:11:18,640 --> 03:11:20,640
that's really lost or we don't have anything scrambled

2916
03:11:20,640 --> 03:11:22,640
just that we have like a general idea

2917
03:11:22,640 --> 03:11:24,640
of okay these are actually

2918
03:11:24,640 --> 03:11:26,640
all the tokens we have

2919
03:11:26,640 --> 03:11:28,640
and then these are

2920
03:11:28,640 --> 03:11:30,640
we found interesting the attention scores

2921
03:11:32,640 --> 03:11:34,640
so

2922
03:11:34,640 --> 03:11:36,640
we have an output which is a blend of input

2923
03:11:36,640 --> 03:11:38,640
vector values and attention placed on each token

2924
03:11:38,640 --> 03:11:40,640
and

2925
03:11:40,640 --> 03:11:42,640
that's pretty much what's happening in scaled dot

2926
03:11:42,640 --> 03:11:44,640
product attention in parallel

2927
03:11:44,640 --> 03:11:46,640
so we have a bunch of these that are just happening

2928
03:11:46,640 --> 03:11:48,640
at the same time

2929
03:11:48,640 --> 03:11:50,640
many of these happening at the same time

2930
03:11:50,640 --> 03:11:52,640
and yeah so

2931
03:11:52,640 --> 03:11:54,640
that's what attention is

2932
03:11:54,640 --> 03:11:56,640
that's what feedforward networks are

2933
03:11:56,640 --> 03:11:58,640
residual connections are

2934
03:12:00,640 --> 03:12:02,640
and yeah

2935
03:12:02,640 --> 03:12:04,640
and then so after this after we've

2936
03:12:04,640 --> 03:12:06,640
fed these into our decoders

2937
03:12:06,640 --> 03:12:08,640
we get an output

2938
03:12:08,640 --> 03:12:10,640
we apply linear transformation to summarize

2939
03:12:10,640 --> 03:12:12,640
softmax probabilities

2940
03:12:12,640 --> 03:12:14,640
and then we generate based on that

2941
03:12:14,640 --> 03:12:16,640
based on everything that we learned

2942
03:12:16,640 --> 03:12:18,640
and

2943
03:12:18,640 --> 03:12:20,640
actually what I didn't quite write a lot about

2944
03:12:20,640 --> 03:12:22,640
was the decoder

2945
03:12:22,640 --> 03:12:24,640
so what I'm actually going to talk about next

2946
03:12:24,640 --> 03:12:26,640
is something I didn't fill in yet

2947
03:12:26,640 --> 03:12:28,640
which is why

2948
03:12:28,640 --> 03:12:30,640
why the heck do we

2949
03:12:30,640 --> 03:12:32,640
use mass attention here

2950
03:12:32,640 --> 03:12:34,640
but not in these places so why the heck

2951
03:12:34,640 --> 03:12:36,640
do we have a multi attention here

2952
03:12:36,640 --> 03:12:38,640
all that attention here but mass attention here

2953
03:12:38,640 --> 03:12:40,640
so why is this

2954
03:12:40,640 --> 03:12:42,640
well the purpose of the encoder

2955
03:12:42,640 --> 03:12:44,640
is to pretty much learn

2956
03:12:44,640 --> 03:12:46,640
the present

2957
03:12:46,640 --> 03:12:48,640
past and future

2958
03:12:48,640 --> 03:12:50,640
and put that into a vector representation

2959
03:12:50,640 --> 03:12:52,640
for the decoder

2960
03:12:52,640 --> 03:12:54,640
that's what the encoder does

2961
03:12:54,640 --> 03:12:56,640
so it's okay if we look into the future

2962
03:12:56,640 --> 03:12:58,640
and understand tokens that way

2963
03:12:58,640 --> 03:13:00,640
because we're technically not cheating

2964
03:13:00,640 --> 03:13:02,640
we're just learning the different attention scores

2965
03:13:02,640 --> 03:13:04,640
and yeah we're just using that

2966
03:13:04,640 --> 03:13:06,640
to help us predict based on

2967
03:13:06,640 --> 03:13:08,640
what the sentence looks like

2968
03:13:08,640 --> 03:13:10,640
but not explicitly giving it away

2969
03:13:10,640 --> 03:13:12,640
just giving it an idea of

2970
03:13:12,640 --> 03:13:14,640
what to look for type of thing

2971
03:13:14,640 --> 03:13:16,640
and then

2972
03:13:16,640 --> 03:13:18,640
we use mass attention here because

2973
03:13:18,640 --> 03:13:20,640
well we don't want to look ahead

2974
03:13:20,640 --> 03:13:22,640
we want to look at the present and the past

2975
03:13:22,640 --> 03:13:24,640
and

2976
03:13:24,640 --> 03:13:26,640
later on

2977
03:13:26,640 --> 03:13:28,640
we're not giving anything explicit

2978
03:13:28,640 --> 03:13:30,640
here we're not giving anything yet

2979
03:13:30,640 --> 03:13:32,640
so we want to make some raw guesses

2980
03:13:32,640 --> 03:13:34,640
they're not going to be very good guesses at first

2981
03:13:34,640 --> 03:13:36,640
we want to make some raw guesses

2982
03:13:36,640 --> 03:13:38,640
and then later on

2983
03:13:38,640 --> 03:13:40,640
we can feed these

2984
03:13:40,640 --> 03:13:42,640
the added and normalized guesses

2985
03:13:42,640 --> 03:13:44,640
into

2986
03:13:44,640 --> 03:13:46,640
this next multi attention

2987
03:13:46,640 --> 03:13:48,640
which isn't masked

2988
03:13:48,640 --> 03:13:50,640
and then we can use

2989
03:13:50,640 --> 03:13:52,640
this max multi head attention

2990
03:13:52,640 --> 03:13:54,640
with the vector representation

2991
03:13:54,640 --> 03:13:56,640
given by the encoder

2992
03:13:56,640 --> 03:13:58,640
and then we can sort of do

2993
03:13:58,640 --> 03:14:00,640
more useful things with that

2994
03:14:00,640 --> 03:14:02,640
rather than just being forced to guess

2995
03:14:02,640 --> 03:14:04,640
raw attention scores

2996
03:14:04,640 --> 03:14:06,640
and then being judged for that

2997
03:14:06,640 --> 03:14:08,640
we can sort of introduce more

2998
03:14:08,640 --> 03:14:10,640
more and more elements

2999
03:14:10,640 --> 03:14:12,640
in this decoder block to help us learn more meaningful things

3000
03:14:12,640 --> 03:14:14,640
so

3001
03:14:14,640 --> 03:14:16,640
we start off with

3002
03:14:16,640 --> 03:14:18,640
making this

3003
03:14:18,640 --> 03:14:20,640
mass multi head attention

3004
03:14:20,640 --> 03:14:22,640
and then combining that

3005
03:14:22,640 --> 03:14:24,640
with

3006
03:14:24,640 --> 03:14:26,640
our

3007
03:14:26,640 --> 03:14:28,640
then afterwards we do a multi head attention

3008
03:14:28,640 --> 03:14:30,640
with the

3009
03:14:30,640 --> 03:14:32,640
vector representation from the encoder

3010
03:14:32,640 --> 03:14:34,640
and then we can make decisions on that

3011
03:14:34,640 --> 03:14:36,640
so that's kind of why that works

3012
03:14:36,640 --> 03:14:38,640
this way

3013
03:14:38,640 --> 03:14:40,640
if you don't think I explain it like amazingly

3014
03:14:40,640 --> 03:14:42,640
well you can totally just

3015
03:14:42,640 --> 03:14:44,640
ask GPT4

3016
03:14:44,640 --> 03:14:46,640
or GPT3.5

3017
03:14:46,640 --> 03:14:48,640
and get a pretty decent answer

3018
03:14:48,640 --> 03:14:50,640
but that's how that works

3019
03:14:50,640 --> 03:14:52,640
and

3020
03:14:52,640 --> 03:14:54,640
another thing I kind of wanted to point out here

3021
03:14:54,640 --> 03:14:56,640
is these linear transformations

3022
03:14:56,640 --> 03:14:58,640
that you see

3023
03:14:58,640 --> 03:15:00,640
I mean there's a lot of them

3024
03:15:00,640 --> 03:15:02,640
in the

3025
03:15:02,640 --> 03:15:04,640
scaled dot project attention

3026
03:15:04,640 --> 03:15:06,640
so you have your linears

3027
03:15:06,640 --> 03:15:08,640
for your value or key value

3028
03:15:08,640 --> 03:15:10,640
and key query and values

3029
03:15:10,640 --> 03:15:12,640
so

3030
03:15:12,640 --> 03:15:14,640
as well as the one up here

3031
03:15:14,640 --> 03:15:16,640
linears are great

3032
03:15:16,640 --> 03:15:18,640
for just expanding or shrinking

3033
03:15:18,640 --> 03:15:20,640
a bunch of important info

3034
03:15:20,640 --> 03:15:22,640
into something easier to work with

3035
03:15:22,640 --> 03:15:24,640
so if you have a bunch of

3036
03:15:24,640 --> 03:15:26,640
if you have a large vector containing a bunch

3037
03:15:26,640 --> 03:15:28,640
of info learned from this

3038
03:15:28,640 --> 03:15:30,640
scaled dot project attention

3039
03:15:30,640 --> 03:15:32,640
you can

3040
03:15:32,640 --> 03:15:34,640
you can sort of just compress

3041
03:15:34,640 --> 03:15:36,640
that into something more manageable

3042
03:15:36,640 --> 03:15:38,640
through a linear transformation

3043
03:15:38,640 --> 03:15:40,640
and it's essentially what's just happening here

3044
03:15:40,640 --> 03:15:42,640
with Softmax as well as

3045
03:15:42,640 --> 03:15:44,640
in our

3046
03:15:44,640 --> 03:15:46,640
scaled dot project attention here

3047
03:15:46,640 --> 03:15:48,640
for these linear transformations

3048
03:15:48,640 --> 03:15:50,640
from our inputs

3049
03:15:50,640 --> 03:15:52,640
to

3050
03:15:52,640 --> 03:15:54,640
quick keys, queries and values

3051
03:15:54,640 --> 03:15:56,640
that's all that's happening

3052
03:15:56,640 --> 03:15:58,640
if you want to read more about

3053
03:15:58,640 --> 03:16:00,640
linear transformations the importance of them

3054
03:16:00,640 --> 03:16:02,640
you can totally go out of your way to do that

3055
03:16:02,640 --> 03:16:04,640
but that's just sort of a brief summary

3056
03:16:04,640 --> 03:16:06,640
as to why they're important

3057
03:16:06,640 --> 03:16:08,640
just shrinking or expanding

3058
03:16:08,640 --> 03:16:10,640
so that's sort of a brief overview on how

3059
03:16:10,640 --> 03:16:12,640
transformers work

3060
03:16:12,640 --> 03:16:14,640
however in this

3061
03:16:14,640 --> 03:16:16,640
course we will not be building the transformer

3062
03:16:16,640 --> 03:16:18,640
architecture we'll be building

3063
03:16:18,640 --> 03:16:20,640
something called a GPT which you're probably familiar

3064
03:16:20,640 --> 03:16:22,640
with and GPT stands for

3065
03:16:22,640 --> 03:16:24,640
Generatively Pre-Trained Transformer

3066
03:16:24,640 --> 03:16:26,640
or Generative Pre-Trained Transformer

3067
03:16:26,640 --> 03:16:28,640
one of the two

3068
03:16:28,640 --> 03:16:30,640
and pretty much what this is

3069
03:16:30,640 --> 03:16:32,640
it's pretty close to the transformer

3070
03:16:32,640 --> 03:16:34,640
this architecture here except

3071
03:16:34,640 --> 03:16:36,640
it only adopts

3072
03:16:36,640 --> 03:16:38,640
the decoder blocks and it takes away

3073
03:16:38,640 --> 03:16:40,640
this multi-head attention here

3074
03:16:40,640 --> 03:16:42,640
so all we're doing is we're removing

3075
03:16:42,640 --> 03:16:44,640
the encoder

3076
03:16:44,640 --> 03:16:46,640
as well as what the encoder plugs into

3077
03:16:46,640 --> 03:16:48,640
so all we have left

3078
03:16:48,640 --> 03:16:50,640
is just some inputs

3079
03:16:50,640 --> 03:16:52,640
our max multi-head

3080
03:16:52,640 --> 03:16:54,640
attention

3081
03:16:54,640 --> 03:16:56,640
our post-norm architecture

3082
03:16:56,640 --> 03:16:58,640
and then

3083
03:16:58,640 --> 03:17:00,640
right after this we're not going to

3084
03:17:00,640 --> 03:17:02,640
a non-mass multi-head attention

3085
03:17:02,640 --> 03:17:04,640
but rather to a feed forward network

3086
03:17:04,640 --> 03:17:06,640
and then a post-norm

3087
03:17:06,640 --> 03:17:08,640
so that's all it is, it's just 1, 2, 3, 4

3088
03:17:08,640 --> 03:17:10,640
that's all it's going to look like

3089
03:17:10,640 --> 03:17:12,640
that's all the blocks are going to be

3090
03:17:12,640 --> 03:17:14,640
it is still important

3091
03:17:14,640 --> 03:17:16,640
to understand the transformer architecture itself

3092
03:17:16,640 --> 03:17:18,640
because you might need that in the future

3093
03:17:18,640 --> 03:17:20,640
and it is sort of a good practice in language

3094
03:17:20,640 --> 03:17:22,640
modeling to

3095
03:17:22,640 --> 03:17:24,640
have a grasp on and to understand

3096
03:17:24,640 --> 03:17:26,640
you know why we use mass multi-head

3097
03:17:26,640 --> 03:17:28,640
attention in the decoder and why we don't

3098
03:17:28,640 --> 03:17:30,640
use it in the encoder and stuff like that

3099
03:17:30,640 --> 03:17:32,640
so anyways

3100
03:17:32,640 --> 03:17:34,640
we're going to go ahead and build this

3101
03:17:34,640 --> 03:17:36,640
if you need to

3102
03:17:36,640 --> 03:17:38,640
look back if something wasn't quite clear

3103
03:17:38,640 --> 03:17:40,640
definitely skip back a few seconds

3104
03:17:40,640 --> 03:17:42,640
or a few minutes through the video and just

3105
03:17:42,640 --> 03:17:44,640
make sure you clarify everything up to this point

3106
03:17:44,640 --> 03:17:46,640
but yeah

3107
03:17:46,640 --> 03:17:48,640
I'm going to go over some more

3108
03:17:48,640 --> 03:17:50,640
math on the side here and just some other

3109
03:17:50,640 --> 03:17:52,640
little

3110
03:17:52,640 --> 03:17:54,640
little widgets we're going to need

3111
03:17:54,640 --> 03:17:56,640
for building the decoder

3112
03:17:56,640 --> 03:17:58,640
GPT architecture

3113
03:17:58,640 --> 03:18:00,640
so let's go ahead and do that

3114
03:18:00,640 --> 03:18:02,640
we're going to jump into

3115
03:18:02,640 --> 03:18:04,640
building the transformer rather than

3116
03:18:04,640 --> 03:18:06,640
building the GPT from scratch

3117
03:18:06,640 --> 03:18:08,640
what I want to do is linger on

3118
03:18:08,640 --> 03:18:10,640
self-attention for a little bit

3119
03:18:10,640 --> 03:18:12,640
or rather just the attention mechanism

3120
03:18:12,640 --> 03:18:14,640
and the matrix multiplication behind it

3121
03:18:14,640 --> 03:18:16,640
and why it works

3122
03:18:16,640 --> 03:18:18,640
so I'm going to use

3123
03:18:18,640 --> 03:18:20,640
whiteboard to illustrate this

3124
03:18:20,640 --> 03:18:22,640
so we're going to go ahead and draw out

3125
03:18:22,640 --> 03:18:24,640
a

3126
03:18:24,640 --> 03:18:26,640
we'll just use maybe a four token

3127
03:18:26,640 --> 03:18:28,640
sequence here of words

3128
03:18:28,640 --> 03:18:30,640
okay

3129
03:18:30,640 --> 03:18:32,640
so

3130
03:18:32,640 --> 03:18:34,640
we're going to highlight which words

3131
03:18:34,640 --> 03:18:36,640
are probably going to end up

3132
03:18:36,640 --> 03:18:38,640
correlating together

3133
03:18:38,640 --> 03:18:40,640
or the attention mechanism

3134
03:18:40,640 --> 03:18:42,640
is going to multiply them together

3135
03:18:42,640 --> 03:18:44,640
to a high amount based on what it learns

3136
03:18:44,640 --> 03:18:46,640
about those tokens this is what this is

3137
03:18:46,640 --> 03:18:48,640
so I'm going to help us illustrate that

3138
03:18:48,640 --> 03:18:50,640
and what the

3139
03:18:50,640 --> 03:18:52,640
GPT is going to see

3140
03:18:52,640 --> 03:18:54,640
sort of from the inside what it looks like from the inside

3141
03:18:54,640 --> 03:18:56,640
so

3142
03:18:56,640 --> 03:18:58,640
I'm going to go ahead and draw this out here

3143
03:19:02,640 --> 03:19:04,640
just make a table here

3144
03:19:04,640 --> 03:19:06,640
we'll give it

3145
03:19:10,640 --> 03:19:12,640
four of these

3146
03:19:14,640 --> 03:19:16,640
and draw a little line through the middle

3147
03:19:16,640 --> 03:19:18,640
my drawing might not be

3148
03:19:18,640 --> 03:19:20,640
perfect but it's definitely better

3149
03:19:20,640 --> 03:19:22,640
than on paper

3150
03:19:22,640 --> 03:19:24,640
so cool we have this

3151
03:19:24,640 --> 03:19:26,640
we have

3152
03:19:26,640 --> 03:19:28,640
my

3153
03:19:30,640 --> 03:19:32,640
I'm going to go here

3154
03:19:32,640 --> 03:19:34,640
dog

3155
03:19:38,640 --> 03:19:40,640
has

3156
03:19:40,640 --> 03:19:42,640
please

3157
03:19:42,640 --> 03:19:44,640
and then my

3158
03:19:46,640 --> 03:19:48,640
my dog

3159
03:19:50,640 --> 03:19:52,640
so I delete that

3160
03:19:54,640 --> 03:19:56,640
my dog has

3161
03:19:56,640 --> 03:19:58,640
please

3162
03:19:58,640 --> 03:20:00,640
cool

3163
03:20:00,640 --> 03:20:02,640
so to what degree

3164
03:20:02,640 --> 03:20:04,640
are these going to interact well my and my

3165
03:20:04,640 --> 03:20:06,640
I mean it doesn't really

3166
03:20:06,640 --> 03:20:08,640
give away that much it's only just the start

3167
03:20:08,640 --> 03:20:10,640
so maybe this will interact to

3168
03:20:10,640 --> 03:20:12,640
a low amount

3169
03:20:12,640 --> 03:20:14,640
and then you have my and dog

3170
03:20:14,640 --> 03:20:16,640
these might interact to a medium

3171
03:20:16,640 --> 03:20:18,640
amount because it's like your dog

3172
03:20:18,640 --> 03:20:20,640
so we might go

3173
03:20:20,640 --> 03:20:22,640
we might go medium

3174
03:20:22,640 --> 03:20:24,640
like that

3175
03:20:24,640 --> 03:20:26,640
and then my and has well that doesn't give away too much

3176
03:20:26,640 --> 03:20:28,640
so maybe that'll be low

3177
03:20:28,640 --> 03:20:30,640
and then my and please it's like oh

3178
03:20:30,640 --> 03:20:32,640
that doesn't really mean much my please that doesn't

3179
03:20:32,640 --> 03:20:34,640
really make sense maybe we'll

3180
03:20:34,640 --> 03:20:36,640
have it interact to a low amount

3181
03:20:36,640 --> 03:20:38,640
and then

3182
03:20:38,640 --> 03:20:40,640
these would be the same

3183
03:20:40,640 --> 03:20:42,640
thing so

3184
03:20:42,640 --> 03:20:44,640
my and dog so be medium

3185
03:20:44,640 --> 03:20:46,640
and then has and has

3186
03:20:46,640 --> 03:20:48,640
would be low

3187
03:20:48,640 --> 03:20:50,640
and then my and please would also be low

3188
03:20:50,640 --> 03:20:52,640
and then you have dog and dog

3189
03:20:52,640 --> 03:20:54,640
so these might interact to a low amount they're the same word

3190
03:20:54,640 --> 03:20:56,640
so we'll just

3191
03:20:56,640 --> 03:20:58,640
forget about that and then we have

3192
03:20:58,640 --> 03:21:00,640
a dog has

3193
03:21:00,640 --> 03:21:02,640
so these might interact to a medium amount

3194
03:21:02,640 --> 03:21:04,640
dog has the dog has

3195
03:21:04,640 --> 03:21:06,640
something

3196
03:21:06,640 --> 03:21:08,640
and then dog and please

3197
03:21:08,640 --> 03:21:10,640
these might interact to a high amount

3198
03:21:10,640 --> 03:21:12,640
because they're associating the dog

3199
03:21:12,640 --> 03:21:14,640
with something else meaning please

3200
03:21:14,640 --> 03:21:16,640
we have has

3201
03:21:16,640 --> 03:21:18,640
and dog these would interact to the same amount so

3202
03:21:18,640 --> 03:21:20,640
medium and then has and has

3203
03:21:20,640 --> 03:21:22,640
be

3204
03:21:22,640 --> 03:21:24,640
probably

3205
03:21:24,640 --> 03:21:26,640
to a low amount

3206
03:21:26,640 --> 03:21:28,640
and then

3207
03:21:28,640 --> 03:21:30,640
we could do low for

3208
03:21:30,640 --> 03:21:32,640
we could do what was it high

3209
03:21:32,640 --> 03:21:34,640
for this one as well please and dog

3210
03:21:34,640 --> 03:21:36,640
so these will interact

3211
03:21:36,640 --> 03:21:38,640
to a high amount

3212
03:21:38,640 --> 03:21:40,640
and then we have has and please

3213
03:21:40,640 --> 03:21:42,640
so

3214
03:21:42,640 --> 03:21:44,640
these could interact maybe a medium

3215
03:21:44,640 --> 03:21:46,640
amount

3216
03:21:46,640 --> 03:21:48,640
medium and then please and please which would be low

3217
03:21:48,640 --> 03:21:50,640
so what you get

3218
03:21:50,640 --> 03:21:52,640
I'll just highlight this in

3219
03:21:52,640 --> 03:21:54,640
I'll just highlight this in green here

3220
03:21:54,640 --> 03:21:56,640
so you get

3221
03:21:56,640 --> 03:21:58,640
all the medium

3222
03:21:58,640 --> 03:22:00,640
and high attention scores

3223
03:22:00,640 --> 03:22:02,640
you'd have your medium here

3224
03:22:02,640 --> 03:22:04,640
medium here

3225
03:22:04,640 --> 03:22:06,640
high medium

3226
03:22:06,640 --> 03:22:08,640
medium high

3227
03:22:08,640 --> 03:22:10,640
medium and medium

3228
03:22:10,640 --> 03:22:12,640
so you can see that these are sort of symmetrical

3229
03:22:12,640 --> 03:22:14,640
and this is what the attention map

3230
03:22:14,640 --> 03:22:16,640
will look like of course there's going to be some

3231
03:22:16,640 --> 03:22:18,640
scaling going on here based on the amount

3232
03:22:18,640 --> 03:22:20,640
of actual attention's

3233
03:22:20,640 --> 03:22:22,640
heads we have running in parallel

3234
03:22:22,640 --> 03:22:24,640
but that's besides the point

3235
03:22:24,640 --> 03:22:26,640
really what's going on here

3236
03:22:26,640 --> 03:22:28,640
is the network

3237
03:22:28,640 --> 03:22:30,640
is going to learn how to place

3238
03:22:30,640 --> 03:22:32,640
the right

3239
03:22:32,640 --> 03:22:34,640
attention scores because attention is simply

3240
03:22:34,640 --> 03:22:36,640
being used to generate tokens

3241
03:22:36,640 --> 03:22:38,640
that's that's how the

3242
03:22:38,640 --> 03:22:40,640
that's how the GPT works it's using attention

3243
03:22:40,640 --> 03:22:42,640
to generate tokens

3244
03:22:42,640 --> 03:22:44,640
so we can make

3245
03:22:44,640 --> 03:22:46,640
those sort of attention

3246
03:22:46,640 --> 03:22:48,640
scores how they're placed

3247
03:22:48,640 --> 03:22:50,640
we can make those learnable

3248
03:22:50,640 --> 03:22:52,640
through all of the like embeddings

3249
03:22:52,640 --> 03:22:54,640
like everything we have in the entire

3250
03:22:54,640 --> 03:22:56,640
network can make sure

3251
03:22:56,640 --> 03:22:58,640
that we place effective attention scores

3252
03:22:58,640 --> 03:23:00,640
and to make sure that they're measured properly

3253
03:23:00,640 --> 03:23:02,640
so

3254
03:23:02,640 --> 03:23:04,640
obviously I didn't quantify these very well

3255
03:23:04,640 --> 03:23:06,640
like not with floating point numbers

3256
03:23:06,640 --> 03:23:08,640
but this is sort of the premise

3257
03:23:08,640 --> 03:23:10,640
of how it works and how we want

3258
03:23:10,640 --> 03:23:12,640
the model to look at different tokens

3259
03:23:12,640 --> 03:23:14,640
and how they relate to one another

3260
03:23:14,640 --> 03:23:16,640
so that's what the

3261
03:23:16,640 --> 03:23:18,640
attention mechanism looks like under the hood

3262
03:23:18,640 --> 03:23:20,640
so this is what the actual

3263
03:23:20,640 --> 03:23:22,640
GPT or decoder only

3264
03:23:22,640 --> 03:23:24,640
transformer architecture looks like

3265
03:23:24,640 --> 03:23:26,640
and

3266
03:23:26,640 --> 03:23:28,640
so I'm just going to go through this step by step here

3267
03:23:28,640 --> 03:23:30,640
and then we can hopefully jump into some of the math

3268
03:23:30,640 --> 03:23:32,640
and code behind how this works

3269
03:23:32,640 --> 03:23:34,640
so we have

3270
03:23:34,640 --> 03:23:36,640
our inputs embeddings and positional

3271
03:23:36,640 --> 03:23:38,640
encodings we have only decoder

3272
03:23:38,640 --> 03:23:40,640
blocks and then some

3273
03:23:40,640 --> 03:23:42,640
linear transformation

3274
03:23:42,640 --> 03:23:44,640
and then pretty much just

3275
03:23:44,640 --> 03:23:46,640
we do some softmax

3276
03:23:46,640 --> 03:23:48,640
probability distribution

3277
03:23:48,640 --> 03:23:50,640
we sample from those and then we

3278
03:23:50,640 --> 03:23:52,640
start just generating some output

3279
03:23:52,640 --> 03:23:54,640
and then we compare those to our inputs

3280
03:23:54,640 --> 03:23:56,640
and see how off they were, optimized from that

3281
03:23:56,640 --> 03:23:58,640
in each of these

3282
03:23:58,640 --> 03:24:00,640
decoder blocks we have our all data

3283
03:24:00,640 --> 03:24:02,640
attention, res connections

3284
03:24:02,640 --> 03:24:04,640
feedforward network consists

3285
03:24:04,640 --> 03:24:06,640
of a linear, real linear

3286
03:24:06,640 --> 03:24:08,640
border and then

3287
03:24:08,640 --> 03:24:10,640
another res connection

3288
03:24:10,640 --> 03:24:12,640
in each of these multi-attentions

3289
03:24:12,640 --> 03:24:14,640
we have

3290
03:24:14,640 --> 03:24:16,640
multiple heads running in parallel

3291
03:24:16,640 --> 03:24:18,640
and each of these heads is going to take a

3292
03:24:18,640 --> 03:24:20,640
key, query and value

3293
03:24:20,640 --> 03:24:22,640
these are all learnable

3294
03:24:22,640 --> 03:24:24,640
linear transformations

3295
03:24:24,640 --> 03:24:26,640
and

3296
03:24:26,640 --> 03:24:28,640
we're going to basically dot product the key and query together

3297
03:24:28,640 --> 03:24:30,640
concatenate these results

3298
03:24:30,640 --> 03:24:32,640
and

3299
03:24:32,640 --> 03:24:34,640
do a little transformation to sort of

3300
03:24:34,640 --> 03:24:36,640
summarize it afterwards

3301
03:24:36,640 --> 03:24:38,640
and then what actually goes on in the

3302
03:24:38,640 --> 03:24:40,640
dot product attention is just the dot

3303
03:24:40,640 --> 03:24:42,640
product meaning of the key and query

3304
03:24:42,640 --> 03:24:44,640
the scaling to prevent

3305
03:24:44,640 --> 03:24:46,640
these values from exploding

3306
03:24:46,640 --> 03:24:48,640
to prevent the vanishing gradient problem

3307
03:24:48,640 --> 03:24:50,640
and then we have our

3308
03:24:50,640 --> 03:24:52,640
masking to make sure that

3309
03:24:52,640 --> 03:24:54,640
these, to make sure the model

3310
03:24:54,640 --> 03:24:56,640
isn't looking ahead and cheating

3311
03:24:56,640 --> 03:24:58,640
and then softmax matrix multiply

3312
03:24:58,640 --> 03:25:00,640
we output that and then

3313
03:25:00,640 --> 03:25:02,640
kind of fill in the blank there, so cool

3314
03:25:02,640 --> 03:25:04,640
this is a little bit

3315
03:25:04,640 --> 03:25:06,640
pretty much the

3316
03:25:06,640 --> 03:25:08,640
transform architecture a little bit dumb

3317
03:25:08,640 --> 03:25:10,640
down a little smaller

3318
03:25:10,640 --> 03:25:12,640
in complexity to actually understand but

3319
03:25:12,640 --> 03:25:14,640
that's kind of the premise of what's going on here

3320
03:25:14,640 --> 03:25:16,640
so still

3321
03:25:16,640 --> 03:25:18,640
implements a self-attention mechanism

3322
03:25:20,640 --> 03:25:22,640
so as you can see now

3323
03:25:22,640 --> 03:25:24,640
I am currently

3324
03:25:24,640 --> 03:25:26,640
on my macbook

3325
03:25:26,640 --> 03:25:28,640
M2 chip, I'm not going to

3326
03:25:28,640 --> 03:25:30,640
go into the specs of why it's important

3327
03:25:30,640 --> 03:25:32,640
but really quick, I'm just going to show you

3328
03:25:32,640 --> 03:25:34,640
how I SSH onto my other PC

3329
03:25:34,640 --> 03:25:36,640
so I go

3330
03:25:36,640 --> 03:25:38,640
SSH

3331
03:25:38,640 --> 03:25:40,640
just like that and then I type in my

3332
03:25:40,640 --> 03:25:42,640
ipv4 address

3333
03:25:42,640 --> 03:25:44,640
and then

3334
03:25:44,640 --> 03:25:46,640
I just

3335
03:25:46,640 --> 03:25:48,640
get a simple password

3336
03:25:48,640 --> 03:25:50,640
here, password that I've never had

3337
03:25:50,640 --> 03:25:52,640
is cool

3338
03:25:52,640 --> 03:25:54,640
so now I'm on my desktop computer

3339
03:25:54,640 --> 03:25:56,640
and this is the command prompt that I use for it

3340
03:25:56,640 --> 03:25:58,640
so awesome

3341
03:25:58,640 --> 03:26:00,640
I'm going to go ahead and go into the

3342
03:26:00,640 --> 03:26:02,640
free code camp

3343
03:26:02,640 --> 03:26:04,640
little directory I have

3344
03:26:04,640 --> 03:26:06,640
so cd desktop

3345
03:26:06,640 --> 03:26:08,640
cd python testing

3346
03:26:08,640 --> 03:26:10,640
and then here I'm actually going to activate

3347
03:26:10,640 --> 03:26:12,640
my CUDA virtual

3348
03:26:12,640 --> 03:26:14,640
environment

3349
03:26:14,640 --> 03:26:16,640
oops, not accelerate

3350
03:26:16,640 --> 03:26:18,640
I'm going to go CUDA

3351
03:26:18,640 --> 03:26:20,640
activate

3352
03:26:20,640 --> 03:26:22,640
cool and then I'm going to go

3353
03:26:22,640 --> 03:26:24,640
cd into free code camp

3354
03:26:24,640 --> 03:26:26,640
gbt course, awesome

3355
03:26:26,640 --> 03:26:28,640
so now, if I actually do

3356
03:26:28,640 --> 03:26:30,640
code on here like this to open up my

3357
03:26:30,640 --> 03:26:32,640
VS code, it doesn't do that

3358
03:26:32,640 --> 03:26:34,640
so there's another little way I have to do this

3359
03:26:34,640 --> 03:26:36,640
and you have to go into

3360
03:26:36,640 --> 03:26:38,640
VS code

3361
03:26:38,640 --> 03:26:40,640
go into a little remote explorer here

3362
03:26:40,640 --> 03:26:42,640
and then you can simply connect

3363
03:26:42,640 --> 03:26:44,640
so I'm just going to connect

3364
03:26:44,640 --> 03:26:46,640
to the current window

3365
03:26:46,640 --> 03:26:48,640
itself

3366
03:26:48,640 --> 03:26:50,640
there's an extension you need for this

3367
03:26:50,640 --> 03:26:52,640
called open SSH server, I think it's what it's called

3368
03:26:52,640 --> 03:26:54,640
and

3369
03:26:54,640 --> 03:26:56,640
it's simply the same password I used in the command prompt

3370
03:26:56,640 --> 03:26:58,640
I can type it correctly

3371
03:27:04,640 --> 03:27:06,640
awesome

3372
03:27:06,640 --> 03:27:08,640
so now it's SSH into my computer

3373
03:27:08,640 --> 03:27:10,640
upstairs

3374
03:27:10,640 --> 03:27:12,640
and I'm just going to open the little editor in here

3375
03:27:14,640 --> 03:27:16,640
nice, so you can see

3376
03:27:16,640 --> 03:27:18,640
that it looks just like that, that's wonderful

3377
03:27:18,640 --> 03:27:20,640
so now

3378
03:27:20,640 --> 03:27:22,640
I'm going to open this in a Jupyter notebook

3379
03:27:24,640 --> 03:27:26,640
actually

3380
03:27:26,640 --> 03:27:28,640
cd into desktop here

3381
03:27:28,640 --> 03:27:30,640
cd python

3382
03:27:30,640 --> 03:27:32,640
cd python testing

3383
03:27:32,640 --> 03:27:34,640
CUDA scripts

3384
03:27:34,640 --> 03:27:36,640
activate

3385
03:27:36,640 --> 03:27:38,640
cd free code camp

3386
03:27:38,640 --> 03:27:40,640
gbt course and then code

3387
03:27:40,640 --> 03:27:42,640
like that and it will open

3388
03:27:42,640 --> 03:27:44,640
perfect

3389
03:27:44,640 --> 03:27:46,640
how wonderful is that and I've already done

3390
03:27:46,640 --> 03:27:48,640
a little bit of this here but

3391
03:27:48,640 --> 03:27:50,640
we're going to

3392
03:27:50,640 --> 03:27:52,640
jump into exactly

3393
03:27:52,640 --> 03:27:54,640
how we can build up this transformer

3394
03:27:54,640 --> 03:27:56,640
or gbt architecture

3395
03:27:56,640 --> 03:27:58,640
in the code itself

3396
03:27:58,640 --> 03:28:00,640
so I'm going to

3397
03:28:00,640 --> 03:28:02,640
pop over to my Jupyter notebook in here

3398
03:28:06,640 --> 03:28:08,640
cool and now this little address

3399
03:28:08,640 --> 03:28:10,640
I'm going to paste that

3400
03:28:10,640 --> 03:28:12,640
into my

3401
03:28:12,640 --> 03:28:14,640
browser

3402
03:28:14,640 --> 03:28:16,640
awesome

3403
03:28:16,640 --> 03:28:18,640
so we have this gbt v1

3404
03:28:18,640 --> 03:28:20,640
Jupyter notebook

3405
03:28:22,640 --> 03:28:24,640
so what I've actually done is

3406
03:28:24,640 --> 03:28:26,640
I've done some importations here

3407
03:28:26,640 --> 03:28:28,640
so I've

3408
03:28:28,640 --> 03:28:30,640
imported all of these

3409
03:28:30,640 --> 03:28:32,640
python importations

3410
03:28:32,640 --> 03:28:34,640
all the hyper parameters that we used from before

3411
03:28:36,640 --> 03:28:38,640
I've imported the data loader

3412
03:28:38,640 --> 03:28:40,640
I've imported the tokenizer

3413
03:28:40,640 --> 03:28:42,640
the train and bell splits

3414
03:28:42,640 --> 03:28:44,640
they get batch function

3415
03:28:44,640 --> 03:28:46,640
estimate loss, just everything

3416
03:28:46,640 --> 03:28:48,640
that we're going to need and it's all in

3417
03:28:48,640 --> 03:28:50,640
neatly organized little code blocks

3418
03:28:50,640 --> 03:28:52,640
so awesome

3419
03:28:52,640 --> 03:28:54,640
now what?

3420
03:28:54,640 --> 03:28:56,640
well let's go ahead and continue here

3421
03:28:56,640 --> 03:28:58,640
with the actual

3422
03:28:58,640 --> 03:29:00,640
upgrading

3423
03:29:00,640 --> 03:29:02,640
from the very

3424
03:29:02,640 --> 03:29:04,640
top level so I remember

3425
03:29:04,640 --> 03:29:06,640
I actually showed

3426
03:29:06,640 --> 03:29:08,640
and you can skip back to this

3427
03:29:08,640 --> 03:29:10,640
I actually showed

3428
03:29:10,640 --> 03:29:12,640
the architecture of the gbt

3429
03:29:12,640 --> 03:29:14,640
sort of

3430
03:29:14,640 --> 03:29:16,640
lined out in I guess a little sketch

3431
03:29:16,640 --> 03:29:18,640
a little sketch that I did

3432
03:29:18,640 --> 03:29:20,640
and all we're going to do

3433
03:29:20,640 --> 03:29:22,640
is pretty much build up from the high level

3434
03:29:22,640 --> 03:29:24,640
the high high level general

3435
03:29:24,640 --> 03:29:26,640
architecture down to the technical stuff

3436
03:29:26,640 --> 03:29:28,640
down to the very root

3437
03:29:28,640 --> 03:29:30,640
dot product attention

3438
03:29:30,640 --> 03:29:32,640
that we're going to be doing here

3439
03:29:32,640 --> 03:29:34,640
so I'm going to go ahead and start off

3440
03:29:34,640 --> 03:29:36,640
with this

3441
03:29:36,640 --> 03:29:38,640
gbt language model which I just

3442
03:29:38,640 --> 03:29:40,640
renamed I replaced

3443
03:29:40,640 --> 03:29:42,640
bygram

3444
03:29:42,640 --> 03:29:44,640
with gbt here

3445
03:29:44,640 --> 03:29:46,640
so that's all we're doing and

3446
03:29:46,640 --> 03:29:48,640
we're going to add some

3447
03:29:48,640 --> 03:29:50,640
little code bits and

3448
03:29:50,640 --> 03:29:52,640
just walk through step by step

3449
03:29:52,640 --> 03:29:54,640
what we're doing so

3450
03:29:54,640 --> 03:29:56,640
let's do that so great

3451
03:29:56,640 --> 03:29:58,640
we're going to next we're going to talk about

3452
03:29:58,640 --> 03:30:00,640
these positional encodings

3453
03:30:00,640 --> 03:30:02,640
so I go back to the paper here

3454
03:30:02,640 --> 03:30:04,640
rather this architecture

3455
03:30:04,640 --> 03:30:06,640
we initially have our tokenize inputs

3456
03:30:06,640 --> 03:30:08,640
and then we give

3457
03:30:08,640 --> 03:30:10,640
we give them embedding

3458
03:30:10,640 --> 03:30:12,640
so token embeddings and then a positional

3459
03:30:12,640 --> 03:30:14,640
encoding so this positional

3460
03:30:14,640 --> 03:30:16,640
encoding going back to the attention paper is right here

3461
03:30:16,640 --> 03:30:18,640
so all it does

3462
03:30:18,640 --> 03:30:20,640
is every

3463
03:30:20,640 --> 03:30:22,640
even token index

3464
03:30:22,640 --> 03:30:24,640
we apply this function

3465
03:30:24,640 --> 03:30:26,640
and then every odd token index

3466
03:30:26,640 --> 03:30:28,640
we apply this function you don't really need to know

3467
03:30:28,640 --> 03:30:30,640
what it's doing other than

3468
03:30:30,640 --> 03:30:32,640
the fact that these are the different sine

3469
03:30:32,640 --> 03:30:34,640
and cosine functions that it uses

3470
03:30:34,640 --> 03:30:36,640
to apply positional encodings

3471
03:30:36,640 --> 03:30:38,640
to the tokenized inputs

3472
03:30:38,640 --> 03:30:40,640
so every

3473
03:30:40,640 --> 03:30:42,640
so on our first

3474
03:30:42,640 --> 03:30:44,640
index or whatever let's say we have hello world

3475
03:30:44,640 --> 03:30:46,640
okay there's five characters here

3476
03:30:46,640 --> 03:30:48,640
h will be index zero

3477
03:30:48,640 --> 03:30:50,640
so it'll get an even

3478
03:30:50,640 --> 03:30:52,640
encoding function

3479
03:30:52,640 --> 03:30:54,640
and then e will be odd

3480
03:30:54,640 --> 03:30:56,640
since it's index one so it'll get this one

3481
03:30:56,640 --> 03:30:58,640
and then l will get this the next l will get

3482
03:30:58,640 --> 03:31:00,640
this and then

3483
03:31:00,640 --> 03:31:02,640
or I don't know if I messed up that

3484
03:31:02,640 --> 03:31:04,640
order but essentially it just iterates

3485
03:31:04,640 --> 03:31:06,640
and it goes back and forth between

3486
03:31:06,640 --> 03:31:08,640
those applying these fixed functions

3487
03:31:08,640 --> 03:31:10,640
and the thing is with fixed functions

3488
03:31:10,640 --> 03:31:12,640
is that they don't actually

3489
03:31:12,640 --> 03:31:14,640
learn about the data at all

3490
03:31:14,640 --> 03:31:16,640
because they're fixed so another way we could

3491
03:31:16,640 --> 03:31:18,640
do this would be using

3492
03:31:18,640 --> 03:31:20,640
nn.embedding which is what we use

3493
03:31:20,640 --> 03:31:22,640
for the token

3494
03:31:22,640 --> 03:31:24,640
embedding so I'm going to go ahead

3495
03:31:24,640 --> 03:31:26,640
and implement this here in our

3496
03:31:26,640 --> 03:31:28,640
gbtv one script so I'm going to go

3497
03:31:28,640 --> 03:31:30,640
ahead and add on this line

3498
03:31:30,640 --> 03:31:32,640
self dot positional

3499
03:31:32,640 --> 03:31:34,640
self dot position embedding table

3500
03:31:34,640 --> 03:31:36,640
nn.embedding block size

3501
03:31:36,640 --> 03:31:38,640
so the block size is the length

3502
03:31:38,640 --> 03:31:40,640
or the sequence length

3503
03:31:40,640 --> 03:31:42,640
which in our case

3504
03:31:42,640 --> 03:31:44,640
it's going to be 8 so there's going to be 8 tokens

3505
03:31:44,640 --> 03:31:46,640
and

3506
03:31:46,640 --> 03:31:48,640
this means

3507
03:31:48,640 --> 03:31:50,640
we're going to have 8 different indices

3508
03:31:50,640 --> 03:31:52,640
and each one is going to be

3509
03:31:52,640 --> 03:31:54,640
of size nn.embed

3510
03:31:54,640 --> 03:31:56,640
and this is a new parameter I actually want to add here

3511
03:31:56,640 --> 03:31:58,640
so

3512
03:31:58,640 --> 03:32:00,640
nn.embed will not only be used

3513
03:32:00,640 --> 03:32:02,640
in positional embedding

3514
03:32:02,640 --> 03:32:04,640
but it will also be used in our

3515
03:32:04,640 --> 03:32:06,640
token embedding because when we actually

3516
03:32:06,640 --> 03:32:08,640
store

3517
03:32:08,640 --> 03:32:10,640
information about the tokens

3518
03:32:10,640 --> 03:32:12,640
we want that to be in a very large

3519
03:32:12,640 --> 03:32:14,640
vector so not necessarily

3520
03:32:14,640 --> 03:32:16,640
a probability distribution

3521
03:32:16,640 --> 03:32:18,640
or what we were using before in the

3522
03:32:18,640 --> 03:32:20,640
bi-gram language model but rather

3523
03:32:20,640 --> 03:32:22,640
a really large vector

3524
03:32:22,640 --> 03:32:24,640
or a list you could think about it

3525
03:32:24,640 --> 03:32:26,640
as a bunch of different

3526
03:32:26,640 --> 03:32:28,640
attributes that

3527
03:32:28,640 --> 03:32:30,640
are about a character so maybe

3528
03:32:30,640 --> 03:32:32,640
you know

3529
03:32:32,640 --> 03:32:34,640
A and E would be pretty close

3530
03:32:34,640 --> 03:32:36,640
but both vowels versus like

3531
03:32:36,640 --> 03:32:38,640
E and Z

3532
03:32:38,640 --> 03:32:40,640
would be very different because Z is not

3533
03:32:40,640 --> 03:32:42,640
a very common letter and E is the most common letter

3534
03:32:42,640 --> 03:32:44,640
in the alphabet so

3535
03:32:44,640 --> 03:32:46,640
we pretty much just want to have

3536
03:32:46,640 --> 03:32:48,640
vectors to differentiate

3537
03:32:48,640 --> 03:32:50,640
these tokens to place some

3538
03:32:50,640 --> 03:32:52,640
semantic meaning on them

3539
03:32:52,640 --> 03:32:54,640
and anyways

3540
03:32:54,640 --> 03:32:56,640
that's a little talk about what token embedding table

3541
03:32:56,640 --> 03:32:58,640
is going to do when we add n.embed

3542
03:32:58,640 --> 03:33:00,640
and then positional embedding table

3543
03:33:00,640 --> 03:33:02,640
is just the same thing

3544
03:33:02,640 --> 03:33:04,640
but instead of each character

3545
03:33:04,640 --> 03:33:06,640
having its own thing

3546
03:33:06,640 --> 03:33:08,640
each letter

3547
03:33:08,640 --> 03:33:10,640
index in the input is going to have its own embedding

3548
03:33:10,640 --> 03:33:12,640
so I can go and add this

3549
03:33:12,640 --> 03:33:14,640
up here

3550
03:33:14,640 --> 03:33:16,640
the n.embed

3551
03:33:16,640 --> 03:33:18,640
and we can just make this

3552
03:33:18,640 --> 03:33:20,640
maybe 384

3553
03:33:20,640 --> 03:33:22,640
so 384 is quite huge

3554
03:33:22,640 --> 03:33:24,640
and it's maybe a little too big

3555
03:33:24,640 --> 03:33:26,640
for your PC but we'll see in a second

3556
03:33:26,640 --> 03:33:28,640
so

3557
03:33:28,640 --> 03:33:30,640
what this is going to do is it's going to have a giant vector

3558
03:33:30,640 --> 03:33:32,640
it's going to be like

3559
03:33:32,640 --> 03:33:34,640
we could say like

3560
03:33:34,640 --> 03:33:36,640
embedding

3561
03:33:36,640 --> 03:33:38,640
embedding vector

3562
03:33:38,640 --> 03:33:40,640
and then it would be like this

3563
03:33:40,640 --> 03:33:42,640
and you would have

3564
03:33:42,640 --> 03:33:44,640
a bunch of different attributes so like 0.1

3565
03:33:44,640 --> 03:33:46,640
0.2

3566
03:33:46,640 --> 03:33:48,640
0.8

3567
03:33:48,640 --> 03:33:50,640
1.1

3568
03:33:50,640 --> 03:33:52,640
right? except

3569
03:33:52,640 --> 03:33:54,640
instead of 4 this is

3570
03:33:54,640 --> 03:33:56,640
384 elements long

3571
03:33:56,640 --> 03:33:58,640
and each of these

3572
03:33:58,640 --> 03:34:00,640
is just going to store a tiny little attribute

3573
03:34:00,640 --> 03:34:02,640
about that token

3574
03:34:02,640 --> 03:34:04,640
so

3575
03:34:04,640 --> 03:34:06,640
let's say we maybe had like a

3576
03:34:06,640 --> 03:34:08,640
two dimensional and we were using a word

3577
03:34:08,640 --> 03:34:10,640
so if we had

3578
03:34:10,640 --> 03:34:12,640
sad versus

3579
03:34:12,640 --> 03:34:14,640
happy

3580
03:34:14,640 --> 03:34:16,640
sad might be

3581
03:34:16,640 --> 03:34:18,640
sad might be

3582
03:34:18,640 --> 03:34:20,640
0.1

3583
03:34:20,640 --> 03:34:22,640
and then

3584
03:34:22,640 --> 03:34:24,640
0.8

3585
03:34:24,640 --> 03:34:26,640
or 0.8

3586
03:34:26,640 --> 03:34:28,640
whereas happy

3587
03:34:28,640 --> 03:34:30,640
sad would be

3588
03:34:30,640 --> 03:34:32,640
maybe the positivity

3589
03:34:32,640 --> 03:34:34,640
of what it's saying and then 0.8 would be

3590
03:34:34,640 --> 03:34:36,640
is it showing some sort of emotion

3591
03:34:36,640 --> 03:34:38,640
which is a lot right?

3592
03:34:38,640 --> 03:34:40,640
it's 80% emotion

3593
03:34:40,640 --> 03:34:42,640
and 0.1

3594
03:34:42,640 --> 03:34:44,640
of maybe positive sentiment

3595
03:34:44,640 --> 03:34:46,640
and then if we had

3596
03:34:46,640 --> 03:34:48,640
0.9

3597
03:34:48,640 --> 03:34:50,640
would be happy because it's happy

3598
03:34:50,640 --> 03:34:52,640
it's very good and then 0.8

3599
03:34:52,640 --> 03:34:54,640
is emotional because they're sort of the same

3600
03:34:54,640 --> 03:34:56,640
emotional level

3601
03:34:56,640 --> 03:34:58,640
but yeah so this is what our embedding vectors

3602
03:34:58,640 --> 03:35:00,640
are pretty much describing and

3603
03:35:00,640 --> 03:35:02,640
all this hyperparameter

3604
03:35:02,640 --> 03:35:04,640
is concerned with is how long

3605
03:35:04,640 --> 03:35:06,640
that vector actually is

3606
03:35:06,640 --> 03:35:08,640
so anyways

3607
03:35:08,640 --> 03:35:10,640
let's continue with the GPT

3608
03:35:10,640 --> 03:35:12,640
language model class so the next bit I like

3609
03:35:12,640 --> 03:35:14,640
to talk about is how many decoder

3610
03:35:14,640 --> 03:35:16,640
layers we have

3611
03:35:16,640 --> 03:35:18,640
so in here let's just say we have

3612
03:35:18,640 --> 03:35:20,640
four decoder layers

3613
03:35:20,640 --> 03:35:22,640
so we have four of these it's going to go through this one

3614
03:35:22,640 --> 03:35:24,640
and then this one and then this one

3615
03:35:24,640 --> 03:35:26,640
then this one this is all happening

3616
03:35:26,640 --> 03:35:28,640
sequentially so we could

3617
03:35:28,640 --> 03:35:30,640
actually make a little

3618
03:35:30,640 --> 03:35:32,640
sequential neural network with

3619
03:35:32,640 --> 03:35:34,640
four decoder layers

3620
03:35:34,640 --> 03:35:36,640
so I'm actually going to add this in

3621
03:35:36,640 --> 03:35:38,640
and then a little bit of extra code which I'll explain

3622
03:35:38,640 --> 03:35:40,640
in a second here so this

3623
03:35:40,640 --> 03:35:42,640
self

3624
03:35:42,640 --> 03:35:44,640
dot blocks is how many

3625
03:35:44,640 --> 03:35:46,640
decoder blocks we have running

3626
03:35:46,640 --> 03:35:48,640
sequentially or layers

3627
03:35:48,640 --> 03:35:50,640
blocks and layers can be used interchangeably in this

3628
03:35:50,640 --> 03:35:52,640
context

3629
03:35:52,640 --> 03:35:54,640
but yeah we have an end dot sequential

3630
03:35:54,640 --> 03:35:56,640
and this asterisk is pretty much saying

3631
03:35:56,640 --> 03:35:58,640
we're going to repeat

3632
03:35:58,640 --> 03:36:00,640
this right here

3633
03:36:00,640 --> 03:36:02,640
for how many

3634
03:36:02,640 --> 03:36:04,640
end layer is and end layer is another hyperparameter

3635
03:36:04,640 --> 03:36:06,640
we're going to add

3636
03:36:06,640 --> 03:36:08,640
we go end underscore layer

3637
03:36:08,640 --> 03:36:10,640
we go equals four

3638
03:36:12,640 --> 03:36:14,640
so end underscore layer equals four

3639
03:36:14,640 --> 03:36:16,640
that means it's going to make four of these

3640
03:36:16,640 --> 03:36:18,640
I guess blocks

3641
03:36:18,640 --> 03:36:20,640
or layers sequentially

3642
03:36:20,640 --> 03:36:22,640
it's going to make four of them

3643
03:36:22,640 --> 03:36:24,640
and this little block thing

3644
03:36:24,640 --> 03:36:26,640
we're going to build on top of this in a second here

3645
03:36:26,640 --> 03:36:28,640
we're going to make an actual block

3646
03:36:28,640 --> 03:36:30,640
class and I'm going to explain what that does

3647
03:36:30,640 --> 03:36:32,640
but for now

3648
03:36:32,640 --> 03:36:34,640
this is going to be some temporary code

3649
03:36:34,640 --> 03:36:36,640
as long as you understand that this is what

3650
03:36:36,640 --> 03:36:38,640
this is how we create our four layers

3651
03:36:38,640 --> 03:36:40,640
our four decoder layers

3652
03:36:40,640 --> 03:36:42,640
that's all you need to know for now

3653
03:36:42,640 --> 03:36:44,640
I'm going to move more into this block later

3654
03:36:44,640 --> 03:36:46,640
as for this

3655
03:36:46,640 --> 03:36:48,640
self dot layer norm final

3656
03:36:48,640 --> 03:36:50,640
this is the final layer norm

3657
03:36:50,640 --> 03:36:52,640
all this is going to do

3658
03:36:52,640 --> 03:36:54,640
is we're just simply going to add this

3659
03:36:54,640 --> 03:36:56,640
to the end of our network here

3660
03:37:00,640 --> 03:37:02,640
just simply at the end here

3661
03:37:02,640 --> 03:37:04,640
and all this is going to do

3662
03:37:04,640 --> 03:37:06,640
is just going to help the model converge better

3663
03:37:06,640 --> 03:37:08,640
layer norms are super useful

3664
03:37:08,640 --> 03:37:10,640
and yeah

3665
03:37:10,640 --> 03:37:12,640
so you'll see more how that works

3666
03:37:12,640 --> 03:37:14,640
I'll actually remove it later on

3667
03:37:14,640 --> 03:37:16,640
and we'll actually

3668
03:37:16,640 --> 03:37:18,640
compare and see

3669
03:37:18,640 --> 03:37:20,640
how good it actually does

3670
03:37:20,640 --> 03:37:22,640
and you can totally go out of your way

3671
03:37:22,640 --> 03:37:24,640
to experiment

3672
03:37:24,640 --> 03:37:26,640
with different normalizations

3673
03:37:26,640 --> 03:37:28,640
and see how well the layer norm

3674
03:37:28,640 --> 03:37:30,640
helps the model perform

3675
03:37:30,640 --> 03:37:32,640
or how well the loss

3676
03:37:32,640 --> 03:37:34,640
sort of converges over time

3677
03:37:34,640 --> 03:37:36,640
when you put the layer norm in different places

3678
03:37:36,640 --> 03:37:38,640
so

3679
03:37:38,640 --> 03:37:40,640
let's go back here

3680
03:37:40,640 --> 03:37:42,640
and now we have this

3681
03:37:42,640 --> 03:37:44,640
end here

3682
03:37:44,640 --> 03:37:46,640
which is the language

3683
03:37:46,640 --> 03:37:48,640
I believe this is the language modeling

3684
03:37:48,640 --> 03:37:50,640
head or something

3685
03:37:50,640 --> 03:37:52,640
again this is what Andrey Karpathy used

3686
03:37:52,640 --> 03:37:54,640
I'm assuming that means language modeling head

3687
03:37:54,640 --> 03:37:56,640
but pretty much

3688
03:37:56,640 --> 03:37:58,640
all we're doing is we're just

3689
03:37:58,640 --> 03:38:00,640
projecting

3690
03:38:00,640 --> 03:38:02,640
we're doing this final

3691
03:38:02,640 --> 03:38:04,640
transformation here

3692
03:38:04,640 --> 03:38:06,640
this final little linear layer here

3693
03:38:06,640 --> 03:38:08,640
from all of these sequential

3694
03:38:08,640 --> 03:38:10,640
decoder outputs

3695
03:38:10,640 --> 03:38:12,640
and we're just going to transform that

3696
03:38:12,640 --> 03:38:14,640
to

3697
03:38:14,640 --> 03:38:16,640
something that the softmax can work with

3698
03:38:16,640 --> 03:38:18,640
so we have our layer norm afterwards

3699
03:38:18,640 --> 03:38:20,640
to sort of normalize help the model converge

3700
03:38:20,640 --> 03:38:22,640
after all these

3701
03:38:22,640 --> 03:38:24,640
after all this computation

3702
03:38:24,640 --> 03:38:26,640
we're going to feed that into a linear layer

3703
03:38:26,640 --> 03:38:28,640
to make it I guess

3704
03:38:28,640 --> 03:38:30,640
softmax

3705
03:38:30,640 --> 03:38:32,640
workable so the softmax can work with it

3706
03:38:32,640 --> 03:38:34,640
and

3707
03:38:34,640 --> 03:38:36,640
yeah so we're just

3708
03:38:36,640 --> 03:38:38,640
simply projecting it from

3709
03:38:38,640 --> 03:38:40,640
an embed which is the vector length that we get

3710
03:38:40,640 --> 03:38:42,640
from our decoder

3711
03:38:42,640 --> 03:38:44,640
and

3712
03:38:46,640 --> 03:38:48,640
and this vocab size

3713
03:38:48,640 --> 03:38:50,640
so the vocab size is going to

3714
03:38:50,640 --> 03:38:52,640
essentially give up a little

3715
03:38:52,640 --> 03:38:54,640
probability distribution on each token that we have

3716
03:38:54,640 --> 03:38:56,640
or the vocabulary

3717
03:38:56,640 --> 03:38:58,640
so anyways

3718
03:38:58,640 --> 03:39:00,640
I'm going to make this back to normal

3719
03:39:00,640 --> 03:39:02,640
here and we're going to just

3720
03:39:02,640 --> 03:39:04,640
apply this

3721
03:39:04,640 --> 03:39:06,640
to the forward pass

3722
03:39:06,640 --> 03:39:08,640
so a little thing I wanted to add on

3723
03:39:08,640 --> 03:39:10,640
to

3724
03:39:10,640 --> 03:39:12,640
this positional embedding

3725
03:39:12,640 --> 03:39:14,640
or rather just the idea of

3726
03:39:14,640 --> 03:39:16,640
embeddings versus

3727
03:39:16,640 --> 03:39:18,640
the fixed definite function

3728
03:39:18,640 --> 03:39:20,640
of the

3729
03:39:20,640 --> 03:39:22,640
sinusoidal functions

3730
03:39:22,640 --> 03:39:24,640
and the cosine functions that we used here

3731
03:39:24,640 --> 03:39:26,640
these are both actually

3732
03:39:26,640 --> 03:39:28,640
used in practice

3733
03:39:28,640 --> 03:39:30,640
the reason I said we're going to use embeddings

3734
03:39:30,640 --> 03:39:32,640
is because we just want it to be more oriented

3735
03:39:32,640 --> 03:39:34,640
around our data

3736
03:39:34,640 --> 03:39:36,640
however in practice

3737
03:39:36,640 --> 03:39:38,640
sinusoidal encodings are used

3738
03:39:38,640 --> 03:39:40,640
in base transformer models

3739
03:39:40,640 --> 03:39:42,640
whereas learned embeddings what we're using

3740
03:39:42,640 --> 03:39:44,640
are used in variants like

3741
03:39:44,640 --> 03:39:46,640
GBT and we are building a

3742
03:39:46,640 --> 03:39:48,640
GBT so we're probably

3743
03:39:48,640 --> 03:39:50,640
going to find out a performance from learning about embeddings

3744
03:39:50,640 --> 03:39:52,640
and this is just

3745
03:39:52,640 --> 03:39:54,640
summing up the experts do

3746
03:39:54,640 --> 03:39:56,640
it's a little practice that experts do

3747
03:39:56,640 --> 03:39:58,640
when they're building transformer models

3748
03:39:58,640 --> 03:40:00,640
versus variants like GBTs

3749
03:40:00,640 --> 03:40:02,640
so that's just a little background on

3750
03:40:02,640 --> 03:40:04,640
why we're using

3751
03:40:04,640 --> 03:40:06,640
learnable embeddings

3752
03:40:06,640 --> 03:40:08,640
so now let's continue

3753
03:40:08,640 --> 03:40:10,640
with the forward pass here

3754
03:40:10,640 --> 03:40:12,640
so I'm going to paste in some more code

3755
03:40:12,640 --> 03:40:14,640
and

3756
03:40:14,640 --> 03:40:16,640
let me just make sure this is

3757
03:40:16,640 --> 03:40:18,640
formatted properly cool

3758
03:40:20,640 --> 03:40:22,640
so we have this

3759
03:40:22,640 --> 03:40:24,640
token embedding which is our token embedding

3760
03:40:24,640 --> 03:40:26,640
table

3761
03:40:26,640 --> 03:40:28,640
we take an IDX

3762
03:40:28,640 --> 03:40:30,640
token embedding here

3763
03:40:30,640 --> 03:40:32,640
then what we do with this positional embedding table

3764
03:40:32,640 --> 03:40:34,640
so we have this torch.arrange

3765
03:40:34,640 --> 03:40:36,640
we make sure this is on the CUDA device

3766
03:40:36,640 --> 03:40:38,640
the GPU device

3767
03:40:38,640 --> 03:40:40,640
so it's in parallel

3768
03:40:40,640 --> 03:40:42,640
and all this is going to do

3769
03:40:42,640 --> 03:40:44,640
is it's going to look at how long is T

3770
03:40:44,640 --> 03:40:46,640
and

3771
03:40:46,640 --> 03:40:48,640
let's say T is our block size

3772
03:40:48,640 --> 03:40:50,640
so T is going to be 8

3773
03:40:50,640 --> 03:40:52,640
so all it's going to do is give us 8 indices

3774
03:40:52,640 --> 03:40:54,640
it's going to be like 0, 1, 2, 3,

3775
03:40:54,640 --> 03:40:56,640
4, 5, 6, 7

3776
03:40:56,640 --> 03:40:58,640
8 of those

3777
03:40:58,640 --> 03:41:00,640
and we're essentially just going to give each of those

3778
03:41:00,640 --> 03:41:02,640
each of those indices

3779
03:41:02,640 --> 03:41:04,640
a different

3780
03:41:06,640 --> 03:41:08,640
a different

3781
03:41:08,640 --> 03:41:10,640
end embedding vector

3782
03:41:10,640 --> 03:41:12,640
for each of those indices

3783
03:41:12,640 --> 03:41:14,640
just a little lookup table

3784
03:41:14,640 --> 03:41:16,640
and that's what that is

3785
03:41:16,640 --> 03:41:18,640
so all we do now

3786
03:41:18,640 --> 03:41:20,640
is it's actually quite simple

3787
03:41:20,640 --> 03:41:22,640
and this is a very efficient way to do it

3788
03:41:22,640 --> 03:41:24,640
is you just add these two together

3789
03:41:24,640 --> 03:41:26,640
broadcasting rules

3790
03:41:26,640 --> 03:41:28,640
which you might want to look into

3791
03:41:28,640 --> 03:41:30,640
I'll actually search that up right now

3792
03:41:30,640 --> 03:41:32,640
torch

3793
03:41:32,640 --> 03:41:34,640
broadcasting semantics

3794
03:41:36,640 --> 03:41:38,640
pie torch

3795
03:41:38,640 --> 03:41:40,640
broadcasting

3796
03:41:40,640 --> 03:41:42,640
I cannot spell

3797
03:41:42,640 --> 03:41:44,640
broadcasting semantics

3798
03:41:44,640 --> 03:41:46,640
so

3799
03:41:46,640 --> 03:41:48,640
these are a little bit funky

3800
03:41:48,640 --> 03:41:50,640
when you look at them the first time

3801
03:41:50,640 --> 03:41:52,640
but pretty much these are just rules

3802
03:41:52,640 --> 03:41:54,640
about how you can do

3803
03:41:54,640 --> 03:41:56,640
arithmetic operations

3804
03:41:56,640 --> 03:41:58,640
and just operations in general

3805
03:41:58,640 --> 03:42:00,640
to tensors

3806
03:42:00,640 --> 03:42:02,640
so tensors are like you think of matrices

3807
03:42:02,640 --> 03:42:04,640
where it's like a 2x2

3808
03:42:04,640 --> 03:42:06,640
tensors can be the same thing

3809
03:42:06,640 --> 03:42:08,640
but they could be like a 2x2x2

3810
03:42:08,640 --> 03:42:10,640
or a 2x2x2x2x2

3811
03:42:10,640 --> 03:42:12,640
whatever dimension you want to have

3812
03:42:12,640 --> 03:42:14,640
there

3813
03:42:14,640 --> 03:42:16,640
and pretty much it's just rules about how you can

3814
03:42:18,640 --> 03:42:20,640
have two of those

3815
03:42:20,640 --> 03:42:22,640
weirdly

3816
03:42:22,640 --> 03:42:24,640
shaped tensors and do things

3817
03:42:24,640 --> 03:42:26,640
to them

3818
03:42:26,640 --> 03:42:28,640
so just some rules here

3819
03:42:28,640 --> 03:42:30,640
I would advise you familiarize yourself with these

3820
03:42:30,640 --> 03:42:32,640
even play around with it if you want

3821
03:42:32,640 --> 03:42:34,640
just for a few minutes

3822
03:42:34,640 --> 03:42:36,640
and just get an idea for

3823
03:42:36,640 --> 03:42:38,640
which, like just try to multiply

3824
03:42:38,640 --> 03:42:40,640
tensors together

3825
03:42:40,640 --> 03:42:42,640
and see which ones throw errors and which ones don't

3826
03:42:42,640 --> 03:42:44,640
so it's a good idea to understand how broadcasting

3827
03:42:44,640 --> 03:42:46,640
rules work

3828
03:42:46,640 --> 03:42:48,640
obviously this term

3829
03:42:48,640 --> 03:42:50,640
is a little fancy and it's like

3830
03:42:50,640 --> 03:42:52,640
that's like a crazy advanced term

3831
03:42:52,640 --> 03:42:54,640
not really

3832
03:42:54,640 --> 03:42:56,640
it's pretty much just

3833
03:42:56,640 --> 03:42:58,640
some rules about how you're

3834
03:42:58,640 --> 03:43:00,640
multiplying these really weirdly shaped tensors

3835
03:43:00,640 --> 03:43:02,640
so yeah

3836
03:43:02,640 --> 03:43:04,640
anyways

3837
03:43:04,640 --> 03:43:06,640
if we go back to here

3838
03:43:08,640 --> 03:43:10,640
we are allowed to broadcast these

3839
03:43:10,640 --> 03:43:12,640
we're allowed to actually add them together

3840
03:43:12,640 --> 03:43:14,640
so the positional embedding and the token embedding

3841
03:43:14,640 --> 03:43:16,640
we get X from this

3842
03:43:16,640 --> 03:43:18,640
B by T by C shape

3843
03:43:18,640 --> 03:43:20,640
so now

3844
03:43:20,640 --> 03:43:22,640
what we can do

3845
03:43:22,640 --> 03:43:24,640
with these is we can actually feed it

3846
03:43:24,640 --> 03:43:26,640
into the

3847
03:43:26,640 --> 03:43:28,640
GPT or I guess

3848
03:43:28,640 --> 03:43:30,640
sort of a transformer network if you want to say that

3849
03:43:30,640 --> 03:43:32,640
so we have these embeddings

3850
03:43:32,640 --> 03:43:34,640
and positional encodings

3851
03:43:34,640 --> 03:43:36,640
we add these together and then we feed them

3852
03:43:36,640 --> 03:43:38,640
into our sequential network

3853
03:43:38,640 --> 03:43:40,640
so how are we doing this

3854
03:43:40,640 --> 03:43:42,640
well we go self dot blocks which is up here

3855
03:43:42,640 --> 03:43:44,640
and we essentially just feed

3856
03:43:44,640 --> 03:43:46,640
an X which is literally

3857
03:43:46,640 --> 03:43:48,640
exactly what happens here

3858
03:43:48,640 --> 03:43:50,640
we have our tokenized inputs

3859
03:43:50,640 --> 03:43:52,640
we got our embeddings and our positional encodings

3860
03:43:52,640 --> 03:43:54,640
through learnable embeddings we add them together

3861
03:43:54,640 --> 03:43:56,640
and then we feed them into the network directly

3862
03:43:56,640 --> 03:43:58,640
so

3863
03:43:58,640 --> 03:44:00,640
that's all that's happening here

3864
03:44:00,640 --> 03:44:02,640
and that's how we're feeding an X

3865
03:44:02,640 --> 03:44:04,640
which is the output of these

3866
03:44:04,640 --> 03:44:06,640
then after

3867
03:44:06,640 --> 03:44:08,640
this is like way after

3868
03:44:08,640 --> 03:44:10,640
we've gotten through all of these

3869
03:44:10,640 --> 03:44:12,640
GPT layers or blocks

3870
03:44:12,640 --> 03:44:14,640
we do this final layer norm

3871
03:44:14,640 --> 03:44:16,640
and then this linear transformation

3872
03:44:16,640 --> 03:44:18,640
to get it to a

3873
03:44:18,640 --> 03:44:20,640
softmax

3874
03:44:20,640 --> 03:44:22,640
to get it to essentially probabilities

3875
03:44:22,640 --> 03:44:24,640
that we can feed into our softmax function

3876
03:44:24,640 --> 03:44:26,640
and then other than that

3877
03:44:26,640 --> 03:44:28,640
this forward pass is exactly the same

3878
03:44:28,640 --> 03:44:30,640
other than this little block of code here

3879
03:44:30,640 --> 03:44:32,640
so if this makes sense so far

3880
03:44:32,640 --> 03:44:34,640
that is absolutely amazing

3881
03:44:34,640 --> 03:44:36,640
let's continue I'm actually going to add

3882
03:44:36,640 --> 03:44:38,640
a little bit of

3883
03:44:38,640 --> 03:44:40,640
in practice

3884
03:44:40,640 --> 03:44:42,640
some little

3885
03:44:42,640 --> 03:44:44,640
weight initializations

3886
03:44:44,640 --> 03:44:46,640
that we should be using

3887
03:44:46,640 --> 03:44:48,640
in our language model

3888
03:44:48,640 --> 03:44:50,640
and in module subclass

3889
03:44:50,640 --> 03:44:52,640
so

3890
03:44:52,640 --> 03:44:54,640
I'm going to go over a little bit of math here

3891
03:44:54,640 --> 03:44:56,640
but this is just really important for practice

3892
03:44:56,640 --> 03:44:58,640
and to make sure that your model

3893
03:44:58,640 --> 03:45:00,640
does not fail in the training process

3894
03:45:00,640 --> 03:45:02,640
this is very important

3895
03:45:02,640 --> 03:45:04,640
it's going to be a little funky

3896
03:45:04,640 --> 03:45:06,640
on the conceptualizing

3897
03:45:06,640 --> 03:45:08,640
but bring out some pen and paper

3898
03:45:08,640 --> 03:45:10,640
and do some math with me

3899
03:45:10,640 --> 03:45:12,640
we've built up some of these

3900
03:45:12,640 --> 03:45:14,640
initial GPT language model architecture

3901
03:45:14,640 --> 03:45:16,640
and before we continue building

3902
03:45:16,640 --> 03:45:18,640
more of it and the other functions

3903
03:45:18,640 --> 03:45:20,640
some of the math stuff that's going on

3904
03:45:20,640 --> 03:45:22,640
the parallelization that's going on in the script

3905
03:45:22,640 --> 03:45:24,640
I want to show you some of the math

3906
03:45:24,640 --> 03:45:26,640
that we're going to use to initialize the weights

3907
03:45:26,640 --> 03:45:28,640
of the model to help it train

3908
03:45:28,640 --> 03:45:30,640
and converge better

3909
03:45:30,640 --> 03:45:32,640
so there's this new thing

3910
03:45:32,640 --> 03:45:34,640
that I want to introduce called standard deviation

3911
03:45:34,640 --> 03:45:36,640
and this is used in intermediate level mathematics

3912
03:45:36,640 --> 03:45:38,640
the symbol essentially looks like this

3913
03:45:38,640 --> 03:45:40,640
population standard deviation

3914
03:45:40,640 --> 03:45:42,640
so

3915
03:45:42,640 --> 03:45:44,640
n

3916
03:45:44,640 --> 03:45:46,640
the size

3917
03:45:46,640 --> 03:45:48,640
so it's just going to be an array

3918
03:45:48,640 --> 03:45:50,640
the length of the array

3919
03:45:50,640 --> 03:45:52,640
and then xi

3920
03:45:52,640 --> 03:45:54,640
we iterate over each value

3921
03:45:54,640 --> 03:45:56,640
so xf position 0

3922
03:45:56,640 --> 03:45:58,640
xf position 1

3923
03:45:58,640 --> 03:46:00,640
xf position 2

3924
03:46:00,640 --> 03:46:02,640
and then this u here is the mean

3925
03:46:02,640 --> 03:46:04,640
so

3926
03:46:04,640 --> 03:46:06,640
we iterate over each element

3927
03:46:06,640 --> 03:46:08,640
we're going to

3928
03:46:08,640 --> 03:46:10,640
subtract it by the mean

3929
03:46:10,640 --> 03:46:12,640
we're going to square that and then keep adding

3930
03:46:12,640 --> 03:46:14,640
all these squared results together

3931
03:46:14,640 --> 03:46:16,640
and then once we get the sum of that

3932
03:46:16,640 --> 03:46:18,640
we're going to

3933
03:46:18,640 --> 03:46:20,640
subtract or we're going to divide

3934
03:46:20,640 --> 03:46:22,640
this by the number of elements there are

3935
03:46:22,640 --> 03:46:24,640
and then once we get this result

3936
03:46:24,640 --> 03:46:26,640
we're going to square root that

3937
03:46:26,640 --> 03:46:28,640
so this symbol here

3938
03:46:28,640 --> 03:46:30,640
might also look a little bit unfamiliar

3939
03:46:30,640 --> 03:46:32,640
and

3940
03:46:32,640 --> 03:46:34,640
I'll illustrate this out for you

3941
03:46:34,640 --> 03:46:36,640
so we go to our whiteboard

3942
03:46:36,640 --> 03:46:38,640
and this e

3943
03:46:38,640 --> 03:46:40,640
looks like

3944
03:46:40,640 --> 03:46:42,640
looks like that

3945
03:46:42,640 --> 03:46:44,640
let's just say we were to put in

3946
03:46:44,640 --> 03:46:46,640
x

3947
03:46:46,640 --> 03:46:48,640
i like that

3948
03:46:48,640 --> 03:46:50,640
and our array

3949
03:46:50,640 --> 03:46:52,640
let's just say for instance

3950
03:46:52,640 --> 03:46:54,640
our array

3951
03:46:54,640 --> 03:46:56,640
is 0.1

3952
03:46:56,640 --> 03:46:58,640
0.2, 0.3

3953
03:46:58,640 --> 03:47:00,640
so what would the result of this be

3954
03:47:00,640 --> 03:47:02,640
well if we look at each element

3955
03:47:02,640 --> 03:47:04,640
iteratively add them together

3956
03:47:04,640 --> 03:47:06,640
so 0.1

3957
03:47:06,640 --> 03:47:08,640
plus 0.2 plus 0.3

3958
03:47:08,640 --> 03:47:10,640
well we get 0.6 from that

3959
03:47:10,640 --> 03:47:12,640
so this would essentially

3960
03:47:12,640 --> 03:47:14,640
be equal to

3961
03:47:14,640 --> 03:47:16,640
0.6

3962
03:47:16,640 --> 03:47:18,640
that's what that equals

3963
03:47:18,640 --> 03:47:20,640
we just add each of these up together

3964
03:47:20,640 --> 03:47:22,640
or we do whatever this is iteratively

3965
03:47:22,640 --> 03:47:24,640
whatever this element is

3966
03:47:24,640 --> 03:47:26,640
we iterate over

3967
03:47:26,640 --> 03:47:28,640
the number of elements we have in

3968
03:47:28,640 --> 03:47:30,640
the arbitrary array

3969
03:47:30,640 --> 03:47:32,640
or

3970
03:47:32,640 --> 03:47:34,640
vector or list or whatever you want to call it

3971
03:47:34,640 --> 03:47:36,640
and then we just

3972
03:47:36,640 --> 03:47:38,640
sort of look at what's going on here

3973
03:47:38,640 --> 03:47:40,640
and we can do some basic arithmetic stuff

3974
03:47:40,640 --> 03:47:42,640
so

3975
03:47:42,640 --> 03:47:44,640
let's walk through a few examples

3976
03:47:44,640 --> 03:47:46,640
just to illustrate to you

3977
03:47:46,640 --> 03:47:48,640
what the results look like

3978
03:47:48,640 --> 03:47:50,640
based on the inputs here

3979
03:47:50,640 --> 03:47:52,640
so I'm going to go back to my whiteboard

3980
03:47:52,640 --> 03:47:54,640
we're going to draw a little line here

3981
03:47:54,640 --> 03:47:56,640
just to separate this

3982
03:47:56,640 --> 03:47:58,640
so

3983
03:47:58,640 --> 03:48:00,640
I want to calculate the standard deviation

3984
03:48:00,640 --> 03:48:02,640
do standard deviation

3985
03:48:02,640 --> 03:48:04,640
of

3986
03:48:06,640 --> 03:48:08,640
and then we'll just make some random array

3987
03:48:08,640 --> 03:48:10,640
negative

3988
03:48:10,640 --> 03:48:12,640
0.38

3989
03:48:12,640 --> 03:48:14,640
negative 0.38

3990
03:48:14,640 --> 03:48:16,640
0.52

3991
03:48:18,640 --> 03:48:20,640
and then 2.48

3992
03:48:20,640 --> 03:48:22,640
cool

3993
03:48:22,640 --> 03:48:24,640
so we have this array this is three elements

3994
03:48:24,640 --> 03:48:26,640
so that means n

3995
03:48:26,640 --> 03:48:28,640
is going to be equal to three

3996
03:48:28,640 --> 03:48:30,640
let me drag this over here

3997
03:48:30,640 --> 03:48:32,640
so n is the number of elements

3998
03:48:32,640 --> 03:48:34,640
so n is going to be equal to three

3999
03:48:34,640 --> 03:48:36,640
our mean

4000
03:48:36,640 --> 03:48:38,640
well

4001
03:48:38,640 --> 03:48:40,640
our mean is just

4002
03:48:40,640 --> 03:48:42,640
we add all these up together and then we average them

4003
03:48:42,640 --> 03:48:44,640
so our mean

4004
03:48:44,640 --> 03:48:46,640
is going to be equal to

4005
03:48:46,640 --> 03:48:48,640
let's just say

4006
03:48:48,640 --> 03:48:50,640
0.38

4007
03:48:50,640 --> 03:48:52,640
plus 0.52

4008
03:48:52,640 --> 03:48:54,640
plus

4009
03:48:54,640 --> 03:48:56,640
2.48

4010
03:48:56,640 --> 03:48:58,640
and then divided by three

4011
03:48:58,640 --> 03:49:00,640
and the answer to this

4012
03:49:00,640 --> 03:49:02,640
I did the math ahead of time

4013
03:49:02,640 --> 03:49:04,640
is literally 0.873

4014
03:49:04,640 --> 03:49:06,640
repeated but we're just going to put 0.87

4015
03:49:06,640 --> 03:49:08,640
for simplicity's sake

4016
03:49:08,640 --> 03:49:10,640
cool so the mean of this

4017
03:49:10,640 --> 03:49:12,640
is 0.87 and n is equal to three

4018
03:49:12,640 --> 03:49:14,640
now we can start doing

4019
03:49:14,640 --> 03:49:16,640
some of the other math

4020
03:49:16,640 --> 03:49:18,640
so

4021
03:49:18,640 --> 03:49:20,640
we have this

4022
03:49:20,640 --> 03:49:22,640
O has a cool line

4023
03:49:24,640 --> 03:49:26,640
and we do

4024
03:49:26,640 --> 03:49:28,640
square root

4025
03:49:28,640 --> 03:49:30,640
one over

4026
03:49:30,640 --> 03:49:32,640
n which is equal to three

4027
03:49:32,640 --> 03:49:34,640
and then we

4028
03:49:34,640 --> 03:49:36,640
multiply this

4029
03:49:36,640 --> 03:49:38,640
by sigma

4030
03:49:38,640 --> 03:49:40,640
that's what this symbol is

4031
03:49:40,640 --> 03:49:42,640
that's sigma that's the name for it

4032
03:49:42,640 --> 03:49:44,640
and then we go

4033
03:49:44,640 --> 03:49:46,640
X

4034
03:49:46,640 --> 03:49:48,640
I

4035
03:49:48,640 --> 03:49:50,640
minus

4036
03:49:50,640 --> 03:49:52,640
and then our mean of

4037
03:49:52,640 --> 03:49:54,640
0.87

4038
03:49:58,640 --> 03:50:00,640
apologies for the sloppy writing

4039
03:50:02,640 --> 03:50:04,640
and then we square that

4040
03:50:04,640 --> 03:50:06,640
so let me drag this out

4041
03:50:06,640 --> 03:50:08,640
awesome

4042
03:50:08,640 --> 03:50:10,640
so let's just do this

4043
03:50:10,640 --> 03:50:12,640
step by step here

4044
03:50:12,640 --> 03:50:14,640
so the first one is going to be

4045
03:50:14,640 --> 03:50:16,640
0.38

4046
03:50:16,640 --> 03:50:18,640
0.

4047
03:50:18,640 --> 03:50:20,640
negative

4048
03:50:20,640 --> 03:50:22,640
0.38

4049
03:50:22,640 --> 03:50:24,640
and we're going to do minus the mean here

4050
03:50:24,640 --> 03:50:26,640
so minus 0.87

4051
03:50:26,640 --> 03:50:28,640
and I'm just going to wrap all this

4052
03:50:28,640 --> 03:50:30,640
in brackets so that we don't miss anything

4053
03:50:30,640 --> 03:50:32,640
wrap it in brackets

4054
03:50:32,640 --> 03:50:34,640
and then just square it and see what we get after

4055
03:50:34,640 --> 03:50:36,640
so I'm just going to write all these out

4056
03:50:36,640 --> 03:50:38,640
then we can do the calculations

4057
03:50:38,640 --> 03:50:40,640
so next up we have 0.52

4058
03:50:40,640 --> 03:50:42,640
minus 0.87

4059
03:50:42,640 --> 03:50:44,640
we'll square that

4060
03:50:46,640 --> 03:50:48,640
and then next up we have

4061
03:50:48,640 --> 03:50:50,640
2.48

4062
03:50:50,640 --> 03:50:52,640
minus 0.87

4063
03:50:52,640 --> 03:50:54,640
and then we square that as well

4064
03:50:54,640 --> 03:50:56,640
so awesome

4065
03:50:56,640 --> 03:50:58,640
what is the result of this

4066
03:50:58,640 --> 03:51:00,640
the result of

4067
03:51:00,640 --> 03:51:02,640
negative 0.38 minus

4068
03:51:02,640 --> 03:51:04,640
0.87

4069
03:51:04,640 --> 03:51:06,640
squared is

4070
03:51:06,640 --> 03:51:08,640
1.57

4071
03:51:08,640 --> 03:51:10,640
the result of

4072
03:51:10,640 --> 03:51:12,640
this line

4073
03:51:12,640 --> 03:51:14,640
is 0.12

4074
03:51:14,640 --> 03:51:16,640
again these are all approximations

4075
03:51:16,640 --> 03:51:18,640
they're not super spot on

4076
03:51:18,640 --> 03:51:20,640
we're just doing this to understand

4077
03:51:20,640 --> 03:51:22,640
what's going on here

4078
03:51:22,640 --> 03:51:24,640
just to overview the function not for precision

4079
03:51:24,640 --> 03:51:26,640
then the next one is going to be

4080
03:51:26,640 --> 03:51:28,640
2.59

4081
03:51:28,640 --> 03:51:30,640
and you can double check all these

4082
03:51:30,640 --> 03:51:32,640
calculations if you'd like

4083
03:51:32,640 --> 03:51:34,640
I have done these preemptively so

4084
03:51:34,640 --> 03:51:36,640
that is that

4085
03:51:36,640 --> 03:51:38,640
and now from here

4086
03:51:38,640 --> 03:51:40,640
what we have to do is add each of these together

4087
03:51:40,640 --> 03:51:42,640
so

4088
03:51:42,640 --> 03:51:44,640
1.57

4089
03:51:44,640 --> 03:51:46,640
plus 0.12

4090
03:51:46,640 --> 03:51:48,640
plus 2.59

4091
03:51:48,640 --> 03:51:50,640
divided by 3

4092
03:51:50,640 --> 03:51:52,640
is

4093
03:51:56,640 --> 03:51:58,640
1.57

4094
03:51:58,640 --> 03:52:00,640
plus 0.12

4095
03:52:00,640 --> 03:52:02,640
plus 2.59

4096
03:52:02,640 --> 03:52:04,640
all that divided by 3

4097
03:52:04,640 --> 03:52:06,640
is going to be equal to 1.42

4098
03:52:06,640 --> 03:52:08,640
keep in mind we also have to

4099
03:52:08,640 --> 03:52:10,640
square root this

4100
03:52:10,640 --> 03:52:12,640
so the square root of that

4101
03:52:12,640 --> 03:52:14,640
is going to be

4102
03:52:14,640 --> 03:52:16,640
1.19

4103
03:52:16,640 --> 03:52:18,640
approximately

4104
03:52:18,640 --> 03:52:20,640
we'll just add

4105
03:52:20,640 --> 03:52:22,640
this guy ahead of it

4106
03:52:22,640 --> 03:52:24,640
so that's what the

4107
03:52:24,640 --> 03:52:26,640
standard deviation of

4108
03:52:26,640 --> 03:52:28,640
this array is

4109
03:52:28,640 --> 03:52:30,640
negative 0.38

4110
03:52:30,640 --> 03:52:32,640
0.52, 2.48

4111
03:52:32,640 --> 03:52:34,640
standard deviation is

4112
03:52:34,640 --> 03:52:36,640
1.19

4113
03:52:36,640 --> 03:52:38,640
let's do another example

4114
03:52:42,640 --> 03:52:44,640
so let's say

4115
03:52:44,640 --> 03:52:46,640
we want to do the standard deviation

4116
03:52:46,640 --> 03:52:48,640
of

4117
03:52:48,640 --> 03:52:50,640
0.48

4118
03:52:52,640 --> 03:52:54,640
0.5

4119
03:52:56,640 --> 03:52:58,640
0.50

4120
03:52:58,640 --> 03:53:00,640
I guess 0.52

4121
03:53:04,640 --> 03:53:06,640
so there's a little pattern here

4122
03:53:06,640 --> 03:53:08,640
just goes up by 0.02 each time

4123
03:53:08,640 --> 03:53:10,640
and

4124
03:53:10,640 --> 03:53:12,640
you're going to see why this is

4125
03:53:12,640 --> 03:53:14,640
vastly different than the other example

4126
03:53:14,640 --> 03:53:16,640
so let's walk through this

4127
03:53:16,640 --> 03:53:18,640
so first of all we have N

4128
03:53:20,640 --> 03:53:22,640
N is equal to 3

4129
03:53:22,640 --> 03:53:24,640
cool

4130
03:53:24,640 --> 03:53:26,640
what does our mean

4131
03:53:26,640 --> 03:53:28,640
our mean

4132
03:53:28,640 --> 03:53:30,640
well if you do our mean our mean is 0.5

4133
03:53:30,640 --> 03:53:32,640
0.48 plus this

4134
03:53:32,640 --> 03:53:34,640
plus that

4135
03:53:34,640 --> 03:53:36,640
that's going to be 0.5

4136
03:53:36,640 --> 03:53:38,640
and

4137
03:53:38,640 --> 03:53:40,640
if you're good with numbers

4138
03:53:40,640 --> 03:53:42,640
you'll probably already be able to do this in your head

4139
03:53:42,640 --> 03:53:44,640
but that's okay if not

4140
03:53:44,640 --> 03:53:46,640
next up

4141
03:53:46,640 --> 03:53:48,640
we're going to do this in the formula

4142
03:53:48,640 --> 03:53:50,640
so

4143
03:53:50,640 --> 03:53:52,640
what do these iterations look like

4144
03:53:52,640 --> 03:53:54,640
so

4145
03:53:54,640 --> 03:53:56,640
0.

4146
03:53:56,640 --> 03:53:58,640
let's just do these in brackets

4147
03:53:58,640 --> 03:54:00,640
the old way

4148
03:54:00,640 --> 03:54:02,640
0.5

4149
03:54:02,640 --> 03:54:04,640
squared

4150
03:54:04,640 --> 03:54:06,640
the next one is

4151
03:54:06,640 --> 03:54:08,640
0.5

4152
03:54:08,640 --> 03:54:10,640
minus 0.5

4153
03:54:10,640 --> 03:54:12,640
squared which we already know is 0

4154
03:54:14,640 --> 03:54:16,640
and this one is 0.52

4155
03:54:16,640 --> 03:54:18,640
minus

4156
03:54:18,640 --> 03:54:20,640
0.5

4157
03:54:20,640 --> 03:54:22,640
squared so the result of 0.48

4158
03:54:22,640 --> 03:54:24,640
minus 0.5 squared

4159
03:54:24,640 --> 03:54:26,640
and what's right equals here

4160
03:54:26,640 --> 03:54:28,640
is going to be

4161
03:54:28,640 --> 03:54:30,640
approximately 0.02

4162
03:54:30,640 --> 03:54:32,640
squared

4163
03:54:32,640 --> 03:54:34,640
so that would be 0.004

4164
03:54:34,640 --> 03:54:36,640
like that

4165
03:54:36,640 --> 03:54:38,640
so I'll make this not actually overlap

4166
03:54:38,640 --> 03:54:40,640
0.004

4167
03:54:40,640 --> 03:54:42,640
and then this one

4168
03:54:42,640 --> 03:54:44,640
we obviously know would be 0

4169
03:54:44,640 --> 03:54:46,640
because 0.5 minus 0.5

4170
03:54:46,640 --> 03:54:48,640
that's 0 then you square 0

4171
03:54:48,640 --> 03:54:50,640
still the same thing

4172
03:54:50,640 --> 03:54:52,640
and then this one is

4173
03:54:52,640 --> 03:54:54,640
0.0004 as well

4174
03:54:54,640 --> 03:54:56,640
so

4175
03:54:58,640 --> 03:55:00,640
when we add these two together

4176
03:55:00,640 --> 03:55:02,640
we're going to get

4177
03:55:02,640 --> 03:55:04,640
0.0008

4178
03:55:04,640 --> 03:55:06,640
just like that

4179
03:55:06,640 --> 03:55:08,640
and then if we divide them by 3 or whatever

4180
03:55:08,640 --> 03:55:10,640
n is

4181
03:55:10,640 --> 03:55:12,640
then we end up getting

4182
03:55:12,640 --> 03:55:14,640
0.00026

4183
03:55:14,640 --> 03:55:16,640
repeating so I'll just write

4184
03:55:16,640 --> 03:55:18,640
266 like that

4185
03:55:18,640 --> 03:55:20,640
and so

4186
03:55:20,640 --> 03:55:22,640
all we have to do at this point

4187
03:55:22,640 --> 03:55:24,640
is do the

4188
03:55:24,640 --> 03:55:26,640
square root of this

4189
03:55:26,640 --> 03:55:28,640
and

4190
03:55:28,640 --> 03:55:30,640
we'll do

4191
03:55:30,640 --> 03:55:32,640
square root of 0.00026

4192
03:55:32,640 --> 03:55:34,640
approximately

4193
03:55:34,640 --> 03:55:36,640
and

4194
03:55:36,640 --> 03:55:38,640
that's going to be equal to about

4195
03:55:38,640 --> 03:55:40,640
0.0163

4196
03:55:40,640 --> 03:55:42,640
so

4197
03:55:42,640 --> 03:55:44,640
that is our

4198
03:55:44,640 --> 03:55:46,640
standard deviation

4199
03:55:46,640 --> 03:55:48,640
of both of these arrays here

4200
03:55:48,640 --> 03:55:50,640
so 0.048

4201
03:55:50,640 --> 03:55:52,640
and then 0.52

4202
03:55:52,640 --> 03:55:54,640
our standard deviation is

4203
03:55:54,640 --> 03:55:56,640
0.0163

4204
03:55:56,640 --> 03:55:58,640
so very small

4205
03:55:58,640 --> 03:56:00,640
and then we have

4206
03:56:00,640 --> 03:56:02,640
negative 0.38, 0.52

4207
03:56:02,640 --> 03:56:04,640
and 2.48

4208
03:56:04,640 --> 03:56:06,640
we get a standard deviation of 1.19

4209
03:56:06,640 --> 03:56:08,640
so you can see that these numbers are vastly different

4210
03:56:08,640 --> 03:56:10,640
one is like

4211
03:56:10,640 --> 03:56:12,640
one is literally

4212
03:56:12,640 --> 03:56:14,640
100 times greater than the other

4213
03:56:14,640 --> 03:56:16,640
so

4214
03:56:16,640 --> 03:56:18,640
the reason for this is because these

4215
03:56:18,640 --> 03:56:20,640
numbers are super

4216
03:56:20,640 --> 03:56:22,640
diverse

4217
03:56:22,640 --> 03:56:24,640
I guess another way

4218
03:56:24,640 --> 03:56:26,640
you could think of them is that

4219
03:56:26,640 --> 03:56:28,640
they stretch out very far from the

4220
03:56:28,640 --> 03:56:30,640
mean

4221
03:56:30,640 --> 03:56:32,640
this essentially means when you're initializing

4222
03:56:32,640 --> 03:56:34,640
your parameters

4223
03:56:34,640 --> 03:56:36,640
that if you have some outliers

4224
03:56:36,640 --> 03:56:38,640
then your network

4225
03:56:38,640 --> 03:56:40,640
is going to be funky

4226
03:56:40,640 --> 03:56:42,640
because it's

4227
03:56:42,640 --> 03:56:44,640
the learning process just messed up because you have outliers

4228
03:56:44,640 --> 03:56:46,640
and it's not just learning the right way

4229
03:56:46,640 --> 03:56:48,640
it's supposed to

4230
03:56:48,640 --> 03:56:50,640
whereas if you had

4231
03:56:50,640 --> 03:56:52,640
way too small of a standard deviation

4232
03:56:52,640 --> 03:56:54,640
from your initial parameters

4233
03:56:54,640 --> 03:56:56,640
like in here but maybe even smaller

4234
03:56:56,640 --> 03:56:58,640
so let's say they were all

4235
03:56:58,640 --> 03:57:00,640
0.5

4236
03:57:00,640 --> 03:57:02,640
then all of your neurons

4237
03:57:02,640 --> 03:57:04,640
would effectively be the same

4238
03:57:04,640 --> 03:57:06,640
and they would all learn the same pattern

4239
03:57:06,640 --> 03:57:08,640
so then you would have no learning done

4240
03:57:08,640 --> 03:57:10,640
so one would either be

4241
03:57:10,640 --> 03:57:12,640
you're learning a super super unstable

4242
03:57:12,640 --> 03:57:14,640
and you have outliers that are

4243
03:57:14,640 --> 03:57:16,640
just learning

4244
03:57:16,640 --> 03:57:18,640
very distinct things and not really

4245
03:57:18,640 --> 03:57:20,640
not really

4246
03:57:20,640 --> 03:57:22,640
not really letting other neurons

4247
03:57:22,640 --> 03:57:24,640
get opportunities to learn

4248
03:57:24,640 --> 03:57:26,640
or rather other parameters to learn

4249
03:57:28,640 --> 03:57:30,640
if you have a lot of diversity

4250
03:57:30,640 --> 03:57:32,640
you just have outliers and then if you have

4251
03:57:32,640 --> 03:57:34,640
no

4252
03:57:34,640 --> 03:57:36,640
diversity at all then

4253
03:57:36,640 --> 03:57:38,640
essentially nothing is learned and your network

4254
03:57:38,640 --> 03:57:40,640
is useless so all we want to do

4255
03:57:40,640 --> 03:57:42,640
is make sure that our standard deviation

4256
03:57:42,640 --> 03:57:44,640
is balanced and stable

4257
03:57:44,640 --> 03:57:46,640
so that the training process

4258
03:57:46,640 --> 03:57:48,640
can learn effective things

4259
03:57:48,640 --> 03:57:50,640
so each neuron can learn a little bit

4260
03:57:50,640 --> 03:57:52,640
so you can see here

4261
03:57:52,640 --> 03:57:54,640
this would probably be an okay standard deviation

4262
03:57:54,640 --> 03:57:56,640
if these were some parameters because

4263
03:57:56,640 --> 03:57:58,640
they're a little bit different than each other

4264
03:57:58,640 --> 03:58:00,640
they're not all like super super

4265
03:58:00,640 --> 03:58:02,640
close to the same

4266
03:58:02,640 --> 03:58:04,640
and yeah

4267
03:58:04,640 --> 03:58:06,640
so essentially what

4268
03:58:06,640 --> 03:58:08,640
this looks like in

4269
03:58:08,640 --> 03:58:10,640
code here is the following

4270
03:58:10,640 --> 03:58:12,640
so you don't actually need to

4271
03:58:12,640 --> 03:58:14,640
memorize what this does as it's

4272
03:58:14,640 --> 03:58:16,640
just used in practice

4273
03:58:16,640 --> 03:58:18,640
by professionals

4274
03:58:18,640 --> 03:58:20,640
but essentially what this does

4275
03:58:20,640 --> 03:58:22,640
is it initializes our weights

4276
03:58:22,640 --> 03:58:24,640
around certain standard deviations

4277
03:58:24,640 --> 03:58:26,640
so here we set it to 0.02

4278
03:58:26,640 --> 03:58:28,640
which is pretty much the same

4279
03:58:28,640 --> 03:58:30,640
as what we had in here

4280
03:58:30,640 --> 03:58:32,640
so

4281
03:58:32,640 --> 03:58:34,640
point

4282
03:58:34,640 --> 03:58:36,640
point

4283
03:58:36,640 --> 03:58:38,640
this one's a little bit off in the standard deviation

4284
03:58:38,640 --> 03:58:40,640
set here

4285
03:58:40,640 --> 03:58:42,640
but essentially

4286
03:58:42,640 --> 03:58:44,640
we're just making sure that our weights

4287
03:58:44,640 --> 03:58:46,640
are initialized properly

4288
03:58:46,640 --> 03:58:48,640
and you don't have to memorize this at all

4289
03:58:48,640 --> 03:58:50,640
it's just used in practice and it's going to help our training

4290
03:58:50,640 --> 03:58:52,640
converge better

4291
03:58:52,640 --> 03:58:54,640
so as long as you understand

4292
03:58:54,640 --> 03:58:56,640
that we can apply some initializations

4293
03:58:56,640 --> 03:58:58,640
on our weights

4294
03:58:58,640 --> 03:59:00,640
that's all that really matters, so cool

4295
03:59:00,640 --> 03:59:02,640
let's move on to the next part

4296
03:59:02,640 --> 03:59:04,640
of our GBT architecture

4297
03:59:04,640 --> 03:59:06,640
so awesome, we finished this GBT language

4298
03:59:06,640 --> 03:59:08,640
class, everything's pretty much done here

4299
03:59:08,640 --> 03:59:10,640
we did our knit

4300
03:59:10,640 --> 03:59:12,640
we did some weight initializations

4301
03:59:12,640 --> 03:59:14,640
and we did our forward pass, so awesome

4302
03:59:14,640 --> 03:59:16,640
that's all done, now let's move on to the next

4303
03:59:16,640 --> 03:59:18,640
which is the

4304
03:59:18,640 --> 03:59:20,640
block class

4305
03:59:20,640 --> 03:59:22,640
so what is block?

4306
03:59:22,640 --> 03:59:24,640
well, if we go back to this diagram

4307
03:59:24,640 --> 03:59:26,640
each of these decoder blocks is a block

4308
03:59:26,640 --> 03:59:28,640
so

4309
03:59:28,640 --> 03:59:30,640
we're pretty much just going to fill in this gap here

4310
03:59:30,640 --> 03:59:32,640
our GBT language model has these two

4311
03:59:32,640 --> 03:59:34,640
where we get our tokenized inputs

4312
03:59:34,640 --> 03:59:36,640
and then we do some transformations

4313
03:59:36,640 --> 03:59:38,640
and the softmax after

4314
03:59:38,640 --> 03:59:40,640
and essentially we're just filling

4315
03:59:40,640 --> 03:59:42,640
in this gap here and then we're going to build out

4316
03:59:42,640 --> 03:59:44,640
and just sort of branch out until it's

4317
03:59:44,640 --> 03:59:46,640
completely built

4318
03:59:46,640 --> 03:59:48,640
so let's go ahead and build these blocks here

4319
03:59:48,640 --> 03:59:50,640
what does this look like?

4320
03:59:50,640 --> 03:59:52,640
that's what this does

4321
03:59:52,640 --> 03:59:54,640
so we have our knit, we have a forward pass

4322
03:59:54,640 --> 03:59:56,640
as per usual

4323
03:59:56,640 --> 03:59:58,640
and knit

4324
03:59:58,640 --> 04:00:00,640
and a forward pass as seen

4325
04:00:00,640 --> 04:00:02,640
in the GBT language model class

4326
04:00:02,640 --> 04:00:04,640
which is going to look like this

4327
04:00:04,640 --> 04:00:06,640
forward and an init

4328
04:00:06,640 --> 04:00:08,640
so the init

4329
04:00:08,640 --> 04:00:10,640
is going to just initialize some things

4330
04:00:10,640 --> 04:00:12,640
it's going to initialize some transformations

4331
04:00:12,640 --> 04:00:14,640
and some things that we're going to do in the forward pass

4332
04:00:14,640 --> 04:00:16,640
that's all it's doing

4333
04:00:16,640 --> 04:00:18,640
so what do we do first?

4334
04:00:18,640 --> 04:00:20,640
well we have this new head size

4335
04:00:20,640 --> 04:00:22,640
parameter introduced

4336
04:00:22,640 --> 04:00:24,640
so head size is the number of features

4337
04:00:24,640 --> 04:00:26,640
that each head will be capturing

4338
04:00:26,640 --> 04:00:28,640
in our multi-head attention

4339
04:00:28,640 --> 04:00:30,640
so all the heads in parallel

4340
04:00:30,640 --> 04:00:32,640
features are each of them capturing

4341
04:00:32,640 --> 04:00:34,640
so we do that by dividing

4342
04:00:34,640 --> 04:00:36,640
n embed by n head

4343
04:00:36,640 --> 04:00:38,640
so n head is the number

4344
04:00:38,640 --> 04:00:40,640
of heads we have

4345
04:00:40,640 --> 04:00:42,640
and n embed is the number of features we have

4346
04:00:42,640 --> 04:00:44,640
where we're capturing

4347
04:00:44,640 --> 04:00:46,640
so 384 features divided by 4 heads

4348
04:00:46,640 --> 04:00:48,640
so each head is going to be capturing

4349
04:00:48,640 --> 04:00:50,640
96 features

4350
04:00:50,640 --> 04:00:52,640
hence head size

4351
04:00:52,640 --> 04:00:54,640
so

4352
04:00:54,640 --> 04:00:56,640
next up we have self.sa

4353
04:00:56,640 --> 04:00:58,640
which is just short for self-attention

4354
04:00:58,640 --> 04:01:00,640
we do a multi-head attention

4355
04:01:00,640 --> 04:01:02,640
we pass in our n head

4356
04:01:02,640 --> 04:01:04,640
and our head size and you'll see how these

4357
04:01:04,640 --> 04:01:06,640
parameters fit in later

4358
04:01:06,640 --> 04:01:08,640
once we build up this multi-head attention

4359
04:01:08,640 --> 04:01:10,640
class so cool

4360
04:01:10,640 --> 04:01:12,640
now we have a feed forward

4361
04:01:12,640 --> 04:01:14,640
which is as explained

4362
04:01:14,640 --> 04:01:16,640
just in the diagram here

4363
04:01:16,640 --> 04:01:18,640
our feed forward is just this

4364
04:01:18,640 --> 04:01:20,640
which we're actually going to build out next

4365
04:01:22,640 --> 04:01:24,640
and we have two layer norms

4366
04:01:24,640 --> 04:01:26,640
and these are just for the

4367
04:01:26,640 --> 04:01:28,640
post norm

4368
04:01:28,640 --> 04:01:30,640
pre norm architecture that we could implement here

4369
04:01:30,640 --> 04:01:32,640
in this case it's going to be

4370
04:01:32,640 --> 04:01:34,640
post norm just because

4371
04:01:34,640 --> 04:01:36,640
I found that it converges better for this

4372
04:01:36,640 --> 04:01:38,640
for this course and the data that we're using

4373
04:01:38,640 --> 04:01:40,640
and just the model parameters

4374
04:01:40,640 --> 04:01:42,640
and what not it just works better

4375
04:01:42,640 --> 04:01:44,640
so

4376
04:01:44,640 --> 04:01:46,640
also that is the original

4377
04:01:46,640 --> 04:01:48,640
architecture that we use in the

4378
04:01:48,640 --> 04:01:50,640
attention paper

4379
04:01:50,640 --> 04:01:52,640
so you might have seen that they do an add a norm

4380
04:01:52,640 --> 04:01:54,640
rather than a norm and add

4381
04:01:54,640 --> 04:01:56,640
anyways

4382
04:01:56,640 --> 04:01:58,640
we've initialized all of these

4383
04:01:58,640 --> 04:02:00,640
so we have head size, self attention

4384
04:02:00,640 --> 04:02:02,640
feed forward and then two layer norms

4385
04:02:02,640 --> 04:02:04,640
so in our forward pass

4386
04:02:04,640 --> 04:02:06,640
we do our self attention first

4387
04:02:06,640 --> 04:02:08,640
let's actually go back to here

4388
04:02:08,640 --> 04:02:10,640
so we do our self attention

4389
04:02:10,640 --> 04:02:12,640
then add a norm

4390
04:02:12,640 --> 04:02:14,640
then a feed forward and then add a norm again

4391
04:02:16,640 --> 04:02:18,640
so what does this look like

4392
04:02:18,640 --> 04:02:20,640
self attention, add a norm

4393
04:02:20,640 --> 04:02:22,640
feed forward, add a norm

4394
04:02:22,640 --> 04:02:24,640
cool

4395
04:02:24,640 --> 04:02:26,640
so we're doing an add so we're going

4396
04:02:26,640 --> 04:02:28,640
x plus the previous

4397
04:02:28,640 --> 04:02:30,640
answer which is adding them together

4398
04:02:30,640 --> 04:02:32,640
and then we're just applying a layer norm to this

4399
04:02:32,640 --> 04:02:34,640
so cool

4400
04:02:34,640 --> 04:02:36,640
if you want to look up more into what layer norm does

4401
04:02:36,640 --> 04:02:38,640
and everything and why it's so useful

4402
04:02:38,640 --> 04:02:40,640
you can totally go out of your way to do that

4403
04:02:40,640 --> 04:02:42,640
but

4404
04:02:42,640 --> 04:02:44,640
layer norm is essentially just going to

4405
04:02:44,640 --> 04:02:46,640
help smoothen out our features

4406
04:02:46,640 --> 04:02:48,640
here

4407
04:02:48,640 --> 04:02:50,640
so

4408
04:02:50,640 --> 04:02:52,640
and honestly there's not much else to that

4409
04:02:52,640 --> 04:02:54,640
we just return this final value here

4410
04:02:54,640 --> 04:02:56,640
and that's pretty much the output of our blocks

4411
04:02:56,640 --> 04:02:58,640
so

4412
04:02:58,640 --> 04:03:00,640
next up I'm going to add

4413
04:03:00,640 --> 04:03:02,640
a new little code block here

4414
04:03:02,640 --> 04:03:04,640
which is going to be

4415
04:03:04,640 --> 04:03:06,640
our feed forward

4416
04:03:06,640 --> 04:03:08,640
so let's go ahead and do that

4417
04:03:08,640 --> 04:03:10,640
so feed forward, it's just going to look exactly like this

4418
04:03:10,640 --> 04:03:12,640
it's actually quite simple

4419
04:03:12,640 --> 04:03:14,640
so all we do is we make an nn dot sequential

4420
04:03:14,640 --> 04:03:16,640
torch dot nn

4421
04:03:16,640 --> 04:03:18,640
we make this a sequential network of linear

4422
04:03:18,640 --> 04:03:20,640
linear, relu, and then linear

4423
04:03:20,640 --> 04:03:22,640
so

4424
04:03:22,640 --> 04:03:24,640
in our linear

4425
04:03:24,640 --> 04:03:26,640
we have to pay attention to the shapes here

4426
04:03:26,640 --> 04:03:28,640
so we have n embed

4427
04:03:28,640 --> 04:03:30,640
and then n embed times 4

4428
04:03:30,640 --> 04:03:32,640
and then the relu will just

4429
04:03:32,640 --> 04:03:34,640
essentially

4430
04:03:34,640 --> 04:03:36,640
what the relu will do is it looks like this

4431
04:03:36,640 --> 04:03:38,640
let me illustrate this for you guys

4432
04:03:38,640 --> 04:03:40,640
so

4433
04:03:40,640 --> 04:03:42,640
essentially you have this graph here

4434
04:03:44,640 --> 04:03:46,640
and

4435
04:03:46,640 --> 04:03:48,640
let's just make this a whole plane actually

4436
04:03:50,640 --> 04:03:52,640
so

4437
04:03:52,640 --> 04:03:54,640
all of these values

4438
04:03:54,640 --> 04:03:56,640
that are below 0

4439
04:03:56,640 --> 04:03:58,640
all these values that are below 0 on the x axis

4440
04:03:58,640 --> 04:04:00,640
and

4441
04:04:00,640 --> 04:04:02,640
equal to 0 will be changed

4442
04:04:02,640 --> 04:04:04,640
just to 0 like that so you have all these values

4443
04:04:04,640 --> 04:04:06,640
that look like this

4444
04:04:06,640 --> 04:04:08,640
and then everything that is above 0 just stays the same

4445
04:04:08,640 --> 04:04:10,640
so you essentially just have this

4446
04:04:10,640 --> 04:04:12,640
funny looking shape it's like straight

4447
04:04:12,640 --> 04:04:14,640
and then diagonal that's what the relu function does

4448
04:04:14,640 --> 04:04:16,640
it looks at a number

4449
04:04:16,640 --> 04:04:18,640
sees if it's equal to or less than 0

4450
04:04:18,640 --> 04:04:20,640
if that's true we give that number 0

4451
04:04:20,640 --> 04:04:22,640
and if it's not

4452
04:04:22,640 --> 04:04:24,640
then we just leave the number alone

4453
04:04:24,640 --> 04:04:26,640
so cool very cool

4454
04:04:26,640 --> 04:04:28,640
non-linearity function

4455
04:04:28,640 --> 04:04:30,640
you can read papers on that if you like

4456
04:04:30,640 --> 04:04:32,640
but

4457
04:04:32,640 --> 04:04:34,640
essentially the shape of this

4458
04:04:34,640 --> 04:04:36,640
just doesn't matter all we're doing is

4459
04:04:36,640 --> 04:04:38,640
we're just making sure that we're just converting

4460
04:04:38,640 --> 04:04:40,640
some values if they're equal to

4461
04:04:40,640 --> 04:04:42,640
or below 0 that's all this is doing

4462
04:04:42,640 --> 04:04:44,640
and then

4463
04:04:44,640 --> 04:04:46,640
we essentially are multiplying

4464
04:04:46,640 --> 04:04:48,640
this we're doing this

4465
04:04:48,640 --> 04:04:50,640
linear transformation times this one

4466
04:04:50,640 --> 04:04:52,640
so we have to make sure that these inner

4467
04:04:52,640 --> 04:04:54,640
we have to make sure that these

4468
04:04:54,640 --> 04:04:56,640
inner dimensions line up so 4 times

4469
04:04:56,640 --> 04:04:58,640
N embed and 4 times N embed

4470
04:04:58,640 --> 04:05:00,640
those are equal to each other so our output shape

4471
04:05:00,640 --> 04:05:02,640
should be N embed

4472
04:05:02,640 --> 04:05:04,640
by N embed cool

4473
04:05:04,640 --> 04:05:06,640
so now we have our dropout

4474
04:05:06,640 --> 04:05:08,640
and in case you don't know what dropout is

4475
04:05:08,640 --> 04:05:10,640
it pretty much just

4476
04:05:10,640 --> 04:05:12,640
makes a certain percentage

4477
04:05:12,640 --> 04:05:14,640
of our neurons just

4478
04:05:14,640 --> 04:05:16,640
dropout and become 0

4479
04:05:16,640 --> 04:05:18,640
this is used to prevent overfitting

4480
04:05:18,640 --> 04:05:20,640
and some other little details

4481
04:05:20,640 --> 04:05:22,640
that I'm sure you could

4482
04:05:22,640 --> 04:05:24,640
you could figure out through experimenting

4483
04:05:24,640 --> 04:05:26,640
so

4484
04:05:26,640 --> 04:05:28,640
all this actually looks like in a parameter form

4485
04:05:28,640 --> 04:05:30,640
is just

4486
04:05:30,640 --> 04:05:32,640
dropout

4487
04:05:34,640 --> 04:05:36,640
dropout equals

4488
04:05:36,640 --> 04:05:38,640
we'll just say 0.2 for the same

4489
04:05:38,640 --> 04:05:40,640
so 0.2 means

4490
04:05:40,640 --> 04:05:42,640
20%

4491
04:05:42,640 --> 04:05:44,640
or 0.2 is going to

4492
04:05:44,640 --> 04:05:46,640
yeah so 0.2

4493
04:05:46,640 --> 04:05:48,640
in percentage form is just going to dropout

4494
04:05:48,640 --> 04:05:50,640
20% of our

4495
04:05:50,640 --> 04:05:52,640
neurons turn them to 0 to prevent overfitting

4496
04:05:52,640 --> 04:05:54,640
that's what that's doing

4497
04:05:54,640 --> 04:05:56,640
so cool

4498
04:05:56,640 --> 04:05:58,640
we have our feedforward network we dropout after

4499
04:05:58,640 --> 04:06:00,640
to prevent overfitting and then we just

4500
04:06:00,640 --> 04:06:02,640
call it forward on this sequential network

4501
04:06:02,640 --> 04:06:04,640
so cool

4502
04:06:04,640 --> 04:06:06,640
feedforward pretty self-explanatory

4503
04:06:06,640 --> 04:06:08,640
we're going to add the

4504
04:06:08,640 --> 04:06:10,640
multi-head attention class

4505
04:06:10,640 --> 04:06:12,640
so we've built all these decoder blocks

4506
04:06:12,640 --> 04:06:14,640
we've built

4507
04:06:14,640 --> 04:06:16,640
inside of the decoder blocks we've built the feedforward

4508
04:06:16,640 --> 04:06:18,640
and our res connections

4509
04:06:18,640 --> 04:06:20,640
and now

4510
04:06:20,640 --> 04:06:22,640
all we have to do left in this block

4511
04:06:22,640 --> 04:06:24,640
is the multi-head attention

4512
04:06:24,640 --> 04:06:26,640
so it's going to look exactly like this here

4513
04:06:26,640 --> 04:06:28,640
we're going to ignore the keys and queers for now

4514
04:06:28,640 --> 04:06:30,640
and save this for dot product attention

4515
04:06:30,640 --> 04:06:32,640
so we're going to

4516
04:06:32,640 --> 04:06:34,640
essentially just make a bunch of these

4517
04:06:34,640 --> 04:06:36,640
multiple

4518
04:06:36,640 --> 04:06:38,640
heads

4519
04:06:38,640 --> 04:06:40,640
and we're going to concatenate results and do a linear

4520
04:06:40,640 --> 04:06:42,640
transformation so what does this look like in code

4521
04:06:42,640 --> 04:06:44,640
well let's go ahead

4522
04:06:44,640 --> 04:06:46,640
and add this here

4523
04:06:46,640 --> 04:06:48,640
all that attention cool

4524
04:06:48,640 --> 04:06:50,640
so multiple heads of attention in parallel

4525
04:06:50,640 --> 04:06:52,640
I explained this earlier so I'm not going to jump into

4526
04:06:52,640 --> 04:06:54,640
too much detail on that

4527
04:06:54,640 --> 04:06:56,640
but we have our knit

4528
04:06:56,640 --> 04:06:58,640
we have our forward

4529
04:06:58,640 --> 04:07:00,640
and what are we doing in here

4530
04:07:00,640 --> 04:07:02,640
so our self dot heads is just a module list

4531
04:07:02,640 --> 04:07:04,640
and

4532
04:07:04,640 --> 04:07:06,640
module list is kind of funky I'll dive into it

4533
04:07:06,640 --> 04:07:08,640
a little bit later

4534
04:07:08,640 --> 04:07:10,640
but essentially what we're doing is we're having

4535
04:07:10,640 --> 04:07:12,640
a bunch of these heads

4536
04:07:12,640 --> 04:07:14,640
essentially in parallel

4537
04:07:14,640 --> 04:07:16,640
for each head

4538
04:07:16,640 --> 04:07:18,640
so num heads let's say our num heads is

4539
04:07:18,640 --> 04:07:20,640
set to

4540
04:07:20,640 --> 04:07:22,640
our num heads

4541
04:07:22,640 --> 04:07:24,640
is set to

4542
04:07:24,640 --> 04:07:26,640
maybe four in this

4543
04:07:26,640 --> 04:07:28,640
block we do multi-head attention

4544
04:07:28,640 --> 04:07:30,640
we do n heads and then head size

4545
04:07:30,640 --> 04:07:32,640
so

4546
04:07:32,640 --> 04:07:34,640
and heads and then head size so num heads

4547
04:07:34,640 --> 04:07:36,640
essentially what it is so for the number of

4548
04:07:36,640 --> 04:07:38,640
heads that we have which is four

4549
04:07:38,640 --> 04:07:40,640
we're going to pretty much make one head

4550
04:07:40,640 --> 04:07:42,640
running in parallel

4551
04:07:42,640 --> 04:07:44,640
so four heads running in parallel is what this

4552
04:07:44,640 --> 04:07:46,640
does here

4553
04:07:46,640 --> 04:07:48,640
then we have this projection

4554
04:07:48,640 --> 04:07:50,640
which is essentially just going to

4555
04:07:50,640 --> 04:07:52,640
project the

4556
04:07:52,640 --> 04:07:54,640
head size

4557
04:07:54,640 --> 04:07:56,640
times the number of

4558
04:07:56,640 --> 04:07:58,640
heads to an embed

4559
04:07:58,640 --> 04:08:00,640
and you might ask well that's

4560
04:08:00,640 --> 04:08:02,640
weird because

4561
04:08:02,640 --> 04:08:04,640
num heads times this is

4562
04:08:04,640 --> 04:08:06,640
literally equal to an embedding

4563
04:08:06,640 --> 04:08:08,640
if you go back to the math

4564
04:08:08,640 --> 04:08:10,640
we did here

4565
04:08:10,640 --> 04:08:12,640
and the purpose of this is just to be

4566
04:08:12,640 --> 04:08:14,640
super hackable so that if you actually do want to

4567
04:08:14,640 --> 04:08:16,640
change these around it won't be throwing you dimensionality

4568
04:08:16,640 --> 04:08:18,640
errors so that's what we're doing

4569
04:08:18,640 --> 04:08:20,640
just a little projection

4570
04:08:20,640 --> 04:08:22,640
from our

4571
04:08:22,640 --> 04:08:24,640
whatever these values are

4572
04:08:24,640 --> 04:08:26,640
up to this

4573
04:08:26,640 --> 04:08:28,640
constant feature

4574
04:08:28,640 --> 04:08:30,640
length of an embed

4575
04:08:30,640 --> 04:08:32,640
so then we just follow that with a drop out

4576
04:08:32,640 --> 04:08:34,640
dropping out 20% of the

4577
04:08:34,640 --> 04:08:36,640
networks neurons

4578
04:08:36,640 --> 04:08:38,640
now let's go into this forward here

4579
04:08:38,640 --> 04:08:40,640
so forward

4580
04:08:40,640 --> 04:08:42,640
torch dot concatenate or torch dot cat

4581
04:08:42,640 --> 04:08:44,640
we do four h and self dot heads

4582
04:08:44,640 --> 04:08:46,640
so we're going to concatenate

4583
04:08:46,640 --> 04:08:48,640
each head together

4584
04:08:48,640 --> 04:08:50,640
along the last dimension

4585
04:08:50,640 --> 04:08:52,640
and the last dimension in this case

4586
04:08:52,640 --> 04:08:54,640
is the

4587
04:08:54,640 --> 04:08:56,640
b batch

4588
04:08:56,640 --> 04:08:58,640
by time

4589
04:08:58,640 --> 04:09:00,640
by we just say feature dimension or channel dimension

4590
04:09:02,640 --> 04:09:04,640
the channel dimension here is the

4591
04:09:04,640 --> 04:09:06,640
last one so we're going to

4592
04:09:06,640 --> 04:09:08,640
concatenate along this feature dimension

4593
04:09:10,640 --> 04:09:12,640
and let me just help you illustrate

4594
04:09:12,640 --> 04:09:14,640
what exactly this looks like

4595
04:09:14,640 --> 04:09:16,640
so

4596
04:09:16,640 --> 04:09:18,640
when we concatenate along these

4597
04:09:18,640 --> 04:09:20,640
we have this b by t

4598
04:09:20,640 --> 04:09:22,640
and then we'll just say

4599
04:09:24,640 --> 04:09:26,640
our features are going to be

4600
04:09:26,640 --> 04:09:28,640
h1 like

4601
04:09:28,640 --> 04:09:30,640
each of our heads here

4602
04:09:30,640 --> 04:09:32,640
another h1

4603
04:09:32,640 --> 04:09:34,640
h1 h1 and these are all just features

4604
04:09:34,640 --> 04:09:36,640
of head one and then our next

4605
04:09:36,640 --> 04:09:38,640
would be h2

4606
04:09:38,640 --> 04:09:40,640
h2 h2 h2

4607
04:09:40,640 --> 04:09:42,640
and then let's just say we have

4608
04:09:42,640 --> 04:09:44,640
a third head go h3

4609
04:09:44,640 --> 04:09:46,640
h3

4610
04:09:46,640 --> 04:09:48,640
h3 h3

4611
04:09:48,640 --> 04:09:50,640
h3 like that

4612
04:09:50,640 --> 04:09:52,640
so we have

4613
04:09:52,640 --> 04:09:54,640
maybe four features per head

4614
04:09:54,640 --> 04:09:56,640
and there's three heads

4615
04:09:56,640 --> 04:09:58,640
so essentially all we're doing

4616
04:09:58,640 --> 04:10:00,640
when we do this concatenate

4617
04:10:00,640 --> 04:10:02,640
is we're just concatenating these along the last

4618
04:10:02,640 --> 04:10:04,640
dimension so to convert

4619
04:10:04,640 --> 04:10:06,640
this like ugly list format

4620
04:10:06,640 --> 04:10:08,640
of just each head

4621
04:10:08,640 --> 04:10:10,640
features sequentially in order

4622
04:10:10,640 --> 04:10:12,640
which is like really hard

4623
04:10:12,640 --> 04:10:14,640
to process we're just concatenating these

4624
04:10:14,640 --> 04:10:16,640
so they're easier to process

4625
04:10:16,640 --> 04:10:18,640
so that's what that does

4626
04:10:18,640 --> 04:10:20,640
and then we just follow this with a dropout

4627
04:10:20,640 --> 04:10:22,640
self dot projection

4628
04:10:22,640 --> 04:10:24,640
and then just follow that with a

4629
04:10:24,640 --> 04:10:26,640
dropout so cool

4630
04:10:26,640 --> 04:10:28,640
if that didn't totally make

4631
04:10:28,640 --> 04:10:30,640
sense you can totally just plug this code into chat

4632
04:10:30,640 --> 04:10:32,640
gbt and

4633
04:10:32,640 --> 04:10:34,640
get a detailed explanation on how it works

4634
04:10:34,640 --> 04:10:36,640
if something wasn't particularly clear

4635
04:10:36,640 --> 04:10:38,640
but essentially that's the premise

4636
04:10:38,640 --> 04:10:40,640
you have your batch by time

4637
04:10:40,640 --> 04:10:42,640
batch by

4638
04:10:42,640 --> 04:10:44,640
sequence length

4639
04:10:44,640 --> 04:10:46,640
or time use interchangeably

4640
04:10:46,640 --> 04:10:48,640
and then you have your features which are all

4641
04:10:48,640 --> 04:10:50,640
just in this weird list format

4642
04:10:50,640 --> 04:10:52,640
of each feature just listed

4643
04:10:52,640 --> 04:10:54,640
after another

4644
04:10:54,640 --> 04:10:56,640
so cool

4645
04:10:56,640 --> 04:10:58,640
that's what multi head attention looks like

4646
04:10:58,640 --> 04:11:00,640
let's go ahead and implement dot product

4647
04:11:00,640 --> 04:11:02,640
attention or scale dot product attention

4648
04:11:02,640 --> 04:11:04,640
so a little something I'd like to cover before

4649
04:11:04,640 --> 04:11:06,640
we go into our next scaled

4650
04:11:06,640 --> 04:11:08,640
dot product attention was just this linear

4651
04:11:08,640 --> 04:11:10,640
transformation here

4652
04:11:10,640 --> 04:11:12,640
and you might think well what's the point if we're just

4653
04:11:12,640 --> 04:11:14,640
transforming

4654
04:11:14,640 --> 04:11:16,640
an embed to an embed right

4655
04:11:16,640 --> 04:11:18,640
we're to have the match like that

4656
04:11:18,640 --> 04:11:20,640
and

4657
04:11:20,640 --> 04:11:22,640
essentially what this does is it just adds in another

4658
04:11:22,640 --> 04:11:24,640
learnable parameter

4659
04:11:24,640 --> 04:11:26,640
for us so it has a weight

4660
04:11:26,640 --> 04:11:28,640
and a bias if we set bias

4661
04:11:28,640 --> 04:11:30,640
to false

4662
04:11:30,640 --> 04:11:32,640
like that then it wouldn't have a bias

4663
04:11:32,640 --> 04:11:34,640
but it does have

4664
04:11:34,640 --> 04:11:36,640
a bias so another just wx

4665
04:11:36,640 --> 04:11:38,640
plus b if you will a weight times x

4666
04:11:38,640 --> 04:11:40,640
plus a bias so it just adds

4667
04:11:40,640 --> 04:11:42,640
more learnable parameters to help our

4668
04:11:42,640 --> 04:11:44,640
network

4669
04:11:44,640 --> 04:11:46,640
learn more about this text

4670
04:11:46,640 --> 04:11:48,640
so cool I'm going to go ahead and add

4671
04:11:48,640 --> 04:11:50,640
in this last but not least

4672
04:11:52,640 --> 04:11:54,640
scale dot product attention

4673
04:11:54,640 --> 04:11:56,640
or head class so there's going to be

4674
04:11:56,640 --> 04:11:58,640
a bunch of these

4675
04:11:58,640 --> 04:12:00,640
heads hence

4676
04:12:00,640 --> 04:12:02,640
class head running in parallel

4677
04:12:02,640 --> 04:12:04,640
and inside of here we're going to do some

4678
04:12:04,640 --> 04:12:06,640
scale dot product attention

4679
04:12:06,640 --> 04:12:08,640
so there's a lot of code in here don't get

4680
04:12:08,640 --> 04:12:10,640
too overwhelmed by this but I'm going to walk

4681
04:12:10,640 --> 04:12:12,640
through this step by step so we have our

4682
04:12:12,640 --> 04:12:14,640
in it we have our forward

4683
04:12:14,640 --> 04:12:16,640
awesome

4684
04:12:16,640 --> 04:12:18,640
so what do we do in our

4685
04:12:18,640 --> 04:12:20,640
architecture here

4686
04:12:20,640 --> 04:12:22,640
so we have a key

4687
04:12:22,640 --> 04:12:24,640
a query and a value

4688
04:12:24,640 --> 04:12:26,640
the keys and the queries dot

4689
04:12:26,640 --> 04:12:28,640
product together they get scaled

4690
04:12:28,640 --> 04:12:30,640
by

4691
04:12:30,640 --> 04:12:32,640
one over the square root of

4692
04:12:32,640 --> 04:12:34,640
length of a row in the keys or queries

4693
04:12:34,640 --> 04:12:36,640
matrix so we'll just say maybe keys

4694
04:12:36,640 --> 04:12:38,640
for example

4695
04:12:38,640 --> 04:12:40,640
the row of keys

4696
04:12:40,640 --> 04:12:42,640
the length of a row in keys

4697
04:12:42,640 --> 04:12:44,640
and then we just do our

4698
04:12:44,640 --> 04:12:46,640
masking to make sure the network

4699
04:12:46,640 --> 04:12:48,640
does not look ahead and cheat

4700
04:12:48,640 --> 04:12:50,640
and then we do a softmax

4701
04:12:50,640 --> 04:12:52,640
and a matrix

4702
04:12:52,640 --> 04:12:54,640
multiply to

4703
04:12:54,640 --> 04:12:56,640
essentially add this

4704
04:12:56,640 --> 04:12:58,640
value weight on top of it

4705
04:12:58,640 --> 04:13:00,640
so cool

4706
04:13:00,640 --> 04:13:02,640
we do this

4707
04:13:02,640 --> 04:13:04,640
keep in mind this initialization

4708
04:13:04,640 --> 04:13:06,640
is not actually doing any calculations

4709
04:13:06,640 --> 04:13:08,640
but just rather initializing

4710
04:13:08,640 --> 04:13:10,640
linear transformations that we will do

4711
04:13:10,640 --> 04:13:12,640
in the forward pass

4712
04:13:12,640 --> 04:13:14,640
so this self dot key

4713
04:13:14,640 --> 04:13:16,640
is just going to

4714
04:13:16,640 --> 04:13:18,640
transform and embed to head size

4715
04:13:18,640 --> 04:13:20,640
bias false and then

4716
04:13:20,640 --> 04:13:22,640
I mean the rest of these are just the same

4717
04:13:22,640 --> 04:13:24,640
and embed to head size because each head

4718
04:13:24,640 --> 04:13:26,640
will have 96 features

4719
04:13:26,640 --> 04:13:28,640
rather than 384

4720
04:13:28,640 --> 04:13:30,640
so we kind of already went over that

4721
04:13:30,640 --> 04:13:32,640
but that's just what that's doing

4722
04:13:32,640 --> 04:13:34,640
cool that's just a linear transformation

4723
04:13:34,640 --> 04:13:36,640
that's happening to convert

4724
04:13:36,640 --> 04:13:38,640
from 384 to 96 features

4725
04:13:38,640 --> 04:13:40,640
then we have this

4726
04:13:40,640 --> 04:13:42,640
self dot register buffer

4727
04:13:42,640 --> 04:13:44,640
well what does this do you might ask

4728
04:13:44,640 --> 04:13:46,640
register buffer is essentially just going

4729
04:13:46,640 --> 04:13:48,640
to register

4730
04:13:48,640 --> 04:13:50,640
this no look ahead

4731
04:13:50,640 --> 04:13:52,640
masking in the model state

4732
04:13:52,640 --> 04:13:54,640
so instead of having to re-initialize

4733
04:13:54,640 --> 04:13:56,640
this every single head for every

4734
04:13:56,640 --> 04:13:58,640
single forward and backward pass

4735
04:13:58,640 --> 04:14:00,640
we're just going to add this to the model

4736
04:14:00,640 --> 04:14:02,640
state so it's going to save us a lot of

4737
04:14:02,640 --> 04:14:04,640
computation that way on our training

4738
04:14:04,640 --> 04:14:06,640
so our training times can be reduced just because

4739
04:14:06,640 --> 04:14:08,640
we're registering this

4740
04:14:08,640 --> 04:14:10,640
yeah

4741
04:14:10,640 --> 04:14:12,640
so it's just going to prevent some of that

4742
04:14:12,640 --> 04:14:14,640
overhead computation of having to redo

4743
04:14:14,640 --> 04:14:16,640
this over and over again

4744
04:14:16,640 --> 04:14:18,640
you could still do training without

4745
04:14:18,640 --> 04:14:20,640
this it would just take longer

4746
04:14:20,640 --> 04:14:22,640
so that's what that's doing

4747
04:14:22,640 --> 04:14:24,640
yeah

4748
04:14:24,640 --> 04:14:26,640
so now we have this drop-out

4749
04:14:26,640 --> 04:14:28,640
of course and then in our forward pass

4750
04:14:28,640 --> 04:14:30,640
let's

4751
04:14:30,640 --> 04:14:32,640
break this down step by step here

4752
04:14:32,640 --> 04:14:34,640
so we have a b by t by c

4753
04:14:34,640 --> 04:14:36,640
so batch by time

4754
04:14:36,640 --> 04:14:38,640
by channel is our shape

4755
04:14:38,640 --> 04:14:40,640
we just unpack those numbers

4756
04:14:40,640 --> 04:14:42,640
and then we have a key

4757
04:14:42,640 --> 04:14:44,640
which is just calling this

4758
04:14:44,640 --> 04:14:46,640
linear transformation here on an input

4759
04:14:46,640 --> 04:14:48,640
x

4760
04:14:48,640 --> 04:14:50,640
and then a query which is also

4761
04:14:50,640 --> 04:14:52,640
calling the same transformation but a different

4762
04:14:52,640 --> 04:14:54,640
learnable transformation on x as well

4763
04:14:54,640 --> 04:14:56,640
so what we get

4764
04:14:56,640 --> 04:14:58,640
is this instead of b by t by c

4765
04:14:58,640 --> 04:15:00,640
we get b by t by head size

4766
04:15:00,640 --> 04:15:02,640
hence this transformation

4767
04:15:02,640 --> 04:15:04,640
from 384 to 96

4768
04:15:04,640 --> 04:15:06,640
so that's what that is

4769
04:15:06,640 --> 04:15:08,640
that's how these turn out here

4770
04:15:08,640 --> 04:15:10,640
so now we can actually compute

4771
04:15:10,640 --> 04:15:12,640
the attention scores

4772
04:15:12,640 --> 04:15:14,640
so what do we do

4773
04:15:14,640 --> 04:15:16,640
we'll just say weights is our attention

4774
04:15:16,640 --> 04:15:18,640
weights are

4775
04:15:18,640 --> 04:15:20,640
I guess you could say that

4776
04:15:20,640 --> 04:15:22,640
we have our queries

4777
04:15:22,640 --> 04:15:24,640
dot product matrix multiply

4778
04:15:24,640 --> 04:15:26,640
with the

4779
04:15:26,640 --> 04:15:28,640
keys transposed

4780
04:15:28,640 --> 04:15:30,640
so

4781
04:15:30,640 --> 04:15:32,640
what does this actually look like

4782
04:15:32,640 --> 04:15:34,640
and I want to help you guys

4783
04:15:34,640 --> 04:15:36,640
sort of understand what transposing does here

4784
04:15:36,640 --> 04:15:38,640
so

4785
04:15:38,640 --> 04:15:40,640
let's go back to here

4786
04:15:40,640 --> 04:15:42,640
and draw out what this is going to look like

4787
04:15:42,640 --> 04:15:44,640
so

4788
04:15:44,640 --> 04:15:46,640
essentially what transposing is going to do

4789
04:15:46,640 --> 04:15:48,640
is

4790
04:15:48,640 --> 04:15:50,640
it is just going to make sure

4791
04:15:50,640 --> 04:15:52,640
let me draw this out

4792
04:15:52,640 --> 04:15:54,640
first

4793
04:15:54,640 --> 04:15:56,640
so let's say you had

4794
04:15:56,640 --> 04:15:58,640
I don't know

4795
04:15:58,640 --> 04:16:00,640
maybe

4796
04:16:00,640 --> 04:16:02,640
a

4797
04:16:06,640 --> 04:16:08,640
b

4798
04:16:08,640 --> 04:16:10,640
c

4799
04:16:10,640 --> 04:16:12,640
d

4800
04:16:12,640 --> 04:16:14,640
and you have a

4801
04:16:14,640 --> 04:16:16,640
b

4802
04:16:16,640 --> 04:16:18,640
c

4803
04:16:18,640 --> 04:16:20,640
and d cool let's draw some lines

4804
04:16:20,640 --> 04:16:22,640
to separate these

4805
04:16:24,640 --> 04:16:26,640
so

4806
04:16:34,640 --> 04:16:36,640
awesome so essentially what this does

4807
04:16:36,640 --> 04:16:38,640
is the transposing

4808
04:16:38,640 --> 04:16:40,640
puts it into this form

4809
04:16:40,640 --> 04:16:42,640
so if we didn't have

4810
04:16:42,640 --> 04:16:44,640
transposed then this would be in a different order

4811
04:16:44,640 --> 04:16:46,640
it wouldn't be a b c d

4812
04:16:46,640 --> 04:16:48,640
in both

4813
04:16:48,640 --> 04:16:50,640
from like top to bottom left to right type of thing

4814
04:16:50,640 --> 04:16:52,640
it would be in a different order

4815
04:16:52,640 --> 04:16:54,640
but essentially not allow us

4816
04:16:54,640 --> 04:16:56,640
to multiply them the same way

4817
04:16:56,640 --> 04:16:58,640
so when we do a by a

4818
04:16:58,640 --> 04:17:00,640
a times b

4819
04:17:00,640 --> 04:17:02,640
it's like sort of a direct

4820
04:17:02,640 --> 04:17:04,640
multiply if you will

4821
04:17:04,640 --> 04:17:06,640
I don't know if you remember times tables at all

4822
04:17:06,640 --> 04:17:08,640
from elementary school

4823
04:17:08,640 --> 04:17:10,640
but that's pretty much what it is

4824
04:17:10,640 --> 04:17:12,640
we're just setting up in a times table form

4825
04:17:12,640 --> 04:17:14,640
and we're computing attention scores that way

4826
04:17:14,640 --> 04:17:16,640
so

4827
04:17:16,640 --> 04:17:18,640
that's what that is

4828
04:17:18,640 --> 04:17:20,640
that's what this transposing is doing

4829
04:17:20,640 --> 04:17:22,640
all this does is it just flips

4830
04:17:22,640 --> 04:17:24,640
the second last dimension

4831
04:17:24,640 --> 04:17:26,640
with the last dimension

4832
04:17:26,640 --> 04:17:28,640
so

4833
04:17:28,640 --> 04:17:30,640
in our case our second last

4834
04:17:30,640 --> 04:17:32,640
is t and our last is head size

4835
04:17:32,640 --> 04:17:34,640
so it just swaps these two

4836
04:17:34,640 --> 04:17:36,640
so we get b by t by head size

4837
04:17:36,640 --> 04:17:38,640
and then b by head size by t

4838
04:17:38,640 --> 04:17:40,640
we dot product these together

4839
04:17:40,640 --> 04:17:42,640
also keeping in mind our scaling

4840
04:17:42,640 --> 04:17:44,640
here

4841
04:17:44,640 --> 04:17:46,640
which is taking this

4842
04:17:46,640 --> 04:17:48,640
we're just taking this scaling

4843
04:17:48,640 --> 04:17:50,640
one

4844
04:17:50,640 --> 04:17:52,640
over the square root of length

4845
04:17:52,640 --> 04:17:54,640
of a row in the keys

4846
04:17:54,640 --> 04:17:56,640
if we look at this here

4847
04:17:56,640 --> 04:17:58,640
now there's little analogy

4848
04:17:58,640 --> 04:18:00,640
I'd like to provide for this scaling

4849
04:18:00,640 --> 04:18:02,640
right here

4850
04:18:02,640 --> 04:18:04,640
so imagine in a room

4851
04:18:04,640 --> 04:18:06,640
with a group of people and you're trying to understand

4852
04:18:06,640 --> 04:18:08,640
the overall conversation

4853
04:18:08,640 --> 04:18:10,640
if everyone is talking at once

4854
04:18:10,640 --> 04:18:12,640
it might be challenging to keep track

4855
04:18:12,640 --> 04:18:14,640
of what's being said

4856
04:18:14,640 --> 04:18:16,640
it would be more manageable if you could focus on

4857
04:18:16,640 --> 04:18:18,640
time right?

4858
04:18:18,640 --> 04:18:20,640
so that's similar to how a multi head attention

4859
04:18:20,640 --> 04:18:22,640
in a transformer works

4860
04:18:22,640 --> 04:18:24,640
so each of these heads

4861
04:18:24,640 --> 04:18:26,640
divides the original problem

4862
04:18:26,640 --> 04:18:28,640
of understanding the entire conversation

4863
04:18:28,640 --> 04:18:30,640
i.e. the entire input sequence

4864
04:18:30,640 --> 04:18:32,640
into smaller more manageable

4865
04:18:32,640 --> 04:18:34,640
sub problems

4866
04:18:34,640 --> 04:18:36,640
each of these sub problems is a head

4867
04:18:36,640 --> 04:18:38,640
so the head size

4868
04:18:38,640 --> 04:18:40,640
is the number of these sub problems

4869
04:18:40,640 --> 04:18:42,640
now consider what happens when each person

4870
04:18:42,640 --> 04:18:44,640
talks louder or quieter

4871
04:18:44,640 --> 04:18:46,640
if someone speaks too loudly

4872
04:18:46,640 --> 04:18:48,640
or

4873
04:18:48,640 --> 04:18:50,640
the values and the vectors are very large

4874
04:18:50,640 --> 04:18:52,640
it might drown out the others

4875
04:18:52,640 --> 04:18:54,640
this could make it difficult to understand the conversation

4876
04:18:54,640 --> 04:18:56,640
because you're only hearing one voice

4877
04:18:56,640 --> 04:18:58,640
or most of one voice

4878
04:18:58,640 --> 04:19:00,640
to prevent this

4879
04:19:00,640 --> 04:19:02,640
we want to control how loud

4880
04:19:02,640 --> 04:19:04,640
or how quiet each person is talking

4881
04:19:04,640 --> 04:19:06,640
so we can hear everyone evenly

4882
04:19:06,640 --> 04:19:08,640
the dot product of the

4883
04:19:08,640 --> 04:19:10,640
query and key vectors

4884
04:19:10,640 --> 04:19:12,640
in the attention mechanism

4885
04:19:12,640 --> 04:19:14,640
we want to check how loud each of voices

4886
04:19:14,640 --> 04:19:16,640
if the vectors are very large

4887
04:19:16,640 --> 04:19:18,640
or high dimensional

4888
04:19:18,640 --> 04:19:20,640
or many people are talking

4889
04:19:20,640 --> 04:19:22,640
the dot product can be very large

4890
04:19:22,640 --> 04:19:24,640
to control this volume

4891
04:19:24,640 --> 04:19:26,640
by scaling down the dot product

4892
04:19:26,640 --> 04:19:28,640
using the square root of the head size

4893
04:19:30,640 --> 04:19:32,640
this scaling helps ensure that no single

4894
04:19:32,640 --> 04:19:34,640
voice is too dominant

4895
04:19:34,640 --> 04:19:36,640
allowing us to hear all the voices evenly

4896
04:19:36,640 --> 04:19:38,640
this is why we don't scale

4897
04:19:38,640 --> 04:19:40,640
by the number of heads

4898
04:19:40,640 --> 04:19:42,640
time steps

4899
04:19:42,640 --> 04:19:44,640
they don't directly affect how loud each voice is

4900
04:19:44,640 --> 04:19:46,640
so in sum

4901
04:19:46,640 --> 04:19:48,640
multi head attention allows us to focus on

4902
04:19:48,640 --> 04:19:50,640
different parts of the conversation

4903
04:19:50,640 --> 04:19:52,640
and scaling helps us to hear

4904
04:19:52,640 --> 04:19:54,640
all parts of the conversation evenly

4905
04:19:54,640 --> 04:19:56,640
allowing us to understand

4906
04:19:56,640 --> 04:19:58,640
the overall conversation better

4907
04:19:58,640 --> 04:20:00,640
so hopefully that helps you understand exactly

4908
04:20:00,640 --> 04:20:02,640
what this scaling is doing

4909
04:20:02,640 --> 04:20:04,640
so now let's go into the rest of this here

4910
04:20:06,640 --> 04:20:08,640
so we have this scaling applied

4911
04:20:08,640 --> 04:20:10,640
for our head size

4912
04:20:10,640 --> 04:20:12,640
our head size dimension

4913
04:20:12,640 --> 04:20:14,640
we're doing this

4914
04:20:14,640 --> 04:20:16,640
dot product matrix multiplication

4915
04:20:16,640 --> 04:20:18,640
here we get our B by T by T

4916
04:20:18,640 --> 04:20:20,640
and then what is this

4917
04:20:20,640 --> 04:20:22,640
masked fill doing

4918
04:20:22,640 --> 04:20:24,640
so let me help you illustrate this here

4919
04:20:24,640 --> 04:20:26,640
so mask fill

4920
04:20:26,640 --> 04:20:28,640
is essentially

4921
04:20:28,640 --> 04:20:30,640
we'll say block size

4922
04:20:30,640 --> 04:20:32,640
is 3 here alright

4923
04:20:32,640 --> 04:20:34,640
so we have

4924
04:20:34,640 --> 04:20:36,640
initially

4925
04:20:36,640 --> 04:20:38,640
like a 1

4926
04:20:38,640 --> 04:20:40,640
a 0.6

4927
04:20:40,640 --> 04:20:42,640
and then like a 0.4

4928
04:20:42,640 --> 04:20:44,640
then our next one is

4929
04:20:46,640 --> 04:20:48,640
yeah we'll just say all of these are the same

4930
04:20:50,640 --> 04:20:52,640
so essentially

4931
04:20:52,640 --> 04:20:54,640
in our first one

4932
04:20:54,640 --> 04:20:56,640
we want to mask out everything

4933
04:20:56,640 --> 04:20:58,640
except for the first time step

4934
04:20:58,640 --> 04:21:00,640
and then when we advance one

4935
04:21:00,640 --> 04:21:02,640
so let's just change this here back to 0

4936
04:21:06,640 --> 04:21:08,640
when we go on to the next time step

4937
04:21:08,640 --> 04:21:10,640
we want to expose the next piece

4938
04:21:10,640 --> 04:21:12,640
so 0.6 I believe it was

4939
04:21:12,640 --> 04:21:14,640
and then a 0 again

4940
04:21:14,640 --> 04:21:16,640
and then when we expose the next time step after that

4941
04:21:16,640 --> 04:21:18,640
we want to expose all of them

4942
04:21:18,640 --> 04:21:20,640
so just kind of what this means is

4943
04:21:20,640 --> 04:21:22,640
as we

4944
04:21:22,640 --> 04:21:24,640
as the time step advances

4945
04:21:24,640 --> 04:21:26,640
in this sort of I guess vertical

4946
04:21:26,640 --> 04:21:28,640
part

4947
04:21:28,640 --> 04:21:30,640
is every time this steps 1

4948
04:21:30,640 --> 04:21:32,640
we just want to expose one more token

4949
04:21:32,640 --> 04:21:34,640
or one more

4950
04:21:34,640 --> 04:21:36,640
and then we'll use sort of in like a staircase format

4951
04:21:36,640 --> 04:21:38,640
so

4952
04:21:38,640 --> 04:21:40,640
essentially what this mask fill is doing

4953
04:21:40,640 --> 04:21:42,640
is it's making this

4954
04:21:42,640 --> 04:21:44,640
T by T so block size by block size

4955
04:21:44,640 --> 04:21:46,640
and

4956
04:21:46,640 --> 04:21:48,640
for each of these values we're going to set them

4957
04:21:48,640 --> 04:21:50,640
to negative infinity

4958
04:21:50,640 --> 04:21:52,640
so for each value that's 0

4959
04:21:52,640 --> 04:21:54,640
we're going to make that the float value negative infinity

4960
04:21:54,640 --> 04:21:56,640
so it's going to look like this

4961
04:21:56,640 --> 04:21:58,640
negative infinity

4962
04:22:00,640 --> 04:22:02,640
negative infinity

4963
04:22:04,640 --> 04:22:06,640
just like that

4964
04:22:06,640 --> 04:22:08,640
so essentially what happens after this

4965
04:22:08,640 --> 04:22:10,640
is our softmax

4966
04:22:10,640 --> 04:22:12,640
is going to take these values

4967
04:22:12,640 --> 04:22:14,640
and it's going to exponentiate normalize them

4968
04:22:14,640 --> 04:22:16,640
we already went over the soft

4969
04:22:16,640 --> 04:22:18,640
softmax previously

4970
04:22:18,640 --> 04:22:20,640
but

4971
04:22:20,640 --> 04:22:22,640
essentially what this is going to do this

4972
04:22:22,640 --> 04:22:24,640
this last dimension here

4973
04:22:24,640 --> 04:22:26,640
concatenate

4974
04:22:26,640 --> 04:22:28,640
or not concatenate

4975
04:22:28,640 --> 04:22:30,640
rather apply the softmax along the last dimension

4976
04:22:30,640 --> 04:22:32,640
is it's going to do that

4977
04:22:32,640 --> 04:22:34,640
in this sort of horizontal here

4978
04:22:34,640 --> 04:22:36,640
so this last

4979
04:22:36,640 --> 04:22:38,640
this last T

4980
04:22:38,640 --> 04:22:40,640
it's like blocks

4981
04:22:40,640 --> 04:22:42,640
it's like block size by block size

4982
04:22:42,640 --> 04:22:44,640
so it's like we'll say

4983
04:22:44,640 --> 04:22:46,640
T1 and T2

4984
04:22:46,640 --> 04:22:48,640
each of these being like the block size

4985
04:22:48,640 --> 04:22:50,640
we're just going to do it to this last T2 here

4986
04:22:50,640 --> 04:22:52,640
and this horizontal is T2

4987
04:22:52,640 --> 04:22:54,640
so

4988
04:22:54,640 --> 04:22:56,640
hopefully that makes sense

4989
04:22:56,640 --> 04:22:58,640
and essentially

4990
04:22:58,640 --> 04:23:00,640
what this exponentiation is going to do

4991
04:23:00,640 --> 04:23:02,640
is it's going to turn these values to 0

4992
04:23:02,640 --> 04:23:04,640
and

4993
04:23:04,640 --> 04:23:06,640
this one is obviously going to remain a 1

4994
04:23:06,640 --> 04:23:08,640
and then

4995
04:23:08,640 --> 04:23:10,640
it's going to turn these

4996
04:23:10,640 --> 04:23:12,640
into 0

4997
04:23:12,640 --> 04:23:14,640
and it's going to probably sharpen this 1 here

4998
04:23:14,640 --> 04:23:16,640
so this 1 is going to be more significant

4999
04:23:16,640 --> 04:23:18,640
it's going to grow more than the 0.6

5000
04:23:18,640 --> 04:23:20,640
because we're exponentiating

5001
04:23:20,640 --> 04:23:22,640
and then same here so this 1 is going to be

5002
04:23:22,640 --> 04:23:24,640
very, very sharp

5003
04:23:24,640 --> 04:23:26,640
compared to 0.6

5004
04:23:26,640 --> 04:23:28,640
or 0.4

5005
04:23:28,640 --> 04:23:30,640
that's what the softmax does

5006
04:23:30,640 --> 04:23:32,640
essentially the point of the softmax function

5007
04:23:32,640 --> 04:23:34,640
is to

5008
04:23:34,640 --> 04:23:36,640
make the values stand out more

5009
04:23:36,640 --> 04:23:38,640
it's to make the model more confident

5010
04:23:38,640 --> 04:23:40,640
in highlighting attention scores

5011
04:23:40,640 --> 04:23:42,640
so when you have one value that's like very big

5012
04:23:42,640 --> 04:23:44,640
but not too big, not exploding

5013
04:23:44,640 --> 04:23:46,640
because of our scaling, right?

5014
04:23:46,640 --> 04:23:48,640
we want to keep a minor scaling

5015
04:23:48,640 --> 04:23:50,640
but when a value is big, when a score

5016
04:23:50,640 --> 04:23:52,640
or attention score is very big

5017
04:23:52,640 --> 04:23:54,640
we want the model to put a lot of focus on that

5018
04:23:54,640 --> 04:23:56,640
and to say this

5019
04:23:56,640 --> 04:23:58,640
the entire sentence or the entire thing of tokens

5020
04:23:58,640 --> 04:24:00,640
and we just want it to learn the most

5021
04:24:00,640 --> 04:24:02,640
from that

5022
04:24:02,640 --> 04:24:04,640
so essentially that's what softmax is doing

5023
04:24:04,640 --> 04:24:06,640
instead of just a normal

5024
04:24:06,640 --> 04:24:08,640
normalizing mechanism

5025
04:24:08,640 --> 04:24:10,640
it's just doing some exponentiation

5026
04:24:10,640 --> 04:24:12,640
to that to make the model more confident

5027
04:24:12,640 --> 04:24:14,640
in its predictions

5028
04:24:14,640 --> 04:24:16,640
so this will help us score better

5029
04:24:16,640 --> 04:24:18,640
in the long run if we just

5030
04:24:18,640 --> 04:24:20,640
highlight what tokens

5031
04:24:20,640 --> 04:24:22,640
and what attention scores are more important

5032
04:24:22,640 --> 04:24:24,640
in the sequence

5033
04:24:24,640 --> 04:24:26,640
and then after this

5034
04:24:26,640 --> 04:24:28,640
softmax here

5035
04:24:28,640 --> 04:24:30,640
we just apply a simple dropout

5036
04:24:30,640 --> 04:24:32,640
on this way variable

5037
04:24:32,640 --> 04:24:34,640
this new

5038
04:24:34,640 --> 04:24:36,640
calculated way

5039
04:24:36,640 --> 04:24:38,640
scale.product.attention

5040
04:24:38,640 --> 04:24:40,640
masked

5041
04:24:40,640 --> 04:24:42,640
and then softmaxed

5042
04:24:42,640 --> 04:24:44,640
we apply a dropout on that

5043
04:24:44,640 --> 04:24:46,640
and then we perform our final

5044
04:24:46,640 --> 04:24:48,640
weighted aggregation

5045
04:24:48,640 --> 04:24:50,640
so this v

5046
04:24:50,640 --> 04:24:52,640
multiplied by the output of the softmax

5047
04:24:52,640 --> 04:24:54,640
cool

5048
04:24:54,640 --> 04:24:56,640
so we get this v

5049
04:24:56,640 --> 04:24:58,640
self.value of x so we just multiply that

5050
04:24:58,640 --> 04:25:00,640
a little pointer

5051
04:25:00,640 --> 04:25:02,640
I wanted to add

5052
04:25:02,640 --> 04:25:04,640
to this

5053
04:25:04,640 --> 04:25:06,640
module list

5054
04:25:06,640 --> 04:25:08,640
module list here

5055
04:25:08,640 --> 04:25:10,640
and then our

5056
04:25:10,640 --> 04:25:12,640
go

5057
04:25:14,640 --> 04:25:16,640
yes our sequential network here

5058
04:25:16,640 --> 04:25:18,640
so we have this

5059
04:25:18,640 --> 04:25:20,640
sequential number of blocks here

5060
04:25:20,640 --> 04:25:22,640
for n layers

5061
04:25:22,640 --> 04:25:24,640
and we have our module

5062
04:25:24,640 --> 04:25:26,640
list so what really is the difference here

5063
04:25:26,640 --> 04:25:28,640
well

5064
04:25:28,640 --> 04:25:30,640
module list is not the same as n and not sequential

5065
04:25:30,640 --> 04:25:32,640
in terms of the

5066
04:25:32,640 --> 04:25:34,640
asterisk usage that we see

5067
04:25:36,640 --> 04:25:38,640
in the language model class

5068
04:25:38,640 --> 04:25:40,640
module list doesn't run one layer

5069
04:25:40,640 --> 04:25:42,640
or head after another

5070
04:25:42,640 --> 04:25:44,640
but rather each is

5071
04:25:44,640 --> 04:25:46,640
isolated and gets its own unique perspective

5072
04:25:46,640 --> 04:25:48,640
sequential processing

5073
04:25:48,640 --> 04:25:50,640
is where one block depends on another

5074
04:25:50,640 --> 04:25:52,640
to synchronously complete

5075
04:25:52,640 --> 04:25:54,640
so that means we're waiting on one

5076
04:25:54,640 --> 04:25:56,640
to finish before we move on to the next

5077
04:25:56,640 --> 04:25:58,640
so they're not completing asynchronously

5078
04:25:58,640 --> 04:26:00,640
or in parallel

5079
04:26:00,640 --> 04:26:02,640
so the multiple heads in a transformer model

5080
04:26:02,640 --> 04:26:04,640
operate independently

5081
04:26:04,640 --> 04:26:06,640
and their computations can be processed

5082
04:26:06,640 --> 04:26:08,640
in parallel however this parallel

5083
04:26:08,640 --> 04:26:10,640
parallelism isn't due

5084
04:26:10,640 --> 04:26:12,640
to the module list that stores

5085
04:26:12,640 --> 04:26:14,640
the heads

5086
04:26:14,640 --> 04:26:16,640
instead

5087
04:26:16,640 --> 04:26:18,640
it's because of how

5088
04:26:18,640 --> 04:26:20,640
the computation are structured

5089
04:26:20,640 --> 04:26:22,640
to take advantage of the GPU's capabilities

5090
04:26:22,640 --> 04:26:24,640
for simultaneous

5091
04:26:24,640 --> 04:26:26,640
computation

5092
04:26:26,640 --> 04:26:28,640
and this is also how the deep learning

5093
04:26:28,640 --> 04:26:30,640
framework PyTorch

5094
04:26:30,640 --> 04:26:32,640
interfaces with the GPU

5095
04:26:32,640 --> 04:26:34,640
so this isn't particularly something we have to worry

5096
04:26:34,640 --> 04:26:36,640
about too much but

5097
04:26:36,640 --> 04:26:38,640
you could supposedly think

5098
04:26:38,640 --> 04:26:40,640
that these are sort of running in parallel

5099
04:26:40,640 --> 04:26:42,640
yeah

5100
04:26:42,640 --> 04:26:44,640
so if you want to get into hardware

5101
04:26:44,640 --> 04:26:46,640
then that's like your whole realm there

5102
04:26:46,640 --> 04:26:48,640
but this is PyTorch, this is software

5103
04:26:48,640 --> 04:26:50,640
not hardware at all

5104
04:26:50,640 --> 04:26:52,640
I don't expect you have to have any hardware

5105
04:26:52,640 --> 04:26:54,640
knowledge about GPU, CPU

5106
04:26:54,640 --> 04:26:56,640
anything like that

5107
04:26:56,640 --> 04:26:58,640
anyways that's just kind of a background

5108
04:26:58,640 --> 04:27:00,640
of what's going on there

5109
04:27:00,640 --> 04:27:02,640
so cool

5110
04:27:02,640 --> 04:27:04,640
so let's actually go over what is going on

5111
04:27:04,640 --> 04:27:06,640
from the ground up here

5112
04:27:06,640 --> 04:27:08,640
so we have this

5113
04:27:08,640 --> 04:27:10,640
GPT language model

5114
04:27:10,640 --> 04:27:12,640
we got our token embeddings, positional embeddings

5115
04:27:12,640 --> 04:27:14,640
we have these sequential blocks

5116
04:27:14,640 --> 04:27:16,640
initialize our weights

5117
04:27:16,640 --> 04:27:18,640
for each of these blocks

5118
04:27:18,640 --> 04:27:20,640
we have a

5119
04:27:20,640 --> 04:27:22,640
this class block

5120
04:27:22,640 --> 04:27:24,640
so we get a head size parameter

5121
04:27:24,640 --> 04:27:26,640
which is n embedded of 384

5122
04:27:26,640 --> 04:27:28,640
divided by n heads which is 4

5123
04:27:28,640 --> 04:27:30,640
so we get 96 from that

5124
04:27:30,640 --> 04:27:32,640
that's the number of features we're capturing

5125
04:27:32,640 --> 04:27:34,640
self-attention

5126
04:27:34,640 --> 04:27:36,640
we do a feed forward to layer norms

5127
04:27:36,640 --> 04:27:38,640
self-attention, layer norm

5128
04:27:38,640 --> 04:27:40,640
feed forward

5129
04:27:40,640 --> 04:27:42,640
layer norm

5130
04:27:42,640 --> 04:27:44,640
in the post norm architecture

5131
04:27:44,640 --> 04:27:46,640
then we do a feed forward

5132
04:27:46,640 --> 04:27:48,640
just a linear

5133
04:27:48,640 --> 04:27:50,640
followed by a relu followed by a linear

5134
04:27:50,640 --> 04:27:52,640
and then dropping that out

5135
04:27:52,640 --> 04:27:54,640
and then we have our multi-head attention

5136
04:27:54,640 --> 04:27:56,640
which just sort of structured

5137
04:27:56,640 --> 04:27:58,640
these attention heads

5138
04:27:58,640 --> 04:28:00,640
running in parallel and then concatenates the results

5139
04:28:00,640 --> 04:28:02,640
and then for each of these heads

5140
04:28:02,640 --> 04:28:04,640
we have our keys, queries

5141
04:28:04,640 --> 04:28:06,640
and values

5142
04:28:06,640 --> 04:28:08,640
we register a model state

5143
04:28:08,640 --> 04:28:10,640
to prevent overhead computation

5144
04:28:10,640 --> 04:28:12,640
excessively

5145
04:28:12,640 --> 04:28:14,640
then we just

5146
04:28:14,640 --> 04:28:16,640
do our scale dot product attention

5147
04:28:16,640 --> 04:28:18,640
in this line, we do our mast field

5148
04:28:18,640 --> 04:28:20,640
to prevent look ahead

5149
04:28:20,640 --> 04:28:22,640
we do our softmax to make our values

5150
04:28:22,640 --> 04:28:24,640
sharper and to make some of them stand out

5151
04:28:24,640 --> 04:28:26,640
and then

5152
04:28:26,640 --> 04:28:28,640
we do a drop out finally

5153
04:28:28,640 --> 04:28:30,640
on that and just some weighted aggregation

5154
04:28:30,640 --> 04:28:32,640
we do our weights

5155
04:28:32,640 --> 04:28:34,640
this final

5156
04:28:34,640 --> 04:28:36,640
weight variable

5157
04:28:36,640 --> 04:28:38,640
multiplied by our

5158
04:28:38,640 --> 04:28:40,640
weighted value

5159
04:28:40,640 --> 04:28:42,640
from this

5160
04:28:42,640 --> 04:28:44,640
initially this linear transformation

5161
04:28:44,640 --> 04:28:46,640
so cool, that's what's happening

5162
04:28:46,640 --> 04:28:48,640
step by step

5163
04:28:48,640 --> 04:28:50,640
in this GBT architecture

5164
04:28:50,640 --> 04:28:52,640
amazing

5165
04:28:52,640 --> 04:28:54,640
give yourself a good pat on the back

5166
04:28:54,640 --> 04:28:56,640
go grab some coffee, do whatever you need to do

5167
04:28:56,640 --> 04:28:58,640
even get some sleep

5168
04:28:58,640 --> 04:29:00,640
and get ready for the next section

5169
04:29:00,640 --> 04:29:02,640
so there's actually another hyper parameter

5170
04:29:02,640 --> 04:29:04,640
I forgot to add

5171
04:29:04,640 --> 04:29:06,640
which is n layer

5172
04:29:06,640 --> 04:29:08,640
and n layer

5173
04:29:08,640 --> 04:29:10,640
is essentially equal to 4

5174
04:29:10,640 --> 04:29:12,640
n layer is essentially equal to

5175
04:29:12,640 --> 04:29:14,640
the number

5176
04:29:14,640 --> 04:29:16,640
of decoder blocks

5177
04:29:16,640 --> 04:29:18,640
we have

5178
04:29:18,640 --> 04:29:20,640
so instead of n block we just say n layers

5179
04:29:20,640 --> 04:29:22,640
doesn't really matter what it's called

5180
04:29:22,640 --> 04:29:24,640
but that's what it means

5181
04:29:24,640 --> 04:29:26,640
and then number of heads is how many heads

5182
04:29:26,640 --> 04:29:28,640
we have running theoretically in parallel

5183
04:29:28,640 --> 04:29:30,640
and then n embed

5184
04:29:30,640 --> 04:29:32,640
is the number of total

5185
04:29:32,640 --> 04:29:34,640
dimensions we want to capture

5186
04:29:34,640 --> 04:29:36,640
from all the heads concatenated together

5187
04:29:36,640 --> 04:29:38,640
type of thing, we already went over that, so cool

5188
04:29:38,640 --> 04:29:40,640
hyper parameters

5189
04:29:40,640 --> 04:29:42,640
block size, sequence length

5190
04:29:42,640 --> 04:29:44,640
batch sizes, how many

5191
04:29:44,640 --> 04:29:46,640
of these do we want at the same time

5192
04:29:46,640 --> 04:29:48,640
max itters is just training

5193
04:29:48,640 --> 04:29:50,640
how many iterations we want to do

5194
04:29:50,640 --> 04:29:52,640
learning rate is

5195
04:29:52,640 --> 04:29:54,640
what we covered that in

5196
04:29:54,640 --> 04:29:56,640
actually the Desmos calculator that I showed

5197
04:29:56,640 --> 04:29:58,640
a little while back

5198
04:29:58,640 --> 04:30:00,640
just showing how

5199
04:30:00,640 --> 04:30:02,640
we update the model weights based on the derivative

5200
04:30:02,640 --> 04:30:04,640
of the loss function

5201
04:30:04,640 --> 04:30:06,640
and then

5202
04:30:06,640 --> 04:30:08,640
validators

5203
04:30:08,640 --> 04:30:10,640
which was just reporting the loss

5204
04:30:10,640 --> 04:30:12,640
and then lastly the

5205
04:30:12,640 --> 04:30:14,640
dropout which is dropping out

5206
04:30:14,640 --> 04:30:16,640
0.2 or 20%

5207
04:30:16,640 --> 04:30:18,640
of the total neurons

5208
04:30:18,640 --> 04:30:20,640
so awesome

5209
04:30:20,640 --> 04:30:22,640
that's pretty cool, let's go ahead and jump into

5210
04:30:22,640 --> 04:30:24,640
some data stuff

5211
04:30:24,640 --> 04:30:26,640
I'm going to pull out a paper here

5212
04:30:26,640 --> 04:30:28,640
so let's just make sure everything works here

5213
04:30:28,640 --> 04:30:30,640
and then we're actually going to download our data

5214
04:30:30,640 --> 04:30:32,640
so I want to try to run some iterations

5215
04:30:32,640 --> 04:30:34,640
and

5216
04:30:34,640 --> 04:30:36,640
just make sure that our, actually I made some changes

5217
04:30:38,640 --> 04:30:40,640
pretty much this was

5218
04:30:40,640 --> 04:30:42,640
weird and didn't work so I just changed

5219
04:30:42,640 --> 04:30:44,640
this around to

5220
04:30:44,640 --> 04:30:46,640
making our characters empty

5221
04:30:46,640 --> 04:30:48,640
opening this text file

5222
04:30:48,640 --> 04:30:50,640
opening it

5223
04:30:50,640 --> 04:30:52,640
storing it in a variable

5224
04:30:52,640 --> 04:30:54,640
format

5225
04:30:54,640 --> 04:30:56,640
and then just making our vocab

5226
04:30:56,640 --> 04:30:58,640
this sorted list

5227
04:30:58,640 --> 04:31:00,640
set of our text

5228
04:31:00,640 --> 04:31:02,640
and then just making the vocab

5229
04:31:02,640 --> 04:31:04,640
size the length of that

5230
04:31:04,640 --> 04:31:06,640
so let's go ahead and actually run

5231
04:31:06,640 --> 04:31:08,640
this through, I did change the block size

5232
04:31:08,640 --> 04:31:10,640
to 64 batch size 128

5233
04:31:10,640 --> 04:31:12,640
some other hype parameters here

5234
04:31:12,640 --> 04:31:14,640
so

5235
04:31:14,640 --> 04:31:16,640
honestly the block size and batch size will depend

5236
04:31:16,640 --> 04:31:18,640
on your

5237
04:31:18,640 --> 04:31:20,640
computational resources

5238
04:31:20,640 --> 04:31:22,640
so

5239
04:31:22,640 --> 04:31:24,640
just experiment with these

5240
04:31:24,640 --> 04:31:26,640
I'm just going to try these out first

5241
04:31:26,640 --> 04:31:28,640
just to show you guys what this looks like

5242
04:31:32,640 --> 04:31:34,640
okay

5243
04:31:34,640 --> 04:31:36,640
so it looks like we're getting idx is not defined

5244
04:31:36,640 --> 04:31:38,640
where could that be

5245
04:31:38,640 --> 04:31:40,640
okay yep

5246
04:31:40,640 --> 04:31:42,640
so this is

5247
04:31:42,640 --> 04:31:44,640
we could just change that

5248
04:31:44,640 --> 04:31:46,640
it's just saying idx is not defined

5249
04:31:46,640 --> 04:31:48,640
we're using index here idx there so that should work now

5250
04:31:50,640 --> 04:31:52,640
and we're getting local variable

5251
04:31:52,640 --> 04:31:54,640
t reference before assignment

5252
04:31:54,640 --> 04:31:56,640
okay so

5253
04:31:56,640 --> 04:31:58,640
we have some

5254
04:31:58,640 --> 04:32:00,640
we have t here and then we initialize

5255
04:32:00,640 --> 04:32:02,640
t there so let's just bring up

5256
04:32:04,640 --> 04:32:06,640
up to there cool

5257
04:32:06,640 --> 04:32:08,640
now let's try and run this

5258
04:32:08,640 --> 04:32:10,640
oh shape is invalid

5259
04:32:10,640 --> 04:32:12,640
for input size of

5260
04:32:12,640 --> 04:32:14,640
okay let's see what we got

5261
04:32:14,640 --> 04:32:16,640
it turns out we don't actually need

5262
04:32:16,640 --> 04:32:18,640
two token embedding tables a little bit of a selling

5263
04:32:18,640 --> 04:32:20,640
mistake but we don't need two of those

5264
04:32:20,640 --> 04:32:22,640
so I'll just delete that

5265
04:32:22,640 --> 04:32:24,640
and then

5266
04:32:24,640 --> 04:32:26,640
what I'm going to do is go ahead and run this

5267
04:32:26,640 --> 04:32:28,640
again let's see a new error

5268
04:32:28,640 --> 04:32:30,640
local variable t reference before assignment

5269
04:32:30,640 --> 04:32:32,640
okay so our

5270
04:32:32,640 --> 04:32:34,640
t is referenced here

5271
04:32:34,640 --> 04:32:36,640
and well how can we initialize this

5272
04:32:36,640 --> 04:32:38,640
what we can do is we could take this index

5273
04:32:38,640 --> 04:32:40,640
here of shape

5274
04:32:40,640 --> 04:32:42,640
b by t because it goes b by t

5275
04:32:42,640 --> 04:32:44,640
plus 1 etc and just keeps growing

5276
04:32:44,640 --> 04:32:46,640
so we could actually unpack that

5277
04:32:46,640 --> 04:32:48,640
so we could go b

5278
04:32:48,640 --> 04:32:50,640
b and

5279
04:32:50,640 --> 04:32:52,640
t is going to be index

5280
04:32:52,640 --> 04:32:54,640
that shape just unpack that

5281
04:32:54,640 --> 04:32:56,640
so cool

5282
04:32:56,640 --> 04:32:58,640
so now we're going to run this training

5283
04:32:58,640 --> 04:33:00,640
loop and

5284
04:33:00,640 --> 04:33:02,640
it looks like it's working so far

5285
04:33:02,640 --> 04:33:04,640
so that's amazing

5286
04:33:04,640 --> 04:33:06,640
super cool

5287
04:33:06,640 --> 04:33:08,640
step 0 train last

5288
04:33:08,640 --> 04:33:10,640
4.4 that's actually a pretty good training

5289
04:33:10,640 --> 04:33:12,640
loss overall so

5290
04:33:12,640 --> 04:33:14,640
we'll come back after this is done

5291
04:33:14,640 --> 04:33:16,640
I've set it to train

5292
04:33:16,640 --> 04:33:18,640
for

5293
04:33:18,640 --> 04:33:20,640
3,000 iterations printing every 500 iterations

5294
04:33:20,640 --> 04:33:22,640
so we'll just see

5295
04:33:22,640 --> 04:33:24,640
the loss six times over this entire

5296
04:33:24,640 --> 04:33:26,640
training process

5297
04:33:26,640 --> 04:33:28,640
or we should

5298
04:33:28,640 --> 04:33:30,640
I don't know why it's going to 100

5299
04:33:30,640 --> 04:33:32,640
eval itters

5300
04:33:34,640 --> 04:33:36,640
eval itters

5301
04:33:36,640 --> 04:33:38,640
estimate loss is

5302
04:33:40,640 --> 04:33:42,640
okay so we don't actually need

5303
04:33:42,640 --> 04:33:44,640
eval interval

5304
04:33:46,640 --> 04:33:48,640
we'll just make this

5305
04:33:48,640 --> 04:33:50,640
sure why not 100

5306
04:33:50,640 --> 04:33:52,640
we'll keep that

5307
04:33:52,640 --> 04:33:54,640
and it's just going to keep going here

5308
04:33:54,640 --> 04:33:56,640
we'll see our loss over time

5309
04:33:56,640 --> 04:33:58,640
it's going to get smaller so

5310
04:33:58,640 --> 04:34:00,640
I'll come back when that's done

5311
04:34:00,640 --> 04:34:02,640
as for the data we're going to be using

5312
04:34:02,640 --> 04:34:04,640
the open web text corpus

5313
04:34:04,640 --> 04:34:06,640
and

5314
04:34:06,640 --> 04:34:08,640
let's just go down here

5315
04:34:08,640 --> 04:34:10,640
so this is a paper called

5316
04:34:10,640 --> 04:34:12,640
survey

5317
04:34:12,640 --> 04:34:14,640
survey of large language models

5318
04:34:14,640 --> 04:34:16,640
so I'll just go back to open

5319
04:34:16,640 --> 04:34:18,640
web

5320
04:34:18,640 --> 04:34:20,640
text

5321
04:34:20,640 --> 04:34:22,640
where that is

5322
04:34:22,640 --> 04:34:24,640
up

5323
04:34:24,640 --> 04:34:26,640
it's just fine

5324
04:34:26,640 --> 04:34:28,640
so open web text

5325
04:34:28,640 --> 04:34:30,640
this is consisted of a bunch of reddit links

5326
04:34:30,640 --> 04:34:32,640
or just reddit upvotes

5327
04:34:32,640 --> 04:34:34,640
so if you go and reddit and you see

5328
04:34:34,640 --> 04:34:36,640
a bunch of those

5329
04:34:36,640 --> 04:34:38,640
posts that are highly upvoted

5330
04:34:38,640 --> 04:34:40,640
or downvoted

5331
04:34:40,640 --> 04:34:42,640
they're pretty much those

5332
04:34:42,640 --> 04:34:44,640
pieces of text are valuable

5333
04:34:44,640 --> 04:34:46,640
and they contain things that we can train them

5334
04:34:46,640 --> 04:34:48,640
so

5335
04:34:48,640 --> 04:34:50,640
pretty much web text is

5336
04:34:50,640 --> 04:34:52,640
just a corpus of all these upvoted links

5337
04:34:52,640 --> 04:34:54,640
but it's not publicly available

5338
04:34:54,640 --> 04:34:56,640
so somebody created an open source version

5339
04:34:56,640 --> 04:34:58,640
called

5340
04:34:58,640 --> 04:35:00,640
open web text

5341
04:35:00,640 --> 04:35:02,640
hence open

5342
04:35:02,640 --> 04:35:04,640
and it's pretty much as an open version of this

5343
04:35:04,640 --> 04:35:06,640
so we're going to download that

5344
04:35:06,640 --> 04:35:08,640
for a here like common crawl which is

5345
04:35:08,640 --> 04:35:10,640
really really big so like

5346
04:35:10,640 --> 04:35:12,640
petabyte scale data volume

5347
04:35:12,640 --> 04:35:14,640
you have a bunch of books

5348
04:35:14,640 --> 04:35:16,640
so this is a good paper to read over

5349
04:35:16,640 --> 04:35:18,640
it's just called

5350
04:35:18,640 --> 04:35:20,640
a survey

5351
04:35:20,640 --> 04:35:22,640
of large language models you can search this up

5352
04:35:22,640 --> 04:35:24,640
and it'll come up you can just download the pdf for

5353
04:35:24,640 --> 04:35:26,640
so this is a really nice paper

5354
04:35:26,640 --> 04:35:28,640
read over that if you'd like

5355
04:35:28,640 --> 04:35:30,640
but anyways

5356
04:35:30,640 --> 04:35:32,640
this is a download link for this open web text corpus

5357
04:35:32,640 --> 04:35:34,640
so just go to this link

5358
04:35:34,640 --> 04:35:36,640
I have it in the github repo

5359
04:35:36,640 --> 04:35:38,640
and you just go to download

5360
04:35:38,640 --> 04:35:40,640
and it'll bring you to this drive

5361
04:35:40,640 --> 04:35:42,640
so you can go in and right click this

5362
04:35:42,640 --> 04:35:44,640
and just hit download

5363
04:35:44,640 --> 04:35:46,640
it'll say 12 gigabytes exceeds

5364
04:35:46,640 --> 04:35:48,640
maximum file size that it can scan so it's like

5365
04:35:48,640 --> 04:35:50,640
this might have a virus

5366
04:35:50,640 --> 04:35:52,640
don't worry it doesn't have a virus this is actually

5367
04:35:52,640 --> 04:35:54,640
created by a researcher so

5368
04:35:54,640 --> 04:35:56,640
not really bad people are in charge of

5369
04:35:56,640 --> 04:35:58,640
creating text corpora

5370
04:35:58,640 --> 04:36:00,640
so go in and download anyway

5371
04:36:00,640 --> 04:36:02,640
I've actually already downloaded this

5372
04:36:02,640 --> 04:36:04,640
so

5373
04:36:04,640 --> 04:36:06,640
yeah I'll come back

5374
04:36:06,640 --> 04:36:08,640
when our training

5375
04:36:08,640 --> 04:36:10,640
is actually done here

5376
04:36:10,640 --> 04:36:12,640
so I'm actually going to stop here iteration

5377
04:36:12,640 --> 04:36:14,640
2000 because

5378
04:36:14,640 --> 04:36:16,640
we're not actually getting that much amazing progress

5379
04:36:16,640 --> 04:36:18,640
and the reason for this is because

5380
04:36:18,640 --> 04:36:20,640
our hyper parameters

5381
04:36:20,640 --> 04:36:22,640
so batch size and block size

5382
04:36:22,640 --> 04:36:24,640
I mean these are okay

5383
04:36:24,640 --> 04:36:26,640
but we might want to change up as our learning rate

5384
04:36:26,640 --> 04:36:28,640
so some combinations of learning rates that are really useful

5385
04:36:28,640 --> 04:36:30,640
is like

5386
04:36:30,640 --> 04:36:32,640
3e to the negative 3

5387
04:36:32,640 --> 04:36:34,640
you go 3e to the

5388
04:36:34,640 --> 04:36:36,640
negative 4

5389
04:36:36,640 --> 04:36:38,640
you go 1e to the negative 3

5390
04:36:38,640 --> 04:36:40,640
1e

5391
04:36:40,640 --> 04:36:42,640
1e to the negative 4

5392
04:36:42,640 --> 04:36:44,640
so these are all learning rates that I like to play around with

5393
04:36:44,640 --> 04:36:46,640
these are just sort of common ones

5394
04:36:46,640 --> 04:36:48,640
it's up to you if you want to use them or not but

5395
04:36:48,640 --> 04:36:50,640
what I might do actually

5396
04:36:50,640 --> 04:36:52,640
is just downgrade to 3e to the negative 4

5397
04:36:52,640 --> 04:36:54,640
and we'll retest it

5398
04:36:54,640 --> 04:36:56,640
as well I'm going to bump up the

5399
04:36:56,640 --> 04:36:58,640
the number of heads

5400
04:36:58,640 --> 04:37:00,640
and the number of layers

5401
04:37:00,640 --> 04:37:02,640
so that we can capture more

5402
04:37:02,640 --> 04:37:04,640
complex relationships in the text

5403
04:37:04,640 --> 04:37:06,640
thus having it learn more

5404
04:37:06,640 --> 04:37:08,640
so I'm going to change each of these

5405
04:37:08,640 --> 04:37:10,640
to 8

5406
04:37:10,640 --> 04:37:12,640
go 8

5407
04:37:12,640 --> 04:37:14,640
actually

5408
04:37:14,640 --> 04:37:16,640
kernel will go

5409
04:37:16,640 --> 04:37:18,640
restart

5410
04:37:20,640 --> 04:37:22,640
now we'll just run this from the top

5411
04:37:28,640 --> 04:37:30,640
and

5412
04:37:32,640 --> 04:37:34,640
and we'll run that

5413
04:37:34,640 --> 04:37:36,640
cool

5414
04:37:36,640 --> 04:37:38,640
so let's see

5415
04:37:38,640 --> 04:37:40,640
what we actually start off with and what our loss looks like over time

5416
04:37:50,640 --> 04:37:52,640
cool

5417
04:37:52,640 --> 04:37:54,640
so we got step 1 4.5 about the same as last time

5418
04:37:54,640 --> 04:37:56,640
it's like 0.2 off

5419
04:37:56,640 --> 04:37:58,640
or something so it's pretty close

5420
04:37:58,640 --> 04:38:00,640
let's see the next iteration here

5421
04:38:26,640 --> 04:38:28,640
that's wonderful

5422
04:38:28,640 --> 04:38:30,640
so before we were getting like 3.1 ish

5423
04:38:30,640 --> 04:38:32,640
or something around that range 3.15

5424
04:38:32,640 --> 04:38:34,640
now we're getting 2.2

5425
04:38:34,640 --> 04:38:36,640
so you can see that

5426
04:38:36,640 --> 04:38:38,640
as we change hyper parameters

5427
04:38:38,640 --> 04:38:40,640
we can actually see a significant change

5428
04:38:40,640 --> 04:38:42,640
in our loss

5429
04:38:42,640 --> 04:38:44,640
this is amazing

5430
04:38:44,640 --> 04:38:46,640
this is just to sort of prove how cool hyper

5431
04:38:46,640 --> 04:38:48,640
parameters are and what they do for you

5432
04:38:48,640 --> 04:38:50,640
so

5433
04:38:50,640 --> 04:38:52,640
let's start

5434
04:38:52,640 --> 04:38:54,640
changing around some data stuff

5435
04:38:54,640 --> 04:38:56,640
this right here is the Wizard of Oz text

5436
04:38:56,640 --> 04:38:58,640
just a simple text file

5437
04:38:58,640 --> 04:39:00,640
it's the size isn't

5438
04:39:00,640 --> 04:39:02,640
super large

5439
04:39:02,640 --> 04:39:04,640
so we can actually open it all into ram at once

5440
04:39:04,640 --> 04:39:06,640
but

5441
04:39:06,640 --> 04:39:08,640
if we were to use the open web text

5442
04:39:08,640 --> 04:39:10,640
we cannot actually read

5443
04:39:10,640 --> 04:39:12,640
you know 45 gigabytes of

5444
04:39:12,640 --> 04:39:14,640
utfa text in ram at once

5445
04:39:14,640 --> 04:39:16,640
just can't do that unless you have like maybe

5446
04:39:16,640 --> 04:39:18,640
64 or 128 gigabytes

5447
04:39:18,640 --> 04:39:20,640
of ram this is really just

5448
04:39:20,640 --> 04:39:22,640
not feasible at all

5449
04:39:22,640 --> 04:39:24,640
so

5450
04:39:24,640 --> 04:39:26,640
we're going to do some data pre-processing

5451
04:39:26,640 --> 04:39:28,640
here some data cleaning

5452
04:39:28,640 --> 04:39:30,640
and then just a way to simply load

5453
04:39:30,640 --> 04:39:32,640
data into the

5454
04:39:32,640 --> 04:39:34,640
GPT so let's go ahead and do that

5455
04:39:34,640 --> 04:39:36,640
so the model has actually gotten really good at predicting

5456
04:39:36,640 --> 04:39:38,640
the next token as you can see

5457
04:39:38,640 --> 04:39:40,640
the train loss here is 1.01

5458
04:39:40,640 --> 04:39:42,640
so let's actually

5459
04:39:42,640 --> 04:39:44,640
find

5460
04:39:44,640 --> 04:39:46,640
what the prediction accuracy of that is

5461
04:39:46,640 --> 04:39:48,640
so I might just go into GPT-4

5462
04:39:48,640 --> 04:39:50,640
here

5463
04:39:50,640 --> 04:39:52,640
and

5464
04:39:52,640 --> 04:39:54,640
just ask it

5465
04:39:54,640 --> 04:39:56,640
what is

5466
04:39:56,640 --> 04:39:58,640
the prediction

5467
04:39:58,640 --> 04:40:00,640
accuracy

5468
04:40:00,640 --> 04:40:02,640
of

5469
04:40:02,640 --> 04:40:04,640
loss 1.01

5470
04:40:06,640 --> 04:40:08,640
the loss value

5471
04:40:08,640 --> 04:40:10,640
comes with a loss function during the pre-process

5472
04:40:10,640 --> 04:40:12,640
okay so let's

5473
04:40:12,640 --> 04:40:14,640
see

5474
04:40:16,640 --> 04:40:18,640
cross entropy loss

5475
04:40:18,640 --> 04:40:20,640
doesn't mean the model is 99% accurate

5476
04:40:22,640 --> 04:40:24,640
okay

5477
04:40:24,640 --> 04:40:26,640
so

5478
04:40:26,640 --> 04:40:28,640
that pretty much means that the model is really accurate

5479
04:40:28,640 --> 04:40:30,640
but I want to find a value here

5480
04:40:30,640 --> 04:40:32,640
so

5481
04:40:32,640 --> 04:40:34,640
if the

5482
04:40:34,640 --> 04:40:36,640
we'll go to Wolfram alpha

5483
04:40:38,640 --> 04:40:40,640
and just we'll just guess some values here

5484
04:40:40,640 --> 04:40:42,640
so negative ln

5485
04:40:42,640 --> 04:40:44,640
of let's say

5486
04:40:44,640 --> 04:40:46,640
0.9

5487
04:40:46,640 --> 04:40:48,640
okay so probably not that

5488
04:40:50,640 --> 04:40:52,640
0.3

5489
04:40:52,640 --> 04:40:54,640
0.2

5490
04:40:54,640 --> 04:40:56,640
0.4

5491
04:40:56,640 --> 04:40:58,640
0.35

5492
04:40:58,640 --> 04:41:00,640
yep so the model

5493
04:41:00,640 --> 04:41:02,640
has about a 35% chance

5494
04:41:02,640 --> 04:41:04,640
of guessing the next token as of right now

5495
04:41:04,640 --> 04:41:06,640
so that's actually pretty good

5496
04:41:06,640 --> 04:41:08,640
so 1 in every 3 tokens

5497
04:41:08,640 --> 04:41:10,640
are spot on

5498
04:41:10,640 --> 04:41:12,640
so that is wonderful

5499
04:41:12,640 --> 04:41:14,640
this is converging even more

5500
04:41:14,640 --> 04:41:16,640
we're getting 0.89 so now it's getting like

5501
04:41:16,640 --> 04:41:18,640
every

5502
04:41:18,640 --> 04:41:20,640
40% are being guessed properly

5503
04:41:20,640 --> 04:41:22,640
our validation is not doing

5504
04:41:22,640 --> 04:41:24,640
amazing though

5505
04:41:24,640 --> 04:41:26,640
but we'll linger on that a little bit here

5506
04:41:26,640 --> 04:41:28,640
and you'll see sort of how this changes

5507
04:41:28,640 --> 04:41:30,640
as we scale our data

5508
04:41:30,640 --> 04:41:32,640
but

5509
04:41:32,640 --> 04:41:34,640
so I've installed this

5510
04:41:34,640 --> 04:41:36,640
webtext.tar file

5511
04:41:36,640 --> 04:41:38,640
tar file is interesting

5512
04:41:38,640 --> 04:41:40,640
so in order to actually extract these

5513
04:41:40,640 --> 04:41:42,640
you simply just

5514
04:41:42,640 --> 04:41:44,640
right click on them

5515
04:41:44,640 --> 04:41:46,640
you go extract to

5516
04:41:46,640 --> 04:41:48,640
and then it'll just make a new file here

5517
04:41:48,640 --> 04:41:50,640
so it'll process this

5518
04:41:50,640 --> 04:41:52,640
you have to make sure you have WinRAR or else this might not work

5519
04:41:52,640 --> 04:41:54,640
to the fullest extent

5520
04:41:54,640 --> 04:41:56,640
and yeah

5521
04:41:56,640 --> 04:41:58,640
so we'll just wait for this to finish up here

5522
04:41:58,640 --> 04:42:00,640
we should end up with something that looks like this

5523
04:42:00,640 --> 04:42:02,640
so open webtext

5524
04:42:02,640 --> 04:42:04,640
and inside of here

5525
04:42:04,640 --> 04:42:06,640
you have a bunch of xz files

5526
04:42:06,640 --> 04:42:08,640
cool so there's actually 20,000

5527
04:42:08,640 --> 04:42:10,640
of these so we're gonna have to do a lot of

5528
04:42:10,640 --> 04:42:12,640
there's definitely gonna be some for loops in here for sure

5529
04:42:12,640 --> 04:42:14,640
so

5530
04:42:14,640 --> 04:42:16,640
let's just handle this

5531
04:42:16,640 --> 04:42:18,640
step by step in this data

5532
04:42:18,640 --> 04:42:20,640
extract file

5533
04:42:20,640 --> 04:42:22,640
so first off

5534
04:42:22,640 --> 04:42:24,640
we're gonna need to import some python modules

5535
04:42:24,640 --> 04:42:26,640
we're gonna use OS for interacting with the operating system

5536
04:42:26,640 --> 04:42:28,640
LZMA

5537
04:42:28,640 --> 04:42:30,640
for handling

5538
04:42:30,640 --> 04:42:32,640
xz files which are a type of compressed file

5539
04:42:32,640 --> 04:42:34,640
like 7zip for example

5540
04:42:34,640 --> 04:42:36,640
and then

5541
04:42:36,640 --> 04:42:38,640
TQDM for displaying a progress bar

5542
04:42:38,640 --> 04:42:40,640
so you see a progress bar left to right

5543
04:42:40,640 --> 04:42:42,640
in the terminal

5544
04:42:42,640 --> 04:42:44,640
and that's pretty much gonna show us how quick we are

5545
04:42:44,640 --> 04:42:46,640
at executing the script

5546
04:42:46,640 --> 04:42:48,640
so next up

5547
04:42:48,640 --> 04:42:50,640
we're gonna define a function

5548
04:42:50,640 --> 04:42:52,640
called xz files in dir

5549
04:42:52,640 --> 04:42:54,640
it takes a directory as an input

5550
04:42:54,640 --> 04:42:56,640
returns a list of all of the xz file names

5551
04:42:56,640 --> 04:42:58,640
in that directory

5552
04:42:58,640 --> 04:43:00,640
it's gonna use os.listdir

5553
04:43:00,640 --> 04:43:02,640
to get all the file names

5554
04:43:02,640 --> 04:43:04,640
and os

5555
04:43:04,640 --> 04:43:06,640
path as file

5556
04:43:06,640 --> 04:43:08,640
to check if each one is a file

5557
04:43:08,640 --> 04:43:10,640
and not a directory or

5558
04:43:10,640 --> 04:43:12,640
symbolic link

5559
04:43:12,640 --> 04:43:14,640
if a file name ends with .xz

5560
04:43:14,640 --> 04:43:16,640
and it's a file

5561
04:43:16,640 --> 04:43:18,640
it'll be added to the list

5562
04:43:18,640 --> 04:43:20,640
so we just have a bunch of these files

5563
04:43:20,640 --> 04:43:22,640
each element

5564
04:43:22,640 --> 04:43:24,640
is just the title of each file in there

5565
04:43:24,640 --> 04:43:26,640
so that's pretty much what that does

5566
04:43:26,640 --> 04:43:28,640
and next up here

5567
04:43:28,640 --> 04:43:30,640
we'll set up some variables

5568
04:43:30,640 --> 04:43:32,640
folder path

5569
04:43:32,640 --> 04:43:34,640
it's just gonna be where our xz files are located

5570
04:43:34,640 --> 04:43:36,640
so I'm actually gonna change this here

5571
04:43:36,640 --> 04:43:38,640
because that's an incorrect file path

5572
04:43:38,640 --> 04:43:40,640
but

5573
04:43:42,640 --> 04:43:44,640
yes

5574
04:43:44,640 --> 04:43:46,640
just like that

5575
04:43:48,640 --> 04:43:50,640
you have to make sure that these

5576
04:43:50,640 --> 04:43:52,640
slashes are actually forward slashes

5577
04:43:52,640 --> 04:43:54,640
or else you might get bytecode errors

5578
04:43:54,640 --> 04:43:56,640
so when it actually tries to read the string

5579
04:43:56,640 --> 04:43:58,640
it doesn't think that

5580
04:43:58,640 --> 04:44:00,640
these are separated

5581
04:44:00,640 --> 04:44:02,640
the backward slashes do weird things

5582
04:44:02,640 --> 04:44:04,640
so you could either do

5583
04:44:04,640 --> 04:44:06,640
a one forward slash

5584
04:44:06,640 --> 04:44:08,640
or two backward slashes

5585
04:44:08,640 --> 04:44:10,640
that should work

5586
04:44:10,640 --> 04:44:12,640
just make sure you get forward slashes

5587
04:44:12,640 --> 04:44:14,640
and you should be good

5588
04:44:14,640 --> 04:44:16,640
so folder path is where all these files are located

5589
04:44:16,640 --> 04:44:18,640
all these xz files are located as you saw

5590
04:44:18,640 --> 04:44:20,640
output file

5591
04:44:20,640 --> 04:44:22,640
is the pattern for output file names

5592
04:44:22,640 --> 04:44:24,640
in case we want to have more than one of them

5593
04:44:24,640 --> 04:44:26,640
so if you want to have 200 output files

5594
04:44:26,640 --> 04:44:28,640
instead of one then it'll just be like

5595
04:44:28,640 --> 04:44:30,640
output 0, output 1, output 2 etc

5596
04:44:30,640 --> 04:44:32,640
and then vocab file is where we want to save

5597
04:44:32,640 --> 04:44:34,640
our vocabulary

5598
04:44:34,640 --> 04:44:36,640
keep in mind in this giant corpus

5599
04:44:36,640 --> 04:44:38,640
you can't push it on to ram at once

5600
04:44:38,640 --> 04:44:40,640
so what we're gonna do is as we're

5601
04:44:40,640 --> 04:44:42,640
reading these little compressed files

5602
04:44:42,640 --> 04:44:44,640
20,000 of them

5603
04:44:44,640 --> 04:44:46,640
we're gonna take all of the new characters

5604
04:44:46,640 --> 04:44:48,640
from them and just push them into some vocab file

5605
04:44:48,640 --> 04:44:50,640
containing all of the different

5606
04:44:50,640 --> 04:44:52,640
characters that we have

5607
04:44:52,640 --> 04:44:54,640
so that way we can handle this later

5608
04:44:54,640 --> 04:44:56,640
and just pretty much sort it into some

5609
04:44:56,640 --> 04:44:58,640
list containing all of our vocabulary

5610
04:44:58,640 --> 04:45:00,640
split files

5611
04:45:00,640 --> 04:45:02,640
how many files do we want to split this into

5612
04:45:02,640 --> 04:45:04,640
so pretty much this

5613
04:45:04,640 --> 04:45:06,640
it ties back to output file

5614
04:45:06,640 --> 04:45:08,640
and just these curly braces here

5615
04:45:08,640 --> 04:45:10,640
how many do we want to have

5616
04:45:10,640 --> 04:45:12,640
if we want to have more than one then we would

5617
04:45:12,640 --> 04:45:14,640
this would take effect

5618
04:45:14,640 --> 04:45:16,640
so cool

5619
04:45:16,640 --> 04:45:18,640
now we'll use our

5620
04:45:18,640 --> 04:45:20,640
x files in dir

5621
04:45:20,640 --> 04:45:22,640
to get a list of file names and store them in this variable

5622
04:45:22,640 --> 04:45:24,640
we'll count

5623
04:45:24,640 --> 04:45:26,640
the number of

5624
04:45:26,640 --> 04:45:28,640
total xd files

5625
04:45:28,640 --> 04:45:30,640
simply the length of our file names

5626
04:45:30,640 --> 04:45:32,640
now in here

5627
04:45:32,640 --> 04:45:34,640
we'll calculate the number of files

5628
04:45:34,640 --> 04:45:36,640
to process for each output file

5629
04:45:36,640 --> 04:45:38,640
if the user is requested

5630
04:45:38,640 --> 04:45:40,640
more than one output file

5631
04:45:40,640 --> 04:45:42,640
request more than one output file

5632
04:45:42,640 --> 04:45:44,640
this is the total number of

5633
04:45:44,640 --> 04:45:46,640
files divided by the number

5634
04:45:46,640 --> 04:45:48,640
output files rounded down

5635
04:45:48,640 --> 04:45:50,640
so

5636
04:45:50,640 --> 04:45:52,640
if the user only wants one

5637
04:45:52,640 --> 04:45:54,640
output file max count is the same as total files

5638
04:45:54,640 --> 04:45:56,640
and

5639
04:45:56,640 --> 04:45:58,640
that's how that works

5640
04:45:58,640 --> 04:46:00,640
so

5641
04:46:00,640 --> 04:46:02,640
next up we'll just create a

5642
04:46:02,640 --> 04:46:04,640
set to store a vocabulary when we

5643
04:46:04,640 --> 04:46:06,640
start appending these new characters into it

5644
04:46:06,640 --> 04:46:08,640
a set is a

5645
04:46:08,640 --> 04:46:10,640
collection of unique items in case you did not know

5646
04:46:10,640 --> 04:46:12,640
entirely what a set was

5647
04:46:14,640 --> 04:46:16,640
now

5648
04:46:16,640 --> 04:46:18,640
this is where it gets interesting

5649
04:46:18,640 --> 04:46:20,640
we're ready to process our

5650
04:46:20,640 --> 04:46:22,640
.xz files

5651
04:46:22,640 --> 04:46:24,640
for each output file we'll process

5652
04:46:24,640 --> 04:46:26,640
max count files

5653
04:46:26,640 --> 04:46:28,640
for each file we'll open it

5654
04:46:28,640 --> 04:46:30,640
read its contents

5655
04:46:30,640 --> 04:46:32,640
and write the contents to the current output file

5656
04:46:32,640 --> 04:46:34,640
and then add any unique characters to our vocabulary

5657
04:46:34,640 --> 04:46:36,640
set

5658
04:46:36,640 --> 04:46:38,640
after processing max count files

5659
04:46:38,640 --> 04:46:40,640
remove them from our list of files

5660
04:46:40,640 --> 04:46:42,640
and then finally

5661
04:46:48,640 --> 04:46:50,640
we'll write all of our vocabulary to this file

5662
04:46:50,640 --> 04:46:52,640
so

5663
04:46:52,640 --> 04:46:54,640
we pretty much just open

5664
04:46:54,640 --> 04:46:56,640
we just

5665
04:46:56,640 --> 04:46:58,640
write all of these characters in the vocab

5666
04:46:58,640 --> 04:47:00,640
to this

5667
04:47:00,640 --> 04:47:02,640
vocab file which is here vocab.txt

5668
04:47:02,640 --> 04:47:04,640
so awesome

5669
04:47:04,640 --> 04:47:06,640
now

5670
04:47:06,640 --> 04:47:08,640
honestly we could just go ahead

5671
04:47:08,640 --> 04:47:10,640
and run this

5672
04:47:10,640 --> 04:47:12,640
so let's go ahead and go in here

5673
04:47:12,640 --> 04:47:14,640
I'm going to go cls to clear that

5674
04:47:14,640 --> 04:47:16,640
we'll go python

5675
04:47:16,640 --> 04:47:18,640
data extract

5676
04:47:18,640 --> 04:47:20,640
.py

5677
04:47:20,640 --> 04:47:22,640
let's see this work it's magic

5678
04:47:24,640 --> 04:47:26,640
how many files would you like to split this into

5679
04:47:26,640 --> 04:47:28,640
we'll go one

5680
04:47:28,640 --> 04:47:30,640
then we get a progress bar

5681
04:47:30,640 --> 04:47:32,640
20,000 files and we'll just let that load

5682
04:47:32,640 --> 04:47:34,640
I'll come back to you in

5683
04:47:34,640 --> 04:47:36,640
about 30 minutes to check up on this

5684
04:47:36,640 --> 04:47:38,640
okay so there's not a little

5685
04:47:38,640 --> 04:47:40,640
one thing we want to consider for

5686
04:47:40,640 --> 04:47:42,640
and

5687
04:47:42,640 --> 04:47:44,640
it's actually quite important is our splits

5688
04:47:44,640 --> 04:47:46,640
for train and file splits

5689
04:47:46,640 --> 04:47:48,640
it would be really inefficient

5690
04:47:48,640 --> 04:47:50,640
to just get blocks and then creating

5691
04:47:50,640 --> 04:47:52,640
train and file splits as we go

5692
04:47:52,640 --> 04:47:54,640
every new batch we get

5693
04:47:54,640 --> 04:47:56,640
so in turn

5694
04:47:56,640 --> 04:47:58,640
what we might be better off doing

5695
04:47:58,640 --> 04:48:00,640
is just creating an

5696
04:48:00,640 --> 04:48:02,640
output train file and an output file file

5697
04:48:02,640 --> 04:48:04,640
so just two of them instead of one

5698
04:48:04,640 --> 04:48:06,640
train is 90% of our data

5699
04:48:06,640 --> 04:48:08,640
file is 10% of our data

5700
04:48:08,640 --> 04:48:10,640
if that makes sense

5701
04:48:10,640 --> 04:48:12,640
so pretty much what I did

5702
04:48:12,640 --> 04:48:14,640
is I got the output line for how many

5703
04:48:14,640 --> 04:48:16,640
files do you want

5704
04:48:16,640 --> 04:48:18,640
so you can see I got quite a bit of files

5705
04:48:18,640 --> 04:48:20,640
produced here

5706
04:48:20,640 --> 04:48:22,640
by not doing that correctly

5707
04:48:22,640 --> 04:48:24,640
so don't do that

5708
04:48:24,640 --> 04:48:26,640
and

5709
04:48:26,640 --> 04:48:28,640
yeah

5710
04:48:28,640 --> 04:48:30,640
essentially we're just

5711
04:48:30,640 --> 04:48:32,640
we're pretty much just doing that

5712
04:48:32,640 --> 04:48:34,640
so we're processing some training files

5713
04:48:34,640 --> 04:48:36,640
we're separating 90%

5714
04:48:36,640 --> 04:48:38,640
of the names on the left side

5715
04:48:38,640 --> 04:48:40,640
and then 10% of the names

5716
04:48:40,640 --> 04:48:42,640
we're just separating those in the two different

5717
04:48:42,640 --> 04:48:44,640
arrays, file names

5718
04:48:44,640 --> 04:48:46,640
and then we're just processing each of those

5719
04:48:46,640 --> 04:48:48,640
arrays based on the file names

5720
04:48:48,640 --> 04:48:50,640
so I took away that little bit

5721
04:48:50,640 --> 04:48:52,640
that was asking

5722
04:48:52,640 --> 04:48:54,640
how many files per

5723
04:48:54,640 --> 04:48:56,640
split do you want

5724
04:48:56,640 --> 04:48:58,640
so I took that away

5725
04:48:58,640 --> 04:49:00,640
and this is effectively the same code

5726
04:49:00,640 --> 04:49:02,640
just a little bit of tweaks

5727
04:49:02,640 --> 04:49:04,640
and yeah

5728
04:49:04,640 --> 04:49:06,640
so I'm going to go ahead and run this

5729
04:49:06,640 --> 04:49:08,640
data extract

5730
04:49:08,640 --> 04:49:10,640
cool

5731
04:49:10,640 --> 04:49:12,640
so we got an output train

5732
04:49:12,640 --> 04:49:14,640
and then after this it's going to do

5733
04:49:14,640 --> 04:49:16,640
the output validation set

5734
04:49:16,640 --> 04:49:18,640
so I'll come back after this is done

5735
04:49:18,640 --> 04:49:20,640
so awesome I have just downloaded

5736
04:49:20,640 --> 04:49:22,640
both or I've both

5737
04:49:22,640 --> 04:49:24,640
got both these splits

5738
04:49:24,640 --> 04:49:26,640
output train and val train so just to

5739
04:49:26,640 --> 04:49:28,640
confirm that they're

5740
04:49:28,640 --> 04:49:30,640
actually the right size got 38.9

5741
04:49:30,640 --> 04:49:32,640
and then 4.27

5742
04:49:32,640 --> 04:49:34,640
so if we do this divided by

5743
04:49:34,640 --> 04:49:36,640
9 so about 30

5744
04:49:36,640 --> 04:49:38,640
8.9 divided by 9

5745
04:49:38,640 --> 04:49:40,640
4.32 and it's

5746
04:49:40,640 --> 04:49:42,640
pretty close to 4.27 so

5747
04:49:42,640 --> 04:49:44,640
we can confirm that these are pretty much

5748
04:49:44,640 --> 04:49:46,640
the

5749
04:49:46,640 --> 04:49:48,640
length that we expect them to be

5750
04:49:48,640 --> 04:49:50,640
so awesome we have this vocab.txt

5751
04:49:50,640 --> 04:49:52,640
file wonderful so now

5752
04:49:52,640 --> 04:49:54,640
we have to focus on is

5753
04:49:54,640 --> 04:49:56,640
getting this into

5754
04:49:56,640 --> 04:49:58,640
our batches so when we call

5755
04:49:58,640 --> 04:50:00,640
our get batch function actually

5756
04:50:00,640 --> 04:50:02,640
cd out of this open this in

5757
04:50:02,640 --> 04:50:04,640
a Jupyter notebook

5758
04:50:06,640 --> 04:50:08,640
copy my desktop

5759
04:50:10,640 --> 04:50:12,640
paste it over here

5760
04:50:12,640 --> 04:50:14,640
and perfect

5761
04:50:14,640 --> 04:50:16,640
so

5762
04:50:16,640 --> 04:50:18,640
this open when web text folder with

5763
04:50:18,640 --> 04:50:20,640
these files awesome

5764
04:50:20,640 --> 04:50:22,640
and our GPTV

5765
04:50:22,640 --> 04:50:24,640
one

5766
04:50:24,640 --> 04:50:26,640
so

5767
04:50:26,640 --> 04:50:28,640
this get batch function

5768
04:50:28,640 --> 04:50:30,640
is going to have to

5769
04:50:30,640 --> 04:50:32,640
change also these

5770
04:50:32,640 --> 04:50:34,640
are going to have to change as well

5771
04:50:34,640 --> 04:50:36,640
and this one too these are probably

5772
04:50:36,640 --> 04:50:38,640
not going to be here

5773
04:50:38,640 --> 04:50:40,640
but pretty much

5774
04:50:40,640 --> 04:50:42,640
let's go ahead and first of all

5775
04:50:42,640 --> 04:50:44,640
get this vocab.txt

5776
04:50:44,640 --> 04:50:46,640
in so what I'm going to do

5777
04:50:46,640 --> 04:50:48,640
I'm just going to go

5778
04:50:48,640 --> 04:50:50,640
we're going to go

5779
04:50:50,640 --> 04:50:52,640
open web text slash

5780
04:50:52,640 --> 04:50:54,640
vocab.txt

5781
04:50:54,640 --> 04:50:56,640
cool so that's our vocab

5782
04:50:56,640 --> 04:50:58,640
right there text read

5783
04:50:58,640 --> 04:51:00,640
vocab size the length of that nice

5784
04:51:00,640 --> 04:51:02,640
so that's what our vocab is

5785
04:51:02,640 --> 04:51:04,640
and then

5786
04:51:04,640 --> 04:51:06,640
what we're going to do next

5787
04:51:06,640 --> 04:51:08,640
is change this get batch function

5788
04:51:08,640 --> 04:51:10,640
around

5789
04:51:10,640 --> 04:51:12,640
so first of all I'm going to go ahead

5790
04:51:12,640 --> 04:51:14,640
and get rid of this here

5791
04:51:14,640 --> 04:51:16,640
and then

5792
04:51:16,640 --> 04:51:18,640
I've actually produced

5793
04:51:18,640 --> 04:51:20,640
some code specifically for

5794
04:51:20,640 --> 04:51:22,640
this so I'm just going to go back

5795
04:51:22,640 --> 04:51:24,640
to my

5796
04:51:24,640 --> 04:51:26,640
I'm just going to find

5797
04:51:26,640 --> 04:51:28,640
this folder

5798
04:51:28,640 --> 04:51:30,640
okay so I've

5799
04:51:30,640 --> 04:51:32,640
actually produced some

5800
04:51:32,640 --> 04:51:34,640
code here

5801
04:51:34,640 --> 04:51:36,640
I produced this off camera

5802
04:51:36,640 --> 04:51:38,640
but

5803
04:51:38,640 --> 04:51:40,640
pretty much what this is going to do

5804
04:51:40,640 --> 04:51:42,640
it's going to let us call a split

5805
04:51:42,640 --> 04:51:44,640
okay so we have our get batch

5806
04:51:44,640 --> 04:51:46,640
function all of this down here is the

5807
04:51:46,640 --> 04:51:48,640
same as our GPTV

5808
04:51:48,640 --> 04:51:50,640
one file and then

5809
04:51:50,640 --> 04:51:52,640
this data is

5810
04:51:52,640 --> 04:51:54,640
just going to get a random chunk of text

5811
04:51:54,640 --> 04:51:56,640
with giant block of text

5812
04:51:56,640 --> 04:51:58,640
and the way

5813
04:51:58,640 --> 04:52:00,640
that we get it is actually pretty interesting

5814
04:52:00,640 --> 04:52:02,640
so the way that we get this text is

5815
04:52:02,640 --> 04:52:04,640
something called memory mapping

5816
04:52:04,640 --> 04:52:06,640
so memory mapping is a way

5817
04:52:06,640 --> 04:52:08,640
to look at disk files

5818
04:52:08,640 --> 04:52:10,640
or to open them and look at pieces of them

5819
04:52:10,640 --> 04:52:12,640
without opening the entire thing at once

5820
04:52:12,640 --> 04:52:14,640
so memory mapping

5821
04:52:14,640 --> 04:52:16,640
I'm not a hardware guy so I can't

5822
04:52:16,640 --> 04:52:18,640
really talk about that

5823
04:52:18,640 --> 04:52:20,640
memory mapping is pretty

5824
04:52:20,640 --> 04:52:22,640
cool and allows us to look at little

5825
04:52:22,640 --> 04:52:24,640
chunks at a time in very large text files

5826
04:52:24,640 --> 04:52:26,640
so that's essentially what we're doing here

5827
04:52:26,640 --> 04:52:28,640
we're passing this split

5828
04:52:28,640 --> 04:52:30,640
split

5829
04:52:30,640 --> 04:52:32,640
file name is equal to train split

5830
04:52:32,640 --> 04:52:34,640
this is just an example text file

5831
04:52:36,640 --> 04:52:38,640
if the split is equal to train then this

5832
04:52:38,640 --> 04:52:40,640
is our file name else

5833
04:52:40,640 --> 04:52:42,640
file split and then we're going to

5834
04:52:42,640 --> 04:52:44,640
open this file name in binary mode

5835
04:52:44,640 --> 04:52:46,640
this has to be in binary mode

5836
04:52:46,640 --> 04:52:48,640
it's also a lot more efficient in binary

5837
04:52:48,640 --> 04:52:50,640
mode and then

5838
04:52:50,640 --> 04:52:52,640
we're going to open this with a mem map

5839
04:52:52,640 --> 04:52:54,640
so I don't expect you to memorize all the mem map syntax

5840
04:52:54,640 --> 04:52:56,640
you can look at the docs if you would like

5841
04:52:56,640 --> 04:52:58,640
but I'm just going to explain

5842
04:52:58,640 --> 04:53:00,640
logically what's happening

5843
04:53:00,640 --> 04:53:02,640
so we're going to open this

5844
04:53:02,640 --> 04:53:04,640
with the mem map library

5845
04:53:04,640 --> 04:53:06,640
and we're going to open this as

5846
04:53:06,640 --> 04:53:08,640
mm so

5847
04:53:08,640 --> 04:53:10,640
the file size is literally

5848
04:53:10,640 --> 04:53:12,640
just the length of it so determining

5849
04:53:12,640 --> 04:53:14,640
the file size and

5850
04:53:14,640 --> 04:53:16,640
all we're doing from this point is we're just finding

5851
04:53:16,640 --> 04:53:18,640
a position so we're using the random library

5852
04:53:18,640 --> 04:53:20,640
and we're finding

5853
04:53:20,640 --> 04:53:22,640
a position between

5854
04:53:22,640 --> 04:53:24,640
0

5855
04:53:24,640 --> 04:53:26,640
and the file size

5856
04:53:26,640 --> 04:53:28,640
minus block size times batch size

5857
04:53:28,640 --> 04:53:30,640
so pretty much we have this

5858
04:53:30,640 --> 04:53:32,640
giant text

5859
04:53:32,640 --> 04:53:34,640
file we could either

5860
04:53:34,640 --> 04:53:36,640
what we want to do is we want to start

5861
04:53:36,640 --> 04:53:38,640
from 0 and go up to like

5862
04:53:38,640 --> 04:53:40,640
just before the end because if we

5863
04:53:40,640 --> 04:53:42,640
actually sample

5864
04:53:42,640 --> 04:53:44,640
that last piece then it's still

5865
04:53:44,640 --> 04:53:46,640
going to have some wiggle room to

5866
04:53:46,640 --> 04:53:48,640
reach further into the file

5867
04:53:48,640 --> 04:53:50,640
if we just made it from like

5868
04:53:50,640 --> 04:53:52,640
the first

5869
04:53:52,640 --> 04:53:54,640
the very start of the file to the very end

5870
04:53:54,640 --> 04:53:56,640
then it would want to do

5871
04:53:56,640 --> 04:53:58,640
is it would want to look past the end

5872
04:53:58,640 --> 04:54:00,640
because it would want to look at more tokens from that

5873
04:54:00,640 --> 04:54:02,640
and then we would just get errors

5874
04:54:02,640 --> 04:54:04,640
because you can't read more than

5875
04:54:04,640 --> 04:54:06,640
the file size if that makes sense

5876
04:54:06,640 --> 04:54:08,640
so that's why I'm just making this little threshold here

5877
04:54:08,640 --> 04:54:10,640
and

5878
04:54:10,640 --> 04:54:12,640
yeah so that's what that does

5879
04:54:12,640 --> 04:54:14,640
that's the starting position could be a random

5880
04:54:14,640 --> 04:54:16,640
number between the start and

5881
04:54:16,640 --> 04:54:18,640
a little bit a little margin from the end

5882
04:54:18,640 --> 04:54:20,640
here so

5883
04:54:20,640 --> 04:54:22,640
next up we have

5884
04:54:22,640 --> 04:54:24,640
this seek function so seek is going to

5885
04:54:24,640 --> 04:54:26,640
go to the start position and then

5886
04:54:26,640 --> 04:54:28,640
block is going to

5887
04:54:28,640 --> 04:54:30,640
read we're going to

5888
04:54:30,640 --> 04:54:32,640
go up to the start position it's going to

5889
04:54:32,640 --> 04:54:34,640
seek up to there that's where it's going to start

5890
04:54:34,640 --> 04:54:36,640
it's going to go up to it and then the read

5891
04:54:36,640 --> 04:54:38,640
function is going to

5892
04:54:38,640 --> 04:54:40,640
find a block of text that is

5893
04:54:40,640 --> 04:54:42,640
block size times batch size so it's

5894
04:54:42,640 --> 04:54:44,640
going to find a little snippet

5895
04:54:44,640 --> 04:54:46,640
of text in there at the starting

5896
04:54:46,640 --> 04:54:48,640
position and it's going to be of size

5897
04:54:48,640 --> 04:54:50,640
it's going to have this the same amount of

5898
04:54:50,640 --> 04:54:52,640
I guess bytes as

5899
04:54:52,640 --> 04:54:54,640
block size time times batch size

5900
04:54:54,640 --> 04:54:56,640
then all that minus one

5901
04:54:56,640 --> 04:54:58,640
just so that it fits into this start position

5902
04:54:58,640 --> 04:55:00,640
we don't get errors here that's why I put the minus one

5903
04:55:00,640 --> 04:55:02,640
but

5904
04:55:02,640 --> 04:55:04,640
yeah so we'll get a pretty

5905
04:55:04,640 --> 04:55:06,640
we'll get a pretty decent

5906
04:55:06,640 --> 04:55:08,640
text amount I guess you could say

5907
04:55:08,640 --> 04:55:10,640
it's going to be enough to work with you could

5908
04:55:10,640 --> 04:55:12,640
you could of course increases if you

5909
04:55:12,640 --> 04:55:14,640
wanted to you could do like

5910
04:55:14,640 --> 04:55:16,640
times eight if you wanted

5911
04:55:16,640 --> 04:55:18,640
times eight and then times eight up here but

5912
04:55:18,640 --> 04:55:20,640
we're not going to do that

5913
04:55:20,640 --> 04:55:22,640
based on my experience this is performed pretty well

5914
04:55:22,640 --> 04:55:24,640
so we're going to stick with this method here

5915
04:55:24,640 --> 04:55:26,640
and then

5916
04:55:26,640 --> 04:55:28,640
we just decode this

5917
04:55:28,640 --> 04:55:30,640
bit of text the reason we decode it is it's

5918
04:55:30,640 --> 04:55:32,640
it's because it's

5919
04:55:32,640 --> 04:55:34,640
we read it in binary form

5920
04:55:34,640 --> 04:55:36,640
so once we have this block of

5921
04:55:36,640 --> 04:55:38,640
text we actually have to decode this to

5922
04:55:38,640 --> 04:55:40,640
UFA format or UTF

5923
04:55:40,640 --> 04:55:42,640
format and then any like

5924
04:55:42,640 --> 04:55:44,640
bytecode errors we get we're just going to ignore

5925
04:55:44,640 --> 04:55:46,640
that this is something you learn

5926
04:55:46,640 --> 04:55:48,640
through practice is when you start dealing

5927
04:55:48,640 --> 04:55:50,640
with like really weird data or if it has

5928
04:55:50,640 --> 04:55:52,640
like corruptions in it you'll get errors

5929
04:55:52,640 --> 04:55:54,640
so all you want to do is all

5930
04:55:54,640 --> 04:55:56,640
this does is it pretty much says

5931
04:55:56,640 --> 04:55:58,640
okay we're just going to ignore this

5932
04:55:58,640 --> 04:56:00,640
bit of text and we're just going to sample

5933
04:56:00,640 --> 04:56:02,640
everything around it and not include that

5934
04:56:02,640 --> 04:56:04,640
part and plus since we're doing so many

5935
04:56:04,640 --> 04:56:06,640
iterations it won't actually interfere

5936
04:56:06,640 --> 04:56:08,640
that much so we should

5937
04:56:08,640 --> 04:56:10,640
be all right and then for this replace

5938
04:56:10,640 --> 04:56:12,640
go function here I was noticing

5939
04:56:12,640 --> 04:56:14,640
I got errors about this slash R

5940
04:56:14,640 --> 04:56:16,640
so all this does is it just replaces that

5941
04:56:16,640 --> 04:56:18,640
with an empty string and then finally

5942
04:56:18,640 --> 04:56:20,640
we have all this

5943
04:56:20,640 --> 04:56:22,640
we have all this decoded data

5944
04:56:22,640 --> 04:56:24,640
so all we're going to do is just encode

5945
04:56:24,640 --> 04:56:26,640
this into the

5946
04:56:26,640 --> 04:56:28,640
tokenized form so it's all in

5947
04:56:28,640 --> 04:56:30,640
it's all in the tokenized form

5948
04:56:30,640 --> 04:56:32,640
integers or torch.longs

5949
04:56:32,640 --> 04:56:34,640
data type

5950
04:56:34,640 --> 04:56:36,640
and we just that's what our data is

5951
04:56:36,640 --> 04:56:38,640
instead of a bunch of characters it's just a bunch

5952
04:56:38,640 --> 04:56:40,640
of numbers and then we

5953
04:56:40,640 --> 04:56:42,640
return that into our get batch

5954
04:56:42,640 --> 04:56:44,640
and this is what our data is

5955
04:56:44,640 --> 04:56:46,640
so that's pretty cool

5956
04:56:46,640 --> 04:56:48,640
we can get either train or a valve

5957
04:56:48,640 --> 04:56:50,640
split and

5958
04:56:50,640 --> 04:56:52,640
that's sort of what it looks like in practice

5959
04:56:52,640 --> 04:56:54,640
that's how we sample from

5960
04:56:54,640 --> 04:56:56,640
very large text files at a smaller

5961
04:56:56,640 --> 04:56:58,640
scale bit by bit so

5962
04:56:58,640 --> 04:57:00,640
let's go ahead and implement this here

5963
04:57:00,640 --> 04:57:02,640
and go grab this entire

5964
04:57:02,640 --> 04:57:04,640
thing

5965
04:57:04,640 --> 04:57:06,640
and pop over to here

5966
04:57:06,640 --> 04:57:08,640
we're just going to replace that

5967
04:57:08,640 --> 04:57:10,640
so

5968
04:57:10,640 --> 04:57:12,640
get random chunk, get batch

5969
04:57:12,640 --> 04:57:14,640
cool

5970
04:57:14,640 --> 04:57:16,640
so now we can actually go ahead and

5971
04:57:16,640 --> 04:57:18,640
perhaps run this

5972
04:57:18,640 --> 04:57:20,640
actually before we run this there's a little something we need to

5973
04:57:20,640 --> 04:57:22,640
add in here

5974
04:57:22,640 --> 04:57:24,640
so I have this

5975
04:57:24,640 --> 04:57:26,640
train split.txt and a valve split.txt

5976
04:57:26,640 --> 04:57:28,640
so I actually need to

5977
04:57:28,640 --> 04:57:30,640
change these

5978
04:57:30,640 --> 04:57:32,640
so let's go rename we'll go

5979
04:57:32,640 --> 04:57:34,640
train split.txt

5980
04:57:34,640 --> 04:57:36,640
and then

5981
04:57:36,640 --> 04:57:38,640
a valve split.txt

5982
04:57:38,640 --> 04:57:40,640
cool

5983
04:57:40,640 --> 04:57:42,640
and then we could just go

5984
04:57:42,640 --> 04:57:44,640
open web text

5985
04:57:44,640 --> 04:57:46,640
forward slash

5986
04:57:46,640 --> 04:57:48,640
and then same thing for here

5987
04:57:48,640 --> 04:57:50,640
cool let's go ahead

5988
04:57:50,640 --> 04:57:52,640
and run this now

5989
04:57:56,640 --> 04:57:58,640
and we're getting errors

5990
04:57:58,640 --> 04:58:00,640
mem map is not defined

5991
04:58:00,640 --> 04:58:02,640
so that's another thing we need to probably

5992
04:58:02,640 --> 04:58:04,640
add in then

5993
04:58:04,640 --> 04:58:06,640
so I'm actually just going to

5994
04:58:06,640 --> 04:58:08,640
stop this process from running here

5995
04:58:08,640 --> 04:58:10,640
we're going to go pip

5996
04:58:10,640 --> 04:58:12,640
install

5997
04:58:12,640 --> 04:58:14,640
mem map

5998
04:58:16,640 --> 04:58:18,640
mem map is not defined

5999
04:58:18,640 --> 04:58:20,640
we don't actually need to install this

6000
04:58:20,640 --> 04:58:22,640
by default comes with the operating system

6001
04:58:22,640 --> 04:58:24,640
so

6002
04:58:24,640 --> 04:58:26,640
what we actually need to do

6003
04:58:26,640 --> 04:58:28,640
is

6004
04:58:28,640 --> 04:58:30,640
just close this

6005
04:58:30,640 --> 04:58:32,640
gptv1

6006
04:58:32,640 --> 04:58:34,640
awesome

6007
04:58:34,640 --> 04:58:36,640
everything is good

6008
04:58:36,640 --> 04:58:38,640
nothing is broken

6009
04:58:38,640 --> 04:58:40,640
so what I actually need to do up here

6010
04:58:40,640 --> 04:58:42,640
is import this

6011
04:58:42,640 --> 04:58:44,640
so I need to go

6012
04:58:44,640 --> 04:58:46,640
import mem map

6013
04:58:46,640 --> 04:58:48,640
just like that

6014
04:58:48,640 --> 04:58:50,640
and

6015
04:58:50,640 --> 04:58:52,640
should be good to start running this script

6016
04:58:52,640 --> 04:58:54,640
name random is not defined

6017
04:58:54,640 --> 04:58:56,640
again another importation we have to make

6018
04:58:56,640 --> 04:58:58,640
import

6019
04:58:58,640 --> 04:59:00,640
import

6020
04:59:00,640 --> 04:59:02,640
random

6021
04:59:06,640 --> 04:59:08,640
and we should start seeing some

6022
04:59:08,640 --> 04:59:10,640
progress going here so once we see the first iteration

6023
04:59:10,640 --> 04:59:12,640
I'm going to stop it come back

6024
04:59:12,640 --> 04:59:14,640
at the last iteration and

6025
04:59:14,640 --> 04:59:16,640
then we'll start adding some little bits and pieces

6026
04:59:16,640 --> 04:59:18,640
onto our script here to make it better

6027
04:59:18,640 --> 04:59:20,640
so we're already about 600 iterations

6028
04:59:20,640 --> 04:59:22,640
in and you can see how the training loss

6029
04:59:22,640 --> 04:59:24,640
is actually done really well so far

6030
04:59:24,640 --> 04:59:26,640
it's gone from 10.5 drop all the way to

6031
04:59:26,640 --> 04:59:28,640
2.38

6032
04:59:28,640 --> 04:59:30,640
and

6033
04:59:30,640 --> 04:59:32,640
we can actually see that

6034
04:59:32,640 --> 04:59:34,640
we might be able to actually get a

6035
04:59:34,640 --> 04:59:36,640
val loss that is lower than the

6036
04:59:36,640 --> 04:59:38,640
train because keep in mind

6037
04:59:38,640 --> 04:59:40,640
in train mode

6038
04:59:40,640 --> 04:59:42,640
the dropout takes effect but in val

6039
04:59:42,640 --> 04:59:44,640
in eval mode

6040
04:59:44,640 --> 04:59:46,640
let me just scroll up to this here

6041
04:59:46,640 --> 04:59:48,640
yes

6042
04:59:48,640 --> 04:59:50,640
so model about eval what this does

6043
04:59:50,640 --> 04:59:52,640
is it turns off the dropout

6044
04:59:52,640 --> 04:59:54,640
so

6045
04:59:54,640 --> 04:59:56,640
we don't lose any of the neurons

6046
04:59:56,640 --> 04:59:58,640
and they're all sort of showing the same

6047
04:59:58,640 --> 05:00:00,640
features and giving all the information that they're supposed

6048
05:00:00,640 --> 05:00:02,640
to because they're all active but in train mode

6049
05:00:02,640 --> 05:00:04,640
20% of them are off so

6050
05:00:04,640 --> 05:00:06,640
once you actually see

6051
05:00:06,640 --> 05:00:08,640
in eval mode it does better

6052
05:00:08,640 --> 05:00:10,640
that means

6053
05:00:10,640 --> 05:00:12,640
that the network has started to

6054
05:00:12,640 --> 05:00:14,640
form a sense

6055
05:00:14,640 --> 05:00:16,640
of completeness in its learning

6056
05:00:16,640 --> 05:00:18,640
so it's just adjusting things a little bit

6057
05:00:18,640 --> 05:00:20,640
once it hits that point

6058
05:00:20,640 --> 05:00:22,640
and we might see this happen

6059
05:00:22,640 --> 05:00:24,640
momentarily but this is

6060
05:00:24,640 --> 05:00:26,640
really good progress so far a loss of

6061
05:00:26,640 --> 05:00:28,640
1.8 is amazing

6062
05:00:28,640 --> 05:00:30,640
so

6063
05:00:30,640 --> 05:00:32,640
in the meantime

6064
05:00:32,640 --> 05:00:34,640
I'm just going to add some little tweaks

6065
05:00:34,640 --> 05:00:36,640
here and there to improve this script

6066
05:00:36,640 --> 05:00:38,640
so I've actually stopped the iteration process

6067
05:00:38,640 --> 05:00:40,640
but we've gotten to 700 steps and we can already

6068
05:00:40,640 --> 05:00:42,640
see that val loss

6069
05:00:42,640 --> 05:00:44,640
is becoming a less than train loss

6070
05:00:44,640 --> 05:00:46,640
which is showing that the model is actually converging

6071
05:00:46,640 --> 05:00:48,640
and doing very well

6072
05:00:48,640 --> 05:00:50,640
so this architecture is amazing

6073
05:00:50,640 --> 05:00:52,640
we've pretty much covered

6074
05:00:52,640 --> 05:00:54,640
every

6075
05:00:54,640 --> 05:00:56,640
architectural, math, pie torch part

6076
05:00:56,640 --> 05:00:58,640
that this script has to offer

6077
05:00:58,640 --> 05:01:00,640
the only thing I want to add

6078
05:01:00,640 --> 05:01:02,640
actually a few things I want to add

6079
05:01:02,640 --> 05:01:04,640
one of them being torch.load

6080
05:01:04,640 --> 05:01:06,640
and torch.save

6081
05:01:06,640 --> 05:01:08,640
so one thing that's going to be really important

6082
05:01:08,640 --> 05:01:10,640
when you start to scale up

6083
05:01:10,640 --> 05:01:12,640
your iterations

6084
05:01:12,640 --> 05:01:14,640
is you don't just want to run a script

6085
05:01:14,640 --> 05:01:16,640
that executes a training loop

6086
05:01:16,640 --> 05:01:18,640
with an architecture and

6087
05:01:18,640 --> 05:01:20,640
that's it. You won't have some way to

6088
05:01:20,640 --> 05:01:22,640
store those learning parameters

6089
05:01:22,640 --> 05:01:24,640
so that's what torch.load and torch.save does

6090
05:01:26,640 --> 05:01:28,640
save some file

6091
05:01:28,640 --> 05:01:30,640
right and

6092
05:01:30,640 --> 05:01:32,640
you can pretty much

6093
05:01:32,640 --> 05:01:34,640
you could put it into like a serialized

6094
05:01:34,640 --> 05:01:36,640
format when you

6095
05:01:36,640 --> 05:01:38,640
save it you take your initial

6096
05:01:38,640 --> 05:01:40,640
architecture in our case it would actually

6097
05:01:40,640 --> 05:01:42,640
be the GPT language model so you would

6098
05:01:42,640 --> 05:01:44,640
save this because it contains

6099
05:01:44,640 --> 05:01:46,640
everything all these other classes

6100
05:01:46,640 --> 05:01:48,640
as well they're all inside of GPT

6101
05:01:48,640 --> 05:01:50,640
language model would save that architecture

6102
05:01:50,640 --> 05:01:52,640
and you essentially

6103
05:01:52,640 --> 05:01:54,640
serialize it into some pickled file

6104
05:01:54,640 --> 05:01:56,640
that would have

6105
05:01:56,640 --> 05:01:58,640
the file extension .pkl

6106
05:01:58,640 --> 05:02:00,640
so

6107
05:02:00,640 --> 05:02:02,640
essentially

6108
05:02:02,640 --> 05:02:04,640
instead of using torch we're just going to use

6109
05:02:04,640 --> 05:02:06,640
a library called pickle because

6110
05:02:06,640 --> 05:02:08,640
they're essentially the same thing

6111
05:02:08,640 --> 05:02:10,640
pickle is a little bit easier

6112
05:02:10,640 --> 05:02:12,640
to use or at least a little bit easier to understand

6113
05:02:12,640 --> 05:02:14,640
there's less to it

6114
05:02:14,640 --> 05:02:16,640
pickle will only work

6115
05:02:16,640 --> 05:02:18,640
on one GPU

6116
05:02:18,640 --> 05:02:20,640
so if you have like 8 GPUs at the same time

6117
05:02:20,640 --> 05:02:22,640
you're going to want to learn a little bit more

6118
05:02:22,640 --> 05:02:24,640
about hardware stuff and

6119
05:02:24,640 --> 05:02:26,640
some PyTorch docs but

6120
05:02:26,640 --> 05:02:28,640
pretty much

6121
05:02:28,640 --> 05:02:30,640
if we want to

6122
05:02:30,640 --> 05:02:32,640
save this after training

6123
05:02:32,640 --> 05:02:34,640
what we're going to do is we're going to use

6124
05:02:34,640 --> 05:02:36,640
a little library called pickle and this comes

6125
05:02:36,640 --> 05:02:38,640
pre-installed with windows

6126
05:02:40,640 --> 05:02:42,640
import pickle

6127
05:02:42,640 --> 05:02:44,640
okay so what we want to do is

6128
05:02:44,640 --> 05:02:46,640
implement this after the training loop

6129
05:02:46,640 --> 05:02:48,640
after all these parameters have been updated

6130
05:02:48,640 --> 05:02:50,640
and learned to the fullest extent

6131
05:02:50,640 --> 05:02:52,640
so after this training loop

6132
05:02:52,640 --> 05:02:54,640
we're simply going to open

6133
05:02:54,640 --> 05:02:56,640
we're going to do with open

6134
05:02:56,640 --> 05:02:58,640
and we could just go

6135
05:02:58,640 --> 05:03:00,640
model 01 like that

6136
05:03:00,640 --> 05:03:02,640
and then

6137
05:03:02,640 --> 05:03:04,640
just that .pkl is the file extension

6138
05:03:04,640 --> 05:03:06,640
for it

6139
05:03:06,640 --> 05:03:08,640
and then since we're writing to it we're going to go

6140
05:03:08,640 --> 05:03:10,640
write binary

6141
05:03:10,640 --> 05:03:12,640
F

6142
05:03:12,640 --> 05:03:14,640
and then in order to actually save this

6143
05:03:14,640 --> 05:03:16,640
we just go pickle.dump

6144
05:03:16,640 --> 05:03:18,640
and then we can use

6145
05:03:18,640 --> 05:03:20,640
model and then

6146
05:03:20,640 --> 05:03:22,640
just F like that

6147
05:03:22,640 --> 05:03:24,640
so

6148
05:03:24,640 --> 05:03:26,640
if I start recording this

6149
05:03:26,640 --> 05:03:28,640
it's going to make

6150
05:03:28,640 --> 05:03:30,640
if I start recording this training process

6151
05:03:30,640 --> 05:03:32,640
it's going to make my clip

6152
05:03:32,640 --> 05:03:34,640
like so

6153
05:03:34,640 --> 05:03:36,640
I'm going to come back to this after we've done

6154
05:03:36,640 --> 05:03:38,640
let's just say about

6155
05:03:38,640 --> 05:03:40,640
100 iterations

6156
05:03:40,640 --> 05:03:42,640
we're going to do 100 editors

6157
05:03:42,640 --> 05:03:44,640
and I'm going to come back and

6158
05:03:44,640 --> 05:03:46,640
show you guys

6159
05:03:46,640 --> 05:03:48,640
what the model file looks like

6160
05:03:48,640 --> 05:03:50,640
what I actually did is I changed some of the model

6161
05:03:50,640 --> 05:03:52,640
hyper parameters because

6162
05:03:52,640 --> 05:03:54,640
it was taking way too long

6163
05:03:54,640 --> 05:03:56,640
to perform what we wanted it to so I changed

6164
05:03:56,640 --> 05:03:58,640
and head to one and layer to one

6165
05:03:58,640 --> 05:04:00,640
and I had half batch size

6166
05:04:00,640 --> 05:04:02,640
all the way down from 64 to 32

6167
05:04:02,640 --> 05:04:04,640
so what I'm actually going to add here is just

6168
05:04:04,640 --> 05:04:06,640
to make sure I like to print this out at the beginning

6169
05:04:06,640 --> 05:04:08,640
of this

6170
05:04:08,640 --> 05:04:10,640
make sure that the device is CUDA

6171
05:04:10,640 --> 05:04:12,640
let's go back down

6172
05:04:12,640 --> 05:04:14,640
so it did in fact train the model

6173
05:04:14,640 --> 05:04:16,640
so we got all this done

6174
05:04:16,640 --> 05:04:18,640
and yeah

6175
05:04:18,640 --> 05:04:20,640
so I don't know why I did 2.54

6176
05:04:20,640 --> 05:04:22,640
whatever that

6177
05:04:22,640 --> 05:04:24,640
that was just the entire loss

6178
05:04:24,640 --> 05:04:26,640
so

6179
05:04:26,640 --> 05:04:28,640
model saved awesome

6180
05:04:28,640 --> 05:04:30,640
what does this actually look like here

6181
05:04:30,640 --> 05:04:32,640
so this model.pkl

6182
05:04:32,640 --> 05:04:34,640
106 megabytes isn't that wonderful

6183
05:04:34,640 --> 05:04:36,640
so this is our model file this is what they look like

6184
05:04:36,640 --> 05:04:38,640
it's just a serialized

6185
05:04:38,640 --> 05:04:40,640
pretty much the entire architecture

6186
05:04:40,640 --> 05:04:42,640
all the parameters of the model the state

6187
05:04:42,640 --> 05:04:44,640
everything that it contains

6188
05:04:44,640 --> 05:04:46,640
and we just compress that

6189
05:04:46,640 --> 05:04:48,640
into a little pkl file take that out

6190
05:04:48,640 --> 05:04:50,640
decompress it and then just use it again

6191
05:04:50,640 --> 05:04:52,640
with all those same parameters so

6192
05:04:52,640 --> 05:04:54,640
awesome

6193
05:04:54,640 --> 05:04:56,640
and all this really took was

6194
05:04:56,640 --> 05:04:58,640
we just open

6195
05:04:58,640 --> 05:05:00,640
as this

6196
05:05:00,640 --> 05:05:02,640
we do a pickle.dump

6197
05:05:02,640 --> 05:05:04,640
to make sure that actually save I just like to add

6198
05:05:04,640 --> 05:05:06,640
a little print statement there cool

6199
05:05:06,640 --> 05:05:08,640
so next

6200
05:05:08,640 --> 05:05:10,640
what I'd like to add is a little

6201
05:05:10,640 --> 05:05:12,640
wait for us to

6202
05:05:12,640 --> 05:05:14,640
instead of just doing all of our training at once

6203
05:05:14,640 --> 05:05:16,640
and then saving the model being able to

6204
05:05:16,640 --> 05:05:18,640
train multiple times

6205
05:05:18,640 --> 05:05:20,640
so I'm gonna go up here

6206
05:05:20,640 --> 05:05:22,640
to our

6207
05:05:22,640 --> 05:05:24,640
GPT language model here

6208
05:05:24,640 --> 05:05:26,640
and

6209
05:05:26,640 --> 05:05:28,640
let's just see

6210
05:05:28,640 --> 05:05:30,640
what I'm gonna do

6211
05:05:30,640 --> 05:05:32,640
with open

6212
05:05:32,640 --> 05:05:34,640
and we're gonna go

6213
05:05:34,640 --> 05:05:36,640
model 01

6214
05:05:36,640 --> 05:05:38,640
pkl

6215
05:05:38,640 --> 05:05:40,640
and we're gonna go read binary

6216
05:05:40,640 --> 05:05:42,640
so actually gonna read it we're gonna

6217
05:05:42,640 --> 05:05:44,640
load this into

6218
05:05:44,640 --> 05:05:46,640
our script here

6219
05:05:46,640 --> 05:05:48,640
so

6220
05:05:48,640 --> 05:05:50,640
we're gonna go as f

6221
05:05:50,640 --> 05:05:52,640
and then

6222
05:05:52,640 --> 05:05:54,640
I believe it's pickle.load

6223
05:05:56,640 --> 05:05:58,640
you just go yeah

6224
05:05:58,640 --> 05:06:00,640
model equals

6225
05:06:00,640 --> 05:06:02,640
pickle.load and then we'll just

6226
05:06:02,640 --> 05:06:04,640
essentially dump that

6227
05:06:04,640 --> 05:06:06,640
right in there

6228
05:06:06,640 --> 05:06:08,640
go print

6229
05:06:08,640 --> 05:06:10,640
loading

6230
05:06:10,640 --> 05:06:12,640
model

6231
05:06:12,640 --> 05:06:14,640
parameters

6232
05:06:14,640 --> 05:06:16,640
dot dot dot

6233
05:06:16,640 --> 05:06:18,640
and then

6234
05:06:18,640 --> 05:06:20,640
just put f in there

6235
05:06:20,640 --> 05:06:22,640
and then once it is loaded

6236
05:06:22,640 --> 05:06:24,640
we'll do print

6237
05:06:24,640 --> 05:06:26,640
loaded

6238
05:06:26,640 --> 05:06:28,640
successfully

6239
05:06:28,640 --> 05:06:30,640
cool

6240
05:06:30,640 --> 05:06:32,640
so I'm actually gonna try this out now

6241
05:06:32,640 --> 05:06:34,640
go

6242
05:06:34,640 --> 05:06:36,640
do that

6243
05:06:36,640 --> 05:06:38,640
boom

6244
05:06:38,640 --> 05:06:40,640
and boom

6245
05:06:40,640 --> 05:06:42,640
okay

6246
05:06:42,640 --> 05:06:44,640
so

6247
05:06:44,640 --> 05:06:46,640
loading model parameters loaded successfully

6248
05:06:46,640 --> 05:06:48,640
and we'll actually see this

6249
05:06:48,640 --> 05:06:50,640
start to work on its own now

6250
05:06:50,640 --> 05:06:52,640
so

6251
05:06:52,640 --> 05:06:54,640
is it going to begin or is it not going to begin

6252
05:06:54,640 --> 05:06:56,640
let's run that

6253
05:06:56,640 --> 05:06:58,640
okay perfect

6254
05:06:58,640 --> 05:07:00,640
so now we should take the loss

6255
05:07:00,640 --> 05:07:02,640
that we had before which was about

6256
05:07:02,640 --> 05:07:04,640
2.54 I believe

6257
05:07:04,640 --> 05:07:06,640
something around those, something along those lines

6258
05:07:06,640 --> 05:07:08,640
you can see that our training process

6259
05:07:08,640 --> 05:07:10,640
is greatly accelerated

6260
05:07:12,640 --> 05:07:14,640
so we had 100

6261
05:07:14,640 --> 05:07:16,640
now it's just gonna do an estimate loss

6262
05:07:16,640 --> 05:07:18,640
cool

6263
05:07:20,640 --> 05:07:22,640
and we're almost done

6264
05:07:25,640 --> 05:07:27,640
1.96 awesome

6265
05:07:27,640 --> 05:07:29,640
and the model saved

6266
05:07:29,640 --> 05:07:31,640
so essentially what we can do with this

6267
05:07:31,640 --> 05:07:33,640
is we can now

6268
05:07:33,640 --> 05:07:35,640
save models

6269
05:07:35,640 --> 05:07:37,640
and then we can load them and then iterate further

6270
05:07:37,640 --> 05:07:39,640
so if you wanted to

6271
05:07:39,640 --> 05:07:41,640
you could create a super cool

6272
05:07:41,640 --> 05:07:43,640
GPT language model

6273
05:07:43,640 --> 05:07:45,640
script here and

6274
05:07:45,640 --> 05:07:47,640
you could essentially give it like 10,000 or 20,000

6275
05:07:47,640 --> 05:07:49,640
iterations to run overnight

6276
05:07:49,640 --> 05:07:51,640
you'd be able to save it

6277
05:07:51,640 --> 05:07:53,640
and then import that into say a chat bot

6278
05:07:53,640 --> 05:07:55,640
if you want

6279
05:07:55,640 --> 05:07:57,640
so that's pretty cool and that's just kind of

6280
05:07:57,640 --> 05:07:59,640
a good thing

6281
05:07:59,640 --> 05:08:01,640
good little, it's kind of

6282
05:08:01,640 --> 05:08:03,640
essential for language modeling because

6283
05:08:03,640 --> 05:08:05,640
what's the point

6284
05:08:05,640 --> 05:08:07,640
in having a machine learning model if you can't

6285
05:08:07,640 --> 05:08:09,640
actually use it and deploy it

6286
05:08:09,640 --> 05:08:11,640
so you need to save for this stuff to work

6287
05:08:11,640 --> 05:08:13,640
alright

6288
05:08:13,640 --> 05:08:15,640
now let's move on to

6289
05:08:15,640 --> 05:08:17,640
a little something in this task manager

6290
05:08:17,640 --> 05:08:19,640
here which I'd like to go over

6291
05:08:19,640 --> 05:08:21,640
so this shared GPU memory here

6292
05:08:21,640 --> 05:08:23,640
and this dedicated GPU memory

6293
05:08:23,640 --> 05:08:25,640
so dedicated

6294
05:08:25,640 --> 05:08:27,640
means how much

6295
05:08:27,640 --> 05:08:29,640
VRAM, video RAM

6296
05:08:29,640 --> 05:08:31,640
does your GPU actually have

6297
05:08:31,640 --> 05:08:33,640
on the card

6298
05:08:33,640 --> 05:08:35,640
so on the card it's going to be very quick memory

6299
05:08:35,640 --> 05:08:37,640
because it doesn't have to

6300
05:08:37,640 --> 05:08:39,640
the electrons don't have to travel as quickly

6301
05:08:39,640 --> 05:08:41,640
that's kind of the logic of it

6302
05:08:41,640 --> 05:08:43,640
the electrons don't have to travel

6303
05:08:43,640 --> 05:08:45,640
they don't have to travel as far

6304
05:08:45,640 --> 05:08:47,640
because

6305
05:08:47,640 --> 05:08:49,640
the little RAM chip is right there

6306
05:08:49,640 --> 05:08:51,640
so

6307
05:08:51,640 --> 05:08:53,640
dedicated GPU memory is a lot faster

6308
05:08:53,640 --> 05:08:55,640
shared GPU memory

6309
05:08:55,640 --> 05:08:57,640
is essentially if this gets overloaded

6310
05:08:57,640 --> 05:08:59,640
it'll use some of the RAM on your

6311
05:08:59,640 --> 05:09:01,640
computer instead

6312
05:09:01,640 --> 05:09:03,640
so this will typically be about half of your

6313
05:09:03,640 --> 05:09:05,640
computer's RAM

6314
05:09:05,640 --> 05:09:07,640
I have 32 gigabytes of RAM on my computer

6315
05:09:07,640 --> 05:09:09,640
so 16.0 makes sense

6316
05:09:09,640 --> 05:09:11,640
half 32

6317
05:09:11,640 --> 05:09:13,640
and yeah

6318
05:09:13,640 --> 05:09:15,640
so you want to make sure you're only using dedicated

6319
05:09:15,640 --> 05:09:17,640
GPU memory

6320
05:09:17,640 --> 05:09:19,640
having your shared GPU memory go up

6321
05:09:19,640 --> 05:09:21,640
is not usually a good thing

6322
05:09:21,640 --> 05:09:23,640
a little bit is fine

6323
05:09:23,640 --> 05:09:25,640
but

6324
05:09:25,640 --> 05:09:27,640
dedicated GPU memory is the fastest

6325
05:09:27,640 --> 05:09:29,640
and you want everything to stick on there

6326
05:09:29,640 --> 05:09:31,640
just try to make sure all of your parameters

6327
05:09:31,640 --> 05:09:33,640
sort of fit around this

6328
05:09:33,640 --> 05:09:35,640
whatever your max capacity is

6329
05:09:35,640 --> 05:09:37,640
maybe it's 4, maybe it's 8

6330
05:09:37,640 --> 05:09:39,640
maybe it's 48

6331
05:09:39,640 --> 05:09:41,640
who knows

6332
05:09:41,640 --> 05:09:43,640
and a good way to figure out

6333
05:09:43,640 --> 05:09:45,640
what you can use on your GPU

6334
05:09:45,640 --> 05:09:47,640
without it getting memory errors

6335
05:09:47,640 --> 05:09:49,640
or using shared memory

6336
05:09:49,640 --> 05:09:51,640
is to actually play around

6337
05:09:51,640 --> 05:09:53,640
with

6338
05:09:53,640 --> 05:09:55,640
these parameters up here

6339
05:09:55,640 --> 05:09:57,640
so

6340
05:09:57,640 --> 05:09:59,640
block size and batch size

6341
05:09:59,640 --> 05:10:01,640
actually let me switch those around

6342
05:10:01,640 --> 05:10:03,640
these are not supposed to be in that order

6343
05:10:03,640 --> 05:10:05,640
but

6344
05:10:05,640 --> 05:10:07,640
all good

6345
05:10:07,640 --> 05:10:09,640
we'll make our batch size

6346
05:10:09,640 --> 05:10:11,640
64

6347
05:10:11,640 --> 05:10:13,640
that's 128

6348
05:10:13,640 --> 05:10:15,640
okay

6349
05:10:15,640 --> 05:10:17,640
so

6350
05:10:17,640 --> 05:10:19,640
batch size and block size

6351
05:10:19,640 --> 05:10:21,640
are very big contributors to how much memory you're going to use

6352
05:10:21,640 --> 05:10:23,640
learning rate is not

6353
05:10:23,640 --> 05:10:25,640
max iterations is not

6354
05:10:25,640 --> 05:10:27,640
evaluators is not

6355
05:10:27,640 --> 05:10:29,640
but these three will

6356
05:10:29,640 --> 05:10:31,640
the amount of features that you store

6357
05:10:31,640 --> 05:10:33,640
the amount of heads you have running in parallel

6358
05:10:33,640 --> 05:10:35,640
and then also

6359
05:10:35,640 --> 05:10:37,640
layers so

6360
05:10:37,640 --> 05:10:39,640
some of these will not

6361
05:10:39,640 --> 05:10:41,640
affect you as much because they're more

6362
05:10:41,640 --> 05:10:43,640
sort of restrained to computation

6363
05:10:43,640 --> 05:10:45,640
how quickly you can do operations if something is sequential

6364
05:10:47,640 --> 05:10:49,640
so N layer won't strain you

6365
05:10:49,640 --> 05:10:51,640
as much as something like batch and block size

6366
05:10:51,640 --> 05:10:53,640
but

6367
05:10:53,640 --> 05:10:55,640
those are just good little things to

6368
05:10:55,640 --> 05:10:57,640
sort of tweak and play around with

6369
05:10:57,640 --> 05:10:59,640
so I found the optimal

6370
05:10:59,640 --> 05:11:01,640
sort of set of

6371
05:11:01,640 --> 05:11:03,640
hyper parameters for my PC

6372
05:11:03,640 --> 05:11:05,640
that happens to be

6373
05:11:05,640 --> 05:11:07,640
8, 8, 3, 8, 4

6374
05:11:07,640 --> 05:11:09,640
learning rates is the same

6375
05:11:09,640 --> 05:11:11,640
and then 64, 128 for this

6376
05:11:11,640 --> 05:11:13,640
so that happened to be the optimal

6377
05:11:13,640 --> 05:11:15,640
hyper parameters for my computer

6378
05:11:15,640 --> 05:11:17,640
it'll probably be different for yours

6379
05:11:17,640 --> 05:11:19,640
if you don't have 8 gigabytes of RAM on your GPU

6380
05:11:21,640 --> 05:11:23,640
so anyways

6381
05:11:23,640 --> 05:11:25,640
that's a little something you have to pay attention to

6382
05:11:25,640 --> 05:11:27,640
to make sure you don't run out of errors

6383
05:11:27,640 --> 05:11:29,640
and a technique you can use

6384
05:11:29,640 --> 05:11:31,640
which I'm not actually going to show you in this course

6385
05:11:31,640 --> 05:11:33,640
but it's quite useful is something called auto tuning

6386
05:11:33,640 --> 05:11:35,640
and what auto tuning does

6387
05:11:35,640 --> 05:11:37,640
is it pretty much runs

6388
05:11:37,640 --> 05:11:39,640
a bunch of these

6389
05:11:39,640 --> 05:11:41,640
a bunch of models with different

6390
05:11:41,640 --> 05:11:43,640
sets of hyper parameters

6391
05:11:43,640 --> 05:11:45,640
so to run like batch size 64

6392
05:11:45,640 --> 05:11:47,640
batch size 32, batch size 16

6393
05:11:47,640 --> 05:11:49,640
batch size maybe 256

6394
05:11:49,640 --> 05:11:51,640
we'll be like okay which ones are throwing errors and which ones aren't

6395
05:11:51,640 --> 05:11:53,640
so what it'll do

6396
05:11:53,640 --> 05:11:55,640
if you properly

6397
05:11:55,640 --> 05:11:57,640
if you properly set up an auto tuning script

6398
05:11:57,640 --> 05:11:59,640
is

6399
05:11:59,640 --> 05:12:01,640
you will be able to find

6400
05:12:01,640 --> 05:12:03,640
the most optimal

6401
05:12:03,640 --> 05:12:05,640
set of parameters for your computer

6402
05:12:05,640 --> 05:12:07,640
most optimal set of hyper parameters

6403
05:12:07,640 --> 05:12:09,640
that is possible

6404
05:12:09,640 --> 05:12:11,640
so auto tuning is cool

6405
05:12:11,640 --> 05:12:13,640
you can definitely look more into that

6406
05:12:13,640 --> 05:12:15,640
there's tons of research on it

6407
05:12:15,640 --> 05:12:17,640
and yeah so

6408
05:12:17,640 --> 05:12:19,640
auto tuning is cool let's dig into the next part

6409
05:12:19,640 --> 05:12:21,640
the next little trick we use in practice

6410
05:12:21,640 --> 05:12:23,640
especially by machine learning engineers

6411
05:12:23,640 --> 05:12:25,640
it's a little something called arguments

6412
05:12:25,640 --> 05:12:27,640
so you pass an argument into

6413
05:12:27,640 --> 05:12:29,640
not necessarily a function but into the command line

6414
05:12:29,640 --> 05:12:31,640
so this is what it'll look like

6415
05:12:31,640 --> 05:12:33,640
this is just a basic example

6416
05:12:33,640 --> 05:12:35,640
of what arg parsing will look like

6417
05:12:35,640 --> 05:12:37,640
so just go

6418
05:12:37,640 --> 05:12:39,640
python, arg parsing

6419
05:12:39,640 --> 05:12:41,640
because that's a script's name

6420
05:12:41,640 --> 05:12:43,640
I go dash

6421
05:12:43,640 --> 05:12:45,640
llms because that's what it says

6422
05:12:45,640 --> 05:12:47,640
right here this is what the argument is

6423
05:12:47,640 --> 05:12:49,640
and then we can just pass in a string

6424
05:12:49,640 --> 05:12:51,640
say hello

6425
05:12:51,640 --> 05:12:53,640
the provided

6426
05:12:53,640 --> 05:12:55,640
whatever is hello

6427
05:12:55,640 --> 05:12:57,640
cool you can add little arguments to this

6428
05:12:57,640 --> 05:12:59,640
and I'm even going to change this around

6429
05:12:59,640 --> 05:13:01,640
I could say

6430
05:13:05,640 --> 05:13:07,640
batch size

6431
05:13:07,640 --> 05:13:09,640
and then

6432
05:13:09,640 --> 05:13:11,640
let's go like that

6433
05:13:11,640 --> 05:13:13,640
batch

6434
05:13:13,640 --> 05:13:15,640
batch size

6435
05:13:19,640 --> 05:13:21,640
please

6436
05:13:21,640 --> 05:13:23,640
provide

6437
05:13:23,640 --> 05:13:25,640
a batch size

6438
05:13:27,640 --> 05:13:29,640
I can do the same thing again

6439
05:13:31,640 --> 05:13:33,640
and see it says

6440
05:13:33,640 --> 05:13:35,640
following arguments required are batch size

6441
05:13:35,640 --> 05:13:37,640
so that obviously didn't work

6442
05:13:37,640 --> 05:13:39,640
and if we actually tried the correct way

6443
05:13:39,640 --> 05:13:41,640
our parsing.py then we go

6444
05:13:41,640 --> 05:13:43,640
dash, batch size

6445
05:13:43,640 --> 05:13:45,640
we can make it 32

6446
05:13:45,640 --> 05:13:47,640
oops

6447
05:13:51,640 --> 05:13:53,640
that's because it's not a string

6448
05:13:53,640 --> 05:13:55,640
so

6449
05:13:55,640 --> 05:13:57,640
what we need to actually do

6450
05:13:57,640 --> 05:13:59,640
is it's bs somewhere

6451
05:13:59,640 --> 05:14:01,640
okay

6452
05:14:01,640 --> 05:14:03,640
so

6453
05:14:03,640 --> 05:14:05,640
args

6454
05:14:05,640 --> 05:14:07,640
parse args

6455
05:14:07,640 --> 05:14:09,640
so we need to change this

6456
05:14:09,640 --> 05:14:11,640
to bs like that

6457
05:14:11,640 --> 05:14:13,640
let me go batch size

6458
05:14:13,640 --> 05:14:15,640
batch size is 32

6459
05:14:15,640 --> 05:14:17,640
okay

6460
05:14:17,640 --> 05:14:19,640
so even I'm a little bit new to arguments as well

6461
05:14:19,640 --> 05:14:21,640
but

6462
05:14:21,640 --> 05:14:23,640
this is something that comes in very handy

6463
05:14:23,640 --> 05:14:25,640
when you're trying to know each time

6464
05:14:25,640 --> 05:14:27,640
you're trying to change some parameters

6465
05:14:27,640 --> 05:14:29,640
if you add

6466
05:14:29,640 --> 05:14:31,640
new gpu or whatever and you're like oh I want to double my batch size

6467
05:14:31,640 --> 05:14:33,640
it's like sure you can easily do that

6468
05:14:33,640 --> 05:14:35,640
so a lot of the times

6469
05:14:35,640 --> 05:14:37,640
it won't just have one but you'll have like

6470
05:14:37,640 --> 05:14:39,640
many meaning like maybe a dozen

6471
05:14:39,640 --> 05:14:41,640
or so of these

6472
05:14:41,640 --> 05:14:43,640
of these little arguments

6473
05:14:43,640 --> 05:14:45,640
so that is what this looks like

6474
05:14:45,640 --> 05:14:47,640
and

6475
05:14:47,640 --> 05:14:49,640
we're going to go ahead and implement this

6476
05:14:49,640 --> 05:14:51,640
into our little script here

6477
05:14:51,640 --> 05:14:53,640
so

6478
05:14:53,640 --> 05:14:55,640
I'm just going to

6479
05:14:55,640 --> 05:14:57,640
pop over to gpt1

6480
05:14:57,640 --> 05:14:59,640
I'm going to pull this up on my

6481
05:14:59,640 --> 05:15:01,640
second monitor here

6482
05:15:03,640 --> 05:15:05,640
and

6483
05:15:05,640 --> 05:15:07,640
in terms of these

6484
05:15:07,640 --> 05:15:09,640
I'm just going to start off

6485
05:15:09,640 --> 05:15:11,640
making a

6486
05:15:11,640 --> 05:15:13,640
importation

6487
05:15:13,640 --> 05:15:15,640
arg

6488
05:15:15,640 --> 05:15:17,640
arg parser

6489
05:15:17,640 --> 05:15:19,640
or arg parse rather

6490
05:15:19,640 --> 05:15:21,640
that's what it's called

6491
05:15:21,640 --> 05:15:23,640
and then we go

6492
05:15:23,640 --> 05:15:25,640
parser is equal to

6493
05:15:25,640 --> 05:15:27,640
I'll just

6494
05:15:27,640 --> 05:15:29,640
copy and paste this entire thing

6495
05:15:29,640 --> 05:15:31,640
and why not

6496
05:15:31,640 --> 05:15:33,640
cool

6497
05:15:35,640 --> 05:15:37,640
okay

6498
05:15:37,640 --> 05:15:39,640
so

6499
05:15:39,640 --> 05:15:41,640
we get a batch size

6500
05:15:41,640 --> 05:15:43,640
or something

6501
05:15:43,640 --> 05:15:45,640
and then

6502
05:15:45,640 --> 05:15:47,640
we'll add in the second part here

6503
05:15:47,640 --> 05:15:49,640
so

6504
05:15:55,640 --> 05:15:57,640
args parse the arguments

6505
05:15:57,640 --> 05:15:59,640
here

6506
05:16:01,640 --> 05:16:03,640
and the little scope

6507
05:16:03,640 --> 05:16:05,640
of

6508
05:16:05,640 --> 05:16:07,640
batch size like that

6509
05:16:07,640 --> 05:16:09,640
our batch size is equal to

6510
05:16:09,640 --> 05:16:11,640
whatever that was

6511
05:16:11,640 --> 05:16:13,640
and we'll just go args

6512
05:16:13,640 --> 05:16:15,640
dot

6513
05:16:15,640 --> 05:16:17,640
args dot batch size so cool

6514
05:16:19,640 --> 05:16:21,640
we're going to run this

6515
05:16:21,640 --> 05:16:23,640
and

6516
05:16:23,640 --> 05:16:25,640
not defined

6517
05:16:25,640 --> 05:16:27,640
so I got a little not defined thing here

6518
05:16:27,640 --> 05:16:29,640
and pretty much

6519
05:16:29,640 --> 05:16:31,640
all I missed was that

6520
05:16:31,640 --> 05:16:33,640
we're doing this so essentially

6521
05:16:33,640 --> 05:16:35,640
this

6522
05:16:35,640 --> 05:16:37,640
should be equal to this right here

6523
05:16:37,640 --> 05:16:39,640
so I'm just going to go ahead and copy that

6524
05:16:39,640 --> 05:16:41,640
and

6525
05:16:43,640 --> 05:16:45,640
boot parse args

6526
05:16:45,640 --> 05:16:47,640
except

6527
05:16:47,640 --> 05:16:49,640
we don't have a parse args function

6528
05:16:49,640 --> 05:16:51,640
so

6529
05:16:51,640 --> 05:16:53,640
what do we need to do instead

6530
05:16:53,640 --> 05:16:55,640
well it

6531
05:16:55,640 --> 05:16:57,640
actually that might just work on it so let's try it out

6532
05:17:01,640 --> 05:17:03,640
okay so it looks like

6533
05:17:03,640 --> 05:17:05,640
it's actually expecting some input here

6534
05:17:05,640 --> 05:17:07,640
in code so

6535
05:17:07,640 --> 05:17:09,640
that's probably working

6536
05:17:09,640 --> 05:17:11,640
and if we

6537
05:17:11,640 --> 05:17:13,640
ported this into a script

6538
05:17:13,640 --> 05:17:15,640
then it would simply ask us for some input

6539
05:17:15,640 --> 05:17:17,640
so I believe we're doing this correctly

6540
05:17:17,640 --> 05:17:19,640
let's go ahead

6541
05:17:19,640 --> 05:17:21,640
and actually switch over

6542
05:17:21,640 --> 05:17:23,640
and pour all of this into some code

6543
05:17:23,640 --> 05:17:25,640
so I'm going to make

6544
05:17:25,640 --> 05:17:27,640
a training file

6545
05:17:27,640 --> 05:17:29,640
and a chat file

6546
05:17:29,640 --> 05:17:31,640
the training file is going to be all of our parameters

6547
05:17:31,640 --> 05:17:33,640
whatever all of our architecture

6548
05:17:33,640 --> 05:17:35,640
and then the actual training loop itself

6549
05:17:35,640 --> 05:17:37,640
we're going to have some arguments in there

6550
05:17:37,640 --> 05:17:39,640
and then the chat bot is going to be

6551
05:17:39,640 --> 05:17:41,640
pretty much just a question-answer

6552
05:17:41,640 --> 05:17:43,640
thing that just reproduces text

6553
05:17:43,640 --> 05:17:45,640
so it'll just be like prompt, completion

6554
05:17:45,640 --> 05:17:47,640
type of thing and

6555
05:17:47,640 --> 05:17:49,640
yeah so let's go ahead and implement that here

6556
05:17:49,640 --> 05:17:51,640
so in our

6557
05:17:51,640 --> 05:17:53,640
GPT course

6558
05:17:53,640 --> 05:17:55,640
here I'm going to go

6559
05:17:55,640 --> 05:17:57,640
training.py

6560
05:17:57,640 --> 05:17:59,640
and we're going to go

6561
05:17:59,640 --> 05:18:01,640
chatbot.py

6562
05:18:01,640 --> 05:18:03,640
just like that

6563
05:18:03,640 --> 05:18:05,640
so in training

6564
05:18:05,640 --> 05:18:07,640
let's go ahead and drag everything in here

6565
05:18:09,640 --> 05:18:11,640
I'm just going to

6566
05:18:11,640 --> 05:18:13,640
move this over to the second screen

6567
05:18:13,640 --> 05:18:15,640
and just copy and paste

6568
05:18:15,640 --> 05:18:17,640
everything in order here

6569
05:18:17,640 --> 05:18:19,640
so next up we have our

6570
05:18:19,640 --> 05:18:21,640
characters

6571
05:18:21,640 --> 05:18:23,640
and then we have our

6572
05:18:23,640 --> 05:18:25,640
tokenizer

6573
05:18:25,640 --> 05:18:27,640
and then our

6574
05:18:27,640 --> 05:18:29,640
getRandomChunk and getBatches

6575
05:18:33,640 --> 05:18:35,640
suite

6576
05:18:35,640 --> 05:18:37,640
our estimateLoss function

6577
05:18:41,640 --> 05:18:43,640
and then this giant piece

6578
05:18:43,640 --> 05:18:45,640
of code

6579
05:18:45,640 --> 05:18:47,640
containing

6580
05:18:47,640 --> 05:18:49,640
most of the architecture we built up

6581
05:18:51,640 --> 05:18:53,640
we're just going to add that in there

6582
05:18:53,640 --> 05:18:55,640
we're not getting any warnings

6583
05:18:57,640 --> 05:18:59,640
and then the training loop

6584
05:19:01,640 --> 05:19:03,640
and the optimizer

6585
05:19:03,640 --> 05:19:05,640
awesome

6586
05:19:05,640 --> 05:19:07,640
then after this

6587
05:19:07,640 --> 05:19:09,640
we would simply have this context

6588
05:19:09,640 --> 05:19:11,640
but the point of this is that we want to have this in our

6589
05:19:11,640 --> 05:19:13,640
chatbot script

6590
05:19:13,640 --> 05:19:15,640
so what I'm going to do

6591
05:19:15,640 --> 05:19:17,640
is in this training.py

6592
05:19:17,640 --> 05:19:19,640
I'm going to keep

6593
05:19:19,640 --> 05:19:21,640
all of these the same I'm going to keep this entire thing

6594
05:19:21,640 --> 05:19:23,640
the same

6595
05:19:23,640 --> 05:19:25,640
get rid of this little block of code

6596
05:19:25,640 --> 05:19:27,640
and we're going to go into

6597
05:19:27,640 --> 05:19:29,640
the chatbot

6598
05:19:29,640 --> 05:19:31,640
here so loadingMile

6599
05:19:31,640 --> 05:19:33,640
parameters good we want to load some in

6600
05:19:33,640 --> 05:19:35,640
train some more and then dump it

6601
05:19:35,640 --> 05:19:37,640
chatbot is not going to dump anything

6602
05:19:37,640 --> 05:19:39,640
it's just going to save so I'm going to take

6603
05:19:39,640 --> 05:19:41,640
all of our training here

6604
05:19:43,640 --> 05:19:45,640
and instead of dumping

6605
05:19:45,640 --> 05:19:47,640
take that away we'll also take

6606
05:19:47,640 --> 05:19:49,640
away the training

6607
05:19:49,640 --> 05:19:51,640
loop as well

6608
05:19:55,640 --> 05:19:57,640
I don't believe we have anything

6609
05:19:57,640 --> 05:19:59,640
else to actually bring in

6610
05:19:59,640 --> 05:20:01,640
we don't need our getBatch

6611
05:20:01,640 --> 05:20:03,640
we do not need our getRandomChunks

6612
05:20:03,640 --> 05:20:05,640
so awesome

6613
05:20:05,640 --> 05:20:07,640
we're just importing these parameters

6614
05:20:07,640 --> 05:20:09,640
by default like that

6615
05:20:09,640 --> 05:20:11,640
awesome

6616
05:20:11,640 --> 05:20:13,640
so from this point

6617
05:20:13,640 --> 05:20:15,640
we have imported

6618
05:20:15,640 --> 05:20:17,640
we've imported our model

6619
05:20:17,640 --> 05:20:19,640
cool so let's go ahead

6620
05:20:19,640 --> 05:20:21,640
and port in our little

6621
05:20:21,640 --> 05:20:23,640
chatbot here

6622
05:20:23,640 --> 05:20:25,640
this little end piece

6623
05:20:25,640 --> 05:20:27,640
which is going to allow us to

6624
05:20:27,640 --> 05:20:29,640
essentially chat with the model

6625
05:20:29,640 --> 05:20:31,640
this is what it looks like a little wild loop

6626
05:20:31,640 --> 05:20:33,640
we have a prompt we just input

6627
05:20:33,640 --> 05:20:35,640
something

6628
05:20:35,640 --> 05:20:37,640
prompt next line that should be fairly self explanatory

6629
05:20:37,640 --> 05:20:39,640
and we have this tensor

6630
05:20:39,640 --> 05:20:41,640
we're going to encode this prompt into a bunch

6631
05:20:41,640 --> 05:20:43,640
of integers or torch.long data types

6632
05:20:43,640 --> 05:20:45,640
on the GPU

6633
05:20:45,640 --> 05:20:47,640
devices CUDA

6634
05:20:47,640 --> 05:20:49,640
and then after

6635
05:20:49,640 --> 05:20:51,640
after we've actually generated these

6636
05:20:51,640 --> 05:20:53,640
so model.generate

6637
05:20:53,640 --> 05:20:55,640
we're going to unsqueeze these

6638
05:20:55,640 --> 05:20:57,640
remember it's a torch.tensor

6639
05:20:57,640 --> 05:20:59,640
so it's going to be in the matrices form

6640
05:20:59,640 --> 05:21:01,640
so it's going to look like this

6641
05:21:01,640 --> 05:21:03,640
it's going to look like this or whatever

6642
05:21:03,640 --> 05:21:05,640
that's essentially what the shape is

6643
05:21:05,640 --> 05:21:07,640
so all we're doing when we unsqueeze it

6644
05:21:07,640 --> 05:21:09,640
is we're just taking away this wrapping

6645
05:21:09,640 --> 05:21:11,640
around it

6646
05:21:11,640 --> 05:21:13,640
so awesome

6647
05:21:13,640 --> 05:21:15,640
we're just going to do some

6648
05:21:15,640 --> 05:21:17,640
tokens for example 150 here

6649
05:21:17,640 --> 05:21:19,640
and then to a list format

6650
05:21:19,640 --> 05:21:21,640
and then we can just print these out

6651
05:21:21,640 --> 05:21:23,640
as January characters

6652
05:21:23,640 --> 05:21:25,640
awesome so we're just going to ask this prompt

6653
05:21:25,640 --> 05:21:27,640
and then do some compute give us a completion

6654
05:21:27,640 --> 05:21:29,640
so on and so forth

6655
05:21:29,640 --> 05:21:31,640
so that's what this is doing here

6656
05:21:31,640 --> 05:21:33,640
and another thing I wanted to point out

6657
05:21:33,640 --> 05:21:35,640
is actually when we load these

6658
05:21:35,640 --> 05:21:37,640
parameters in

6659
05:21:37,640 --> 05:21:39,640
at least on training

6660
05:21:39,640 --> 05:21:41,640
it's going to initially give us errors

6661
05:21:41,640 --> 05:21:43,640
from we're going to get errors from that

6662
05:21:43,640 --> 05:21:45,640
because the model will just not be

6663
05:21:45,640 --> 05:21:47,640
anything and we won't be able to import stuff

6664
05:21:47,640 --> 05:21:49,640
so that's going to give you errors first of all

6665
05:21:49,640 --> 05:21:51,640
another thing you want to pay attention to

6666
05:21:51,640 --> 05:21:53,640
is to make sure that when you've actually trained

6667
05:21:53,640 --> 05:21:55,640
this initial model that it matches

6668
05:21:55,640 --> 05:21:57,640
all of the architectural

6669
05:21:57,640 --> 05:21:59,640
stuff and the hyper parameters

6670
05:21:59,640 --> 05:22:01,640
that you used

6671
05:22:01,640 --> 05:22:03,640
that when you're using to load up again

6672
05:22:03,640 --> 05:22:05,640
so

6673
05:22:05,640 --> 05:22:07,640
when you're running your forward pass and whatnot

6674
05:22:07,640 --> 05:22:09,640
you just want to make sure that this architecture

6675
05:22:09,640 --> 05:22:11,640
sort of lines up with it

6676
05:22:11,640 --> 05:22:13,640
just so that you don't get any architectural errors

6677
05:22:13,640 --> 05:22:15,640
those can be really confusing to debug

6678
05:22:15,640 --> 05:22:17,640
so yeah

6679
05:22:17,640 --> 05:22:19,640
and the way we can do this is actually just

6680
05:22:19,640 --> 05:22:21,640
commenting it out here

6681
05:22:21,640 --> 05:22:23,640
awesome, we're able to save load models

6682
05:22:23,640 --> 05:22:25,640
and

6683
05:22:25,640 --> 05:22:27,640
we're able to use a little loop

6684
05:22:27,640 --> 05:22:29,640
to create a sort of

6685
05:22:29,640 --> 05:22:31,640
chat-up that's not really helpful

6686
05:22:31,640 --> 05:22:33,640
because we haven't trained it

6687
05:22:33,640 --> 05:22:35,640
an insane amount on

6688
05:22:35,640 --> 05:22:37,640
data that actually is useful

6689
05:22:37,640 --> 05:22:39,640
so another little detail that's very important

6690
05:22:39,640 --> 05:22:41,640
is to actually

6691
05:22:41,640 --> 05:22:43,640
make sure that you have nn-module in all

6692
05:22:43,640 --> 05:22:45,640
of these classes and subclasses

6693
05:22:45,640 --> 05:22:47,640
nn.module basically works

6694
05:22:47,640 --> 05:22:49,640
as a tracker for all of your

6695
05:22:49,640 --> 05:22:51,640
parameters it makes

6696
05:22:51,640 --> 05:22:53,640
make sure that all of your

6697
05:22:53,640 --> 05:22:55,640
nn extensions run correctly

6698
05:22:55,640 --> 05:22:57,640
and just overall a cornerstone

6699
05:22:57,640 --> 05:22:59,640
for PyTorch like you need it

6700
05:22:59,640 --> 05:23:01,640
so make sure you have nn-module in all of these classes

6701
05:23:01,640 --> 05:23:03,640
I know that

6702
05:23:03,640 --> 05:23:05,640
block sort of comes out of GPT

6703
05:23:05,640 --> 05:23:07,640
language model and so on and so forth

6704
05:23:07,640 --> 05:23:09,640
but just all of these

6705
05:23:09,640 --> 05:23:11,640
classes with nn

6706
05:23:11,640 --> 05:23:13,640
or any learnable parameters

6707
05:23:13,640 --> 05:23:15,640
you will need it in it's overall just

6708
05:23:15,640 --> 05:23:17,640
a good practice to have nn-module in all

6709
05:23:17,640 --> 05:23:19,640
of your classes overall

6710
05:23:19,640 --> 05:23:21,640
just to sort of avoid those errors

6711
05:23:21,640 --> 05:23:23,640
so cool

6712
05:23:23,640 --> 05:23:25,640
I didn't explicitly go over that

6713
05:23:25,640 --> 05:23:27,640
at the beginning but that's just a heads up

6714
05:23:27,640 --> 05:23:29,640
you always want to make sure nn-module is inside of these

6715
05:23:29,640 --> 05:23:31,640
so cool

6716
05:23:31,640 --> 05:23:33,640
now

6717
05:23:33,640 --> 05:23:35,640
something I'd like to highlight

6718
05:23:35,640 --> 05:23:37,640
is a little error that we get

6719
05:23:37,640 --> 05:23:39,640
we try to generate when we have max new

6720
05:23:39,640 --> 05:23:41,640
tokens above block size so let me show you

6721
05:23:41,640 --> 05:23:43,640
that right now

6722
05:23:43,640 --> 05:23:45,640
you just go python, chat bot

6723
05:23:45,640 --> 05:23:47,640
and then batch size 32

6724
05:23:47,640 --> 05:23:49,640
so we could say

6725
05:23:49,640 --> 05:23:51,640
we could say hello

6726
05:23:51,640 --> 05:23:53,640
for example

6727
05:23:55,640 --> 05:23:57,640
okay so it's going to give us

6728
05:23:57,640 --> 05:23:59,640
some errors here and what exactly

6729
05:23:59,640 --> 05:24:01,640
does this error mean

6730
05:24:01,640 --> 05:24:03,640
well when we try to

6731
05:24:03,640 --> 05:24:05,640
generate 150 new

6732
05:24:05,640 --> 05:24:07,640
tokens what it's doing

6733
05:24:07,640 --> 05:24:09,640
is it's taking the previous

6734
05:24:09,640 --> 05:24:11,640
you know

6735
05:24:11,640 --> 05:24:13,640
H-E-L-L-O

6736
05:24:13,640 --> 05:24:15,640
exclamation mark 6 tokens

6737
05:24:15,640 --> 05:24:17,640
and it's pretty much adding up 150

6738
05:24:17,640 --> 05:24:19,640
on top of that so we have

6739
05:24:19,640 --> 05:24:21,640
156 tokens

6740
05:24:21,640 --> 05:24:23,640
that we're now trying to fit inside of block size

6741
05:24:23,640 --> 05:24:25,640
which in our case is

6742
05:24:25,640 --> 05:24:27,640
128

6743
05:24:27,640 --> 05:24:29,640
so of course

6744
05:24:29,640 --> 05:24:31,640
156 does not fit

6745
05:24:31,640 --> 05:24:33,640
into 128 and that's

6746
05:24:33,640 --> 05:24:35,640
why we get some errors here

6747
05:24:35,640 --> 05:24:37,640
so

6748
05:24:37,640 --> 05:24:39,640
all we have to do is make sure

6749
05:24:39,640 --> 05:24:41,640
that

6750
05:24:41,640 --> 05:24:43,640
we essentially

6751
05:24:43,640 --> 05:24:45,640
what we could do is make sure that max new tokens

6752
05:24:45,640 --> 05:24:47,640
is small enough and then be sort of

6753
05:24:47,640 --> 05:24:49,640
paying attention when we make prompts

6754
05:24:49,640 --> 05:24:51,640
or

6755
05:24:51,640 --> 05:24:53,640
we could actually make a little

6756
05:24:53,640 --> 05:24:55,640
cropping

6757
05:24:55,640 --> 05:24:57,640
cropping tool here so what this will do

6758
05:24:57,640 --> 05:24:59,640
is it will pretty much crop

6759
05:24:59,640 --> 05:25:01,640
through the last block size tokens

6760
05:25:01,640 --> 05:25:03,640
and

6761
05:25:03,640 --> 05:25:05,640
this is super useful because it

6762
05:25:05,640 --> 05:25:07,640
pretty much doesn't make us have to pay

6763
05:25:07,640 --> 05:25:09,640
attention to max new tokens all the time

6764
05:25:09,640 --> 05:25:11,640
and it just essentially

6765
05:25:11,640 --> 05:25:13,640
crops it around that 128 limit

6766
05:25:13,640 --> 05:25:15,640
so

6767
05:25:15,640 --> 05:25:17,640
I'm going to go ahead and replace index here

6768
05:25:17,640 --> 05:25:19,640
with index con or index condition

6769
05:25:19,640 --> 05:25:21,640
and

6770
05:25:21,640 --> 05:25:23,640
we go ahead and run this again

6771
05:25:27,640 --> 05:25:29,640
so I could say hello

6772
05:25:31,640 --> 05:25:33,640
and we get a successful

6773
05:25:33,640 --> 05:25:35,640
completion awesome

6774
05:25:35,640 --> 05:25:37,640
we can keep asking new prompts like this

6775
05:25:37,640 --> 05:25:39,640
right

6776
05:25:43,640 --> 05:25:45,640
and awesome so

6777
05:25:45,640 --> 05:25:47,640
yeah we're not really getting any of these

6778
05:25:47,640 --> 05:25:49,640
dimensionality like

6779
05:25:49,640 --> 05:25:51,640
architecture fitting type errors if you want to call them

6780
05:25:51,640 --> 05:25:53,640
if you want to make it super fancy that way

6781
05:25:53,640 --> 05:25:55,640
but yeah

6782
05:25:55,640 --> 05:25:57,640
not really that much else to do

6783
05:25:57,640 --> 05:25:59,640
yeah there's a few points I want to go over

6784
05:25:59,640 --> 05:26:01,640
including fine tuning

6785
05:26:01,640 --> 05:26:03,640
so I'm going to go over a little

6786
05:26:03,640 --> 05:26:05,640
illustrative example as to what

6787
05:26:05,640 --> 05:26:07,640
fine tuning actually looks like in practice

6788
05:26:07,640 --> 05:26:09,640
so in pre-training

6789
05:26:09,640 --> 05:26:11,640
which is what this course is based off of

6790
05:26:11,640 --> 05:26:13,640
in pre-training you have this

6791
05:26:13,640 --> 05:26:15,640
giant text corpus right you have this

6792
05:26:15,640 --> 05:26:17,640
giant corpus here

6793
05:26:19,640 --> 05:26:21,640
some text in it

6794
05:26:21,640 --> 05:26:23,640
and essentially

6795
05:26:23,640 --> 05:26:25,640
what you do is you take out little snippets

6796
05:26:25,640 --> 05:26:27,640
these are called

6797
05:26:27,640 --> 05:26:29,640
blocks or batches

6798
05:26:29,640 --> 05:26:31,640
or chunks you could say you take out little batches

6799
05:26:31,640 --> 05:26:33,640
of these you sample

6800
05:26:33,640 --> 05:26:35,640
random little blocks and you take multiple batches

6801
05:26:35,640 --> 05:26:37,640
of them and

6802
05:26:37,640 --> 05:26:39,640
you essentially have this

6803
05:26:39,640 --> 05:26:41,640
let's just say

6804
05:26:41,640 --> 05:26:43,640
H E L L O

6805
05:26:43,640 --> 05:26:45,640
and maybe the next

6806
05:26:45,640 --> 05:26:47,640
predict maybe the outputs

6807
05:26:47,640 --> 05:26:49,640
or the targets rather

6808
05:26:49,640 --> 05:26:51,640
or

6809
05:26:51,640 --> 05:26:53,640
the L L O

6810
05:26:53,640 --> 05:26:55,640
exclamation mark

6811
05:26:55,640 --> 05:26:57,640
so it's just shifted over by one

6812
05:26:57,640 --> 05:26:59,640
and so given this

6813
05:26:59,640 --> 05:27:01,640
sequence of characters

6814
05:27:01,640 --> 05:27:03,640
you want to predict this which is just

6815
05:27:03,640 --> 05:27:05,640
the input shifted by one

6816
05:27:05,640 --> 05:27:07,640
that's what pre-training is

6817
05:27:07,640 --> 05:27:09,640
and keep in mind that these are the same size

6818
05:27:09,640 --> 05:27:11,640
this is one, two,

6819
05:27:11,640 --> 05:27:13,640
three, four, and five

6820
05:27:13,640 --> 05:27:15,640
same thing here these are both

6821
05:27:15,640 --> 05:27:17,640
five characters long

6822
05:27:17,640 --> 05:27:19,640
fine tuning however is not completely the same

6823
05:27:19,640 --> 05:27:21,640
so I could have

6824
05:27:21,640 --> 05:27:23,640
hello

6825
05:27:23,640 --> 05:27:25,640
and then maybe like a question mark

6826
05:27:25,640 --> 05:27:27,640
and it would respond

6827
05:27:27,640 --> 05:27:29,640
you know

6828
05:27:31,640 --> 05:27:33,640
the model might respond

6829
05:27:33,640 --> 05:27:35,640
L R U

6830
05:27:35,640 --> 05:27:37,640
maybe that's just a

6831
05:27:37,640 --> 05:27:39,640
a response that it gives us

6832
05:27:39,640 --> 05:27:41,640
we can obviously see that hello does not have the same amount of characters

6833
05:27:41,640 --> 05:27:43,640
with the same amount of indices

6834
05:27:43,640 --> 05:27:45,640
as how are you

6835
05:27:45,640 --> 05:27:47,640
so

6836
05:27:47,640 --> 05:27:49,640
this is essentially the difference between

6837
05:27:49,640 --> 05:27:51,640
fine tuning and pre-training

6838
05:27:51,640 --> 05:27:53,640
with fine tuning you just have to add a little bit of

6839
05:27:53,640 --> 05:27:55,640
different things in your generate function

6840
05:27:55,640 --> 05:27:57,640
to compensate for not having

6841
05:27:57,640 --> 05:27:59,640
the same

6842
05:27:59,640 --> 05:28:01,640
amount of indices in your inputs

6843
05:28:01,640 --> 05:28:03,640
and targets and rather just

6844
05:28:03,640 --> 05:28:05,640
generate until you receive an end token

6845
05:28:05,640 --> 05:28:07,640
so

6846
05:28:07,640 --> 05:28:09,640
what they don't explicitly say here is at the

6847
05:28:09,640 --> 05:28:11,640
end of this question

6848
05:28:11,640 --> 05:28:13,640
there's actually a little end token which we usually

6849
05:28:13,640 --> 05:28:15,640
do

6850
05:28:15,640 --> 05:28:17,640
looks like this

6851
05:28:17,640 --> 05:28:19,640
like that

6852
05:28:19,640 --> 05:28:21,640
or

6853
05:28:21,640 --> 05:28:23,640
like this

6854
05:28:23,640 --> 05:28:25,640
these are end tokens and then you typically

6855
05:28:25,640 --> 05:28:27,640
have the same for start tokens like an s

6856
05:28:27,640 --> 05:28:29,640
or

6857
05:28:29,640 --> 05:28:31,640
start

6858
05:28:31,640 --> 05:28:33,640
like that, pretty simple

6859
05:28:33,640 --> 05:28:35,640
and essentially you would just append them

6860
05:28:37,640 --> 05:28:39,640
and

6861
05:28:39,640 --> 05:28:41,640
a start token

6862
05:28:41,640 --> 05:28:43,640
the start token doesn't matter as much

6863
05:28:43,640 --> 05:28:45,640
as we essentially just are looking at

6864
05:28:45,640 --> 05:28:47,640
what this does and then

6865
05:28:47,640 --> 05:28:49,640
we start generating the start doesn't really

6866
05:28:49,640 --> 05:28:51,640
matter because

6867
05:28:51,640 --> 05:28:53,640
we don't really need to know when to start generating

6868
05:28:53,640 --> 05:28:55,640
it just happens but the end token is

6869
05:28:55,640 --> 05:28:57,640
important because we don't want to just generate

6870
05:28:57,640 --> 05:28:59,640
an infinite number of tokens

6871
05:28:59,640 --> 05:29:01,640
because these aren't the same size

6872
05:29:01,640 --> 05:29:03,640
it could theoretically generate a really

6873
05:29:03,640 --> 05:29:05,640
really long completion

6874
05:29:05,640 --> 05:29:07,640
so all we want to make sure

6875
05:29:07,640 --> 05:29:09,640
is that it's not generating an infinite amount of tokens

6876
05:29:09,640 --> 05:29:11,640
consuming an infinite amount of computation

6877
05:29:11,640 --> 05:29:13,640
and just to prevent that loop

6878
05:29:13,640 --> 05:29:15,640
so that's why we append this end token

6879
05:29:15,640 --> 05:29:17,640
to the end here

6880
05:29:19,640 --> 05:29:21,640
we have this little end bit

6881
05:29:21,640 --> 05:29:23,640
and

6882
05:29:23,640 --> 05:29:25,640
essentially once this end token is sampled

6883
05:29:25,640 --> 05:29:27,640
you would end the generation

6884
05:29:27,640 --> 05:29:29,640
simple as that

6885
05:29:29,640 --> 05:29:31,640
and we don't actually

6886
05:29:31,640 --> 05:29:33,640
sample from the token itself

6887
05:29:33,640 --> 05:29:35,640
but rather the actual

6888
05:29:35,640 --> 05:29:37,640
the

6889
05:29:37,640 --> 05:29:39,640
I guess you could say index

6890
05:29:39,640 --> 05:29:41,640
or the miracle value

6891
05:29:41,640 --> 05:29:43,640
the encoded version of end

6892
05:29:43,640 --> 05:29:45,640
which

6893
05:29:45,640 --> 05:29:47,640
is usually just going to be the length of your vocab

6894
05:29:47,640 --> 05:29:49,640
size

6895
05:29:49,640 --> 05:29:51,640
plus one

6896
05:29:51,640 --> 05:29:53,640
so if your vocab size in our case

6897
05:29:53,640 --> 05:29:55,640
is like maybe 32,000

6898
05:29:55,640 --> 05:29:57,640
your end token would be at index

6899
05:29:57,640 --> 05:29:59,640
32,001

6900
05:29:59,640 --> 05:30:01,640
so that way when you sample

6901
05:30:01,640 --> 05:30:03,640
when you sample an end token

6902
05:30:03,640 --> 05:30:05,640
when you sample that

6903
05:30:05,640 --> 05:30:07,640
32,001 token

6904
05:30:09,640 --> 05:30:11,640
you actually just end the sequence

6905
05:30:11,640 --> 05:30:13,640
and of course when you train

6906
05:30:13,640 --> 05:30:15,640
your model you're always

6907
05:30:15,640 --> 05:30:17,640
appending this end token to the end

6908
05:30:17,640 --> 05:30:19,640
so you get your initial inputs

6909
05:30:19,640 --> 05:30:21,640
and then inside of either your

6910
05:30:21,640 --> 05:30:23,640
training data

6911
05:30:23,640 --> 05:30:25,640
or when you actually are processing it

6912
05:30:25,640 --> 05:30:27,640
and feeding it into that transformer

6913
05:30:27,640 --> 05:30:29,640
you have some sort of function that's just appending

6914
05:30:29,640 --> 05:30:31,640
that little

6915
05:30:31,640 --> 05:30:33,640
32,001 token index

6916
05:30:33,640 --> 05:30:35,640
to it

6917
05:30:35,640 --> 05:30:37,640
so that's pretty much what fine tuning is

6918
05:30:37,640 --> 05:30:39,640
it comes up fine tuning

6919
05:30:39,640 --> 05:30:41,640
and the whole process of creating

6920
05:30:41,640 --> 05:30:43,640
these giant language models

6921
05:30:43,640 --> 05:30:45,640
is to of course help people

6922
05:30:45,640 --> 05:30:47,640
and there's no better way to do that

6923
05:30:47,640 --> 05:30:49,640
than to

6924
05:30:49,640 --> 05:30:51,640
literally have all the information

6925
05:30:51,640 --> 05:30:53,640
that humans have ever known meaning like common crawl

6926
05:30:53,640 --> 05:30:55,640
open web text or Wikipedia

6927
05:30:55,640 --> 05:30:57,640
and even research papers

6928
05:30:57,640 --> 05:30:59,640
pre-training on all of that

6929
05:30:59,640 --> 05:31:01,640
so just doing again the same size

6930
05:31:01,640 --> 05:31:03,640
and then shift over for targets

6931
05:31:03,640 --> 05:31:05,640
and then after you've iterated on that

6932
05:31:05,640 --> 05:31:07,640
many many times you switch over to fine tuning

6933
05:31:07,640 --> 05:31:09,640
where you have these

6934
05:31:09,640 --> 05:31:11,640
specifically picked out

6935
05:31:11,640 --> 05:31:13,640
prompt and completion pairs

6936
05:31:13,640 --> 05:31:15,640
and you just train on those for a really long time

6937
05:31:15,640 --> 05:31:17,640
until you are satisfied

6938
05:31:17,640 --> 05:31:19,640
with your result

6939
05:31:19,640 --> 05:31:21,640
and yeah that's what language modeling is

6940
05:31:21,640 --> 05:31:23,640
there are a few key pointers I want to leave you with

6941
05:31:23,640 --> 05:31:25,640
before you head on your way to

6942
05:31:25,640 --> 05:31:27,640
research and development and machine learning

6943
05:31:27,640 --> 05:31:29,640
so first things first

6944
05:31:29,640 --> 05:31:31,640
there's a little something called

6945
05:31:31,640 --> 05:31:33,640
efficiency testing

6946
05:31:33,640 --> 05:31:35,640
or just finding out how quickly

6947
05:31:35,640 --> 05:31:37,640
certain operations takes

6948
05:31:37,640 --> 05:31:39,640
we'll just call this

6949
05:31:39,640 --> 05:31:41,640
efficiency testing and I'll show you

6950
05:31:41,640 --> 05:31:43,640
exactly how to do this right here

6951
05:31:43,640 --> 05:31:45,640
efficiency

6952
05:31:45,640 --> 05:31:47,640
yeah

6953
05:31:47,640 --> 05:31:49,640
I don't know if I spelled that correctly

6954
05:31:49,640 --> 05:31:51,640
I don't know what it's doing now

6955
05:31:51,640 --> 05:31:53,640
anyways

6956
05:31:53,640 --> 05:31:55,640
we'll just pop into code here

6957
05:31:55,640 --> 05:31:57,640
and

6958
05:31:57,640 --> 05:31:59,640
essentially

6959
05:31:59,640 --> 05:32:01,640
we'll just do

6960
05:32:01,640 --> 05:32:03,640
I don't know

6961
05:32:03,640 --> 05:32:05,640
I'm testing

6962
05:32:05,640 --> 05:32:07,640
import time

6963
05:32:07,640 --> 05:32:09,640
and

6964
05:32:09,640 --> 05:32:11,640
essentially

6965
05:32:11,640 --> 05:32:13,640
all we're going to do is just

6966
05:32:13,640 --> 05:32:15,640
time how long operations take

6967
05:32:15,640 --> 05:32:17,640
so

6968
05:32:17,640 --> 05:32:19,640
in here you can go

6969
05:32:19,640 --> 05:32:21,640
you can go start time

6970
05:32:21,640 --> 05:32:23,640
equals time dot time

6971
05:32:23,640 --> 05:32:25,640
and essentially what this function does

6972
05:32:25,640 --> 05:32:27,640
is it just takes a look at the current time right now

6973
05:32:27,640 --> 05:32:29,640
the current like millisecond

6974
05:32:29,640 --> 05:32:31,640
very precise

6975
05:32:31,640 --> 05:32:33,640
and we can do some little

6976
05:32:33,640 --> 05:32:35,640
operation like

6977
05:32:35,640 --> 05:32:37,640
I don't know 4

6978
05:32:37,640 --> 05:32:39,640
I in range

6979
05:32:41,640 --> 05:32:43,640
we'll just go

6980
05:32:43,640 --> 05:32:45,640
10,000

6981
05:32:45,640 --> 05:32:47,640
go

6982
05:32:47,640 --> 05:32:49,640
print

6983
05:32:49,640 --> 05:32:51,640
I

6984
05:32:51,640 --> 05:32:53,640
print I times 2

6985
05:32:53,640 --> 05:32:55,640
and then we can just end the time here

6986
05:32:55,640 --> 05:32:57,640
so go end time

6987
05:32:57,640 --> 05:32:59,640
equals time dot time again

6988
05:32:59,640 --> 05:33:01,640
calling the current time so we're doing

6989
05:33:01,640 --> 05:33:03,640
right now versus back then

6990
05:33:03,640 --> 05:33:05,640
and that little difference is how long it took to execute

6991
05:33:05,640 --> 05:33:07,640
so all we can do

6992
05:33:07,640 --> 05:33:09,640
is just do we can say total time

6993
05:33:09,640 --> 05:33:11,640
we can say total time equals

6994
05:33:11,640 --> 05:33:13,640
end time

6995
05:33:13,640 --> 05:33:15,640
minus start time

6996
05:33:15,640 --> 05:33:17,640
and we'll just go print

6997
05:33:17,640 --> 05:33:19,640
end time

6998
05:33:19,640 --> 05:33:21,640
or

6999
05:33:23,640 --> 05:33:25,640
I'm taking

7000
05:33:25,640 --> 05:33:27,640
let's go total

7001
05:33:29,640 --> 05:33:31,640
total time like that

7002
05:33:31,640 --> 05:33:33,640
just execute this

7003
05:33:35,640 --> 05:33:37,640
Python

7004
05:33:37,640 --> 05:33:39,640
time testing

7005
05:33:39,640 --> 05:33:41,640
cool

7006
05:33:41,640 --> 05:33:43,640
time taken 1.32 seconds

7007
05:33:43,640 --> 05:33:45,640
so you can essentially time every single operation

7008
05:33:45,640 --> 05:33:47,640
you do with this method

7009
05:33:47,640 --> 05:33:49,640
and you can see even in your

7010
05:33:49,640 --> 05:33:51,640
I encourage you to actually try this out

7011
05:33:51,640 --> 05:33:53,640
I'm not going to but I encourage you to try out

7012
05:33:53,640 --> 05:33:55,640
how long the model actually takes

7013
05:33:55,640 --> 05:33:57,640
to do certain things like how long does it take

7014
05:33:57,640 --> 05:33:59,640
to load a model how does it take to save a model

7015
05:33:59,640 --> 05:34:01,640
how long does it take to estimate the loss

7016
05:34:01,640 --> 05:34:03,640
right

7017
05:34:03,640 --> 05:34:05,640
play around with hyperparameters see how long things take

7018
05:34:05,640 --> 05:34:07,640
and maybe you'll figure out something new who knows

7019
05:34:07,640 --> 05:34:09,640
but this is a little something we use

7020
05:34:09,640 --> 05:34:11,640
to pretty much test how long something

7021
05:34:11,640 --> 05:34:13,640
takes how efficient it is

7022
05:34:13,640 --> 05:34:15,640
and then to also see if

7023
05:34:15,640 --> 05:34:17,640
it's worth investigating a new way of approaching

7024
05:34:17,640 --> 05:34:19,640
something in case it takes

7025
05:34:19,640 --> 05:34:21,640
ridiculous amount of time

7026
05:34:21,640 --> 05:34:23,640
so that's time testing

7027
05:34:23,640 --> 05:34:25,640
and efficiency testing for you

7028
05:34:25,640 --> 05:34:27,640
the next little bit I want to cover

7029
05:34:27,640 --> 05:34:29,640
is the history

7030
05:34:29,640 --> 05:34:31,640
I'm not going to go over the entire history

7031
05:34:31,640 --> 05:34:33,640
of AI and LLMs

7032
05:34:33,640 --> 05:34:35,640
but essentially

7033
05:34:35,640 --> 05:34:37,640
we originated with something called RNNs

7034
05:34:37,640 --> 05:34:39,640
okay RNNs are called

7035
05:34:39,640 --> 05:34:41,640
recurrent neural networks

7036
05:34:41,640 --> 05:34:43,640
and they're really inefficient

7037
05:34:43,640 --> 05:34:45,640
at least for scaled

7038
05:34:45,640 --> 05:34:47,640
AI systems so RNNs

7039
05:34:47,640 --> 05:34:49,640
are a little essentially think of it as a little loop

7040
05:34:49,640 --> 05:34:51,640
keeps learning and learning

7041
05:34:51,640 --> 05:34:53,640
and this is sequential right

7042
05:34:53,640 --> 05:34:55,640
it does this and then this and then this

7043
05:34:55,640 --> 05:34:57,640
has to wait for each completion

7044
05:34:57,640 --> 05:34:59,640
synchronous you can't have multiple of them at once

7045
05:34:59,640 --> 05:35:01,640
because they're complex

7046
05:35:01,640 --> 05:35:03,640
GPUs cannot run complex things

7047
05:35:03,640 --> 05:35:05,640
they're only designed for just

7048
05:35:05,640 --> 05:35:07,640
matrix multiplication and very simple

7049
05:35:07,640 --> 05:35:09,640
math like that

7050
05:35:09,640 --> 05:35:11,640
so RNNs are essentially

7051
05:35:11,640 --> 05:35:13,640
a little bit dumber than transformers

7052
05:35:13,640 --> 05:35:15,640
and they

7053
05:35:15,640 --> 05:35:17,640
are run on the CPU

7054
05:35:17,640 --> 05:35:19,640
so RNNs was where we last sort of stopped at

7055
05:35:19,640 --> 05:35:21,640
and what I encourage you to do

7056
05:35:21,640 --> 05:35:23,640
is look into more of the language

7057
05:35:23,640 --> 05:35:25,640
modeling and AI

7058
05:35:25,640 --> 05:35:27,640
history and research that has led up to this

7059
05:35:27,640 --> 05:35:29,640
point so you can have an idea

7060
05:35:29,640 --> 05:35:31,640
as to how researchers

7061
05:35:31,640 --> 05:35:33,640
have been able to quickly innovate

7062
05:35:33,640 --> 05:35:35,640
given

7063
05:35:35,640 --> 05:35:37,640
all these historical innovations

7064
05:35:37,640 --> 05:35:39,640
so you have like all these things leading up to the transformer

7065
05:35:39,640 --> 05:35:41,640
well how did they all

7066
05:35:41,640 --> 05:35:43,640
philosophize

7067
05:35:43,640 --> 05:35:45,640
up to that point

7068
05:35:45,640 --> 05:35:47,640
and yeah it's just

7069
05:35:47,640 --> 05:35:49,640
something good to sort of be confident

7070
05:35:49,640 --> 05:35:51,640
in is innovating

7071
05:35:51,640 --> 05:35:53,640
as both a researcher

7072
05:35:53,640 --> 05:35:55,640
and engineer and a

7073
05:35:55,640 --> 05:35:57,640
business person

7074
05:35:57,640 --> 05:35:59,640
so cool

7075
05:35:59,640 --> 05:36:01,640
RNNs were where we sort of

7076
05:36:01,640 --> 05:36:03,640
finished off and now it's transformers and GPTs

7077
05:36:03,640 --> 05:36:05,640
that's the current state of AI

7078
05:36:05,640 --> 05:36:07,640
next up I

7079
05:36:07,640 --> 05:36:09,640
would like to go over something called

7080
05:36:09,640 --> 05:36:11,640
quantization

7081
05:36:11,640 --> 05:36:13,640
so quantization is essentially

7082
05:36:13,640 --> 05:36:15,640
a way to reduce the memory

7083
05:36:15,640 --> 05:36:17,640
usage by your parameters

7084
05:36:17,640 --> 05:36:19,640
so there's actually a paper here

7085
05:36:19,640 --> 05:36:21,640
called QLaura Efficient Fine

7086
05:36:21,640 --> 05:36:23,640
Tuning of Quantized

7087
05:36:23,640 --> 05:36:25,640
LLMs so

7088
05:36:25,640 --> 05:36:27,640
all this does in simple

7089
05:36:27,640 --> 05:36:29,640
form is pretty much instead of

7090
05:36:29,640 --> 05:36:31,640
using 32 bit floating

7091
05:36:31,640 --> 05:36:33,640
point numbers it goes not only

7092
05:36:33,640 --> 05:36:35,640
to 16 bit of half precision

7093
05:36:35,640 --> 05:36:37,640
but all the way down to 4

7094
05:36:37,640 --> 05:36:39,640
so what this actually

7095
05:36:39,640 --> 05:36:41,640
looks like is in binary code

7096
05:36:41,640 --> 05:36:43,640
or in bytecode

7097
05:36:43,640 --> 05:36:45,640
it will look

7098
05:36:45,640 --> 05:36:47,640
here there's some array

7099
05:36:47,640 --> 05:36:49,640
of numbers

7100
05:36:49,640 --> 05:36:51,640
that it uses

7101
05:36:57,640 --> 05:36:59,640
okay I can't find it

7102
05:36:59,640 --> 05:37:01,640
but pretty much what it is

7103
05:37:01,640 --> 05:37:03,640
it is a bunch of

7104
05:37:03,640 --> 05:37:05,640
it's a bunch of floating point numbers

7105
05:37:05,640 --> 05:37:07,640
and they're all between

7106
05:37:07,640 --> 05:37:09,640
negative one and one

7107
05:37:09,640 --> 05:37:11,640
and there are 16 of them

7108
05:37:11,640 --> 05:37:13,640
if you have a 4 bit number

7109
05:37:13,640 --> 05:37:15,640
that means it can hold 16 different

7110
05:37:15,640 --> 05:37:17,640
values 0 through 15

7111
05:37:17,640 --> 05:37:19,640
which is 16 values

7112
05:37:19,640 --> 05:37:21,640
and all you pretty much do is you have this

7113
05:37:21,640 --> 05:37:23,640
array of floating point numbers

7114
05:37:23,640 --> 05:37:25,640
you use the bytecode of

7115
05:37:25,640 --> 05:37:27,640
that 4 bit

7116
05:37:27,640 --> 05:37:29,640
number to look up the index

7117
05:37:29,640 --> 05:37:31,640
in that array and that is your weight

7118
05:37:31,640 --> 05:37:33,640
that is the weight they use

7119
05:37:33,640 --> 05:37:35,640
in your model

7120
05:37:35,640 --> 05:37:37,640
so this way instead of using 32 bit

7121
05:37:37,640 --> 05:37:39,640
having these super long numbers

7122
05:37:39,640 --> 05:37:41,640
that are super precise

7123
05:37:41,640 --> 05:37:43,640
you can have super precise numbers

7124
05:37:43,640 --> 05:37:45,640
that are just generally good parameters

7125
05:37:45,640 --> 05:37:47,640
to have that just perform

7126
05:37:47,640 --> 05:37:49,640
decently

7127
05:37:49,640 --> 05:37:51,640
they're just sort of well spread out

7128
05:37:51,640 --> 05:37:53,640
and experimented on and they just

7129
05:37:53,640 --> 05:37:55,640
happen to work and you have 16 of them

7130
05:37:55,640 --> 05:37:57,640
instead of a lot

7131
05:37:57,640 --> 05:37:59,640
so that's

7132
05:37:59,640 --> 05:38:01,640
another cool little thing that's going on right now

7133
05:38:01,640 --> 05:38:03,640
is 4 bit quantizations

7134
05:38:03,640 --> 05:38:05,640
it's a little bit harder

7135
05:38:05,640 --> 05:38:07,640
to implement

7136
05:38:07,640 --> 05:38:09,640
I would encourage you to experiment with half precision

7137
05:38:09,640 --> 05:38:11,640
meaning 16 bit

7138
05:38:11,640 --> 05:38:13,640
floating point numbers

7139
05:38:13,640 --> 05:38:15,640
so that means it occupies

7140
05:38:15,640 --> 05:38:17,640
16 on and off switches

7141
05:38:17,640 --> 05:38:19,640
or capacitors on your GPU

7142
05:38:19,640 --> 05:38:21,640
and

7143
05:38:21,640 --> 05:38:23,640
so quantization is cool to

7144
05:38:23,640 --> 05:38:25,640
sort of scale down the memory

7145
05:38:25,640 --> 05:38:27,640
so that way you can scale up all of your hyper parameters

7146
05:38:27,640 --> 05:38:29,640
and have a more complex model

7147
05:38:29,640 --> 05:38:31,640
with these

7148
05:38:31,640 --> 05:38:33,640
yeah just essentially to have bigger models

7149
05:38:33,640 --> 05:38:35,640
with less space

7150
05:38:35,640 --> 05:38:37,640
take it up

7151
05:38:37,640 --> 05:38:39,640
so that is

7152
05:38:39,640 --> 05:38:41,640
quantization

7153
05:38:41,640 --> 05:38:43,640
and this is the paper for it

7154
05:38:43,640 --> 05:38:45,640
it's a little link you can search out if you want to get

7155
05:38:45,640 --> 05:38:47,640
more familiar with this see

7156
05:38:47,640 --> 05:38:49,640
sort of performance standards and what not

7157
05:38:49,640 --> 05:38:51,640
the next thing I'd like to cover

7158
05:38:51,640 --> 05:38:53,640
is gradient accumulation

7159
05:38:53,640 --> 05:38:55,640
so you might have heard of this you might not have heard of this

7160
05:38:55,640 --> 05:38:57,640
gradient accumulation

7161
05:38:57,640 --> 05:38:59,640
will

7162
05:38:59,640 --> 05:39:01,640
what gradient accumulation does

7163
05:39:01,640 --> 05:39:03,640
is it will accumulate

7164
05:39:03,640 --> 05:39:05,640
gradients

7165
05:39:05,640 --> 05:39:07,640
over say we just set a variable

7166
05:39:07,640 --> 05:39:09,640
x so every x iterations

7167
05:39:09,640 --> 05:39:11,640
it'll just accumulate those

7168
05:39:11,640 --> 05:39:13,640
iterations, average them

7169
05:39:13,640 --> 05:39:15,640
and what this allows you to do

7170
05:39:15,640 --> 05:39:17,640
is instead of

7171
05:39:17,640 --> 05:39:19,640
updating each iteration

7172
05:39:19,640 --> 05:39:21,640
you're updating every x iterations

7173
05:39:21,640 --> 05:39:23,640
so that allows you to fit

7174
05:39:23,640 --> 05:39:25,640
more parameters and more info

7175
05:39:25,640 --> 05:39:27,640
or generalization into this one piece

7176
05:39:27,640 --> 05:39:29,640
so that way when you

7177
05:39:29,640 --> 05:39:31,640
update your parameters

7178
05:39:31,640 --> 05:39:33,640
it's able to generalize more

7179
05:39:33,640 --> 05:39:35,640
over maybe a higher batch size

7180
05:39:35,640 --> 05:39:37,640
or a higher block size

7181
05:39:37,640 --> 05:39:39,640
so when you distribute this

7182
05:39:39,640 --> 05:39:41,640
over many

7183
05:39:41,640 --> 05:39:43,640
iterations and average them

7184
05:39:43,640 --> 05:39:45,640
you can fit more into each iteration

7185
05:39:45,640 --> 05:39:47,640
because it's sort of calculating

7186
05:39:47,640 --> 05:39:49,640
all of them combined

7187
05:39:49,640 --> 05:39:51,640
so yeah that's a cool little trick

7188
05:39:51,640 --> 05:39:53,640
you can use if

7189
05:39:53,640 --> 05:39:55,640
your GPU maybe isn't

7190
05:39:55,640 --> 05:39:57,640
as big if it doesn't have as much

7191
05:39:57,640 --> 05:39:59,640
VRAM on it

7192
05:39:59,640 --> 05:40:01,640
so gradient accumulation is wonderful

7193
05:40:01,640 --> 05:40:03,640
and it's used lots in practice

7194
05:40:03,640 --> 05:40:05,640
the final thing I'd like to leave

7195
05:40:05,640 --> 05:40:07,640
you guys off with is something called

7196
05:40:07,640 --> 05:40:09,640
hugging face and you've probably

7197
05:40:09,640 --> 05:40:11,640
heard a lot about this so far

7198
05:40:11,640 --> 05:40:13,640
but let me just guide you through

7199
05:40:13,640 --> 05:40:15,640
and show you how absolutely explosive

7200
05:40:15,640 --> 05:40:17,640
hugging face is

7201
05:40:17,640 --> 05:40:19,640
for machine learning so you have

7202
05:40:19,640 --> 05:40:21,640
a bunch of models, data sets

7203
05:40:21,640 --> 05:40:23,640
spaces, docs, etc

7204
05:40:23,640 --> 05:40:25,640
and

7205
05:40:25,640 --> 05:40:27,640
let's go to models for example

7206
05:40:27,640 --> 05:40:29,640
so let's just showcase how cool this is

7207
05:40:29,640 --> 05:40:31,640
you have multimodal AIs which could be

7208
05:40:31,640 --> 05:40:33,640
like

7209
05:40:33,640 --> 05:40:35,640
image and text or video

7210
05:40:35,640 --> 05:40:37,640
etc you have multiple different modes

7211
05:40:37,640 --> 05:40:39,640
so it's not just text or not just video

7212
05:40:39,640 --> 05:40:41,640
it's many different ones at the same time

7213
05:40:41,640 --> 05:40:43,640
so you have multimodal models

7214
05:40:43,640 --> 05:40:45,640
you have computer vision

7215
05:40:45,640 --> 05:40:47,640
you have natural language processing

7216
05:40:47,640 --> 05:40:49,640
and we're actually doing natural language

7217
05:40:49,640 --> 05:40:51,640
processing in this course

7218
05:40:51,640 --> 05:40:53,640
we have audio, a tabular

7219
05:40:53,640 --> 05:40:55,640
and reinforcement learning

7220
05:40:55,640 --> 05:40:57,640
so this is really cool

7221
05:40:57,640 --> 05:40:59,640
and you can actually just download these models

7222
05:40:59,640 --> 05:41:01,640
and host them on your own computer

7223
05:41:01,640 --> 05:41:03,640
that is really cool

7224
05:41:03,640 --> 05:41:05,640
you also have data sets which are even cooler

7225
05:41:05,640 --> 05:41:07,640
and these are pretty much

7226
05:41:07,640 --> 05:41:09,640
just really high quality data sets

7227
05:41:09,640 --> 05:41:11,640
of prompt and answer completions

7228
05:41:11,640 --> 05:41:13,640
at least for our purpose

7229
05:41:13,640 --> 05:41:15,640
if you want to use those

7230
05:41:15,640 --> 05:41:17,640
so you have

7231
05:41:17,640 --> 05:41:19,640
question answering

7232
05:41:19,640 --> 05:41:21,640
or conversational

7233
05:41:21,640 --> 05:41:23,640
work data set for example

7234
05:41:23,640 --> 05:41:25,640
as 9000 downloads

7235
05:41:25,640 --> 05:41:27,640
500 likes

7236
05:41:27,640 --> 05:41:29,640
it has a bunch of

7237
05:41:29,640 --> 05:41:31,640
IDs, system prompts

7238
05:41:31,640 --> 05:41:33,640
so you're an AI assistant or whatever

7239
05:41:33,640 --> 05:41:35,640
and then you have the cool stuff which is

7240
05:41:35,640 --> 05:41:37,640
you'll be given a definition of a task first

7241
05:41:37,640 --> 05:41:39,640
and some input of the task etc

7242
05:41:39,640 --> 05:41:41,640
and then the response it's like oh

7243
05:41:41,640 --> 05:41:43,640
we just gave it an input and asked it to answer

7244
05:41:43,640 --> 05:41:45,640
in a format and actually did that

7245
05:41:45,640 --> 05:41:47,640
correctly so

7246
05:41:47,640 --> 05:41:49,640
you could pretty much train these

7247
05:41:49,640 --> 05:41:51,640
on a bunch of

7248
05:41:51,640 --> 05:41:53,640
prompts that you would be able to feed into GPT-4

7249
05:41:53,640 --> 05:41:55,640
and try to make your model perform that way

7250
05:41:55,640 --> 05:41:57,640
and this actually has

7251
05:41:57,640 --> 05:41:59,640
4.23 million rows

7252
05:41:59,640 --> 05:42:01,640
in the training split which is amazing

7253
05:42:01,640 --> 05:42:03,640
so

7254
05:42:03,640 --> 05:42:05,640
data sets are wonderful

7255
05:42:05,640 --> 05:42:07,640
and you can find the best ones

7256
05:42:07,640 --> 05:42:09,640
at least the best fine tuning data sets on OpenORCA

7257
05:42:09,640 --> 05:42:11,640
really good

7258
05:42:11,640 --> 05:42:13,640
as for pre-training

7259
05:42:13,640 --> 05:42:15,640
I believe I mentioned this earlier

7260
05:42:15,640 --> 05:42:17,640
in this survey of large language models

7261
05:42:17,640 --> 05:42:19,640
that we just

7262
05:42:19,640 --> 05:42:21,640
put down through Reddit links

7263
05:42:25,640 --> 05:42:27,640
yep so you could use like OpenWebText

7264
05:42:27,640 --> 05:42:29,640
you could use CommonCrawl

7265
05:42:29,640 --> 05:42:31,640
you could use Books

7266
05:42:31,640 --> 05:42:33,640
you could use Wikipedia

7267
05:42:33,640 --> 05:42:35,640
these are all pre-training data sources

7268
05:42:35,640 --> 05:42:37,640
so yeah

7269
05:42:37,640 --> 05:42:39,640
hopefully that leaves you with a better understanding

7270
05:42:39,640 --> 05:42:41,640
on how to create GPTs, transformers

7271
05:42:41,640 --> 05:42:43,640
and

7272
05:42:43,640 --> 05:42:45,640
pretty good large language models from scratch

7273
05:42:45,640 --> 05:42:47,640
with your own data that you scraped

7274
05:42:47,640 --> 05:42:49,640
or that you downloaded

7275
05:42:49,640 --> 05:42:51,640
and yeah

7276
05:42:51,640 --> 05:42:53,640
that's it, thanks for watching

7277
05:42:53,640 --> 05:42:55,640
so you've learned a ton in this course

7278
05:42:55,640 --> 05:42:57,640
about language modeling

7279
05:42:57,640 --> 05:42:59,640
how to use data, how to create architecture

7280
05:42:59,640 --> 05:43:01,640
from scratch

7281
05:43:01,640 --> 05:43:03,640
maybe even how to look at research papers

7282
05:43:03,640 --> 05:43:05,640
so if you really enjoy this content

7283
05:43:05,640 --> 05:43:07,640
I would encourage you to maybe subscribe

7284
05:43:07,640 --> 05:43:09,640
and like on my YouTube channel

7285
05:43:09,640 --> 05:43:11,640
which is in the description

7286
05:43:11,640 --> 05:43:13,640
I make many videos about AI

7287
05:43:13,640 --> 05:43:15,640
and computer science in general

7288
05:43:15,640 --> 05:43:17,640
so

7289
05:43:17,640 --> 05:43:19,640
you could totally feel free to subscribe there

7290
05:43:19,640 --> 05:43:21,640
if you don't want to subscribe, that's fine

7291
05:43:21,640 --> 05:43:23,640
you could always unsubscribe later if you want to

7292
05:43:23,640 --> 05:43:25,640
it's completely free

7293
05:43:25,640 --> 05:43:27,640
but yeah, also have a GitHub repo in the description

7294
05:43:27,640 --> 05:43:29,640
for all the code that we used

7295
05:43:29,640 --> 05:43:31,640
not the data because it's way too big

7296
05:43:31,640 --> 05:43:33,640
but

7297
05:43:33,640 --> 05:43:35,640
all of the code and the Wizard of Oz

7298
05:43:35,640 --> 05:43:37,640
Text file

7299
05:43:37,640 --> 05:43:39,640
so that's all in the GitHub repo in the description

7300
05:43:39,640 --> 05:43:41,640
thanks for watching

