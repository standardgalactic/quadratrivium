1
00:00:00,000 --> 00:00:04,320
Hello, everybody, and welcome to an absolutely massive TensorFlow

2
00:00:04,320 --> 00:00:07,800
slash machine learning slash artificial intelligence course.

3
00:00:08,000 --> 00:00:11,000
Now, please stick with me for this short introduction, as I am going to give you a

4
00:00:11,000 --> 00:00:15,040
lot of important information regarding the course concept, the resources for the

5
00:00:15,040 --> 00:00:18,120
course and what you can expect after going through this.

6
00:00:18,440 --> 00:00:21,840
Now, first, I will tell you who this course is aimed for.

7
00:00:22,080 --> 00:00:25,440
So this course is aimed for people that are beginners in machine learning

8
00:00:25,440 --> 00:00:28,440
and artificial intelligence, or maybe have a little bit of understanding

9
00:00:28,440 --> 00:00:32,200
that are trying to get better, but do have a basic fundamental knowledge

10
00:00:32,200 --> 00:00:34,040
of programming and Python.

11
00:00:34,320 --> 00:00:36,880
So this is not a course you're going to take if you haven't done any

12
00:00:36,880 --> 00:00:40,280
programming before, or if you don't know any Python syntax in general.

13
00:00:40,640 --> 00:00:43,840
It's going to be highly advised that you understand the basic syntax

14
00:00:43,840 --> 00:00:46,920
behind Python, as I'm not going to be explaining that throughout this course.

15
00:00:47,280 --> 00:00:50,320
Now, in terms of your instructor for this course, that is going to be me.

16
00:00:50,360 --> 00:00:51,280
My name is Tim.

17
00:00:51,280 --> 00:00:54,960
Some of you may know me as Tech with Tim from my YouTube channel, where I teach

18
00:00:54,960 --> 00:00:56,880
all kinds of different programming topics.

19
00:00:57,000 --> 00:00:59,720
And I've actually been working with Free Code Camp and posted some of my

20
00:00:59,720 --> 00:01:01,480
series on their channel as well.

21
00:01:01,880 --> 00:01:04,720
Now, let's get into the course breakdown and talk about exactly what you're

22
00:01:04,720 --> 00:01:06,880
going to learn and what you can expect from this course.

23
00:01:07,120 --> 00:01:09,920
So as this course is geared towards beginners and people just getting

24
00:01:09,920 --> 00:01:13,040
started in the machine learning and AI world, we're going to start by

25
00:01:13,040 --> 00:01:16,600
breaking down exactly what machine learning and artificial intelligence is.

26
00:01:16,720 --> 00:01:20,040
So talking about what the differences are between them, the different types

27
00:01:20,040 --> 00:01:23,760
of machine learning, reinforcement learning, for example, versus neural

28
00:01:23,760 --> 00:01:26,400
networks versus simple machine learning.

29
00:01:26,640 --> 00:01:28,480
We're going to go through all those different differences.

30
00:01:28,680 --> 00:01:31,720
And then we're going to get into a general introduction of TensorFlow.

31
00:01:32,280 --> 00:01:34,800
Now, for those of you that don't know, TensorFlow is a module

32
00:01:34,800 --> 00:01:38,440
developed and maintained by Google, which can be used within Python to do

33
00:01:38,440 --> 00:01:41,400
a ton of different scientific computing, machine learning and

34
00:01:41,400 --> 00:01:43,160
artificial intelligence applications.

35
00:01:43,160 --> 00:01:45,840
We're going to be working with that through the entire tutorial series.

36
00:01:46,080 --> 00:01:48,800
And after we do that general introduction to TensorFlow, we're going

37
00:01:48,800 --> 00:01:51,040
to get into our core learning algorithms.

38
00:01:51,360 --> 00:01:54,120
Now, these are the learning algorithms that you need to know before we can get

39
00:01:54,120 --> 00:01:55,640
further into machine learning.

40
00:01:55,880 --> 00:01:57,680
They build a really strong foundation.

41
00:01:57,840 --> 00:02:01,280
They're pretty easy to understand and implement, and they're extremely powerful.

42
00:02:01,680 --> 00:02:04,760
After we do that, we're going to get into neural networks, discuss all the

43
00:02:04,760 --> 00:02:07,920
different things that go into how neural networks work, how we can use them

44
00:02:07,920 --> 00:02:09,480
and then do a bunch of different examples.

45
00:02:09,760 --> 00:02:12,560
And then we're going to get into some more complex aspects of machine

46
00:02:12,560 --> 00:02:15,840
learning and artificial intelligence and get to convolutional neural networks,

47
00:02:15,840 --> 00:02:18,600
which can do things like image recognition and detection.

48
00:02:18,800 --> 00:02:20,960
And then we're going to get into recurrent neural networks, which are

49
00:02:20,960 --> 00:02:25,000
going to do things like natural language processing, chatbots, text

50
00:02:25,040 --> 00:02:27,080
processing, all those different kinds of things.

51
00:02:27,280 --> 00:02:29,760
And finally ended off with reinforcement learning.

52
00:02:30,040 --> 00:02:33,320
Now, in terms of resources for this course, there are a ton.

53
00:02:33,520 --> 00:02:36,720
And what we're going to be doing to make this really easy for you and for me

54
00:02:36,760 --> 00:02:39,120
is doing everything through Google Collaboratory.

55
00:02:39,280 --> 00:02:42,200
Now, if you haven't heard of Google Collaboratory, essentially it's a

56
00:02:42,200 --> 00:02:46,720
collaborative coding environment that runs an iPython notebook in the cloud

57
00:02:46,960 --> 00:02:50,560
on a Google machine where you can do all of your machine learning for free.

58
00:02:50,800 --> 00:02:52,600
So you don't need to install any packages.

59
00:02:52,600 --> 00:02:53,760
You don't need to use PIP.

60
00:02:53,960 --> 00:02:55,560
You don't need to get your environment set up.

61
00:02:55,640 --> 00:02:58,840
All you need to do is open a new Google Collaboratory window and you can

62
00:02:58,840 --> 00:02:59,920
start writing code.

63
00:03:00,120 --> 00:03:01,720
And that's what we're going to be doing in this series.

64
00:03:01,920 --> 00:03:04,880
If you look in the description right now, you will see links to all of the

65
00:03:04,880 --> 00:03:07,080
notebooks that I use throughout this guide.

66
00:03:07,200 --> 00:03:10,120
So if there's anything that you want to be cleared up, if you want the code

67
00:03:10,120 --> 00:03:13,320
for yourself, if you want just text based descriptions of the things that I'm

68
00:03:13,320 --> 00:03:15,960
saying, you can click those links and gain access to them.

69
00:03:16,160 --> 00:03:18,440
So with that being said, I'm very excited to get started.

70
00:03:18,440 --> 00:03:20,000
I hope you guys are as well.

71
00:03:20,200 --> 00:03:22,800
And let's go ahead and get into the content.

72
00:03:24,640 --> 00:03:29,760
So in this first section, I'm going to spend a few minutes discussing the

73
00:03:29,760 --> 00:03:34,280
difference between artificial intelligence, neural networks and machine learning.

74
00:03:34,560 --> 00:03:36,840
Now, the reason we need to go into this is because we're going to be covering

75
00:03:36,840 --> 00:03:38,560
all of these topics throughout this course.

76
00:03:38,760 --> 00:03:41,760
So it's vital that you guys understand what these actually mean.

77
00:03:41,760 --> 00:03:43,720
And you can kind of differentiate between them.

78
00:03:43,720 --> 00:03:45,240
So that's what we're going to focus on now.

79
00:03:45,520 --> 00:03:48,240
Now, quick disclaimer here, just so everyone's aware, I'm using something

80
00:03:48,240 --> 00:03:49,440
called Windows, Inc.

81
00:03:49,480 --> 00:03:51,200
This just default comes with Windows.

82
00:03:51,400 --> 00:03:52,880
I have a drawing tabled down here.

83
00:03:52,880 --> 00:03:55,840
And this is what I'm going to be using for some of the explanatory parts where

84
00:03:55,840 --> 00:04:00,000
there's no real coding, just to kind of illustrate some concepts and topics to you.

85
00:04:00,200 --> 00:04:02,200
Now, I have very horrible handwriting.

86
00:04:02,240 --> 00:04:03,920
I'm not artistic whatsoever.

87
00:04:03,920 --> 00:04:09,520
Programming is definitely more of my thing than drawing and doing diagrams and stuff.

88
00:04:09,760 --> 00:04:10,760
But I'm going to try my best.

89
00:04:10,760 --> 00:04:14,360
And this is just the way that I find I can convey information the best to you guys.

90
00:04:14,800 --> 00:04:19,280
So anyways, let's get started and discuss the first topic here, which is artificial intelligence.

91
00:04:19,640 --> 00:04:22,160
Now, artificial intelligence is a huge hype nowadays.

92
00:04:22,440 --> 00:04:25,640
And it's funny because a lot of people actually don't know what this means, or

93
00:04:25,640 --> 00:04:29,360
they try to tell people that what they've created is not artificial intelligence,

94
00:04:29,600 --> 00:04:31,320
when in reality, it actually is.

95
00:04:31,920 --> 00:04:35,480
Now, the kind of formal definition of AI, and I'm just going to read it off

96
00:04:35,480 --> 00:04:38,720
of my slide here to make sure that I'm not messing this up, is the effort

97
00:04:38,720 --> 00:04:42,400
to automate intellectual tasks normally performed by humans.

98
00:04:42,760 --> 00:04:45,120
Now, that's a fairly big definition, right?

99
00:04:45,160 --> 00:04:47,560
What is considered an intellectual task?

100
00:04:47,600 --> 00:04:50,760
And, you know, really, that doesn't help us too much.

101
00:04:50,760 --> 00:04:54,120
So what I'm going to do is bring us back to when AI was created, first created

102
00:04:54,280 --> 00:04:58,200
to kind of explain to you how AI has evolved and what it really started out being.

103
00:04:58,600 --> 00:05:02,640
So back in 1950, there was kind of the question being asked by scientists

104
00:05:02,640 --> 00:05:06,800
and researchers, can computers think, can we get them to figure things out?

105
00:05:06,800 --> 00:05:08,760
Can we get away from just hard coding?

106
00:05:09,000 --> 00:05:12,800
And, you know, having like, can we get a computer to think and it do its own thing?

107
00:05:13,440 --> 00:05:15,000
So that was kind of the question that was asked.

108
00:05:15,200 --> 00:05:19,000
And that's when the term artificial intelligence was kind of coined and created.

109
00:05:19,320 --> 00:05:23,400
Now, back then, AI was simply a predefined set of rules.

110
00:05:23,440 --> 00:05:27,720
So if you're thinking about an AI for maybe like tic-tac-toe or an AI for chess,

111
00:05:27,920 --> 00:05:32,040
all they would have had back then is predefined rules that humans had come up

112
00:05:32,040 --> 00:05:35,920
with and typed into the computer in code, and the computer would simply execute

113
00:05:35,920 --> 00:05:38,200
those set of rules and follow those instructions.

114
00:05:38,440 --> 00:05:41,960
So there was no deep learning, machine learning, crazy algorithms happening.

115
00:05:42,040 --> 00:05:45,480
It was simply if you wanted the computer to do something, you would have to tell

116
00:05:45,480 --> 00:05:49,280
it beforehand, say you're in this position and this happens, do this.

117
00:05:49,320 --> 00:05:50,640
And that's what AI was.

118
00:05:50,880 --> 00:05:55,400
And very good AI was simply just a very good set of rules or a ton of different

119
00:05:55,400 --> 00:05:58,200
rules that humans had implemented into some program.

120
00:05:58,440 --> 00:06:01,800
You can have AI programs that are stretching, you know, half a million lines

121
00:06:01,800 --> 00:06:05,400
of code, just with tons and tons and tons of different rules that have been

122
00:06:05,400 --> 00:06:06,960
created for that AI.

123
00:06:07,600 --> 00:06:12,360
So just be aware that AI does not necessarily mean anything crazy, complex

124
00:06:12,360 --> 00:06:14,080
or super complicated.

125
00:06:14,080 --> 00:06:17,760
But essentially, if you're trying to simulate some intellectual task, like

126
00:06:17,760 --> 00:06:21,680
playing a game that a human would do with a computer, that is considered AI.

127
00:06:21,840 --> 00:06:25,680
So even a very basic artificial intelligence for a tic-tac-toe game

128
00:06:25,680 --> 00:06:28,360
where it plays against you, that is still considered AI.

129
00:06:28,400 --> 00:06:31,360
And if we think of something like Pac-Man, right, where we have, you know,

130
00:06:31,360 --> 00:06:35,040
our little ghost, and this will be my rough sketch of a ghost, and we have our

131
00:06:35,040 --> 00:06:36,720
Pac-Man guy who will just be this.

132
00:06:37,080 --> 00:06:39,720
Well, would we consider this ghost AI?

133
00:06:40,320 --> 00:06:44,360
What it does is it attempts to find and kind of simulate how it would get

134
00:06:44,360 --> 00:06:45,560
to Pac-Man, right?

135
00:06:45,760 --> 00:06:49,120
And the way this works is just using a very basic path finding algorithm.

136
00:06:49,280 --> 00:06:52,520
This is nothing to do with deep learning or machine learning or anything crazy.

137
00:06:52,640 --> 00:06:55,040
But this is still considered artificial intelligence.

138
00:06:55,160 --> 00:06:58,840
The computer is figuring out how it can kind of play and do something

139
00:06:58,840 --> 00:07:00,360
by following an algorithm.

140
00:07:00,360 --> 00:07:04,200
So we don't necessarily need to have anything crazy, stupid, complex to be

141
00:07:04,200 --> 00:07:08,720
considered AI, it simply needs to just be simulating some intellectual human

142
00:07:08,720 --> 00:07:12,200
behavior. That's kind of the definition of artificial intelligence.

143
00:07:12,720 --> 00:07:16,640
Now, obviously today, AI has evolved into a much more complex field where we

144
00:07:16,640 --> 00:07:19,280
now have machine learning and deep learning and all these other techniques,

145
00:07:19,520 --> 00:07:20,920
which is what we're going to talk about now.

146
00:07:21,200 --> 00:07:24,000
So what I want to start by doing is just drawing a circle here.

147
00:07:24,480 --> 00:07:28,240
And I want to label this circle and say AI like that.

148
00:07:28,480 --> 00:07:31,440
So this is going to define AI because everything I'm going to put inside of

149
00:07:31,440 --> 00:07:33,600
here is considered artificial intelligence.

150
00:07:34,160 --> 00:07:36,280
So now let's get into machine learning.

151
00:07:36,760 --> 00:07:38,920
So what I'm going to do is draw another circle inside of here.

152
00:07:39,800 --> 00:07:43,240
And we're going to label this circle ML for machine learning.

153
00:07:43,600 --> 00:07:46,360
Now notice I put this inside of the artificial intelligence circle.

154
00:07:46,480 --> 00:07:50,440
This is because machine learning is a part of artificial intelligence.

155
00:07:51,000 --> 00:07:52,840
Now, what is machine learning?

156
00:07:53,320 --> 00:07:57,880
Well, what we talked about previously was kind of the idea that AI used to just

157
00:07:57,880 --> 00:08:00,040
be a predefined set of rules, right?

158
00:08:00,560 --> 00:08:04,800
Where what would happen is we would feed some data, we would go through the rules

159
00:08:04,800 --> 00:08:07,000
by and then analyze the data with the rules.

160
00:08:07,000 --> 00:08:09,760
And then we'd spit out some output, which would be, you know, what we're going to do.

161
00:08:10,080 --> 00:08:14,400
So in the classic example of chess, say we're in check, well, we pass that board

162
00:08:14,400 --> 00:08:18,080
information to the computer, it looks at its sets of rules, it determines we're in

163
00:08:18,080 --> 00:08:19,880
check, and then it moves us somewhere else.

164
00:08:20,280 --> 00:08:22,680
Now, what is machine learning in contrast to that?

165
00:08:23,000 --> 00:08:26,920
Well, machine learning is kind of the first field that's actually figuring out

166
00:08:26,920 --> 00:08:28,080
the rules for us.

167
00:08:28,400 --> 00:08:32,280
So rather than us hard coding the rules into the computer, what machine learning

168
00:08:32,320 --> 00:08:36,920
attempts to do is take the data and take what the output should be and figure

169
00:08:36,920 --> 00:08:38,040
out the rules for us.

170
00:08:38,240 --> 00:08:41,520
So you'll often hear that, you know, machine learning requires a lot of data

171
00:08:41,520 --> 00:08:46,520
and you need ton of examples and, you know, input data to really train a good

172
00:08:46,520 --> 00:08:49,880
model. Well, the reason for that is because the way that machine learning

173
00:08:49,880 --> 00:08:52,200
works is it generates the rules for us.

174
00:08:52,440 --> 00:08:55,880
We give it some input data, we give it what the output data should be.

175
00:08:56,120 --> 00:09:00,360
And then it looks at that information and figures out what rules can we generate

176
00:09:00,560 --> 00:09:04,760
so that when we look at new data, we can have the best possible output for that.

177
00:09:04,960 --> 00:09:08,440
Now, that's also why a lot of the times machine learning models do not have

178
00:09:08,440 --> 00:09:12,520
a hundred percent accuracy, which means that they may not necessarily get the

179
00:09:12,520 --> 00:09:14,400
correct answer every single time.

180
00:09:14,680 --> 00:09:18,240
And our goal when we create machine learning models is to raise our accuracy

181
00:09:18,240 --> 00:09:22,320
as high as possible, which means it's going to make the fewest mistakes possible.

182
00:09:22,360 --> 00:09:25,360
Because just like a human, you know, our machine learning models, which are

183
00:09:25,360 --> 00:09:28,800
trying to simulate, you know, human behavior can make mistakes.

184
00:09:28,920 --> 00:09:31,920
But to summarize that, essentially, machine learning, the difference

185
00:09:31,920 --> 00:09:36,600
between that and kind of, you know, algorithms and basic artificial intelligence

186
00:09:36,880 --> 00:09:41,000
is the fact that rather get that rather than us, the programmer giving it the

187
00:09:41,000 --> 00:09:43,720
rules, it figures out the rules for us.

188
00:09:43,880 --> 00:09:47,520
And we might not necessarily know explicitly what those rules are when we

189
00:09:47,520 --> 00:09:49,840
look at machine learning and create machine learning models.

190
00:09:50,120 --> 00:09:53,640
But we know that we're giving some input data, we're giving the expected

191
00:09:53,640 --> 00:09:57,640
output data, and then it looks at all of that information, does some algorithms,

192
00:09:57,680 --> 00:10:02,040
which we'll talk about later on that, and figures out the rules for us so that

193
00:10:02,040 --> 00:10:05,520
later when we give it some input data, and we don't know the output data, it

194
00:10:05,520 --> 00:10:08,640
can use those rules that it's figured out from our examples and all that

195
00:10:08,640 --> 00:10:11,280
training data that we gave it to generate some output.

196
00:10:12,000 --> 00:10:13,560
Okay, so that's machine learning.

197
00:10:13,960 --> 00:10:15,800
Now we've covered AI and machine learning.

198
00:10:16,040 --> 00:10:18,960
And now it's time to cover neural networks or deep learning.

199
00:10:19,480 --> 00:10:22,840
Now this circle gets to go right inside of the machine learning right here.

200
00:10:23,080 --> 00:10:26,400
I'm just going to label this one NN, which stands for neural networks.

201
00:10:26,840 --> 00:10:28,920
Now neural networks get a big hype.

202
00:10:28,960 --> 00:10:31,640
They're usually what the first, you know, when you get into machine learning,

203
00:10:31,640 --> 00:10:34,480
you want to learn neural networks, you're kind of like neural networks are

204
00:10:34,480 --> 00:10:35,920
cool, they're capable of a lot.

205
00:10:36,520 --> 00:10:38,160
But let's discuss what these really are.

206
00:10:38,400 --> 00:10:42,400
So the easiest way to define a neural network is it is a form of machine

207
00:10:42,400 --> 00:10:45,920
learning that uses a layered representation of data.

208
00:10:46,240 --> 00:10:48,960
Now we're not going to really understand this completely right now.

209
00:10:48,960 --> 00:10:52,520
But as we get further in that should start to make more sense as a definition.

210
00:10:53,000 --> 00:10:56,600
But what I need to kind of illustrate to you is that in the previous example,

211
00:10:56,600 --> 00:10:59,240
where we just talked about machine learning, essentially what we had is we

212
00:10:59,240 --> 00:11:01,640
had some input bubbles, which I'm going to define as these.

213
00:11:01,960 --> 00:11:04,640
We had some set of rules that is going to be in between here.

214
00:11:04,680 --> 00:11:05,840
And then we had some output.

215
00:11:06,160 --> 00:11:09,280
And what would happen is we feed this input to this set of rules.

216
00:11:10,080 --> 00:11:11,400
Something happens in here.

217
00:11:11,560 --> 00:11:12,720
And then we get some output.

218
00:11:13,040 --> 00:11:15,320
And then that is what, you know, our program does.

219
00:11:15,320 --> 00:11:16,680
So that's what we get from the model.

220
00:11:16,720 --> 00:11:18,600
We pretty much just have two layers.

221
00:11:18,640 --> 00:11:21,560
We have kind of the input layer, the output layer.

222
00:11:21,760 --> 00:11:24,720
And the rules are kind of just what connects those two layers together.

223
00:11:25,280 --> 00:11:30,600
Now in neural networks and what we call deep learning, we have more than two layers.

224
00:11:30,600 --> 00:11:33,560
Now I'm just trying to erase all this quickly so I can show you that.

225
00:11:33,880 --> 00:11:36,840
So let's say, and I'll draw this one another color, because why not?

226
00:11:36,880 --> 00:11:41,000
If we're talking about neural networks, what we might have, and this will vary.

227
00:11:41,000 --> 00:11:44,520
And I'll talk about this in a second, is the fact that we have an input layer,

228
00:11:44,520 --> 00:11:46,040
which will be our first layer of data.

229
00:11:46,200 --> 00:11:50,600
We could have some layers in between this layer that are all connected together.

230
00:11:51,160 --> 00:11:53,240
And then we could have some output layer.

231
00:11:53,520 --> 00:11:58,360
So essentially, what happens is our data is going to be transformed

232
00:11:58,360 --> 00:12:01,480
through different layers, and different things are going to happen.

233
00:12:01,480 --> 00:12:04,680
There's going to be different connections between these layers.

234
00:12:05,000 --> 00:12:07,080
And then eventually we'll reach an output.

235
00:12:07,280 --> 00:12:11,120
Now it's very difficult to explain neural networks without going completely in depth.

236
00:12:11,120 --> 00:12:13,160
So I'll cover a few more notes that I have here.

237
00:12:13,640 --> 00:12:16,840
Essentially, in neural networks, we just have multiple layers.

238
00:12:16,840 --> 00:12:18,320
That's kind of the way to think of them.

239
00:12:18,600 --> 00:12:22,160
And as we see machine learning, you guys should start to understand this more.

240
00:12:22,640 --> 00:12:25,480
But just understand that we're dealing with multiple layers.

241
00:12:25,480 --> 00:12:30,440
And a lot of people actually call this a multi stage information extraction process.

242
00:12:30,680 --> 00:12:32,040
Now, I did not come up with that term.

243
00:12:32,040 --> 00:12:33,720
I think that's from a book or something.

244
00:12:33,720 --> 00:12:37,600
But essentially what ends up happening is we have our data at this first layer,

245
00:12:37,600 --> 00:12:40,920
which is that input information, which we're going to be passing to the model

246
00:12:40,920 --> 00:12:42,440
that we're going to do something with.

247
00:12:42,440 --> 00:12:45,720
It then goes to another layer where it will be transformed.

248
00:12:45,720 --> 00:12:49,880
It will change into something else using a predefined kind of set of

249
00:12:50,520 --> 00:12:52,680
rules and weights that we'll talk about later.

250
00:12:53,080 --> 00:12:56,120
Then it will pass through all of these different layers where different

251
00:12:56,120 --> 00:12:59,280
kind of features of the data, which again, we'll discuss in a second,

252
00:12:59,600 --> 00:13:03,560
will be extracted, will be figured out, will be found until eventually

253
00:13:03,560 --> 00:13:06,560
we reach an output layer where we can kind of combine everything

254
00:13:06,560 --> 00:13:11,040
we've discovered about the data into some kind of output that's meaningful to our program.

255
00:13:11,480 --> 00:13:14,440
So that's kind of the best that I can do to explain neural networks

256
00:13:14,440 --> 00:13:16,160
without going on to a deeper level.

257
00:13:16,160 --> 00:13:19,200
I understand that a lot of you probably don't understand what they are right now.

258
00:13:19,200 --> 00:13:20,480
And that's totally fine.

259
00:13:20,480 --> 00:13:23,520
But just know that there are layered representation of data.

260
00:13:23,560 --> 00:13:27,840
We have multiple layers of information, whereas in standard machine learning,

261
00:13:27,840 --> 00:13:32,000
we only have, you know, one or two layers and an artificial intelligence.

262
00:13:32,000 --> 00:13:36,400
In general, we don't necessarily have to have like a predefined set of layers.

263
00:13:37,080 --> 00:13:40,320
OK, so that is pretty much it for neural networks.

264
00:13:40,320 --> 00:13:43,480
There's one last thing I will say about them is that they're actually not

265
00:13:43,480 --> 00:13:44,920
modeled after the brain.

266
00:13:44,920 --> 00:13:48,400
So a lot of people seem to think that neural networks are modeled after the brain

267
00:13:48,400 --> 00:13:52,880
and the fact that you have neurons firing in your brain, and that can relate to neural networks.

268
00:13:52,960 --> 00:13:57,000
Now, there is a biological inspiration for the name neural networks

269
00:13:57,000 --> 00:14:00,960
in the way that they work from, you know, human biology, but it is not

270
00:14:00,960 --> 00:14:03,320
necessarily modeled about the way that our brain works.

271
00:14:03,320 --> 00:14:07,160
And in fact, we actually don't really know how a lot of the things in our brain

272
00:14:07,160 --> 00:14:08,120
operate and work.

273
00:14:08,120 --> 00:14:11,040
So it would be impossible for us to say that neural networks are modeled

274
00:14:11,040 --> 00:14:15,160
after the brain, because we actually don't know how information is kind of

275
00:14:15,160 --> 00:14:18,880
happens and occurs and transfers through our brain, or at least we don't know

276
00:14:18,880 --> 00:14:22,280
enough to be able to say this is exactly what it is a neural network.

277
00:14:22,280 --> 00:14:24,520
So anyways, that was kind of the last point there.

278
00:14:24,520 --> 00:14:26,920
OK, so now we need to talk about data.

279
00:14:26,920 --> 00:14:30,520
Now, data is the most important part of machine learning and artificial

280
00:14:30,520 --> 00:14:32,400
intelligence neural networks as well.

281
00:14:32,400 --> 00:14:36,960
And it's very important that we understand how important data is and what

282
00:14:36,960 --> 00:14:39,800
the different kind of parts of it are, because they're going to be referenced

283
00:14:39,800 --> 00:14:42,480
a lot in any of the resources that we're using.

284
00:14:42,480 --> 00:14:45,320
Now, what I want to do is just create an example here where I'm going to make a

285
00:14:45,320 --> 00:14:50,600
data set that is about students final grades in like a school system.

286
00:14:50,600 --> 00:14:53,480
So essentially, we're going to make this a very easy example where all we're

287
00:14:53,480 --> 00:14:56,960
going to have for this data set is we're going to have information about students.

288
00:14:56,960 --> 00:15:01,360
So we're going to have their midterm one grade, their midterm two grade, and then

289
00:15:01,360 --> 00:15:02,760
we're going to have their final grade.

290
00:15:02,760 --> 00:15:06,920
So I'm just going to say midterm one.

291
00:15:06,920 --> 00:15:08,720
And again, excuse my handwriting here.

292
00:15:08,760 --> 00:15:11,680
It's not the easiest thing to write with this drawing tablet.

293
00:15:11,680 --> 00:15:13,920
And then I'll just do final.

294
00:15:13,920 --> 00:15:15,560
So this is going to be our data set.

295
00:15:15,560 --> 00:15:19,120
And we'll actually see some similar data sets to this as we go through and do

296
00:15:19,120 --> 00:15:20,760
some examples later on.

297
00:15:20,760 --> 00:15:25,080
So for student one, which we'll just put some students here, we're going to have

298
00:15:25,080 --> 00:15:28,800
their midterm one grade, maybe that's a 70, their midterm two grade, maybe that

299
00:15:28,800 --> 00:15:29,440
was an 80.

300
00:15:29,440 --> 00:15:33,760
And then let's say their final was like their final term grade, not just the

301
00:15:33,760 --> 00:15:35,320
mark on the final exam.

302
00:15:35,320 --> 00:15:37,480
Let's give them a 77.

303
00:15:37,520 --> 00:15:39,720
Now, for midterm one, we can give someone a 60.

304
00:15:39,720 --> 00:15:41,120
Maybe we give them a 90.

305
00:15:41,120 --> 00:15:45,840
And then we determined that the final grade on their exam was let's say an 84.

306
00:15:46,200 --> 00:15:48,440
And then we could do something with maybe a lower grade here.

307
00:15:48,440 --> 00:15:54,560
So 40, 50, and then maybe they got a 38 or something in the final grade.

308
00:15:54,880 --> 00:15:58,880
Now, obviously, we could have some other information here that we're omitting.

309
00:15:58,880 --> 00:16:01,880
Like maybe there was some exam, some assignments, whatever, some other things

310
00:16:01,880 --> 00:16:03,560
they did that contributed to their grade.

311
00:16:03,800 --> 00:16:07,840
But the problem that I want to consider here is the fact that given our midterm

312
00:16:07,840 --> 00:16:11,400
one grade and our midterm two grade and our final grade, how can I use this

313
00:16:11,400 --> 00:16:14,600
information to predict any one of these three columns?

314
00:16:14,840 --> 00:16:19,080
So if I were given a student's midterm one grade, and I were given a student's

315
00:16:19,160 --> 00:16:22,160
final grade, how could I predict their midterm two grade?

316
00:16:23,120 --> 00:16:26,560
So this is where we're going to talk about features and labels.

317
00:16:26,760 --> 00:16:30,680
Now, whatever information we have, that is the input information, which is the

318
00:16:30,680 --> 00:16:34,040
information we will always have that we need to give to the model to get

319
00:16:34,040 --> 00:16:36,320
some output is what we call our features.

320
00:16:36,480 --> 00:16:39,960
So in the example where we're trying to predict midterm two, and let's just do

321
00:16:39,960 --> 00:16:41,800
this and highlight this in red.

322
00:16:41,800 --> 00:16:46,600
So we understand what we would have as our features, our input information

323
00:16:46,600 --> 00:16:50,440
are going to be midterm one and final, because this is the information

324
00:16:50,440 --> 00:16:52,920
we are going to use to predict something.

325
00:16:52,960 --> 00:16:55,800
It is the input, it is what we need to give the model.

326
00:16:55,960 --> 00:16:59,440
And if we're training a model to look at midterm one and final grade, whenever

327
00:16:59,440 --> 00:17:02,800
we want to make a new prediction, we need to have that information to do so.

328
00:17:03,160 --> 00:17:04,720
Now, what's highlighted in red.

329
00:17:04,720 --> 00:17:09,320
So this midterm two here is what we would call the label or the output.

330
00:17:09,600 --> 00:17:13,800
Now, the label is simply what we are trying to look for or predict.

331
00:17:13,960 --> 00:17:18,200
So when we talk about features versus labels, features is our input information,

332
00:17:18,240 --> 00:17:21,480
the information that we have that we need to use to make a prediction.

333
00:17:21,760 --> 00:17:25,240
And our label is that output information that is just representing, you know,

334
00:17:25,240 --> 00:17:26,360
what we're looking for.

335
00:17:26,600 --> 00:17:31,160
So when we feed our features to a model, it will give to us a label.

336
00:17:31,280 --> 00:17:33,240
And that is kind of the point that we need to understand.

337
00:17:33,520 --> 00:17:34,760
So that was the basic here.

338
00:17:35,400 --> 00:17:38,240
And now I'm just going to talk a little bit more about data, because we will

339
00:17:38,240 --> 00:17:41,720
get into this more as we continue going and about the importance of it.

340
00:17:42,000 --> 00:17:46,120
So the reason why data is so important is this is kind of the key thing

341
00:17:46,120 --> 00:17:48,120
that we use to create models.

342
00:17:48,320 --> 00:17:52,320
So whenever we're doing AI and machine learning, we need data pretty much.

343
00:17:52,320 --> 00:17:55,600
Unless you're doing a very specific type of machine learning and artificial

344
00:17:55,640 --> 00:17:57,240
intelligence, which we'll talk about later.

345
00:17:57,760 --> 00:18:00,240
Now, for most of these models, we need tons of different data.

346
00:18:00,280 --> 00:18:01,960
We need tons of different examples.

347
00:18:02,160 --> 00:18:05,560
And that's because we know how machine learning works now, which is essentially

348
00:18:05,840 --> 00:18:08,960
we're trying to come up with rules for a data set.

349
00:18:09,160 --> 00:18:10,360
We have some input information.

350
00:18:10,360 --> 00:18:13,400
We have some output information or some features and some labels.

351
00:18:13,640 --> 00:18:16,560
We can give that to a model and tell it to start training.

352
00:18:16,680 --> 00:18:20,320
And what it will do is come up with rules such that we can just give some

353
00:18:20,320 --> 00:18:22,000
features to the model in the future.

354
00:18:22,160 --> 00:18:25,520
And then it should be able to give us a pretty good estimate of what the output

355
00:18:25,520 --> 00:18:30,240
should be. So when we're training, we have a set of training data.

356
00:18:30,440 --> 00:18:34,560
And that is data where we have all of the features and all of the labels.

357
00:18:34,560 --> 00:18:36,320
So we have all of this information.

358
00:18:36,800 --> 00:18:40,840
Then when we're going to test the model or use the model later on, we would not

359
00:18:40,840 --> 00:18:42,440
have this midterm to information.

360
00:18:42,440 --> 00:18:43,960
We wouldn't pass this to the model.

361
00:18:44,160 --> 00:18:47,440
We would just pass our features, which is midterm one and final.

362
00:18:47,440 --> 00:18:49,400
And then we would get the output of midterm two.

363
00:18:49,800 --> 00:18:50,960
So I hope that makes sense.

364
00:18:51,200 --> 00:18:53,400
That just means data is extremely important.

365
00:18:53,400 --> 00:18:57,360
If we're feeding incorrect data or data that we shouldn't be using to the model,

366
00:18:57,520 --> 00:18:59,840
that could definitely result in a lot of mistakes.

367
00:19:00,040 --> 00:19:03,160
And if we have incorrect output information or incorrect input

368
00:19:03,160 --> 00:19:06,080
information, that is going to cause a lot of mistakes as well, because that is

369
00:19:06,080 --> 00:19:10,000
essentially what the model is using to learn and to kind of develop and figure

370
00:19:10,000 --> 00:19:12,160
out what it's going to do with new input information.

371
00:19:12,520 --> 00:19:14,200
So that means that is enough of data.

372
00:19:14,240 --> 00:19:16,880
Now let's talk about the different types of machine learning.

373
00:19:17,360 --> 00:19:19,760
OK, so now that we've discussed the difference between artificial

374
00:19:19,760 --> 00:19:23,840
intelligence, machine learning and neural networks, we have a kind of decent idea

375
00:19:23,840 --> 00:19:27,000
about what data is in the difference between features and labels.

376
00:19:27,320 --> 00:19:31,400
It's time to talk about the different types of machine learning specifically,

377
00:19:31,760 --> 00:19:36,320
which are unsupervised learning, supervised learning and reinforcement learning.

378
00:19:36,600 --> 00:19:39,560
Now, these are just the different types of learning, the different types

379
00:19:39,560 --> 00:19:41,080
of figuring things out.

380
00:19:41,080 --> 00:19:44,520
Now, different kind of algorithms fit into these different categories

381
00:19:44,520 --> 00:19:48,440
from within artificial intelligence, within machine learning and within neural networks.

382
00:19:48,880 --> 00:19:51,760
So the first one we're going to talk about is supervised learning,

383
00:19:51,760 --> 00:19:53,680
which is kind of what we've already discussed.

384
00:19:53,920 --> 00:19:58,520
So I'll just write supervised up here again, excuse the handwriting.

385
00:19:59,200 --> 00:20:01,000
So supervised learning.

386
00:20:01,000 --> 00:20:02,360
Now, what is this?

387
00:20:02,360 --> 00:20:04,640
Well, supervised learning is kind of everything we've already learned,

388
00:20:04,640 --> 00:20:07,240
which is we have some features.

389
00:20:07,280 --> 00:20:09,760
So we'll write our features like this, right?

390
00:20:10,000 --> 00:20:14,200
We have some features and those features correspond to some label

391
00:20:14,200 --> 00:20:15,640
or potentially labels.

392
00:20:15,640 --> 00:20:17,760
Sometimes we might predict more than one information.

393
00:20:18,160 --> 00:20:21,840
So when we have this information, we have the features and we have the labels.

394
00:20:22,040 --> 00:20:25,360
What we do is we pass this information to some machine learning model.

395
00:20:25,600 --> 00:20:27,120
It figures out the rules for us.

396
00:20:27,120 --> 00:20:31,040
And then later on, all we need is the features and it will give us some labels

397
00:20:31,040 --> 00:20:32,120
using those rules.

398
00:20:32,120 --> 00:20:36,280
But essentially, what supervised learning is, is when we have both of this information.

399
00:20:36,680 --> 00:20:40,040
The reason it's called supervised is because what ends up happening

400
00:20:40,040 --> 00:20:44,200
when we train our machine learning model is we pass the input information.

401
00:20:44,520 --> 00:20:48,280
It makes some arbitrary prediction using the rules it already knows.

402
00:20:48,440 --> 00:20:51,640
And then it compares that prediction that it made to what the actual

403
00:20:51,640 --> 00:20:53,880
prediction is, which is this label.

404
00:20:54,160 --> 00:20:58,720
So we supervise the model and we say, OK, so you predicted that the color was red,

405
00:20:58,920 --> 00:21:01,720
but really the color of whatever we passed in should have been blue.

406
00:21:01,880 --> 00:21:05,240
So we need to tweak you just a little bit so that you get a little bit better

407
00:21:05,240 --> 00:21:06,960
and you move in the correct direction.

408
00:21:06,960 --> 00:21:08,840
And that's kind of the way that this works.

409
00:21:08,840 --> 00:21:11,680
For example, say we're predicting, you know, students final grade.

410
00:21:11,960 --> 00:21:16,840
Well, if we predict that the final grade is 76, but the actual grade is 77,

411
00:21:17,040 --> 00:21:19,160
we were pretty close, but we're not quite there.

412
00:21:19,320 --> 00:21:22,320
So we supervise the model and we say, hey, we're going to tweak you

413
00:21:22,320 --> 00:21:24,520
just a little bit, move you in the correct direction.

414
00:21:24,720 --> 00:21:26,360
And hopefully we get you to 77.

415
00:21:26,920 --> 00:21:29,520
And that is kind of the way to explain this, right?

416
00:21:29,520 --> 00:21:31,360
You have the features, you have the labels.

417
00:21:31,520 --> 00:21:35,080
When you pass the features, the model has some rules that it's already built.

418
00:21:35,120 --> 00:21:36,320
It makes a prediction.

419
00:21:36,480 --> 00:21:40,120
And then it compares that prediction to the label and then re tweaks the model

420
00:21:40,200 --> 00:21:44,920
and continues doing this with thousands upon thousands upon thousands of pieces

421
00:21:44,920 --> 00:21:48,520
of data, until eventually it gets so good that we can stop training it.

422
00:21:48,680 --> 00:21:50,600
And that is what supervised learning is.

423
00:21:50,800 --> 00:21:52,520
It's the most common type of learning.

424
00:21:52,520 --> 00:21:55,240
It's definitely the most applicable in a lot of instances.

425
00:21:55,440 --> 00:21:58,600
And most machine learning algorithms that are actually used

426
00:21:58,600 --> 00:22:00,560
use a form of supervised machine learning.

427
00:22:00,880 --> 00:22:04,240
A lot of people seem to think that this is, you know, a less complicated,

428
00:22:04,240 --> 00:22:07,800
less advanced way of doing things that is definitely not true.

429
00:22:07,840 --> 00:22:09,920
All of the different methods I'm going to tell you have different

430
00:22:09,920 --> 00:22:11,520
advantages and disadvantages.

431
00:22:11,720 --> 00:22:15,600
And this has a massive advantage when you have a ton of information

432
00:22:15,600 --> 00:22:18,360
and you have the output of that information as well.

433
00:22:18,560 --> 00:22:20,600
But sometimes we don't have the luxury of doing that.

434
00:22:20,600 --> 00:22:22,760
And that's where we talk about unsupervised learning.

435
00:22:23,120 --> 00:22:25,600
So hopefully that made sense for supervised learning.

436
00:22:26,080 --> 00:22:27,440
Tried my best to explain that.

437
00:22:27,640 --> 00:22:30,200
And now let's go into or sorry for supervised learning.

438
00:22:30,240 --> 00:22:32,520
Now let's go into unsupervised learning.

439
00:22:33,200 --> 00:22:36,040
So if we know the definition of supervised learning,

440
00:22:36,280 --> 00:22:39,400
we should hopefully be able to come up with a definition of unsupervised

441
00:22:39,400 --> 00:22:42,400
learning, which is when we only have features.

442
00:22:42,760 --> 00:22:47,760
So given a bunch of features like this and absolutely no labels,

443
00:22:47,760 --> 00:22:52,400
no output for these features, what we want to do is have the model

444
00:22:52,400 --> 00:22:54,320
come up with those labels for us.

445
00:22:54,520 --> 00:22:55,480
Now, this is kind of weird.

446
00:22:55,480 --> 00:22:57,440
You're kind of like, wait, how does that work?

447
00:22:57,440 --> 00:22:59,000
Why would we even want to do that?

448
00:22:59,240 --> 00:23:00,840
Well, let's take this for an example.

449
00:23:01,320 --> 00:23:04,360
We have some access, some axes of data.

450
00:23:04,360 --> 00:23:07,040
Okay, and we have like a two dimensional data point.

451
00:23:07,040 --> 00:23:10,600
So I'm just going to call this, let's say X and let's say Y.

452
00:23:10,800 --> 00:23:11,200
Okay.

453
00:23:11,600 --> 00:23:14,400
And I'm going to just put a bunch of dots on the screen that kind of

454
00:23:14,400 --> 00:23:17,360
represents like maybe a scatter plot of some of our different data.

455
00:23:18,360 --> 00:23:21,840
And I'm just going to put some dots specifically closer to other ones.

456
00:23:21,840 --> 00:23:24,640
Just so you guys kind of get the point of what we're trying to do here.

457
00:23:24,920 --> 00:23:26,360
So let's do that.

458
00:23:26,800 --> 00:23:27,000
Okay.

459
00:23:27,000 --> 00:23:30,520
So let's say I have this data set, this here is what we're working with.

460
00:23:30,560 --> 00:23:32,240
And we have these features.

461
00:23:32,600 --> 00:23:35,880
The features in this instance are going to be X and Y, right?

462
00:23:35,880 --> 00:23:38,320
So X and Y are my features.

463
00:23:38,520 --> 00:23:41,880
Now, we don't have any output specifically for these data points.

464
00:23:42,160 --> 00:23:45,520
What we actually want to do is we want to create some kind of model

465
00:23:46,160 --> 00:23:50,400
that can cluster these data points, which means figure out kind of, you know,

466
00:23:50,440 --> 00:23:54,600
unique groups of data and say, okay, so you're in group one, you're in group two,

467
00:23:54,800 --> 00:23:56,480
you're in group three, and you're in group four.

468
00:23:57,120 --> 00:24:01,360
We may not necessarily know how many groups we have, although sometimes we do.

469
00:24:01,920 --> 00:24:05,360
But what we want to do is just group them and kind of say, okay, we want to

470
00:24:05,360 --> 00:24:08,680
figure out which ones are similar and we want to combine those together.

471
00:24:09,000 --> 00:24:12,160
So hopefully what we would do with an unsupervised machine learning model

472
00:24:12,200 --> 00:24:16,000
is pass all of these features and then have the model create kind of these

473
00:24:16,000 --> 00:24:21,400
groupings. So like maybe this is a group, maybe this is a group, maybe this is a

474
00:24:21,400 --> 00:24:24,600
group, if we were having four groupings, and maybe if we had two groupings,

475
00:24:24,800 --> 00:24:27,360
we might get groupings that look something like this, right?

476
00:24:27,760 --> 00:24:32,760
And then when we pass a new data point in, that could, we could figure out what

477
00:24:32,760 --> 00:24:36,080
group that was a part of by determining, you know, which one it is closer to.

478
00:24:36,720 --> 00:24:38,560
Now, this is kind of a rough example.

479
00:24:38,560 --> 00:24:42,160
It's hard to again, explain all of these without going very in depth into

480
00:24:42,160 --> 00:24:46,000
the specific algorithms, but unsupervised machine learning or just learning

481
00:24:46,000 --> 00:24:49,800
in general is when you don't have some output information, you actually want

482
00:24:49,840 --> 00:24:52,080
the model to figure out the output for you.

483
00:24:52,320 --> 00:24:55,640
You don't really care how it gets there, you just want it to get there.

484
00:24:55,800 --> 00:24:59,560
And again, a good example is clustering data points, and we'll talk about some

485
00:24:59,560 --> 00:25:03,400
specific applications of when we might even want to use that later on, just

486
00:25:03,400 --> 00:25:06,880
understand you have the features, you don't have the labels, and you get the

487
00:25:06,920 --> 00:25:09,720
unsupervised model to kind of figure it out for you.

488
00:25:10,120 --> 00:25:13,640
Okay, so now our last type, which is very different than the two types I just

489
00:25:13,640 --> 00:25:15,760
explained is called reinforcement learning.

490
00:25:16,120 --> 00:25:19,080
Now personally, reinforcement learning, and I don't even know if I want to

491
00:25:19,080 --> 00:25:22,520
spell this because I feel like I'm going to mess it up.

492
00:25:23,240 --> 00:25:27,440
Reinforcement learning is the coolest type of machine learning, in my opinion.

493
00:25:27,480 --> 00:25:31,800
And this is when you actually don't have any data, you have what you call an

494
00:25:31,840 --> 00:25:34,640
agent, an environment and a reward.

495
00:25:34,880 --> 00:25:39,440
I'm going to explain this very briefly with a very, very, very simple example

496
00:25:39,440 --> 00:25:40,840
because it's hard to get too far.

497
00:25:41,160 --> 00:25:44,320
So let's say we have a very basic game, you know, maybe we made this game

498
00:25:44,320 --> 00:25:48,080
ourselves, and essentially, the objective of the game is to get to the flag.

499
00:25:48,320 --> 00:25:49,600
Okay, that's all it is.

500
00:25:49,600 --> 00:25:53,360
We have some ground, you can move left or right, and we want to get to this

501
00:25:53,360 --> 00:25:58,000
flag. Well, we want to train some artificial intelligence, some machine

502
00:25:58,000 --> 00:26:00,400
learning model that can figure out how to do this.

503
00:26:00,800 --> 00:26:03,640
So what we do is we call this our agent.

504
00:26:04,840 --> 00:26:06,640
We call this entire thing.

505
00:26:06,640 --> 00:26:09,160
So this whole thing here, the environment.

506
00:26:09,440 --> 00:26:11,200
So I guess I could write that here.

507
00:26:11,200 --> 00:26:15,600
So n by our meant think I spelled that correctly.

508
00:26:16,040 --> 00:26:17,800
And then we have something called a reward.

509
00:26:18,200 --> 00:26:21,640
And a reward is essentially what the agent gets when it does something

510
00:26:21,640 --> 00:26:25,520
correctly. So let's say the agent takes one step over this way.

511
00:26:25,520 --> 00:26:26,920
So let's say he's a new position is here.

512
00:26:26,920 --> 00:26:27,880
I just don't want to keep drawing him.

513
00:26:27,880 --> 00:26:28,760
So I'm just going to use a dot.

514
00:26:29,400 --> 00:26:31,640
Well, he got closer to the flag.

515
00:26:32,000 --> 00:26:35,400
So what I'm actually going to do is give him a plus two reward.

516
00:26:36,240 --> 00:26:38,240
So let's say he moves again closer to the flag.

517
00:26:38,880 --> 00:26:41,960
Maybe I give him now plus one this time he got even closer.

518
00:26:42,760 --> 00:26:45,800
And as he gets closer, I give him more and more reward.

519
00:26:46,640 --> 00:26:48,720
Now what happens if he moves backwards?

520
00:26:49,200 --> 00:26:51,800
So let's erase this and let's say that at some point in time,

521
00:26:52,000 --> 00:26:55,680
rather than moving closer to the threat, the flag, he moves backwards.

522
00:26:56,320 --> 00:26:58,280
Well, he might get a negative reward.

523
00:26:59,000 --> 00:27:02,520
Now, essentially, what the objective of this agent is to do

524
00:27:03,080 --> 00:27:05,080
is to maximize its reward.

525
00:27:05,080 --> 00:27:07,880
So if you give it a negative reward for moving backwards,

526
00:27:08,160 --> 00:27:09,560
it's going to remember that.

527
00:27:09,560 --> 00:27:13,080
And it's going to say, OK, at this position here, where I was standing,

528
00:27:13,080 --> 00:27:15,880
when I moved backwards, I got a negative reward.

529
00:27:16,280 --> 00:27:20,080
So if I get to this position again, I don't want to go backwards anymore.

530
00:27:20,400 --> 00:27:24,640
I want to go forwards because that should give me a positive reward.

531
00:27:25,240 --> 00:27:27,920
And the whole point of this is we have this agent

532
00:27:28,160 --> 00:27:31,840
that starts off with absolutely no idea, no kind of, you know,

533
00:27:31,880 --> 00:27:33,800
knowledge of the environment.

534
00:27:33,800 --> 00:27:36,080
And what it does is it starts exploring.

535
00:27:36,080 --> 00:27:39,160
And it's a mixture of randomly exploring and exploring

536
00:27:39,160 --> 00:27:41,720
using kind of some of the things that's figured out so far

537
00:27:41,880 --> 00:27:44,000
to try to maximize its reward.

538
00:27:44,240 --> 00:27:47,040
So eventually, when the agent gets to the flag,

539
00:27:47,120 --> 00:27:50,520
it will have the most the highest possible reward that it can have.

540
00:27:50,840 --> 00:27:54,480
And then next time that we plug this agent into the environment,

541
00:27:54,640 --> 00:27:56,920
it will know how to get to the flag immediately

542
00:27:56,920 --> 00:27:58,600
because it's kind of figured that out.

543
00:27:58,600 --> 00:28:00,840
It's determined that in all of these different positions,

544
00:28:00,840 --> 00:28:03,480
if I move here, this is the best place to move.

545
00:28:03,480 --> 00:28:05,680
So if I get in this position, move there.

546
00:28:06,080 --> 00:28:09,200
Now, this is, again, hard to explain without more detailed examples

547
00:28:09,200 --> 00:28:11,200
and going more mathematically and all of that.

548
00:28:11,200 --> 00:28:13,120
But essentially, just understand we have the agent,

549
00:28:13,120 --> 00:28:17,320
which is kind of what the thing is that's moving around in our environment.

550
00:28:17,600 --> 00:28:21,880
We have this environment, which is just what the agent can move around in.

551
00:28:22,080 --> 00:28:23,400
And then we have a reward.

552
00:28:23,400 --> 00:28:26,160
And the reward is what we need to figure out as the programmer,

553
00:28:26,160 --> 00:28:29,920
a way to reward the agent correctly so that it gets to the objective

554
00:28:30,240 --> 00:28:32,480
in the best possible way.

555
00:28:32,480 --> 00:28:35,040
But the agent simply maximizes that reward.

556
00:28:35,120 --> 00:28:37,760
So it just figures out where I need to go to maximize that reward.

557
00:28:37,880 --> 00:28:41,080
It starts at the beginning, kind of randomly exploring the environment

558
00:28:41,080 --> 00:28:44,120
because it doesn't know any of the rewards it gets at any of the positions.

559
00:28:44,360 --> 00:28:46,480
And then as it explores some more different areas,

560
00:28:46,480 --> 00:28:49,680
it kind of figures out the rules and the way that the environment works

561
00:28:49,880 --> 00:28:52,840
and then will determine how to reach the objective,

562
00:28:52,840 --> 00:28:54,560
which is whatever it is that it is.

563
00:28:54,560 --> 00:28:55,720
This is a very simple example.

564
00:28:55,720 --> 00:28:59,120
You could train a reinforcement model to do this and, you know, like half a second, right?

565
00:28:59,360 --> 00:29:01,400
But there is way more advanced examples

566
00:29:01,400 --> 00:29:03,880
and there's been examples of reinforcement learning,

567
00:29:04,080 --> 00:29:07,760
like of AI is pretty much figuring out how to play games together.

568
00:29:07,760 --> 00:29:09,200
How to it's it's actually pretty cool.

569
00:29:09,200 --> 00:29:11,280
Some of the stuff that reinforcement learning is doing.

570
00:29:11,480 --> 00:29:14,240
And it's a really awesome kind of advancement in the field,

571
00:29:14,240 --> 00:29:16,400
because it means we don't need all this data anymore.

572
00:29:16,600 --> 00:29:19,800
We can just get this to kind of figure out how to do things for us

573
00:29:19,800 --> 00:29:21,880
and explore the environment and learn on its own.

574
00:29:22,160 --> 00:29:23,760
Now, this can take a really long time.

575
00:29:23,760 --> 00:29:26,800
This can take a very short amount of time, really depends on the environment.

576
00:29:27,000 --> 00:29:30,280
But a real application of this is training AIs to play games,

577
00:29:30,280 --> 00:29:32,840
as you might be able to tell by kind of what I was explaining here.

578
00:29:33,120 --> 00:29:35,840
And yeah, so that is kind of the fundamental differences

579
00:29:35,840 --> 00:29:39,160
between supervised, unsupervised and reinforcement learning.

580
00:29:39,160 --> 00:29:41,960
We're going to cover all three of these topics throughout this course.

581
00:29:42,240 --> 00:29:45,560
And it's really interesting to see some of the applications we can actually do with this.

582
00:29:45,800 --> 00:29:49,240
So with that being said, I'm going to kind of end what I'm going to call module one,

583
00:29:49,240 --> 00:29:52,160
which is just a general overview of the different topics,

584
00:29:52,160 --> 00:29:54,640
some definitions and getting a fundamental knowledge.

585
00:29:54,920 --> 00:29:58,880
And in the next one, what we're going to be talking about is what TensorFlow is.

586
00:29:58,880 --> 00:30:01,800
We're going to get into coding a little bit and we're going to discuss

587
00:30:02,160 --> 00:30:04,640
some different aspects of TensorFlow and things we need to know

588
00:30:04,640 --> 00:30:06,680
to be able to move forward and do some more advanced things.

589
00:30:09,840 --> 00:30:12,560
So now in module two of this course, what we're going to be doing

590
00:30:12,560 --> 00:30:16,760
is getting a general introduction to TensorFlow, understanding what a tensor is,

591
00:30:16,960 --> 00:30:19,280
understanding shapes and data representation,

592
00:30:19,280 --> 00:30:23,000
and then how TensorFlow actually works on a bit of a lower level.

593
00:30:23,240 --> 00:30:26,280
This is very important because you can definitely go through and learn

594
00:30:26,280 --> 00:30:29,720
how to do machine learning without kind of gaining this information and knowledge.

595
00:30:29,960 --> 00:30:32,320
But it makes it a lot more difficult to tweak your models

596
00:30:32,320 --> 00:30:34,920
and really understand what's going on if you don't, you know,

597
00:30:34,920 --> 00:30:39,080
have that fundamental lower level knowledge of how TensorFlow actually works

598
00:30:39,080 --> 00:30:41,760
and operates. So that's exactly what we're going to cover here.

599
00:30:42,080 --> 00:30:44,320
Now, for those of you that don't know what TensorFlow is,

600
00:30:44,320 --> 00:30:47,400
essentially, this is an open source machine learning library.

601
00:30:47,560 --> 00:30:49,240
It's one of the largest ones in the world.

602
00:30:49,240 --> 00:30:53,360
It's one of the most well known and it's maintained and supported by Google.

603
00:30:53,720 --> 00:30:58,440
Now, TensorFlow essentially allows us to do and create machine learning models

604
00:30:58,440 --> 00:31:01,600
and neural networks and all of that without having to have a very complex

605
00:31:01,600 --> 00:31:05,800
math background. Now, as we get further in and we start discussing more in detail,

606
00:31:05,800 --> 00:31:08,840
how neural networks work in machine learning algorithms actually function,

607
00:31:09,080 --> 00:31:11,560
you'll realize there's a lot of math that goes into this.

608
00:31:11,920 --> 00:31:15,640
Now, it starts off being very kind of fundamental, like basic calculus

609
00:31:15,640 --> 00:31:19,360
and basic linear algebra, and then it gets much more advanced into things

610
00:31:19,360 --> 00:31:23,280
like gradient descent and some more regression techniques and classification.

611
00:31:23,680 --> 00:31:28,080
And essentially, you know, a lot of us don't know that and we don't really need to know that.

612
00:31:28,280 --> 00:31:30,560
So long as we have a basic understanding of it,

613
00:31:30,840 --> 00:31:34,560
then we can use the tools that TensorFlow provides for us to create models.

614
00:31:34,560 --> 00:31:36,280
And that's exactly what TensorFlow does.

615
00:31:36,640 --> 00:31:40,080
Now, what I'm in right now is what I call Google Collaboratory.

616
00:31:40,160 --> 00:31:42,080
I'm going to talk about this more in depth in a second.

617
00:31:42,080 --> 00:31:46,680
But what I've done for this whole course is I've transcribed very detailed

618
00:31:47,000 --> 00:31:49,440
everything that I'm going to be covering through each module.

619
00:31:49,600 --> 00:31:54,600
So this is kind of the transcription of module one, which is the introduction to TensorFlow.

620
00:31:54,840 --> 00:31:58,200
You can see it's not crazy long, but I wanted to do this so that any of you

621
00:31:58,240 --> 00:32:02,640
can follow along with kind of the text base and kind of my lecture notes.

622
00:32:02,640 --> 00:32:05,040
I almost want to call them as I go through the different content.

623
00:32:05,320 --> 00:32:08,720
So in the description, there will be links to all of these different notebooks.

624
00:32:08,760 --> 00:32:12,520
This is in something called Google Collaboratory, which again, we're going to discuss in a second.

625
00:32:12,800 --> 00:32:17,240
But you can see here that I have a bunch of text and it gets down to some different coding aspects.

626
00:32:17,480 --> 00:32:21,160
And what I'm going to be doing to make sure that I stay on track is simply following along

627
00:32:21,160 --> 00:32:24,480
through this, I might deviate slightly, I might go into some other examples.

628
00:32:24,720 --> 00:32:28,240
This will be kind of everything that I'm going to be covering through each module.

629
00:32:28,640 --> 00:32:31,720
So again, to follow along, click the link in the description.

630
00:32:32,280 --> 00:32:35,160
All right. So what can we do with TensorFlow?

631
00:32:35,440 --> 00:32:37,840
Well, these are some of the different things I've listed them here.

632
00:32:37,840 --> 00:32:42,320
So I don't forget we can do image classification, data clustering, regression,

633
00:32:42,640 --> 00:32:46,560
reinforcement learning, natural language processing, and pretty much anything

634
00:32:46,560 --> 00:32:48,640
that you can imagine with machine learning.

635
00:32:49,280 --> 00:32:52,760
Essentially, what TensorFlow does is gives us a library of tools

636
00:32:52,880 --> 00:32:56,800
that allow us to omit having to do these very complicated math operations.

637
00:32:57,280 --> 00:32:58,720
It just does them for us.

638
00:32:58,720 --> 00:33:01,680
Now, there is a bit that we need to know about them, but nothing too complex.

639
00:33:02,200 --> 00:33:05,160
Now, let's talk about how TensorFlow actually works.

640
00:33:05,640 --> 00:33:09,400
So TensorFlow has two main components that we need to understand

641
00:33:09,760 --> 00:33:12,840
to figure out how operations and math are actually performed.

642
00:33:13,080 --> 00:33:15,720
Now, we have something called graphs and sessions.

643
00:33:16,200 --> 00:33:21,880
Now, the way that TensorFlow works is it creates a graph of partial computations.

644
00:33:22,200 --> 00:33:24,200
Now, I know this is going to sound a little bit complicated.

645
00:33:24,200 --> 00:33:27,720
Some of you guys just try to kind of forget about the complex vocabulary

646
00:33:27,720 --> 00:33:32,200
and follow along. But essentially, what we do when we write code in TensorFlow

647
00:33:32,200 --> 00:33:34,040
is we create a graph.

648
00:33:34,040 --> 00:33:38,120
So if I were to create some variable, that variable gets added to the graph.

649
00:33:38,400 --> 00:33:42,680
And maybe that variable is the sum or the summation of two other variables.

650
00:33:43,000 --> 00:33:46,400
What the graph will define now is say, you know, we have variable one,

651
00:33:46,760 --> 00:33:51,000
which is equal to the sum of variable two and variable three.

652
00:33:51,560 --> 00:33:55,560
But what we need to understand is that it doesn't actually evaluate that.

653
00:33:55,560 --> 00:33:59,360
It simply states that that is the computation that we've defined.

654
00:33:59,760 --> 00:34:04,320
So it's almost like writing down an equation without actually performing any math.

655
00:34:04,560 --> 00:34:07,240
We kind of just, you know, have that equation there.

656
00:34:07,440 --> 00:34:10,840
We know that this is the value, but we haven't evaluated it.

657
00:34:10,840 --> 00:34:13,280
So we don't know that the value is like seven per se.

658
00:34:13,440 --> 00:34:17,200
We just know that it's the sum of, you know, vector one and vector two.

659
00:34:17,240 --> 00:34:20,840
Or it's the sum of this or it's the cross product or the dot product.

660
00:34:20,840 --> 00:34:24,080
We just define all of the different partial computations

661
00:34:24,320 --> 00:34:26,680
because we haven't evaluated those computation yet.

662
00:34:26,880 --> 00:34:28,840
And that is what is stored in the graph.

663
00:34:29,680 --> 00:34:32,400
Now, the reason it's called a graph is because different

664
00:34:32,400 --> 00:34:34,600
computations can be related to each other.

665
00:34:34,920 --> 00:34:38,720
For example, if I want to figure out the value of vector one,

666
00:34:38,920 --> 00:34:42,800
but vector one is equal to the value of vector three plus vector four,

667
00:34:43,000 --> 00:34:45,840
I need to determine the value of vector three and vector four

668
00:34:46,320 --> 00:34:48,240
before I can do that computation.

669
00:34:48,240 --> 00:34:49,800
So they're kind of linked together.

670
00:34:49,800 --> 00:34:51,840
And I hope that makes a little bit of sense.

671
00:34:52,440 --> 00:34:54,200
Now, what is a session?

672
00:34:54,200 --> 00:34:58,920
Well, session is essentially a way to execute part or the entire graph.

673
00:34:59,280 --> 00:35:03,840
So when we start a session, what we do is we start executing different aspects

674
00:35:03,840 --> 00:35:06,960
of the graph. So we start at the lowest level of the graph

675
00:35:06,960 --> 00:35:08,880
where nothing is dependent on anything else.

676
00:35:08,880 --> 00:35:12,040
We have maybe constant values or something like that.

677
00:35:12,280 --> 00:35:14,360
And then we move our way through the graph

678
00:35:14,360 --> 00:35:17,960
and start doing all of the different partial computations that we've defined.

679
00:35:18,520 --> 00:35:20,200
Now, I hope that this isn't too confusing.

680
00:35:20,200 --> 00:35:21,640
I know this is kind of a lot of lingo.

681
00:35:21,640 --> 00:35:23,640
You guys will understand this as we go through.

682
00:35:23,840 --> 00:35:26,280
And again, you can read through some of these components here

683
00:35:26,280 --> 00:35:29,000
that I have in collaboratory, if I'm kind of skipping through anything,

684
00:35:29,000 --> 00:35:31,080
or you don't truly understand.

685
00:35:31,320 --> 00:35:33,520
But that is the way that graphs and sessions work.

686
00:35:33,880 --> 00:35:35,480
We won't go too in depth with them.

687
00:35:35,480 --> 00:35:38,160
We do need to understand that that is the way TensorFlow works.

688
00:35:38,160 --> 00:35:42,480
And there's some times where we can't use a specific value in our code yet

689
00:35:42,680 --> 00:35:45,000
because we haven't evaluated the graph.

690
00:35:45,000 --> 00:35:47,640
We haven't created a session and gotten the values yet.

691
00:35:47,840 --> 00:35:50,200
Which we might need to do before we can actually, you know,

692
00:35:50,200 --> 00:35:51,800
use some specific value.

693
00:35:51,800 --> 00:35:53,800
So that's just something to consider.

694
00:35:53,800 --> 00:35:56,160
All right, so now we're actually going to get into coding,

695
00:35:56,160 --> 00:35:58,560
importing and installing TensorFlow.

696
00:35:58,880 --> 00:36:01,720
Now, this is where I'm going to introduce you to Google Collaboratory

697
00:36:01,720 --> 00:36:03,520
and explain how you guys can follow along

698
00:36:03,520 --> 00:36:06,360
without having to install anything on your computer.

699
00:36:06,520 --> 00:36:09,160
And it doesn't matter if you have like a really crappy computer

700
00:36:09,160 --> 00:36:12,560
or even if you're on like an iPhone per se, you can actually do this,

701
00:36:12,560 --> 00:36:13,760
which is amazing.

702
00:36:13,760 --> 00:36:17,120
So all you need to do is Google, Google Collaboratory

703
00:36:17,360 --> 00:36:19,600
and create a new notebook.

704
00:36:19,600 --> 00:36:22,720
Now, what Google Collaboratory is, is essentially a free

705
00:36:22,720 --> 00:36:25,040
Jupyter notebook in the cloud for you.

706
00:36:25,320 --> 00:36:27,920
The way this works is you can open up this notebook.

707
00:36:27,920 --> 00:36:30,480
You can see this is called I pi NB.

708
00:36:31,200 --> 00:36:32,120
I yeah, what is that?

709
00:36:32,120 --> 00:36:35,400
I pi NB, which I think just stands for I Python notebook.

710
00:36:35,680 --> 00:36:39,760
And what you can do in here is actually write code and write text as well.

711
00:36:40,040 --> 00:36:43,760
So this in here is what it's called, you know, Google Collaboratory Notebook.

712
00:36:43,960 --> 00:36:45,920
And essentially why it's called a notebook

713
00:36:45,920 --> 00:36:49,320
is because not only can you put code, but you can also put notes,

714
00:36:49,320 --> 00:36:52,400
which is what I've done here with these specific titles.

715
00:36:52,680 --> 00:36:55,280
So you can actually use Markdown inside of this.

716
00:36:55,280 --> 00:36:58,720
So if I open up one of these, you can see that I've used Markdown text

717
00:36:59,280 --> 00:37:01,280
to actually kind of create these sections.

718
00:37:01,640 --> 00:37:04,760
And yeah, that is kind of how Collaboratory works.

719
00:37:05,040 --> 00:37:08,680
But what you can do in Collaboratory is forget about having to install

720
00:37:08,680 --> 00:37:11,840
all of these modules, they're already installed for you.

721
00:37:12,080 --> 00:37:15,400
So what you're actually going to do when you open a Collaboratory window

722
00:37:15,400 --> 00:37:18,680
is Google is going to automatically connect you to one of their servers

723
00:37:18,680 --> 00:37:22,680
or one of their machines that has all of this stuff done and set up for you.

724
00:37:22,880 --> 00:37:26,200
And you can start writing code and executing it off their machine

725
00:37:26,200 --> 00:37:27,800
and seeing the result.

726
00:37:27,800 --> 00:37:31,760
So for example, if I want to print hello like this,

727
00:37:31,760 --> 00:37:33,960
and I'll zoom in a little bit so you guys can read this.

728
00:37:33,960 --> 00:37:37,880
All I do is I create a new code block, which I can do by clicking code.

729
00:37:38,480 --> 00:37:40,800
Like that, I can delete one like that as well.

730
00:37:40,800 --> 00:37:42,920
And I hit run.

731
00:37:42,920 --> 00:37:46,040
Now notice, give it a second, it does take longer than typically on your own

732
00:37:46,040 --> 00:37:48,720
machine, and we get hello popping up here.

733
00:37:49,000 --> 00:37:52,360
So the great thing about Collaboratory is the fact that we can have multiple

734
00:37:52,360 --> 00:37:55,760
code blocks and we can run them in whatever sequence we want.

735
00:37:56,040 --> 00:37:58,680
So to create another code block, you can just, you know, do another

736
00:37:58,680 --> 00:38:01,840
code block from up here or by just by looking down here, you get code

737
00:38:01,840 --> 00:38:04,800
and you get text and I can run this in whatever order I want.

738
00:38:04,800 --> 00:38:06,840
So I could do like print.

739
00:38:06,840 --> 00:38:10,680
Yes, for example, I could run yes, and we'll see the output of yes.

740
00:38:10,680 --> 00:38:14,040
And then I could print hello one more time and notice that it's showing me

741
00:38:14,040 --> 00:38:18,040
the number on this left hand side here on which these kind of code blocks were

742
00:38:18,040 --> 00:38:22,040
run. Now, all of these code blocks can kind of access each other.

743
00:38:22,040 --> 00:38:26,320
So for example, I do define funk and we'll just take some parameter H.

744
00:38:26,360 --> 00:38:30,880
And all we'll do is just print H. Well, if I create another code block down here,

745
00:38:30,880 --> 00:38:37,080
so let's go code, I can call funk with say, hello, make sure I run this block

746
00:38:37,080 --> 00:38:41,240
first, so we define the function. Now I'll run funk and notice we get the output

747
00:38:41,240 --> 00:38:44,840
hello, so we can access all of the variables, all the functions, anything

748
00:38:44,840 --> 00:38:48,240
we've defined in other code blocks from code blocks that are below it or code

749
00:38:48,240 --> 00:38:51,600
blocks that have executed after it. Now, another thing that's great about

750
00:38:51,600 --> 00:38:54,880
collaboratory is the fact that we can import pretty much any module we can

751
00:38:54,880 --> 00:38:58,360
imagine, and we don't need to install it. So I'm not actually going to be going

752
00:38:58,360 --> 00:39:02,600
through how to install TensorFlow completely. There is a little bit on how

753
00:39:02,600 --> 00:39:06,320
to install TensorFlow on your local machine inside of this notebook, which

754
00:39:06,360 --> 00:39:09,280
I'll refer you to. But essentially, if you know how to use pip, it's pretty

755
00:39:09,280 --> 00:39:13,000
straightforward, you can pip install TensorFlow, or pip install TensorFlow

756
00:39:13,000 --> 00:39:16,720
GPU, if you have a compatible GPU, which you can check from the link that's in

757
00:39:16,720 --> 00:39:21,120
this notebook. Now, if I want to import something, what I can do is literally

758
00:39:21,120 --> 00:39:24,800
just write the import. So I can say import numpy like this. And usually numpy

759
00:39:24,800 --> 00:39:28,320
is a module that you need to install. But we don't need to do that here. It's

760
00:39:28,360 --> 00:39:31,240
already installed on the machine. So again, we hook up to those Google

761
00:39:31,240 --> 00:39:35,200
servers, we can use their hardware to perform machine learning. And this is

762
00:39:35,200 --> 00:39:38,680
awesome. This is amazing. And it gives you performance benefits when you're

763
00:39:38,680 --> 00:39:42,240
running on like a lower kind of crappier machine, right? So we can have a look

764
00:39:42,240 --> 00:39:45,680
at the RAM in the disk space of our computer, we can see we have 12 gigs of

765
00:39:45,680 --> 00:39:50,080
RAM, we're dealing with 107 gigabytes of data on our disk space. And we can

766
00:39:50,080 --> 00:39:54,080
obviously, you know, look at that if we want, we can connect to our local

767
00:39:54,080 --> 00:39:56,960
runtime, which I believe connects to your local machine. But I'm not going to go

768
00:39:56,960 --> 00:40:00,080
through all of that. I just want to show you guys some basic components of

769
00:40:00,080 --> 00:40:03,680
collaboratory. Now, some other things that are important to understand is this

770
00:40:03,680 --> 00:40:08,840
runtime tab, which you might see me use. So restart runtime essentially clears

771
00:40:08,840 --> 00:40:12,800
all of your output, and just restarts whatever's happened. Because the great

772
00:40:12,800 --> 00:40:16,640
thing with collaboratory is since I can run specific code blocks, I don't need

773
00:40:16,640 --> 00:40:21,080
to execute the entire thing of code every time I want to run something. If I've

774
00:40:21,080 --> 00:40:25,520
just made a minor change in one code block, I can just run that code. Sorry, I

775
00:40:25,520 --> 00:40:29,160
can just run that code block. I don't need to run everything before it or even

776
00:40:29,160 --> 00:40:33,200
everything after it, right? But sometimes you want to restart everything and just

777
00:40:33,240 --> 00:40:37,440
rerun everything. So to do that, you click restart runtime, that's just going to

778
00:40:37,440 --> 00:40:41,480
clear everything you have. And then restart and run all will restart the

779
00:40:41,480 --> 00:40:46,480
runtime as well as run every single block of code you have in sequential order in

780
00:40:46,480 --> 00:40:50,200
which it shows up in the thing. So I recommend you guys open up one of these

781
00:40:50,200 --> 00:40:53,480
windows. You can obviously follow along with this notebook if you want. But if

782
00:40:53,480 --> 00:40:56,880
you want to type it out on your own and kind of mess with it, open up a notebook,

783
00:40:57,120 --> 00:41:01,040
save it. It's very easy. And these are again, extremely similar to Jupiter

784
00:41:01,040 --> 00:41:06,160
notebooks, Jupiter notebooks, they're pretty much the same. Okay, so that is

785
00:41:06,160 --> 00:41:11,200
kind of the Google Collaboratory aspect how to use that. Let's get into importing

786
00:41:11,200 --> 00:41:15,160
TensorFlow. Now this is going to be kind of specific to Google Collaboratory. So

787
00:41:15,160 --> 00:41:18,320
you can see here, these are kind of the steps we need to follow to import

788
00:41:18,320 --> 00:41:22,400
TensorFlow. So since we're working in Google Collaboratory, they have

789
00:41:22,400 --> 00:41:25,520
multiple versions of TensorFlow, they have the original version of TensorFlow,

790
00:41:25,520 --> 00:41:30,360
which is 1.0, and the 2.0 version. Now to define the fact that we want to use

791
00:41:30,360 --> 00:41:34,840
TensorFlow 2.0, just because we're in this notebook, we need to write this line

792
00:41:34,840 --> 00:41:39,440
of code at the very beginning of all of our notebooks. So percent TensorFlow

793
00:41:39,440 --> 00:41:43,800
underscore version 2.x. Now this is simply just saying we need to use

794
00:41:43,800 --> 00:41:48,000
TensorFlow 2.x. So whatever version that is, and this is only required in a

795
00:41:48,000 --> 00:41:51,240
notebook, if you're doing this on your local machine in a text editor, you're

796
00:41:51,240 --> 00:41:55,080
not going to need to write this. Now once we do that, we typically import

797
00:41:55,120 --> 00:42:00,000
TensorFlow as an alias name of TF. Now to do that, we simply import the

798
00:42:00,000 --> 00:42:04,040
TensorFlow module, and then we write as TF. If you're on your local machine,

799
00:42:04,040 --> 00:42:07,280
again, you're going to need to install TensorFlow first to make sure that

800
00:42:07,280 --> 00:42:10,040
you're able to do this. But since we're in Collaboratory, we don't need to do

801
00:42:10,040 --> 00:42:14,600
that. Now, since we've defined the fact we're using version 2.x, when we

802
00:42:14,600 --> 00:42:19,520
print the TensorFlow version, we can see here that it says version two, which is

803
00:42:19,520 --> 00:42:23,680
exactly what we're looking for. And then it says TensorFlow 2.1.0. So make

804
00:42:23,680 --> 00:42:27,320
sure that you print your version, you're using version 2.0, because there is a

805
00:42:27,320 --> 00:42:31,640
lot of what I'm using in this series that is kind of, if you're in TensorFlow

806
00:42:31,640 --> 00:42:35,440
1.0, it's not going to work. So it's new in TensorFlow 2.0, or it's been

807
00:42:35,440 --> 00:42:39,160
refactored and the names have been changed. Okay, so now that we've done

808
00:42:39,160 --> 00:42:42,280
that, we've imported TensorFlow, we've got this here, and I'm actually going to

809
00:42:42,280 --> 00:42:45,600
go to my fresh notebook and just do this. So we'll just copy these lines over

810
00:42:45,600 --> 00:42:49,000
just so we have some fresh code, and I don't have all this text that we have to

811
00:42:49,040 --> 00:42:55,200
deal with. So let's do this TensorFlow, let's import TensorFlow as TF, and then

812
00:42:55,200 --> 00:43:01,840
we can print the TF dot version and have a look at that. So version. Okay, so

813
00:43:01,840 --> 00:43:05,160
let's run our code here, we can see TensorFlow is already loaded. Oh, it says

814
00:43:05,160 --> 00:43:08,680
1.0. So if you get this error, it's actually good, I ran into this where

815
00:43:08,680 --> 00:43:11,920
TensorFlow has already been loaded. All you need to do is just restart your

816
00:43:11,920 --> 00:43:15,560
runtime. So I'm going to restart and run all just click Yes. And now we should

817
00:43:15,600 --> 00:43:19,880
see that we get that version 2.0. Once this starts running, give it a second

818
00:43:19,880 --> 00:43:25,200
TensorFlow 2.0 selected, we're going to import that module. And there we go, we

819
00:43:25,200 --> 00:43:30,800
have version 2.0. Okay, so now it's time to talk about tensors. Now, what is a

820
00:43:30,840 --> 00:43:34,400
tensor? Now, tensor just immediately seems kind of like a complicated name, you're

821
00:43:34,400 --> 00:43:38,840
like, All right, tensor, like this is confusing. But what is it? Well, obviously

822
00:43:38,840 --> 00:43:42,600
this is going to be a primary aspect of TensorFlow, considering the name

823
00:43:42,600 --> 00:43:47,720
similarities. And essentially, all it is is a vector generalized to higher

824
00:43:47,720 --> 00:43:52,120
dimensions. Now, what is a vector? Well, if you've ever done any linear algebra

825
00:43:52,120 --> 00:43:55,480
or even some basic kind of vector calculus, you should hopefully know what

826
00:43:55,480 --> 00:44:00,000
that is. But essentially, it is kind of a data point is kind of the way that I

827
00:44:00,000 --> 00:44:03,920
like to describe it. And the reason we call it a vector is because it doesn't

828
00:44:03,920 --> 00:44:08,760
necessarily have a certain coordinate. So like if you're talking about a two

829
00:44:08,760 --> 00:44:12,840
dimensional data point, you have, you know, maybe an x and a y value, or like an

830
00:44:12,840 --> 00:44:18,000
x one value and an x two value. Now a vector can have any amount of dimensions

831
00:44:18,040 --> 00:44:22,000
in it, it could have one dimension, which simply means it's just one number, could

832
00:44:22,000 --> 00:44:25,480
have two dimensions, which means we're having two numbers. So like an x and a

833
00:44:25,480 --> 00:44:29,600
y value, if we're thinking about a two dimensional graph, we'd have three

834
00:44:29,600 --> 00:44:32,960
dimensions, if we're thinking about a three dimensional graph, so that would be

835
00:44:32,960 --> 00:44:36,440
three data points, we could have four dimensions, if we're talking about

836
00:44:36,480 --> 00:44:40,360
sometimes some image data and some video data, five dimensions, and we can

837
00:44:40,360 --> 00:44:45,120
keep going, going, going with vectors. So essentially, what a tensor is, and I'll

838
00:44:45,120 --> 00:44:48,160
just read this formal definition to make sure I haven't butchered anything

839
00:44:48,160 --> 00:44:52,160
that's from the actual TensorFlow website. A tensor is a generalization of

840
00:44:52,160 --> 00:44:56,480
vectors and matrices to potentially higher dimensions, internally TensorFlow

841
00:44:56,480 --> 00:45:00,400
represents tensors as n dimensional arrays of base data types. Now we'll

842
00:45:00,400 --> 00:45:05,400
understand what that means in a second, but hopefully that makes sense. Now, since

843
00:45:05,400 --> 00:45:09,120
tensors are so important to TensorFlow, they're kind of the main object that

844
00:45:09,120 --> 00:45:13,040
we're going to be working with, manipulating and viewing. And it's the main

845
00:45:13,040 --> 00:45:17,520
object that's passed around through our program. Now, what we can see here is

846
00:45:17,520 --> 00:45:21,480
each tensor represents a partially defined computation that will eventually

847
00:45:21,480 --> 00:45:25,880
produce a value. So just like we talked about in the graphs and sessions, what

848
00:45:25,880 --> 00:45:29,280
we're going to do is when we create our program, we're going to be creating a

849
00:45:29,280 --> 00:45:32,520
bunch of tensors and TensorFlow is going to be creating them as well. And those

850
00:45:32,520 --> 00:45:37,640
are going to store partially defined computations in the graph. Later, when we

851
00:45:37,640 --> 00:45:41,520
actually build the graph and have the session running, we will run different

852
00:45:41,520 --> 00:45:44,800
parts of the graph, which means we'll execute different tensors, and be able

853
00:45:44,800 --> 00:45:48,720
to get different results from our tensors. Now each tensor has what we call a

854
00:45:48,720 --> 00:45:53,640
data type and a shape, and that's we're going to get into now. So a data type is

855
00:45:53,640 --> 00:45:57,680
simply what kind of information is stored in the tensor. Now it's very rare that

856
00:45:57,680 --> 00:46:01,440
we see any data types different than numbers, although there is the data type

857
00:46:01,480 --> 00:46:04,560
of strings and a few others as well. But I haven't included all of them here

858
00:46:04,560 --> 00:46:08,840
because they're not that important. But some examples we can see our float 32 in

859
00:46:08,840 --> 00:46:14,720
32 string and others. Now the shape is simply the representation of the

860
00:46:14,720 --> 00:46:18,600
tensor in terms of what dimension it is. And we'll get some examples because I

861
00:46:18,600 --> 00:46:21,600
don't want to explain the shape until we can see some examples to really dial

862
00:46:21,600 --> 00:46:26,360
in. But here are some examples of how we would create different tensors. So what

863
00:46:26,360 --> 00:46:32,560
you can do is you can simply do TF dot variable. And then you can do the value

864
00:46:32,560 --> 00:46:36,920
and the data type that your tensor is. So in this case, we've created a string

865
00:46:36,920 --> 00:46:41,960
tensor which stores one string. And it is TF dot strings, we define the data type

866
00:46:41,960 --> 00:46:47,360
second, we have a number tensor which stores some integer value. And then that

867
00:46:47,360 --> 00:46:52,480
is up type TF int 16. And we have a floating point tensor, which stores a

868
00:46:52,520 --> 00:46:58,240
simple floating point. Now these tensors have a shape of I believe it's going to

869
00:46:58,240 --> 00:47:03,120
be one, which simply means they are a scalar. Now a scalar value and you might

870
00:47:03,120 --> 00:47:08,160
hear me say this a lot simply means just one value. That's all it means. When we

871
00:47:08,160 --> 00:47:12,720
talk about like vector values, that typically means more than one value. And

872
00:47:12,720 --> 00:47:16,840
we talk about matrices, we're having different it just it goes up but scalar

873
00:47:16,840 --> 00:47:22,240
simply means one number. So yeah, that is what we get for the different data

874
00:47:22,280 --> 00:47:25,320
types and creating tensors, we're not really going to do this very much in our

875
00:47:25,320 --> 00:47:29,440
program. But just for some examples here, that's how we do it. So we've imported

876
00:47:29,440 --> 00:47:31,960
them. So I can actually run these. And I mean, we're not going to really get any

877
00:47:31,960 --> 00:47:36,280
output by running this code because well, there's nothing to see. But now we're

878
00:47:36,280 --> 00:47:41,040
going to talk about the rank slash degree of tensors. So another word for rank is

879
00:47:41,040 --> 00:47:45,440
agree. So these are interchangeably. And again, this simply means the the number

880
00:47:45,440 --> 00:47:50,840
of dimensions involved in the tensor. So when we create a tensor of rank zero,

881
00:47:50,880 --> 00:47:54,800
which is what we've done up here, we call that a scalar. Now the reason this has

882
00:47:54,800 --> 00:47:59,760
rank zero is because it's simply one thing, we don't have any dimensions to

883
00:47:59,760 --> 00:48:04,320
this, there's like zero dimensionality of that. It was even a word, it's just one

884
00:48:04,320 --> 00:48:10,400
value. Whereas here, we have an array. Now when we have an array or a list, we

885
00:48:10,400 --> 00:48:15,360
immediately have at least rank one. Now the reason for that is because this

886
00:48:15,360 --> 00:48:19,120
array can store more than one value in one dimension, right? So I can do

887
00:48:19,120 --> 00:48:24,760
something like test, I could do okay, I could do Tim, which is my name, and we

888
00:48:24,760 --> 00:48:28,280
can run this and we're not going to get any output obviously here. But this is

889
00:48:28,280 --> 00:48:34,080
what we would call a rank one tensor, because it is simply one list, one array,

890
00:48:34,280 --> 00:48:38,840
which means one dimension. And again, you know, that's also like a vector. Now

891
00:48:38,840 --> 00:48:43,280
this, what we're looking at here is a rank to tensor. The reason this is a rank

892
00:48:43,320 --> 00:48:47,680
to tensor is because we have a list inside of a list, or in this case,

893
00:48:47,720 --> 00:48:52,000
multiple lists inside of a list. So the way that you can actually determine the

894
00:48:52,000 --> 00:48:57,760
rank of a tensor is the deepest level of a nested list, at least in Python with

895
00:48:57,760 --> 00:49:02,600
our representation, that's what that is. So here we can see we have a list inside

896
00:49:02,600 --> 00:49:06,320
of a list, and then another list inside of this upper list. So this would give us

897
00:49:06,480 --> 00:49:11,240
rank two. And this is what we typically call a matrices. And this again, is going

898
00:49:11,240 --> 00:49:16,560
to be of TF dot strings. So that's the data type for this tensor variable. So all

899
00:49:16,600 --> 00:49:20,120
of these that we've created are tensors, they have a data type, and they have some

900
00:49:20,120 --> 00:49:23,880
rank and some shape, and we're going to talk about the shape in a second. So to

901
00:49:23,880 --> 00:49:28,840
determine the rank of a tensor, we can simply use the method TF dot rank. So

902
00:49:28,840 --> 00:49:34,160
notice when I run this, we get the shape which is blank of rank to tensor. That's

903
00:49:34,160 --> 00:49:39,560
fine. And then we get num pi two, which simply means that this is of rank two. Now

904
00:49:39,560 --> 00:49:44,160
if I go for that rank one tensor, and I print this out. So let's have a look at

905
00:49:44,160 --> 00:49:48,840
it, we get num pi one here, which is telling us that this is simply of rank

906
00:49:48,920 --> 00:49:52,320
one. Now if I want to use one of these ones up here and see what it is, so let's

907
00:49:52,320 --> 00:49:57,320
try it, we can do numbers. So TF dot ring numbers. So we'll print that here. And

908
00:49:57,320 --> 00:50:00,680
we get num pi zero, because that's rank zero, right? So we'll go back to what we

909
00:50:00,680 --> 00:50:03,920
had, which was ranked to tensor. But again, those are kind of the examples we

910
00:50:03,920 --> 00:50:08,080
want to look at. Okay, so shapes of a tensor. So this is a little bit different

911
00:50:08,080 --> 00:50:13,520
now. What a shape simply tells us is how many items we have in each dimension. So

912
00:50:13,520 --> 00:50:18,720
in this case, when we're looking at rank two, tensor dot shape, so we have dot

913
00:50:18,720 --> 00:50:22,960
shape here, that's an attribute of all of our tensors, we get two two. Now let's

914
00:50:22,960 --> 00:50:28,400
look up here. What we have is Whoa, look at this two, and two. So we have two

915
00:50:28,400 --> 00:50:31,120
elements in the first dimension, right, and then two elements in the second

916
00:50:31,120 --> 00:50:35,200
dimension. That's pretty much what this is telling us. Now let's look at the rank

917
00:50:35,480 --> 00:50:40,960
for the shape of rank one tensor, we get three. So because we only have a rank

918
00:50:40,960 --> 00:50:46,560
one, notice we only get one number. Whereas when we had rank two, we got two

919
00:50:46,560 --> 00:50:49,880
numbers, and it told us how many elements were in each of these lists, right? So if

920
00:50:49,880 --> 00:50:54,480
I go and I add another one here, like that, and we have a look now at the shape.

921
00:50:55,080 --> 00:51:00,400
Oops, I got to run this first. So that's something can convert non square to

922
00:51:00,440 --> 00:51:04,320
tensor. Ah, sorry, so I need to have a uniform amount of elements in each one

923
00:51:04,320 --> 00:51:08,560
here, I can't just do what I did there. So add a third element here. Now what we

924
00:51:08,560 --> 00:51:14,120
can do is run this shouldn't get any issues. Let's have a look at the shape and

925
00:51:14,120 --> 00:51:19,880
notice we get now two three. So we have two lists, and each of those lists have

926
00:51:19,880 --> 00:51:24,000
three elements inside of them. So that's how the shape works. Now I could go ahead

927
00:51:24,040 --> 00:51:29,600
and add another list in here if I wanted to and I could say like, okay, okay,

928
00:51:30,960 --> 00:51:35,400
okay, so let's run this hopefully no errors. Looks like we're good. Now let's

929
00:51:35,400 --> 00:51:38,400
look at the shape again. And now we get a shape of three, three, because we have

930
00:51:38,440 --> 00:51:42,640
three interior lists. And in each of those lists, we have three elements. And

931
00:51:42,640 --> 00:51:46,880
that is pretty much how that works. Now again, we could go even further here and

932
00:51:46,880 --> 00:51:50,760
we could put another list inside of here that would give us a rank three tensor.

933
00:51:50,760 --> 00:51:54,520
And we'd have to do that inside of all of these lists. And then what that would

934
00:51:54,520 --> 00:51:58,680
give us now would be three numbers representing how many elements we have in

935
00:51:58,720 --> 00:52:04,600
each of those different dimensions. Okay, so changing shape. Alright, so this is

936
00:52:04,640 --> 00:52:08,000
what we need to do a lot of times when we're dealing with tensors and tensor

937
00:52:08,000 --> 00:52:12,360
flow. So essentially, there is many different shapes that can represent the

938
00:52:12,360 --> 00:52:18,160
same number of elements. So up here, we have three elements in a rank one

939
00:52:18,160 --> 00:52:23,400
tensor. And then here we have nine elements in a rank two tensor. Now there's

940
00:52:23,400 --> 00:52:27,600
ways that we can reshape this data so that we have the same amount of

941
00:52:27,600 --> 00:52:31,840
elements, but in a different shape. For example, I could flatten this, right,

942
00:52:31,840 --> 00:52:36,400
take all of these elements and throw them into a rank one tensor that simply is

943
00:52:36,400 --> 00:52:40,840
a length of nine elements. So how do we do that? Well, let me just run this code

944
00:52:40,840 --> 00:52:43,520
for us here and have a look at this. So what we've done is we've created tensor

945
00:52:43,520 --> 00:52:48,000
one, that is TF dot ones, what this stands for is we're going to create a

946
00:52:48,000 --> 00:52:54,880
tensor that simply is populated completely with ones of this shape. So shape one,

947
00:52:54,880 --> 00:52:58,560
two, three, which means, you know, that's the shape we're going to get. So let's

948
00:52:58,560 --> 00:53:03,520
print this out and look at tensor one, just so I can better illustrate this. So

949
00:53:03,520 --> 00:53:09,640
tensor one, look at the shape that we have one, two, three, right? So we have one

950
00:53:09,640 --> 00:53:13,440
interior list, which we're looking at here. And then we have two lists inside

951
00:53:13,440 --> 00:53:17,200
of that list. And then each of those lists, we have three elements. So that's

952
00:53:17,200 --> 00:53:21,800
the shape we just defined. Now we have six elements inside of here. So there

953
00:53:21,800 --> 00:53:25,680
must be a way that we can reshape this data to have six elements, but in a

954
00:53:25,720 --> 00:53:30,560
different shape. In fact, what we can do is reshape this into a two, three, one

955
00:53:30,560 --> 00:53:34,200
shape, where we're going to have two lists, right? We're going to have three

956
00:53:34,200 --> 00:53:36,800
inside of those. And then inside of each of those, we're going to have one

957
00:53:36,800 --> 00:53:40,600
element. So let's have a look at that one. So let's have a look at tensor two.

958
00:53:40,600 --> 00:53:43,360
Actually, what am I doing? We print all we can print all of them here. So let's

959
00:53:43,360 --> 00:53:46,680
just print them and have a look at them. So when we look at tensor one, we saw

960
00:53:46,680 --> 00:53:50,840
this was a shape. And now we look at this tensor two. And we can see that we

961
00:53:50,840 --> 00:53:55,240
have two lists, right? Inside of each of those lists, we have three lists. And

962
00:53:55,240 --> 00:53:59,720
inside of each of those lists, we have one element. Now, finally, our tensor

963
00:53:59,720 --> 00:54:05,160
three is a shape of three negative one. Well, what is negative one? When we put

964
00:54:05,160 --> 00:54:09,840
negative one here, what this does is infer what this number actually needs to

965
00:54:09,840 --> 00:54:14,880
be. So if we define an initial shape of three, what this does is say, Okay, we're

966
00:54:14,880 --> 00:54:19,640
going to have three lists. That's our first level. And then we need to figure

967
00:54:19,640 --> 00:54:23,240
out based on how many elements we have in this reshape, which is the method we're

968
00:54:23,240 --> 00:54:26,440
using, which I didn't even talk about, which we'll go into a second, what this

969
00:54:26,440 --> 00:54:30,840
next dimension should be. Now, obviously, this is going to need to be three. So three

970
00:54:30,840 --> 00:54:34,240
three, right, because we're going to have three lists inside of each of those lists

971
00:54:34,240 --> 00:54:37,000
we need to have. Or actually, is that correct? Let's see if that's even the

972
00:54:37,000 --> 00:54:41,400
shape, three, two, my bad. So this actually needs to change to three, two, I

973
00:54:41,400 --> 00:54:45,000
don't know why I wrote three, three there. But you get the point, right? So what

974
00:54:45,000 --> 00:54:48,160
this does is we have three lists, we have six elements, this number obviously needs

975
00:54:48,200 --> 00:54:51,720
to be two, because well, three times two is going to give us six. And that is

976
00:54:51,720 --> 00:54:55,360
essentially how you can determine how many elements are actually in a tensor by

977
00:54:55,360 --> 00:54:59,600
just looking at its shape. Now, this is the reshape method, where all we need to

978
00:54:59,600 --> 00:55:03,680
do is call tf dot reshape, give the tensor and give the shape we want to change

979
00:55:03,680 --> 00:55:08,120
it to. So long as that's a valid shape. And when we multiply all of the numbers

980
00:55:08,120 --> 00:55:11,920
in here, it's equal to the number of elements in this tensor that will reshape

981
00:55:11,920 --> 00:55:16,280
it for us and give us that new shaped data. This is very useful. We'll use this

982
00:55:16,320 --> 00:55:19,680
actually a lot as we go through TensorFlow. So make sure you're kind of

983
00:55:19,680 --> 00:55:23,880
familiar with how that works. All right. So now we're moving on to types of

984
00:55:23,920 --> 00:55:28,680
tensors. So there is a bunch of different types of tensors that we can use. So

985
00:55:28,680 --> 00:55:33,400
far, the only one we've looked at is variable. So we've created tf dot

986
00:55:33,400 --> 00:55:36,680
variables and kind of just hard coded our own tensors. We're not really going to

987
00:55:36,680 --> 00:55:40,960
do that very much. But just for that example. So we have these different

988
00:55:40,960 --> 00:55:45,640
types, we have constant placeholder sparse tensor variable. And there's actually

989
00:55:45,640 --> 00:55:49,960
a few other ones as well. Now, we're not going to really talk about these two

990
00:55:50,000 --> 00:55:53,600
that much, although constant and variable are important to understand the

991
00:55:53,600 --> 00:55:57,760
difference between. So we can read this says with the exception of variable, all

992
00:55:57,760 --> 00:56:01,080
of these tensors are immutable, meaning their value may not change during

993
00:56:01,120 --> 00:56:05,480
execution. So essentially, all of these when we create a tensor mean we have

994
00:56:05,480 --> 00:56:09,520
some constant value, which means that whatever we've defined here, it's not

995
00:56:09,520 --> 00:56:14,160
going to change. Whereas the variable tensor could change. So that's just

996
00:56:14,160 --> 00:56:17,560
something to keep in mind when we use variable, that's because we think we

997
00:56:17,560 --> 00:56:20,760
might need to change the value of that tensor later on. Whereas if we're using

998
00:56:20,760 --> 00:56:24,080
a constant value tensor, we cannot change it. So that's just something to keep

999
00:56:24,080 --> 00:56:28,440
in mind, we can obviously copy it, but we can't change it. Okay, so evaluating

1000
00:56:28,440 --> 00:56:31,080
tensors, we're almost at the end of the section, I know, and then we'll get into

1001
00:56:31,080 --> 00:56:35,280
some more kind of deeper code. So there will be some times for this guide, we

1002
00:56:35,280 --> 00:56:38,960
need to evaluate a tense, of course, so what we need to do to evaluate a tensor

1003
00:56:38,960 --> 00:56:43,880
is create a session. Now, this isn't really like, we're not going to do this that

1004
00:56:43,880 --> 00:56:47,000
much. But I just figured I'd mention it to make sure that you guys are aware of

1005
00:56:47,000 --> 00:56:51,000
what I'm doing. If I start kind of typing this later on. Essentially, sometimes

1006
00:56:51,000 --> 00:56:54,720
we have some tensor object. And throughout our code, we actually need to

1007
00:56:54,720 --> 00:56:59,520
evaluate it to be able to do something else. So to do that, all we need to do

1008
00:56:59,560 --> 00:57:04,240
is literally just use this kind of default template, a block of code. Well, we

1009
00:57:04,400 --> 00:57:08,440
say with TF dot session, as some kind of session doesn't really matter what we

1010
00:57:08,440 --> 00:57:13,760
put here, then we can just do whatever the tensor name is dot eval. And calling

1011
00:57:13,760 --> 00:57:17,240
that will actually have TensorFlow just figure out what it needs to do to find

1012
00:57:17,240 --> 00:57:20,600
the value of this tensor, it will evaluate it, and then it will allow us to

1013
00:57:20,600 --> 00:57:23,640
actually use that value. So I put this in here, you guys can obviously read

1014
00:57:23,640 --> 00:57:26,960
through this if you want to understand some more in depth on how that works. And

1015
00:57:26,960 --> 00:57:30,080
the source for this is straight from the TensorFlow website. A lot of this is

1016
00:57:30,080 --> 00:57:34,200
straight up copied from there. And I've just kind of added my own spin to it and

1017
00:57:34,200 --> 00:57:37,760
made it a little bit easier to understand. Okay, so we've done all that. So let's

1018
00:57:37,760 --> 00:57:40,880
just go in here and do a few examples of reshaping just to make sure that

1019
00:57:40,880 --> 00:57:43,800
everyone's kind of on the same page. And then we'll move on to actually talking

1020
00:57:43,800 --> 00:57:47,840
about some simple learning algorithms. So I want to create a tensor that we can

1021
00:57:47,840 --> 00:57:51,960
kind of mess with in reshape. So what I'm going to do is just say t equals and

1022
00:57:51,960 --> 00:57:57,560
we'll say TF dot ones. Now what TF dot ones does is just create again, all of

1023
00:57:57,560 --> 00:58:01,480
the values to be ones that we're going to have and whatever shape. Now we can

1024
00:58:01,480 --> 00:58:05,160
also do zeros and zeros is just going to give us a bunch of zeros. And let's

1025
00:58:05,160 --> 00:58:08,120
create some like crazy shape and just visualize this. Let's see like a five

1026
00:58:08,120 --> 00:58:11,760
by five by five. So obviously, if we want to figure out how many elements are

1027
00:58:11,760 --> 00:58:14,720
going to be in here, we need to multiply this value. So I believe this is going to

1028
00:58:14,720 --> 00:58:18,600
be 625 because that should be five to the power of four. So five times five times

1029
00:58:18,600 --> 00:58:23,160
five times five. And let's actually print T and have a look at that and see

1030
00:58:23,160 --> 00:58:26,760
what this is. So we run this now. And you can see this is the output we're

1031
00:58:26,760 --> 00:58:30,600
getting. So obviously, this is a pretty crazy looking tensor, but you get the

1032
00:58:30,600 --> 00:58:35,480
point, right? And it tells us the shape is 55555. Now watch what happens when I

1033
00:58:35,520 --> 00:58:40,160
reshape this tensor. So if I want to take all of these elements and flatten them

1034
00:58:40,160 --> 00:58:48,160
out, what I could do is simply say, we'll say T equals TF dot reshape like

1035
00:58:48,160 --> 00:58:55,040
that. And we'll reshape the tensor T to just the shape 625. Now if we do this

1036
00:58:55,080 --> 00:59:01,240
and we run here, oops, I got a print T at the bottom after we've done that if I

1037
00:59:01,280 --> 00:59:06,240
could spell the print statement correctly, you can see that now we just get this

1038
00:59:06,240 --> 00:59:12,080
massive list that just has 625 zeros. And again, if we wanted to reshape this to

1039
00:59:12,080 --> 00:59:15,360
something like 125, and maybe we weren't that good at math and couldn't figure out

1040
00:59:15,360 --> 00:59:19,320
that this last value should be five, we could put a negative one, this would mean

1041
00:59:19,320 --> 00:59:23,200
that TensorFlow would infer now what the shape needs to be. And now when we look

1042
00:59:23,200 --> 00:59:27,200
at it, we can see that we're what we're going to get is well, just simply five

1043
00:59:27,200 --> 00:59:31,600
kind of sets of these, I don't know, matrices, whatever you want to call them in

1044
00:59:31,600 --> 00:59:37,000
our shape is 125 five. So that is essentially how that works. So that's how

1045
00:59:37,000 --> 00:59:41,480
we reshape. That's how we kind of deal with tensors create variables, how that

1046
00:59:41,480 --> 00:59:44,640
works in terms of sessions and graphs. And hopefully with that, that gives you

1047
00:59:44,640 --> 00:59:50,600
enough of an understanding of tensors of shapes of ranks of value so that when we

1048
00:59:50,600 --> 00:59:53,440
move into the next part of the tutorial, where we're actually writing code, and I

1049
00:59:53,440 --> 00:59:56,440
promise we're going to be writing some more advanced code, you'll understand how

1050
00:59:56,480 --> 01:00:03,480
that works. So with that being said, let's get into the next section. So welcome

1051
01:00:03,480 --> 01:00:07,080
to module three of this course. Now what we're going to be doing in this module is

1052
01:00:07,080 --> 01:00:11,320
learning the core machine learning algorithms that come with TensorFlow. Now

1053
01:00:11,320 --> 01:00:14,480
these algorithms are not specific to TensorFlow, but they are used within

1054
01:00:14,480 --> 01:00:17,640
there and we'll use some tools from TensorFlow to kind of implement them. But

1055
01:00:17,640 --> 01:00:20,800
essentially, these are the building blocks before moving on to things like

1056
01:00:20,800 --> 01:00:24,280
neural networks and more advanced machine learning techniques. You really need to

1057
01:00:24,320 --> 01:00:27,960
understand how these work because they're kind of used in a lot of different

1058
01:00:27,960 --> 01:00:30,960
techniques and combined together. And one of them but to show you is actually

1059
01:00:30,960 --> 01:00:35,080
very powerful if you use it in the right way. A lot of what machine learning

1060
01:00:35,080 --> 01:00:38,200
actually is in a lot of machine learning algorithms and implementations and

1061
01:00:38,200 --> 01:00:42,800
businesses and applications and stuff like that, actually just use pretty basic

1062
01:00:43,120 --> 01:00:46,680
models, because these models are capable of actually doing, you know, very

1063
01:00:46,680 --> 01:00:50,080
powerful things. When you're not dealing with anything that's crazy complicated,

1064
01:00:50,080 --> 01:00:53,680
you just need some basic machine learning, some basic classification, you can

1065
01:00:53,680 --> 01:00:57,680
use these kind of fundamental core learning algorithms. Now the first one

1066
01:00:57,680 --> 01:01:00,120
we're going to go through is a linear regression, but we will cover

1067
01:01:00,120 --> 01:01:04,280
classification, clustering and hidden Markov models. And those are kind of

1068
01:01:04,280 --> 01:01:08,720
going to give us a good spread of the different core algorithms. Now there is

1069
01:01:08,760 --> 01:01:12,840
a ton, ton, like thousands of different machine learning algorithms. These are

1070
01:01:12,840 --> 01:01:16,320
kind of the main categories that you'll cover. But within these categories,

1071
01:01:16,320 --> 01:01:19,400
there is more specific algorithms that you can get into. I just feel like I

1072
01:01:19,400 --> 01:01:22,680
need to mention that because I know a lot of you will have maybe seen some

1073
01:01:22,680 --> 01:01:25,400
different ways of doing things in this course might show you, you know, a

1074
01:01:25,400 --> 01:01:28,640
different perspective on that. So let me just quickly talk about how I'm going

1075
01:01:28,640 --> 01:01:32,040
to go through this. It's very similar to before I have this notebook, as I've

1076
01:01:32,040 --> 01:01:35,040
kind of talked about, there is a link in the description, I would recommend that

1077
01:01:35,040 --> 01:01:38,440
you guys hit that and follow along with what I'm doing and read through the

1078
01:01:38,440 --> 01:01:41,440
notebook, but I will just be going through the notebook. And then occasionally

1079
01:01:41,440 --> 01:01:45,640
what I will actually do, oops, I need to open this up here is go to this kind

1080
01:01:45,640 --> 01:01:49,200
of untitled tab I have here and write some code in here. Because most of what

1081
01:01:49,240 --> 01:01:53,000
I'm going to do is just copy code over into here so we can see it all in kind

1082
01:01:53,000 --> 01:01:57,200
of one block. And then we'll be good to go. And the last note before we really

1083
01:01:57,200 --> 01:02:00,000
get into it, and I'm sorry I'm talking a lot, but it is important to make you

1084
01:02:00,000 --> 01:02:03,200
guys aware of this, you're going to see that we use a lot of complicated

1085
01:02:03,200 --> 01:02:07,120
syntax throughout this kind of series and the rest of the course in general. I

1086
01:02:07,120 --> 01:02:11,080
just want to make it extremely clear that you should not have to memorize or

1087
01:02:11,080 --> 01:02:15,520
even feel obligated to memorize any of the syntax that you see, everything that

1088
01:02:15,520 --> 01:02:19,160
you see here, I personally don't even have memorized is a lot of what's in here

1089
01:02:19,160 --> 01:02:22,640
that I can't just come up with on the top of my head. When we're dealing with

1090
01:02:22,640 --> 01:02:26,960
kind of a library and modules so big that like TensorFlow, it's hard to

1091
01:02:26,960 --> 01:02:30,200
memorize all those different components. So just make sure you understand what's

1092
01:02:30,200 --> 01:02:33,240
happening, but you don't need to memorize it. If you're ever going to need to use

1093
01:02:33,240 --> 01:02:35,880
any of these tools, you're going to look them up, you're going to see what it is

1094
01:02:35,880 --> 01:02:38,280
you're going to be like, okay, I've used this before, you're going to understand

1095
01:02:38,280 --> 01:02:41,040
it, and then you can go ahead and you know, copy that code in and use it in

1096
01:02:41,040 --> 01:02:44,760
whatever way you need to, you don't need to memorize anything that we do. All

1097
01:02:44,760 --> 01:02:48,720
right, so let's go ahead and get started with linear regression. So what is

1098
01:02:48,760 --> 01:02:52,520
linear regression? What's one of those basic forms of machine learning? And

1099
01:02:52,520 --> 01:02:56,640
essentially, what we try to do is have a linear correspondence between data

1100
01:02:56,640 --> 01:02:59,960
points. So I'm just going to scroll down here, do a good example. So what I've

1101
01:02:59,960 --> 01:03:03,760
done is use map plot live just to plot a little graph here. So we can see this

1102
01:03:03,760 --> 01:03:07,360
one right here. And essentially, this is kind of our data set. This is what we'll

1103
01:03:07,360 --> 01:03:11,800
call your data set. What we want to do is use linear regression to come up with

1104
01:03:11,840 --> 01:03:15,400
a model that can give us some good predictions for our data points. So in

1105
01:03:15,400 --> 01:03:19,320
this instance, maybe what we want to do is given some x value for a data point,

1106
01:03:19,320 --> 01:03:23,720
we want to predict the y value. Now, in this case, we can see there is kind of

1107
01:03:23,720 --> 01:03:28,360
some correspondence linearly for these data points. Now, what that means is we

1108
01:03:28,360 --> 01:03:32,360
can draw something called a line of best fit through these data points that can

1109
01:03:32,400 --> 01:03:36,320
kind of accurately classify them, if that makes any sense. So I'm going to

1110
01:03:36,320 --> 01:03:39,920
scroll down here and look at what our line of best fit for this data set

1111
01:03:39,920 --> 01:03:43,880
actually is, you can see this blue line, a pretty much, I mean, it is the

1112
01:03:43,880 --> 01:03:48,800
perfect line of best fit for this data set. And using this line, we can actually

1113
01:03:48,800 --> 01:03:53,520
predict future values in our data set. So essentially, linear regression is used

1114
01:03:53,520 --> 01:03:57,560
when you have data points that correlate in kind of a linear fashion. Now, this is

1115
01:03:57,560 --> 01:04:02,160
a very basic example, because we're doing this in two dimensions with x and y. But

1116
01:04:02,160 --> 01:04:05,720
oftentimes, what you'll have is you'll have data points that have, you know, eight

1117
01:04:05,720 --> 01:04:10,160
or nine kind of input values. So that gives us, you know, a nine dimensional kind

1118
01:04:10,200 --> 01:04:13,680
of data set. What we'll do is predict one of the different values. So in the

1119
01:04:13,680 --> 01:04:16,280
instance where we were talking about students before, maybe we have a

1120
01:04:16,280 --> 01:04:20,000
student, what is it midterm grade, and their second midterm grade, and then we

1121
01:04:20,000 --> 01:04:23,920
want to predict their final grade, what we can do is use linear regression to do

1122
01:04:23,920 --> 01:04:27,440
that, where our kind of input values are going to be the two midterm grades and

1123
01:04:27,440 --> 01:04:31,960
the output value is going to be that final grade that we're looking to predict. So

1124
01:04:31,960 --> 01:04:35,800
if we were to plot that, we would plot that on a three dimensional graph, and we

1125
01:04:35,800 --> 01:04:39,800
would draw a three dimensional line that would represent the line of best fit for

1126
01:04:39,800 --> 01:04:43,200
that data set. Now, for any of you that don't know what line of best fit stands

1127
01:04:43,200 --> 01:04:46,920
for, it says line, or this is just the definition I got from this website here,

1128
01:04:47,120 --> 01:04:50,440
line of best fit refers to a line through a scatter plot of data points that

1129
01:04:50,440 --> 01:04:54,080
best expresses the relationship between those points. So exactly what I've kind

1130
01:04:54,080 --> 01:04:58,320
of been trying to explain, when we have data that correlates linearly, and I

1131
01:04:58,360 --> 01:05:02,320
always butcher that word, what we can do is draw a line through it, and then we

1132
01:05:02,320 --> 01:05:06,160
can use that line to predict new data points, because if that line is good,

1133
01:05:06,200 --> 01:05:10,480
it's a good line of best fit for the data set, then hopefully we would assume

1134
01:05:10,480 --> 01:05:14,360
that we can just, you know, pick some point, find where it would be on that

1135
01:05:14,360 --> 01:05:18,160
line, and that'll be kind of our predicted value. So I'm going to go into an

1136
01:05:18,160 --> 01:05:20,800
example now where I start drawing and going into a little bit of math. So we

1137
01:05:20,800 --> 01:05:23,960
understand how this works on a deeper level. But that should give you a

1138
01:05:23,960 --> 01:05:26,800
surface level understanding. So actually, I'll leave this up because I was

1139
01:05:26,960 --> 01:05:31,400
messing with this beforehand. This is kind of a data set that I've drawn on

1140
01:05:31,400 --> 01:05:36,000
here. So we have our x, and we have our y, and we have our line of best fit. Now,

1141
01:05:36,000 --> 01:05:39,640
what I want to do is I want to use this line of best fit to predict a new

1142
01:05:39,640 --> 01:05:42,920
data point. So all these red data points are ones that we've trained our model

1143
01:05:42,920 --> 01:05:46,320
with their information that we gave to the model so that it could create this

1144
01:05:46,320 --> 01:05:50,600
line of best fit. Because essentially, all linear regression really does is look

1145
01:05:50,600 --> 01:05:55,200
at all of these data points and create a line of best fit for them. That's all it

1146
01:05:55,200 --> 01:05:59,720
does. It's pretty, I don't know the word for it. It's pretty easy to actually do

1147
01:05:59,720 --> 01:06:02,600
this. This algorithm is not that complicated. It's not that advanced. And

1148
01:06:02,640 --> 01:06:06,360
that's why we start with it here, because it just makes sense to explain. So I

1149
01:06:06,360 --> 01:06:10,520
hope that a lot of you would know in two dimensions, a line can be defined as

1150
01:06:10,520 --> 01:06:16,680
follows. So with the equation y equals mx plus b. Now b stands for the y

1151
01:06:16,680 --> 01:06:20,280
intercept, which means somewhere on this line. So essentially, where the line

1152
01:06:20,280 --> 01:06:25,120
starts. So in this instance, our b value is going to be right here. So this is

1153
01:06:25,120 --> 01:06:29,120
going to be b, because that is the y intercept. So we could say that that's

1154
01:06:29,160 --> 01:06:34,440
like maybe, you know, we go on, we'll do this, we'll say this is like 123, we

1155
01:06:34,440 --> 01:06:39,200
might say b is something like 0.4, right? So I could just pencil that into 0.4.

1156
01:06:40,120 --> 01:06:45,160
And then what is mx and y? Well, x and y stand for the coordinates of this

1157
01:06:45,160 --> 01:06:50,360
data point. So this would have, you know, some x, y value. In this case, we might

1158
01:06:50,360 --> 01:06:55,200
call it, you know, something like, what do you want to say to 2.7, that might be

1159
01:06:55,200 --> 01:07:00,400
the value of this data point. So that's our x and y. And then our m stands for

1160
01:07:00,400 --> 01:07:05,080
the slope, which is probably the most important part. Now slope simply defines

1161
01:07:05,280 --> 01:07:09,200
the steepness of this line of best fit that we've done here. Now the way we

1162
01:07:09,200 --> 01:07:13,840
calculate slope is using rise over run. Now rise over run essentially just

1163
01:07:13,840 --> 01:07:17,480
means how much we went up versus how much we went across. So if you want to

1164
01:07:17,480 --> 01:07:20,800
calculate the slope of a line, what you can actually do is just draw a triangle.

1165
01:07:21,440 --> 01:07:25,840
So a right angle triangle anywhere on the line. So just pick two data points. And

1166
01:07:25,840 --> 01:07:29,720
what you can do is calculate this distance, and this distance. And then you

1167
01:07:29,720 --> 01:07:33,800
can simply divide the distance up by the distance across. And that gives you the

1168
01:07:33,800 --> 01:07:36,600
slope. I'm not going to go too far into slope because I feel like you guys

1169
01:07:36,600 --> 01:07:40,320
probably understand what that is. But let's just pick some values for this line.

1170
01:07:40,320 --> 01:07:43,200
And I want to actually show you some real examples of math and how we're going

1171
01:07:43,200 --> 01:07:46,800
to do this. So let's say that our linear regression algorithm, you know, comes up

1172
01:07:46,800 --> 01:07:49,840
with this line, I'm not going to discuss really how it does that, although it

1173
01:07:49,840 --> 01:07:53,360
just pretty much looks at all these data points, and finds a line that you know,

1174
01:07:53,400 --> 01:07:58,600
goes, it splits these data points evenly. So essentially, you want to be as close

1175
01:07:58,600 --> 01:08:02,680
to every data point as possible. And you want to have as many data points, you

1176
01:08:02,680 --> 01:08:05,600
want to have like the same amount of data points on the left side and the right

1177
01:08:05,600 --> 01:08:08,600
side of the line. So in this example, we have, you know, a data point on the

1178
01:08:08,600 --> 01:08:11,840
left, a data point on the left, we have what two that are pretty much on the

1179
01:08:11,840 --> 01:08:15,040
line. And then we have two that are on the right. So this is a pretty good line

1180
01:08:15,040 --> 01:08:19,400
of best fit, because all of the points are very close to the line. And they

1181
01:08:19,440 --> 01:08:23,680
split them evenly. So that's kind of how you come up with a line of best fit. So

1182
01:08:23,680 --> 01:08:27,640
let's say that the equation for this line is something like y equals, let's just

1183
01:08:27,640 --> 01:08:35,440
give it 1.5 and x plus and let's say that value is just 0.5 to make it easy. So

1184
01:08:35,440 --> 01:08:39,200
this is going to be the equation of our line. Now notice that x and y don't have

1185
01:08:39,200 --> 01:08:42,960
a value, that's because we need to give the value to come up with one of the

1186
01:08:42,960 --> 01:08:47,360
other ones. So what we can do is we can say if we have either the y value or we

1187
01:08:47,360 --> 01:08:51,560
have the x value of some point, and we want to figure out, you know, where it

1188
01:08:51,560 --> 01:08:56,320
is on the line, what we can do is just feed one in, do a calculation, and that

1189
01:08:56,320 --> 01:08:59,520
will actually give us the other value. So in this instance, let's say that you

1190
01:08:59,520 --> 01:09:03,320
know, I'm trying to predict something and I'm given the that the fact that x

1191
01:09:03,360 --> 01:09:07,760
equals two, I know that x equals two, and I want to figure out what y would be

1192
01:09:07,800 --> 01:09:12,280
if x equals two. Well, I can use this line to do so. So what I would do is I'm

1193
01:09:12,280 --> 01:09:20,560
going to say y equals 1.5 times two plus 0.5. Now, all of you quick math majors

1194
01:09:20,560 --> 01:09:25,480
out there give me the value of 3.5, which means that if x was at two, then I

1195
01:09:25,480 --> 01:09:29,920
would have my data point as a prediction here on this line. And I would say, okay,

1196
01:09:29,920 --> 01:09:33,400
so if you're telling me x is two, my prediction is that y is going to be

1197
01:09:33,400 --> 01:09:37,560
equal to 3.5, because given the line of best fit for this data set, that's where

1198
01:09:37,560 --> 01:09:42,480
this point will lie on that line. So I hope that makes sense. You can actually

1199
01:09:42,480 --> 01:09:46,680
do this the reverse way as well. So if I'm just given some y values to say, I

1200
01:09:46,680 --> 01:09:51,840
know that, you know, my y value is at like 2.7 or something, I can plug that in,

1201
01:09:51,840 --> 01:09:55,440
just rearrange the numbers in this equation and then solve for x. Now,

1202
01:09:55,440 --> 01:09:58,720
obviously, this is a very basic example, because we're just doing all of this in

1203
01:09:58,720 --> 01:10:02,560
two dimensions. But you can do this in higher dimensions as well. So actually,

1204
01:10:02,560 --> 01:10:05,160
most times, what's going to end up happening is you're going to have, you

1205
01:10:05,200 --> 01:10:08,680
know, like eight or nine input variables. And then you're going to have one output

1206
01:10:08,680 --> 01:10:12,960
variable that you're predicting. Now, so long as our data points are correlated

1207
01:10:12,960 --> 01:10:16,600
linearly in three dimensions, we can still do this. So I'm going to attempt to

1208
01:10:16,600 --> 01:10:19,880
show you this actually, in three dimensions, just to hopefully clear some

1209
01:10:19,880 --> 01:10:24,280
things up, because it is important to kind of get a grasp and perspective of the

1210
01:10:24,280 --> 01:10:28,720
different dimensions. So let's say we have a bunch of data points that are kind

1211
01:10:28,720 --> 01:10:34,040
of like this, now I'm trying my best to kind of draw them in some linear fashion

1212
01:10:34,120 --> 01:10:38,680
using like all the dimensions here. But it is hard because drawing in three

1213
01:10:38,680 --> 01:10:42,280
dimensions on a two dimensional screen is not easy. Okay, so let's say this is

1214
01:10:42,280 --> 01:10:45,680
kind of like what our data points look like. Now, I would say that these

1215
01:10:45,680 --> 01:10:50,160
correlate linearly, like pretty, pretty well, they kind of go up in one fashion,

1216
01:10:50,160 --> 01:10:53,440
and we don't know the scale of this. So this is probably fun. So the line of

1217
01:10:53,440 --> 01:10:57,720
best fit for this data set, and I'll just put my kind of thickness up might be

1218
01:10:57,720 --> 01:11:03,080
something like this, right? Now notice that this line is in three dimensions,

1219
01:11:03,080 --> 01:11:08,560
right? This is going to cross our, I guess this is our x, y, and z axes. So we

1220
01:11:08,560 --> 01:11:11,440
have a three dimensional line. Now the equation for this line is a little bit

1221
01:11:11,440 --> 01:11:14,920
more complicated. I'm not going to talk about exactly what it is. But essentially

1222
01:11:14,920 --> 01:11:18,960
what we do is we make this line, and then we say, Okay, what value do I want to

1223
01:11:18,960 --> 01:11:25,760
predict? Do I want to predict y, x or z? Now, so long as I have two values, so two

1224
01:11:25,800 --> 01:11:29,240
values, I can always predict the other one. So if I have, you know, the x, y of

1225
01:11:29,280 --> 01:11:35,120
the data point, that will give me the z. And if I have the z, y, that will give

1226
01:11:35,120 --> 01:11:40,520
me the x. So so long as you have, you know, all of the data points, except one,

1227
01:11:40,680 --> 01:11:44,320
you can always find what that point is, based on the fact that, you know, we have

1228
01:11:44,320 --> 01:11:47,680
this line, and we're using that to predict. So I think I'm going to leave it at

1229
01:11:47,680 --> 01:11:52,440
that for the explanation. I hope that makes sense. Again, just understand that we

1230
01:11:52,440 --> 01:11:56,200
use linear regression when our data points are correlated linearly. Now some

1231
01:11:56,200 --> 01:11:59,480
good examples of linear regression were, you know, that kind of student

1232
01:11:59,480 --> 01:12:03,520
predicting the grade kind of thing, you would assume that if someone has, you

1233
01:12:03,520 --> 01:12:07,080
know, a low grade, then they would finish with a lower grade, and you would

1234
01:12:07,080 --> 01:12:10,720
assume if they have a higher grade, they would finish with a higher grade. Now you

1235
01:12:10,720 --> 01:12:14,680
could also do something like predicting, you know, future life expectancy. Now this

1236
01:12:14,680 --> 01:12:18,360
is kind of a darker example. But essentially, what you could think of here is

1237
01:12:18,360 --> 01:12:23,080
if someone is older, they're expected to live, you know, like not as long. Or you

1238
01:12:23,080 --> 01:12:26,520
could look at health conditions, if someone is in critical illness condition,

1239
01:12:26,560 --> 01:12:30,240
they have a critical illness, then chances are their life expectancy is lower. So

1240
01:12:30,240 --> 01:12:33,400
that's an example of something that is correlated linearly. Essentially,

1241
01:12:33,400 --> 01:12:36,040
something goes up, and something goes down, or something goes up, the other

1242
01:12:36,040 --> 01:12:38,840
thing goes up. That's kind of what you need to think of when you think of a

1243
01:12:38,840 --> 01:12:42,880
linear correlation. Now the magnitude of that correlation, so you know, how much

1244
01:12:42,880 --> 01:12:46,320
does one go up versus how much one goes down is exactly what our algorithm

1245
01:12:46,320 --> 01:12:49,960
figures out for us, we just need to know to pick linear regression when we think

1246
01:12:50,000 --> 01:12:53,720
things are going to be correlated in that sense. Okay, so that is enough of the

1247
01:12:53,720 --> 01:12:56,760
explanation of linear regression. Now we're going to get into actually coding

1248
01:12:56,760 --> 01:12:59,840
and creating a model. But we first need to talk about the data set that we're

1249
01:12:59,840 --> 01:13:02,760
going to use in the example we're going to kind of illustrate linear regression

1250
01:13:02,760 --> 01:13:06,680
with. Okay, so I'm here and I'm back in the notebook. Now these are the imports

1251
01:13:06,680 --> 01:13:09,840
we need to start with to actually start programming and getting some stuff done.

1252
01:13:10,160 --> 01:13:14,160
Now the first thing we need to do is actually install SK learn. Now even if

1253
01:13:14,160 --> 01:13:17,240
you're in a notebook, you actually need to do this because for some reason it

1254
01:13:17,240 --> 01:13:20,000
doesn't come by default with the notebook. So to do this, we just did an

1255
01:13:20,000 --> 01:13:24,680
exclamation point, pip install hyphen q SK learn. Now if you're going to be working

1256
01:13:24,680 --> 01:13:28,000
on your own machine, again, you can use pip to install this. And I'm assuming

1257
01:13:28,000 --> 01:13:31,320
that you know to use pip if you're going to be going along in that direction. Now

1258
01:13:31,320 --> 01:13:33,960
as before, since we're in the notebook, we need to define we're going to use

1259
01:13:33,960 --> 01:13:38,680
TensorFlow version to point x. So to do that, we're going to just, you know, do

1260
01:13:38,680 --> 01:13:41,920
that up here with the percent sign. And then we have all these imports, which

1261
01:13:41,920 --> 01:13:45,400
we're going to be using throughout here. So from future import, absolutely import

1262
01:13:45,440 --> 01:13:48,800
division, print function, Unicode literals, and then obviously the big one. So

1263
01:13:48,800 --> 01:13:54,200
NumPy, pandas, map plot lib, we're gonna be using I Python, we're gonna be using

1264
01:13:54,200 --> 01:13:57,560
TensorFlow. And yeah, so I'm actually just gonna explain what some of these

1265
01:13:57,560 --> 01:14:01,560
modules are, because I feel like some of you may actually not know. NumPy is

1266
01:14:01,560 --> 01:14:07,360
essentially a very optimized version of arrays in Python. So what this allows

1267
01:14:07,360 --> 01:14:11,680
us to do is lots of kind of multi dimensional calculations. So essentially

1268
01:14:11,680 --> 01:14:15,040
if you have a multi dimensional array, which we've talked about before, right

1269
01:14:15,040 --> 01:14:19,200
when we had, you know, those crazy shapes like 5555, NumPy allows us to

1270
01:14:19,200 --> 01:14:22,880
represent data in that form, and then very quickly manipulate and perform

1271
01:14:22,880 --> 01:14:27,240
operations on it. So we can do things like cross product, dot product, matrix

1272
01:14:27,240 --> 01:14:31,600
addition, matrix subtraction, element wise addition, subtraction, you know,

1273
01:14:31,600 --> 01:14:35,320
vector operations, that's what this does for us. It's pretty complex, but we're

1274
01:14:35,320 --> 01:14:39,400
going to be using it a fair amount. Pandas. Now what pandas does is it's kind

1275
01:14:39,400 --> 01:14:43,480
of a data analytics tool, I almost want to say, I don't know the formal

1276
01:14:43,480 --> 01:14:47,640
definition of what pandas is. But it allows us to very easily manipulate

1277
01:14:47,640 --> 01:14:52,800
data. So you know, load in data sets, view data sets, cut off specific columns

1278
01:14:52,800 --> 01:14:56,440
or cut out rows from our data sets, visualize the data sets. That's what

1279
01:14:56,440 --> 01:15:00,520
pandas does for us. Now map plot lib is actually a visualization of kind of

1280
01:15:00,520 --> 01:15:04,080
graphs and charts. So we'll use that a little bit lower when I actually

1281
01:15:04,440 --> 01:15:09,120
graph some different aspects of our data set. The IPython display, this is

1282
01:15:09,120 --> 01:15:11,920
just specific for this notebook, it's just to clear the output, there's

1283
01:15:11,920 --> 01:15:15,400
nothing crazy with that. And then obviously, we know what TensorFlow is,

1284
01:15:15,600 --> 01:15:20,000
this crazy import for TensorFlow here. So compact v to feature column as FC,

1285
01:15:20,000 --> 01:15:23,560
we'll talk about later, but we need something called a feature column when

1286
01:15:23,560 --> 01:15:27,600
we create a linear regression algorithm or model and TensorFlow. So we're going

1287
01:15:27,600 --> 01:15:31,640
to use that. Okay. So now that we've gone through all that, we need to start

1288
01:15:31,640 --> 01:15:34,280
talking about the data set that we're going to use for linear regression. And

1289
01:15:34,280 --> 01:15:37,200
for this example, because what we're going to do is, you know, actually create

1290
01:15:37,200 --> 01:15:41,040
this model, and start using it to predict values. So the data set that we're

1291
01:15:41,080 --> 01:15:44,080
going to use, actually, I need to read this, because I forget exactly what the

1292
01:15:44,080 --> 01:15:48,440
name of it is, is the Titanic data set, that's what it is. So essentially, what

1293
01:15:48,440 --> 01:15:52,320
this does is aim to predict who's going to survive or the likelihood that

1294
01:15:52,320 --> 01:15:57,200
someone will survive, being on the Titanic, given a bunch of information. So

1295
01:15:57,200 --> 01:15:59,760
what we need to do is load in this data set. Now, I know this seems like a

1296
01:15:59,760 --> 01:16:02,880
bunch of gibberish, but this is how we need to load it. So we're going to use

1297
01:16:02,880 --> 01:16:09,480
pandas. So PD dot read CSV from this URL. So what this is going to do is take

1298
01:16:09,520 --> 01:16:13,480
this CSV file, which stands for comma, separated values. And we can actually

1299
01:16:13,480 --> 01:16:17,400
look at this if we want, I think so I said, it said control click, let's see

1300
01:16:17,400 --> 01:16:20,920
if this pops up. So let's actually download this. And let's open this up

1301
01:16:20,920 --> 01:16:23,800
ourselves and have a look at what it is in Excel. So I'm going to bring this

1302
01:16:23,800 --> 01:16:28,400
up here. You can see that link. And this is what our data set is. So we have

1303
01:16:28,400 --> 01:16:32,520
our columns, which just stand for, you know, what is it the different

1304
01:16:32,520 --> 01:16:35,160
attributes in our data set of the different features and labels of our

1305
01:16:35,160 --> 01:16:38,760
data set, we have survived. So this is what we're actually going to be aiming

1306
01:16:38,800 --> 01:16:42,040
to predict. So we're going to call this our label, right, or our output

1307
01:16:42,040 --> 01:16:47,560
information. So here, a zero stands for the fact that someone did not survive.

1308
01:16:47,600 --> 01:16:51,360
And one stands for the fact that someone did survive. Now, just thinking about

1309
01:16:51,360 --> 01:16:54,720
it on your own for a second, and looking at some of the categories we have up

1310
01:16:54,720 --> 01:16:58,640
here, can you think about why linear regression would be a good algorithm for

1311
01:16:58,640 --> 01:17:03,480
something like this? Well, for example, if someone is a female, we can kind of

1312
01:17:03,480 --> 01:17:05,960
assume that they're going to have a higher chance of surviving on the

1313
01:17:05,960 --> 01:17:09,680
Titanic, just because of, you know, the kind of the way that our culture works,

1314
01:17:09,720 --> 01:17:12,280
you know, saving women and children first, right? And if we look through this

1315
01:17:12,280 --> 01:17:16,520
data set, we'll see that when we see females, it's pretty rare that they

1316
01:17:16,520 --> 01:17:19,760
don't survive. Although as I go through, there is quite a few that didn't

1317
01:17:19,760 --> 01:17:22,440
survive. But if we look at it compared to males, you know, there's definitely

1318
01:17:22,440 --> 01:17:26,960
strong correlation that being a female results in a stronger survival rate. Now,

1319
01:17:26,960 --> 01:17:30,080
if we look at age, right, can we think of how age might affect this? Well, I

1320
01:17:30,080 --> 01:17:33,240
would assume if someone's way younger, they probably have a higher chance of

1321
01:17:33,280 --> 01:17:38,320
surviving, because they would be, you know, prioritized in terms of lifeboats or

1322
01:17:38,320 --> 01:17:41,040
whatever it was. I don't know much about the Titanic. So I can't talk about that

1323
01:17:41,040 --> 01:17:43,680
specifically. But I'm just trying to go through the categories and explain to

1324
01:17:43,680 --> 01:17:47,160
you why we picked this algorithm. Now, number of siblings, that one might not

1325
01:17:47,160 --> 01:17:50,880
be as, you know, influential, in my opinion, parched. I don't actually

1326
01:17:50,880 --> 01:17:56,760
remember what parched stands for. I think it is like what parched, I don't know

1327
01:17:56,760 --> 01:17:59,600
exactly what this column stands for. So unfortunately, I can't tell you guys

1328
01:17:59,640 --> 01:18:04,640
that one. But we'll talk about some more of the second fair. Again, not exactly

1329
01:18:04,640 --> 01:18:07,640
sure what fair stands for. I'm going to look on the TensorFlow website after

1330
01:18:07,640 --> 01:18:11,560
this and get back to you guys. And we have a class. So class is what class they

1331
01:18:11,560 --> 01:18:14,800
were on the boat, right? So first class, second class, third class. So you might

1332
01:18:14,800 --> 01:18:18,080
think someone that's in a higher class might have a higher chance of surviving.

1333
01:18:18,480 --> 01:18:22,400
We have decks, this is what deck they were on when it crashed. So unknown is

1334
01:18:22,400 --> 01:18:26,160
pretty common. And then we have all these other decks, you know, if someone got

1335
01:18:26,200 --> 01:18:30,200
hit, if someone was standing on the deck that had the initial impact, we might

1336
01:18:30,200 --> 01:18:34,000
assume that they would have a lower chance of survival. embark to is where

1337
01:18:34,000 --> 01:18:37,320
they were going. And then are they alone? Yes or no. And this one, you know, this

1338
01:18:37,320 --> 01:18:40,520
is interesting, we're going to see does this make an effect? If someone is alone,

1339
01:18:40,520 --> 01:18:43,920
is that a higher chance of survival? Is that a lower chance of survival? So this

1340
01:18:43,920 --> 01:18:46,200
is kind of interesting. And this is what I want you guys to think about is that

1341
01:18:46,200 --> 01:18:50,560
when we have information and data like this, we don't necessarily know what

1342
01:18:50,560 --> 01:18:54,400
correlations there might be. But we can kind of assume there's some linear

1343
01:18:54,480 --> 01:18:58,000
thing that we're looking for some kind of pattern, right? Whereas if something is

1344
01:18:58,000 --> 01:19:01,520
true, then you know, maybe it's more likely someone will survive. Whereas like,

1345
01:19:01,520 --> 01:19:05,080
if they're not alone, maybe it's less likely. And maybe there's no correlation

1346
01:19:05,080 --> 01:19:08,840
whatsoever. But that's where we're going to find out as we do this model. So let

1347
01:19:08,840 --> 01:19:12,280
me look actually, on the TensorFlow website and see if I can remember what

1348
01:19:12,280 --> 01:19:16,600
parched and I guess what fair was. So let's go up to the top here. Again, a lot

1349
01:19:16,600 --> 01:19:20,000
of this stuff is just straight up copied from the TensorFlow website. I've just

1350
01:19:20,000 --> 01:19:23,680
added my own stuff to it. You can see like, I just copied all this, we're just

1351
01:19:23,720 --> 01:19:27,880
bringing it in there. Let's see what it says about the different columns, if it

1352
01:19:27,880 --> 01:19:33,440
gives us any exact explanations. Okay, so I couldn't find what parts were fair

1353
01:19:33,440 --> 01:19:36,320
stands for. For some reason, it's not on the TensorFlow website, either. I

1354
01:19:36,320 --> 01:19:39,040
couldn't really find any information about it. If you guys know, you know,

1355
01:19:39,040 --> 01:19:41,760
leave a comment down below, but it's not that important, we just want to use

1356
01:19:41,760 --> 01:19:46,320
this data to do a test. So what I've done here, if I've loaded in my data set,

1357
01:19:46,320 --> 01:19:50,560
and notice that I've loaded a training data set in a testing data set. Now we'll

1358
01:19:50,600 --> 01:19:54,160
talk about this more later. This is important, I have two different data

1359
01:19:54,160 --> 01:19:58,640
sets, one to train the model with, and one to test the model with. Now kind of the

1360
01:19:58,640 --> 01:20:02,080
basic reason we would do this is because when we test our model for accuracy to

1361
01:20:02,080 --> 01:20:05,800
see how well it's doing, it doesn't make sense to test it on data, it's already

1362
01:20:05,800 --> 01:20:09,760
seen, it needs to see fresh data, so we can make sure there's no bias, and it

1363
01:20:09,760 --> 01:20:14,840
hasn't simply just memorize the data, you know, that we had. Now what I'm doing

1364
01:20:14,840 --> 01:20:19,800
here, at the bottom with this y train in this y eval, is I'm essentially popping

1365
01:20:19,880 --> 01:20:24,560
a column off of this data set. So if I print out the data set here, and I'm

1366
01:20:24,560 --> 01:20:27,120
actually I'm going to show you a cool trick with pandas that we can use to

1367
01:20:27,120 --> 01:20:33,520
look at this. So I can say D F train dot head. So if I look at this, by just

1368
01:20:33,520 --> 01:20:36,800
looking at the head, and we'll print this out, oh, I might need to import some

1369
01:20:36,800 --> 01:20:40,880
stuff above. We'll see if this works or not. Yeah, so I need to just do these

1370
01:20:40,880 --> 01:20:44,640
imports. So let's install. And let's do these imports. I'll wait for the

1371
01:20:44,640 --> 01:20:48,080
surrounding. Okay, so I've just selected TensorFlow 2.0. We're just importing

1372
01:20:48,080 --> 01:20:51,920
this now should be done in one second. And now what we'll do is we'll print

1373
01:20:51,960 --> 01:20:56,200
out the the data frame here. So essentially what this does is load this

1374
01:20:56,200 --> 01:21:00,600
into a pandas data frame. This is a specific type of object. Now we're not

1375
01:21:00,600 --> 01:21:04,240
going to go into this specifically, but a data frame allows us to view a lot of

1376
01:21:04,240 --> 01:21:08,400
different aspects about the data and kind of store it in a nice form, as opposed

1377
01:21:08,400 --> 01:21:11,960
to just loading it in and storing it in like a list or a NumPy array, which we

1378
01:21:11,960 --> 01:21:15,440
might do if we didn't know how to use pandas. This is a really nice way to do

1379
01:21:15,520 --> 01:21:19,680
it read CSV, load it into a data frame object, which actually means we can

1380
01:21:19,680 --> 01:21:23,800
reference specific columns and specific rows in the data frame. So let's run this

1381
01:21:23,800 --> 01:21:30,760
and just have a look at it. Yeah, I got need to print dftrain.head. So let's do

1382
01:21:30,760 --> 01:21:35,920
that. And there we go. So this is what our data frame head looks like. Now head

1383
01:21:35,920 --> 01:21:40,800
what that does is show us the first five entries in our data set, as well as show

1384
01:21:40,800 --> 01:21:44,760
us a lot of the different columns that are in it. Now since we have more than

1385
01:21:44,760 --> 01:21:47,520
you know, we have a few different columns, it's not showing us all of them, it's

1386
01:21:47,520 --> 01:21:50,920
just giving us the dot dot dot. But we can see this is what the data frame

1387
01:21:50,920 --> 01:21:54,960
looks like. And this is kind of the representation internally. So we have

1388
01:21:55,160 --> 01:22:00,040
entry zero, survived zero, survived one, we have male, female, all that. Now

1389
01:22:00,040 --> 01:22:03,800
notice that this has the survived column. Okay. Because what I'm going to do is

1390
01:22:03,800 --> 01:22:10,440
I'm going to print the data frame head again. So dftrain.head after we run

1391
01:22:10,440 --> 01:22:15,840
these two lines. Now what this line does is takes this entire survived column, so

1392
01:22:15,840 --> 01:22:20,400
all these zeros and ones, and removes it from this data frame, so the head data

1393
01:22:20,400 --> 01:22:24,720
frame, and stores it in the variable y train. The reason we need to do that is

1394
01:22:24,720 --> 01:22:27,480
because we need to separate the data, we're going to be classifying from the

1395
01:22:27,480 --> 01:22:31,480
data that is kind of our input information or our initial data set, right? So

1396
01:22:31,480 --> 01:22:35,080
since we're looking for the survived information, we're going to put that in

1397
01:22:35,080 --> 01:22:39,240
its own, you know, kind of variable store here. Now we'll do the same thing for

1398
01:22:39,240 --> 01:22:44,240
the evaluation data set, which is DF evaluation or testing data. And notice

1399
01:22:44,240 --> 01:22:48,840
that here this was trained on CSV, and this one was eval.csv. Now these have

1400
01:22:48,840 --> 01:22:53,320
the exact same form, they look the like completely identical, it's just that

1401
01:22:53,320 --> 01:22:56,400
you know, some entries, we've just kind of arbitrarily split them. So we're going

1402
01:22:56,400 --> 01:22:59,120
to have a lot of entries in this training set, and we'll have a few in the

1403
01:22:59,120 --> 01:23:03,000
testing set that we'll just use to do an evaluation on the model later on. So we

1404
01:23:03,000 --> 01:23:07,800
pop them off by doing this pop removes and returns this column. So if I print

1405
01:23:07,840 --> 01:23:11,160
out why train, which are actually let's look at this one first, just to show

1406
01:23:11,160 --> 01:23:15,200
you how it's been removed, we can see that we have the survived column here, we

1407
01:23:15,200 --> 01:23:18,600
popped and now the survived column is removed from that data set. So that's

1408
01:23:18,600 --> 01:23:22,120
just important to understand. Now we can print out some other stuff too. So we can

1409
01:23:22,120 --> 01:23:25,960
look at the why train and see what that is. Just to make sure we really

1410
01:23:25,960 --> 01:23:29,360
understand this data. So let's look at why train. And you can see that we have

1411
01:23:29,360 --> 01:23:34,960
626 or 627 entries and address, you know, zeros or ones representing whether

1412
01:23:34,960 --> 01:23:39,720
someone survived or whether they did not. Now the corresponding indexes in this

1413
01:23:39,800 --> 01:23:44,160
kind of list or data frame correspond to the indexes in the testing and

1414
01:23:44,200 --> 01:23:48,200
training data frame. What I mean by that is, you know, entry zero in this

1415
01:23:48,240 --> 01:23:53,960
specific data frame corresponds to entry zero in our why train variable. So if

1416
01:23:53,960 --> 01:23:58,240
someone survived, you know, at entry zero, it would say one here, right? Or in

1417
01:23:58,240 --> 01:24:02,360
this case, entry zero did not survive. Now, I hope that's clear. I hope I'm not

1418
01:24:02,400 --> 01:24:06,320
confusing you with that. But I just want to show one more example to make sure. So

1419
01:24:06,320 --> 01:24:09,880
we'll say D F train zero, I'm going to print that and I'm going to print why

1420
01:24:09,880 --> 01:24:14,880
train at index zero. Oops, if I didn't mess up my brackets, and we'll have a

1421
01:24:14,880 --> 01:24:18,240
look at it. Okay, so I've just looked up the documentation because I totally

1422
01:24:18,240 --> 01:24:22,280
forgot that I couldn't do that. If I want to find one specific row in my data

1423
01:24:22,280 --> 01:24:27,600
frame, what I can do is print dot loc. So I do my data frame and then dot loc and

1424
01:24:27,600 --> 01:24:31,800
then whatever index I want. So in this case, I'm locating row zero, which is

1425
01:24:31,800 --> 01:24:35,480
this. And then on the why train, I'm doing the same thing, I'm locating row

1426
01:24:35,480 --> 01:24:40,280
zero. Now what I had before, right, if I did D F train, and I put square brackets

1427
01:24:40,520 --> 01:24:44,000
inside here, what I can actually do is reference a specific column. So if I

1428
01:24:44,000 --> 01:24:47,960
wanted to look at, you know, say the column for age, right, so we have a

1429
01:24:47,960 --> 01:24:52,840
column for age, what I can do is do D F train age. And then I can print this out

1430
01:24:52,840 --> 01:24:56,440
like this. And it gives me all of the different age values. So that's kind of

1431
01:24:56,440 --> 01:24:59,800
how we use a data frame, we'll see that as we go further on. Now let's go back to

1432
01:24:59,800 --> 01:25:03,520
the other example I had, because I just erased it, where I want to show you the

1433
01:25:03,920 --> 01:25:08,960
row zero in the data frame that's training, and then in the why train, you

1434
01:25:08,960 --> 01:25:13,080
know, output, whatever that is. So the survival. So you can see here that this

1435
01:25:13,080 --> 01:25:17,520
is what we get from printing D F train loc zero. So row zero, this is all the

1436
01:25:17,520 --> 01:25:21,200
information. And then here, this corresponds to the fact that they did not

1437
01:25:21,200 --> 01:25:25,400
survive at row zero, because it's simply just the output is value zero. Now I

1438
01:25:25,440 --> 01:25:29,320
know this is weird, it's saying like name, zero, d type, object, zero, don't

1439
01:25:29,320 --> 01:25:31,560
worry about that. It's just because it's trying to print it with some

1440
01:25:31,560 --> 01:25:35,880
information. But essentially, this just means this person who was male 22 and had

1441
01:25:35,880 --> 01:25:40,480
one sibling did not survive. Okay, so let's get out of this. Now we can close

1442
01:25:40,480 --> 01:25:44,360
this and let's go to Oh, we've pretty much already done what I've just have

1443
01:25:44,360 --> 01:25:47,800
down here. But we can look at the data frame head. This is a little bit of a

1444
01:25:47,800 --> 01:25:51,880
nicer output when we just have D F train dot head, we can see that we get kind of

1445
01:25:51,880 --> 01:25:55,280
a nice outputted little graph. We've already looked at this information. So we

1446
01:25:55,320 --> 01:25:59,200
know kind of some of the attributes of the data set. Now we want to describe the

1447
01:25:59,200 --> 01:26:03,520
data set sometimes. What describe does is just give us some overall information. So

1448
01:26:03,520 --> 01:26:08,480
let's have a look at it here. We can see that we have 627 entries, the mean of

1449
01:26:08,520 --> 01:26:13,360
age is 29, the standard deviation is you know, 12 point whatever. And then we get

1450
01:26:13,360 --> 01:26:17,040
the same information about all of these other different attributes. So for example,

1451
01:26:17,040 --> 01:26:20,320
it gives us you know, the mean fair, the minimum fair, and just some statistics,

1452
01:26:20,360 --> 01:26:23,760
because understand this great, if you don't doesn't really matter. The important

1453
01:26:23,760 --> 01:26:26,560
thing to look at typically is just how many entries we have is sometimes we need

1454
01:26:26,560 --> 01:26:29,880
that information. And sometimes the mean can be helpful as well, because you can

1455
01:26:29,880 --> 01:26:34,080
kind of get an average of like what the average value is in the data set. So if

1456
01:26:34,080 --> 01:26:37,520
there's any bias later on, you can figure that out. But it's not crazy important.

1457
01:26:37,920 --> 01:26:41,680
Okay, so let's have a look at the shape. So just like NumPy arrays and tensors

1458
01:26:41,680 --> 01:26:45,080
have a shape attribute, so do data frames. So we want to look at the shape, you

1459
01:26:45,080 --> 01:26:49,320
know, we can just print out D F train dot shape, we get 627 by nine, which

1460
01:26:49,320 --> 01:26:55,880
essentially means we have 627 rows, and nine columns or nine attributes. So yeah,

1461
01:26:55,880 --> 01:27:00,480
that's what it says here, you know, 627 entries, nine features, we can interchange

1462
01:27:00,480 --> 01:27:04,360
attributes and features. And we can look at the head information for why so we can

1463
01:27:04,360 --> 01:27:07,240
see that here, which we've already looked at before. And that gives us the name,

1464
01:27:07,280 --> 01:27:11,440
which was survived. Okay, so now what we can actually do is make some kind of

1465
01:27:11,440 --> 01:27:14,520
graphs about this data. Now I've just stolen this code, you know, straight up

1466
01:27:14,520 --> 01:27:18,320
from the TensorFlow website, I wouldn't expect you guys to do any of this, you

1467
01:27:18,320 --> 01:27:21,760
know, like output any of these values. What we're going to do is create a few

1468
01:27:21,760 --> 01:27:25,360
histograms and some plots just to look at kind of some correlations in the data.

1469
01:27:25,360 --> 01:27:28,680
So that when we start creating this model, we have some intuition on what we

1470
01:27:28,680 --> 01:27:33,600
might expect. So let's look at age. So this gives us a histogram of the age. So

1471
01:27:33,600 --> 01:27:38,560
we can see that there's about 25 people that are kind of between zero and five.

1472
01:27:38,760 --> 01:27:43,600
There is you know, maybe like five people that are in between five and 10. And

1473
01:27:43,600 --> 01:27:47,120
then the most amount of people are kind of in between their 20s and 30s. So in the

1474
01:27:47,120 --> 01:27:51,480
mid 20s, this is good information to know, because that's going to introduce a

1475
01:27:51,480 --> 01:27:55,920
little bit of bias into kind of our linear correlation graph, right? So just

1476
01:27:56,160 --> 01:27:59,720
understanding, you know, that we have like a large subset, there's some outliers

1477
01:27:59,720 --> 01:28:02,760
here, like there's one person that's 80, right over here, a few people that are

1478
01:28:02,760 --> 01:28:06,080
70, some important things to kind of understand before we move on to the

1479
01:28:06,080 --> 01:28:10,960
algorithm. So let's look at the sex values now. So this is how many female and

1480
01:28:10,960 --> 01:28:15,360
how many male, we can see that there's many more males than there is females. We

1481
01:28:15,400 --> 01:28:18,560
can have a look at the class. So we can see if they're in first, second or third

1482
01:28:18,560 --> 01:28:23,800
class, most people are in third, then followed by first and then second. And

1483
01:28:23,800 --> 01:28:27,240
then lastly, we can look at what is this that we're doing? Oh, the percentage

1484
01:28:27,240 --> 01:28:32,320
survival by sex. So we can see how likely a specific person or a specific sex is

1485
01:28:32,320 --> 01:28:36,240
to survive just by plotting this. So we can see that males have about a 20%

1486
01:28:36,240 --> 01:28:40,640
survival rate, whereas females are all the way up to about 78%. So that's

1487
01:28:40,640 --> 01:28:43,880
important to understand that kind of confirms that what we were looking at

1488
01:28:43,880 --> 01:28:46,920
before in the data set when we were exploring it. And you don't need to do

1489
01:28:46,920 --> 01:28:50,160
this every time that you're looking at a data set, but it is good to kind of get

1490
01:28:50,160 --> 01:28:53,000
some intuition about it. So this is what we've learned so far, majority

1491
01:28:53,000 --> 01:28:56,720
passengers are in their 20s or 30s, then majority passengers are male, they're in

1492
01:28:56,720 --> 01:28:59,960
third class, and females have a much higher chance of survival, kind of

1493
01:28:59,960 --> 01:29:03,040
already knew that. Alright, so training and testing data sets. Now we already

1494
01:29:03,040 --> 01:29:06,040
kind of went through this all skim through it quickly. Essentially, what we

1495
01:29:06,040 --> 01:29:10,600
did above is load in two different data sets. The first data set was that

1496
01:29:10,600 --> 01:29:14,920
training data set which had the shape of 627 by nine. What I'm actually going to

1497
01:29:14,920 --> 01:29:20,640
do is create a code block here, and just have a look at what was this Df eval dot

1498
01:29:20,640 --> 01:29:24,440
shape to show you how many entries we have in here. So here in our testing

1499
01:29:24,440 --> 01:29:29,680
data set, you can see we have significantly less at 264 entries, or rows,

1500
01:29:29,680 --> 01:29:33,400
whatever you want to call them. So that's how many things we have to actually

1501
01:29:33,400 --> 01:29:37,080
test our model. So what we do is we use that training data to create the model,

1502
01:29:37,120 --> 01:29:39,880
and then the testing data to evaluate it and make sure that it's working

1503
01:29:39,880 --> 01:29:43,160
properly. So these things are important whenever we're doing machine learning

1504
01:29:43,160 --> 01:29:48,120
models, we typically have testing and training data. And yeah, that is pretty

1505
01:29:48,120 --> 01:29:51,440
much it. Now I'm just going to take one second to copy over a lot of this code

1506
01:29:51,720 --> 01:29:55,440
into the kind of other notebook I have, just so we can see all of it at once. And

1507
01:29:55,440 --> 01:29:58,200
then I'll be back and we'll get into actually making the model. Okay, so I've

1508
01:29:58,200 --> 01:30:01,080
copied in some code here. I know this seems like a lot of kind of gibberish

1509
01:30:01,080 --> 01:30:04,080
right now, but I'm going to break down line by line what all this is doing and

1510
01:30:04,080 --> 01:30:07,520
why we have this here. But we first need to discuss something called feature

1511
01:30:07,520 --> 01:30:13,040
columns and the difference between categorical and numeric data. So categorical

1512
01:30:13,040 --> 01:30:16,240
data is actually fairly common. Now when we're looking at our data set, and

1513
01:30:16,240 --> 01:30:19,920
actually I can open I don't have it open in Excel anymore, but let's open this

1514
01:30:19,920 --> 01:30:25,640
from my downloads. So let's go downloads. Where is this train? Okay, awesome. So we

1515
01:30:25,640 --> 01:30:30,480
have this Excel data sheet here. And we can see what a categorical data or what

1516
01:30:30,480 --> 01:30:35,200
categorical data is is something that's not numeric. So for example, unknown, C

1517
01:30:35,320 --> 01:30:41,040
first, third, city, and why right so anything that has different categories,

1518
01:30:41,720 --> 01:30:44,800
there's going to be like a specific set of different categories there could be. So

1519
01:30:44,800 --> 01:30:48,480
for example, for age, kind of the set of values we get out for age, well, this is

1520
01:30:48,480 --> 01:30:51,280
numeric. So that's different. But for categorical, we can have male or we

1521
01:30:51,280 --> 01:30:54,640
can have female. And I suppose we could have other but in this data set, we just

1522
01:30:54,640 --> 01:30:58,560
have male and we just have female. For class, we can have first, second, third.

1523
01:30:58,800 --> 01:31:02,320
For deck, we can have unknown CA, I'm sure through all the letters of the

1524
01:31:02,320 --> 01:31:04,720
alphabet, but that is still considered categorical.

1525
01:31:05,320 --> 01:31:09,520
Now, what do we do with categorical data? Well, we always need to transform this

1526
01:31:09,520 --> 01:31:14,160
data into numbers somehow. So what we actually end up doing is we encode

1527
01:31:14,160 --> 01:31:18,640
this data using an integer value. So for the example of male and female, what

1528
01:31:18,640 --> 01:31:21,960
we might say, and this is what we're going to do in a second is that female is

1529
01:31:21,960 --> 01:31:26,120
represented by zero and male is represented by one. We do this because

1530
01:31:26,120 --> 01:31:30,280
although it's interesting to know what the actual class is, the model doesn't

1531
01:31:30,280 --> 01:31:32,960
care, right, female and male, it doesn't make a difference to it. It just needs

1532
01:31:32,960 --> 01:31:36,080
to know that those values are the different are different or those values

1533
01:31:36,080 --> 01:31:39,640
are the same. So rather than using strings and trying to find some way to

1534
01:31:39,640 --> 01:31:43,280
pass that in and do math with that, we need to turn those into integers, we

1535
01:31:43,280 --> 01:31:47,800
turn those into zeros and ones, right? Now for class, right, so first, second,

1536
01:31:47,800 --> 01:31:50,320
third, you know, you guys can probably assume what we're going to encode this

1537
01:31:50,320 --> 01:31:53,720
with, we're going to encode it with zero, one, two. Now, again, this doesn't

1538
01:31:53,720 --> 01:31:57,680
necessarily need to be in order. So third could be represented by one and

1539
01:31:57,680 --> 01:32:01,360
first could be represented by two, right? It doesn't need to be in order, it

1540
01:32:01,360 --> 01:32:05,000
doesn't matter. So long as every third has the same number, every first has the

1541
01:32:05,000 --> 01:32:07,800
same number and every second has the same number. And then same thing with

1542
01:32:07,800 --> 01:32:11,520
that same thing with embark and same thing with alone. Now we could have an

1543
01:32:11,520 --> 01:32:14,800
instance where you know, we've encoded every single one of these values with a

1544
01:32:14,800 --> 01:32:19,000
different value. So in the, you know, rare occasion where there's one category

1545
01:32:19,000 --> 01:32:23,640
that's categorical, and every single value in that category is different, then

1546
01:32:23,640 --> 01:32:28,600
we will have, you know, 627 in this instance, different encoding labels that

1547
01:32:28,640 --> 01:32:31,680
are going to be numbers, that's fine, we can do that. And actually, we don't

1548
01:32:31,680 --> 01:32:35,320
really need to do that, because you're going to see how TensorFlow can handle

1549
01:32:35,320 --> 01:32:39,280
that for us. So that is categorical data, numeric columns are pretty

1550
01:32:39,280 --> 01:32:42,080
straightforward. They're anything that just have integer or float values

1551
01:32:42,080 --> 01:32:46,760
already. So in this case, age and fair. And yeah, so that's what we've done. We've

1552
01:32:46,760 --> 01:32:50,680
just defined our categorical columns here, and our numeric columns here. This is

1553
01:32:50,680 --> 01:32:53,360
important because we're going to loop through them, which we're doing here to

1554
01:32:53,360 --> 01:32:56,640
create something called feature columns, feature columns are nothing special.

1555
01:32:56,680 --> 01:33:00,760
They're just what we need to feed to our linear estimator or linear model to

1556
01:33:00,760 --> 01:33:04,200
actually make predictions. So kind of our steps here that we've gone through so

1557
01:33:04,200 --> 01:33:09,200
far is import, load the data set, explore the data set, make sure we

1558
01:33:09,200 --> 01:33:13,200
understand it, create our categorical columns and our numeric columns. So I've

1559
01:33:13,200 --> 01:33:17,760
just hard coded these in right like sex, parts, class, deck, alone, all these

1560
01:33:17,760 --> 01:33:21,040
ones. And then same thing with the numeric columns. And then for a linear

1561
01:33:21,040 --> 01:33:24,480
estimator, we need to create these as feature columns using some kind of

1562
01:33:24,480 --> 01:33:28,240
advanced syntax, which we're going to look at here. So we create a blank list,

1563
01:33:28,240 --> 01:33:30,760
which is our feature columns, which will just store our different feature

1564
01:33:30,760 --> 01:33:36,120
columns. We loop through each feature name in the categorical columns. And what

1565
01:33:36,120 --> 01:33:41,120
we do is we define a vocabulary, which is equal to the data frame at that

1566
01:33:41,120 --> 01:33:44,640
feature name. So first, we would start with sex, then we go and siblings, then

1567
01:33:44,640 --> 01:33:49,760
we go parts, then we go class. And we get all of the different unique values. So

1568
01:33:49,760 --> 01:33:53,360
that's actually what this does dot unique, gets a list of all unique values

1569
01:33:53,400 --> 01:33:57,840
from the feature call. And I can print this out. She'll print this in a

1570
01:33:57,840 --> 01:34:00,320
different line, we'll just take this value and have a look at actually what

1571
01:34:00,320 --> 01:34:06,520
this is, right? So if I run, I guess we'll have to run all these in order. And

1572
01:34:06,520 --> 01:34:10,720
then we'll create a new code block while we wait for that to happen. Let's see if

1573
01:34:10,720 --> 01:34:22,360
we can get this installing fast enough. Run, run, run. Okay, now we go to df

1574
01:34:22,360 --> 01:34:25,400
train. And we can see this is what this looks like. So these are all the

1575
01:34:25,400 --> 01:34:28,960
different unique values that we had in that specific feature name. Now that

1576
01:34:28,960 --> 01:34:33,880
feature name was what categorical columns? Oh, what I do feature name up,

1577
01:34:33,880 --> 01:34:36,920
sorry, that's going to be the unique one. Let's just put rather than feature

1578
01:34:36,920 --> 01:34:41,120
name. Let's put sex, right? And let's have a look at what this is. So we can

1579
01:34:41,120 --> 01:34:44,920
see that the two unique values are male and female. Now I actually want to do

1580
01:34:45,480 --> 01:34:48,800
what is it embark town? And I want to see what this one is. So how many

1581
01:34:48,800 --> 01:34:51,920
different values we have. So we'll copy that in. And we can see we have

1582
01:34:51,960 --> 01:34:56,520
Southampton cannot pronounce that and then the other cities and unknown. And

1583
01:34:56,520 --> 01:34:59,360
that is kind of how we get the unique value. So that's what that method is

1584
01:34:59,360 --> 01:35:02,160
doing there. Let's actually delete this code block because we don't need

1585
01:35:02,160 --> 01:35:06,200
anymore. Alright, so that's what we do. And then what we do down here is we say

1586
01:35:06,200 --> 01:35:10,600
feature columns dot append. So just add to this list, the TensorFlow feature

1587
01:35:10,600 --> 01:35:15,120
column dot categorical column with vocabulary list. Now I know this is a

1588
01:35:15,120 --> 01:35:17,760
mouthful, but this is kind of something again, you're just going to look up

1589
01:35:17,760 --> 01:35:20,480
when you need to use it, right? So understand you need to make feature

1590
01:35:20,480 --> 01:35:23,600
columns for linear regression, you don't really need to completely understand

1591
01:35:23,600 --> 01:35:26,360
how, but you just need to know that that's something you need to do. And then

1592
01:35:26,360 --> 01:35:29,960
you can look up the syntax and understand. So this is what this does. This is

1593
01:35:29,960 --> 01:35:34,200
actually going to create for us a column, it's going to be in the form of a

1594
01:35:34,200 --> 01:35:38,400
like NumPy array kind of that has the feature name. So whatever one we've

1595
01:35:38,400 --> 01:35:42,760
looped through, and then all the different vocabulary associated with it. Now

1596
01:35:42,760 --> 01:35:46,800
we need this because we just need to create this column so that we can create

1597
01:35:46,800 --> 01:35:50,280
our model using those different columns, if that makes any sense. So our linear

1598
01:35:50,280 --> 01:35:53,080
model needs to have, you know, all the different columns we're going to use, it

1599
01:35:53,080 --> 01:35:56,400
needs to know all of the different entries that could be in that column, and

1600
01:35:56,400 --> 01:36:00,280
needs to know whether this is a categorical column or a numeric column. In

1601
01:36:00,280 --> 01:36:03,320
previous examples, what we might have done is actually change the data set

1602
01:36:03,320 --> 01:36:07,320
manually, so encoded it manually. TensorFlow just can do this for us now

1603
01:36:07,320 --> 01:36:11,440
in TensorFlow 2.0. So we'll just use that too. Okay, so that's what we did with

1604
01:36:11,440 --> 01:36:14,960
these feature columns. Now for the numeric columns, a little bit different. It's

1605
01:36:14,960 --> 01:36:17,800
actually easier. All we need to do is give the feature name and whatever the

1606
01:36:17,840 --> 01:36:21,520
data type is and create a column with that. So notice we don't, we can omit

1607
01:36:21,520 --> 01:36:24,040
this unique value, because we know when it's numeric, that you know, there

1608
01:36:24,040 --> 01:36:26,840
could be an infinite amount of values. And then I've just printed out the

1609
01:36:26,840 --> 01:36:29,640
feature columns, you can see what this looks like. So vocabulary list,

1610
01:36:29,640 --> 01:36:33,280
categorical column gives us the number of siblings. And then the vocabulary

1611
01:36:33,280 --> 01:36:37,120
list is these are all the different encoding values that is created. And

1612
01:36:37,120 --> 01:36:40,400
then same thing, you know, we can go down here, parts, these are different

1613
01:36:40,400 --> 01:36:43,720
encodings. So they're not necessarily in order is like what I was talking about

1614
01:36:43,760 --> 01:36:50,560
before. Let's go do a numeric one. What do we have here? Yeah, so for a numeric

1615
01:36:50,560 --> 01:36:54,400
column, just as is the key, that's the shape we're expecting. And this is the

1616
01:36:54,400 --> 01:36:59,000
data type. So that is pretty much it. We're actually loading these in. So now

1617
01:36:59,000 --> 01:37:02,760
it's almost time to create the model. So what we're going to do to create the

1618
01:37:02,760 --> 01:37:07,120
model now is talk about first the training process and training some kind

1619
01:37:07,120 --> 01:37:11,640
of, you know, machine learning model. Okay, so the training process. Now the

1620
01:37:11,680 --> 01:37:14,880
training process of our model is actually fairly simple, at least for a

1621
01:37:14,880 --> 01:37:19,400
linear model. Now, the way that we train the model is we feed it information,

1622
01:37:19,400 --> 01:37:24,040
right? So we feed it that those data points from our data set. But how do we

1623
01:37:24,040 --> 01:37:26,880
do that? Right? Like how do we feed that to the model? Do we just give it all at

1624
01:37:26,880 --> 01:37:31,200
once? Well, in our case, we only have 627 rows, which isn't really that much

1625
01:37:31,200 --> 01:37:34,360
data, like we can fit that in RAM in our computer, right? But what if we're

1626
01:37:34,360 --> 01:37:38,680
training a crazy machine learning model, and we have, you know, 25 terabytes of

1627
01:37:38,680 --> 01:37:42,240
data that we need to pass it, we can't load that into RAM, at least I don't

1628
01:37:42,240 --> 01:37:45,360
know any RAM that's that large. So we need to find a way that we can kind of

1629
01:37:45,360 --> 01:37:49,720
load it in what's called batches. So the way that we actually load this model is

1630
01:37:49,720 --> 01:37:53,080
we load it in batches. Now we don't need to understand really kind of how this

1631
01:37:53,080 --> 01:37:58,200
process works and how batching kind of occurs. What we do is give 32 entries at

1632
01:37:58,200 --> 01:38:02,640
once to the model. Now the reason we don't just feed one at a time is because

1633
01:38:02,640 --> 01:38:07,440
that's a lot slower, we can load, you know, a small batch size of 32, that can

1634
01:38:07,480 --> 01:38:10,520
increase our speed dramatically. And that's kind of a lower level

1635
01:38:10,520 --> 01:38:14,080
understanding. So I'm not going to go too far into that. Now that we understand, we

1636
01:38:14,080 --> 01:38:17,280
kind of load it in batches, right? So we don't load it entirely all at once, we

1637
01:38:17,280 --> 01:38:22,720
just load a specific set of kind of elements as we go. What we have is

1638
01:38:22,720 --> 01:38:28,080
called epochs. Now what are epochs? Well, epochs are essentially how many times the

1639
01:38:28,080 --> 01:38:32,400
model is going to see the same data. So we might be the case, right? And when we

1640
01:38:32,400 --> 01:38:36,160
pass the data to our model, the first time, it's pretty bad, like it looks at the

1641
01:38:36,160 --> 01:38:39,320
model creates our line of best fit, but it's not great, it's not working

1642
01:38:39,320 --> 01:38:42,600
perfectly. So we need to use something called an epoch, which means we're just

1643
01:38:42,600 --> 01:38:46,560
going to feed the model, feed the data again, but in a different order. So we

1644
01:38:46,560 --> 01:38:50,840
do this multiple times, so that the model will look at a data, look at the data in

1645
01:38:50,840 --> 01:38:54,720
a different way, then kind of a different form, and see the same data a few

1646
01:38:54,720 --> 01:38:58,080
different times and pick up on patterns. Because the first time it sees a new

1647
01:38:58,080 --> 01:39:01,240
data point, it's probably not going to have a good idea how to make a prediction

1648
01:39:01,240 --> 01:39:05,600
for that. So if we can feed it more and more and more, then you know, we can get

1649
01:39:05,640 --> 01:39:08,440
a better prediction. Now, this is where we talk about something called

1650
01:39:08,480 --> 01:39:13,720
overfitting, though, sometimes we can see the data too much, we can pass too much

1651
01:39:13,720 --> 01:39:17,640
data to our model, to the point where it just straight up memorizes those data

1652
01:39:17,640 --> 01:39:21,720
points. And it's, it's really good at classifying for those data points. But

1653
01:39:21,720 --> 01:39:25,520
when we pass it some new data points, like our testing data, for example, it's

1654
01:39:25,520 --> 01:39:30,120
horrible at kind of, you know, classifying those. So what we do to kind of

1655
01:39:30,120 --> 01:39:33,480
prevent this from happening, is we just make sure that we start with like a

1656
01:39:33,520 --> 01:39:36,360
lower amount of epochs, and then we can work our way up and kind of

1657
01:39:36,360 --> 01:39:40,440
incrementally change that if we need to, you know, go higher, right, we need more

1658
01:39:40,440 --> 01:39:44,960
epochs. So yeah, so that's kind of it for epochs. Now, I will say that this

1659
01:39:44,960 --> 01:39:49,000
training process kind of applies to all the different, what is it, machine learning

1660
01:39:49,000 --> 01:39:51,960
models that we're going to look at, we have epochs, we have batches, we have a

1661
01:39:51,960 --> 01:39:56,760
batch size, and now we have something called an input function. Now, this is

1662
01:39:57,080 --> 01:40:01,000
pretty complicated. This is the code for the input function. I don't like that we

1663
01:40:01,000 --> 01:40:05,880
need to do this. But it's necessary. So essentially, what an input function is

1664
01:40:05,920 --> 01:40:10,320
is the way that we define how our data is going to be broke into epochs and into

1665
01:40:10,320 --> 01:40:14,720
batches to feed to our model. Now, these, you probably aren't ever going to

1666
01:40:14,720 --> 01:40:18,720
really need to code like from scratch by yourself. But this is the one I've just

1667
01:40:18,720 --> 01:40:21,360
stolen from the TensorFlow website, pretty much like everything else that's

1668
01:40:21,360 --> 01:40:28,280
in the series. And what this does is it takes our data and encodes it in a tf

1669
01:40:28,320 --> 01:40:34,080
dot data dot data set object. Now, this is because our model needs this specific

1670
01:40:34,120 --> 01:40:38,040
object to be able to work, it needs to see a data set object to be able to use

1671
01:40:38,040 --> 01:40:42,320
that data to create the model. So what we need to do is take this pandas data

1672
01:40:42,320 --> 01:40:46,200
frame, we need to turn it into that object. And the way we do that is with

1673
01:40:46,200 --> 01:40:50,320
the input function. So we can see that what this is doing here. So this is make

1674
01:40:50,360 --> 01:40:54,280
input function, we actually have a function defined inside of another function. I

1675
01:40:54,280 --> 01:40:57,640
know this is kind of complicated for some of you guys. But and what I'm

1676
01:40:57,640 --> 01:41:00,160
actually gonna do, sorry, I'm gonna just copy this into the other page, because I

1677
01:41:00,160 --> 01:41:03,280
think it's easier to explain without all the text around. So let's create a new

1678
01:41:03,280 --> 01:41:08,360
code block. Let's paste this in. And let's have a look at what this does. So

1679
01:41:08,360 --> 01:41:13,240
actually, let me just tab down. Okay, so make input function, we have our

1680
01:41:13,240 --> 01:41:17,080
parameters data data frame, which is our pandas data frame, our label data

1681
01:41:17,080 --> 01:41:22,640
frame, which stands for those labels. So that y train or that eval y eval, right,

1682
01:41:22,840 --> 01:41:25,680
we have a number of epochs, which is how many epochs we're going to do, we set

1683
01:41:25,720 --> 01:41:30,760
the default 10 shuffle, which means are we going to shuffle our data and mix it

1684
01:41:30,760 --> 01:41:35,080
up before we pass it to the model, and batch size, which is how many elements

1685
01:41:35,120 --> 01:41:40,000
are we going to give to the data to the model? Well, it's training at once. Now,

1686
01:41:40,000 --> 01:41:44,720
what this does is we have an input function defined inside of this function. And

1687
01:41:44,720 --> 01:41:51,120
we say data set equals tensor frame dot data dot data set from tensor slices,

1688
01:41:51,360 --> 01:41:55,320
dict data frame, a label data frame. Now, what this does, and we can read the

1689
01:41:55,320 --> 01:41:59,240
comment, I mean, create a tf dot data dot data set object with the data and

1690
01:41:59,240 --> 01:42:03,360
its label. Now, I can't explain to you like how this works on a lower level. But

1691
01:42:03,360 --> 01:42:07,840
essentially, we pass a dictionary representation of our data frame, which is

1692
01:42:07,840 --> 01:42:11,840
whatever we passed in here. And then we pass the label data frame, which is

1693
01:42:11,840 --> 01:42:16,120
going to be, you know, all those y values. And we create this object. And

1694
01:42:16,120 --> 01:42:20,560
that's what this line of code does. So tf data dot data set from tensor slices,

1695
01:42:20,680 --> 01:42:23,320
which is just what you're going to use. I mean, we can read this documentation,

1696
01:42:23,320 --> 01:42:27,320
create a data set whose elements are slices of the given tensors, the given

1697
01:42:27,320 --> 01:42:31,000
tensors are sliced along their first dimension, this operation preserves the

1698
01:42:31,000 --> 01:42:34,600
structure of the input tensors, removing the first dimension of each tensor and

1699
01:42:34,600 --> 01:42:37,760
using it as the data set dimension. So I mean, you guys can look at that, like

1700
01:42:37,760 --> 01:42:40,960
read through the documentation, if you want. But essentially, what it does is

1701
01:42:40,960 --> 01:42:48,120
create the data set object for us. Now, if shuffle DS equals DS dot shuffle 1000,

1702
01:42:48,280 --> 01:42:50,880
what this does is just shuffle the data set, you don't really need to

1703
01:42:50,880 --> 01:42:55,040
understand more than that. And then what we do is we say data set equals data set

1704
01:42:55,040 --> 01:42:59,240
dot batch, the batch size, which is going to be 32, and then repeat for the

1705
01:42:59,240 --> 01:43:02,760
number of epochs. So what this is going to do is essentially take our data set

1706
01:43:02,760 --> 01:43:08,280
and split it into the number of, I don't want to, what do I want to call it? Like

1707
01:43:08,280 --> 01:43:12,480
blocks that are going to be passed to our model. So we can do this by knowing

1708
01:43:12,480 --> 01:43:15,800
the batch size, it obviously knows how many elements because that's the data set

1709
01:43:15,800 --> 01:43:20,040
object itself, and then repeat number of epochs. So this can figure out, you know,

1710
01:43:20,080 --> 01:43:26,480
how many one how many blocks do I need to split it into to feed it to my model. Now

1711
01:43:26,480 --> 01:43:30,560
return data set, simply from this function here, we'll return that data set

1712
01:43:30,600 --> 01:43:34,880
object. And then on the outside return, we actually return this function. So

1713
01:43:34,880 --> 01:43:38,440
what this out exterior function does, and I'm really just trying to break this

1714
01:43:38,440 --> 01:43:42,840
down. So you guys understand is make an input function, it literally makes a

1715
01:43:42,840 --> 01:43:48,320
function and returns the function object to wherever we call it from. So that's

1716
01:43:48,320 --> 01:43:52,920
how that works. Now we have a train input function and an eval input function. And

1717
01:43:52,920 --> 01:43:56,160
what we need to do to create these, it's just use this function that we've

1718
01:43:56,160 --> 01:44:01,320
defined above. So we say make input function, df train, y train, so our data

1719
01:44:01,320 --> 01:44:04,960
frame for training and our data frame for the labels of that. So we can see the

1720
01:44:04,960 --> 01:44:08,320
comment, you know, here, we will call the input function, right? And then eval

1721
01:44:08,320 --> 01:44:12,200
train, so it's going to be the same thing, except for the evaluation, we don't

1722
01:44:12,240 --> 01:44:15,680
need to shuffle the data because we're not training it, we only need one epoch.

1723
01:44:15,720 --> 01:44:19,720
Because again, we're just training it. And we'll pass the evaluation data set and

1724
01:44:19,720 --> 01:44:25,440
the evaluation value from Y. Okay, so that's it for making the input function. Now

1725
01:44:25,440 --> 01:44:29,600
I know this is complicated, but that's the way we have to do it. And unfortunately,

1726
01:44:29,600 --> 01:44:32,520
if you don't understand after that, there's not much more I can do, you might

1727
01:44:32,520 --> 01:44:36,320
just have to read through some of the documentation. Alright, creating the

1728
01:44:36,320 --> 01:44:39,240
model, we are finally here, I know this has been a while, but I need to get

1729
01:44:39,280 --> 01:44:42,360
through everything. So linear estimate, so we're going to copy this, and I'm just

1730
01:44:42,360 --> 01:44:46,200
going to put it in here and we'll talk about what this does. So linear underscore

1731
01:44:46,200 --> 01:44:52,520
EST equals tf dot estimator dot linear classifier, and we're giving it the feature

1732
01:44:52,520 --> 01:44:55,640
columns that we created up here. So this work was not for nothing, we have this

1733
01:44:55,640 --> 01:45:00,360
feature column, which defines, you know, what is in every single, like what should

1734
01:45:00,360 --> 01:45:05,400
we expect for our input data, we pass that to a linear classifier object from

1735
01:45:05,400 --> 01:45:10,320
the estimator module from TensorFlow. And then that creates the model for us. Now,

1736
01:45:10,440 --> 01:45:13,160
this again is syntax that you don't need to memorize, you just need to understand

1737
01:45:13,160 --> 01:45:16,800
how it works. What we're doing is creating an estimator, all of these kind of core

1738
01:45:16,800 --> 01:45:20,480
learning algorithms use what's called estimators, which are just basic

1739
01:45:20,480 --> 01:45:24,040
implementations of algorithms and TensorFlow. And again, pass the feature

1740
01:45:24,040 --> 01:45:28,880
columns. That's how that works. Alright, so now let's go to training the model. Okay,

1741
01:45:28,880 --> 01:45:31,360
so I'm just going to copy this again, I know you guys think I'm just copying the

1742
01:45:31,360 --> 01:45:34,360
code back and forth, but I'm not going to memorize the syntax, I just want to

1743
01:45:34,360 --> 01:45:37,480
explain to you how all this works. And again, you guys will have all this code,

1744
01:45:37,480 --> 01:45:41,840
you can mess with it, play with it, and learn on your own that way. So to train

1745
01:45:41,880 --> 01:45:46,400
is really easy. All we need to do, I say linear EST dot train, and then just give

1746
01:45:46,400 --> 01:45:50,400
that input function. So that input function that we created up here, right,

1747
01:45:50,400 --> 01:45:54,920
which was returned from make input function, like this train input function

1748
01:45:54,920 --> 01:45:59,600
here is actually equal to a function, it's equal to a function object itself. If I

1749
01:45:59,640 --> 01:46:05,160
were to call train underscore input function like this, this would actually

1750
01:46:05,200 --> 01:46:09,120
call this function. That's how this works in Python. It's a little bit of a

1751
01:46:09,120 --> 01:46:13,920
complicated syntax, but that's how it works. We pass that function here. And then

1752
01:46:13,920 --> 01:46:17,680
this will use the function to grab all of the input that we need and train the

1753
01:46:17,680 --> 01:46:21,880
model. Now the result is going to be rather than trained, we're going to

1754
01:46:21,880 --> 01:46:25,240
evaluate right and notice that we didn't store this one in a variable, but we're

1755
01:46:25,240 --> 01:46:29,360
storing the result in a variable so that we can look at it. Now clear output is

1756
01:46:29,400 --> 01:46:32,520
just from what we import above just going to clear the console output, because

1757
01:46:32,520 --> 01:46:35,880
there will be some output while we're training. And then we can present print

1758
01:46:35,920 --> 01:46:40,440
the accuracy of this model. So let's actually run this and see how this

1759
01:46:40,440 --> 01:46:43,640
works. This will take a second. So I'll be back once this is done. Okay, so we're

1760
01:46:43,640 --> 01:46:49,680
back, and we've got a 73.8% accuracy. So essentially what we've done right is

1761
01:46:49,680 --> 01:46:52,560
we've trained the model, you might have seen a bunch of output while you were

1762
01:46:52,560 --> 01:46:56,640
doing this on your screen. And then we printed out the accuracy after

1763
01:46:56,680 --> 01:47:00,480
evaluating the model. This accuracy isn't very good. But for our first shot,

1764
01:47:00,480 --> 01:47:04,680
this okay, we're going to talk about how to improve this in a second. Okay, so

1765
01:47:04,680 --> 01:47:08,040
we've evaluated the data set, we stored that in result. I want to actually look

1766
01:47:08,040 --> 01:47:12,120
at what result is, because obviously you can see we've referenced the accuracy

1767
01:47:12,120 --> 01:47:15,800
part, like you know, as if this was a Python dictionary. So let's run this one

1768
01:47:15,800 --> 01:47:19,440
more time. Oh, this is going to take a second again. So okay, so we printed out

1769
01:47:19,440 --> 01:47:22,520
result here, and we can see that we have actually a bunch of different values. So

1770
01:47:22,520 --> 01:47:26,560
we have accuracy, accuracy baseline, AUC, and all these different kinds of

1771
01:47:26,560 --> 01:47:30,240
statistical values. Now, these aren't really going to mean much to you guys, but

1772
01:47:30,240 --> 01:47:33,360
I just want to show you that we do have those statistics and to access any

1773
01:47:33,360 --> 01:47:36,480
specific one, this is really just a dictionary object. So we can just

1774
01:47:36,480 --> 01:47:40,520
reference the key that we want, which is what we did with accuracy. Now, notice

1775
01:47:40,760 --> 01:47:45,520
our accuracy actually changed here. We went to 76. The reason for this is like

1776
01:47:45,520 --> 01:47:48,680
I said, you know, our data is getting shuffled, it's getting put in a different

1777
01:47:48,680 --> 01:47:52,400
order. And based on the order in which we see data, our model will, you know,

1778
01:47:52,440 --> 01:47:56,360
make different predictions and be trained differently. So if we had, you know,

1779
01:47:56,720 --> 01:48:01,360
another epoch, right, if I change epochs to say 11 or 15, our accuracy will

1780
01:48:01,360 --> 01:48:04,160
change. Now it might go up, it might go down. That's something we have to play

1781
01:48:04,160 --> 01:48:07,480
with as you know, our machine, a machine learning developer, right? That's what

1782
01:48:07,480 --> 01:48:11,120
your goal is, is to get the most accurate model. Okay, so now it's time to

1783
01:48:11,120 --> 01:48:14,240
actually use the model to make predictions. So up until this point, we've

1784
01:48:14,240 --> 01:48:17,600
just been doing a lot of work to understand how to create the model, you

1785
01:48:17,600 --> 01:48:21,560
know, what the model is, how we make an input function, training, testing data, I

1786
01:48:21,560 --> 01:48:26,160
know a lot, a lot, a lot of stuff. Now to actually use this model and like make

1787
01:48:26,240 --> 01:48:30,000
accurate predictions with it is somewhat difficult, but I'm going to show you

1788
01:48:30,000 --> 01:48:34,840
how. So essentially, TensorFlow models are built to make predictions on a lot

1789
01:48:34,840 --> 01:48:39,320
of things at once. They're not great at making predictions on like one piece of

1790
01:48:39,320 --> 01:48:43,040
data, you just want like one passenger to make a prediction for, they're much

1791
01:48:43,040 --> 01:48:46,600
better at working in like large batches of data. Now you can definitely do it

1792
01:48:46,600 --> 01:48:50,160
with one, but I'm just going to show you how we can make a prediction for every

1793
01:48:50,200 --> 01:48:54,600
single point that's in that evaluation data set. So right now we looked at the

1794
01:48:54,600 --> 01:48:58,440
accuracy, and the way we determine the accuracy was by essentially comparing

1795
01:48:58,600 --> 01:49:02,840
the results that the predictions gave from our model versus what the actual

1796
01:49:02,840 --> 01:49:06,400
results were for every single one of those passengers. And that's how we came

1797
01:49:06,400 --> 01:49:11,800
up with an accuracy of 76%. Now if we want to actually check and get predictions

1798
01:49:11,800 --> 01:49:15,400
from the model and see what those actual predictions are, what we can do is use

1799
01:49:15,440 --> 01:49:19,760
a method called dot predict. So what I'm going to do is I'm going to say, I

1800
01:49:19,760 --> 01:49:25,240
guess, results like this equals, and in this case, we're going to do the model

1801
01:49:25,240 --> 01:49:31,520
name, which is linear EST dot predict. And then inside here, what we're going to

1802
01:49:31,520 --> 01:49:36,520
pass is that input function we use for the evaluation. So just like, you know, we

1803
01:49:36,520 --> 01:49:40,160
need to pass an input function to actually train the model, we also need to

1804
01:49:40,160 --> 01:49:43,680
pass an input function to make a prediction. Now this input function could

1805
01:49:43,680 --> 01:49:46,800
be a little bit different, we can modify this a bit if we wanted to, but to keep

1806
01:49:46,800 --> 01:49:50,280
things simple, we use the same one for now. So what I'm going to do is just use

1807
01:49:50,280 --> 01:49:53,360
this eval input function. So the one we've already created where we did, you

1808
01:49:53,360 --> 01:49:57,480
know, one epoch, we don't need to shuffle because it's just the evaluation set. So

1809
01:49:57,480 --> 01:50:01,640
inside here, we're going to eval input function. Now what we need to do though is

1810
01:50:01,640 --> 01:50:05,400
convert this to a list, just because we're going to loop through it. And I'm

1811
01:50:05,400 --> 01:50:08,480
actually going to print out this value so we can see what it is before we go to

1812
01:50:08,480 --> 01:50:13,440
the next step. So let's run this and have a look at what we get. Okay, so we get

1813
01:50:13,480 --> 01:50:18,320
logistics array, we can see all these different values. So we have, you know,

1814
01:50:18,320 --> 01:50:23,840
this array with this value, we have probabilities, this value. And this is

1815
01:50:23,840 --> 01:50:26,440
kind of what we're getting. So we're getting logistic, all classes, like

1816
01:50:26,440 --> 01:50:29,720
there's all this random stuff. What you hopefully should notice, and I know I'm

1817
01:50:29,720 --> 01:50:33,440
just like whizzing through is that we have a dictionary that represents the

1818
01:50:33,440 --> 01:50:37,240
predictions. And I'll see if I can find the end of the dictionary here for every

1819
01:50:37,240 --> 01:50:43,800
single, what is it prediction? So since we've passed, you know, 267 input data

1820
01:50:43,800 --> 01:50:48,360
from this, you know, eval input function, what was returned to us is a list of all

1821
01:50:48,360 --> 01:50:51,880
of these different dictionaries that represent each prediction. So what we

1822
01:50:51,880 --> 01:50:56,680
need to do is look at each dictionary so that we can determine what the actual

1823
01:50:56,680 --> 01:51:02,440
prediction was. So what I'm going to do is actually just present to result. I'm

1824
01:51:02,440 --> 01:51:05,120
wondering to result zero, because this is a list. So that should mean we can

1825
01:51:05,120 --> 01:51:09,200
index it. So we can actually look at one prediction. Okay, so this is the

1826
01:51:09,280 --> 01:51:13,440
dictionary of one prediction. So I know this seems like a lot. But this is what

1827
01:51:13,440 --> 01:51:16,840
we have. This is our prediction. So logistics, we get some array, we have

1828
01:51:16,840 --> 01:51:21,440
logistic in here in this dictionary, and then we have probabilities. So what I

1829
01:51:21,440 --> 01:51:26,240
actually want is probability. Now, since what we ended up having was a

1830
01:51:26,240 --> 01:51:30,120
prediction of two classes, right, either zero or one, we're predicting either

1831
01:51:30,120 --> 01:51:33,800
someone survived, or they didn't survive, or what their percentage should be. We

1832
01:51:33,800 --> 01:51:39,040
can see that the percentage of survival here is actually 96%. And the percentage

1833
01:51:39,080 --> 01:51:43,520
that it thinks that it won't survive is, you know, 3.3%. So if we want to

1834
01:51:43,560 --> 01:51:49,680
access this, what we need to do is click do result at some index. So whatever,

1835
01:51:49,680 --> 01:51:53,080
you know, one we want. So we're gonna say result. And then here, we're going to

1836
01:51:53,080 --> 01:51:56,680
put probabilities. So I'm just going to print that like that. And then we can

1837
01:51:56,680 --> 01:52:00,680
see the probabilities. So let's run this. And now we see our probabilities are

1838
01:52:00,720 --> 01:52:06,880
96 and 33. Now, if we want the probability of survival, so I think I

1839
01:52:06,880 --> 01:52:10,640
actually might have messed this up, I'm pretty sure the survival probability is

1840
01:52:10,640 --> 01:52:14,400
actually the last one, whereas like the non survival is the first one, because

1841
01:52:14,400 --> 01:52:17,400
zero means you didn't survive and one means you did survive. So that's my

1842
01:52:17,400 --> 01:52:20,960
bad, I messed that up. So I actually want their chance of survival, I'll index

1843
01:52:20,960 --> 01:52:26,720
one. So if I index one, you see we get 3.3%. But if I wanted their chance of

1844
01:52:26,720 --> 01:52:30,800
survival, not surviving, I would index zero. And that makes sense because zero

1845
01:52:30,800 --> 01:52:34,400
is you know, what we're looking like zero represents they didn't survive, whereas

1846
01:52:34,400 --> 01:52:38,440
one represents they did survive. So that's kind of how we do that. So that's

1847
01:52:38,440 --> 01:52:41,960
how we get them. Now, if we wanted to loop through all of these, we could we

1848
01:52:41,960 --> 01:52:45,000
could loop through every dictionary, we could print every single probability of

1849
01:52:45,000 --> 01:52:48,560
each person. We can also look at that person's stats and then look at their

1850
01:52:48,560 --> 01:52:53,000
probability. So let's see the probability of surviving is in this case, you

1851
01:52:53,000 --> 01:52:57,320
know, 3%, or whatever it was 3.3%. But let's look at the person that we were

1852
01:52:57,320 --> 01:53:01,880
actually predicting them and see if that makes sense. So if I go eval, or what

1853
01:53:01,880 --> 01:53:09,240
was it df eval dot loc, zero, we print that and then we print the result. What

1854
01:53:09,240 --> 01:53:13,840
we can see is that for the person who was male and 35 that had no siblings, their

1855
01:53:13,840 --> 01:53:16,680
fair was this, they're in third class, we don't know what deck they were on and

1856
01:53:16,680 --> 01:53:21,560
they were alone. They have a 3.3% chance of survive. Now, if we change this, we

1857
01:53:21,600 --> 01:53:26,000
could go like to to let's have a look at this second person and see what their

1858
01:53:26,000 --> 01:53:29,800
chance of survival is. Okay, so they have a higher percent chance of 38% chance

1859
01:53:29,840 --> 01:53:32,840
they're female, they're a little bit older. So that might be a reason why their

1860
01:53:32,840 --> 01:53:36,840
survival rates a bit lower. I mean, we can keep doing this and look through and

1861
01:53:36,840 --> 01:53:41,880
see what it is, right? If we want to get the actual value, like if this person

1862
01:53:41,880 --> 01:53:48,040
survived, or if they didn't survive, and what I can do is I can print df eval.

1863
01:53:48,720 --> 01:53:53,160
Actually, it's not going to be eval, it's going to be y underscore eval. Yeah. And

1864
01:53:53,160 --> 01:53:58,240
that's going to be loc three. Now this will give us if they survived or not. So

1865
01:53:58,240 --> 01:54:02,040
actually, in this case, that person did survive, but we're only predicting a 32%.

1866
01:54:02,320 --> 01:54:05,200
So you can see that that's, you know, represented in the fact that we only have

1867
01:54:05,200 --> 01:54:09,720
about a 76% accuracy, because this model is not perfect. And in this instance, it

1868
01:54:09,720 --> 01:54:13,560
was pretty bad. It's saying they have a 32% chance of surviving, but they actually

1869
01:54:13,560 --> 01:54:16,400
did survive. So maybe that should be higher, right? So we could change this

1870
01:54:16,400 --> 01:54:21,560
number, go for four, I'm just messing around and showing you guys, you know, how

1871
01:54:21,560 --> 01:54:25,920
we use this. So in this one, you know, same thing, this person survived, although

1872
01:54:26,800 --> 01:54:30,640
what is it, they only were given a 14% chance of survival. So anyways, that is

1873
01:54:30,640 --> 01:54:34,000
how that works. This is how we actually make predictions and look at the

1874
01:54:34,000 --> 01:54:37,440
predictions, you understand that now what's happening is I've converted this to

1875
01:54:37,440 --> 01:54:40,640
a list just because this is actually a generator object, which means it's meant

1876
01:54:40,640 --> 01:54:43,560
to just be looped through rather than just look at it with a list, but that's

1877
01:54:43,560 --> 01:54:47,680
fine, we'll use a list. And then we can just print out, you know, result at

1878
01:54:47,680 --> 01:54:51,080
whatever index probabilities, and then one to represent their chance of

1879
01:54:51,080 --> 01:54:55,000
survival. Okay, so that has been it for linear regression. Now let's get into

1880
01:54:55,000 --> 01:54:59,200
classification. And now we are on to classification. So essentially,

1881
01:54:59,200 --> 01:55:04,680
classification is differentiating between, you know, data points and separating

1882
01:55:04,680 --> 01:55:08,400
them into classes. So rather than predicting a numeric value, which we did

1883
01:55:08,400 --> 01:55:11,840
with regression earlier, so linear regression, and you know, the percentage

1884
01:55:11,880 --> 01:55:16,520
survival chance, which is a numeric value, we actually want to predict classes. So

1885
01:55:16,520 --> 01:55:21,160
what we're going to end up doing is predicting the probability that a specific

1886
01:55:21,160 --> 01:55:24,880
data point or a specific entry or whatever we're going to call it is within

1887
01:55:24,880 --> 01:55:27,960
all of the different classes it could be. So for the example here, we're going to

1888
01:55:27,960 --> 01:55:32,560
use flowers. So it's called the iris. I think it's the iris flower data set or

1889
01:55:32,560 --> 01:55:35,360
something like that. And we're going to use some different properties of flowers

1890
01:55:35,360 --> 01:55:38,480
to predict what species of flower it is. So that's the difference between

1891
01:55:38,480 --> 01:55:42,000
classification and regression. Now I'm not going to talk about the specific

1892
01:55:42,000 --> 01:55:45,200
algorithm we're going to use here for classification, because there's just so

1893
01:55:45,240 --> 01:55:50,640
many different ones you can use. But yeah, I mean, if you really care about how

1894
01:55:50,640 --> 01:55:54,000
they work on a lower mathematical level, I'm not going to be explaining that

1895
01:55:54,000 --> 01:55:57,120
because it doesn't make sense to explain it for one algorithm when there's like

1896
01:55:57,160 --> 01:56:00,840
hundreds and they all work a little bit differently. So you guys can kind of look

1897
01:56:00,840 --> 01:56:04,880
that up. And I'll tell you some resources and where you can find that. I'm also

1898
01:56:04,880 --> 01:56:07,880
going to go faster through this example, just because I've already covered kind

1899
01:56:07,880 --> 01:56:10,960
of a lot of the fundamental stuff in linear regression. So hopefully we should

1900
01:56:10,960 --> 01:56:14,360
get this one done a little bit quicker and move on to the next kind of aspects

1901
01:56:14,360 --> 01:56:18,720
in this series. Alright, so first steps, load tensor flow, import tensor flow, we've

1902
01:56:18,720 --> 01:56:22,440
done that already, data set, we need to talk about this. So the data that we're

1903
01:56:22,440 --> 01:56:26,560
using is that iris flowers data set like I talked about. And this specific data set

1904
01:56:26,560 --> 01:56:29,680
separates flowers into three different species. So we have these different

1905
01:56:29,680 --> 01:56:33,560
species. This is the information we have. So septal length width, petal length,

1906
01:56:33,560 --> 01:56:36,480
petal width, we're going to use that information obviously to make the

1907
01:56:36,480 --> 01:56:40,840
predictions. So given this information, you know, in our final model, can it tell

1908
01:56:40,840 --> 01:56:45,040
us which one of these flowers it's most likely to be? Okay, so what we're going to

1909
01:56:45,040 --> 01:56:49,320
do now is define the CSV column names and species. So the column names are just

1910
01:56:49,320 --> 01:56:52,640
going to define what we're going to have in our data set is like the headers for

1911
01:56:52,640 --> 01:56:56,240
the columns, species obviously is just the species and we'll throw them there.

1912
01:56:57,240 --> 01:57:00,240
Alright, so now we're going to load in our data sets. So this is going to be

1913
01:57:00,240 --> 01:57:03,040
different every time you're kind of working with models, depending on where

1914
01:57:03,040 --> 01:57:05,440
you're getting your data from. In our example, we're going to get it from

1915
01:57:05,480 --> 01:57:10,280
Keras, which is kind of in sub module of TensorFlow. It has a lot of useful

1916
01:57:10,280 --> 01:57:15,040
data sets and tools that we'll be using throughout the series. But keras.utils.get

1917
01:57:15,040 --> 01:57:18,400
file, again, don't really focus on this, just understand what this is going to

1918
01:57:18,400 --> 01:57:23,360
do is save this file onto our computer as iris training dot CSV, grab it from

1919
01:57:23,360 --> 01:57:27,120
this link. And then what we're going to do down here is load the train and test

1920
01:57:27,120 --> 01:57:31,280
and again, notice this training and this is testing into two separate data

1921
01:57:31,280 --> 01:57:35,040
frames. So here we're going to use the names of the columns as the CSV column

1922
01:57:35,040 --> 01:57:38,240
names, we're going to use the path as whatever we loaded here, header equals

1923
01:57:38,240 --> 01:57:43,320
zero, which just means row zero is the header. Alright, so now we will move down

1924
01:57:43,320 --> 01:57:46,560
and we'll have a look at our data set. So like we've done before, oh, I've got

1925
01:57:46,560 --> 01:57:51,200
to run this code first. CSV column names. Okay, so we've just we're just running

1926
01:57:51,200 --> 01:57:55,200
things in the wrong order here, apparently. Okay, so let's look at the head. So

1927
01:57:55,200 --> 01:57:58,360
we can see this is kind of what our data frame looks like. And notice that our

1928
01:57:58,360 --> 01:58:03,080
species here are actually defined numerically. So rather than before, when

1929
01:58:03,080 --> 01:58:06,080
we had to do that thing where you know, we made those feature columns, and we

1930
01:58:06,080 --> 01:58:09,800
converted the categorical data into numeric data with those kind of weird

1931
01:58:09,800 --> 01:58:14,360
TensorFlow tools. This is actually already encoded for us. Now zero stands

1932
01:58:14,360 --> 01:58:19,200
for SITOSA. And then one and two obviously stand for these ones respectively. And

1933
01:58:19,200 --> 01:58:24,080
that's how that works. Now these I believe are in centimeters, the septal length,

1934
01:58:24,320 --> 01:58:28,240
petal length, petal width, that's not super important. But sometimes you do

1935
01:58:28,240 --> 01:58:32,400
want to know that information. Okay, so now we're going to pop up those columns

1936
01:58:32,400 --> 01:58:36,480
for the species like we did before and separate that into train white test, why

1937
01:58:36,480 --> 01:58:41,040
and then have a look at the head again. So let's do that and run this notice that

1938
01:58:41,040 --> 01:58:45,200
is gone. Again, we've talked about how that works. And then these, if we want to

1939
01:58:45,200 --> 01:58:49,200
have a look at them, and actually, let's do this, by just having a new block,

1940
01:58:49,200 --> 01:58:57,880
let's say train underscore y dot, what is it dot head? If I could spell head

1941
01:58:57,880 --> 01:59:01,720
correctly. Okay, so we run head, and we can see this is what it looks like

1942
01:59:01,720 --> 01:59:05,400
nothing special. That's what we're getting. Alright, so let's delete that. Let's

1943
01:59:05,400 --> 01:59:08,440
look at the shape of our training data. I mean, we can probably guess what it is

1944
01:59:08,440 --> 01:59:12,000
already, right? We're going to have shape four, because we have four features. And

1945
01:59:12,000 --> 01:59:16,040
then how many entries do we have? Well, I'm sure this will tell us so 120 entries

1946
01:59:16,200 --> 01:59:20,520
in shape four. Awesome. That's our shape. Okay, input function. So we're moving

1947
01:59:20,520 --> 01:59:22,920
fast here already, we're getting into a lot of the coding. So what I'm actually

1948
01:59:22,920 --> 01:59:26,360
going to do is again, copy this over into a separate document, and I'll be back in

1949
01:59:26,360 --> 01:59:29,840
a second with all that. Okay, so input function time, we already know what the

1950
01:59:29,880 --> 01:59:34,040
input function does because we used it previously. Now this input function is a

1951
01:59:34,040 --> 01:59:38,360
little bit different than before, just because we're kind of changing things

1952
01:59:38,360 --> 01:59:43,640
slightly. So here, we don't actually have any, what do you call it, we don't have

1953
01:59:43,640 --> 01:59:47,240
any epochs, and our batch size is different. So what we've done here is

1954
01:59:47,240 --> 01:59:52,280
rather than actually, you know, defining like make input function, we just have

1955
01:59:52,400 --> 01:59:56,880
input function like this. And what we're going to do is a little bit different

1956
01:59:56,880 --> 01:59:59,560
when we pass this input function, I'll kind of show you it's a little bit more

1957
01:59:59,560 --> 02:00:02,680
complicated. But you can see that we've cleaned this up a little bit. So exactly

1958
02:00:02,680 --> 02:00:05,640
we're doing what we did do before, we're converting this data, which is our

1959
02:00:05,640 --> 02:00:09,360
features, which we're passing in here into a data set. And then we're passing

1960
02:00:09,360 --> 02:00:13,560
those labels as well. And then if we're training, so if training is true, what

1961
02:00:13,560 --> 02:00:17,440
we're going to do is say data set is equal to the data set dot shuffle. So we're

1962
02:00:17,440 --> 02:00:21,280
going to shuffle that information, and then repeat that. And that is all we

1963
02:00:21,280 --> 02:00:25,920
really need to do. We can do data set dot batch at the batch size 256 return that.

1964
02:00:26,240 --> 02:00:29,520
And we're good to go. So this is our input function. Again, these are kind of

1965
02:00:29,520 --> 02:00:33,040
complicated. You kind of have to just get experience seeing a bunch of

1966
02:00:33,040 --> 02:00:36,320
different ones to understand how to actually make one on your own. For now

1967
02:00:36,320 --> 02:00:39,360
on, don't worry about it too much. You can pretty much just copy the input

1968
02:00:39,360 --> 02:00:42,880
functions you've created before, and modify them very slightly if you're

1969
02:00:42,880 --> 02:00:45,480
going to be doing your own models. But by the end of this, you should have a

1970
02:00:45,480 --> 02:00:48,520
good idea of how these input functions work. We will have seen like four or

1971
02:00:48,520 --> 02:00:51,760
five different ones. And then, you know, we can kind of mess with them and tweak

1972
02:00:51,760 --> 02:00:55,800
them as we go on, but don't focus on it too much. Okay, so input function, this

1973
02:00:55,840 --> 02:00:59,000
is our input function, I'm not really going to go in too much more detail with

1974
02:00:59,000 --> 02:01:03,200
that. And now our feature columns. So this is again, pretty straightforward for

1975
02:01:03,200 --> 02:01:06,080
the feature columns. All we need to do for this is since our all numeric feature

1976
02:01:06,080 --> 02:01:09,120
columns is rather than having two for loops where we were separating the

1977
02:01:09,120 --> 02:01:13,960
numeric and categorical feature columns before, we can just loop through all of

1978
02:01:13,960 --> 02:01:18,240
the keys in our training data set. And then we can append to my feature

1979
02:01:18,240 --> 02:01:23,240
columns blank list, the feature column, the numeric column, and the key is

1980
02:01:23,240 --> 02:01:26,200
equal to whatever key we've looped through here. Now I'm going to show you

1981
02:01:26,200 --> 02:01:29,560
what this means in case anyone's confused. Again, you can see when I print my

1982
02:01:29,560 --> 02:01:34,120
feature columns, we get key equals septal length, we get our shape, and we get

1983
02:01:34,160 --> 02:01:37,640
all of that other nice information. So let's copy this into the other one and

1984
02:01:37,640 --> 02:01:42,840
have a look at our output after this. Okay, so my feature columns for key and

1985
02:01:42,840 --> 02:01:48,360
train dot keys. So notice train is here, train dot keys. What that does is

1986
02:01:48,360 --> 02:01:52,360
actually give us all the columns. So this was a really quick and easy way to

1987
02:01:52,360 --> 02:01:55,280
kind of loop through all the different columns. Although I could have looped

1988
02:01:55,280 --> 02:01:59,280
through CSV column names and just remove the species column to do that. But

1989
02:01:59,800 --> 02:02:03,160
again, we don't really need to. So for key and train dot keys, my feature

1990
02:02:03,160 --> 02:02:06,720
columns dot append tf feature column, numeric column, key equals key, this

1991
02:02:06,720 --> 02:02:09,080
was just going to create those feature columns, we don't need to do that

1992
02:02:09,080 --> 02:02:12,320
vocabulary thing and that dot unique because again, these are all already

1993
02:02:12,320 --> 02:02:16,280
encoded for us. Okay, awesome. So that was the next step. So let's go back

1994
02:02:16,280 --> 02:02:19,480
here, building the model. Okay, so this is where we need to talk a bit more

1995
02:02:19,480 --> 02:02:22,920
in depth of what we're actually going to build. So the model for this is a

1996
02:02:22,920 --> 02:02:26,920
classification model. Now there is like hundreds of different classification

1997
02:02:26,920 --> 02:02:30,800
models we can use that are pre made in TensorFlow. And so far, what we've done

1998
02:02:30,800 --> 02:02:35,000
with that linear classifier is that's a pre made model that we kind of just

1999
02:02:35,000 --> 02:02:38,920
feed a little bit of information to and it just works for us. Now here we have

2000
02:02:38,920 --> 02:02:43,120
two kind of main choices that we can use for this kind of classification task

2001
02:02:43,120 --> 02:02:47,120
that are pre built in TensorFlow, we have a DNN classifier, which stands for a

2002
02:02:47,160 --> 02:02:51,200
deep neural network, which we've talked about very vaguely, very briefly. And we

2003
02:02:51,200 --> 02:02:54,920
have a linear classifier. Now a linear classifier works very similarly to

2004
02:02:54,920 --> 02:02:58,760
linear regression, except it does classification, rather than regression. So

2005
02:02:58,760 --> 02:03:02,480
we get actually numeric value, or we get, sorry, you know, the labels like

2006
02:03:02,480 --> 02:03:07,520
probability of being a specific label, rather than a numeric value. But in this

2007
02:03:07,520 --> 02:03:10,680
instance, we're actually going to go with deep neural network. Now that's simply

2008
02:03:10,680 --> 02:03:14,760
because TensorFlow on their website, like this is all of this is kind of

2009
02:03:14,800 --> 02:03:18,200
building off of TensorFlow website, just all the code is very similar. And I've

2010
02:03:18,200 --> 02:03:22,280
just added my own spin and explain things very in depth. They've recommended

2011
02:03:22,280 --> 02:03:26,240
using that deep neural network for this is a better kind of choice. But typically

2012
02:03:26,240 --> 02:03:29,040
when you're creating machine learning apps, you'll mess around with different

2013
02:03:29,040 --> 02:03:32,480
models and kind of tweak them. And you'll notice that it's not that difficult to

2014
02:03:32,480 --> 02:03:35,400
change models, because most of the work comes from loading and kind of

2015
02:03:35,400 --> 02:03:40,400
pre processing our data. Okay, so what we need to do is build a deep neural

2016
02:03:40,400 --> 02:03:44,600
network with two hidden later, two hidden layers with 30 nodes and 10 hidden

2017
02:03:44,600 --> 02:03:48,320
nodes each. Now I'm going to draw out the architecture of this neural network in

2018
02:03:48,320 --> 02:03:51,080
just one second. But I want to show you what we've done here. So we said

2019
02:03:51,080 --> 02:03:56,160
classifier equals tf dot estimator. So this estimator module just stores a

2020
02:03:56,160 --> 02:04:00,200
bunch of pre made models from TensorFlow. So in this case, DNN classifier is

2021
02:04:00,200 --> 02:04:03,680
one of those. What we need to do is pass our feature columns just like we did to

2022
02:04:03,680 --> 02:04:08,160
our linear classifier. And now we need to define the hidden units. Now hidden

2023
02:04:08,200 --> 02:04:12,360
units is essentially us a building the architecture of the neural network. So

2024
02:04:12,360 --> 02:04:15,840
like you saw before, we had an input layer, we had some like middle layers

2025
02:04:15,840 --> 02:04:18,480
called our hidden layers in a neural network. And then we had our output

2026
02:04:18,480 --> 02:04:22,080
layer. I'm going to explain neural networks in the next module. So this will

2027
02:04:22,080 --> 02:04:26,120
all kind of click and make sense. For now, we've arbitrarily decided 30 nodes in

2028
02:04:26,120 --> 02:04:30,520
the first hidden layer, 10 in the second, and the number of classes is going to

2029
02:04:30,520 --> 02:04:33,320
be three. Now that's something that we need to decide. We know there's three

2030
02:04:33,320 --> 02:04:37,200
classes for the flowers. So that's what we've defined. Okay, so let's copy this

2031
02:04:37,200 --> 02:04:42,440
in. Go back to the other page here. And that is now our model. And now it is

2032
02:04:42,440 --> 02:04:45,920
time to talk about how we can actually train the model, which is coming down

2033
02:04:45,920 --> 02:04:49,280
here. Okay, so I'm going to copy this, I'm going to paste it over here and

2034
02:04:49,280 --> 02:04:52,480
let's just dig through this because this is a bit more of a complicated piece of

2035
02:04:52,480 --> 02:04:56,040
code than we usually used to work with. I'm also going to remove these comments

2036
02:04:56,040 --> 02:04:59,880
just to clean things up in here. So we've defined the classifier, which is a

2037
02:04:59,880 --> 02:05:03,040
deep neural network classifier, we have our feature columns hidden units

2038
02:05:03,040 --> 02:05:08,440
classes. Now to train the classifier. So we have this input function here. This

2039
02:05:08,440 --> 02:05:11,600
input function is different than the one we created previously. Remember when we

2040
02:05:11,600 --> 02:05:15,880
had previously was like make input, whatever function, I won't continue

2041
02:05:15,880 --> 02:05:19,720
typing in the inside it to find another function. And it actually returned that

2042
02:05:19,720 --> 02:05:23,840
function from this function. I know, complicated. If you're not a Python kind

2043
02:05:23,840 --> 02:05:27,720
of pro, I don't expect that to make perfect sense. But here, we just have a

2044
02:05:27,720 --> 02:05:30,960
function, right? We do not returning a function from another function, it's just

2045
02:05:31,000 --> 02:05:37,520
one function. So when we want to use this to train our model, what we do is

2046
02:05:37,520 --> 02:05:41,640
create something called a lambda. Now a lambda is an anonymous function that can

2047
02:05:41,640 --> 02:05:45,960
be defined in one line. When you write lambda, what that means is essentially

2048
02:05:45,960 --> 02:05:50,520
this is a function. So this is a function. And whatever's after the colon is what

2049
02:05:50,520 --> 02:05:54,960
this function does. Now this is a one line function. So like, if I create a

2050
02:05:54,960 --> 02:06:02,840
lambda here, right, and I say lambda, print, hi, and I said, x equals lambda,

2051
02:06:02,880 --> 02:06:08,040
and I called x like that, this works, this is a valid line of syntax. Actually,

2052
02:06:08,040 --> 02:06:10,920
I want to make sure that I'm not just like messing with you. And I say that

2053
02:06:10,920 --> 02:06:15,040
and then this is actually correct. Okay, so sorry, I just accidentally trained

2054
02:06:15,040 --> 02:06:17,960
the model. So I just commented that out. You can see we're printing high, right?

2055
02:06:17,960 --> 02:06:21,200
At the bottom of the screen, I know it's kind of small, but does say hi. That's

2056
02:06:21,200 --> 02:06:23,840
how this works. Okay, so this is a cool thing. If you haven't seen this in

2057
02:06:23,880 --> 02:06:27,840
Python before, that's what a lambda does allows you to define a function in one

2058
02:06:27,840 --> 02:06:31,760
line. Now the thing that's great about this is that we can say, like, you know,

2059
02:06:31,800 --> 02:06:35,000
x equals lambda, and here put another function, which is exactly what will be

2060
02:06:35,000 --> 02:06:38,960
done with this print function. And that means when we call x, it will, you know,

2061
02:06:39,000 --> 02:06:42,320
execute this function, which will just execute the other function. So it's kind

2062
02:06:42,320 --> 02:06:46,200
of like a chain where you call x, x is a function. And inside that function, it

2063
02:06:46,200 --> 02:06:48,920
does another function, right? It just like calling a function from inside a

2064
02:06:48,920 --> 02:06:55,080
function. So what is lambda doing here? Well, since we need the actual function

2065
02:06:55,120 --> 02:07:01,200
object, what we do is we define a function that returns to us a function. So this

2066
02:07:01,200 --> 02:07:06,120
actually just like it calls this function, when you put this here. Now there's no

2067
02:07:06,160 --> 02:07:09,480
I can't it's it's very difficult to explain this if you don't really

2068
02:07:09,480 --> 02:07:13,600
understand the concept of lambdas, and you don't understand the input functions. But

2069
02:07:13,600 --> 02:07:17,560
just know we're doing this because of the fact that we didn't embed another

2070
02:07:17,600 --> 02:07:22,160
function and return the function object. If we had done that, if we had done that,

2071
02:07:22,200 --> 02:07:26,080
you know, input function that we had created before where we had the interior

2072
02:07:26,080 --> 02:07:29,720
function, then we wouldn't need to do this because what would happen is we would

2073
02:07:29,720 --> 02:07:36,000
return the input function, right, like that, which means when we passed it into

2074
02:07:36,000 --> 02:07:40,920
here, it could just call that directly. It didn't need to have a lambda. Whereas

2075
02:07:40,920 --> 02:07:46,120
here, though, since we need to just put a lambda, we need to define what this is

2076
02:07:46,120 --> 02:07:49,280
and then and then this works. That's just there's no other way to really explain

2077
02:07:49,280 --> 02:07:53,640
this. So yeah, what we do is we create this input function. So we pass we have

2078
02:07:53,640 --> 02:07:58,160
train, we have train y and we have training equals true. And then we do steps

2079
02:07:58,160 --> 02:08:02,880
equals 5000. So this is similar to an epoch, except this is just defining a set

2080
02:08:02,880 --> 02:08:05,760
amount of steps we're going to go through. So rather than saying like we'll go

2081
02:08:05,760 --> 02:08:08,840
through the data set 10 times, we're just going to say we'll go through the data

2082
02:08:08,840 --> 02:08:13,720
set until we fit 5000 numbers, like 5000 things that have been looked at. So

2083
02:08:13,760 --> 02:08:16,960
that's what this does with that train. Let's run this and just look at the

2084
02:08:16,960 --> 02:08:21,240
training output from our model, it gives us some like, things here, we can kind of

2085
02:08:21,240 --> 02:08:25,160
see how this is working. Notice that if I can stop here for a second, it tells us

2086
02:08:25,160 --> 02:08:30,080
the current step, it tells us the loss, the lowest, the lower this number, the

2087
02:08:30,080 --> 02:08:34,200
better. And then it tells us global steps per second. So how many steps we're

2088
02:08:34,200 --> 02:08:40,120
completing per second. Now at the end here, we get final step loss of 39, which

2089
02:08:40,120 --> 02:08:45,240
is pretty high, which means this is pretty bad. But that's fine. This is kind

2090
02:08:45,240 --> 02:08:48,560
of just our first test at training in neural network. So this is just giving

2091
02:08:48,560 --> 02:08:51,800
us output while it's training to kind of save what's happening. Now, in our case,

2092
02:08:51,800 --> 02:08:55,160
we don't really care because this is a very small model. When you're training

2093
02:08:55,160 --> 02:08:58,720
models that are massive and take terabytes of data, you kind of care about the

2094
02:08:58,720 --> 02:09:03,080
progress of them. So that's when you would use kind of that output, right? And

2095
02:09:03,080 --> 02:09:05,760
you would actually look at that. Okay, so now that we've trained the model, let's

2096
02:09:05,760 --> 02:09:09,520
actually do an evaluation on the model. So we're just going to say classifier dot

2097
02:09:09,560 --> 02:09:13,240
evaluate. And what we're going to do is a very similar thing to what we've done

2098
02:09:13,240 --> 02:09:18,080
here is just pass this input function, right, like here with a lambda once

2099
02:09:18,080 --> 02:09:22,840
again, and reason we add the lambda when we don't have this like, double

2100
02:09:22,840 --> 02:09:26,200
function going on, like a nested function, we need the lambda. And then in

2101
02:09:26,200 --> 02:09:30,200
here, what we do is rather than passing train and train y, we're going to pass

2102
02:09:30,240 --> 02:09:35,720
test, I believe. And I think it's I just called it test y. Okay, and then for

2103
02:09:35,720 --> 02:09:40,360
training, obviously, this is false. So we can just set that false like that. I'm

2104
02:09:40,360 --> 02:09:42,760
just going to look at the other screen and make sure I didn't mess this up. Because

2105
02:09:42,760 --> 02:09:46,720
again, I don't remember the syntax. Yes, a cluster classifier dot evaluate test

2106
02:09:46,720 --> 02:09:50,520
test y looks good to me. We'll take this print statement just so we get a nice

2107
02:09:50,560 --> 02:09:55,280
output for our accuracy. Okay, so let's look at this. Again, we're going to have

2108
02:09:55,280 --> 02:09:59,040
to wait for this to train. But I will show you a way that we don't need to wait

2109
02:09:59,040 --> 02:10:03,840
for this to train every time in one second. And I'll be right back. Okay, so

2110
02:10:03,880 --> 02:10:06,800
what I'm actually going to do, and I've just kind of pause like the execution of

2111
02:10:06,800 --> 02:10:11,720
this code is throw this in the next block under, because the nice thing about

2112
02:10:11,720 --> 02:10:15,800
Google Collaboratory is that I can run this block of code, right, I can train

2113
02:10:15,800 --> 02:10:19,160
all this stuff, which is what I'll run now while we're talking just so it happens.

2114
02:10:19,440 --> 02:10:23,560
And then I can have another code block kind of below it, which I have here. And

2115
02:10:23,600 --> 02:10:26,800
it doesn't matter. I don't need to rerun that block every time I change

2116
02:10:26,800 --> 02:10:29,720
something here. So if I change something in any lower blocks, I don't need to

2117
02:10:29,720 --> 02:10:32,720
change the upper block, which means I don't need to wait for this to train every

2118
02:10:32,720 --> 02:10:36,600
time I want to do an evaluation on it. Anyways, so we've done this, we got test,

2119
02:10:36,600 --> 02:10:40,880
we got test why I just need to change this instead of eval result. Actually, I

2120
02:10:40,880 --> 02:10:46,600
need to say eval underscore result equals classifier dot evaluate so that we can

2121
02:10:46,600 --> 02:10:49,960
actually store this somewhere and get the answer. And now we'll print this and

2122
02:10:49,960 --> 02:10:54,520
notice this happens much, much faster. We get a test accuracy of 80%. So if I were

2123
02:10:54,520 --> 02:10:58,440
to retrain the model, chances are this accuracy would change again, because of

2124
02:10:58,440 --> 02:11:01,640
the order in which we're seeing different flowers. But this is pretty decent

2125
02:11:01,680 --> 02:11:06,000
considering we don't have that much test data. And we don't really know what

2126
02:11:06,000 --> 02:11:08,440
we're doing, right? We're kind of just messing around and experimenting for

2127
02:11:08,440 --> 02:11:12,120
right now. So to get 80% is pretty good. Okay, so actually, what am I doing? We

2128
02:11:12,120 --> 02:11:15,200
need to go back now and do predictions. So how am I going to predict this for

2129
02:11:15,200 --> 02:11:19,440
specific flowers? So let's go back to our core learning algorithms. And let's go

2130
02:11:19,440 --> 02:11:24,120
to predictions. Now, I've written a script already, just to save a bit of time

2131
02:11:24,120 --> 02:11:29,480
that allows us to do a prediction on any given flower. So what I'm going to do is

2132
02:11:29,520 --> 02:11:33,320
create a new block down here, code block and copy this function in. Now we're

2133
02:11:33,320 --> 02:11:36,400
going to digest this and kind of go through this on our own to make sure

2134
02:11:36,400 --> 02:11:40,720
this makes sense. But what this little script does is allow the user to type in

2135
02:11:40,720 --> 02:11:45,200
some numbers. So the septal length width, and I guess petal length and width, and

2136
02:11:45,200 --> 02:11:49,840
then it will spit out to you what the predicted class of that flower is. So we

2137
02:11:49,840 --> 02:11:54,200
could do a prediction on every single one of our data points like we did

2138
02:11:54,200 --> 02:11:56,800
previously. And we already know how to do that. I showed you that with linear

2139
02:11:56,840 --> 02:12:01,760
regression. But here I just wanted to do it on one entry. So what do we do? So I

2140
02:12:01,760 --> 02:12:06,360
start by creating a input function, it's very basic, we have batch size 256. All

2141
02:12:06,360 --> 02:12:10,280
we do is we give some features, and we created data set from those features

2142
02:12:10,320 --> 02:12:15,120
that's a dict and then dot batch and the batch size. So what this is doing is

2143
02:12:15,120 --> 02:12:19,080
notice we don't give any y value, right? We don't give any labels. The reason we

2144
02:12:19,080 --> 02:12:22,880
do we don't do that is because when we're making a prediction, we don't know

2145
02:12:22,880 --> 02:12:27,080
the label, right? Like we actually want that the model to give us the answer. So

2146
02:12:27,080 --> 02:12:30,480
here I wrote down the features, I created a predict dictionary, just because I'm

2147
02:12:30,480 --> 02:12:33,880
going to add things to it. And then I just prompted here with a print statement,

2148
02:12:33,880 --> 02:12:39,360
please type numeric values as prompted. So for feature and feature, valid equals

2149
02:12:39,360 --> 02:12:43,280
true, well valid, valid equals input feature colon. So this just means what

2150
02:12:43,280 --> 02:12:46,120
we're going to do is for each feature, we're going to wait to get some valid

2151
02:12:46,120 --> 02:12:50,960
response. Once we get some valid response, what we're going to do is add that to

2152
02:12:51,000 --> 02:12:55,000
our dictionary. So we're going to say predict feature. So whatever that feature

2153
02:12:55,000 --> 02:13:00,280
was, so septal length, septal width, petal length or pep, petal width is equal

2154
02:13:00,280 --> 02:13:05,560
to a list that has in this instance, whatever that value was. Now the reason

2155
02:13:05,560 --> 02:13:09,360
we need to do this is because again, the predict method from TensorFlow works

2156
02:13:09,360 --> 02:13:13,280
on predicting for multiple things, not just one value. So even if we only have

2157
02:13:13,280 --> 02:13:16,800
one value we want to predict for it, we need to put it inside of a list because

2158
02:13:16,800 --> 02:13:20,320
it's expecting the fact that we will probably have more than one value in

2159
02:13:20,360 --> 02:13:23,920
which we would have multiple values in the list, right, each representing a

2160
02:13:23,920 --> 02:13:28,320
different row or a new flower to make a prediction for. Okay, now we say

2161
02:13:28,320 --> 02:13:31,720
predictions equals classifier dot predict. And then in this case, we have input

2162
02:13:31,720 --> 02:13:36,240
function lambda input function predict, which is this input function up here. And

2163
02:13:36,240 --> 02:13:40,280
then we say for prediction dictionaries, because remember, every prediction comes

2164
02:13:40,280 --> 02:13:44,200
back as a dictionary in predictions, we'll say the class ID is equal to

2165
02:13:44,240 --> 02:13:49,160
whatever the class IDs of the prediction dictionary at zero. And these are

2166
02:13:49,160 --> 02:13:54,840
simply what I don't know exactly how to explain this. We'll look at in a second,

2167
02:13:54,840 --> 02:13:58,080
and I'll go through that. And then we have the probability is equal to the

2168
02:13:58,080 --> 02:14:03,640
prediction dictionary probabilities of class ID. Okay, then we're going to say

2169
02:14:03,640 --> 02:14:07,960
print prediction is we're going to do this weird format thing, I just stole

2170
02:14:07,960 --> 02:14:12,360
this from TensorFlow. And it's going to be the species at the class ID, and then

2171
02:14:12,360 --> 02:14:15,640
100 times probability, which will give us actual integer value, we're going to

2172
02:14:15,640 --> 02:14:18,600
digest this, but let's run this right now and have a look. So please type

2173
02:14:18,640 --> 02:14:24,160
numeric values as prompted septal length, let's type like 2.4, septal width 2.6,

2174
02:14:24,600 --> 02:14:30,320
petal width, let's just say that's like 6.5. And yeah, petal width like 6.3. Okay,

2175
02:14:30,360 --> 02:14:34,520
so then it calls this and it says prediction is virginica, I guess that's

2176
02:14:34,560 --> 02:14:39,840
the the class we're going with. And it says that's an 83 or 86.3% chance that

2177
02:14:39,840 --> 02:14:43,920
that is the prediction. So yeah, that is how that works. So that's what this

2178
02:14:43,920 --> 02:14:47,000
does. I wanted to give a little script, I wrote most of this, I mean, I stole

2179
02:14:47,040 --> 02:14:50,760
some of this from TensorFlow. But just to show you how we actually predict on

2180
02:14:50,760 --> 02:14:55,360
one value. So let's look at these prediction dictionary, because I just

2181
02:14:55,360 --> 02:14:59,080
want to show you what one of them actually is. So I'm going to say print,

2182
02:15:00,240 --> 02:15:03,760
pred underscore dict. And then this will allow me to actually walk through what

2183
02:15:03,760 --> 02:15:08,400
class IDs are probabilities are and how I've kind of done this. So let's run

2184
02:15:08,400 --> 02:15:13,640
this up to length. Okay, let's just go like 1.4, 2.3. I don't know what these

2185
02:15:13,640 --> 02:15:19,840
values are going to end up being. And we get prediction is same one with 77.1%,

2186
02:15:19,840 --> 02:15:23,680
which makes sense, because these values are similar kind of in difference to what

2187
02:15:23,680 --> 02:15:27,360
I did before. Okay, so this is the dictionary. So let's look for what we

2188
02:15:27,360 --> 02:15:31,480
were looking for. So probabilities notice we get three probabilities, one for

2189
02:15:31,520 --> 02:15:35,320
each of the different classes. So we can actually say what, you know, the

2190
02:15:35,880 --> 02:15:40,760
percentages for every single one of the predictions. Then what we have is class

2191
02:15:40,800 --> 02:15:46,920
IDs. Now class IDs, what this does is tell us what class ID, it predicts is

2192
02:15:46,960 --> 02:15:50,600
actually the flower, right? So here it says two, which means that this

2193
02:15:50,600 --> 02:15:55,000
probability at 77%. That's that index two in this array, right? So that's why

2194
02:15:55,000 --> 02:15:59,680
this value is two. So it's saying that that class is to it thinks it's class

2195
02:15:59,680 --> 02:16:05,280
two, like that's whatever was encoded in our system is two. And that's how that

2196
02:16:05,280 --> 02:16:10,400
works. So that's how I know, which one to print out is because this tells me

2197
02:16:10,400 --> 02:16:15,160
it's class two. And I know for making this list all the way back up here, if I

2198
02:16:15,160 --> 02:16:20,920
can get rid of this output. Where is it? When I say species, that number two is

2199
02:16:20,920 --> 02:16:25,200
virginica, or I guess that's how you say it. So that is what the classification

2200
02:16:25,200 --> 02:16:27,840
is. So that's what the prediction is. So that's how I do that. And that's how

2201
02:16:27,840 --> 02:16:33,840
that works. Okay, so I think that is pretty much it for actually classification. So

2202
02:16:33,840 --> 02:16:36,800
it was pretty basic. I'm going to go and see if there's anything else that I did

2203
02:16:36,840 --> 02:16:41,160
for classification in here. Okay, so here, I just put some examples. So here's

2204
02:16:41,160 --> 02:16:44,280
some example input and expected classes. So you guys could try to do these if you

2205
02:16:44,280 --> 02:16:51,920
want. So for example, this one, septal length, septal width. So for 5.1, 3.3, 1.7

2206
02:16:51,920 --> 02:16:58,280
and 0.5, the output should be Satosa. For 5.9, 3.0, 4.2, 1.5, it should be this

2207
02:16:58,280 --> 02:17:02,720
one. And then obviously this for this, just so you guys can mess with them if you

2208
02:17:02,720 --> 02:17:07,080
want. But that's pretty much it for classification. And now on to clustering.

2209
02:17:08,680 --> 02:17:13,160
Okay, so now we're moving on to clustering. Now clustering is the first unsupervised

2210
02:17:13,520 --> 02:17:17,600
learning algorithm that we're going to see in this series. And it's very powerful. Now

2211
02:17:17,600 --> 02:17:22,720
clustering only works for a very specific set of problems. And you use clustering

2212
02:17:22,720 --> 02:17:26,760
when you have a bunch of input information or features, you don't have any labels or

2213
02:17:26,760 --> 02:17:39,440
open information. Essentially, what clustering does is finds clusters of like data points

2214
02:17:39,640 --> 02:17:43,800
and tells you the location of those clusters. So you give a bunch of training data, you

2215
02:17:43,800 --> 02:17:48,200
can pick how many clusters you want to find. So maybe we're going to be classifying digits

2216
02:17:48,200 --> 02:17:52,080
right handwritten digits using k means clustering. In that instance, we would have 10

2217
02:17:52,080 --> 02:17:56,760
different clusters for the digits zero through nine. And you pass all this information and

2218
02:17:56,760 --> 02:18:01,160
the algorithm actually finds those clusters in the data set for you. We're going to walk

2219
02:18:01,160 --> 02:18:05,520
through an example that'll make sense. But I just want to quickly explain the basic

2220
02:18:05,560 --> 02:18:10,600
algorithm behind k means essentially the set of steps. These are going to walk you through

2221
02:18:10,600 --> 02:18:16,040
them and with a visual example. So we're going to start by randomly picking k points to

2222
02:18:16,040 --> 02:18:21,720
place k centroids. Now a centroid stands for where our current cluster is kind of

2223
02:18:21,760 --> 02:18:25,880
defined. And we'll see it in a second. The next step is we're going to assign all of the

2224
02:18:25,880 --> 02:18:30,640
data points to the centroids by distance. So actually, now that I'm talking about this,

2225
02:18:30,640 --> 02:18:33,920
I think it just makes more sense to get right into the example, because if I keep talking

2226
02:18:33,920 --> 02:18:36,480
about this, you guys are probably just going to be confused, although I might come back

2227
02:18:36,480 --> 02:18:41,760
to this just to reference those points. Okay, so let's create a little graph like this in

2228
02:18:41,760 --> 02:18:46,240
two dimensions for our basic example. And let's make some data points here. So I'm just

2229
02:18:46,240 --> 02:18:49,520
going to make them all read. And you're going to notice that I'm going to make this kind

2230
02:18:49,520 --> 02:18:53,960
of easier for ourselves by putting them in like their own unique little groups, right?

2231
02:18:53,960 --> 02:19:00,120
So actually, we'll add one up here. Then we can add some down here and down here. Now

2232
02:19:00,120 --> 02:19:04,600
the algorithm starts for k means clustering. And you guys understand how this works as

2233
02:19:04,600 --> 02:19:11,040
we continue by randomly picking k centroids. Now I'm going to denote a centroid by a little

2234
02:19:11,040 --> 02:19:17,320
filled in triangle like this. And essentially what these are is where these different clusters

2235
02:19:17,360 --> 02:19:22,280
currently exist. So we start by randomly picking k, which is what we've defined. So like me

2236
02:19:22,280 --> 02:19:27,360
in this instance, that we're going to say k equals three, k centroid, wherever. So maybe

2237
02:19:27,360 --> 02:19:31,680
we put one, you know, somewhere like here, you know, I might not bother filling these

2238
02:19:31,680 --> 02:19:35,480
in because they're going to take a while. Maybe we pull in here, maybe we end up putting

2239
02:19:35,480 --> 02:19:40,920
one over here. Now, I've kind of put them close to where clusters are, but these are

2240
02:19:40,920 --> 02:19:47,080
going to be completely random. Now what happens next is each group, or each data point is

2241
02:19:47,080 --> 02:19:52,520
assigned to a cluster by distance. So essentially, what we do is for every single data point

2242
02:19:52,520 --> 02:19:56,640
that we have, we find what's known as the Euclidean distance, or it actually could be

2243
02:19:56,640 --> 02:20:00,760
a different distance, you'd use like Manhattan distance, if you guys know what that is, to

2244
02:20:00,760 --> 02:20:04,720
all of the centroids. So let's say we're looking at this data point here, what we do is find

2245
02:20:04,720 --> 02:20:10,800
the distance to all of these different centroids. And we assign this data point to the closest

2246
02:20:10,800 --> 02:20:14,680
centroid. So the closest one by distance. Now in this instance is looking like it's

2247
02:20:14,680 --> 02:20:18,000
going to be a bit of a tie between this centroid and this centroid. But I'm going to give it

2248
02:20:18,000 --> 02:20:23,720
to the one on the left. So what we do is we're going to say this is now a part of this centroid.

2249
02:20:23,720 --> 02:20:28,000
So if I'm calling this like, let's just say this is centroid one, this is centroid two,

2250
02:20:28,000 --> 02:20:31,800
and this is centroid three, then this now is going to be a part of centroid one, because

2251
02:20:31,800 --> 02:20:35,640
it's closest to centroid one. And we can go through and we do this for every single data

2252
02:20:35,640 --> 02:20:40,120
point. So obviously, we know all of these are going to be our ones, right? And we know

2253
02:20:40,160 --> 02:20:44,360
these are going to be our two. So two, two, two. And then these are obviously going to

2254
02:20:44,360 --> 02:20:48,840
be our three. Now I'm actually just going to add a few other data points, because I want

2255
02:20:48,840 --> 02:20:55,000
to make this a little bit more sophisticated, almost, if that makes any sense. So add those

2256
02:20:55,000 --> 02:20:58,920
data points here, we've been add one here, and that will give these labels. So these

2257
02:20:58,920 --> 02:21:02,080
ones are close. So I'm going to say this one's one, I'm going to say this one's two, I know

2258
02:21:02,080 --> 02:21:06,200
it's not closest to it. But just because I want to do that for now, we'll say two for

2259
02:21:06,200 --> 02:21:10,120
that. And we'll say three here. Okay, so now that we've done that, we've labeled all these

2260
02:21:10,120 --> 02:21:15,760
points, what we do is we now move these centroids that we've defined into the middle of all

2261
02:21:15,760 --> 02:21:21,800
of their data points. So what I do is I essentially find it's called center of mass, the center

2262
02:21:21,800 --> 02:21:26,120
of mass between all of the data points that are labeled the same. So in this case, these

2263
02:21:26,120 --> 02:21:29,360
will be all the ones that are labeled the same. And I take this centroid, which I'm

2264
02:21:29,360 --> 02:21:33,640
going to have to erase, get rid of it here. And I put it right in the middle. So let's

2265
02:21:33,680 --> 02:21:38,080
go back to blue. And let's say the middle of these data points ends up being somewhere

2266
02:21:38,080 --> 02:21:42,920
around here. So we put it in here. And this is what we call center of mass. And this again,

2267
02:21:42,920 --> 02:21:47,840
it'd be centroid two. So let's just erase this. And there we go. Now we do the same

2268
02:21:47,840 --> 02:21:53,120
thing with the other centroid. So let's remove these ones, to remove these ones. So for three,

2269
02:21:53,120 --> 02:21:58,680
I'm saying it's probably going to be somewhere in here. And then for one, our center of mass

2270
02:21:58,760 --> 02:22:05,760
is probably going to be located somewhere about here. Now what I do is I repeat the process

2271
02:22:05,760 --> 02:22:10,760
that I just did. And I reassign all the points now to the closest centroid. So all these

2272
02:22:10,760 --> 02:22:14,960
points are labeled one to all that, you know, we can kind of remove their labels. And this

2273
02:22:14,960 --> 02:22:19,440
is just going to be great. Me trying to erase the labels, I shouldn't have wrote them on

2274
02:22:19,440 --> 02:22:23,040
top. But essentially, what we do is we're just going to be like reassigning them. So

2275
02:22:23,040 --> 02:22:26,440
I'm going to say, okay, so this is two, and we just do the same thing as before, find

2276
02:22:26,440 --> 02:22:30,600
the closest distance. So we'll say, you know, these can stay in the same cluster, maybe

2277
02:22:30,600 --> 02:22:36,280
this one actually here gets changed to one now, because it's closest to centroid one.

2278
02:22:36,280 --> 02:22:39,440
And we just reassigned all these points. And maybe, you know, this one now, if it was two

2279
02:22:39,440 --> 02:22:44,200
before, let's say like this one's one, and we just reassigned them. Now we repeat this

2280
02:22:44,200 --> 02:22:50,920
process of finding the closest or assigning all the points that are closest centroid,

2281
02:22:50,920 --> 02:22:55,520
moving the centroid into the center of mass, and we keep doing this until eventually we

2282
02:22:55,600 --> 02:23:00,320
reach a point where none of these points are changing which centroid they're a part of.

2283
02:23:00,320 --> 02:23:04,000
So eventually we reach a point where I'm just going to erase this and draw like a new graph

2284
02:23:04,000 --> 02:23:07,320
because it'll be a little bit cleaner. But what we have is, you know, like a bunch of

2285
02:23:07,320 --> 02:23:13,080
data points. So we have some over here, some over here, maybe we'll just put some here,

2286
02:23:13,080 --> 02:23:17,440
and maybe we'll do like a K equals four example for this one. And we have all these centroids

2287
02:23:17,440 --> 02:23:21,760
and I'll just draw these centroids with blue again, that are directly in the middle of

2288
02:23:21,760 --> 02:23:25,240
all of their data points, they're like as in the middle as they can get, none of our

2289
02:23:25,280 --> 02:23:30,920
data points have moved. And we call this now our cluster. So now we have these clusters,

2290
02:23:30,920 --> 02:23:34,280
we have these centroids, right, we know where they are. And what we do is when we have a

2291
02:23:34,280 --> 02:23:38,720
new data point that we want to make a prediction for or figure out what cluster it's a part

2292
02:23:38,720 --> 02:23:43,360
of, what we do is we will plot that data point. So let's say it's this new data point here,

2293
02:23:43,360 --> 02:23:48,200
we find the distance to all of the clusters that exist. And then we assign it to the closest

2294
02:23:48,200 --> 02:23:52,360
one. So obviously it would be assigned to that one. And we can do this for any data point,

2295
02:23:52,360 --> 02:23:56,560
right? So even if I put a data point all the way over here, well, it's closest cluster

2296
02:23:56,560 --> 02:24:02,160
is this, so it gets assigned to this cluster. And my output will be whatever this label

2297
02:24:02,160 --> 02:24:06,200
of this cluster is. And that's essentially how this works, you're just clustering data

2298
02:24:06,200 --> 02:24:09,680
points, figuring out which ones are similar. And this is a pretty basic algorithm, I mean,

2299
02:24:09,680 --> 02:24:13,040
you draw your little triangle, you find the distance from every point of the triangle,

2300
02:24:13,040 --> 02:24:17,480
or to all of the triangles, actually. And then what you do is just simply assign those

2301
02:24:17,480 --> 02:24:21,840
values to that centroid, you move that centroid to the center of mass, and you repeat this

2302
02:24:21,860 --> 02:24:25,760
process constantly, until eventually you get to a point where none of your data points

2303
02:24:25,760 --> 02:24:30,240
are moving. That means you found the best clusters that you can, essentially. Now, the

2304
02:24:30,240 --> 02:24:34,440
only thing with this is you do need to know how many clusters you want for k means clustering,

2305
02:24:34,440 --> 02:24:38,280
because k is a variable that you need to define. Although there is some algorithms that can

2306
02:24:38,280 --> 02:24:42,600
actually determine the best amount of clusters for a specific data set. But that's a little

2307
02:24:42,600 --> 02:24:46,120
bit beyond what we're going to be focused on focusing on right now. So that is pretty

2308
02:24:46,120 --> 02:24:50,280
much clustering. There's not really much more to talk about it, especially because we can't

2309
02:24:50,280 --> 02:24:55,720
really code anything for it now. So we're going to move on to hidden Markov models. Now

2310
02:24:55,720 --> 02:24:59,920
hidden Markov models are way different than what we've seen so far, we've been using kind

2311
02:24:59,920 --> 02:25:04,560
of algorithms that rely on data. So like k means clustering, we gave a lot of data,

2312
02:25:04,560 --> 02:25:08,800
and we know clustered all those data points found those centroids, use those centroids

2313
02:25:08,800 --> 02:25:14,000
to find where new data points should be. Same thing with linear regression and classification.

2314
02:25:14,000 --> 02:25:18,680
Whereas hidden Markov models, we actually deal with probability distributions. Now,

2315
02:25:18,680 --> 02:25:21,800
the example we're going to go into here, and it's kind of I have to do a lot of examples

2316
02:25:21,800 --> 02:25:27,080
for this because it's a very abstract concept is a basic weather model. So what we actually

2317
02:25:27,080 --> 02:25:34,320
want to do is predict the weather on any given day, given the probability of different events

2318
02:25:34,320 --> 02:25:38,640
occurring. So let's say we know, you know, maybe in like a simulated environment or something

2319
02:25:38,640 --> 02:25:44,560
like that, this might be an application, that we have some specific things about our environment,

2320
02:25:44,640 --> 02:25:48,920
we know if it's sunny, there's an 80% chance that the next day, it's going to be sunny

2321
02:25:48,920 --> 02:25:53,320
again, and a 20% chance that it's going to rain. Maybe we know some information about

2322
02:25:53,320 --> 02:25:57,780
sunny days and about cold days. And we also know some information about the average temperature

2323
02:25:57,780 --> 02:26:03,160
on those days. Using this information, we can create a hidden Markov model that will

2324
02:26:03,160 --> 02:26:08,600
allow us to make a prediction for the weather in future days, given kind of that probability

2325
02:26:08,600 --> 02:26:12,360
that we've discovered. Now you might be like, Well, how do we know this? Like how do I know

2326
02:26:12,400 --> 02:26:16,840
this probability? A lot of the times you actually do know the probability of certain events

2327
02:26:16,840 --> 02:26:20,480
occurring or certain things happening, which makes these models really good. But there's

2328
02:26:20,480 --> 02:26:24,600
some times where what you actually do is you have a huge data set, and you calculate the

2329
02:26:24,600 --> 02:26:29,240
probability of things occurring based on that data set. So we're not going to do that part

2330
02:26:29,240 --> 02:26:31,760
because that's just kind of going a little bit too far. And the whole point of this is

2331
02:26:31,760 --> 02:26:36,200
just to introduce you to some different models. But in this example, what we will do is use

2332
02:26:36,200 --> 02:26:40,720
some predefined probability distributions. So let me just read out the exact definition

2333
02:26:40,720 --> 02:26:44,200
of a hidden Markov model will start going more in depth. So the hidden Markov model

2334
02:26:44,200 --> 02:26:48,640
is a finite set of states, each of which is associated with a generally multi dimensional

2335
02:26:48,640 --> 02:26:53,160
probability distribution, transitions among the states are governed by a set of probabilities

2336
02:26:53,160 --> 02:26:59,040
called transition probabilities. So in a hidden Markov model, we have a bunch of states. Now

2337
02:26:59,040 --> 02:27:02,200
in the example that I was just talking about with this weather model, the states we would

2338
02:27:02,200 --> 02:27:09,520
have is hot day and cold day. Now, these are what we call hidden, because never do we actually

2339
02:27:09,560 --> 02:27:14,880
access or look at these states, while we interact with the model. In fact, what we look at is

2340
02:27:14,880 --> 02:27:19,320
something called observations. Now at each state, we have an observation, I'll give you

2341
02:27:19,320 --> 02:27:25,200
an example of an observation. If it is hot outside, Tim has an 80% chance of being happy.

2342
02:27:25,200 --> 02:27:30,600
If it is cold outside, Tim has a 20% chance of being happy. That is an observation. So

2343
02:27:30,600 --> 02:27:36,720
at that state, we can observe the probability of something happening during that state is

2344
02:27:36,760 --> 02:27:42,800
x, right, or is y or whatever it is. So we don't actually care about the states in particular,

2345
02:27:42,800 --> 02:27:46,440
we care about the observations we get from that state. Now in our example, what we're

2346
02:27:46,440 --> 02:27:50,880
actually going to do is we're going to look at the weather as an observation for the state. So

2347
02:27:50,880 --> 02:27:55,760
for example, on a sunny day, the weather has, you know, the probability of being between

2348
02:27:55,760 --> 02:28:00,840
five and 15 degrees Celsius, with an average temperature of 11 degrees. That's like, that's

2349
02:28:00,840 --> 02:28:05,720
a probability we can use. Now I know this is slightly abstract, but I just want to talk

2350
02:28:05,720 --> 02:28:09,440
about the data we're going to work with here. I'm going to draw out a little example, go

2351
02:28:09,440 --> 02:28:12,840
through it and then we'll actually get into the code. So let's start by discussing the

2352
02:28:12,840 --> 02:28:17,120
type of data we're going to use. So typically in previous ones, right, we use like hundreds,

2353
02:28:17,120 --> 02:28:22,240
if not like thousands of entries or rows or data points for our models to train. For this,

2354
02:28:22,240 --> 02:28:26,960
we don't need any of that. In fact, all we need is just constant values for probability

2355
02:28:26,960 --> 02:28:32,280
and or what is it transition distributions and observation distributions. Now what I'm

2356
02:28:32,320 --> 02:28:36,720
going to do is go in here and talk about states observations and transitions. So we have a

2357
02:28:36,720 --> 02:28:42,000
certain amount of states. Now we will define how many states we have, we don't really care

2358
02:28:42,000 --> 02:28:47,280
what that state is. So we could have states, for example, like warm, cold, high, low, red,

2359
02:28:47,280 --> 02:28:51,480
green, blue, you have as many states as we want, we could have one state to be honest,

2360
02:28:51,480 --> 02:28:54,760
although that would be kind of strange to have that. And these are called hidden because

2361
02:28:54,760 --> 02:28:59,640
we don't directly observe. Now observations. So each state has a particular outcome or

2362
02:28:59,720 --> 02:29:04,200
observation associated with it based on a probability distribution. So it could be the

2363
02:29:04,200 --> 02:29:10,440
fact that during a hot day, it is 100% true that Tim is happy. Although, in a hot day, we

2364
02:29:10,440 --> 02:29:16,080
could observe that 80% of the time Tim is happy. And 20% of the time, he is sad, right? Those

2365
02:29:16,080 --> 02:29:20,640
are observations we make about each state. And each state will have their different observations

2366
02:29:20,640 --> 02:29:26,360
and different probabilities of those observations occurring. So if we were just going to have

2367
02:29:26,440 --> 02:29:30,400
like an outcome for the state, that means it's always the same, there's no probability that

2368
02:29:30,400 --> 02:29:34,800
something happens. And in that case, that's just called an outcome, because the probability

2369
02:29:34,800 --> 02:29:40,600
of the event occurring will be 100%. Okay, then we have transitions. So each state will

2370
02:29:40,600 --> 02:29:44,880
have a probability defining the likelihood of transitioning to a different state. So

2371
02:29:44,880 --> 02:29:49,840
for example, if we have a hot day, there will be a percentage chance the next day will be

2372
02:29:49,840 --> 02:29:53,160
a cold day. And if we have a cold day, there will be a percentage chance that the next

2373
02:29:53,160 --> 02:29:57,680
day is either a hot day or a cold day. So we're going to go through like the exact what

2374
02:29:57,680 --> 02:30:02,160
we have for our specific model below. But just understand there's a probability that

2375
02:30:02,160 --> 02:30:06,400
we could transition into a different state. And from each state, we can transition into

2376
02:30:06,400 --> 02:30:12,280
every other state or a defined set of states given a certain probability. So I know it's

2377
02:30:12,280 --> 02:30:17,360
a mouthful, I know it's a lot. But let's go into a basic drawing example, because I just

2378
02:30:17,360 --> 02:30:21,320
want to illustrate like graphically a little bit kind of how this works. In case these

2379
02:30:21,360 --> 02:30:26,120
are ideas are a little bit too abstract for any of you. Okay, I'm just pulling out the

2380
02:30:26,120 --> 02:30:32,040
drawing tablet, just one second here, and let's do this basic weather model. So what

2381
02:30:32,040 --> 02:30:36,840
I'm going to do is just simply draw two states. Actually, let's do it with some colors because

2382
02:30:36,840 --> 02:30:40,960
why not? So we're going to use yellow. And this is going to be our hot day. Okay, this

2383
02:30:40,960 --> 02:30:45,640
is going to be our sun. And then I'm just going to make a cloud. We'll just do like

2384
02:30:45,640 --> 02:30:50,520
a gray cloud. This will be my cloud. And we'll just say it's going to be raining over here.

2385
02:30:50,520 --> 02:30:55,360
Okay, so these are my two states. Now, in each state, there's a probability of transitioning

2386
02:30:55,360 --> 02:31:02,440
to the other state. So for example, in a hot day, we have a let's say 20% chance of transitioning

2387
02:31:02,440 --> 02:31:08,720
to a cold day. And we have a 80% chance of transitioning to another hot day, like the

2388
02:31:08,720 --> 02:31:14,480
next day, right? Now in a cold day, we have let's say a 30% chance of transitioning to

2389
02:31:14,520 --> 02:31:20,760
a hot day. And we have in this case, what is that going to be a 70% chance of transitioning

2390
02:31:20,760 --> 02:31:25,360
to another cold day. Now, on each of these days, we have a list of observations. So these

2391
02:31:25,360 --> 02:31:29,280
are what we call states, right? So this could be s one, and this could be s two, it doesn't

2392
02:31:29,280 --> 02:31:32,960
really matter, like if we name them or anything, we just we have two states, that's what we

2393
02:31:32,960 --> 02:31:37,600
know. We know the transition probability, that's what we've just defined. Now we want

2394
02:31:37,600 --> 02:31:42,880
the observation probability or distribution for that. So essentially, on a hot day, our

2395
02:31:42,920 --> 02:31:50,000
observation is going to be that the temperature could be between 15 and 25 degrees Celsius

2396
02:31:50,000 --> 02:31:57,600
with an average temperature of let's say 20. So we can say observation, right? So say observation,

2397
02:31:57,600 --> 02:32:03,280
and we'll say that the mean so the average temperature is going to be 20. And then the

2398
02:32:03,280 --> 02:32:09,200
distribution for that will be like the minimum value is going to be 15. And the max is going

2399
02:32:09,200 --> 02:32:14,120
to be 25. So this is what we call actually like a standard deviation. I'm not really

2400
02:32:14,120 --> 02:32:17,200
going to explain exactly what standard deviation is, although you can kind of think of it as

2401
02:32:17,200 --> 02:32:22,560
something like this. So essentially, there's a mean, which is the middle point, the most

2402
02:32:22,560 --> 02:32:27,480
common event that could occur. And at different levels of standard deviation, which is going

2403
02:32:27,480 --> 02:32:30,640
into statistics, which I don't really want to mention that much, because I'm definitely

2404
02:32:30,640 --> 02:32:35,760
non expert. We have a probability of hitting different temperatures as we move to the left

2405
02:32:35,800 --> 02:32:41,160
and right of this value. So on this curve somewhere, we have 15. And on this curve to the

2406
02:32:41,160 --> 02:32:44,880
right somewhere, we have 25. Now, we're just defining the fact that this is where we're

2407
02:32:44,880 --> 02:32:49,960
going to kind of end our curve. So we're going to say that like the probability is in between

2408
02:32:49,960 --> 02:32:54,280
these numbers, it's going to be in between 15 and 25 with an average of 20. And then our

2409
02:32:54,280 --> 02:32:58,760
model will kind of figure out some things to do with that. That's as far as I really

2410
02:32:58,760 --> 02:33:03,320
want to go in standard deviation. And I'm sure that's like a really horrible explanation.

2411
02:33:03,360 --> 02:33:06,040
That's kind of the best I'm going to give you guys for right now. Okay, so that's our

2412
02:33:06,040 --> 02:33:09,360
observation here. Now our observation over here is going to be similar. So we're going

2413
02:33:09,360 --> 02:33:14,160
to say mean on a cold day, temperature is going to be five degrees. We'll say the minimum

2414
02:33:14,160 --> 02:33:18,480
temperature maybe is going to be something like negative five and the max could be something

2415
02:33:18,480 --> 02:33:24,760
like 15 or like, yeah, we can say 15. So we'll have some distribution, not just what we want

2416
02:33:24,760 --> 02:33:28,720
to understand, right? And this is kind of a strange distribution because we're dealing

2417
02:33:28,760 --> 02:33:34,080
with what is it standard deviation, although we can just deal with like straight percentage

2418
02:33:34,080 --> 02:33:38,480
observations. So for example, you know, there's a 20% chance that Tim is happy, or there's an

2419
02:33:38,480 --> 02:33:44,800
80% chance that he is sad, like those are probabilities that we can have as our observation

2420
02:33:44,800 --> 02:33:49,840
probabilities in the model. Okay, so there's a lot of lingo. There's a lot going on, we're

2421
02:33:49,840 --> 02:33:53,560
going to get into like a concrete example now. So hopefully this should make more sense. But

2422
02:33:53,560 --> 02:33:57,680
again, just understand states, transitions, observations, we don't actually ever look at

2423
02:33:57,680 --> 02:34:01,800
the states, we just have to know how many we have, and the transition probability and

2424
02:34:01,800 --> 02:34:07,880
observation probability in each of them. Okay, so what I want to say now, though, is what

2425
02:34:07,880 --> 02:34:11,960
do we even do with this model? So once I make this right, once I make this hidden markup

2426
02:34:11,960 --> 02:34:15,560
model, what's the point of it? Well, the point of it is to predict future events based on

2427
02:34:15,560 --> 02:34:20,320
past events. So we know that probability distribution. And I want to predict the weather for the

2428
02:34:20,320 --> 02:34:24,440
next week, I can use that model to do that, because I can say, well, if the current day

2429
02:34:24,520 --> 02:34:29,000
today is warm, then what is the likelihood that the next day tomorrow is going to be

2430
02:34:29,000 --> 02:34:33,480
cold, right? And that's what we're kind of doing with this model, we're making predictions

2431
02:34:33,480 --> 02:34:40,160
for the future based on probability of past events occur. Okay, so imports and so let's

2432
02:34:40,160 --> 02:34:46,000
just run this already loaded import tensorflow. And notice that here I've imported tensorflow

2433
02:34:46,000 --> 02:34:52,680
probability is TFP. This is because this is a separate module from TensorFlow that

2434
02:34:52,720 --> 02:34:58,200
deals with probability. Now, we also need tensorflow two. But for this hidden markup

2435
02:34:58,200 --> 02:35:02,560
model, we're going to use the tensorflow probability module, not a huge deal. Okay, so

2436
02:35:02,560 --> 02:35:06,960
weather model. So this is just going to define what our model actually is so the different

2437
02:35:06,960 --> 02:35:11,440
parts of it. So this is taken directly from the documentation of tensorflow. You guys can

2438
02:35:11,440 --> 02:35:15,280
see you know, where I have all this information from like I've sourced all of it. But essentially

2439
02:35:15,280 --> 02:35:18,480
what the model we're going to try to create is that cold days are encoded by zero and

2440
02:35:18,520 --> 02:35:23,200
hot days are encoded by one. The first day in our sequence has an 80% chance of being

2441
02:35:23,200 --> 02:35:26,680
cold. So whatever day we're starting out at has an 80% chance of being cold, which would

2442
02:35:26,680 --> 02:35:32,000
mean 20% chance of being warm. A cold day has a 30% chance of being followed by hot day.

2443
02:35:32,000 --> 02:35:35,360
And a hot day has a 20% chance of being followed by a cold day, which would mean you know,

2444
02:35:35,360 --> 02:35:40,880
70% cold to cold and 80% hot to hot. On each day, the temperature is normally distributed

2445
02:35:40,880 --> 02:35:45,120
with mean and standard deviation zero and five on a cold day and mean and standard deviation

2446
02:35:45,160 --> 02:35:49,760
15 and 10 on a hot day. Now what that means standard deviation is essentially I mean, we

2447
02:35:49,760 --> 02:35:54,520
can read this thing here is that on a hot day, the average temperature is 15 that's mean

2448
02:35:54,520 --> 02:35:59,840
and ranges from five to 25 because the standard deviation is 10 of that, which just means 10

2449
02:35:59,840 --> 02:36:04,680
on each side kind of the min max value. Again, I'm not in statistics. So please don't quote

2450
02:36:04,680 --> 02:36:08,480
me on any definitions of standard deviation. I just trying to explain it enough so that

2451
02:36:08,480 --> 02:36:13,200
you guys can understand what we're doing. Okay, so what we're going to do to model this

2452
02:36:13,200 --> 02:36:17,160
and I'm just kind of going through this fairly quickly because it's pretty easy to really do

2453
02:36:17,160 --> 02:36:23,920
this is I'm going to load the TensorFlow probability distributions kind of module and just save

2454
02:36:23,920 --> 02:36:29,400
that as TFD. And I'm just going to do that so I don't need to write TFP dot distributions dot

2455
02:36:29,400 --> 02:36:33,400
all of this, I can just kind of shortcut it. So you'll notice I'm referencing TFD here,

2456
02:36:33,400 --> 02:36:38,560
which just stands for TFP dot distributions and TFP is TensorFlow probability. Okay,

2457
02:36:38,720 --> 02:36:45,200
so my initial distribution is TensorFlow probability distributions dot categorical. This

2458
02:36:45,200 --> 02:36:50,560
is probability of 80% and 20%. Now this refers to point two. So let's look at point two. The

2459
02:36:50,560 --> 02:36:55,040
first day in our sequence has an 80% chance of being cold. So we're saying that that's

2460
02:36:55,040 --> 02:37:00,640
essentially what this is the initial distribution of being cold is 80%. And then 20% after categorical

2461
02:37:00,640 --> 02:37:06,440
is just a way that we can do this distribution. Okay, so transition distribution. What is

2462
02:37:06,440 --> 02:37:14,520
it TensorFlow probability categorical, the probability is 70% and 30% and 20% 80%. Now

2463
02:37:14,520 --> 02:37:19,880
notice that since we have two states, we've defined two probabilities. Notice since we have two

2464
02:37:19,880 --> 02:37:24,600
states, we have defined two probabilities, the probability of landing on each of these states

2465
02:37:24,600 --> 02:37:28,920
at the very beginning of our sequence. This is the transition probability referred to points three

2466
02:37:28,920 --> 02:37:36,120
and four above. So this is what we have here. So cold is 30% chance 20% chance for a hot day. And

2467
02:37:36,120 --> 02:37:40,680
that's what we've defined. So we say this is going to be cold day state one, we have 70% chance

2468
02:37:40,680 --> 02:37:44,840
of being cold day again, we have 30% chance of going hot day and then you know, reverse here.

2469
02:37:45,720 --> 02:37:51,400
Okay, so observation distribution. Now this one is a little bit different, but essentially we do

2470
02:37:51,400 --> 02:37:56,680
tfd dot normal. Now I don't know, I'm not going to explain exactly what all this is, but when you're

2471
02:37:56,680 --> 02:38:00,520
doing standard deviation, you're going to do it like this, where you're going to say, look, which

2472
02:38:00,520 --> 02:38:05,000
stands for your average or your mean, right? So that was our average temperature is going to be

2473
02:38:05,000 --> 02:38:10,600
zero on a hot day, 15 on a cold day. The standard deviation on the cold days five, which means we

2474
02:38:10,600 --> 02:38:17,400
range from five, or negative five to five degrees. And on a hot day, it's 10. So that is going to be

2475
02:38:17,400 --> 02:38:22,360
we go range from five to 25 degrees, and our average temperature is 15. Now the reason we've

2476
02:38:22,360 --> 02:38:27,240
added dot here is because these just need to be float values. So rather than inserting integers

2477
02:38:27,240 --> 02:38:32,680
here and having potentially type errors later on, we just have floats. Okay, so the low argument

2478
02:38:32,680 --> 02:38:35,800
represents the mean and the scales of standard deviation. Yeah, exactly what we just defined

2479
02:38:35,800 --> 02:38:40,520
there. Alright, so let's run this, I think we actually already did. And now we can create our

2480
02:38:40,520 --> 02:38:44,760
model. So to create the models pretty easy. I mean, all we do is say model equals TensorFlow

2481
02:38:44,760 --> 02:38:48,920
distribution dot hidden Markov model, give it the initial distribution, which is equal to

2482
02:38:48,920 --> 02:38:53,960
initial distribution, transition distribution, observation, distribution and steps. Now what

2483
02:38:53,960 --> 02:38:59,720
is steps? Well, steps is how many days we want to predict for. So the number of steps is how many

2484
02:38:59,720 --> 02:39:05,240
times we're going to step through this probability cycle, and run the model essentially. Now remember,

2485
02:39:05,240 --> 02:39:09,240
what we want to do is we want to predict the average temperature on each day, right? Like

2486
02:39:09,240 --> 02:39:13,640
that's what the goal of our example is is to predict the average temperature. So given this

2487
02:39:13,640 --> 02:39:19,240
information, using these observations and using these transitions, what we'll do is predict that.

2488
02:39:19,240 --> 02:39:25,240
So I'm going to run this model. What is the issue here? tensor is on hash of tensor is

2489
02:39:25,400 --> 02:39:29,480
okay, give me one sec, I'll have a look here, though I haven't had this issue before. Okay,

2490
02:39:29,480 --> 02:39:34,520
so after a painful amount of searching on stack overflow and Google and actually just reading

2491
02:39:34,520 --> 02:39:39,320
through more documentation on TensorFlow, I have determined the issue. So remember the error was

2492
02:39:39,320 --> 02:39:43,640
we were getting on actually this line here, I think I can see what the output is. Oh, this

2493
02:39:43,640 --> 02:39:47,000
okay, well, this is a different error. But anyways, there was an error at this line. Essentially,

2494
02:39:47,000 --> 02:39:51,800
what was happening is we have a mismatch between the two versions here. So the most recent version

2495
02:39:52,440 --> 02:39:58,120
of TensorFlow is not compatible with the older version of TensorFlow probability, at least in

2496
02:39:58,120 --> 02:40:01,720
the sense that the things that we're trying to do with it. So I just need to make sure that I

2497
02:40:01,720 --> 02:40:07,160
installed the most recent version of TensorFlow probability. So what you need to do if this

2498
02:40:07,160 --> 02:40:11,960
is in your notebook, and this should actually work fine for you guys, because this will be updated

2499
02:40:11,960 --> 02:40:15,960
by the time you get there. But in case you run into the issue, I'll, you know, deal with it.

2500
02:40:15,960 --> 02:40:20,360
But essentially, we're going to select version 2.x of TensorFlow, you're going to run this

2501
02:40:20,440 --> 02:40:25,160
install commands, you're going to install TensorFlow probability, just run this command.

2502
02:40:25,160 --> 02:40:30,120
Then after you run this command, you're going to need to restart your run times, go to run time,

2503
02:40:30,120 --> 02:40:34,280
and then restart run time. And then you can just continue on with the script, select TensorFlow

2504
02:40:34,280 --> 02:40:39,000
2.x again, do your imports, and then you know, we'll test if this is actually going to work for

2505
02:40:39,000 --> 02:40:44,840
us here, run our distributions, create the model without any issues this time, notice no red text,

2506
02:40:44,840 --> 02:40:49,640
and then run this final line, which will give you the output. Now, this is what I wanted to

2507
02:40:49,640 --> 02:40:53,880
talk about here that we didn't quite get to because we were having some bugs. But this is how we

2508
02:40:53,880 --> 02:40:59,480
can actually kind of run our model and see the output. So what you can do is do model dot mean,

2509
02:40:59,480 --> 02:41:04,680
so you say mean equals model dot mean. And what this is going to do is essentially just calculate

2510
02:41:04,680 --> 02:41:10,680
the probability is going to essentially take that from the model. Now, when we have model dot mean,

2511
02:41:10,680 --> 02:41:15,160
this is what we call, you know, a partially defined tensor. So remember our tensors were like

2512
02:41:15,240 --> 02:41:19,720
partially defined computations. Well, that's what model dot mean actually is. That's what

2513
02:41:19,720 --> 02:41:25,240
this method is. So if we want to get the value of that, what we actually need to do is create a

2514
02:41:25,240 --> 02:41:30,440
new session in TensorFlow, run this part of the graph, which we're going to get by doing mean

2515
02:41:30,440 --> 02:41:34,520
dot numpy, and then we can print that out. So I know this might seem a little bit confusing,

2516
02:41:34,520 --> 02:41:40,040
but essentially to run a session in the new version of TensorFlow, so 2.x, or 2.1 or whatever

2517
02:41:40,040 --> 02:41:47,400
it is, you're going to type with TF dot compact dot v one dot session as sesh. And then I mean,

2518
02:41:47,400 --> 02:41:50,760
this doesn't really matter what you have here, but whatever you want. And then what I'm doing is

2519
02:41:50,760 --> 02:41:55,480
just printing mean dot numpy. So to actually get the value from this here, this variable,

2520
02:41:55,480 --> 02:42:00,280
I call dot numpy. And then what it does is print out this array that gives me the expected

2521
02:42:00,280 --> 02:42:07,000
temperatures on each day. So we have, you know, three, six, essentially 7.5, 8.25. And you can

2522
02:42:07,080 --> 02:42:12,120
see these are the temperatures, based on the fact that we start with an initial probability of

2523
02:42:12,120 --> 02:42:16,360
starting on a cold day. So we kind of get that here, right, we're starting at three degrees.

2524
02:42:16,360 --> 02:42:20,680
That's what it's determined, we're going to start at. And then we have all of these other

2525
02:42:20,680 --> 02:42:25,960
temperatures is predicting for the next days. Now notice if we recreate this model, so just

2526
02:42:25,960 --> 02:42:30,760
rerun the distributions, rerun them and go model dot mean again, this stays the same, right? Well,

2527
02:42:30,760 --> 02:42:34,360
because our probabilities are the same, this model is going to do the calculation the exact

2528
02:42:34,360 --> 02:42:38,760
same, there's not really any training that goes into this. So we get, you know, very similar,

2529
02:42:38,760 --> 02:42:42,680
if not the exact same values, I can't remember if these are identical, but that's what it looks

2530
02:42:42,680 --> 02:42:46,600
like to me. I mean, we can run this again, see, we get the same one, and we'll create the model

2531
02:42:46,600 --> 02:42:50,120
one more time. And let me just check these values here to make sure I'm not lying to you as yes,

2532
02:42:50,120 --> 02:42:53,960
they are the exact same. Okay, so let's start messing with a few probabilities and see what we

2533
02:42:53,960 --> 02:43:00,520
can do to this temperature and see what changes we can cause. So if I do 0.5 here, and I do 0.5

2534
02:43:00,600 --> 02:43:05,560
for the categorical probability, remember this refers to points three and four above. So it's

2535
02:43:05,560 --> 02:43:09,880
a cold day has a 30% chance of being followed by hot day and then a hot day has a 20% chance of

2536
02:43:09,880 --> 02:43:13,960
being followed by cold day. So what I've just done now is change the probability to be 50%

2537
02:43:14,760 --> 02:43:19,240
so that a cold day now has a 50% chance of being followed by hot day and a 50% chance of

2538
02:43:19,240 --> 02:43:24,360
being followed by cold day. And let's recreate this model. Let's rerun this and let's see if

2539
02:43:24,360 --> 02:43:29,720
we get a difference. But we do notice this, the temperature now has been a is going a little bit

2540
02:43:29,720 --> 02:43:34,440
higher. Now notice that we get the same starting temperature because that's just the average

2541
02:43:34,440 --> 02:43:38,760
based on this probability that we have here. But if we wanted to potentially start, you know,

2542
02:43:38,760 --> 02:43:44,840
hotter, we could reverse these numbers, we go 0.2 0.8. Let's rerun all of this. And now look at

2543
02:43:44,840 --> 02:43:50,040
this what our temperatures are, we start at 12. And then we actually drop our temperature down to 10.

2544
02:43:50,040 --> 02:43:54,120
So that's how this hidden Markov model works. Now this is nice, because you can just tweak the

2545
02:43:54,120 --> 02:43:58,520
probabilities. This happens pretty well instantly. And we can have a look at our output very nicely.

2546
02:43:58,520 --> 02:44:03,320
So obviously, this is representing the temperature on our like the first day, this would be the

2547
02:44:03,320 --> 02:44:08,200
second day, third day, fourth day, fifth, sixth, seventh, and obviously, like the more days you

2548
02:44:08,200 --> 02:44:13,000
go on, the least accurate, this is probably going to be because it just runs off probability. And

2549
02:44:13,000 --> 02:44:16,520
if you're going to try to predict, you know, a year in advance, and you're using the weather that you

2550
02:44:16,520 --> 02:44:20,600
have from I guess the previous year, you're probably not going to get a very accurate prediction.

2551
02:44:20,600 --> 02:44:24,440
But anyways, these are hidden Markov models. They're not like extremely useful. There's

2552
02:44:24,440 --> 02:44:27,800
some situations where you might want to use something like this. So that's why we're

2553
02:44:27,800 --> 02:44:31,480
implementing them in kind of in this course and showing you how they work. It's also another

2554
02:44:31,480 --> 02:44:35,560
feature of TensorFlow that a lot of people don't talk about or see. And you know, personally,

2555
02:44:35,560 --> 02:44:39,720
I hadn't really heard of hidden Markov models until I started developing this course. So anyways,

2556
02:44:39,720 --> 02:44:45,240
that has been it for this module. Now I hope that this kind of gave you guys a little bit of an idea

2557
02:44:45,240 --> 02:44:48,840
of how we can actually implement some of these machine learning algorithms, a little bit of

2558
02:44:48,840 --> 02:44:53,640
idea of how to work with data, how we can feed that to a model, the importance between testing

2559
02:44:53,640 --> 02:44:57,800
and training data. And then obviously, linear regression is one we focused a lot on. So I

2560
02:44:57,800 --> 02:45:01,800
hope you guys are very comfortable with that algorithm. And then what was the last the second

2561
02:45:01,800 --> 02:45:05,640
one we did, I got to go up to remember exactly the sequence we had here. So classification,

2562
02:45:05,640 --> 02:45:10,760
that one was important as well. So I hope you guys really understood that clustering, we didn't go

2563
02:45:10,760 --> 02:45:14,920
too far into that. But again, this is an interesting algorithm. And if you need to do some kind of

2564
02:45:14,920 --> 02:45:19,320
clustering, you now know of one algorithm to do that called K means clustering, and you understand

2565
02:45:19,320 --> 02:45:23,080
how that works. And now you know, hidden Markov models. So in the next module, we're going to

2566
02:45:23,080 --> 02:45:26,920
start covering neural networks, we now have the knowledge we need to really dive in there and

2567
02:45:26,920 --> 02:45:30,600
start doing some cool stuff. And then in the future modules, we're going to do deep computer

2568
02:45:30,600 --> 02:45:34,760
vision, I believe we're going to do chatbots with recurrent neural networks, and then some form

2569
02:45:34,760 --> 02:45:39,320
of reinforcement learning at the end. So with that being said, let's go to the next module.

2570
02:45:42,680 --> 02:45:47,240
Hello, everybody, and welcome to module four. Now in this module of this course, we're going to be

2571
02:45:47,240 --> 02:45:51,960
talking about neural networks, discussing how neural networks work, a little bit of the math

2572
02:45:51,960 --> 02:45:57,400
behind them, talking about gradient descent and back propagation, and how information actually

2573
02:45:57,400 --> 02:46:00,920
flows through the neural network, and then getting into an example where we use a neural

2574
02:46:00,920 --> 02:46:05,400
network to classify articles of clothing. So I know that was a lot, but that's what we're

2575
02:46:05,400 --> 02:46:10,040
going to be covering here. Now neural networks are complex. There's kind of a lot of components

2576
02:46:10,040 --> 02:46:13,800
that go into them. And I'm going to apologize right now, because it's very difficult to explain

2577
02:46:13,800 --> 02:46:18,200
it all at once. What I'm going to be trying to do is kind of piece things together and explain

2578
02:46:18,280 --> 02:46:23,320
them in blocks. And then at the end, you know, kind of combine everything together. Now I will

2579
02:46:23,320 --> 02:46:27,480
say in case any of you didn't watch the beginning of this course, I do have very horrible handwriting,

2580
02:46:27,480 --> 02:46:31,880
but this is the easiest way to explain things to you guys. So bear with me, you know, I'm sure

2581
02:46:31,880 --> 02:46:35,880
you'll be able to understand what I'm saying, but it might just be painful to read some of it.

2582
02:46:35,880 --> 02:46:39,640
All right, so let's get into it right away and start discussing what neural networks are and

2583
02:46:39,640 --> 02:46:44,680
how they work. Well, the whole point of a neural network is to provide, you know, classification

2584
02:46:44,680 --> 02:46:49,800
or predictions for us. So we have some input information, we feed it to the neural network,

2585
02:46:49,800 --> 02:46:53,960
and then we want it to give us some output. So if we think of the neural network as this black box,

2586
02:46:53,960 --> 02:46:57,640
we have all this input, right, we give all this data to the neural network, maybe we're talking

2587
02:46:57,640 --> 02:47:02,440
about an image, maybe we're talking about just some random data points, maybe we're talking about a

2588
02:47:02,440 --> 02:47:07,880
data set, and then we get some meaningful output. This is what we're looking at. So if we're just

2589
02:47:07,880 --> 02:47:11,560
looking at a neural network from kind of the outside, we think of it as this magical black

2590
02:47:11,560 --> 02:47:16,200
box, we give some input, it gives us some output. And I mean, we could call this black box just some

2591
02:47:16,200 --> 02:47:20,520
function, right, where it's a function of the input maps it to some output. And that's exactly

2592
02:47:20,520 --> 02:47:25,800
what a neural network does. It takes input and maps that input to some output, just like any

2593
02:47:25,800 --> 02:47:31,640
other function, right, just like if you had a straight line like this, this is a function,

2594
02:47:31,640 --> 02:47:36,520
you know, this is your line, you know, whatever it is, you're going to say y equals like four x,

2595
02:47:36,600 --> 02:47:41,720
maybe that's your line, you give some input x, and it gives you some value y, this is a mapping

2596
02:47:41,720 --> 02:47:48,040
of your input to your output. Alright, so now that we have that down, what is a neural network

2597
02:47:48,040 --> 02:47:52,920
made up of? Well, a neural network is made up of layers. And remember, we talked about the layered

2598
02:47:52,920 --> 02:47:57,880
representation of data when we talked about neural networks. So I'm going to draw a very basic

2599
02:47:57,880 --> 02:48:04,360
neural network, we're going to start with the input layer. Now the input layer is always the

2600
02:48:04,360 --> 02:48:09,720
first layer in our neural network. And it is what is going to accept our raw data. Now what I mean

2601
02:48:09,720 --> 02:48:15,400
by raw data is whatever data we like want to give to the network, whatever we want to classify

2602
02:48:15,400 --> 02:48:20,520
whatever our input information is, that's what this layer is going to receive in the neural

2603
02:48:20,520 --> 02:48:25,480
network. So we can say, you know, these arrows represent our input, and they come to our first

2604
02:48:25,480 --> 02:48:30,840
input layer. So this means, for example, if you had an image, and this image, and I'll just draw

2605
02:48:30,920 --> 02:48:34,920
like one like this, let's say this our image, and it has all these different pixels, right,

2606
02:48:34,920 --> 02:48:38,280
all these different pixels in the image, and you want to make a classification on this image.

2607
02:48:39,000 --> 02:48:44,360
Well, maybe it has a width and a height and a classic width and height example is 28 by 28.

2608
02:48:44,360 --> 02:48:48,520
If you had 28 by 28 pixels, and you want to make a classification on this image,

2609
02:48:49,080 --> 02:48:53,160
how many input neurons you think you would need in your neural network to do this?

2610
02:48:54,120 --> 02:48:57,560
Well, this is kind of, you know, a tough question if you don't know a lot about neural networks.

2611
02:48:58,520 --> 02:49:02,360
If you're predicting for the image, if you're going to be looking at the entire image to

2612
02:49:02,360 --> 02:49:08,200
make a prediction, you're going to need every single one of those pixels, which is 28 times 28

2613
02:49:08,200 --> 02:49:14,120
pixels, which I believe is something like 784. I could be wrong on that number, but I believe

2614
02:49:14,120 --> 02:49:20,440
that's what it is. So you would need 784 input input neurons. Now, that's totally fine. That

2615
02:49:20,440 --> 02:49:24,120
might seem like a big number, but we deal with massive numbers when it comes to computers. So

2616
02:49:24,200 --> 02:49:28,680
this really isn't that many. But that's an example of, you know, how you would use a neural network

2617
02:49:28,680 --> 02:49:35,080
input layer to represent an image, you would have 784 input neurons, and you would pass

2618
02:49:35,080 --> 02:49:39,240
one pixel to every single one of those neurons. Now, if we're doing an example where maybe we

2619
02:49:39,240 --> 02:49:44,600
just have one piece of input information, maybe it's literally just one number. Well, then all

2620
02:49:44,600 --> 02:49:50,920
we need is one input nerve. If we have an example where we have four pieces of information, we would

2621
02:49:50,920 --> 02:49:56,040
need four input neurons, right? Now, this can get a little bit more complicated. But that's

2622
02:49:56,040 --> 02:49:59,720
the basis that I want you to understand is, you know, the pieces of input you're going to have

2623
02:49:59,720 --> 02:50:04,200
regardless of what they are, you need one input neuron for each piece of that information, unless

2624
02:50:04,200 --> 02:50:08,040
you're going to be reshaping or putting that information in different form. Okay, so let's

2625
02:50:08,040 --> 02:50:13,640
just actually skip ahead and go to now our output layer. So this is going to be our output. Now,

2626
02:50:13,640 --> 02:50:18,440
what is our output layer? Well, our output layer is going to have as many neurons. And again,

2627
02:50:18,440 --> 02:50:24,680
the neurons are just representing like a node in the layer as output pieces that we want. Now,

2628
02:50:24,680 --> 02:50:29,800
let's say we're doing a classification for images, right? And maybe there's two classes

2629
02:50:29,800 --> 02:50:34,840
that we could represent. Well, there's a few different ways we could design our output layer.

2630
02:50:34,840 --> 02:50:39,720
What we could do is say, okay, we're going to use one output neuron. This output neuron is going to

2631
02:50:39,720 --> 02:50:46,920
give us some value. We want this value to be between zero and one. And we'll say that's inclusive.

2632
02:50:47,560 --> 02:50:53,000
Now, what we can do now if we're predicting two classes say, Okay, so if my output neuron is

2633
02:50:53,000 --> 02:50:58,120
going to give me some value, if that value is closer to zero, then that's going to be class zero.

2634
02:50:58,120 --> 02:51:02,920
If this value is closer to one, it's going to be class one, right? And that would mean

2635
02:51:03,640 --> 02:51:07,400
when we have our training data, right, and we talked about training and testing data,

2636
02:51:07,400 --> 02:51:13,160
we'd give our input and our output would need to be the value zero or one, because it's either

2637
02:51:13,160 --> 02:51:16,520
the correct class, which is zero, right, or the correct class, which is one. So like our

2638
02:51:16,920 --> 02:51:21,800
what am I saying, our labels for our training data set would be zero and one. And then this value

2639
02:51:21,800 --> 02:51:26,040
on our output neuron will be guaranteed to be between zero and one, based on something that

2640
02:51:26,040 --> 02:51:29,640
I'm going to talk about a little bit later. That's one way to approach it, right? We have a single

2641
02:51:29,640 --> 02:51:35,160
value, we look at that value. And based on what that value is, we can determine, you know, what

2642
02:51:35,160 --> 02:51:40,040
class we predicted, not work sometimes. But in other instances, when we're doing classification,

2643
02:51:40,040 --> 02:51:45,720
what makes more sense is to have as many output neurons as classes you're looking to predict for.

2644
02:51:45,720 --> 02:51:49,320
So let's say we're going to have, you know, like five classes that we're predicting for maybe

2645
02:51:49,320 --> 02:51:54,680
these three pieces of input information are enough to make that prediction. Well, we'd actually have

2646
02:51:54,680 --> 02:52:01,000
five output neurons. And each of these neurons would have a value between zero and one. And the

2647
02:52:01,000 --> 02:52:07,160
combination, so the sum of every single one of these values would be equal to one. Now, can you

2648
02:52:07,160 --> 02:52:12,440
think of what this means? If every single one of these neurons is a value between zero and one,

2649
02:52:12,440 --> 02:52:17,000
and their sum is one, what does this look like to you? Well, to me, this looks like a probability

2650
02:52:17,000 --> 02:52:21,240
distribution. And essentially, what's going to happen is we're going to make predictions for how

2651
02:52:21,240 --> 02:52:27,480
strongly we think each input information is each class. So if we think that it's like class one,

2652
02:52:27,480 --> 02:52:32,920
maybe we'll just label these like this, then what we would do is say, okay, this is going to be

2653
02:52:32,920 --> 02:52:42,920
0.9 representing 90%. Maybe this is like 0.001, maybe this is 0.05, 0.003, you get the point,

2654
02:52:42,920 --> 02:52:46,600
it's going to add up to one, and this is a probability distribution for our output layer.

2655
02:52:47,160 --> 02:52:51,480
So that's a way to do it as well. And then obviously, if we're doing some kind of regression task,

2656
02:52:51,480 --> 02:52:55,720
we can just have one neuron and that will just predict some value. And we'll define, you know,

2657
02:52:55,720 --> 02:53:01,160
what we want that value to be. Okay, so that's my example for my output. Now let's erase this

2658
02:53:01,160 --> 02:53:04,200
and let's actually just go back to one output neuron, because that's what I want to use for

2659
02:53:04,200 --> 02:53:10,200
this example. Now, we have something in between these layers, because obviously, you know, we

2660
02:53:10,200 --> 02:53:14,680
can't just go from input to output with nothing else. What we have here is called a hidden layer.

2661
02:53:15,400 --> 02:53:18,520
Now, in neural networks, we can have many different hidden layers, we can have, you know,

2662
02:53:18,520 --> 02:53:22,920
hidden layers that are connecting to other hidden layers, and like we could have hundreds,

2663
02:53:22,920 --> 02:53:28,360
thousands, if we wanted to, for this basic example, we'll use one. And I'll write this as hidden.

2664
02:53:29,320 --> 02:53:33,320
So now we have our three layers. Now, why is this called hidden? The reason this is called

2665
02:53:33,320 --> 02:53:37,720
hidden is because we don't observe it when we're using the neural network, we pass information

2666
02:53:37,720 --> 02:53:41,800
to the input layer, we get information from the output layer, we don't know what happens

2667
02:53:41,800 --> 02:53:46,600
in this hidden layer or in these hidden layers. Now, how are these layers connected to each other?

2668
02:53:46,600 --> 02:53:50,200
How do we get from this input layer to the hidden layer to the output layer and get some

2669
02:53:50,200 --> 02:53:56,200
meaningful output? Well, every single layer is connected to another layer with something called

2670
02:53:56,280 --> 02:54:00,120
weights. Now, we can have different kind of architectures of connections, which means I

2671
02:54:00,120 --> 02:54:05,080
could have something like this one connects to this, this connects to this, this connects to this,

2672
02:54:05,080 --> 02:54:09,080
and that could be like my connection kind of architecture, right? We could have another one

2673
02:54:09,080 --> 02:54:14,920
where this one goes here. And you know, maybe this one goes here. And actually, after I've drawn

2674
02:54:14,920 --> 02:54:19,800
this line, now we get what we're going to be talking about a lot, which is called a densely

2675
02:54:19,800 --> 02:54:25,160
connected neural network. Now a densely connected neural network or a densely connected layer,

2676
02:54:25,240 --> 02:54:30,120
essentially means that is connected to every node from the previous layer. So in this case,

2677
02:54:30,120 --> 02:54:35,720
you can see every single node in the input layer is connected to every single node in the output

2678
02:54:35,720 --> 02:54:40,760
layer or in the hidden layer, my bad. And these connections are what we call weights. Now these

2679
02:54:40,760 --> 02:54:46,360
weights are actually what the neural network is going to change and optimize to determine the

2680
02:54:46,360 --> 02:54:50,360
mapping from our input to our output. Because again, remember, that's what we're trying to do.

2681
02:54:50,360 --> 02:54:54,520
We have some kind of function, we give some input, it gives us some output. How do we get that input

2682
02:54:54,520 --> 02:54:58,680
and output? Well, by modifying these weights, it's a little bit more complex, but this is the

2683
02:54:58,680 --> 02:55:02,920
starting. So these lines that I've drawn are really just numbers. And every single one of these

2684
02:55:02,920 --> 02:55:07,560
lines is some numeric value. Typically, these numeric values are between zero and one, but

2685
02:55:07,560 --> 02:55:12,280
they can be large, they can be negative. It really depends on what kind of network you're doing and

2686
02:55:12,280 --> 02:55:17,240
how you've designed it. Now, let's just write some random numbers, we have like 0.1, this could

2687
02:55:17,240 --> 02:55:21,800
be like 0.7, you get the point, right? We just have numbers for every single one of these lines.

2688
02:55:22,600 --> 02:55:26,440
And these are what we call the trainable parameters that our neural network will

2689
02:55:26,440 --> 02:55:32,280
actually tweak and change as we train to get the best possible result. So we have these

2690
02:55:32,280 --> 02:55:35,800
connections. Now our hidden layer is connected to our output layer as well. This is again,

2691
02:55:35,800 --> 02:55:41,800
another densely connected layer, because every layer or every nor neuron from the previous layer

2692
02:55:41,800 --> 02:55:45,960
is connected to every neuron from the next layer, you would like to determine how many

2693
02:55:45,960 --> 02:55:49,880
connections you have, what you can do is say there's three neurons here, there's two neurons

2694
02:55:49,880 --> 02:55:55,240
here, three times two equals six connections. That's how that works from layers. And then

2695
02:55:55,240 --> 02:55:59,880
obviously, you can just multiply all of the neurons together as you go through and determine

2696
02:56:00,680 --> 02:56:05,960
what that's going to be. Okay, so that is how we connect these layers, we have these weights. So

2697
02:56:05,960 --> 02:56:10,120
let's just write a w on here. So we remember that those are weights. Now, we also have something

2698
02:56:10,120 --> 02:56:16,520
called biases. So let's add a bias here, I'm going to label this B. Now biases are a little bit

2699
02:56:16,600 --> 02:56:22,040
different than these nodes we have regularly. There's only one bias, and a bias exists in

2700
02:56:22,040 --> 02:56:27,320
the previous layer to the layer that it affects. So in this case, what we actually have is a bias

2701
02:56:27,320 --> 02:56:33,160
that connects to each neuron in the next layer from this layer, right? So it's still densely connected.

2702
02:56:34,200 --> 02:56:38,600
But it's just a little bit different. Now notice that this bias doesn't have an arrow beside it

2703
02:56:38,600 --> 02:56:44,040
because this doesn't take any input information. This is another trainable parameter for the

2704
02:56:44,040 --> 02:56:50,520
network. And this bias is just some constant numeric value that we're going to connect to the

2705
02:56:50,520 --> 02:56:55,640
hidden layer. So we can do a few things with it. Now these weights always have a value of one.

2706
02:56:56,280 --> 02:57:00,040
We're going to talk about why they have a value of one in a second. But just know that whenever

2707
02:57:00,040 --> 02:57:05,720
a bias is connected to another layer or to another neuron, its weight is typically one.

2708
02:57:06,680 --> 02:57:11,080
Okay, so we have that connected, we have our bias, and that actually means we have a bias

2709
02:57:11,080 --> 02:57:16,520
here as well. And this bias connects to this. Notice that our biases do not connect with each

2710
02:57:16,520 --> 02:57:19,960
other. The reason for this, again, is they're just some constant value, and they're just something

2711
02:57:19,960 --> 02:57:25,160
we're kind of adding into the network is another trainable parameter that we can use. Now let's

2712
02:57:25,160 --> 02:57:29,480
talk about how we actually pass information through the network and why we even use these

2713
02:57:29,480 --> 02:57:34,920
weights and biases of what they do. So let's say we have, I can't really think of a good example,

2714
02:57:34,920 --> 02:57:40,360
so we're just going to do some arbitrary stuff. Let's say we have like data points, right? X, Y, Z,

2715
02:57:41,480 --> 02:57:45,640
and all of these data points have some mapped value, right? There's some value that we're

2716
02:57:45,640 --> 02:57:48,920
looking for for them, or there's some class we're trying to put them in, maybe we're clustering

2717
02:57:48,920 --> 02:57:56,280
them between like, red dots and blue dots. So let's do that. Let's say an XYZ is either a part of

2718
02:57:56,280 --> 02:58:02,360
the red class, or the blue class, let's just do that. So what we want this output neuron to give

2719
02:58:02,360 --> 02:58:07,080
us is red or blue. So what I'm going to do is say since it's just one class, we'll get this

2720
02:58:07,160 --> 02:58:12,520
output neuron in between the range is your own one, we'll say, okay, if it's closer to zero,

2721
02:58:12,520 --> 02:58:17,160
that's red, if it's closer to one, that's blue. And that's what we'll do for this network. And for

2722
02:58:17,160 --> 02:58:24,200
this example, now our input neurons are going to obviously be X, Y, and Z. So let's pick some

2723
02:58:24,200 --> 02:58:28,600
data point. And let's say we have, you know, the value two, two, two, that's our data point. And

2724
02:58:28,600 --> 02:58:34,120
we want to predict whether it's red or blue. How do we pass it through? Well, what we need to do

2725
02:58:34,120 --> 02:58:40,200
is determine how we can, you know, find the value of this hidden layer node, we already know the

2726
02:58:40,200 --> 02:58:44,840
value of these input node, but now we need to go to the next layer using these connections and find

2727
02:58:44,840 --> 02:58:49,320
what the value of these nodes are. Well, the way we determine these values is I'm going to say,

2728
02:58:49,320 --> 02:58:54,920
and I've just said n one, just to represent like this is a node, like this is node one, maybe this

2729
02:58:54,920 --> 02:59:01,880
one should be node two, is equal to what we call the weighted sum of all of the previous nodes that

2730
02:59:01,880 --> 02:59:07,480
are connected to it. If that makes any sense to you guys. So a weighted sum is something like this.

2731
02:59:07,480 --> 02:59:11,000
So I'm just going to write the equation, I'll explain it, I'm going to say n one is equal to

2732
02:59:11,000 --> 02:59:19,320
the sum of, let's not say n equals zero, let's say, I equals zero to n of, in this case, we're

2733
02:59:19,320 --> 02:59:26,920
going to say w i times x i plus b. Now, I know this equation looks really mathy and complicated,

2734
02:59:26,920 --> 02:59:32,440
it's really not what this symbol and this equation here means is take the weighted sum

2735
02:59:32,440 --> 02:59:37,480
of all the neurons that are connected to this neuron. So in this case, we have a neuron x neuron

2736
02:59:37,480 --> 02:59:43,480
y and neuron z connected to n one. So when we take the weighted sum, or we calculate this,

2737
02:59:43,480 --> 02:59:49,960
what this is really equal to is the weight at neuron x, we can say w x times the value at

2738
02:59:49,960 --> 02:59:56,600
neuron x, which in this case, is just equal to two, right, plus whatever the weight is at neuron y.

2739
02:59:56,600 --> 03:00:03,080
So in this case, this is w y. And then times two, and then you get the point where we have

2740
03:00:03,080 --> 03:00:08,280
w z and I'm trying on the edge of my drawing tablet to write this times two. Now, obviously,

2741
03:00:08,280 --> 03:00:12,280
these weights have some numeric value. Now, when we start our neural network, these weights are

2742
03:00:12,280 --> 03:00:17,160
just completely random. They don't make any sense or just some random values that we can use. As the

2743
03:00:17,160 --> 03:00:21,640
neural network gets better, these weights are updated and changed to make more sense in our

2744
03:00:21,640 --> 03:00:26,440
network. So right now, we'll just leave them as w x, w y, w z. But no, these are some numeric

2745
03:00:26,440 --> 03:00:32,280
values. So this returns to a sum value, right, some value, let's just call this value v. And

2746
03:00:32,280 --> 03:00:38,040
that's what this is equal to. So v. Then what we do is we add the bias. Now remember, the bias

2747
03:00:38,040 --> 03:00:44,760
was connected with a weight of one, which means if we take the weighted sum of the bias, right,

2748
03:00:44,760 --> 03:00:49,640
all we're doing is adding whatever that biases value was. So if this bias value was 100,

2749
03:00:49,640 --> 03:00:54,840
then what we do is we add 100. Now, I've just written the plus B to explicitly state the fact

2750
03:00:54,840 --> 03:00:59,160
that we're adding the bias, although it could really be considered as a part of the summation

2751
03:00:59,160 --> 03:01:04,840
equation, because it's another connection to the neural. Now, let's just talk about what

2752
03:01:04,840 --> 03:01:09,720
this symbol means for anyone that's confused about that. Essentially, this stands for sum,

2753
03:01:09,720 --> 03:01:16,120
I stands for an index, and n stands for what index will go up to now n means how many neurons we

2754
03:01:16,120 --> 03:01:21,320
had in the previous layer. And then what we're doing here is saying wi xi. So we're going to say

2755
03:01:21,320 --> 03:01:27,400
weight zero x zero plus weight one x one plus weight two x two, it's almost like a for loop where

2756
03:01:27,400 --> 03:01:31,880
we're just adding them all together. And then we add the B. And I hope that makes enough sense

2757
03:01:31,880 --> 03:01:36,520
so that we understand that. So that is our weighted sum and our bias. So essentially, what we do is

2758
03:01:36,520 --> 03:01:40,120
we go through and we calculate these values. So this gets some value, maybe this value is like

2759
03:01:40,120 --> 03:01:45,720
0.3, maybe this value seven, whatever it is, and we do the same thing now at our output neuron.

2760
03:01:45,720 --> 03:01:51,080
So we take the weighted sum of this value times its weight. And then we take the weighted sum,

2761
03:01:51,080 --> 03:01:57,080
so this value times its weight, plus the bias, this is given some value here. And then we can

2762
03:01:57,080 --> 03:02:02,280
look at that value and determine what the output of our neural network is. So that is pretty much

2763
03:02:02,280 --> 03:02:06,600
how that works in terms of the weighted sums, the weights and the biases. Now, let's talk about the

2764
03:02:06,600 --> 03:02:10,920
kind of the training process and another thing called an activation function. So I've lied to

2765
03:02:10,920 --> 03:02:14,200
a little bit because I've said I'm just going to start erasing some stuff. So we have a little bit

2766
03:02:14,200 --> 03:02:19,000
more room on here. So I've lied to you and I've said that this is completely how this works.

2767
03:02:19,000 --> 03:02:23,080
Well, we're missing one key feature that I want to talk about, which is called an activation

2768
03:02:23,080 --> 03:02:29,320
function. Now remember how we want this value to be in between zero and one right at our output layer.

2769
03:02:29,320 --> 03:02:33,400
Well, right now, we can't really guarantee that that's going to happen. I mean, especially for

2770
03:02:33,400 --> 03:02:38,440
starting with random weights and random biases in our neural network, we're passing this information

2771
03:02:38,440 --> 03:02:44,120
through, we could get to this, you know, point here, we could have like 700 as our value.

2772
03:02:44,840 --> 03:02:48,600
That's kind of crazy to me, right? We have this huge value, how do we look at 700 and

2773
03:02:48,600 --> 03:02:52,120
determine whether this is red or whether this is blue? Well, we can use something called an

2774
03:02:52,120 --> 03:02:56,280
activation function. Now, I'm going to go back to my slides here, whatever you want to call this

2775
03:02:56,280 --> 03:03:00,200
this notebook, just to talk about what an activation function is. And you guys can see here, you can

2776
03:03:00,200 --> 03:03:05,800
follow along, I have all the equations kind of written out here as well. So let's go to activation

2777
03:03:05,880 --> 03:03:10,360
function, which is right here. Okay. So these are some examples of an activation function. And I

2778
03:03:10,360 --> 03:03:14,680
just want you to look at what they do. So this first one is called rectified linear unit. Now

2779
03:03:14,680 --> 03:03:20,200
notice that essentially what this activation function does is take any values that are less

2780
03:03:20,200 --> 03:03:25,080
than zero and just make them zero. So any x values that are, you know, in the negative, it just makes

2781
03:03:25,080 --> 03:03:30,440
their y zero. And then any values that are positive, it's just equal to whatever their positive value

2782
03:03:30,440 --> 03:03:35,160
is. So if it's 10, it's 10. This allows us to just pretty much eliminate any negative numbers,

2783
03:03:35,160 --> 03:03:40,760
right? That's kind of what rectified linear unit does. Now 10 h or hyperbolic tangent.

2784
03:03:41,400 --> 03:03:46,520
What does this do? This actually squishes our values between negative one and one. So it takes

2785
03:03:46,520 --> 03:03:51,720
whatever values we have, and the more positive they are, the closer to one they are, the more

2786
03:03:51,720 --> 03:03:56,360
negative they are, the closer to negative one they are. So when we see why this might be useful,

2787
03:03:56,360 --> 03:04:00,200
right for a neural network, and then last one is sigmoid, what this does is squish our values

2788
03:04:00,200 --> 03:04:05,400
between zero and one, a lot of people call it like the squishifier function. Because all it does

2789
03:04:05,400 --> 03:04:09,960
is take any extremely negative numbers and put them closer to zero and any extremely positive

2790
03:04:09,960 --> 03:04:13,560
numbers and put them close to one, any values in between, you're going to get some number that's

2791
03:04:13,560 --> 03:04:18,760
kind of in between that, based on the equation one over one plus e to the negative z. And this is

2792
03:04:18,760 --> 03:04:23,640
theta z, I guess, is equal to that. Okay, so that's how that works. Those are some activation

2793
03:04:23,640 --> 03:04:27,240
functions. Now I hope that's not too much math for you. But let's talk about how we use them,

2794
03:04:27,240 --> 03:04:32,120
right? So essentially, what we do is at each of our neurons, we're going to have an activation

2795
03:04:32,120 --> 03:04:37,560
function that is applied to the output of that neuron. So we take this this weighted sum plus

2796
03:04:37,560 --> 03:04:43,720
the bias, and then we apply an activation function to it before we send that value to the next neuron.

2797
03:04:43,720 --> 03:04:50,680
So in this case, n one isn't actually just equal to this, what n one is equal to is n one is equal

2798
03:04:50,680 --> 03:04:56,760
to f, which stands for activation function of this equation, right? So we say I equals zero,

2799
03:04:58,040 --> 03:05:06,520
w i x i plus B. And that's what n one's value is equal to when it comes to this output neuron.

2800
03:05:07,160 --> 03:05:11,400
So each of these have an activation function on them. And two has the same activation function

2801
03:05:11,400 --> 03:05:16,280
as n one. And we can define what activation function we want to apply at each neuron.

2802
03:05:16,920 --> 03:05:20,840
Now at our output neuron, the activation function is very important, because we need to determine

2803
03:05:20,840 --> 03:05:24,040
what we want our value to look like. Do we want it between negative one on one? Do we want it

2804
03:05:24,040 --> 03:05:29,960
between zero and one? Or do we want it to be some massively large number? Do we want it between zero

2805
03:05:29,960 --> 03:05:35,000
and positive infinity? What do we want? Right? So what we do is we pick some activation function

2806
03:05:35,000 --> 03:05:41,000
for our output neuron. And based on what I said, where we want our values between zero and one,

2807
03:05:41,080 --> 03:05:48,040
I'm going to be picking the sigmoid function. So sigmoid, recall, squishes our values between

2808
03:05:48,040 --> 03:05:55,160
zero and one. So what we'll do here is we'll take n one, right? So n one times whatever the weight

2809
03:05:55,160 --> 03:06:03,240
is there. So weight zero, plus n two times weight one, plus a bias, and apply sigmoid.

2810
03:06:04,280 --> 03:06:10,120
And then this will give us some value between zero and one, then we can look at that value and we

2811
03:06:10,120 --> 03:06:14,520
can determine what the output of this network is. So that's great. And that makes sense. Why

2812
03:06:14,520 --> 03:06:19,400
we would use that on the output neuron, right? So we can squish our value in between some kind

2813
03:06:19,400 --> 03:06:22,440
of value. So we can actually look at it and determine, you know, what to do with it, rather

2814
03:06:22,440 --> 03:06:27,080
than just having these crazy, and I want to see if I can make this eraser any bigger. Ah, that's

2815
03:06:27,080 --> 03:06:32,680
much better. Okay. So there we go. Let's just erase some of this. And now let's talk about why we

2816
03:06:32,680 --> 03:06:37,480
would use the activation function on like an intermediate layer like this. Well, the whole

2817
03:06:37,480 --> 03:06:43,480
point of an activation function is to introduce complexity into our neural network. So essentially,

2818
03:06:43,480 --> 03:06:47,720
you know, we just have these basic weights and these biases. And this is kind of just,

2819
03:06:47,720 --> 03:06:51,000
you know, like a complex function at this point, we have a bunch of weights, we have a bunch of

2820
03:06:51,000 --> 03:06:54,600
biases. And those are the only things that we're training. And the only things that we're changing

2821
03:06:54,600 --> 03:06:59,880
to make our network better. Now, what an activation function can do is, for example,

2822
03:06:59,880 --> 03:07:03,000
take a bunch of points that are on the same like plane, right? So let's just say,

2823
03:07:03,880 --> 03:07:09,560
these are in some plane. If we can apply an activation function of these, where we

2824
03:07:10,520 --> 03:07:14,920
introduce a higher dimensionality, so an activation function like sigmoid that is like a

2825
03:07:14,920 --> 03:07:21,800
higher dimension function, we can hopefully spread these points out and move them up or down off the

2826
03:07:21,800 --> 03:07:28,760
plane in a hopes of extracting kind of some different features. Now, it's hard to explain

2827
03:07:28,760 --> 03:07:33,800
this until we get into the training process of the neural network. But I'm hoping this is maybe

2828
03:07:33,800 --> 03:07:39,000
giving you a little bit of idea, if we can introduce a complex activation function into this kind of

2829
03:07:39,000 --> 03:07:43,560
process, then it allows us to make some more complex predictions, we can pick up on some

2830
03:07:43,560 --> 03:07:48,360
different patterns. If I can see that, you know, when sigmoid or rectify linear unit is applied

2831
03:07:48,360 --> 03:07:53,000
to this output, it moves my point up or it moves it down or moves it in like whatever direction

2832
03:07:53,000 --> 03:07:57,560
and n dimensional space, then I can determine specific patterns I couldn't determine in the

2833
03:07:57,560 --> 03:08:01,960
previous dimension. That's just like if we're looking at something in two dimensions, if I can

2834
03:08:01,960 --> 03:08:05,880
move that into three dimensions, I immediately see more detail, there's more things that I can

2835
03:08:05,880 --> 03:08:10,360
look at, right? And I'll try to do a good example of why we might use it like this. So let's say

2836
03:08:10,360 --> 03:08:14,280
we have a square, right, like this, right? And I ask you, I'm like, tell me some information

2837
03:08:14,280 --> 03:08:17,080
about the square. Well, what you can tell me immediately is you can tell me the width, you

2838
03:08:17,080 --> 03:08:21,160
can tell me the height, and I guess you could tell me the color, right? You can tell me it has

2839
03:08:21,160 --> 03:08:24,920
one face, you can tell me it has four vertexes, you can tell me a fair amount about the square,

2840
03:08:24,920 --> 03:08:29,400
you can tell me its area. Now what happens as soon as I extend the square and I make it into a cube?

2841
03:08:30,280 --> 03:08:34,760
Well, now you can immediately tell me a lot more information, you can tell me, you know, the height,

2842
03:08:35,640 --> 03:08:40,600
or I guess the depth with height depth, yeah, whatever you want to call it there. You can tell

2843
03:08:40,600 --> 03:08:44,600
me how many faces it has, you can tell me what color each of the faces are, you can tell me how

2844
03:08:44,600 --> 03:08:49,880
many vertexes you can tell me if this cube or the square, this rectangle is uniform or not,

2845
03:08:49,880 --> 03:08:53,880
and you can pick up on a lot more information. So that's kind of I mean, this is a very over

2846
03:08:53,880 --> 03:08:59,160
simplification of what this actually does. But this is kind of the concept, right, is that if we

2847
03:08:59,160 --> 03:09:03,720
are in two dimensions, if we can somehow move our data points into a higher dimension by applying

2848
03:09:03,720 --> 03:09:08,360
some function to them, then what we can do is get more information and extract more information

2849
03:09:08,360 --> 03:09:13,320
about the data points, which will lead to better predictions. Okay, so now that we've talked about

2850
03:09:13,320 --> 03:09:16,360
all this, it's time to talk about how neural networks train. And I think you guys are ready

2851
03:09:16,360 --> 03:09:21,400
for this. This is a little bit more complicated. But again, it's not that crazy. Alright, so we

2852
03:09:21,480 --> 03:09:25,400
talked about these weights and biases. And these weights and biases are what our network will

2853
03:09:25,400 --> 03:09:31,000
come up with and determine to, you know, like make the network better. So essentially, what we're

2854
03:09:31,000 --> 03:09:36,120
going to do now is talk about something called a loss function. So as our network starts, right,

2855
03:09:36,120 --> 03:09:41,000
the way that we train it, just like we've trained other networks, or other machine learning models

2856
03:09:41,000 --> 03:09:45,960
is we give it some information, we give it what the expected output is. And then we just see what

2857
03:09:45,960 --> 03:09:50,440
the expected output or what the output was from the network, compare it to the expected output

2858
03:09:50,440 --> 03:09:55,240
and modify it like that. So essentially, what we start with is we say, okay, 222, we say this

2859
03:09:55,240 --> 03:10:00,840
class is red, which I forget what I labeled that was as but let's just say, like that was a zero,

2860
03:10:00,840 --> 03:10:07,000
okay. So this class is zero. So I want this network to give me a zero for the point 222. Now,

2861
03:10:07,000 --> 03:10:12,360
this network starts with completely random weights and completely random biases. So chances are,

2862
03:10:12,360 --> 03:10:16,840
when we get to this output here, we're not going to get zero, maybe we get some value after applying

2863
03:10:16,840 --> 03:10:24,040
the sigmoid function, that's like 0.7. Well, this is pretty far away from red. But how far away is

2864
03:10:24,040 --> 03:10:29,400
it? Well, this is where we use something called a loss function. Now, what a loss function does is

2865
03:10:29,400 --> 03:10:36,440
calculate how far away our output was from our expected output. So if our expected output is

2866
03:10:36,440 --> 03:10:42,200
zero, and our output was 0.7, the loss function is going to give us some value that represents

2867
03:10:42,200 --> 03:10:47,800
like how bad or how good this network was. Now, if it tells us this network was really bad,

2868
03:10:47,800 --> 03:10:52,520
it gives us like a really high loss, then that tells us that we need to tweak the weights and

2869
03:10:52,520 --> 03:10:58,040
biases more and move the network in a different direction. We're starting to get into gradient

2870
03:10:58,040 --> 03:11:02,120
descent. But let's understand the loss function first. So it's going to say, if it was really

2871
03:11:02,120 --> 03:11:05,800
bad, let's move it more, let's change the weights more drastically, let's change the biases more

2872
03:11:05,800 --> 03:11:10,840
drastically. Whereas, if it was really good, it'll be like, Okay, so that one was actually decent,

2873
03:11:10,840 --> 03:11:14,600
you know, you only need to tweak a little bit, and you only need to move this, this and this.

2874
03:11:14,600 --> 03:11:18,680
So that's good. And that's the point of this loss function, it just calculates some value,

2875
03:11:18,680 --> 03:11:22,600
the higher the value, the worse our network was a few examples of loss function.

2876
03:11:23,320 --> 03:11:30,520
Let's go down here, because I think I had a few optimizer loss here, mean squared error,

2877
03:11:30,520 --> 03:11:36,200
mean absolute error and hinge loss. Now mean absolute error, you know, let's actually just

2878
03:11:36,280 --> 03:11:44,120
look one up here. So mean, absolute error, and have a look at what this is. So images,

2879
03:11:44,120 --> 03:11:51,560
let's pick something. This is mean absolute error. This is the equation for mean absolute error.

2880
03:11:51,560 --> 03:11:59,720
Okay, so the summation of the absolute value of yi minus lambda of xi over n. Now, this is kind of

2881
03:11:59,720 --> 03:12:03,400
complicated. I'm not going to go into it too much. I was expecting I was hoping I was going to get

2882
03:12:03,400 --> 03:12:13,960
like a better example for mean squared error. Okay, so these are the three loss functions here.

2883
03:12:13,960 --> 03:12:17,960
So mean squared error, mean absolute error, hinge loss, obviously, there's a ton more that we could

2884
03:12:17,960 --> 03:12:22,280
use. I'm not going to talk about which how each of these work specifically, I mean, you can look

2885
03:12:22,280 --> 03:12:26,920
them up pretty easily. And also, so you know, these are also referenced as cost functions,

2886
03:12:26,920 --> 03:12:32,680
so cost or loss, you might just hear these, these terms kind of interchanged cost and loss

2887
03:12:32,680 --> 03:12:37,000
essentially mean the same thing, you want your network to cost the least, you want your network

2888
03:12:37,000 --> 03:12:41,720
to have the least amount of loss. Okay, so now that we have talked about the loss function,

2889
03:12:42,360 --> 03:12:47,800
we need to talk about how we actually update these weights and biases. Now, actually, let's

2890
03:12:47,800 --> 03:12:51,800
go back to here, because I think I had some notes on it. This is what we call gradient descent.

2891
03:12:52,520 --> 03:12:56,840
So essentially, the parameters for our network are weights and biases. And by changing these

2892
03:12:56,840 --> 03:13:01,640
weights and biases, we will, you know, either make the network better or make the network worse,

2893
03:13:01,720 --> 03:13:05,880
the loss function will determine if the network is getting better, if it's getting worse, and then

2894
03:13:05,880 --> 03:13:11,240
we can determine how we're going to move the network to change that. So this is now gradient

2895
03:13:11,240 --> 03:13:15,880
descent, where the math gets a little bit more complicated. So this is an example of what your

2896
03:13:15,880 --> 03:13:22,360
neural network function might look like. Now, as you have higher dimensional math, you have,

2897
03:13:22,360 --> 03:13:26,600
you know, a lot more dimensions, a lot more space to explore when it comes to creating

2898
03:13:26,600 --> 03:13:31,160
different parameters and creating different biases and activation functions and all of that.

2899
03:13:31,240 --> 03:13:35,240
So as we apply our activation functions, we're kind of spreading our network into higher

2900
03:13:35,240 --> 03:13:39,240
dimensions, which just makes things much more complicated. Now, essentially, what we're trying

2901
03:13:39,240 --> 03:13:44,280
to do with the neural network is optimize this loss function. This loss function is telling us

2902
03:13:44,280 --> 03:13:48,520
how good it is or how bad it is. So if we can get this loss function as low as possible,

2903
03:13:48,520 --> 03:13:52,920
then that means we should technically have the best neural network. So this is our kind of

2904
03:13:52,920 --> 03:13:57,800
loss functions, like mapping or whatever, what we're looking for is something called a global

2905
03:13:57,800 --> 03:14:03,560
minimum, we're looking for the minimum point where we get the least possible loss from our

2906
03:14:03,560 --> 03:14:07,880
neural network. So if we start where these red circles are, right, and I've just stole this

2907
03:14:07,880 --> 03:14:14,120
image off Google images, what we're trying to do is move downwards into this global global

2908
03:14:14,120 --> 03:14:18,920
minimum. And this is with a process of called gradient descent. So we calculate this loss,

2909
03:14:18,920 --> 03:14:24,440
and we use an algorithm called gradient descent, which tells us what direction we need to move

2910
03:14:24,520 --> 03:14:29,400
our function to determine or to get to this global minimum. So it essentially looks where

2911
03:14:29,400 --> 03:14:33,400
we are. It says this was the loss. And it says, okay, I'm going to calculate what's called a

2912
03:14:33,400 --> 03:14:38,040
gradient, which is literally just a steepness or a direction. And we're going to move in that

2913
03:14:38,040 --> 03:14:43,000
direction. And then the algorithm called brought back propagation, we'll go backwards through

2914
03:14:43,000 --> 03:14:48,040
the network and update the weights and biases so that we move in that direction. Now, I think this

2915
03:14:48,040 --> 03:14:52,040
is as far as I really want to go, because I know this is getting more complicated already,

2916
03:14:52,040 --> 03:14:56,760
then some of you guys probably can handle and that I can probably explain. But that's kind of

2917
03:14:56,760 --> 03:15:00,760
the basic principle. We'll go back to the drawing board and we'll do a very quick recap before we

2918
03:15:00,760 --> 03:15:06,760
get into some of the other stuff, neural networks, input, output hidden layers connected with weights,

2919
03:15:06,760 --> 03:15:11,720
there's biases that connect to each layer. These biases can be thought of as y intercepts, they'll

2920
03:15:11,720 --> 03:15:17,480
simply move completely up or move completely down that entire, you know, activation function,

2921
03:15:17,480 --> 03:15:21,960
right, we're shifting things left or right, because this will allow us to get a better

2922
03:15:21,960 --> 03:15:26,600
prediction and have another parameter that we can train and add a little bit of complexity

2923
03:15:26,600 --> 03:15:32,120
to our neural network model. Now, the way that information is passed through these layers is

2924
03:15:32,120 --> 03:15:37,800
we take the weighted sum at a neuron of all of the connected neurons to it, we then add this

2925
03:15:37,800 --> 03:15:43,720
bias neuron, and we apply some activation function that's going to put this, you know, these values

2926
03:15:43,720 --> 03:15:48,600
in between two set values. So for example, when we talk about sigmoid, that's going to squish our

2927
03:15:48,600 --> 03:15:52,600
values between zero and one, when we talk about hyperbolic tangent, that's going to squish our

2928
03:15:52,600 --> 03:15:56,760
values between negative one and one. And when we talk about rectifier linear unit, that's going to

2929
03:15:56,760 --> 03:16:01,320
squish our values between zero and positive infinity. So we apply those activation functions,

2930
03:16:01,320 --> 03:16:06,040
and then we continue the process. So n one gets its value and two gets its value. And then finally,

2931
03:16:06,040 --> 03:16:09,480
we make our way to our output layer, we might have passed through some other hidden layers

2932
03:16:09,480 --> 03:16:14,200
before that. And then we do the same thing, we take the weighted sum, we add the bias,

2933
03:16:14,200 --> 03:16:19,880
we apply an activation function, we look at the output, and we determine whether we know we are

2934
03:16:19,880 --> 03:16:24,440
a class y or we are class z or whether this is the value we're looking for. And and that's how

2935
03:16:24,440 --> 03:16:29,880
it works. Now we're at the training process, right? So we're doing this now, that's kind of how this

2936
03:16:29,880 --> 03:16:34,280
worked when we were making a prediction. So when we're training, essentially, what happens is we

2937
03:16:34,280 --> 03:16:41,000
just make predictions, we compare those predictions to whatever these expected value should be using

2938
03:16:41,000 --> 03:16:46,840
this loss function. Then we calculate what's called a gradient, a gradient is the direction we need

2939
03:16:46,840 --> 03:16:51,560
to move to minimize this loss function. And this is where the advanced math happens and why I'm

2940
03:16:51,560 --> 03:16:56,760
kind of skimming over this aspect. And then we use an algorithm called back propagation, where we

2941
03:16:56,760 --> 03:17:01,720
step backwards through the network, and update the weights and biases, according to the gradient

2942
03:17:01,800 --> 03:17:07,640
that we calculated. Now that is pretty much how this works. So you know, the more info we have,

2943
03:17:08,440 --> 03:17:12,600
likely unless we're overfitting, but you know, if we have a lot of data, if we can keep feeding

2944
03:17:12,600 --> 03:17:17,240
the network, it starts off being really horrible, having no idea what's going on. And then as more

2945
03:17:17,240 --> 03:17:21,960
and more information comes in, it updates these weights and biases gets better and better sees

2946
03:17:21,960 --> 03:17:26,440
more examples. And after you know, a certain amount of epochs or certain amount of pieces of

2947
03:17:26,440 --> 03:17:30,760
information, our network is making better and better predictions and having a lower and lower

2948
03:17:30,760 --> 03:17:35,320
loss. And the way we will calculate how well our network is doing is by passing it, you know,

2949
03:17:35,320 --> 03:17:42,680
our validation data set, where it can say, okay, so we got an 85% accuracy on this data set, we're

2950
03:17:42,680 --> 03:17:47,240
doing okay, you know, let's tweak this, let's tweak that, let's do this. So the loss function,

2951
03:17:47,240 --> 03:17:52,680
the lower this is the better, also known as the cost function. And that is kind of neural networks

2952
03:17:52,680 --> 03:17:57,480
in a nutshell. Now I know this wasn't really in a nutshell, because it was 30 minutes long. But that

2953
03:17:57,480 --> 03:18:01,720
is, you know, as much of an explanation as I can really give you without going too far into the

2954
03:18:01,720 --> 03:18:06,040
mathematics behind everything. And again, remember, the activation function is to move us up in

2955
03:18:06,040 --> 03:18:10,760
dimensionality. The bias is another layer of complexity and a trainable parameter for our

2956
03:18:10,760 --> 03:18:17,000
network allows us to shift this kind of activation function left, right up, down. And yeah, that

2957
03:18:17,000 --> 03:18:23,400
is how that works. Okay, so now we have an optimizer. This is kind of the last thing on

2958
03:18:23,480 --> 03:18:27,880
how neural networks work. optimizer is literally just the algorithm that does the gradient descent

2959
03:18:27,880 --> 03:18:32,520
and back propagation for us. So I mean, you guys can read through some of them here, we'll be using

2960
03:18:33,320 --> 03:18:37,560
probably the atom optimizer for most of our examples, although there's, you know, lots of

2961
03:18:37,560 --> 03:18:42,200
different ones that we can pick from. Now this optimization technique, again, is just a different

2962
03:18:42,200 --> 03:18:45,560
algorithm. There's some of them are faster, some of them are slower, some of them work a little bit

2963
03:18:45,560 --> 03:18:50,120
differently. And we're not really going to get into picking optimizers in this course, because

2964
03:18:50,200 --> 03:18:54,600
that's more of an advanced machine learning technique. All right, so enough explaining,

2965
03:18:54,600 --> 03:19:01,480
enough math, enough drawings, enough talking. Now it is time to create our first official

2966
03:19:01,480 --> 03:19:06,200
neural network. Now these are the imports we're going to need. So import TensorFlow is TF from

2967
03:19:06,200 --> 03:19:10,200
TensorFlow import Keras again, so this does actually come with TensorFlow. I forget if I

2968
03:19:10,200 --> 03:19:15,800
said you need to install that before. My apologies and then import numpy as NP, import map plot

2969
03:19:15,880 --> 03:19:21,000
live dot pi plot as PLT. Alright, so I'm going to do actually similar thing to what I did before

2970
03:19:21,000 --> 03:19:25,160
where I'm kind of just going to copy some of this code into another notebook, just to make sure

2971
03:19:25,160 --> 03:19:29,880
that we can look at everything at the end, and then kind of step through the code step by step

2972
03:19:29,880 --> 03:19:35,960
rather than all of the text kind of happening here. Alright, so the data set, and the problem

2973
03:19:35,960 --> 03:19:40,920
we are going to consider for our first neural network is the fashion MNIST data set. Now the

2974
03:19:41,000 --> 03:19:46,680
fashion MNIST data set contains 60,000 images for training and 10,000 images for validating and

2975
03:19:46,680 --> 03:19:54,600
testing 70,000 images. And it is essentially pixel data of clothing articles. So what we're

2976
03:19:54,600 --> 03:19:59,880
going to do to load in this data set from Keras, this actually built into Keras, it's meant as

2977
03:19:59,880 --> 03:20:06,280
like a beginner, like testing training data set, we're going to say fashion underscore MNIST

2978
03:20:06,280 --> 03:20:12,920
equals Keras dot data sets dot fashion MNIST. Now this will get the data set object, and then we

2979
03:20:12,920 --> 03:20:18,920
can load that object by doing fashion MNIST dot load data. Now by doing this by having the tuples

2980
03:20:18,920 --> 03:20:25,480
train images train labels, test images test labels equals this, this will automatically split our

2981
03:20:25,480 --> 03:20:30,760
data into the sets that we need. So we need the training, and we need the testing. And again,

2982
03:20:30,760 --> 03:20:35,160
we've talked about all of that. So I'm going to kind of skim through that. And now we have it in

2983
03:20:35,160 --> 03:20:39,800
all of these kind of tuples here. Alright, so let's have a look at this data set to see what

2984
03:20:39,800 --> 03:20:42,920
we're working with. Okay, so let's run some of this code, let's get this import going,

2985
03:20:43,640 --> 03:20:49,880
if it doesn't take forever. Okay, let's get the data sets. Yeah, this will take a second to

2986
03:20:49,880 --> 03:20:54,360
download for you guys, if you don't already have it cached. And then we'll go train images dot

2987
03:20:54,360 --> 03:20:59,880
shape. And let's look at what one of the images looks like. Or sorry, what our data set looks

2988
03:20:59,960 --> 03:21:06,200
like. So we have 60,000 images that are 28 by 28. Now what that means is we have 28 pixels or 28

2989
03:21:06,200 --> 03:21:11,080
rows of 28 pixels, right? So that's kind of what our, you know, information is. So we're going to

2990
03:21:11,080 --> 03:21:19,240
have in total 784 pixels, which I've denoted here. So let's have a look at one pixel. So to reference

2991
03:21:19,240 --> 03:21:24,680
one pixel, this is what I what I'm doing, this comes in as a, actually, I'm not sure what type of

2992
03:21:24,680 --> 03:21:29,400
data frame this is. But let's have a look at it. So let's say type of train underscore images,

2993
03:21:29,400 --> 03:21:35,720
because I want to see that. So that's an numpy array. So to reference the different indexes in

2994
03:21:35,720 --> 03:21:40,760
this is similar to pandas, we're just going to do zero comma 23 comma 23, which stands for, you

2995
03:21:40,760 --> 03:21:48,360
know, image zero 23, and then 23. And this gives us one pixel. So row 23 column 23, which will be

2996
03:21:48,360 --> 03:21:55,480
that. Okay, so let's run this. And let's see this value is 194. Okay, so that's kind of interesting.

2997
03:21:55,480 --> 03:21:59,800
That's what one pixel looks like. So let's look at what multiple pixels look like. So we'll print

2998
03:22:00,440 --> 03:22:08,040
train underscore images. And okay, so we get all these zeros, let's print train images, zero,

2999
03:22:08,040 --> 03:22:13,880
colon, that should work for us. And we're getting all these zeros. Okay, so that's the border of

3000
03:22:13,880 --> 03:22:19,160
the picture. That's okay, I can't show you what I wanted to show you. Anyways, one pixel. And I

3001
03:22:19,160 --> 03:22:24,680
wanted to have you guys guess it is simply just represented by a number between zero and 255.

3002
03:22:25,240 --> 03:22:29,560
Now what this stands for is the grayscale value of this pixel. So we're dealing with grayscale

3003
03:22:29,560 --> 03:22:35,720
images, although we can deal with, you know, 3d, 4d, 5d images as well, or not 5d images,

3004
03:22:35,720 --> 03:22:40,440
but we can deal with images that have like RGB values. First, so for example, we could have

3005
03:22:40,440 --> 03:22:45,720
a number between zero 255, another number between zero and 255 and another number between zero and

3006
03:22:45,720 --> 03:22:51,960
255 for every single pixel, right? Whereas this one is just one simple static value. Okay, so it

3007
03:22:51,960 --> 03:22:57,640
says that here, a pixel values between zero and 255, zero being black and 255 being white. So

3008
03:22:57,640 --> 03:23:02,360
essentially, you know, if it's 255, that means that this is white, if it's zero, that means that

3009
03:23:02,360 --> 03:23:07,320
it is black. Alright, so let's have a look at the first 10 training labels. So that was our

3010
03:23:07,320 --> 03:23:12,200
training images. Now what are the training labels? Okay, so we have an array, and we get values from

3011
03:23:12,200 --> 03:23:19,160
zero to nine. Now this is because we have 10 different classes that we could have for our

3012
03:23:19,240 --> 03:23:24,040
dataset. So there's 10 different articles of clothing that are represented. I don't know what

3013
03:23:24,040 --> 03:23:29,320
all of them are, although they are right here. So t shirt, trouser, pullover, dress, coat, sandals,

3014
03:23:29,320 --> 03:23:35,400
shirt, sneaker, bag, ankle boot. Okay, so let's run this class names, just so that we have that

3015
03:23:35,400 --> 03:23:40,200
saved. And now what I'm going to do is use matplotlib to show you what one of the images looks like.

3016
03:23:40,200 --> 03:23:44,600
So in this case, this is a shirt. I know this is printing out kind of weird, but I'm just showing

3017
03:23:44,600 --> 03:23:48,280
the image. I know it's like different colors, but that's because if we don't define that we're

3018
03:23:48,280 --> 03:23:52,520
drawing a grayscale, it's going to do this. But anyways, that is what we get for the shirt. So

3019
03:23:52,520 --> 03:23:57,160
let's go to another image and let's have a look at what this one is. I actually don't know what

3020
03:23:57,160 --> 03:24:04,920
that is. So we'll skip that maybe that's a what is it t shirt or top. This I guess is going to be

3021
03:24:04,920 --> 03:24:10,840
like a dress. Yeah, so we do have dressed there. Let's go for have a look at this. And some of

3022
03:24:10,840 --> 03:24:16,040
these are like hard to even make out when I'm looking at them myself. And then I guess this

3023
03:24:16,120 --> 03:24:19,320
will be like a hoodie or something. I'm trying to get one of the sandals to show you guys a

3024
03:24:19,320 --> 03:24:23,720
few different ones. There we go. So that is a sandal or a sneaker. Okay, so that is kind of

3025
03:24:23,720 --> 03:24:27,880
how we do that and how we look at the different images. So if you wanted to draw it out, all you

3026
03:24:27,880 --> 03:24:32,600
do is just make a figure, you just show the image, do the color bar, which is just giving you this,

3027
03:24:32,600 --> 03:24:35,560
then you're going to say, I don't want to grid and then you can just show the image, right? Because

3028
03:24:35,560 --> 03:24:41,000
if you don't have this line here, and you show with the grid, oh, it's actually not showing the grid

3029
03:24:41,000 --> 03:24:45,000
that's interesting. Although I thought it was going to show me those pixelated grid. So I guess

3030
03:24:45,080 --> 03:24:50,040
you don't need that line. Alright, so data pre processing. Alright, so this is an important

3031
03:24:50,040 --> 03:24:54,920
step in neural networks. And a lot of times when we have our data, we have it in these like random

3032
03:24:54,920 --> 03:24:59,240
forms, or we're missing data, or there's information we don't know, or that we haven't seen. And

3033
03:24:59,240 --> 03:25:03,800
typically what we need to do is pre process it. Now, what I'm going to do here is squish all my

3034
03:25:03,800 --> 03:25:08,840
values between zero and one. Typically, it's a good idea to get all of your input values in a

3035
03:25:08,840 --> 03:25:13,560
neural network in between, like that range in between, I would say negative one and one is what

3036
03:25:13,640 --> 03:25:17,480
you're trying to do, you're trying to make your numbers as small as possible to feed to the neural

3037
03:25:17,480 --> 03:25:21,880
network. The reason for this is your neural network starts out with random weights and biases

3038
03:25:21,880 --> 03:25:27,240
that are in between the range zero and one, unless you change that value. So if you have massive

3039
03:25:27,240 --> 03:25:31,400
input information and tiny weights, then you're kind of having a bit of a mismatch, and you're

3040
03:25:31,400 --> 03:25:36,040
going to make it much more difficult for your network to actually classify your information,

3041
03:25:36,040 --> 03:25:40,040
because it's going to have to work harder to update those weights and biases to reduce how

3042
03:25:40,040 --> 03:25:45,800
large those values are going to be, if that makes any sense. So it usually is a good idea to

3043
03:25:45,800 --> 03:25:51,160
pre process these and make them in between the value of zero and one. Now, since we know that

3044
03:25:51,160 --> 03:25:55,960
we're just going to have pixel values that are in the range of 255, we can just divide by 255,

3045
03:25:55,960 --> 03:26:01,640
and that will automatically scale it down for us. Although it is extremely important that we do this

3046
03:26:01,640 --> 03:26:07,640
to not only the training images, but the testing images as well. If you just pre process your

3047
03:26:07,640 --> 03:26:12,760
training images, and then you pass in, you know, new data that's not pre processed, that's going to

3048
03:26:12,760 --> 03:26:17,000
be a huge issue. You need to make sure that your data comes in the same form. And that means when

3049
03:26:17,000 --> 03:26:22,360
we're using the model to to make predictions, whatever, you know, I guess it pixel data we

3050
03:26:22,360 --> 03:26:26,920
have, we need to pre process in the same way that we pre processed our other data. Okay,

3051
03:26:26,920 --> 03:26:31,800
so let's pre process that so train images and test images. And I'm just going to actually steal

3052
03:26:32,760 --> 03:26:36,680
some of this stuff here, and throw it in my other one before we get too far. So let's get this

3053
03:26:36,760 --> 03:26:42,520
data sets. And let's throw it in here, just so we can come back and reference all this together.

3054
03:26:42,520 --> 03:26:49,080
Let's go class names. We don't actually need the figures, a few things I can skip, we do need

3055
03:26:49,080 --> 03:26:56,040
this pre processing step. Like that, if I could go over here. And then what else do we need,

3056
03:26:56,040 --> 03:26:59,480
we're going to need this model. Okay, so let's actually just copy the model into this and just

3057
03:26:59,480 --> 03:27:05,160
make it a little bit cleaner. We can have a look at it. So new code block model. Okay, so model,

3058
03:27:05,880 --> 03:27:09,720
creating our model. Now creating our models actually really easy. I'm hoping what you guys

3059
03:27:09,720 --> 03:27:13,960
have realized so far is that data is usually the hardest part of machine learning and neural

3060
03:27:13,960 --> 03:27:18,040
networks, getting your data in the right form, the right shape and you know, pre processed

3061
03:27:18,040 --> 03:27:22,120
correctly, building the models usually pretty easy because we have tools like TensorFlow and Keras

3062
03:27:22,120 --> 03:27:27,320
that can do it for us. So we're going to say model equals Keras dot sequential. Now sequential

3063
03:27:27,320 --> 03:27:31,160
simply stands for the most basic form of neural network, which we've talked about so far, which

3064
03:27:31,160 --> 03:27:35,880
is just information going from the left side to the right side, passing through the layers,

3065
03:27:35,880 --> 03:27:40,680
sequentially, right, called sequential, we have not talked about recurrent or convolutional

3066
03:27:40,680 --> 03:27:46,920
neural networks yet. Now what we're going to do here is go Keras dot layers dot flat. So sorry,

3067
03:27:46,920 --> 03:27:51,240
inside here, we're going to define the layers that we want in our neural network. This first

3068
03:27:51,240 --> 03:27:57,720
layer is our input layer. And what flatten does is allows us to take in a shape of 28 by 28,

3069
03:27:58,360 --> 03:28:05,640
which we've defined here, and flatten all of the pixels into 784 pixels. So we take this 28 by 28

3070
03:28:05,640 --> 03:28:11,480
kind of matrix like structure, and just flatten it out. And Keras will do that for us, we don't

3071
03:28:11,480 --> 03:28:17,080
actually need to take our you know, matrix data and transform before passing. So we've done that.

3072
03:28:17,800 --> 03:28:26,280
Next, we have Keras dot layers dot dense 128 activation equals rectify linear unit. So this

3073
03:28:26,360 --> 03:28:32,760
is our first hidden layer, layer two, right, that's what I've denoted here. And this is a dense layer.

3074
03:28:32,760 --> 03:28:38,680
Now dense, again, means that all of the, what is it, the neurons in the previous layer are connected

3075
03:28:38,680 --> 03:28:44,680
to every neuron in this layer. So we have 828 neurons here. How do we pick that number? We

3076
03:28:44,680 --> 03:28:48,440
don't know, we kind of just came up with it. Usually, it's a good idea that you're going to do

3077
03:28:48,440 --> 03:28:53,160
this as like a little bit smaller than what your input layer is, although sometimes it's going to

3078
03:28:53,160 --> 03:28:56,760
be bigger, you know, sometimes it's going to be half the size, it really depends on the problem,

3079
03:28:56,760 --> 03:29:00,840
I can't really give you a straight answer for that. And then our activation function will

3080
03:29:00,840 --> 03:29:05,880
define as rectify linear unit. Now we could pick a bunch of different activation functions, there's

3081
03:29:05,880 --> 03:29:11,400
time we can pick sigmoid, we could pick 10 h, which is hyperbolic tangent, doesn't really matter.

3082
03:29:11,960 --> 03:29:16,200
And then we're going to define our last layer, which is our output layer, which is a dense layer

3083
03:29:16,200 --> 03:29:22,200
of 10 output neurons with the activation of softmax. Okay, so can we think of why we would have picked

3084
03:29:22,280 --> 03:29:26,440
10 here, right? I'll give you guys a second to think about it, based on the fact that our output

3085
03:29:26,440 --> 03:29:31,480
layer, you know, is supposed to have as many neurons as classes we're going to predict for.

3086
03:29:31,480 --> 03:29:37,560
So that is exactly what we have 10. If we look, we have 10 classes here. So we're going to have 10

3087
03:29:37,560 --> 03:29:43,480
output neurons in our output layer. And again, we're going to have this probability distribution.

3088
03:29:43,480 --> 03:29:49,160
And the way we do that is using the activation function softmax. So softmax will make sure

3089
03:29:49,240 --> 03:29:54,040
that all of the values of our neurons add up to one, and that they're between zero and one.

3090
03:29:54,760 --> 03:29:59,000
So that is our, our model, we've created the model now. So let's actually run this.

3091
03:30:00,840 --> 03:30:05,560
See, are we going to get any errors here? Is this going to run? And then we'll run the model,

3092
03:30:05,560 --> 03:30:09,320
and then we'll go on to the next step, which actually going to be training and testing the

3093
03:30:09,320 --> 03:30:14,040
model. Okay, so let's create the model now, shouldn't get any issues, and we're good. And now let's

3094
03:30:14,040 --> 03:30:19,080
move on to the next step. I'm forgetting what it is, though, which is training the model. Oh,

3095
03:30:19,080 --> 03:30:24,440
sorry, compiling the model. Okay, so compiling the model. So we've built now what we call the

3096
03:30:24,440 --> 03:30:28,920
architecture of our neural network, right, we've defined the amount of neurons in each layer,

3097
03:30:28,920 --> 03:30:33,880
we've defined the activation function, and we define the type of layer and the type of connections.

3098
03:30:33,880 --> 03:30:38,840
The next thing we need to pick is the optimizer, the loss and the metrics we're going to be looking

3099
03:30:38,840 --> 03:30:43,880
at. So the optimizer we're going to use is Adam. This is again, just the algorithm that performs

3100
03:30:43,880 --> 03:30:48,520
the gradient descent, you don't really need to look at these too much, you can read up on some

3101
03:30:48,520 --> 03:30:53,640
different activation functions or sorry, optimizers, if you want to kind of see the difference between

3102
03:30:53,640 --> 03:30:59,560
them, but that's not crazy, we're going to pick a loss. So in this case, sparse categorical cross

3103
03:30:59,560 --> 03:31:03,240
entropy, again, not going to go into depth about that, you guys can look that up if you want to

3104
03:31:03,240 --> 03:31:07,800
see how it works. And then metrics. So what we're looking for the output that we want to see from

3105
03:31:07,800 --> 03:31:13,800
the network, which is accuracy. Now from, you know, kind of right now with our current knowledge,

3106
03:31:13,800 --> 03:31:18,360
we're just going to stick with this as what we're going to compile our neural networks with,

3107
03:31:18,360 --> 03:31:23,720
we can pick different values if we want. And these are what we call, what is it hyper parameter

3108
03:31:23,720 --> 03:31:28,680
tuning. So the parameters that are inside here, so like the weights and the biases are things that

3109
03:31:28,680 --> 03:31:34,120
we can't manually change. But these are things that we can change, right, the optimizer, the loss,

3110
03:31:34,120 --> 03:31:38,840
the metrics, the activation function, we can change that. So these are called hyper parameters,

3111
03:31:38,840 --> 03:31:44,520
same thing with the number of neurons in each layer. So hyper parameter tuning is a process of

3112
03:31:44,520 --> 03:31:49,960
changing all of these values and looking at how models perform with different hyper parameters

3113
03:31:49,960 --> 03:31:53,720
change. So I'm not really going to talk about that too much. But that is something to note,

3114
03:31:53,720 --> 03:31:58,520
because you'll probably hear that, you know, this hyper parameter kind of idea. Okay, so we've

3115
03:31:58,520 --> 03:32:02,280
compiled the model now using this, which just means we've picked all the different things that

3116
03:32:02,280 --> 03:32:07,080
we need to use for it. And now on to training the model. So I'm just going to copy this in.

3117
03:32:08,040 --> 03:32:13,640
Again, remember this, these parts are pretty syntaxually heavy, but fairly easy to actually do.

3118
03:32:13,640 --> 03:32:18,120
So we're going to fit the model. So fit just means we're fitting it to the training data. It's

3119
03:32:18,120 --> 03:32:21,880
another word for training, essentially. So we're going to pass it the training images,

3120
03:32:21,880 --> 03:32:26,600
the training labels, and notice how much easier it is to pass this. Now, we don't need to do this

3121
03:32:26,600 --> 03:32:30,280
input function, we don't need to do all of that, because Keras can handle it for us. And we define

3122
03:32:30,280 --> 03:32:36,120
our epochs as 10 epochs is another hyper parameter that you could tune and change if you wanted to.

3123
03:32:36,680 --> 03:32:41,240
All right, so that will actually fit our model. So what I'm going to do is put this in another

3124
03:32:41,240 --> 03:32:46,360
code block. So I don't need to keep retraining this. So we'll go like that. And let's actually

3125
03:32:46,360 --> 03:32:50,840
look at this training process. So we've run the model, this should compile. And now let's fit

3126
03:32:50,840 --> 03:32:56,120
it and let's see what we actually end up getting. Alright, so epoch one, and we can see that we're

3127
03:32:56,120 --> 03:33:00,600
getting a loss and we're getting accuracy printing out on the side here. Now, this is going to take

3128
03:33:00,600 --> 03:33:05,400
a second, like this is going to take a few minutes, as opposed to our other models that we made are

3129
03:33:05,400 --> 03:33:10,440
not a few minutes, but you know, a few seconds, when you have 60,000 images, and you have a network

3130
03:33:10,440 --> 03:33:16,680
that's comprised of 784 neurons, 128 neurons, and then 10 neurons, you have a lot of weights and

3131
03:33:16,680 --> 03:33:21,160
biases and a lot of math that needs to go on. So this will take a few seconds to run. Now,

3132
03:33:21,160 --> 03:33:24,760
if you're on a much faster computer, you'll probably be faster than this. But this is why I

3133
03:33:24,760 --> 03:33:28,600
like Google Collaboratory, because you know, this isn't using any of my computer's resources

3134
03:33:28,600 --> 03:33:35,160
to train. It's using this. And we can see, like the RAM and the disk. How do I look at this?

3135
03:33:36,120 --> 03:33:40,680
In this network? Oh, is it going to let me look at this now? Okay, I don't know why it's not letting

3136
03:33:40,680 --> 03:33:45,400
me click this, but usually you can have a look at it. And now we've trained and we've fit the

3137
03:33:45,400 --> 03:33:51,240
model. So we can see that we have an accuracy of 91%. But the thing is, this is the accuracy

3138
03:33:51,960 --> 03:33:57,480
on or testing or our training data. So now if we want to find what the true accuracy is,

3139
03:33:58,040 --> 03:34:02,040
what we need to do is actually test it on our testing data. So I'm going to steal

3140
03:34:02,040 --> 03:34:06,520
this line of code here. This is how we test our model. Pretty straightforward. I'll just

3141
03:34:06,520 --> 03:34:12,120
close this. So let's go into code block. So we have test loss test accuracy is model dot

3142
03:34:12,120 --> 03:34:18,920
evaluate test images test labels verbose equals one. Now what is verbose? I was hoping it was

3143
03:34:18,920 --> 03:34:23,160
going to give me the thing so I could just read it to you guys. But verbose essentially is just

3144
03:34:23,160 --> 03:34:28,120
are we looking at output or not? So like how much information are we seeing as this model evaluates?

3145
03:34:28,760 --> 03:34:33,080
It's like how much is printing out to the console? That's what that means. And yes,

3146
03:34:33,080 --> 03:34:37,560
this will just split up kind of the metrics that are returned to this into test loss and test accuracy

3147
03:34:37,560 --> 03:34:43,640
so we can have a look at it. Now you will notice when I run this, that the accuracy will likely

3148
03:34:43,640 --> 03:34:48,840
be lower on this than it was on our model. So actually, the accuracy we had from this

3149
03:34:48,840 --> 03:34:54,680
was about 91. And now we're only getting 88.5. So this is an example of something we call

3150
03:34:54,680 --> 03:35:00,760
overfitting. Our model seemed like it was doing really well on the testing data or sorry,

3151
03:35:00,760 --> 03:35:06,440
the training data. But that's because it was seeing that data so often, right with 10 epochs,

3152
03:35:06,440 --> 03:35:12,280
it started to just kind of memorize that data and get good at seeing that data. Whereas now

3153
03:35:12,280 --> 03:35:18,040
when we pass it new data that it's never seen before, it's only 88.5% accurate, which means

3154
03:35:18,040 --> 03:35:23,160
we overfit our model. And it's not as good at generalizing for other data sets, which is usually

3155
03:35:23,160 --> 03:35:27,560
the goal, right? When we create a model, we want the highest accuracy possible, but we want the

3156
03:35:27,560 --> 03:35:34,440
highest accuracy possible on new data. So we need to make sure our model generalizes properly.

3157
03:35:34,440 --> 03:35:38,760
Now in this instance, you know, like, it's, it's hard to figure out how do we do that because

3158
03:35:38,760 --> 03:35:43,080
we don't know that much about neural networks. But this is the idea of overfitting and of

3159
03:35:43,080 --> 03:35:47,640
hyper parameter tuning, right? So if we can start changing some of this architecture, and we can

3160
03:35:47,640 --> 03:35:52,440
change maybe the optimizer, the loss function, maybe we go epochs eight, let's see if this

3161
03:35:52,440 --> 03:35:57,800
does any better, right? So let's now fit the model with eight epochs, we'll have a look at what this

3162
03:35:57,800 --> 03:36:02,840
accuracy is. And then we'll test it and see if we get a higher accuracy on the testing data set.

3163
03:36:02,840 --> 03:36:07,240
And this is kind of the idea of that hyper parameter tuning, right? Well, we just look at

3164
03:36:07,240 --> 03:36:12,680
each epoch, or not each epoch, we look at each parameter, we tweak them a little bit. And usually

3165
03:36:12,680 --> 03:36:17,960
we'll like write some code that automates this for us. But that's the idea is we want to get the most

3166
03:36:17,960 --> 03:36:23,240
generalized accuracy that we can. So I'll wait for this to train. We're actually almost done. So I

3167
03:36:23,240 --> 03:36:28,040
won't even bother cutting the video. And then we'll run at this evaluation. And we'll see now if we

3168
03:36:28,040 --> 03:36:31,640
got a better accuracy. Now I'm getting a little bit scared because the accuracy is getting very

3169
03:36:31,640 --> 03:36:36,040
high here. And sometimes, you know, like you want the accuracy to be high on your training data.

3170
03:36:36,120 --> 03:36:39,720
But when it gets to a point where it's very high, you're in a situation where it's likely

3171
03:36:39,720 --> 03:36:45,240
that you've overfit. So let's look at this now. And let's see what we get. So 88.4. So we actually

3172
03:36:45,240 --> 03:36:49,160
dropped down a little bit. And it seemed like those epochs didn't make a big difference. So maybe

3173
03:36:49,160 --> 03:36:54,920
if I train it on one epoch, let's have an idea and see what this does. You know, make your prediction,

3174
03:36:54,920 --> 03:36:58,440
you think we're going to be better, do you think we're going to be worse? It's only seen the training

3175
03:36:58,440 --> 03:37:07,080
data one time. Let's run this. And let's see 89.34. So in this situation, less epochs was actually

3176
03:37:07,080 --> 03:37:12,120
better. So that's something to consider. You know, a lot of people I see just go like 100 epochs and

3177
03:37:12,120 --> 03:37:16,760
just think their model is going to be great. That's actually not good to do. A lot of the

3178
03:37:16,760 --> 03:37:20,040
times you're going to have a worse model because what's going to end up happening is it's going to

3179
03:37:20,040 --> 03:37:26,520
be seeing the same information so much tweaking so specifically to that information that it's seen

3180
03:37:26,600 --> 03:37:30,920
that when you show it new data, it can't actually, you know, classify and generalize on that.

3181
03:37:31,640 --> 03:37:35,560
All right. So let's go back and let's see what else we're doing now with this. Okay,

3182
03:37:35,560 --> 03:37:39,320
so now that we've done that, we need to make predictions. So to make predictions is actually

3183
03:37:39,320 --> 03:37:44,520
pretty easy. So I'm actually just going to copy this line in, we'll go into a new code block down

3184
03:37:44,520 --> 03:37:49,880
here. So all you have to do is say model that predict, and then you're going to give it an array

3185
03:37:49,880 --> 03:37:54,520
of images that you want to predict on. So in this case, if we look at test images shape, so actually

3186
03:37:54,520 --> 03:38:01,480
let's make a new code block and let's go here. So let's say test underscore images dot shape.

3187
03:38:03,640 --> 03:38:10,040
All right, give me a second. So we have 10,000 by 28 by 28. So this is an array of 10,000 entries

3188
03:38:10,040 --> 03:38:15,720
of images. Now, if I just wanted to predict on one image, what I could do is say test images,

3189
03:38:15,720 --> 03:38:21,480
zero and then put that inside of an array. The reason I need to do that is because the data

3190
03:38:21,480 --> 03:38:26,840
that this model is used to see in is an array of images to make a prediction on, that's what this

3191
03:38:26,840 --> 03:38:31,720
predict method needs. And it's much better at making predictions on many things at once than

3192
03:38:31,720 --> 03:38:37,560
just one specific item. So if you are predicting one item only, you do need to put it in an array,

3193
03:38:37,560 --> 03:38:41,320
because it's used to seeing that form. So we could do this. I'm, I mean, I'm just going to leave it.

3194
03:38:41,320 --> 03:38:44,920
So we're just going to predict on every single one of the test images, because then we can have

3195
03:38:44,920 --> 03:38:49,160
a look at a cool function I've kind of made. So let's actually do this predictions equals model

3196
03:38:49,240 --> 03:38:55,640
dot predict test images. I mean, let's print predictions. And look at actually what it is.

3197
03:38:56,280 --> 03:39:01,320
Where is my autocomplete? There it is. Okay. So let's have a look. Is this some object? Whoa,

3198
03:39:01,320 --> 03:39:07,800
okay. So this is a raise of arrays that looks like we have some like really tiny numbers in them.

3199
03:39:07,800 --> 03:39:13,800
So what this is, is essentially every single, you know, prediction or every single image has

3200
03:39:13,800 --> 03:39:17,400
a list that represents the prediction for it, just like we've done with kind of the linear

3201
03:39:17,400 --> 03:39:22,360
models and stuff like that. So if I want to see the prediction for test image zero, I would say

3202
03:39:22,360 --> 03:39:27,720
prediction zero, right? Let's print this out. And this is the array that we're getting. These

3203
03:39:27,720 --> 03:39:33,960
this is the probability distribution that was calculated on our output layer for, you know,

3204
03:39:33,960 --> 03:39:39,080
these, what is it for that image? So if we want to figure out what class we actually think that

3205
03:39:39,080 --> 03:39:44,840
this is predicting for, we can use a cool function from NumPy called arg max, which essentially is

3206
03:39:44,920 --> 03:39:50,200
just going to take the index, this is going to return to us the index of the maximum value in

3207
03:39:50,200 --> 03:39:55,000
this list. So let's say that it was I'm looking for the least negative, which I believe is this,

3208
03:39:55,000 --> 03:40:00,760
so this should be nine, this should return to us nine, because this is the index of the highest

3209
03:40:00,760 --> 03:40:06,280
value in this list. Unless I'm just wrong when I'm looking at the negatives here. So nine, that's

3210
03:40:06,280 --> 03:40:12,200
what we got. Okay, so now if we want to see what the actual classes, while we have our class names

3211
03:40:12,200 --> 03:40:16,840
up here, so we know class nine is actually ankle boot. So let's see if this is actually an ankle

3212
03:40:16,840 --> 03:40:23,320
boot. So I'm just going to do class underscore names, I think that's what I called it, like this,

3213
03:40:23,320 --> 03:40:28,520
so that should print out what it thinks it is. Yeah, class underscore names. But now let's actually

3214
03:40:28,520 --> 03:40:33,560
show the image of this prediction. So to do that, I'm just going to steal some code from here because

3215
03:40:34,360 --> 03:40:37,960
I don't remember all the syntax off the top of my head. So this

3216
03:40:38,760 --> 03:40:43,640
is what it looks like. So let's steal this figure. Let's show this and let's see if it actually looks

3217
03:40:43,640 --> 03:40:49,640
like an ankle boot. So to do that, we're going to say test underscore images zero, because obviously

3218
03:40:49,640 --> 03:40:55,240
image zero corresponds to predict prediction zero. And that will show this and see what we get. Okay,

3219
03:40:55,240 --> 03:41:00,600
so ankle boot, and we'll be looking at the image is actually an ankle boot. And we can do this for

3220
03:41:00,600 --> 03:41:06,120
any of the images that we want, right? So if I do prediction one, prediction one, now let's have a

3221
03:41:06,120 --> 03:41:10,760
look pull over kind of looks like a pull over to me. I mean, I don't know if it actually is,

3222
03:41:10,760 --> 03:41:19,080
but that's what it looks like. You do to to have a look here. Okay, trouser. Yep, looks like trousers

3223
03:41:19,080 --> 03:41:23,160
to me. And we can see that that is how we get the predictions from our model, we use model dot

3224
03:41:23,160 --> 03:41:29,400
predict. Alright, so let's move down here now to the next thing that we did. Alright, so we've

3225
03:41:29,400 --> 03:41:33,720
already done that. So verifying predictions. Okay, so this is actually a cool kind of script

3226
03:41:33,720 --> 03:41:39,160
that I wrote, I'll zoom out a little bit so we can read it. What this does is let us use our model

3227
03:41:39,160 --> 03:41:44,520
to actually make. And I've stolen some of this from TensorFlow to make predictions on any

3228
03:41:44,520 --> 03:41:48,840
entry that we want. So what it's going to do is ask us to type in some number, we're going to type

3229
03:41:48,840 --> 03:41:53,240
in that number, it's going to find that image in the test data set, it's going to make your

3230
03:41:53,240 --> 03:41:58,600
prediction on that from the model, and then show us what it actually is versus what it was predicted

3231
03:41:58,600 --> 03:42:03,560
being. Now, I just need to actually run. Actually, let's just steal this code and bring it in the

3232
03:42:03,560 --> 03:42:06,840
other one, because I've already trained the model there. So we don't have to wait again. So let's

3233
03:42:06,840 --> 03:42:15,240
go f 11, f 11, let's go to a new code block, and run that. So let's run this script. Have a look

3234
03:42:15,240 --> 03:42:19,880
down here. So pick a number, we'll pick some number, let's go 45. And then what it's going to do is

3235
03:42:19,880 --> 03:42:25,320
say expected sneaker, guess sneaker, and actually show us the image that's there. So we can see

3236
03:42:25,320 --> 03:42:30,360
this is what you know, our pixel kind of data looks like. And this is what the expected was,

3237
03:42:30,360 --> 03:42:34,120
and this is what the guess was from the neural network. Now we can do the same thing if we run

3238
03:42:34,120 --> 03:42:41,000
it again, pick a number 34. Let's see here, expected bag, guess bag. So that's kind of showing you

3239
03:42:41,000 --> 03:42:47,480
how we can actually use this model. So anyways, that has been it for this kind of module on neural

3240
03:42:47,480 --> 03:42:52,120
networks. Now I did this in about an hour, I'm hoping I explained a good amount that you guys

3241
03:42:52,120 --> 03:42:56,760
understand now how neural networks work. In the next module, we're going to move on to convolutional

3242
03:42:56,760 --> 03:43:00,760
neural networks, which again should help, you know, kind of get your understanding of neural

3243
03:43:00,760 --> 03:43:05,480
networks up as well as learn how we can do deep computer vision, object recognition and detection

3244
03:43:05,480 --> 03:43:09,640
using convolutional neural networks. So that being said, let's get into the next module.

3245
03:43:12,920 --> 03:43:17,720
Hello, everyone, and welcome to the next module in this TensorFlow course. So what we're going

3246
03:43:17,720 --> 03:43:22,200
to be doing here is talking about deep computer vision, which is very exciting, very cool. This

3247
03:43:22,200 --> 03:43:26,520
has been used for all kinds of things you ever seen the self driving cars, for example, Tesla,

3248
03:43:26,520 --> 03:43:32,760
they actually use a TensorFlow deep learning model, obviously very complicated, more than I can

3249
03:43:32,760 --> 03:43:37,240
really explain here to do a lot of their computer vision for self driving, we've used computer vision

3250
03:43:37,240 --> 03:43:41,800
in the medicine field, computer vision is actually used in sports a lot for things like goal line

3251
03:43:41,800 --> 03:43:46,600
technology and even detecting images and players on the field doing analysis, there's lots of cool

3252
03:43:46,680 --> 03:43:50,440
things are doing with it nowadays. And for our purposes, what we're going to be doing is using

3253
03:43:50,440 --> 03:43:55,960
this for to perform classification, although it can be used for object detection and recognition,

3254
03:43:55,960 --> 03:44:00,360
as well as facial detection and recognition as well. So all kinds of applications, in my opinion,

3255
03:44:00,360 --> 03:44:05,160
one of the cooler things in deep learning that we're doing right now. And let's go ahead and talk

3256
03:44:05,160 --> 03:44:08,680
about what we're actually going to be focusing on here. So we're going to start by discussing what

3257
03:44:08,680 --> 03:44:13,560
a convolutional neural network is, which is essentially the way that we do deep learning,

3258
03:44:13,640 --> 03:44:17,080
we're going to learn about image data. So what's the difference between image data and other

3259
03:44:17,080 --> 03:44:21,800
regular data? We're going to talk about convolutional layers and pooling layers and how stacks of those

3260
03:44:21,800 --> 03:44:26,760
work together as what we call a convolutional base for our convolutional neural network. We're

3261
03:44:26,760 --> 03:44:31,800
going to talk about CNN architectures and get into actually using pre trained models that have been

3262
03:44:31,800 --> 03:44:36,040
developed by companies such as Google and TensorFlow themselves to perform classification

3263
03:44:36,040 --> 03:44:40,840
tasks for us. So that is pretty much the breakdown of what we're about to learn. There's quite a

3264
03:44:40,840 --> 03:44:44,520
bit in this module, it's probably the more difficult one or the most difficult one we've

3265
03:44:44,520 --> 03:44:48,200
been doing so far. So if you do get lost at any point, or you don't understand some of it,

3266
03:44:48,200 --> 03:44:52,360
don't feel bad, this stuff is very difficult. And I would obviously recommend reading through

3267
03:44:52,360 --> 03:44:55,960
some of the descriptions I have here in this notebook, which again, you can find from the

3268
03:44:55,960 --> 03:45:00,360
link in the description or looking up some things that maybe I don't go into enough enough depth

3269
03:45:00,360 --> 03:45:05,560
about in your own time, as I can't really spend, you know, 10, 11 hours explaining a convolutional

3270
03:45:05,640 --> 03:45:11,000
neural network. So let's now talk about image data, which is the first thing we need to understand.

3271
03:45:11,000 --> 03:45:16,200
So in our previous examples, what we did with when we had a neural network is we had two dimensional

3272
03:45:16,200 --> 03:45:20,360
data, right, we had a width and height when we were trying to classify some kind of images using

3273
03:45:20,360 --> 03:45:24,920
a dense neural network. And well, that's what we use two dimensions. Well, with an image, we

3274
03:45:24,920 --> 03:45:30,200
actually have three dimensions. And what makes up those dimensions? Well, we have a height, and we

3275
03:45:30,200 --> 03:45:34,280
have a width, and then we have something called a color channels. Now, it's very important to

3276
03:45:34,280 --> 03:45:38,440
understand this, because we're going to see this a lot as we get into convolutional networks, that

3277
03:45:38,440 --> 03:45:44,200
the same image is really represented by three specific layers, right? We have the first layer,

3278
03:45:44,200 --> 03:45:49,080
which tells us all of the red values of the pixels, the second layer, which tells us all the green

3279
03:45:49,080 --> 03:45:53,480
values, and the third layer, which tells us all the blue values. So in this case, those are the

3280
03:45:53,480 --> 03:45:58,040
covered channels. And we're going to be talking about channels in depth quite a bit in this series.

3281
03:45:58,040 --> 03:46:03,080
So just understand that although you think of an image as a two dimensional kind of thing,

3282
03:46:03,160 --> 03:46:07,800
and our computer, it's really represented by three dimensions where these channels are telling us

3283
03:46:07,800 --> 03:46:13,240
the color of each pixel. Because remember, in red, green, blue, you have three values for each pixel,

3284
03:46:13,240 --> 03:46:18,120
which means that you're going to need three layers to represent that pixel, right? So this is what

3285
03:46:18,120 --> 03:46:23,160
we can kind of think of it as a stack of layers. And in this case, a stack of pixels, right, or

3286
03:46:23,160 --> 03:46:27,400
stack of colors really telling us the value for each pixel. So if we were to draw this to the

3287
03:46:27,400 --> 03:46:33,880
screen, we would get the blue, green and red values of each pixel, determine the color of it,

3288
03:46:33,880 --> 03:46:39,000
and then draw the two dimensional image right based on the width and the height. Okay, so now

3289
03:46:39,000 --> 03:46:42,520
we're going to talk about a convolutional neural network and the difference between that in a dense

3290
03:46:42,520 --> 03:46:46,680
neural network. So in our previous examples, when we use the dense neural network to do some kind

3291
03:46:46,680 --> 03:46:52,280
of image classification, like that fashion, and this data set, what it essentially did was look

3292
03:46:52,280 --> 03:46:58,520
at the entire image at once and determined based on finding features in specific areas of the image,

3293
03:46:58,520 --> 03:47:03,240
what that image was, right? Maybe it found an edge here, a line here, maybe it found a shape,

3294
03:47:03,240 --> 03:47:07,560
maybe it found a horizontal diagonal line. The important thing to understand, though, is that

3295
03:47:07,560 --> 03:47:12,200
when it found these patterns and learned the patterns that made up specific shapes, it learned

3296
03:47:12,200 --> 03:47:17,400
them in specific areas. It knew that if we're in between, for example, looking at this cat image,

3297
03:47:17,480 --> 03:47:22,120
we're going to classify this as a cat, if an eye exists on, you know, the left side of the screen

3298
03:47:22,120 --> 03:47:27,480
where the eyes are here, then that's a cat. It doesn't necessarily know that if we flipped this

3299
03:47:27,480 --> 03:47:32,680
cat, we did a horizontal flip of this cat, and the eyes were over here, that that is a pattern that

3300
03:47:32,680 --> 03:47:37,720
makes up a cat. So the idea is that the dense network looks at things globally, it looks at the

3301
03:47:37,720 --> 03:47:43,640
entire image and learns patterns in specific areas. That's why we need things to be centered, we need

3302
03:47:43,640 --> 03:47:48,680
things to be very similar when we use a dense neural network to actually perform image classification,

3303
03:47:49,320 --> 03:47:54,280
because it cannot learn local patterns, and apply those to different areas of the image.

3304
03:47:54,280 --> 03:47:58,600
So for example, some patterns we might look for, when we're looking at an image like a cat here

3305
03:47:58,600 --> 03:48:03,000
would be something like this, right, we would hope that maybe we could find a few ears, we could find

3306
03:48:03,000 --> 03:48:10,200
the eyes, the nose, and you know, the paws here. And those features would tell us that this makes

3307
03:48:10,280 --> 03:48:15,400
up a cat. Now with a dense neural network, it would find these features, it would learn them,

3308
03:48:15,400 --> 03:48:20,120
learn these patterns, we only learn them in this specific area where they're boxed off,

3309
03:48:20,120 --> 03:48:23,960
which means if I horizontally flip this image, right, and I go like that,

3310
03:48:23,960 --> 03:48:27,960
then it's not going to know that that's a cat, because it learned that pattern in a specific

3311
03:48:27,960 --> 03:48:33,080
area, it'll need to relearn that pattern in the other area. Now a convolutional neural network,

3312
03:48:33,080 --> 03:48:38,120
on the other hand, learns local patterns. So rather than learning that the ear exists in,

3313
03:48:38,120 --> 03:48:42,680
you know, this specific location, it just learns that this is what an ear looks like,

3314
03:48:42,680 --> 03:48:47,480
and it can find that anywhere in the image. And we'll talk about how we do that as we get to the

3315
03:48:47,480 --> 03:48:51,880
explanation. But the whole point is that our convolutional neural network will scan through

3316
03:48:51,880 --> 03:48:57,720
our entire image, it will pick up features and find features in the image. And then based on the

3317
03:48:57,720 --> 03:49:02,920
features that exist in that image will pass that actually to a dense neural network or a dense

3318
03:49:02,920 --> 03:49:07,720
classifier, it will look at the presence of these features and determine, you know, the combination

3319
03:49:07,800 --> 03:49:12,280
of these presences of features that make up specific classes or make up specific objects.

3320
03:49:12,280 --> 03:49:16,520
So that's kind of the point. I hope that makes sense. The main thing to remember is that dense

3321
03:49:16,520 --> 03:49:21,400
neural networks work on a global scale, meaning they learn global patterns, which are specific and

3322
03:49:21,400 --> 03:49:27,320
are found in specific areas. Whereas convolutional neural networks or convolutional layers will

3323
03:49:27,320 --> 03:49:31,800
find patterns that exist anywhere in the image, because they know what the pattern looks like,

3324
03:49:31,800 --> 03:49:37,560
not that it just exists in a specific area. Alright, so how they work, right? So let's see

3325
03:49:37,560 --> 03:49:41,720
when a neural network, regular neural network looks at this dog image, this is a good example,

3326
03:49:41,720 --> 03:49:47,160
I should have been using this before, it will find that there's two eyes that exist here, right?

3327
03:49:47,160 --> 03:49:51,800
And we'll say, okay, so I found that these eyes make up a dog. This is its training image, for

3328
03:49:51,800 --> 03:49:55,880
example, and it's like, okay, so this pattern makes up the dog, the IR is in this location.

3329
03:49:56,680 --> 03:50:01,880
Now, what happens when we do this? And we flip the image to the other side. Well, our neural

3330
03:50:01,880 --> 03:50:05,640
network starts looking for these eyes, right on the left side of the image where it found them

3331
03:50:05,640 --> 03:50:10,040
previously and where it was trained on. It obviously doesn't find them there. And so it says

3332
03:50:10,040 --> 03:50:14,280
that our image isn't a dog, although it clearly is a dog, it's just a dog that's orientated

3333
03:50:14,280 --> 03:50:18,200
differently. In fact, it's just flipped horizontally, right? Or actually, I guess I would say

3334
03:50:18,200 --> 03:50:23,640
vertically, flip vertically. So since it doesn't find the eyes in this location, and it can only

3335
03:50:23,640 --> 03:50:28,200
look at patterns that it's learned in specific locations, it knows that this, or it's going

3336
03:50:28,200 --> 03:50:33,080
to say this isn't a dog, even though it is. Whereas our convolutional layer will find the eyes

3337
03:50:33,080 --> 03:50:37,720
regardless of where they are in the image, and still tell us that this is a dog, because even

3338
03:50:37,720 --> 03:50:41,480
though the dogs moved over, it knows what an eye looks like, so it can find the eye anywhere in

3339
03:50:41,480 --> 03:50:46,760
the image. So that's kind of the point of the convolutional neural network and the convolutional

3340
03:50:46,760 --> 03:50:52,040
layer. And what the convolutional layer does is look at our image and essentially feedback to us,

3341
03:50:52,040 --> 03:50:57,080
what we call an output feature map that tells us about the presence of specific features,

3342
03:50:57,080 --> 03:51:02,040
or what we're going to call filters in our image. So that is kind of the way that works.

3343
03:51:02,760 --> 03:51:07,160
Now, essentially, the thing we have to remember is that our dense neural networks output just a

3344
03:51:07,160 --> 03:51:11,640
bunch of numeric values. Whereas what our convolutional layers are actually going to be

3345
03:51:11,640 --> 03:51:16,040
doing is outputting what we call a feature map. Now I'm going to scroll down here to show you

3346
03:51:16,040 --> 03:51:20,840
this example. What we're actually going to do is run what we call a filter over our image,

3347
03:51:20,840 --> 03:51:24,280
we're going to sample the image at all these different areas. And then we're going to create

3348
03:51:24,280 --> 03:51:29,960
what we call an output feature map that quantifies the presence of the filters pattern at different

3349
03:51:29,960 --> 03:51:35,880
locations. And we'll run many, many, many different filters over our image at a time.

3350
03:51:35,880 --> 03:51:39,800
So we have all these different feature maps telling us about the presence of all these

3351
03:51:39,800 --> 03:51:44,920
different features. So one convolutional layer, we'll start by doing that with very small,

3352
03:51:44,920 --> 03:51:50,200
simple filters such as straight lines like this. And then other convolutional layers on top of

3353
03:51:50,200 --> 03:51:54,840
that, right, because it's going to return a map that looks something like this out of the layer,

3354
03:51:54,840 --> 03:51:59,480
we'll take this map in now, the one that was created from the previous layer, and say, okay,

3355
03:51:59,480 --> 03:52:03,320
what this map is representing to me, for example, the presence of these diagonal lines,

3356
03:52:03,880 --> 03:52:08,280
let me try to look for curves, right, or let me try to look for edges. So it will look at the

3357
03:52:08,280 --> 03:52:12,520
presence of the features from the previous convolutional layer, and then say, okay, well,

3358
03:52:12,520 --> 03:52:17,080
if I have all these lines combined together, that makes up an edge, and it will look for that,

3359
03:52:17,080 --> 03:52:21,320
right? And that's kind of the way that a convolutional neural network works and why we

3360
03:52:21,320 --> 03:52:25,400
stack these different layers. Now, we also use something called pooling, and there's a few

3361
03:52:25,400 --> 03:52:29,560
other things that we're going to get into. But that is the basics, I'm going to go into a drawing

3362
03:52:29,560 --> 03:52:34,200
example and show you exactly how that works. But hopefully this makes a little bit of sense

3363
03:52:34,200 --> 03:52:39,240
that the convolutional layer returns a feature map that quantifies the presence of a filter

3364
03:52:39,240 --> 03:52:44,200
at a specific location. And this filter, the advantage of it is that we slide it across

3365
03:52:44,200 --> 03:52:49,240
the entire image. So if this filter or this feature is presence anywhere in the image,

3366
03:52:49,240 --> 03:52:52,760
we will know about it rather than in our dense network, where it had to learn that pattern

3367
03:52:52,760 --> 03:52:58,120
in a specific global location. Okay, so let's get on the drawing tablet and do a few examples.

3368
03:52:58,120 --> 03:53:01,480
All right, so I'm here on my drawing tablet, and we're going to explain exactly how a

3369
03:53:01,480 --> 03:53:07,160
convolutional layer works, and how the network kind of works together. So this is an image I've

3370
03:53:07,160 --> 03:53:11,880
drawn on the left side of our screen here. I know this is very basic, you know, this is just an X,

3371
03:53:11,880 --> 03:53:15,080
right? This is what our images, we're just going to assume this is grayscale, we're going to avoid

3372
03:53:15,080 --> 03:53:20,120
doing anything with color channels the second just because they're not that important. But

3373
03:53:20,120 --> 03:53:24,040
just understand that what I'm going to show you does apply to color channels as well and to

3374
03:53:24,040 --> 03:53:29,080
multiple kind of layers and depth. And then if we can understand it on a simple level,

3375
03:53:29,080 --> 03:53:34,840
we should be able to understand it more thoroughly. So what we want essentially is our convolutional

3376
03:53:34,840 --> 03:53:40,120
layer to give us some output. It's meaningful about this image. So we're going to assume this is

3377
03:53:40,120 --> 03:53:45,400
the first convolutional layer. And what it needs to do essentially is return to us some feature

3378
03:53:45,400 --> 03:53:51,560
map that tells us about the presence of specific what we call filters in this image. So each

3379
03:53:51,560 --> 03:53:57,320
convolutional layer has a few properties to it. The first one is going to be the input size.

3380
03:53:58,360 --> 03:54:05,400
So what can we expect? Wow, what is that that was as the as the input size, how many filters

3381
03:54:05,400 --> 03:54:10,760
are we going to have so filters like this? And what's the sample size of our filters?

3382
03:54:11,720 --> 03:54:17,400
That's what we need to know for each of our convolutional neural networks. So essentially,

3383
03:54:17,400 --> 03:54:21,080
what is a filter? Well, a filter is just some pattern of pixels. And we saw them before,

3384
03:54:21,080 --> 03:54:26,440
we'll do a pretty basic one here, as the filter we're going to look for, which will look something

3385
03:54:26,440 --> 03:54:32,280
like this. This will be the first filter we're going to look for just to illustrate how this works.

3386
03:54:32,280 --> 03:54:37,480
But the idea is that at each convolutional layer, we look for many different filters. And in fact,

3387
03:54:37,480 --> 03:54:44,360
the number we're typically looking for is actually about times 32 filters. Sometimes we have 64

3388
03:54:44,360 --> 03:54:50,120
filters as well. And sometimes even 128. So we can do as many filters as we want, as few filters

3389
03:54:50,120 --> 03:54:55,800
as we want. But the filters are what is going to be trained. So this filter is actually what is

3390
03:54:55,800 --> 03:55:01,320
going to be found by the neural network. It's what's going to change. It's, you know, this is

3391
03:55:01,320 --> 03:55:05,000
essentially what we're looking for. This is what's created in the program. And that's kind of like

3392
03:55:05,000 --> 03:55:11,080
the trainable parameter of a convolutional neural network is the filter. So the amount of filters

3393
03:55:11,080 --> 03:55:16,680
and what they are will change as the program goes on, as we're learning more and figuring out what

3394
03:55:16,680 --> 03:55:21,480
features that make up, you know, a specific image. So I'm going to get rid of this stuff right now,

3395
03:55:21,480 --> 03:55:26,520
just so we can draw and do a basic example. But I want to show you how we look for a filter in the

3396
03:55:26,520 --> 03:55:31,480
image. So we have filters, right, they'll come up with them, they're gonna start completely random,

3397
03:55:31,480 --> 03:55:35,240
but they'll change as we go on. So let's say the filter we're looking for is that one I drew

3398
03:55:35,240 --> 03:55:38,360
before, I'm just going to redraw it at the top here a little bit smaller. And we'll just say

3399
03:55:38,360 --> 03:55:43,800
it's a diagonal line, right? But another filter we could look for might be something like, you know,

3400
03:55:43,800 --> 03:55:48,840
a straight line, just like that all across, we could have a horizontal line. And in fact, we'll

3401
03:55:48,840 --> 03:55:53,400
have 32 of them. And when we're doing just, you know, three by three grids of filters, well,

3402
03:55:54,040 --> 03:55:57,640
there's not that many, you know, combinations, we're going to do at least grayscale wise.

3403
03:55:58,200 --> 03:56:02,680
So what we'll do is we'll define the sample size, which is how big our filter is going to be three

3404
03:56:02,680 --> 03:56:06,600
by three, which we know right now, which means that what we're going to do is we're going to look

3405
03:56:06,600 --> 03:56:14,040
at three by three spots in our image, and look at the pixels, and try to find how closely these

3406
03:56:14,040 --> 03:56:19,000
filters match with the pixels we're looking at on each sample. So what this is going to do,

3407
03:56:19,000 --> 03:56:23,880
this convolutional layer is going to output us what we call a feature map, which can be a little

3408
03:56:23,880 --> 03:56:28,840
bit smaller than the original image. And you'll see why in a second, but that tells us about the

3409
03:56:28,840 --> 03:56:34,120
presence of specific features in areas of the image. So since we're looking for two filters here,

3410
03:56:34,120 --> 03:56:37,800
actually, we'll do two filters, which means that we're actually going to have a depth to

3411
03:56:38,920 --> 03:56:42,920
feature map being returned to us, right? Because for two filters, that means we need two maps,

3412
03:56:43,560 --> 03:56:48,760
quantifying the presence of both of those filters. So for this green box that we're looking on at

3413
03:56:48,760 --> 03:56:53,880
the left side here, we'll look for this first filter here. And what do we get? Well, the way we

3414
03:56:53,880 --> 03:56:58,680
actually do this, the way we look at this filter, is we take the cross product, or actually not the

3415
03:56:58,680 --> 03:57:03,480
cross product, the dot product, sorry, between this little green box and this filter, right,

3416
03:57:03,480 --> 03:57:08,040
because they're both pixels, they're both actually numeric values down at the bottom. So what we do

3417
03:57:08,040 --> 03:57:13,880
is we take that dot product, which essentially means we're element wise, adding, or what is it

3418
03:57:13,880 --> 03:57:18,920
element wise, multiplying all of these pixels by each other. So if this pixel values is zero,

3419
03:57:18,920 --> 03:57:22,840
right, because it's white, or it could be the other way around, we could say white is one,

3420
03:57:22,840 --> 03:57:27,720
black is zero, it doesn't really matter, right? If this is a zero, and this is a one, these are

3421
03:57:27,720 --> 03:57:32,680
obviously very different. And when we do the dot product of those two, so we multiply them together,

3422
03:57:32,680 --> 03:57:36,920
then in our output feature, we would have a zero, right, that's kind of the way it works. So we do

3423
03:57:36,920 --> 03:57:42,040
this dot product of this entire thing. If you don't know the dot product is I'm not really going to

3424
03:57:42,040 --> 03:57:45,560
go into that. But we do the dot product, and that gives us some value essentially telling us how

3425
03:57:45,560 --> 03:57:50,680
similar these two blocks are. So how similar this sample is that we're taking of the image and the

3426
03:57:50,680 --> 03:57:55,160
filter that we're looking for. They're very similar, we're going to likely put a one or something telling

3427
03:57:55,160 --> 03:57:59,240
us, you know, they're very close together. They're not similar at all, we're going to put a zero. So

3428
03:57:59,240 --> 03:58:03,400
in this case, for our first filter, we're probably going to have a value because this middle pixel

3429
03:58:03,400 --> 03:58:08,600
is the same as something like 0.12, right? But the all the other values are different. So it's

3430
03:58:08,600 --> 03:58:14,440
not going to be very similar whatsoever. So then what we're going to do now is we'll look at the

3431
03:58:14,440 --> 03:58:19,400
actually second filter, which is this horizontal line. And in fact, we're going to get a very

3432
03:58:19,400 --> 03:58:23,960
similar output response here, probably something like, you know, 0.12, that's going to go in the

3433
03:58:23,960 --> 03:58:29,720
top left. And again, these are both maps representing each filter, right? So now we'll move our green

3434
03:58:29,720 --> 03:58:37,560
box over one, like this. So just shift that over one. And now we'll start looking at the next section.

3435
03:58:37,560 --> 03:58:41,720
And in fact, I'm going to see if I can erase this just to make it a little bit cleaner here.

3436
03:58:44,120 --> 03:58:48,360
Get rid of the green, there we go. Okay, so we'll move this box over like this. And now we'll

3437
03:58:48,360 --> 03:58:52,520
start looking at this one, we'll do the exact same thing we did again before. So we're going to say,

3438
03:58:52,520 --> 03:58:56,520
alright, how similar are these? Well, they're not similar at all. So we're going to get a zero for

3439
03:58:56,520 --> 03:59:01,960
that first filter. How similar the other ones? Oh, actually, they're like a little bit similar.

3440
03:59:01,960 --> 03:59:05,480
There's a lot of white that's kind of in the same space, like, you know, stuff like that. So we'll

3441
03:59:05,480 --> 03:59:10,760
say maybe this is like 0.7, right? I'm just randomly picking these numbers. They are going to be much

3442
03:59:10,760 --> 03:59:15,000
different than what I'm putting in here. But I'm just trying to get you to understand what's kind

3443
03:59:15,000 --> 03:59:18,520
of happening, right? And this is completely random, the way I'm making the numbers to just make sure

3444
03:59:18,520 --> 03:59:21,880
you understand that because this is not exactly what it would look like. Okay, so then we're going

3445
03:59:21,880 --> 03:59:25,160
to move the box over one more time. Let's just erase this to keep this clean. This will be the

3446
03:59:25,160 --> 03:59:29,800
last time we do this for the purpose of this example. And now what we're going to have is wow,

3447
03:59:29,800 --> 03:59:34,040
we have a perfect match for the first filter. So we put one, the other ones like ads kind of

3448
03:59:34,120 --> 03:59:38,040
similar as a few things that are different. So maybe this gets like a 0.4 or something, right?

3449
03:59:38,040 --> 03:59:41,160
Whatever they are, we end up getting some value. So we'll fill in all these values, let's just

3450
03:59:41,160 --> 03:59:46,280
put some arbitrary values here for now, just so we can do something with the example 0.7, 0,

3451
03:59:47,000 --> 04:00:00,040
0.12, 0.42, 0.3, 0.9, 0.1, again, completely random, 0.4, 0.6. Alright, so this is now what

3452
04:00:00,040 --> 04:00:06,120
we've gotten our response map from looking at two filters on our original image of five by five.

3453
04:00:06,120 --> 04:00:10,280
Now notice that the size of these is three by three. And obviously, the reason for that is

3454
04:00:10,280 --> 04:00:15,160
because in a five by five image, when we're taking three by three samples, well, we can only take

3455
04:00:15,160 --> 04:00:20,040
nine three by three samples, because when we go down a row, right, we're going to move down one,

3456
04:00:20,040 --> 04:00:23,240
and we're going to do the same thing we did before of this, these three by three samples. And if we

3457
04:00:23,240 --> 04:00:28,280
add the amount of times we can do that, well, we just get three by three, which is not. So this

3458
04:00:28,280 --> 04:00:33,560
now is kind of telling us of the presence of features in this original image map. Now the

3459
04:00:33,560 --> 04:00:39,400
thing is, though, we're going to do this 64 times, right, for 64 filters or 32 filters or the amount

3460
04:00:39,400 --> 04:00:43,960
of filters that we have. So we're going to have a lot of layers, like a ton of different layers,

3461
04:00:43,960 --> 04:00:48,200
which means that we're going to be constantly expanding as we go through the convolutional

3462
04:00:48,200 --> 04:00:54,360
layers, the depth of this, this kind of output feature map. And that means that there's a lot

3463
04:00:54,360 --> 04:00:57,560
of computations that need to be done. And essentially, that means that this can be very

3464
04:00:57,560 --> 04:01:02,600
slow. So now we need to talk about an operation called pooling. So I'll backtrack a little bit,

3465
04:01:02,600 --> 04:01:06,600
but we will talk about pooling in a second. What's going to happen, right, is when we have all these

3466
04:01:06,600 --> 04:01:12,040
layers that are generated, so this is called the output feature map, right, from this original

3467
04:01:12,040 --> 04:01:16,760
image. What we're going to do is the next convolutional layer in the network is now going

3468
04:01:16,760 --> 04:01:22,040
to do the process we just talked about, except on this output feature map, which means that

3469
04:01:22,120 --> 04:01:27,960
since this one was picking up things like lines and edges, right, the next convolutional layer,

3470
04:01:27,960 --> 04:01:34,120
we'll pick up combinations of lines and edges and maybe find what a curve is, right, we'll slowly

3471
04:01:34,120 --> 04:01:39,640
work our way up from very, very small amount of pixels, defining more and more, almost I want to

3472
04:01:39,640 --> 04:01:45,720
say abstract, different features that exist in the image. And this is what really allows us to

3473
04:01:45,720 --> 04:01:49,160
do some amazing things with a convolutional neural network, when we have a ton of different

3474
04:01:49,160 --> 04:01:53,480
layers stacking up on each other, we can pick out all the small little edges, which are pretty

3475
04:01:53,480 --> 04:01:59,160
easy to find. And with all these combinations of layers working together, we can even find things

3476
04:01:59,160 --> 04:02:07,160
like, say, eyes, right, or feet, or heads or face, right, we can find very complicated structures,

3477
04:02:07,160 --> 04:02:11,880
because we slowly work our way up starting by solving very easy problem, which are like finding

3478
04:02:11,880 --> 04:02:16,840
lines, and then finding combinations of lines, combination of edges, shapes and very abstract

3479
04:02:16,920 --> 04:02:22,040
things. That's how this convolutional network works. So we've done that now, it's now time to

3480
04:02:22,040 --> 04:02:26,840
talk about pooling. And we'll also talk about pat actually, we'll go padding first before we go

3481
04:02:26,840 --> 04:02:30,840
pooling, I just, it doesn't really matter what order we talk about this in. But I just think

3482
04:02:30,840 --> 04:02:35,960
padding makes sense based on the way we're going right now. So sometimes we want to make sure that

3483
04:02:35,960 --> 04:02:43,720
the output feature map from our original image here is the same dimensions or same size as this,

3484
04:02:43,720 --> 04:02:48,680
right? So this was five by five, obviously. And this is three by three. So if we want this to be

3485
04:02:48,680 --> 04:02:53,320
five by five, as an output, what we need to do is add something called padding to our original

3486
04:02:53,320 --> 04:02:58,120
image. So padding is essentially just adding an extra row and column on each side of our image

3487
04:02:58,120 --> 04:03:03,320
here. So that when we, and we just fill in all these pixels in like kind of the padded pixels

3488
04:03:03,320 --> 04:03:08,440
here, I just blank random pixels, they don't mean anything. Essentially, why we do that is so that

3489
04:03:08,520 --> 04:03:15,080
when we do our three by three sample size here like this, we can take a three by three sample

3490
04:03:15,080 --> 04:03:20,840
where every single pixel is in the center of that sample. Because right now, this pixel is not in

3491
04:03:20,840 --> 04:03:25,880
the center. This pixel can never be in the center, this pixel can never be in the center, only,

3492
04:03:25,880 --> 04:03:30,840
you know, a few pixels get to be in the center. And what this allows us to do is generate an output

3493
04:03:30,840 --> 04:03:35,880
map that is the same size as our original input, and allows us to look at features that are maybe

3494
04:03:35,880 --> 04:03:40,680
right on the edges of images that we might not have been able to see before. Now, this isn't

3495
04:03:40,680 --> 04:03:44,280
super important when you go to like very large images, but it just something to consider you

3496
04:03:44,280 --> 04:03:49,240
can add padding, we may do this as we get through our examples. And there's also something called

3497
04:03:49,240 --> 04:03:54,360
stride, which I want to talk about as well. So what a stride is is essentially how much we move

3498
04:03:54,360 --> 04:04:00,360
this sample box, every time that we're about to move it, right? So before, like, so let's say

3499
04:04:00,360 --> 04:04:04,440
we're doing example with padding here, right, we are first sample, we would take here. And again,

3500
04:04:04,440 --> 04:04:09,720
these pixels are just added, we added them in to make this work better for us. You would assume

3501
04:04:09,720 --> 04:04:13,640
that the next time we move the box, we're going to move it one pixel over, that's called a stride of

3502
04:04:13,640 --> 04:04:18,760
one, we can do that. But we also can employ stride of two, which means we'll move over by two.

3503
04:04:19,400 --> 04:04:23,560
Obviously, the larger your stride, the smaller your output feature map is going to be. So you

3504
04:04:23,560 --> 04:04:27,000
might want to add more padding. Well, you don't want to add too much padding, but it's just

3505
04:04:27,000 --> 04:04:32,360
something to consider. And we will use a stride in different instances. Okay, so that's great.

3506
04:04:32,360 --> 04:04:36,200
That hopefully makes sense. Let's erase this. Now we don't need this anymore. We talked about

3507
04:04:36,200 --> 04:04:39,800
padding, we talked about the stride. And now we're going to talk about a pooling operation,

3508
04:04:39,800 --> 04:04:45,560
which is very important. So kind of the idea is that we're going to have a ton of layers, right,

3509
04:04:45,560 --> 04:04:49,320
for all these filters. And we're just going to have a lot of numbers, a lot of computations. And

3510
04:04:49,320 --> 04:04:52,680
there must be some way to make these a little bit simpler, a little bit easier to use. Well,

3511
04:04:52,680 --> 04:04:57,240
yes, that's true. And there is a way to do that. And that's called pooling. So there's three different

3512
04:04:57,240 --> 04:05:04,040
types of pooling. Well, it is more but the basic ones are min, max, and average. And essentially,

3513
04:05:04,040 --> 04:05:11,160
a pooling operation is just taking specific values from a sample of the output feature map. So

3514
04:05:11,160 --> 04:05:16,280
once we generate this output feature map, what we do to reduce its dimensionality and just make it

3515
04:05:16,280 --> 04:05:23,240
a little bit easier to work with is what we sample typically two by two areas of this output feature

3516
04:05:23,240 --> 04:05:29,640
map. And just take either the min, max or average value of all the values inside of here and map

3517
04:05:29,640 --> 04:05:35,480
these, we're going to go back this way to a new feature map that's twice, like one times the size

3518
04:05:35,480 --> 04:05:39,240
essentially, or not, what am I saying, two times smaller than this original map. And it's kind of

3519
04:05:39,240 --> 04:05:42,840
hard with three, like three by three, to really show you this. But essentially, what's going to

3520
04:05:42,840 --> 04:05:47,640
end up happening is we're going to have something like this. So we're going to take the sample here,

3521
04:05:47,640 --> 04:05:51,320
we're going to say, okay, what are we doing min, max or average pooling, if we're doing min pooling,

3522
04:05:51,400 --> 04:05:54,920
we're going to take the smallest value, which means we'll take zero. If we're doing max pooling,

3523
04:05:54,920 --> 04:05:59,320
we'll take the maximum value, which means we'll take 0.3. If we're doing average, we're probably

3524
04:05:59,320 --> 04:06:04,440
going to get an average value of close to what 0.2, maybe. So let's say 0.2, we'll go there.

3525
04:06:05,000 --> 04:06:10,200
That's how we do that with pooling. Again, just to make this feature map smaller. So we'll do that

3526
04:06:10,200 --> 04:06:16,120
for both of the filters. But let's just say this is 0.2. Let's say this here that I'm blocking off

3527
04:06:16,120 --> 04:06:23,160
is, I don't know, what is this going to be 0.6? It's hard to do this average with four numbers,

3528
04:06:23,160 --> 04:06:31,240
let's say this one down here is going to be 0. I don't know, let's just do two one or something.

3529
04:06:31,240 --> 04:06:35,240
And then this last one here, we okay, we got some bigger values, maybe this will be like 0.4.

3530
04:06:36,360 --> 04:06:41,000
Okay, so that's one, the one down here will have some values of its own, we'll just do squiggles

3531
04:06:41,000 --> 04:06:45,800
to represent that it has something. And we've effectively done a max pooling operation on

3532
04:06:45,800 --> 04:06:51,160
this, we've reduced the size of it by about half. And that is kind of how that works. Now,

3533
04:06:51,160 --> 04:06:57,160
typically what we do is we use a two by two pooling or like sample size like that, with a

3534
04:06:57,160 --> 04:07:02,200
stride of two, which actually means that we would straw it like this. But since we're not going to

3535
04:07:02,200 --> 04:07:07,320
do padding on this layer right now, we'll just do a stride of one. And this is how we pull it.

3536
04:07:07,320 --> 04:07:10,600
Now the different kinds of pooling are used for different kind of things. The reason we would

3537
04:07:10,600 --> 04:07:17,400
use a max pooling operation is to pretty much tell us about the maximum presence of a feature in that

3538
04:07:17,400 --> 04:07:23,240
kind of local area. We really only care if the feature exists, where if it doesn't exist, an

3539
04:07:23,240 --> 04:07:27,640
average pooling is not very often used, although in this case, we did use an average pooling.

3540
04:07:28,360 --> 04:07:31,800
But you know, it's just different kinds of pooling and average tells you about the average

3541
04:07:31,800 --> 04:07:36,920
presence of the feature in that area. Max tells you about is that feature present in that area

3542
04:07:36,920 --> 04:07:40,920
at all and men tells you does it not exist. If it doesn't exist, right, we're just going to have a

3543
04:07:40,920 --> 04:07:46,040
zero if there's even one zero in that area. So that's the point of pooling. That's the point of

3544
04:07:46,040 --> 04:07:49,480
convolutional layers. I think I'm done with the white boarding for now, we're actually going to

3545
04:07:49,480 --> 04:07:53,960
start getting into a little bit of code and talking about creating our own convolutional networks,

3546
04:07:53,960 --> 04:07:58,040
which hopefully will make this a lot more clear. So let's go ahead and get into that. All right,

3547
04:07:58,040 --> 04:08:01,960
so now it is time to create our first convolutional neural network. Now we're going to be using

3548
04:08:01,960 --> 04:08:07,480
Keras to do this. And we're also going to be using the CI FAR image data set that contains 60,000

3549
04:08:07,480 --> 04:08:12,760
images of 10 different classes of everyday objects. Now these images are 32 by 32, which

3550
04:08:12,760 --> 04:08:18,760
essentially means they are blurs, and they are colorful. Now, I just want to emphasize as we

3551
04:08:18,760 --> 04:08:23,080
get into this, that the reason I'm not typing all of these lines out and I just have them in here

3552
04:08:23,080 --> 04:08:27,400
already is because this is likely what you guys will be using or doing when you actually make

3553
04:08:27,400 --> 04:08:32,280
your own models. Chances are that you are not going to sit unless you're a pro at TensorFlow,

3554
04:08:32,280 --> 04:08:37,000
and I am not even there yet either with my knowledge of it, and have all of the lines memorized and

3555
04:08:37,000 --> 04:08:41,720
not have to go reference the syntax. So the point is here, so long as you can understand why this

3556
04:08:41,720 --> 04:08:46,200
works and what these lines are doing, you're going to be fine, you don't need to memorize them and I

3557
04:08:46,200 --> 04:08:50,360
have not memorized them. And I don't I look up for the documentation, I copy and paste what I need,

3558
04:08:50,360 --> 04:08:54,120
I alter them, I write a little bit of my own code. But that's kind of what you're going to end up

3559
04:08:54,120 --> 04:08:59,400
doing. So that's what I'm doing here. So this is the image data set. We have truck, horse, ship,

3560
04:08:59,400 --> 04:09:03,720
airplane, you know, just some everyday, regular objects, there is 60,000 images, as we said,

3561
04:09:03,720 --> 04:09:09,880
and 6000 images of each class. So we don't have too many images of just one specific class. So

3562
04:09:09,880 --> 04:09:15,160
we'll start by importing our modules. So TensorFlow, we're going to import TensorFlow dot Keras,

3563
04:09:15,160 --> 04:09:20,040
we're going to use the data set built into Keras for this. So that's the CI FR image data set,

3564
04:09:20,040 --> 04:09:23,720
which you can actually look at just by clicking at this, it'll bring you and give the information

3565
04:09:23,720 --> 04:09:28,200
about the data set. Although we don't need that right now, because I already know the information

3566
04:09:28,200 --> 04:09:32,920
about it. And now we're just going to load our images in. So again, this stuff, the way this

3567
04:09:32,920 --> 04:09:39,080
works is you're gonna say data sets dot CI FR 10 dot load data. Now this loads it in as like a very

3568
04:09:39,080 --> 04:09:43,960
strange TensorFlow object, that's like a data set object. So this is different from what we've

3569
04:09:43,960 --> 04:09:49,400
used before, where some of our objects have actually been like in NumPy arrays, where we can look at

3570
04:09:49,400 --> 04:09:53,320
them better. This is not going to be in that. So just something to keep in mind here. And we're

3571
04:09:53,320 --> 04:09:58,680
going to normalize this data into train images and test images, but just dividing both of them by

3572
04:09:58,680 --> 04:10:03,240
255. Now, again, we're doing that because we want to make sure that our values are between zero and

3573
04:10:03,240 --> 04:10:07,000
one, because that's just a lot better to work with in our neural networks, rather than large

3574
04:10:07,000 --> 04:10:11,960
integer values, just causes, you know, some things to mess up sometimes. Now class names,

3575
04:10:11,960 --> 04:10:15,400
we're just going to find a list here. So we have all the class names so that zero represents

3576
04:10:15,400 --> 04:10:21,880
airplane one auto avail so far in tilt truck, run that block of code here, we'll download this

3577
04:10:21,880 --> 04:10:26,120
data set, although I don't think it takes that long to do that. So okay, so wait, I guess,

3578
04:10:26,120 --> 04:10:30,520
yeah, I guess that's good. I think we're okay there. And now let's just have a look at actually

3579
04:10:30,520 --> 04:10:35,160
some of the images here by running this script. So we can see this is a truck can change the

3580
04:10:35,160 --> 04:10:41,560
image index to be two, we can see this is another truck. Let's go to say six, we get a bird. And

3581
04:10:41,560 --> 04:10:44,920
you can see these are really blurry. But that's fine. For this example, we're just trying to get

3582
04:10:44,920 --> 04:10:49,400
something that works all right. Okay, so that's a horse, you know, you get the point. All right.

3583
04:10:49,400 --> 04:10:54,440
So now CNN architecture. So essentially, we've already talked about how a convolutional neural

3584
04:10:54,440 --> 04:10:58,520
network works, we haven't talked about the architecture and how we actually make one.

3585
04:10:58,520 --> 04:11:03,160
Essentially, what we do is we stack a bunch of convolutional layers and max pooling,

3586
04:11:03,160 --> 04:11:08,040
min pooling or average pooling layers together in something like this, right. So after each

3587
04:11:08,040 --> 04:11:12,680
convolutional layer, we have a max pooling layer, some kind of pooling layer typically to reduce

3588
04:11:12,680 --> 04:11:16,680
the dimensionality, although you don't need that, you could just go straight into three

3589
04:11:16,760 --> 04:11:22,520
convolutional layers. And on our first layer, what we do is we define the amount of filters

3590
04:11:22,520 --> 04:11:28,120
just like here, we define the sample size. So how big are those filters and activation function,

3591
04:11:28,120 --> 04:11:33,720
which essentially means after we apply that, what is it that cross not cross product dot

3592
04:11:33,720 --> 04:11:38,280
product operation that we talked about, we'll apply rectifier linear unit to that and then

3593
04:11:38,280 --> 04:11:42,360
put that in the output feature map. Again, we've talked about activations functions before. So I

3594
04:11:42,360 --> 04:11:46,040
won't go too far into depth with them. And then we define the input shape, which essentially

3595
04:11:46,040 --> 04:11:50,520
means what can we expect in this first layer? Well, 32 by 32 by three, these ones, we don't

3596
04:11:50,520 --> 04:11:53,720
need to do that, because they're going to figure out what that is based on the input from the

3597
04:11:53,720 --> 04:11:58,520
previous layer. Alright, so these are just a breakdown of the layers. The convolution or the

3598
04:11:58,520 --> 04:12:02,200
max pooling layers here, two by two, essentially means that what we're going to do is we're going

3599
04:12:02,200 --> 04:12:06,760
to have a two by two sample size with actually a stride of two. Again, the whole point of this

3600
04:12:06,760 --> 04:12:12,760
is to actually divide or, you know, shrink it by a factor of two, how large each of these layers

3601
04:12:12,840 --> 04:12:17,720
are. Alright, so now let's have a summary. It's already printed out here. We can see that we

3602
04:12:17,720 --> 04:12:24,520
have, oh, wait, is this correct? mobile net v two, I don't think that's correct. That's because I

3603
04:12:24,520 --> 04:12:29,400
haven't run this one. My apologies on that guys, this is from something later in the tutorial,

3604
04:12:29,400 --> 04:12:33,640
we can see that we have calm 2d as our first layer. This is the output shape of that layer.

3605
04:12:33,640 --> 04:12:41,080
Notice that it is not 32 by 32 by 32. It is 30 by 30 by 32, because when we do that sampling

3606
04:12:41,080 --> 04:12:45,160
without padding, right, that's what we're going to get. We're going to get two pixels less,

3607
04:12:45,960 --> 04:12:51,880
because the amount of samples we can take. Alright, next, we have the max pooling 2d layer.

3608
04:12:51,880 --> 04:12:57,640
So this now says the output shape is 15 by 15 by 32, which means we've shrunk this shape by a

3609
04:12:57,640 --> 04:13:03,880
factor of two, we do a convolution on this, which means that now we get 1313 and we're doing 64,

3610
04:13:03,880 --> 04:13:09,640
because we're going to take 64 filters this time. And then max pooling again, we go six by six by

3611
04:13:10,200 --> 04:13:14,760
64, because we're going to divide this again by factor of two. Notice that it just rounded,

3612
04:13:14,760 --> 04:13:19,560
right? And then calm 2d. So another layer here, we get four by four by 64, again, because of the

3613
04:13:19,560 --> 04:13:25,960
way we take those values. So this is what we've defined so far. But this is not the end of our

3614
04:13:25,960 --> 04:13:31,320
convolutional neural network. In fact, this doesn't really mean much to us, right? This just tells us

3615
04:13:31,320 --> 04:13:35,960
about the presence of specific features, as we've gone through this convolution base, which is what

3616
04:13:35,960 --> 04:13:41,720
this is called the stack of convolution and max pooling layers. So what we actually need to do

3617
04:13:41,720 --> 04:13:47,560
is now pass this information into some kind of dense layer classifier, which is actually going to

3618
04:13:47,560 --> 04:13:53,160
take this pixel data that we've kind of calculated and found. So the almost extraction of features

3619
04:13:53,160 --> 04:13:58,360
that exist in the image, and tell us which combination of these features map to either,

3620
04:13:58,360 --> 04:14:03,320
you know, what one of these 10 classes are. So that's kind of the point you do this convolution

3621
04:14:03,320 --> 04:14:08,920
base, which extracts all of the features out of your image. And then you use the dense network

3622
04:14:08,920 --> 04:14:13,880
to say, Okay, well, if these combination of features exist, then that means this image is this,

3623
04:14:13,880 --> 04:14:18,920
otherwise, it's this and that and so on. So that's what we're doing here. Alright, so let's say adding

3624
04:14:18,920 --> 04:14:22,920
the dense layers. So to add the dense layers pretty easy model dot add is just how we add them,

3625
04:14:22,920 --> 04:14:28,680
right? So we're going to flatten all of those pixels was which essentially means take the four

3626
04:14:28,680 --> 04:14:33,080
by four by 64, and just put those all into a straight line, like we've done before. So just

3627
04:14:33,080 --> 04:14:39,000
one dimensional, then we're going to have a 64 neuron dense layer that connects all of those

3628
04:14:39,000 --> 04:14:43,480
things to it with an activation function of rectifier linear unit, then our output layer of

3629
04:14:43,480 --> 04:14:48,360
a dense layer with 10 neurons, obviously 10, because that's the amount of classes we have for

3630
04:14:48,360 --> 04:14:52,040
this problem. So let's run this here, we'll add those layers, let's look at a summary and see

3631
04:14:52,040 --> 04:14:58,360
how things have changed now. So we go from four by four by 64 to 2024. Notice that that is precisely

3632
04:14:58,360 --> 04:15:03,320
the calculation of four times four times 64. That's how we get that number here. Then we have a

3633
04:15:03,320 --> 04:15:07,000
dense layer and another dense layer. And this is our output layer. Finally, this is what we're

3634
04:15:07,000 --> 04:15:11,160
getting is we're going to get 10 neurons out. So essentially, just a list of values. And that's

3635
04:15:11,160 --> 04:15:17,480
how we can determine which class is predicted. So this up to here is the convolutional base,

3636
04:15:17,480 --> 04:15:21,960
this is what we call the classifier, and they work together to essentially extract the features,

3637
04:15:21,960 --> 04:15:27,000
and then look at the features and predict the actual object or whatever it is the class. Alright,

3638
04:15:27,640 --> 04:15:30,360
so that's how that works. Now it's time to train again, we'll go through this quickly.

3639
04:15:31,000 --> 04:15:35,000
I believe I've already trained this this takes a long time to train. So I'm actually going to

3640
04:15:35,000 --> 04:15:41,800
reduce the epochs here to just be for I'd recommend you guys train this on higher. So like 10, if

3641
04:15:41,800 --> 04:15:45,560
you're going to do it, it does take a while. So for our purposes, and for my time, we'll leave

3642
04:15:45,560 --> 04:15:49,400
a little bit shorter right now, but you should be getting about 70% accuracy. And you can see I've

3643
04:15:49,400 --> 04:15:53,720
trained this previously, if you train it on 10 epochs, but I'm just going to train up to four,

3644
04:15:53,800 --> 04:15:58,040
we get our 6768%. And that should be fine. So we'll be back once this is trained,

3645
04:15:58,040 --> 04:16:01,880
then we'll talk about how some of this works. Okay, so the model is finally finished training,

3646
04:16:01,880 --> 04:16:07,800
we did about four epochs, you can see we got an accuracy about 67% on the evaluation data.

3647
04:16:07,800 --> 04:16:14,120
To quickly go over this stuff. optimizers, Adam talked about that before. loss function is sparse

3648
04:16:14,120 --> 04:16:18,440
categorical cross entropy. That one, I mean, you can read this if you want computes the cross

3649
04:16:18,440 --> 04:16:22,920
entropy loss between the labels and predictions. And I'm not going to go into that. But these

3650
04:16:22,920 --> 04:16:26,360
kind of things are things that you can look up if you really understand why they work.

3651
04:16:26,920 --> 04:16:30,840
For most problems, you can just if you want to figure out what, you know, loss function

3652
04:16:30,840 --> 04:16:36,040
or optimizer to use, just use the basics, like use Adam, use a categorical cross entropy,

3653
04:16:36,040 --> 04:16:40,040
using a classification task, you want to do something like this, there's just you can go

3654
04:16:40,040 --> 04:16:44,280
up and look kind of all of the different loss functions, and it'll tell you when to use which

3655
04:16:44,280 --> 04:16:49,480
one and you can kind of mess with them and tweak them if you want. Now history equals model dot

3656
04:16:49,480 --> 04:16:54,040
fit. This is just so we can access some of the statistics from this model dot fit. Obviously,

3657
04:16:54,040 --> 04:16:58,840
it's just training the data to this test images, test labels and train images and train labels

3658
04:16:58,840 --> 04:17:03,560
where this is the validation data suite. So evaluating the model, we want to evaluate the

3659
04:17:03,560 --> 04:17:07,080
model, we can evaluate it now on the test images and test labels, we're obviously going to get

3660
04:17:07,080 --> 04:17:11,480
the same thing because the valuation is test images and test labels. So we should get the

3661
04:17:11,480 --> 04:17:17,480
same accuracy as 6735, which we do right here. Alright, so there we go, we get about 70%. If

3662
04:17:17,560 --> 04:17:21,080
you guys train this on 10 epochs, you should get close to 70. I'm a little bit lower just

3663
04:17:21,080 --> 04:17:25,400
because I didn't want to go that high. And that is now the model. I mean, we could use this if

3664
04:17:25,400 --> 04:17:29,400
we want, we could use predict, we could pass in some image, and we could see the prediction for

3665
04:17:29,400 --> 04:17:33,000
it. I'm not going to do that just because we've already talked about that enough. And I want to

3666
04:17:33,000 --> 04:17:38,120
get into some of the cooler stuff when we're working with smaller data sets. So the basic idea

3667
04:17:38,120 --> 04:17:42,040
here is this is actually a pretty small data set, right? We use about 60,000 images. And if you

3668
04:17:42,040 --> 04:17:47,880
think about the amount of different patterns we need to pick up to classify, you know, things like

3669
04:17:47,880 --> 04:17:53,480
horses versus trucks, that's a pretty difficult task to do, which means that we need a lot of data.

3670
04:17:53,480 --> 04:17:58,200
And in fact, some of the best convolutional networks that are out there are trained on millions

3671
04:17:58,200 --> 04:18:02,680
of pieces of, you know, sample information or data. So obviously, we don't have that kind of data.

3672
04:18:02,680 --> 04:18:07,480
So how can we work with, you know, a few images, maybe like a few 1000 images, and still get a

3673
04:18:07,480 --> 04:18:12,520
decent model. Well, the thing is, you can't unless we use some of the techniques that I have to show

3674
04:18:12,520 --> 04:18:18,120
you. So working with small data sets. So just like I mentioned, it's difficult to create a very good

3675
04:18:18,120 --> 04:18:23,320
convolutional neural network from scratch, if you're using a small amount of data, that is why we

3676
04:18:23,320 --> 04:18:28,840
can actually employ these techniques, the first one data augmentation, but also using pre trained

3677
04:18:28,840 --> 04:18:32,440
models to kind of accomplish what we need to do. And that's what we're going to be talking about

3678
04:18:32,440 --> 04:18:35,480
now in the second part of the tutorial, we're going to create another convolutional neural

3679
04:18:35,560 --> 04:18:39,320
network. So just to clarify, this is created, we've made the model up here already. This is

3680
04:18:39,320 --> 04:18:43,000
all we need to do to do it. This is the architecture. And this was just to get you familiar with the

3681
04:18:43,000 --> 04:18:50,520
idea. So data augmentation. So this is basically the idea, if you have one image, we can turn that

3682
04:18:50,520 --> 04:18:57,560
image into several different images, and train and pass all those images to our, our model. So

3683
04:18:57,560 --> 04:19:02,520
essentially, if we can rotate the image, if we can flip it, if we can stretch it, compress it,

3684
04:19:03,080 --> 04:19:07,560
you know, shift it, zoom it, whatever it is, and pass that to our model, it should be better at

3685
04:19:07,560 --> 04:19:13,720
generalizing, because we'll see the same image, but modified and augmented multiple times, which

3686
04:19:13,720 --> 04:19:20,840
means that we can turn a data set say of 10,000 images into 40,000 images, by doing four augmentations

3687
04:19:20,840 --> 04:19:26,600
on every single image. Now, obviously, you still want a lot of unique images, but this technique

3688
04:19:26,600 --> 04:19:31,560
can help a lot and is used quite a bit, because that allows our kind of model to be able to pick

3689
04:19:31,560 --> 04:19:35,400
up images that maybe are orientated differently or zoomed in a bit or stretch something different,

3690
04:19:35,400 --> 04:19:39,800
right, just better at generalizing, which is the whole point. So I'm not going to go through this

3691
04:19:39,800 --> 04:19:44,280
in too depth, too much depth, but this is essentially a script that does data augmentation

3692
04:19:44,280 --> 04:19:50,440
for you. We're gonna use this image data generator from the Keras dot preprocessing dot image module,

3693
04:19:51,080 --> 04:19:55,320
we're going to create an image data generator object. Now essentially, what this allows us to

3694
04:19:55,320 --> 04:19:59,720
do is specify some parameters on how we want to modify our image. In this case, we have the

3695
04:19:59,720 --> 04:20:06,680
rotation range, some shifts, shear, zoom horizontal flip and the mode. Now I'm not going to go into

3696
04:20:06,680 --> 04:20:10,440
how this works, you can look at the documentation if you'd like. But essentially, this will just

3697
04:20:10,440 --> 04:20:16,280
allow us to augment our images. Now what I'm going to do is pick one arbitrary image from the test

3698
04:20:16,280 --> 04:20:21,160
image data set, just our test image, I guess, group of photos, whatever you want to call it.

3699
04:20:21,160 --> 04:20:25,320
I'm going to convert that to an image array, which essentially takes it from the weird data set

3700
04:20:25,320 --> 04:20:30,520
object that it kind of is and turns it into a NumPy array. Then we're going to reshape this.

3701
04:20:30,520 --> 04:20:34,600
So that's in the form one comma, which essentially means one, and then this will figure out what

3702
04:20:34,600 --> 04:20:39,720
the rest of the shape should be. Oh, sorry, one and then plus the image shape, which is whatever

3703
04:20:39,720 --> 04:20:45,960
this shape is. So we'll reshape that. And then what we're going to do is we're going to say for batch

3704
04:20:45,960 --> 04:20:51,720
in data flow gen dot flow. Talk about how that works in a second. Essentially, this is just going

3705
04:20:51,800 --> 04:20:56,120
to augment the image for us and actually save it onto our drive. So in this instance, what's

3706
04:20:56,120 --> 04:21:00,840
going to happen is this data gen dot flow is going to take the image which we've created here, right?

3707
04:21:00,840 --> 04:21:05,080
And we formatted it correctly by doing these two steps, which you need to do beforehand,

3708
04:21:05,080 --> 04:21:09,400
it's going to save this image as test dot jpeg. And this will be the prefix, which means there'll

3709
04:21:09,400 --> 04:21:14,840
be some information after. And it will do this as many times until we break. So essentially,

3710
04:21:14,840 --> 04:21:19,640
given an image, it will do test one, test two, test three, test four, test five, with random

3711
04:21:19,640 --> 04:21:25,160
augmentations using this, until eventually we decided to break out of this. Now what I'm doing

3712
04:21:25,160 --> 04:21:29,480
is just showing the image by doing this and batch zero is just showing us the you know,

3713
04:21:29,480 --> 04:21:35,080
that first image in there. And that's kind of how this works. So you can mess with the script

3714
04:21:35,080 --> 04:21:37,960
and figure out a way to use it. But I would recommend if you want to do data augmentation,

3715
04:21:37,960 --> 04:21:42,360
just look into image data generator. This is something that I just want to show you so

3716
04:21:42,360 --> 04:21:45,960
you're aware of and I'll just run it so you can see exactly how this works. So essentially,

3717
04:21:45,960 --> 04:21:49,320
given an image of a truck, what it will do is augmented in these different ways.

3718
04:21:49,880 --> 04:21:55,160
You can see kind of the shifts, the translations, the rotations, all of that. And we'll do

3719
04:21:56,040 --> 04:21:58,920
actually a different image here to see what one looks like. Let's just do image say 20.

3720
04:21:59,800 --> 04:22:04,040
See if we get something different. So in this case, I believe this is maybe like a deer or

3721
04:22:04,040 --> 04:22:08,120
rabbit or a dog or something. I don't really know exactly what it is because it's so blurry.

3722
04:22:08,120 --> 04:22:11,560
But you can see that's kind of the shifts we're getting. And it makes sense because you want

3723
04:22:11,560 --> 04:22:15,560
to have images in different areas so that we have a better generalization. All right,

3724
04:22:15,640 --> 04:22:20,760
let's close that. Okay, so now we're going to talk about using or sorry, what is it pre trained

3725
04:22:20,760 --> 04:22:25,320
models? Okay, so we talked about data augmentation. That's a great technique if you want to increase

3726
04:22:25,320 --> 04:22:29,480
the size of your data set. But what if even after that, we still don't have enough images in our

3727
04:22:29,480 --> 04:22:34,360
data set? Well, what we can do is use something called a pre trained model. Now companies like

3728
04:22:34,360 --> 04:22:39,560
Google and you know, TensorFlow, which is owned by Google, make their own amazing convolutional

3729
04:22:39,560 --> 04:22:43,320
neural networks that are completely open source that we can use. So what we're going to do is

3730
04:22:43,320 --> 04:22:47,960
actually use part of a convolutional neural network that they've trained already on, I believe

3731
04:22:47,960 --> 04:22:54,520
1.4 million images. And we're just going to use part of that model as kind of the base of our

3732
04:22:54,520 --> 04:22:59,480
models that we have a really good starting point. And all we need to do is what's called fine tune,

3733
04:22:59,480 --> 04:23:04,520
the last few layers of that network, so that they work a little bit better for our purposes.

3734
04:23:05,240 --> 04:23:09,960
So what we're going to do essentially say, All right, we have this model that Google's trained,

3735
04:23:09,960 --> 04:23:14,280
they've trained it on 1.4 million images, it's capable of classifying, let's say 1000 different

3736
04:23:14,280 --> 04:23:19,480
classes, which is actually the example we'll look at later. So obviously, the beginning of that model

3737
04:23:20,120 --> 04:23:25,400
is what's picking up on the smaller edges, and you know, kind of the very general things that

3738
04:23:25,400 --> 04:23:30,600
appear in all of our images. So if we can use the base of that model, so kind of the beginning of

3739
04:23:30,600 --> 04:23:35,160
it, that does a really good job picking up on edges and general things that will apply to any

3740
04:23:35,160 --> 04:23:40,200
images. Then what we can do is just change the top layers of that model a tiny bit or add our own

3741
04:23:40,200 --> 04:23:45,240
layers to it to classify for the problem that we want. And that should be a very effective way

3742
04:23:45,240 --> 04:23:49,960
to use this pre trained model. We're saying we're going to use the beginning part that's really

3743
04:23:49,960 --> 04:23:54,840
good at kind of the generalization step, then we'll pass it into our own layers that we'll do

3744
04:23:54,840 --> 04:23:59,480
whatever we need to do specifically for our problem. That's what's like the fine tuning

3745
04:23:59,480 --> 04:24:02,680
step. And then we should have a model that works pretty well. And in fact, that's what we're going

3746
04:24:02,760 --> 04:24:06,920
to do in this example now. So that's kind of the point of what I'm talking about here is using

3747
04:24:06,920 --> 04:24:11,240
part of a model that already exists, that's very good at generalizing, and it's been trained on

3748
04:24:11,240 --> 04:24:16,760
so many different images. And then we'll pass our own training data in, we won't modify the

3749
04:24:16,760 --> 04:24:21,400
beginning aspect of our neural network, because it already works really well, we'll just modify the

3750
04:24:21,400 --> 04:24:26,760
last few layers that are really good at classifying, for example, just cats and dogs, which is exactly

3751
04:24:26,760 --> 04:24:30,040
the example we're actually going to do here. So I hope that makes sense as we get through this

3752
04:24:30,120 --> 04:24:34,200
should be cleared up a little bit. But using a pretrained model is now the section we're

3753
04:24:34,200 --> 04:24:37,960
getting into. So this is based on this documentation, as always, I'm referencing

3754
04:24:37,960 --> 04:24:41,640
everything. So you guys can go see that if you'd like, and do our imports like this,

3755
04:24:41,640 --> 04:24:45,160
we're going to load a data set that actually takes a second to load the data set, I believe,

3756
04:24:45,160 --> 04:24:50,600
oh, maybe not. And essentially, the problem we're doing is trying to classify dogs versus cats with

3757
04:24:50,600 --> 04:24:56,040
a fair degree of accuracy. In fact, we'd like to get above 90%. So this is the data set we're

3758
04:24:56,040 --> 04:25:01,800
loading in from TensorFlow data sets as TFDS. This is kind of a weird way to load it in again,

3759
04:25:01,800 --> 04:25:05,880
stuff like this, you just have to reference the documentation, I can explain it to you, but

3760
04:25:05,880 --> 04:25:08,840
it's not really going to help when the next example is going to be a different way of

3761
04:25:08,840 --> 04:25:12,600
loading the data, right? So so long as you know how to get the data in the correct form,

3762
04:25:12,600 --> 04:25:16,440
you can get it into some kind of NumPy array, you can split it into training, testing and

3763
04:25:16,440 --> 04:25:20,600
validation data, you should be okay. And if you're using a TensorFlow data set, it should

3764
04:25:20,600 --> 04:25:25,160
tell you in the documentation how to load it in properly. So we'll load it in here, we're training

3765
04:25:25,160 --> 04:25:31,400
80% train will go 10% for what is it raw validation and 10% for the testing data.

3766
04:25:32,120 --> 04:25:36,440
So we've loaded that. And now what we're doing here is just we're going to look at a few images.

3767
04:25:36,440 --> 04:25:40,680
So this actually creates a function, I know this is a weird thing, this is pretty unique to this

3768
04:25:40,680 --> 04:25:47,000
example, that allows us to call this function with some integer, essentially, and get what the

3769
04:25:47,000 --> 04:25:51,240
actual string representation of that is to the label for it. And what I'm doing here is just

3770
04:25:51,240 --> 04:25:55,720
taking two images from our raw training data set, and just displaying them. And you can see

3771
04:25:55,720 --> 04:26:01,320
that's where we're getting here dog and dog. If I go ahead and take five, we'll see, these are

3772
04:26:01,320 --> 04:26:07,400
what our images look like. Right? So here's example of a dog, we have a cat, right? And so on so forth,

3773
04:26:07,400 --> 04:26:11,960
you kind of you get the you get the point there. Now, notice, though, that these images are

3774
04:26:11,960 --> 04:26:15,320
different dimensions. In fact, none of these images other than these two actually are the same

3775
04:26:15,320 --> 04:26:19,640
dimension at all. Oh, actually, I don't think these ones are either. So obviously, there's a

3776
04:26:19,640 --> 04:26:23,720
step that we need to do, which is we need to scale all these images to be the same size.

3777
04:26:24,440 --> 04:26:28,840
So to do that, what we're going to do is write a little function like this, and essentially,

3778
04:26:28,840 --> 04:26:34,280
we'll return an image that is reshaped. So I guess that is reshaped to the image size, which I'm

3779
04:26:34,280 --> 04:26:39,160
going to set at 160 by 160. Now, we can make this bigger if we want. But the problem sometimes is

3780
04:26:39,160 --> 04:26:44,680
if you make an image that is bigger than like you want to make your image bigger than most of your

3781
04:26:44,680 --> 04:26:48,520
data set examples, and that means you're going to be really stretching a lot of the examples out

3782
04:26:48,520 --> 04:26:52,760
and you're losing a lot of detail. So it's much better to make the image size smaller rather

3783
04:26:52,760 --> 04:26:56,920
than bigger. You might say, well, if you make it smaller, you're going to lose detail too. But

3784
04:26:56,920 --> 04:27:01,800
it's just it's better to compress it smaller than it is to go really big, even just when it comes

3785
04:27:01,800 --> 04:27:05,880
to the amount of training time and how complex networks going to be. So that's something to

3786
04:27:05,880 --> 04:27:09,400
consider. You can mess around with those when you're making your own networks. But again,

3787
04:27:09,400 --> 04:27:13,480
smaller is typically better in my opinion, you don't want to go too small, but something that's

3788
04:27:13,480 --> 04:27:18,200
like, you know, half the size of what an average image would be. Alright, so we're going to go

3789
04:27:18,200 --> 04:27:22,360
format example. So we're going to just take an image and a label. And what this will do is return

3790
04:27:22,360 --> 04:27:26,840
to us just the reshaped image and label. So in this case, we're going to cast, which means convert

3791
04:27:26,840 --> 04:27:31,880
every single pixel in our image to be a float 32 value, because it could be integers, we're then

3792
04:27:31,880 --> 04:27:39,400
going to divide that by 127.5, which taken is exactly half of 255. And then subtract one,

3793
04:27:39,480 --> 04:27:44,360
then we're going to resize this image to be the image size. So sorry, the image will be

3794
04:27:44,360 --> 04:27:49,480
resized to the image size of 160 by 160 and we'll return the new image and the label. So now we

3795
04:27:49,480 --> 04:27:53,320
can apply this function to all of our images using map, if you don't know what map is, essentially,

3796
04:27:53,320 --> 04:27:58,360
it takes every single example in in this case, going to be raw train and applies the function

3797
04:27:58,360 --> 04:28:04,040
to it, which will mean that it will convert raw train into images that are all resized to 160

3798
04:28:04,040 --> 04:28:09,320
by 160. And we'll do the same thing for validation and test. So run that no issue there. Now

3799
04:28:09,320 --> 04:28:13,880
let's have a look at our images and see what we get. And there we are. Now I've just messed up

3800
04:28:13,880 --> 04:28:20,520
the color because I didn't add a CMAP thing, which I think I needed. Where was the CMAP?

3801
04:28:22,040 --> 04:28:24,920
Anyways, you know what, that's fine for now. This is what our images look like. This is the

3802
04:28:24,920 --> 04:28:31,400
resize. Now we get all images 160 by 160. And we are good to go. Alright, so

3803
04:28:32,680 --> 04:28:36,360
now let's have a look at the shape of an original image versus our new image. So I mean, this was

3804
04:28:36,440 --> 04:28:41,960
just to prove that essentially our original shapes were like 262 409 by some random values,

3805
04:28:41,960 --> 04:28:46,840
and they're all reshaped now to 160 160 by three, three obviously is the color channel of the images.

3806
04:28:47,480 --> 04:28:51,400
Alright, so picking a pre trained model. So this is the next step, this is probably one of the

3807
04:28:51,400 --> 04:28:55,560
harder steps is picking a model that you would actually like to use the base up. Now we're going

3808
04:28:55,560 --> 04:28:59,720
to use one called mobile net v two, which is actually from Google, it's built into TensorFlow

3809
04:28:59,720 --> 04:29:03,640
itself. That's why I've picked it. And all we're going to do is set this. So essentially, we're

3810
04:29:03,640 --> 04:29:09,240
going to say the base model in our code is equal to TF dot keras dot applications dot mobile net

3811
04:29:09,240 --> 04:29:13,960
v two, which is just telling us the architecture of the model that we want. And we'll have a look

3812
04:29:13,960 --> 04:29:18,360
at it down below here. In just a second, we'll define the input shape, which is important,

3813
04:29:18,360 --> 04:29:22,680
because this can take any input shape that we want. So we'll change it to 160 160 by three,

3814
04:29:22,680 --> 04:29:28,280
which we've defined up here, include top, very important means do we include the

3815
04:29:28,280 --> 04:29:32,840
classifier that comes with this network already or not. Now in our case, we're going to be

3816
04:29:32,840 --> 04:29:38,600
retraining parts of this network so that it works specifically for dogs and cats, and not for 1000

3817
04:29:38,600 --> 04:29:43,480
different classes, which is what this model was actually aimed to do is train a 1.4 million images

3818
04:29:43,480 --> 04:29:48,280
for 1000 different classes of everyday objects. So we're going to not include the top, which means

3819
04:29:48,280 --> 04:29:53,400
I don't include the classifier for these 1000 classes. And we're going to load the weights

3820
04:29:53,400 --> 04:29:58,200
from what's called image net, which is just a specific save of the weights. So this is the

3821
04:29:58,200 --> 04:30:02,120
architecture. And this is kind of the data that we're filling in for that architecture. So the

3822
04:30:02,120 --> 04:30:06,920
weights, and we'll load that in which we have here. So base model, now let's look at it. So

3823
04:30:06,920 --> 04:30:11,160
let's have a summary. You can see this is a pretty crazy model. I mean, we would never be

3824
04:30:11,160 --> 04:30:14,840
expected to create something like this by ourselves. This is, you know, teams of data

3825
04:30:14,840 --> 04:30:19,080
scientists, PhD students, engineers, would I write the experts in the field that have created a

3826
04:30:19,080 --> 04:30:23,160
network like this. So that's why we're going to use it because it works so effectively for

3827
04:30:23,160 --> 04:30:27,800
the generalization at the beginning, which is what we want. And then we can take those features

3828
04:30:27,800 --> 04:30:32,680
that this takes out. So in five by five by 1280, which is what I want us to focus on the output

3829
04:30:32,680 --> 04:30:38,120
of this actual network here. So really, you can see this last layer, we're going to take this and

3830
04:30:38,120 --> 04:30:44,200
using this information, pass that to some more convolutional layers and actually our own classifier,

3831
04:30:44,200 --> 04:30:50,360
I believe, and use that to predict versus dogs versus cats. So at this point, the base model

3832
04:30:50,360 --> 04:30:55,000
will simply output a shape 32 by five by five by 1280. That's the tensor that we're going to get

3833
04:30:55,000 --> 04:30:59,080
out of this, that's the shape, you can watch how this kind of works as you go through it.

3834
04:31:00,200 --> 04:31:04,840
And yes, all right. So we can just have a look at this here. This what I wanted to do essentially

3835
04:31:04,840 --> 04:31:09,720
was just look at what the actual shape was going to be. So 32 five by five by 1280, just because

3836
04:31:09,720 --> 04:31:14,120
this gives us none until it knows what the input is. And now it's time to talk about freezing the

3837
04:31:14,120 --> 04:31:18,920
base. So essentially, the point is, we want to use this as the base of our network, which means

3838
04:31:18,920 --> 04:31:23,640
we don't want to change it. If we just put this network in right now is the base to our neural

3839
04:31:23,640 --> 04:31:28,520
network. Well, what's going to happen is, it's going to start retraining all these weights and

3840
04:31:28,520 --> 04:31:34,600
biases. And in fact, it's going to train 2.257 million more weights and biases, when in fact,

3841
04:31:34,600 --> 04:31:37,960
we don't want to change these because these have already been defined, they've been set. And we

3842
04:31:37,960 --> 04:31:41,960
know that they work well for the problem already, right, they worked well for classifying 1000

3843
04:31:41,960 --> 04:31:45,480
classes. Why are we going to touch this now? And if we were going to touch this, what's the point

3844
04:31:45,480 --> 04:31:49,240
of even using this base, right, we don't want to train this, we want to leave it the same.

3845
04:31:49,320 --> 04:31:53,320
So to do that, we're just going to freeze it. Now, freezing is a pretty, I mean, it just

3846
04:31:53,320 --> 04:31:58,760
essentially means turning the trainable attribute of a layer off or of the model off. So what we do

3847
04:31:58,760 --> 04:32:02,120
is you just say base model dot trainable equals false, which essentially means that we are no

3848
04:32:02,120 --> 04:32:06,680
longer going to be training any aspect of that, I want to say model, although we'll just call it

3849
04:32:06,680 --> 04:32:11,240
the base layer for now, or the base model. So now if we look at the summary, we can see when we

3850
04:32:11,240 --> 04:32:17,480
scroll down to the bottom, if we get there any day soon, that now the trainable parameters is

3851
04:32:17,480 --> 04:32:23,160
zero instead of 2.257 million, which it was before. And now it's time to add our own classifier on

3852
04:32:23,160 --> 04:32:27,240
top of this. So essentially, we've got a pretty good network, right, five by five by 1280s,

3853
04:32:27,240 --> 04:32:33,320
our last output. And what we want to do now is take that. And we want to use it to classify

3854
04:32:33,320 --> 04:32:38,520
either cat or either dog, right? So what we're going to do is add a global average layer,

3855
04:32:38,520 --> 04:32:44,760
which essentially is going to take the entire average of every single so 1280 different layers

3856
04:32:44,840 --> 04:32:50,680
that are five by five, and put that into a 1d tensor, which is kind of flattening that for us.

3857
04:32:50,680 --> 04:32:55,000
So we do that global average pooling. And then we're just going to add the prediction layer,

3858
04:32:55,000 --> 04:32:59,800
which essentially is going to just be one dense node. And since we're only classifying two different

3859
04:32:59,800 --> 04:33:04,680
classes, right, dogs and cats, we only need one, then we're going to add all these models together.

3860
04:33:04,680 --> 04:33:09,080
So the base model, and I guess layers, the global average layer that we define there, and then the

3861
04:33:09,080 --> 04:33:14,920
prediction layer to create our final model. So let's do this global average layer, prediction

3862
04:33:14,920 --> 04:33:20,760
layer model, give that a second to kind of run there. Now when we look at the summary, we can see

3863
04:33:20,760 --> 04:33:25,720
we have mobile net v2, which is actually a model, but that is our base layer. And that's fine,

3864
04:33:25,720 --> 04:33:31,640
because the output shape is that then global average pooling, which again, just takes this

3865
04:33:31,640 --> 04:33:36,600
flattens it out does the average for us. And then finally, our dense layer, which is going to

3866
04:33:36,600 --> 04:33:42,680
simply have one neuron, which is going to be our output. Now notice that we have 2.25 and nine

3867
04:33:42,680 --> 04:33:48,920
million parameters in total, and only 1281 of them are trainable. That's because we have 1280

3868
04:33:48,920 --> 04:33:55,000
connections from this layer to this layer, which means 1280 weights and one bias. So that is what

3869
04:33:55,000 --> 04:33:58,760
we're doing. This is what we have created now, this base, the majority of the network has been

3870
04:33:58,760 --> 04:34:03,560
done for us. And we just add our own little classifier on top of this. And now we're going to

3871
04:34:03,560 --> 04:34:08,600
feed some training samples and data to this. Remember, we're not training this base layer

3872
04:34:08,600 --> 04:34:12,840
whatsoever. So the only thing that needs to be learned is the weights and biases on these two

3873
04:34:12,840 --> 04:34:18,040
layers here. Once we have that, we should have a decent model ready to go. So let's actually train

3874
04:34:18,040 --> 04:34:23,400
this now. I'm going to compile this here, I'm picking a learning rate that's very slow, what

3875
04:34:23,400 --> 04:34:27,800
essentially what the learning rate means is how much am I allowed to modify the weights and biases

3876
04:34:27,800 --> 04:34:32,040
of this network, which is what I've done, just made that very low, because we don't want to make

3877
04:34:32,040 --> 04:34:37,800
any major changes if we don't have to, because we're already using a base model that exists,

3878
04:34:37,800 --> 04:34:41,160
right? So we'll set the learning rate, I'm not going to talk about what this does specifically,

3879
04:34:41,160 --> 04:34:45,400
you can look that up if you'd like to. And then the loss function will use binary cross entropy,

3880
04:34:45,400 --> 04:34:50,280
just because we're using two classes, if you're using more than more than two classes, you would

3881
04:34:50,280 --> 04:34:55,560
just have cross entropy, or some other type of cross entropy. And then what we're going to do is

3882
04:34:55,560 --> 04:35:01,640
actually evaluate the model right now, before we even train it. So I've compiled it, I've just set

3883
04:35:01,720 --> 04:35:07,560
what we'll end up using. But I want to evaluate the model currently, without training it whatsoever,

3884
04:35:07,560 --> 04:35:13,640
on our validation data or validation batches, and see what it actually looks like, what it

3885
04:35:13,640 --> 04:35:18,120
actually, you know, what we're getting right now, with the current base model being the way it is,

3886
04:35:18,120 --> 04:35:22,040
and not having changed the weights and biases that completely random from the global average

3887
04:35:22,040 --> 04:35:26,920
pooling in the dense layer. So let's evaluate. Let's see what we get as an accuracy. Okay,

3888
04:35:26,920 --> 04:35:31,320
so we can actually see that with the random weights and biases for those last layer that we added,

3889
04:35:31,720 --> 04:35:36,040
we're getting an accuracy of 56%, which pretty much means that it's guessing, right? It's,

3890
04:35:36,040 --> 04:35:40,760
you know, 50% is only two classes. So if we got anything lower than 50, like 50 should have been

3891
04:35:40,760 --> 04:35:46,040
our guess, which is what we're getting. So now what we're going to do. And actually, I've trained

3892
04:35:46,040 --> 04:35:53,000
this already, I think so I might not have to do it again, is train this model on all of our images.

3893
04:35:53,000 --> 04:35:57,560
So all of our images and cats and cats and dogs that we've loaded in before, which will allow us

3894
04:35:57,640 --> 04:36:03,160
now to modify these weights and biases of this layer. So hopefully it can determine what features

3895
04:36:03,160 --> 04:36:08,040
need to be present for a dog to be a dog and for a cat to be a cat, right? And then it can make a

3896
04:36:08,040 --> 04:36:11,960
pretty good prediction. In fact, I'm not going to train this in front of us right now, because

3897
04:36:11,960 --> 04:36:16,440
it actually takes close to an hour to train just because there is a lot of images that it needs

3898
04:36:16,440 --> 04:36:21,960
to look at and a lot of calculations that need to happen. But when you do end up training this,

3899
04:36:21,960 --> 04:36:27,560
you end up getting an accuracy of a close to 92 or 93%, which is pretty good,

3900
04:36:27,560 --> 04:36:33,160
considering the fact that all we did was use an original layer, like base layer that classified

3901
04:36:33,160 --> 04:36:38,520
up to 1000 different images, so very general, and applied that just to cats and dogs by adding

3902
04:36:38,520 --> 04:36:43,240
our dense layer classifier on top. So you can see this was kind of the accuracy I had from

3903
04:36:43,240 --> 04:36:47,000
training this previously, I don't want to train again, because it takes so long. But I did want

3904
04:36:47,080 --> 04:36:53,480
to show that you can save a model and load a model by doing this syntax. So essentially,

3905
04:36:53,480 --> 04:36:58,360
on your model object, you can call model dot save, save it as whatever name you like dot h5,

3906
04:36:58,360 --> 04:37:03,960
which is just a format for saving models and Keras is specific to Keras, not TensorFlow.

3907
04:37:03,960 --> 04:37:08,600
And then you can load the model by doing this. So this is useful because after you train this

3908
04:37:08,600 --> 04:37:12,600
for an hour, obviously, you don't want to retrain this if you don't have to to actually use it to

3909
04:37:12,600 --> 04:37:18,280
make predictions. So you can just load the model. Now, I'm not going to go into using the model

3910
04:37:18,280 --> 04:37:21,880
specifically, you guys can look up the documentation to do that. We're at the point now where I've

3911
04:37:21,880 --> 04:37:26,280
showed you so much syntax on predicting and how we actually use the models. But the basic idea

3912
04:37:26,280 --> 04:37:30,120
would be to do model dot predict, right? And then you can see that it's even giving me the input

3913
04:37:30,120 --> 04:37:35,080
here. So model dot predict, give it some x batch size or both, right, because it will predict on

3914
04:37:35,080 --> 04:37:38,920
multiple things. And that will spit back to you a class, which then you can figure out, okay,

3915
04:37:38,920 --> 04:37:42,920
this is a cat, or this is a dog, you're going to pass this obviously the same input information

3916
04:37:42,920 --> 04:37:47,320
we have before, which is 160 by 160 by three. And that will make the prediction for you.

3917
04:37:47,960 --> 04:37:52,600
So that's kind of the thing there. I was getting an OS error just because I hadn't saved this

3918
04:37:52,600 --> 04:37:56,040
previously. But that's how you save and load models, which I think is important when you're

3919
04:37:56,040 --> 04:38:00,680
doing very large models. So when you fit this, feel free to change the epochs to be something

3920
04:38:00,680 --> 04:38:06,200
slower if you'd like, again, right, this takes a long time to actually end up running. But you

3921
04:38:06,200 --> 04:38:10,920
can see that the accuracy increases pretty well, exponentially, exponentially from when we didn't

3922
04:38:10,920 --> 04:38:15,560
even have that classifier on it. Now, the last thing that I want to talk about is object detection,

3923
04:38:15,560 --> 04:38:18,920
I'm just going to load up a page, we're not going to do any examples, I'm just going to give you a

3924
04:38:18,920 --> 04:38:23,000
brief introduction, because we're kind of running out of time for this module, because you can use

3925
04:38:23,000 --> 04:38:27,640
TensorFlow to do object detection and recognition, which is kind of cool. So let's get into that

3926
04:38:27,640 --> 04:38:32,440
now. Okay, so right now, I'm on a GitHub page that's built by TensorFlow here, I'm going to leave

3927
04:38:32,440 --> 04:38:36,600
that link in the notebook where it said object detection, so you guys can look at that. But

3928
04:38:36,600 --> 04:38:40,600
essentially, there is an API for TensorFlow that does object detection for you. And in fact,

3929
04:38:40,600 --> 04:38:44,280
it works very well, and even gives you confidence scores. So you can see this is what you'll

3930
04:38:44,280 --> 04:38:48,200
actually end up getting if you end up using this API. Now, unfortunately, we don't have time to go

3931
04:38:48,200 --> 04:38:52,280
through this because this will take a good amount of time to talk about the setup and how to actually

3932
04:38:52,280 --> 04:38:56,520
use this project properly. But if you go through this documentation, you should be able to figure

3933
04:38:56,520 --> 04:38:59,960
it out. And now you guys are familiar with TensorFlow, and you understand some of the

3934
04:38:59,960 --> 04:39:04,920
concepts here. This runs a very different model than what we've discussed before. Unfortunately,

3935
04:39:04,920 --> 04:39:07,960
again, we don't have time to get into it. But just something I wanted to make clear is that you

3936
04:39:07,960 --> 04:39:12,360
can do something like this with TensorFlow. And I will leave that resource so that if you'd like

3937
04:39:12,360 --> 04:39:16,680
to check this out, you can use it. There's also a great module in Python called facial recognition,

3938
04:39:16,680 --> 04:39:21,000
it's not a part of TensorFlow. But it does use some kind of convolutional neural network to do

3939
04:39:21,000 --> 04:39:25,800
facial detection and recognition, which is pretty cool as well. So I'll put that link in here.

3940
04:39:25,880 --> 04:39:30,680
But for that, for now, that's going to be our, what is it convolutional neural network kind of

3941
04:39:30,680 --> 04:39:36,200
module. So I hope that says cleared some things up on how deep vision works and how convolutional

3942
04:39:36,200 --> 04:39:40,680
neural networks work. I know I haven't gone into crazy examples of what I've shown you some different

3943
04:39:40,680 --> 04:39:46,360
techniques that hopefully you'll go look up kind of on your own and really dive into. Because now

3944
04:39:46,360 --> 04:39:50,360
you have that base kind of domain knowledge where you're going to be able to follow along with the

3945
04:39:50,360 --> 04:39:54,760
tutorial and understand exactly what to do. And if you want to create your own model, so long as

3946
04:39:54,760 --> 04:39:59,480
you can get enough sufficient training data, you can load that training data into your computer,

3947
04:39:59,480 --> 04:40:04,760
put that in a NumPy array. Then what you can do is create a model like we've just done using even

3948
04:40:05,640 --> 04:40:10,440
something like the mobile nets, what was it v two that we talked about previously, if I could even

3949
04:40:10,440 --> 04:40:15,480
get up and need to close this output. Oh my gosh, this is this is massive output here. Where is

3950
04:40:15,480 --> 04:40:20,280
this begin to pre train model? Yeah, mobile net v two, you can use the base of that, and then add

3951
04:40:20,280 --> 04:40:24,920
your own classifier on do a similar thing to what I've done with that dense neuron and that global

3952
04:40:24,920 --> 04:40:28,680
average layer. And hopefully you should get a decent result from that. So this is just showing

3953
04:40:28,680 --> 04:40:33,720
you what you can do. Obviously, you can pick a different base layer, depending on what kind of

3954
04:40:33,720 --> 04:40:37,560
problem you're trying to solve. So anyways, that has been convolutional neural networks. I hope

3955
04:40:37,560 --> 04:40:41,480
you enjoyed that module. Now we're on to recurrent neural networks, which is actually going to be

3956
04:40:41,480 --> 04:40:48,840
pretty interesting. So I'll see you in that module. Hello, everyone, and welcome to the next module

3957
04:40:48,840 --> 04:40:52,920
in this course, which is covering natural language processing with recurrent neural

3958
04:40:52,920 --> 04:40:58,440
networks. Now what we're going to be doing in this module here is first of all, first off discussing

3959
04:40:58,440 --> 04:41:02,680
what natural language processing is, which I guess I'll start with here. Essentially, for those of

3960
04:41:02,680 --> 04:41:08,520
you that don't know, natural language processing or NLP for short, is the field or discipline in

3961
04:41:08,520 --> 04:41:14,200
computing or machine learning that deals with trying to understand natural or human languages.

3962
04:41:14,200 --> 04:41:18,280
Now, the reason we call them natural is because these are not computer languages or

3963
04:41:18,280 --> 04:41:24,280
programming languages, per se. And actually, computers are quite bad at understanding textual

3964
04:41:24,280 --> 04:41:29,400
information and human languages. And that's why we've come up with this entire discipline focused

3965
04:41:29,400 --> 04:41:33,880
on how they can do that. So we're going to do that using something called recurrent neural

3966
04:41:33,880 --> 04:41:38,920
networks. But some examples of natural language processing would be something like spell check,

3967
04:41:38,920 --> 04:41:45,400
autocomplete voice assistance, translation between languages, there's all different kinds of things,

3968
04:41:45,480 --> 04:41:50,920
chatbots, but essentially anything that deals with textual data. So you like paragraphs,

3969
04:41:50,920 --> 04:41:56,360
sentences, even words, that is probably going to be classified under natural language processing

3970
04:41:56,360 --> 04:42:01,000
in terms of doing some kind of machine learning stuff with it. Now, we are going to be talking

3971
04:42:01,000 --> 04:42:05,000
about a different kind of neural network in this series called recurrent neural networks.

3972
04:42:05,000 --> 04:42:09,720
Now, these are very good at classifying and understanding textual data. And that's why we'll

3973
04:42:09,720 --> 04:42:14,440
be using them. But they are fairly complex. And there's a lot of stuff that goes into them.

3974
04:42:14,440 --> 04:42:18,840
Now, in the interest of time, and just not knowing a lot of your math background, I'm not

3975
04:42:18,840 --> 04:42:23,880
going to be getting into the exact details of how this works on a lower level, like I did when I

3976
04:42:23,880 --> 04:42:29,000
explained kind of our, I guess, fundamental learning algorithms, which are a bit easier to grasp,

3977
04:42:29,000 --> 04:42:33,080
and even just regular neural networks in general, we're going to be kind of skipping over that and

3978
04:42:33,080 --> 04:42:39,480
really focusing on why this works the way it does, rather than how and when you should use this.

3979
04:42:40,200 --> 04:42:43,560
And then maybe understanding a few of the different kinds of layers that have to do with

3980
04:42:43,640 --> 04:42:47,480
recurrent neural networks. But again, we're not going to get into the math. If you'd like to

3981
04:42:47,480 --> 04:42:51,160
learn about that, there will be some sources at the bottom of the guide. And you can also just

3982
04:42:51,160 --> 04:42:55,320
look up recurrent neural networks, and you'll find lots of resources that explain all of the

3983
04:42:55,320 --> 04:43:00,840
fancy math that goes on behind them. Now, the exact applications and kind of things will be working

3984
04:43:00,840 --> 04:43:06,120
towards here is sentiment analysis. That's the first kind of task or thing we're going to do.

3985
04:43:06,120 --> 04:43:10,200
We're actually going to use movie reviews and try to determine whether these movie reviews are

3986
04:43:10,280 --> 04:43:15,720
positive or negative by performing sentiment analysis on them. Now, if you're unfamiliar

3987
04:43:15,720 --> 04:43:19,320
with sentiment analysis, we'll talk about it more later, but essentially means trying to

3988
04:43:19,320 --> 04:43:24,120
determine how positive or negative a sentence or piece of text is, which you can see why that would

3989
04:43:24,120 --> 04:43:30,120
be useful for movie reviews. Next, we're going to do character slash text generation. So essentially,

3990
04:43:30,120 --> 04:43:34,920
we're going to use a natural language processing model, I guess, if you want to call it that,

3991
04:43:35,560 --> 04:43:40,920
to generate the next character in a sequence of text for us. And we're going to use that model a

3992
04:43:40,920 --> 04:43:46,120
bunch of times to actually generate an entire play. Now, I know this seems a little bit ridiculous

3993
04:43:46,120 --> 04:43:50,520
compared to some of the trivial examples we've done before. This will be quite a bit more code

3994
04:43:50,520 --> 04:43:53,800
than anything we've really looked at yet. But this is very cool, because we're going to actually

3995
04:43:53,800 --> 04:43:58,680
going to train a model to learn how to write a play. That's literally what it's going to do.

3996
04:43:58,680 --> 04:44:02,760
It's going to read through a play, I believe it's Romeo and Juliet. And then we're going to give it

3997
04:44:02,840 --> 04:44:07,480
a little prompt when we're actually using the model and say, okay, this is the first part of

3998
04:44:07,480 --> 04:44:11,640
the play, write the rest of it. And then it will actually go and write the rest of the characters

3999
04:44:11,640 --> 04:44:15,560
in the play. And we'll see that we can get something that's pretty good using the techniques that

4000
04:44:15,560 --> 04:44:20,760
we'll talk about. So the first thing that I want to do is talk about data. So I'm going to hop onto

4001
04:44:20,760 --> 04:44:26,040
my drawing tablet here. And we're going to compare the difference between textual data and numeric

4002
04:44:26,040 --> 04:44:30,360
data, like we've seen before, and why we're going to have to employ some pretty complex and

4003
04:44:30,360 --> 04:44:34,760
different steps to turn something like this, you know, a block of text into some meaningful

4004
04:44:34,760 --> 04:44:39,560
information that our neural networks actually going to be actually going to be able to understand

4005
04:44:39,560 --> 04:44:43,720
and process. So let's go ahead and get over to that. Okay, so now we're going to get into the

4006
04:44:43,720 --> 04:44:49,960
problem of how we can turn some textual data into numeric data that we can feed to our neural

4007
04:44:49,960 --> 04:44:54,680
network. Now, this is a pretty interesting problem. And we'll kind of go through as we start going

4008
04:44:54,680 --> 04:44:58,360
through it, you should see why this is interesting and why there's a lot of difficulties with the

4009
04:44:58,360 --> 04:45:02,200
different methods that we pick. But the first method that I want to talk about is something

4010
04:45:02,200 --> 04:45:07,800
called bag of words, in terms of how we can kind of encode and pre process text into integers.

4011
04:45:07,800 --> 04:45:12,920
Now, obviously, I'm not the first person to come up with this bag of words is a very famous,

4012
04:45:12,920 --> 04:45:18,200
almost I want to say algorithm or method of converting textual data to numeric data, although

4013
04:45:18,200 --> 04:45:23,400
it is pretty flawed and only really works for simple tasks. And we're going to understand why

4014
04:45:23,400 --> 04:45:28,280
in a second. So we're going to call this bag of words. Essentially, what bag of words says is

4015
04:45:28,280 --> 04:45:31,880
what we're going to do is we're going to look at our entire training data set, right, because we're

4016
04:45:31,880 --> 04:45:36,120
going to be turning our training data set into a form the network can understand. And we're going

4017
04:45:36,120 --> 04:45:42,280
to create a dictionary lookup of the vocabulary. Now, what I mean by that is we're going to say

4018
04:45:42,280 --> 04:45:49,080
that every single unique word in our data set is the vocabulary, right? That's the amount of words

4019
04:45:49,080 --> 04:45:53,320
that the model is expected to understand, because we're, you know, going to show all those words

4020
04:45:53,320 --> 04:45:57,400
to the model. And we're going to say that every single one of these words. So every single one

4021
04:45:57,400 --> 04:46:02,440
of these words in the vocabulary is going to be placed in a dictionary. And beside that, we're

4022
04:46:02,440 --> 04:46:07,320
going to have some integer that represents it. So for example, maybe the vocabulary of our data

4023
04:46:07,320 --> 04:46:15,960
set is the words, you know, I, a, maybe Tim, maybe day, me, right, we're gonna have a bunch of arbitrary

4024
04:46:15,960 --> 04:46:20,520
words, I'll just put dot dot dot to show that this kind of goes to the length of the vocabulary.

4025
04:46:20,520 --> 04:46:23,800
And every single one of these words will be placed in a dictionary, which we're going to just

4026
04:46:23,800 --> 04:46:29,320
going to call kind of our lookup table or word index table. And we're going to have a number

4027
04:46:29,320 --> 04:46:35,160
that represents every single one of them. So you can imagine that in very large data sets, we're

4028
04:46:35,160 --> 04:46:40,280
going to have, you know, tens of thousands of hundreds of thousands, sometimes even maybe millions

4029
04:46:40,280 --> 04:46:44,680
of different words, and they're all going to be encoded by different integers. Now, the reason

4030
04:46:44,760 --> 04:46:49,960
we call this bag of words is because what we're actually going to do when we look at a sentence

4031
04:46:49,960 --> 04:46:55,480
is we're only going to keep track of the words that are present and the frequency of those words.

4032
04:46:55,480 --> 04:47:01,720
And in fact, what we'll do well is we'll create what we call a bag. And whenever we see a word

4033
04:47:01,720 --> 04:47:08,040
appears, we'll simply add its number into the bag. So if I have a sentence like, you know, I

4034
04:47:09,000 --> 04:47:16,360
am Tim day, day, I'm just going to do like a random sentence like that. Then what we're going

4035
04:47:16,360 --> 04:47:21,000
to do is every time we see a word, we're going to take its number and throw it into the bag. So

4036
04:47:21,000 --> 04:47:28,360
we're going to say, all right, I, that's zero, and that's one, Tim, that's two, day, that's three,

4037
04:47:29,160 --> 04:47:33,960
that's three again. And notice that what's happening here is we're losing the ordering

4038
04:47:34,040 --> 04:47:38,920
of these words, but we're just keeping track of the frequency. Now, there's lots of different

4039
04:47:38,920 --> 04:47:44,200
ways to kind of format how we want to do bag of words. But this is the basic idea, I'm not going

4040
04:47:44,200 --> 04:47:47,880
to go too far in because we're not actually really going to use this technique. But essentially,

4041
04:47:47,880 --> 04:47:52,840
you lose the ordering in which words appear, but you just keep track of the frequency and what

4042
04:47:52,840 --> 04:47:57,640
words appear. So this could be very useful when you're looking, you know, you're doing very simple

4043
04:47:57,640 --> 04:48:03,400
tasks where the presence of a certain word will really influence the kind of type of sentence

4044
04:48:03,480 --> 04:48:07,240
that it is, or the meaning that you're going to get from it. But when we're looking at more

4045
04:48:07,240 --> 04:48:12,040
complex input, where, you know, different words have different meanings, depending on where they

4046
04:48:12,040 --> 04:48:18,360
are in a sentence, this is a pretty flawed way to encode this data. Now, I won't go much further

4047
04:48:18,360 --> 04:48:23,480
into this. This is not the exact way the bag of words works. But I just wanted to show you

4048
04:48:23,480 --> 04:48:28,840
kind of an idea here, which is we just encode every single unique word by an integer. And then we

4049
04:48:28,840 --> 04:48:33,800
don't even really care about where these words are. We just throw them into a bag and we say,

4050
04:48:33,800 --> 04:48:38,040
All right, you know, this is our bag right here that I'm doing the arrow to, we'll just throw in

4051
04:48:38,040 --> 04:48:42,680
three as many times as you know, the word day appears, we'll throw in one as many times as the

4052
04:48:42,680 --> 04:48:49,640
word, I guess, am appears, and so on and so forth. And then what will happen is we'll feed this bag

4053
04:48:49,640 --> 04:48:53,720
to our neural network in some form, depending on the network that we're using. And it will just

4054
04:48:53,720 --> 04:48:57,560
look at and say, Okay, so I have all these different numbers, that means these words are present

4055
04:48:57,640 --> 04:49:01,640
and try to do something with it. Now, I'm going to show you a few examples of where this kind of

4056
04:49:01,640 --> 04:49:05,320
breaks down, but just understand that this is how this works. This is the first technique called

4057
04:49:05,320 --> 04:49:10,360
bag of words, which again, we will not be using. So what happens when we have a sentence where

4058
04:49:10,360 --> 04:49:16,200
the same word conveys a very different meaning, right? And I'm actually, I think I have an example

4059
04:49:16,200 --> 04:49:22,600
on the slides here that I'll go into. Yes, select this. Okay, so for our bag of words technique,

4060
04:49:22,600 --> 04:49:27,160
which we can kind of see here, maybe we'll go through it. Let's consider the two sentences

4061
04:49:27,560 --> 04:49:31,800
where are they here? I thought the movie was going to be bad, but it was actually amazing.

4062
04:49:31,800 --> 04:49:36,200
And I thought the movie was going to be amazing, but it was actually bad, right? So consider these

4063
04:49:36,200 --> 04:49:39,560
two sentences. Now, I know you guys already know what I'm going to get at. But essentially, these

4064
04:49:39,560 --> 04:49:45,320
sentences use the exact same words. In fact, they use the exact same number of words, the exact same

4065
04:49:45,320 --> 04:49:51,800
words in total. And well, they have a very different meaning. With our bag of words technique,

4066
04:49:51,800 --> 04:49:57,080
we're actually going to encode these two sentences using the exact same representation. Because

4067
04:49:57,160 --> 04:50:02,440
remember, all we do is we care about the frequency and what words appear, but we don't care about

4068
04:50:02,440 --> 04:50:08,680
where they appear. So we end up losing that meaning from the sentence, because the sentence I thought

4069
04:50:08,680 --> 04:50:12,920
the movie was going to be bad, but it was actually amazing is encoded and represented by the same

4070
04:50:12,920 --> 04:50:17,640
thing as this sentence is. So that, you know, obviously is an issue that's a flaw. And that's

4071
04:50:17,640 --> 04:50:22,760
one of the reasons why bag of words is not very good to use, because we lose the context of the

4072
04:50:22,760 --> 04:50:27,560
words within the sentence, we just pick up the frequency and the fact that these words exist.

4073
04:50:27,560 --> 04:50:31,080
So that's the first technique that's called bag of words. I've actually written a little

4074
04:50:31,080 --> 04:50:35,560
function here that does this for us. This is not really the exact way that we would write a bag

4075
04:50:35,560 --> 04:50:41,720
of words function. But you kind of get the idea that when I have a text, this is a test to see

4076
04:50:41,720 --> 04:50:47,560
if this test will work is test a I just did a bunch of random stuff. So we can see what I'm

4077
04:50:47,560 --> 04:50:51,560
doing is printing out the bag, which I get from this function. And you guys can look at this if

4078
04:50:51,560 --> 04:50:55,400
you kind of want to see how this works. And essentially, what it tells us is the word one

4079
04:50:55,400 --> 04:51:02,200
appears two times. Yes, the word two appears three times the word three appears three times word

4080
04:51:02,200 --> 04:51:07,320
four appears three times five ones, six, one, seven ones. So on, that's the information we get

4081
04:51:07,320 --> 04:51:12,040
from our bag, right from that encoding. And then if we look up here, this is our vocabulary. So

4082
04:51:12,040 --> 04:51:16,440
this stands for one is is two is three so on. And you can kind of get the idea from that.

4083
04:51:17,240 --> 04:51:22,120
So that is how we would use bag of words, right? If we did an encoding kind of like this, that's

4084
04:51:22,120 --> 04:51:26,920
what that does. And that's one way of encoding it. Now, I'm going to go back and we'll talk about

4085
04:51:26,920 --> 04:51:31,160
another method here as well, actually, a few more methods before we get into anything further.

4086
04:51:31,160 --> 04:51:35,400
All right, so I'm sure a lot of you were looking at the previous example I did. And you saw the

4087
04:51:35,400 --> 04:51:41,480
fact that what I did was completely remove the idea of kind of sequence or ordering of words,

4088
04:51:41,480 --> 04:51:44,760
right? And what I did was just throw everything in a bag. And I said, All right, we're just going

4089
04:51:44,760 --> 04:51:51,160
to keep track of the fact that we have, you know, three A's, or we have four, those or seven, Tim's

4090
04:51:51,160 --> 04:51:54,920
right. And we're going to just going to lose the fact that, you know, words come after one

4091
04:51:54,920 --> 04:51:58,760
each other, we're going to lose their ordering in the sentence. And that's how we're going to

4092
04:51:58,760 --> 04:52:03,320
encode it. And I'm sure a lot of you were saying, Well, why don't we just not lose the ordering of

4093
04:52:03,320 --> 04:52:07,800
those words? We'll just encode every single word with an integer and just leave it in its space

4094
04:52:07,800 --> 04:52:11,560
where it would have been in the original string. Okay, good idea. So what you're telling me to

4095
04:52:11,560 --> 04:52:17,560
do is something like this, you know, Tim is here will be our sentence. Let's say we encode the word

4096
04:52:17,560 --> 04:52:22,520
Tim with zero is as one, here's two. And then that means our translation goes zero, one, two.

4097
04:52:23,160 --> 04:52:28,360
And that means, right, if we have a translation, say, like two, one, zero, even though these use

4098
04:52:28,360 --> 04:52:34,520
the exact same number of words and exact same representation for all these words. Well, this

4099
04:52:34,520 --> 04:52:39,080
is a different sentence. And our model should be able to tell that because these words come in a

4100
04:52:39,080 --> 04:52:43,080
different order. And to you, good point, if you made that point, but I'm going to discuss where

4101
04:52:43,080 --> 04:52:48,200
this falls apart as well. And why we're not going to use this method. So although this does solve

4102
04:52:48,200 --> 04:52:53,080
the problem I talked about previously, where we're going to kind of lose out on the context of a word,

4103
04:52:53,080 --> 04:52:56,920
there's still a lot of issues with this and they come, especially when you're dealing with very

4104
04:52:56,920 --> 04:53:01,960
large vocabularies. Now, let's take an example where we actually have a vocabulary of say 100,000

4105
04:53:01,960 --> 04:53:07,800
words. And we know that that means we're going to have to have 100,000 unique mappings from words

4106
04:53:07,800 --> 04:53:14,440
to integers. So let's say our mappings are something like this, one maps to the string happy,

4107
04:53:15,160 --> 04:53:24,200
the word happy, right, to maps to sad. And let's say that the string 100,000 or the number 100,000

4108
04:53:24,200 --> 04:53:30,120
maps to the word, I don't know, let's say good. Now, we know as humans, by kind of just thinking

4109
04:53:30,120 --> 04:53:34,440
about, let's consider the fact that we're going to try to classify sentences as a positive or

4110
04:53:34,440 --> 04:53:39,000
negative. So sentiment analysis, that the words happy and good in that regard, you know, sentiment

4111
04:53:39,000 --> 04:53:43,000
analysis are probably pretty similar words, right? And then if we were going to group these words,

4112
04:53:43,000 --> 04:53:47,000
we'd probably put them in a similar group, we'd classify them as similar words, we get probably

4113
04:53:47,000 --> 04:53:51,560
interchange them in a sentence, and it wouldn't change the meaning a whole ton. I mean, it might,

4114
04:53:51,560 --> 04:53:56,600
but it might not as well. And that we could say these are kind of similar. But our model or our

4115
04:53:56,600 --> 04:54:03,400
encoder, right, whatever we're doing to translate our text into integers here, has decided that 100,000

4116
04:54:03,480 --> 04:54:07,080
is going to represent good, and one is going to represent happy. And well, there's an issue with

4117
04:54:07,080 --> 04:54:12,360
that, because that means when we pass in something like one, or 100,000 to our model, it's going to

4118
04:54:12,360 --> 04:54:17,800
have a very difficult time determining the fact that one and 100,000, although they're 99,999,

4119
04:54:19,640 --> 04:54:24,200
kind of units apart, are actually very similar words. And that's the issue we get into when we

4120
04:54:24,200 --> 04:54:28,920
do something like this is that the numbers we decide to pick to represent each word are very

4121
04:54:28,920 --> 04:54:34,600
important. And we don't really have a way of being able to look at words group them and saying,

4122
04:54:34,600 --> 04:54:39,720
okay, well, we need to put all of the happy words and the range like zero to 100, all of the like

4123
04:54:39,720 --> 04:54:44,440
adjectives in this range, we don't really have a way to do that. And this gets even harder for our

4124
04:54:44,440 --> 04:54:48,680
model when we have these arbitrary mappings, right? And then we have something like two in

4125
04:54:48,680 --> 04:54:54,040
between where two is very close to one, right? Yet these words are complete opposites. In fact,

4126
04:54:54,040 --> 04:54:58,280
I'd say they're probably polar opposites, our model trying to learn that the difference between one

4127
04:54:58,360 --> 04:55:03,320
and two is actually way larger than the difference between one and 100,000 is going to be very

4128
04:55:03,320 --> 04:55:08,840
difficult. And say it's even able to do that, as soon as we throw in the mapping 900, right,

4129
04:55:08,840 --> 04:55:15,000
the 99,900, we put that as bad. Well, now it gets even more difficult, because it's now like, okay,

4130
04:55:15,000 --> 04:55:19,080
what the range is this big, then that means these words are actually very similar. But then you

4131
04:55:19,080 --> 04:55:23,320
throw another word in here like this, and it messes up the entire system. So that's kind of

4132
04:55:23,320 --> 04:55:27,000
what I want to show is that that's where this breaks apart on these large vocabularies. And

4133
04:55:27,080 --> 04:55:31,800
that's why I'm going to introduce us now to another concept called word embeddings. Now,

4134
04:55:31,800 --> 04:55:36,680
what word embeddings does is essentially try to find a way to represent words that are similar

4135
04:55:36,680 --> 04:55:42,360
using very similar numbers. And in fact, what a word embedding is actually going to do, I'll talk

4136
04:55:42,360 --> 04:55:49,560
about this more in detail as we go on, is classify or translate every single one of our words into

4137
04:55:49,560 --> 04:55:55,320
a vector. And that vector is going to have some, you know, n amount of dimensions. Usually, we're

4138
04:55:55,320 --> 04:56:01,320
going to use something like 64, maybe 128 dimensions for each vector. And every single component of

4139
04:56:01,320 --> 04:56:07,160
that vector will kind of tell us what group it belongs to or how similar it is to other words.

4140
04:56:07,160 --> 04:56:10,680
So let me give you an idea of what I mean. So we're going to create something called the word

4141
04:56:10,680 --> 04:56:15,560
embeddings. Now, don't ask why it's called embeddings, I don't know the exact reason, but I

4142
04:56:15,560 --> 04:56:19,960
believe it's to have has to do something with the fact that they're vectors. And let's just say we

4143
04:56:19,960 --> 04:56:23,800
have a 3d plane like this. And we've already kind of looked at what vectors are before. So I'll skip

4144
04:56:23,880 --> 04:56:29,800
over explaining them. And what we're going to do is take some word. So let's say we have the word

4145
04:56:29,800 --> 04:56:34,840
good. And instead of picking some integer to represent it, we're going to pick some vector,

4146
04:56:34,840 --> 04:56:38,680
which means we're going to draw some vector in this 3d space. Actually, let's make this a different

4147
04:56:38,680 --> 04:56:46,360
color. Let's make this vector say red, like this. And this vector represents this word good. And in

4148
04:56:46,440 --> 04:56:53,240
this case, we'll say we have x one, x two, x three is our dimensions, which means that every single

4149
04:56:53,240 --> 04:56:58,120
word in our data set will be represented by three coordinates. So one vector with three different

4150
04:56:58,120 --> 04:57:03,960
dimensions, where we have x one x two and x three. And our hope is that by using this word

4151
04:57:03,960 --> 04:57:08,680
embeddings layer, and we'll talk about how it accomplishes this in a second, is that we can

4152
04:57:08,680 --> 04:57:13,640
have vectors that represent very similar words being very similar, which means that you know,

4153
04:57:13,640 --> 04:57:19,960
if we have the vector good here, we would hope the vector happy from our previous example, right,

4154
04:57:19,960 --> 04:57:25,160
will be a vector that points in a similar direction to it. That is kind of a similar looking thing

4155
04:57:25,160 --> 04:57:30,360
where the angle between these two vectors, right, and maybe I'll draw it here so we can see is small

4156
04:57:30,360 --> 04:57:35,480
so that we know that these words are similar. And then we would hope that if we had a word that

4157
04:57:35,480 --> 04:57:39,800
was much different, maybe say like the word bad, that that would point in a different direction,

4158
04:57:39,800 --> 04:57:44,680
the vector that represents it. And that that would tell our model, because the angle between

4159
04:57:44,680 --> 04:57:49,000
these two vectors is so big, that these are very different words, right? Now, in theory,

4160
04:57:49,000 --> 04:57:53,320
does the embedding work layer work like this, you know, not always, but this is what it's

4161
04:57:53,320 --> 04:57:59,960
trying to do is essentially pick some representation in a vector form for each word. And then these

4162
04:57:59,960 --> 04:58:03,880
vectors, we hope if they're similar words, they're going to be pointing in a very similar

4163
04:58:03,880 --> 04:58:08,920
direction. And that's kind of the best explanation of a word embeddings layer I can give you.

4164
04:58:08,920 --> 04:58:13,400
Now, how do we do this, though, how do we actually, you know, go from word to vector,

4165
04:58:14,440 --> 04:58:19,160
and have that be meaningful? Well, this is actually what we call a layer. So word embeddings

4166
04:58:19,160 --> 04:58:23,560
is actually a layer, and it's something we're going to add to our model. And that means that

4167
04:58:23,560 --> 04:58:28,920
this actually learns the embeddings for our words. And the way it does that is by trying to kind of

4168
04:58:28,920 --> 04:58:34,760
pick out context in the sentence and determine based on where a word is in a sentence, kind of what

4169
04:58:34,840 --> 04:58:40,680
it means, and then encodes it doing that. Now, I know that's kind of a rough explanation to give

4170
04:58:40,680 --> 04:58:44,760
to you guys, I don't want to go too far into word embeddings in terms of the math, because I don't

4171
04:58:44,760 --> 04:58:49,080
want to get, you know, waste our time or get too complicated if we don't need to. But just understand

4172
04:58:49,080 --> 04:58:53,400
that our word embeddings are actually trained, and that the model actually learns these word

4173
04:58:53,400 --> 04:58:59,160
embeddings as it goes. And we hope that by the time it's looked at enough training data, it's

4174
04:58:59,160 --> 04:59:04,600
determined really good ways to represent all of our different words, so that they make sense to

4175
04:59:04,600 --> 04:59:09,160
our model in the further layers. And we can use pre trained word embedding layers, if we'd like,

4176
04:59:09,160 --> 04:59:14,360
just like we use that pre trained convolutional base in the previous section. And we might actually

4177
04:59:14,360 --> 04:59:17,800
end up doing that. Actually, probably not in this tutorial, but it is something to consider that

4178
04:59:17,800 --> 04:59:22,280
you can do that. So that's how word embeddings work. This is how we encode textual data. And this

4179
04:59:22,280 --> 04:59:26,920
is why it's so important that we kind of consider the way that we pass information to our neural

4180
04:59:26,920 --> 04:59:31,960
network, because it makes a huge difference. Okay, so now that we've talked about kind of the form

4181
04:59:32,040 --> 04:59:35,960
that we need to get our data in before we can pass it further in the neural network, right,

4182
04:59:35,960 --> 04:59:40,600
before it can get past that embedding layer, before it can get put in, put into any dense

4183
04:59:40,600 --> 04:59:45,000
neurons, before we can even really do any math with it, we need to turn it into numbers, right,

4184
04:59:45,000 --> 04:59:50,040
our textual data. So now that we know that it's time to talk about recurrent neural networks.

4185
04:59:50,040 --> 04:59:54,280
Now recurrent neural networks are the type of networks we use when we process textual data,

4186
04:59:54,280 --> 04:59:58,600
typically, you don't always have to use these, but they are just the best for natural language

4187
04:59:58,600 --> 05:00:03,080
processing. And that's why they're kind of their own class, right? Now, the fundamental difference

4188
05:00:03,080 --> 05:00:07,320
between a recurrence neural network and something like a dense neural network, or a convolutional

4189
05:00:07,320 --> 05:00:13,400
neural network, is the fact that it contains an internal loop. Now, what this really means is

4190
05:00:13,400 --> 05:00:18,680
that the recurrent neural network does not process our entire data at once. So it doesn't

4191
05:00:18,680 --> 05:00:24,360
process the entire training example, or the entire input to the model at once, what it does is

4192
05:00:24,440 --> 05:00:30,920
processes it at different time steps, and maintains what we call an internal memory, and kind of an

4193
05:00:30,920 --> 05:00:37,240
internal state, so that when it looks at a new input, it will remember what it's seen previously

4194
05:00:37,240 --> 05:00:42,600
and treat that input based on kind of the context or the understanding it's already developed.

4195
05:00:42,600 --> 05:00:47,240
Now, I understand that this doesn't make any sense right now. But with a dense neural network,

4196
05:00:47,240 --> 05:00:52,680
or the neural networks we looked at so far, we call those something called feed forward neural

4197
05:00:52,680 --> 05:00:58,280
networks. What that means is we give all of our data to it at once, and we pass that data from

4198
05:00:58,280 --> 05:01:03,880
left to right, or I guess for you guys from left to right. So we give all of the information, you

4199
05:01:03,880 --> 05:01:07,960
know, we would pass those through the convolutional layer to start, maybe we have passed them through

4200
05:01:07,960 --> 05:01:13,000
dense neurons, but they get given all of the info. And then that information gets translated

4201
05:01:13,000 --> 05:01:18,200
through the network to the very end, again, from left to right. Whereas here, with recurrent neural

4202
05:01:18,200 --> 05:01:23,400
networks, we actually have a loop, which means that we don't feed the entire textual data at

4203
05:01:23,400 --> 05:01:29,960
once, we actually feed one word at a time, it processes that word, generate some output based

4204
05:01:29,960 --> 05:01:35,800
on that word, and uses the internal memory state that it's keeping track of to do that as part of

4205
05:01:35,800 --> 05:01:40,920
the calculation. So essentially, the reason we do this is because just like humans, when we, you

4206
05:01:40,920 --> 05:01:47,240
know, look at text, we don't just take a photo of this text and process it all at once, we read it

4207
05:01:47,320 --> 05:01:52,920
left to right, word to word. And based on the words that we've already read, we start to slowly

4208
05:01:52,920 --> 05:01:59,160
develop an understanding of what we're reading, right? If I just read the word now, that doesn't

4209
05:01:59,160 --> 05:02:04,520
mean much to me, if I just read the word in code, that doesn't mean much. Whereas if I read the entire

4210
05:02:04,520 --> 05:02:09,480
sentence, now that we've learned a little bit about how we can encode text, I start to develop

4211
05:02:09,480 --> 05:02:14,680
an understanding about what this next word means, based on the previous words before it, right?

4212
05:02:14,760 --> 05:02:18,200
And that's kind of the point here is that this is what a recurrent neural network is going to do

4213
05:02:18,200 --> 05:02:24,440
for us. It's going to read one word at a time, and slowly start building up its understanding

4214
05:02:24,440 --> 05:02:29,960
of what the entire textual data means. And this works in kind of a more complicated sense than

4215
05:02:29,960 --> 05:02:34,840
that will draw it out a little bit. But this is kind of what would happen if we on, I guess,

4216
05:02:34,840 --> 05:02:39,240
unraveled a recurrent layer, because recurrent neural network, yes, it has a loop in it. But

4217
05:02:39,240 --> 05:02:43,480
really the recurrent aspect of a neural network is the layer that implements this

4218
05:02:44,040 --> 05:02:49,000
recurrent functionality with a loop. Essentially, what we can see here is that if we're saying x

4219
05:02:49,000 --> 05:02:55,960
is our input and h is our output, x t is going to be our input at time t, whereas h t is going to

4220
05:02:55,960 --> 05:03:01,880
be our output at time t. If we had a text of say length four, so four words, like we've encoded

4221
05:03:01,880 --> 05:03:07,320
them into integers now at this point, the first input at time zero will be the first word into

4222
05:03:07,320 --> 05:03:12,680
our network, right, or the first word that this layer is going to see. And the output at that time

4223
05:03:12,680 --> 05:03:17,720
is going to be our current understanding of the entire text after looking at just that one word.

4224
05:03:18,600 --> 05:03:25,080
Next, what we're going to do is process input one, which will be the next word in the sentence.

4225
05:03:25,080 --> 05:03:30,680
But we're going to use the output from the previous kind of computation or the previous iteration

4226
05:03:31,400 --> 05:03:36,360
to do this. So we're going to process this word in combination with what we've already seen,

4227
05:03:36,360 --> 05:03:40,040
and then have a new output, which hopefully should now give us an understanding of what

4228
05:03:40,120 --> 05:03:46,280
those two words mean. Next, we'll go to the third word. And so forth, and slowly start building

4229
05:03:46,280 --> 05:03:51,720
our understanding of what the entire textual data means by building it up one by one. The reason

4230
05:03:51,720 --> 05:03:56,360
we don't pass the entire sequence at once is because it's very, very difficult to just kind

4231
05:03:56,360 --> 05:04:01,880
of look at this huge blob of integers and figure out what the entire thing means. If we can do it

4232
05:04:01,880 --> 05:04:06,840
one by one and understand the meaning of specific words based on the words that have came before

4233
05:04:06,920 --> 05:04:11,320
it and start learning those patterns, that's going to be a lot easier for a neural network to deal

4234
05:04:11,320 --> 05:04:16,120
with than just passing it all at once, looking at it and trying to get some output. And that's

4235
05:04:16,120 --> 05:04:20,600
why we have these recurrent layers, there's a few different types of them. And I'm going to go

4236
05:04:20,600 --> 05:04:25,080
through them, and then we'll talk a little bit more in depth of how they work. So the first one

4237
05:04:25,080 --> 05:04:30,520
is called long short term memory. And actually, in fact, before we get into this, let's, let's

4238
05:04:30,520 --> 05:04:35,000
talk about just a first like a simple layer so that we kind of have a reference point before

4239
05:04:35,000 --> 05:04:40,280
going here. Okay, so this is kind of the example I want to use here to illustrate however current

4240
05:04:40,280 --> 05:04:45,960
neural network works and a more teaching style rather than what I was doing before. So essentially,

4241
05:04:45,960 --> 05:04:51,080
the way that this works is that this whole thing that I'm drawing here, right, all of this circle

4242
05:04:51,080 --> 05:04:56,760
stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing

4243
05:04:56,760 --> 05:05:03,000
you kind of how this works in a series of steps. So rather than passing all the information at once,

4244
05:05:03,080 --> 05:05:07,640
we're going to pass it as a sequence, which means that we're going to have all these different words

4245
05:05:07,640 --> 05:05:12,520
and we're going to pass them one at a time to the kind of to the layer, right, to this recurrent

4246
05:05:12,520 --> 05:05:17,400
layer. So we're going to start from this left side over here. So this right, you know, start over

4247
05:05:17,400 --> 05:05:23,320
here at time step zero, that's what zero means. So time step is just, you know, the order. In this

4248
05:05:23,320 --> 05:05:28,440
case, this is the first word. So let's say we have the sentence Hi, I am Tim, right, we've broken

4249
05:05:28,440 --> 05:05:31,960
these down into vectors, they've been turned into their numbers, I'm just writing them here so we

4250
05:05:31,960 --> 05:05:37,720
can kind of see what I mean in like a natural language. And they are the input to this recurrent

4251
05:05:37,720 --> 05:05:43,320
layer. So all of our different words, right, that's how many kind of little cells we're going to draw

4252
05:05:43,320 --> 05:05:46,920
here is how many words we have in this sequence that we're talking about. So in this case, we have

4253
05:05:46,920 --> 05:05:52,120
four, right, four words. So that's why I've drawn four cells to illustrate that. Now what we do is

4254
05:05:52,120 --> 05:05:58,760
that time step zero, the internal state of this layer is nothing, there's no previous output,

4255
05:05:58,760 --> 05:06:04,760
we haven't seen anything yet, which means that this first kind of cell, which is what I'm looking

4256
05:06:04,760 --> 05:06:09,960
at right here, what I'm drawing in this first cell is only going to look and consider this first word

4257
05:06:09,960 --> 05:06:14,760
and kind of make some prediction about it and do something with it. We're going to pass high to

4258
05:06:14,760 --> 05:06:19,560
this cell, some math's going to go on in here. And then what it's going to do is it's going to output

4259
05:06:19,560 --> 05:06:25,400
some value, which, you know, tells us something about the word high, right, some numeric value,

4260
05:06:25,400 --> 05:06:28,280
we're not going to talk about what that is, but it's going to do is going to be some output.

4261
05:06:29,080 --> 05:06:34,200
Now, what happens is after this cell has finished processing this, so right, so this one's done,

4262
05:06:34,200 --> 05:06:39,080
this has completed h zero, the outputs there, we'll do a check mark to say that that's done,

4263
05:06:39,080 --> 05:06:44,840
it's finished processing, this output gets fed into actually the same thing again, we're kind of

4264
05:06:44,840 --> 05:06:52,120
just keeping track of it. And now what we do is we process the next input, which is I, and we use

4265
05:06:52,120 --> 05:06:57,960
the output from the previous cell to process this and understand what it means. So now, technically,

4266
05:06:57,960 --> 05:07:03,240
we should have some output from the previous cell. So from whatever high was, right, we do some

4267
05:07:03,240 --> 05:07:09,560
analysis on the word I, we kind of combine these things together. And that's the output of this

4268
05:07:09,560 --> 05:07:16,040
cell is our understanding of not only the current input, but the previous input with the current

4269
05:07:16,040 --> 05:07:21,240
input. So we're slowly kind of building up our understanding of what this word I means,

4270
05:07:21,240 --> 05:07:25,800
based on the words we saw before. And that's the point I'm trying to get at is that

4271
05:07:25,800 --> 05:07:30,920
this network uses what it's seen previously to understand the next thing that it sees,

4272
05:07:30,920 --> 05:07:36,440
it's building a context is trying to understand not only the word, but what the word means,

4273
05:07:36,440 --> 05:07:42,120
you know, in relation to what's come before it. So that's what's happening here. So then

4274
05:07:42,120 --> 05:07:47,560
this output here, right, we get some output, we finish this, we get some output h one,

4275
05:07:47,560 --> 05:07:53,240
h one is passed into here. And now we have the understanding of what high and I means,

4276
05:07:53,240 --> 05:07:59,240
and we add am like that, we do some kind of computations, we build an understanding of what

4277
05:07:59,240 --> 05:08:06,120
this sentence is. And then we get the output h two, that passes to h three. And now finally,

4278
05:08:06,120 --> 05:08:09,720
we have this final output h three, which is going to understand hopefully,

4279
05:08:10,520 --> 05:08:16,600
what this entire thing means. Now, this is good, this works fairly well. And this is called a

4280
05:08:16,600 --> 05:08:22,680
simple RNN layer, which means that all we do is we take the output from the previous cell or the

4281
05:08:22,680 --> 05:08:27,880
previous iteration, because really, all of these cells is just an iteration almost in a for loop,

4282
05:08:27,880 --> 05:08:33,240
right, based on all the different words in our sequence. And we slowly start building to that

4283
05:08:33,240 --> 05:08:39,720
understanding as we go through the entire sequence. Now, the only issue with this is that as we have

4284
05:08:39,720 --> 05:08:46,520
a very long sequence, so sequences of length, say 100 or 150, the beginning of those sequences

4285
05:08:46,520 --> 05:08:51,880
starts to kind of get lost. As we go through this, because remember, all we're doing, right,

4286
05:08:51,880 --> 05:08:57,320
is the output from h two is really a combination of the output from h zero and h one, and then

4287
05:08:57,320 --> 05:09:02,200
there's a new word that we've looked at. And h three is now a combination of everything before it,

4288
05:09:02,200 --> 05:09:06,440
and this new word. So it becomes increasingly difficult for our model to actually

4289
05:09:07,000 --> 05:09:12,040
build a really good understanding of the text in general, when the sequence gets long, because

4290
05:09:12,040 --> 05:09:16,920
it's hard for it to remember what it's seen at the very beginning, because that is now so insignificant,

4291
05:09:16,920 --> 05:09:21,800
there's been so many outputs tacked on to that, that it's hard for it to go back and see that if

4292
05:09:21,800 --> 05:09:26,520
that makes any sense. Okay, so what I'm going to do now is try to explain the next layer we're

4293
05:09:26,520 --> 05:09:32,120
going to look at, which is called LSTM. So the previous layer we just looked at the recurrent

4294
05:09:32,120 --> 05:09:36,680
layer was called a simple RNN layer. So simple recurrent neural network layer, whatever you

4295
05:09:36,680 --> 05:09:41,240
want to call it, right, simple recurrent layer. Now we're going to talk about the layer, which is

4296
05:09:41,240 --> 05:09:47,640
LSTM, which stands for long, short term memory. Now, long and short are hyphenated together.

4297
05:09:47,640 --> 05:09:51,320
But essentially, what we're doing, and it just gets a little bit more complex, but I won't go

4298
05:09:51,320 --> 05:09:58,120
into the math, is we add another component that keeps track of the internal state. So right now,

4299
05:09:58,120 --> 05:10:03,320
the only thing that we were tracking as kind of our internal state as the memory for this model

4300
05:10:03,320 --> 05:10:09,640
was the previous output. So whatever the previous output was. So for example, at time zero here,

4301
05:10:10,280 --> 05:10:16,120
there was no previous output. So there was nothing being kept in this model. But at time one, the

4302
05:10:16,120 --> 05:10:23,320
output from this cell right here was what we were storing. And then at cell two, the only thing we

4303
05:10:23,400 --> 05:10:30,440
were storing was the output at time one, right? And we've lost now the output from time zero.

4304
05:10:31,080 --> 05:10:37,000
What we're adding in long, short term memory is an ability to access the output from any

4305
05:10:37,000 --> 05:10:42,040
previous state at any point in the future when we want it. Now, what this means is that rather

4306
05:10:42,040 --> 05:10:47,800
than just keeping track of the previous output, we'll add all of the outputs that we've seen so far

4307
05:10:47,800 --> 05:10:51,480
into what I'm going to call my little kind of conveyor belt, it's going to run at the top

4308
05:10:51,480 --> 05:10:54,920
up here. I know it's kind of hard to see, but it's just what I'm highlighting. It's almost just

4309
05:10:54,920 --> 05:11:01,080
like a lookup table that can tell us the output at any previous cell that we want. So we can kind

4310
05:11:01,080 --> 05:11:05,080
of add things to this conveyor belt, we can pull things off, we can look at them. And this just

4311
05:11:05,080 --> 05:11:10,280
adds a little bit of complexity to the model. It allows us to not just remember the last state,

4312
05:11:10,280 --> 05:11:16,760
but look anywhere at any point in time, which can be useful. Now, I don't want to go into much

4313
05:11:16,760 --> 05:11:21,240
more depth about exactly how this works. But essentially, you know, just think about the

4314
05:11:21,240 --> 05:11:26,680
idea that as the sequence gets very long, it's pretty easy to forget the things we saw at the

4315
05:11:26,680 --> 05:11:30,920
beginning. So if we can keep track of some of the things we've seen at the beginning, and some of

4316
05:11:30,920 --> 05:11:35,320
the things in between on this little conveyor belt, and we can access them whenever we want,

4317
05:11:35,320 --> 05:11:39,080
then that's going to make this probably a much more useful layer, right? We could look at the

4318
05:11:39,080 --> 05:11:45,240
first sentence and the last sentence of a big piece of text at any point that we want and say,

4319
05:11:45,240 --> 05:11:50,520
okay, you know, this tells us X about the meaning of this text, right? So that's what this LSTM

4320
05:11:50,520 --> 05:11:55,000
does. Again, I don't want to go too far. We've already spent a lot of time kind of covering,

4321
05:11:55,000 --> 05:11:58,840
you know, recurrent layers and how all this works. Anyways, if you do want to look it up,

4322
05:11:58,840 --> 05:12:02,920
some great mathematical definitions, again, I will source everything at the bottom of this

4323
05:12:02,920 --> 05:12:07,400
document so you can go there. But again, that's LSTM, long short term memory, that's what we're

4324
05:12:07,400 --> 05:12:12,520
going to use for some of our examples, although simple RNN does work fairly well for shorter

4325
05:12:12,520 --> 05:12:17,080
length sequences. And again, remember, we're treating our text as a sequence now, where we're

4326
05:12:17,080 --> 05:12:21,720
going to feed each word into the recurrent layer, and it's going to slowly start to develop an

4327
05:12:21,720 --> 05:12:25,960
understanding as it reads through each word, right and processes that. Okay, so now we are

4328
05:12:25,960 --> 05:12:31,000
on to our first example, where we're going to be performing sentiment analysis on movie reviews

4329
05:12:31,000 --> 05:12:35,560
to determine whether they are positive reviews or negative reviews. Now, we already know what

4330
05:12:35,560 --> 05:12:39,160
sentiment means. That's essentially what I just described. So picking up, you know, whether a

4331
05:12:39,800 --> 05:12:44,600
block of text is considered positive or negative. And for this example, we're going to be using the

4332
05:12:44,600 --> 05:12:49,960
movie review data sets. Now, as per usual, this is based off of this TensorFlow tutorial slash

4333
05:12:49,960 --> 05:12:55,320
guide. I found this one kind of confusing to follow in the TensorFlow website, but obviously

4334
05:12:55,320 --> 05:12:59,640
you can follow along with that if you don't prefer that version over mine. But anyways,

4335
05:12:59,640 --> 05:13:03,800
we're going to be talking about the movie review data set. So this data set is straight from Keras,

4336
05:13:03,880 --> 05:13:09,960
and it contains 25,000 reviews, which are already pre processed and labeled. Now, what that means

4337
05:13:09,960 --> 05:13:14,760
for us is that every single word is actually already encoded by an integer. And in fact,

4338
05:13:14,760 --> 05:13:20,360
they've done kind of a clever encoding system where what they've done is said, if a character is

4339
05:13:20,360 --> 05:13:27,320
encoded by say integer zero, that represents how common that word is in the entire data set. So

4340
05:13:27,320 --> 05:13:31,480
if an integer was encoded, but are not interested, a word was encoded by integer three, that would

4341
05:13:31,480 --> 05:13:35,720
mean that it is the third most common word in the data set. And in this specific data set,

4342
05:13:35,720 --> 05:13:41,720
we have a vocabulary size of 88,584 unique words, which means that something that was

4343
05:13:41,720 --> 05:13:47,640
classified as this. So 88,584 would be the least common word in the data set. So something to

4344
05:13:47,640 --> 05:13:51,720
keep in mind, we're going to load in the data set and do our imports just by hitting run here.

4345
05:13:51,720 --> 05:13:56,040
And as I've mentioned previously, you know, I'm not going to be typing this stuff out. It's just

4346
05:13:56,040 --> 05:14:00,520
it's kind of a waste of time. I don't have all the syntax memorized. I would never expect you

4347
05:14:00,520 --> 05:14:05,560
guys to memorize this either. But what I will do is obviously walk through the code step by step,

4348
05:14:05,560 --> 05:14:11,640
and make sure you understand why it is that we have what we have here. Okay, so what we've done

4349
05:14:11,640 --> 05:14:17,640
is to find the vocabulary size, the max length of a review, and the batch size. Now what we've

4350
05:14:17,640 --> 05:14:22,440
done is just loaded in our data set by defining the vocabulary size. So this is just the words

4351
05:14:22,440 --> 05:14:27,080
it will include. So in this case, all of them, then we have trained data, trained labels, test

4352
05:14:27,160 --> 05:14:31,640
data, test labels. And we can look at a review and see what it looks like by doing something

4353
05:14:31,640 --> 05:14:36,120
like this. So this is an example of our first review, we can see kind of the different encodings

4354
05:14:36,120 --> 05:14:40,840
for all of these words. And this is what it looks like, they're already in integer form.

4355
05:14:40,840 --> 05:14:45,560
Now, just something to note here is that the length of our reviews are not unique. So if I do

4356
05:14:45,560 --> 05:14:50,840
the length of trained data, I guess I wouldn't say unique, but I mean, they're just all different.

4357
05:14:50,840 --> 05:14:54,280
So the length of trained data is zero is different than the length of trained data one,

4358
05:14:54,280 --> 05:14:57,880
right? So that's something to consider as we go through this and something we're actually going

4359
05:14:57,880 --> 05:15:02,680
to have to handle. Okay, so more pre processing. So this is what I was talking about. If you have

4360
05:15:02,680 --> 05:15:06,280
a look at our loaded interviews, we'll notice there are different lengths, this is an issue,

4361
05:15:06,280 --> 05:15:10,520
we cannot pass different length data into our neural network, which is true. Therefore, we

4362
05:15:10,520 --> 05:15:14,760
must make each review the same length. Okay, so what we're going to do for now is we're actually

4363
05:15:14,760 --> 05:15:19,800
going to pad our sequences. Now what that means is we're going to follow this kind of step that

4364
05:15:19,880 --> 05:15:25,880
I've talked about here. So if the review is greater than 250 words, we will trim off extra words,

4365
05:15:25,880 --> 05:15:31,640
if the review is less than 250 words, we'll add the necessary amount of this should actually be

4366
05:15:31,640 --> 05:15:38,360
zeros in here, let's fix this of zeros to make it equal to 250. So what that means is we're

4367
05:15:38,360 --> 05:15:42,040
essentially going to add some kind of padding to our review. So in this case, I believe we're

4368
05:15:42,040 --> 05:15:46,280
actually going to pad to the left side, which means that say we have a review of length, you know,

4369
05:15:46,360 --> 05:15:51,640
200, we're going to add 50, just kind of blank words, which will represent with the index zero

4370
05:15:51,640 --> 05:15:57,640
to the left side of the review to make it the necessary length. So that's, that's good, we'll

4371
05:15:57,640 --> 05:16:02,200
do that. So if we look at train data and test data, what this does is we're just going to use

4372
05:16:02,200 --> 05:16:06,680
something from Keras, which we've imported above. So we're saying from Keras dot pre processing

4373
05:16:06,680 --> 05:16:11,000
import sequence, again, we're treating our text data as a sequence, as we've talked about,

4374
05:16:11,000 --> 05:16:15,960
we're going to say sequence dot pad sequences, train data, and then we define the length that

4375
05:16:15,960 --> 05:16:21,080
we want to pad it to. So that's what this will do. It will perform these steps that we've already

4376
05:16:21,080 --> 05:16:25,560
talked about. And again, we're just going to assign test data and train data to, you know,

4377
05:16:25,560 --> 05:16:29,880
whatever this does for us, we can pass the entire thing, it'll pad all of them for us at once.

4378
05:16:30,600 --> 05:16:36,360
Okay, so let's run that. And then let's just have a look at say train data one now, because

4379
05:16:36,360 --> 05:16:42,200
remember, this was like 189, right? So if we look at train data, so train underscore data one,

4380
05:16:43,000 --> 05:16:49,000
like that, we can see that as an array with a bunch of zeros before, because that is the padding

4381
05:16:49,000 --> 05:16:53,960
that we've employed to make it the correct length. Okay, so that's padding, that's something that

4382
05:16:53,960 --> 05:16:58,440
we're probably going to have to do most of the time, when we feed something to our neural networks.

4383
05:16:58,440 --> 05:17:01,640
All right, so the next step is actually to create the model. Now this model is pretty

4384
05:17:01,640 --> 05:17:07,240
straightforward. We have an embedding layer and LSTM in a dense layer here. So the reason we've

4385
05:17:07,240 --> 05:17:11,560
done dense with the activation function of sigmoid at the end is because we're trying to

4386
05:17:11,560 --> 05:17:16,760
pretty much predict the sentiment of this, right? Which means that if we have the sentiment between

4387
05:17:16,760 --> 05:17:22,840
zero and one, then if a number is greater than 0.5, we could classify that as a positive review.

4388
05:17:22,840 --> 05:17:26,680
And if it's less than 0.5 or equal, you know, whatever you want to set the bounds at,

4389
05:17:26,680 --> 05:17:31,240
then we could say that's a negative review. So sigmoid, as we probably might recall,

4390
05:17:31,240 --> 05:17:35,640
squishes our values between zero and one. So whatever the value is at the end of the network

4391
05:17:35,640 --> 05:17:39,960
will be between zero and one, which means that, you know, we can make the accurate prediction.

4392
05:17:40,760 --> 05:17:45,160
Now here, the reason we have the embedding layer, like, well, we've already pre processed our review

4393
05:17:45,160 --> 05:17:49,560
is even though we've pre processed this with these integers, and they are a bit more meaningful than

4394
05:17:49,560 --> 05:17:54,040
just our random lookup table that we've talked about before, we still want to pass that to an

4395
05:17:54,040 --> 05:17:59,640
embedding layer, which is going to find a way more meaningful representation for those numbers

4396
05:17:59,640 --> 05:18:03,720
than just their integer values already. So it's going to create those vectors for us. And this

4397
05:18:03,720 --> 05:18:09,560
32 is denoting the fact that we're going to make the output of every single one of our embeddings

4398
05:18:09,640 --> 05:18:15,480
or vectors that are created 32 dimensions, which means that when we pass them to the LSTM layer,

4399
05:18:15,480 --> 05:18:20,200
we need to tell the LSTM layer, it's going to have 32 dimensions for every single word,

4400
05:18:20,200 --> 05:18:24,680
which is what we're doing. And this will implement that long short term memory process we talked

4401
05:18:24,680 --> 05:18:32,280
about before, and output the final output to TF dot cares dot layers dot dense, which will tell us,

4402
05:18:32,280 --> 05:18:38,520
you know, that's what this is, right? It'll make the prediction. So that's what this model is.

4403
05:18:39,080 --> 05:18:43,720
We can see, give us a second to run here, the model summary, which is already printed out,

4404
05:18:43,720 --> 05:18:47,560
we can look at the fact that the embedding layer actually has the most amount of parameters,

4405
05:18:47,560 --> 05:18:51,000
because essentially, it's trying to figure out, you know, all these different numbers,

4406
05:18:51,000 --> 05:18:56,120
how can we convert that into a tensor of 32 dimensions, which is not that easy to do. And

4407
05:18:56,120 --> 05:19:00,200
this is going to be the major aspect that's being trained. And then we have our LSTM layer,

4408
05:19:00,200 --> 05:19:04,920
we can see the parameters there. And our final dense layer, which is eight getting 33 parameters,

4409
05:19:04,920 --> 05:19:10,680
that's because the output from every single one of these dimensions 32 plus a bias node, right,

4410
05:19:10,680 --> 05:19:14,440
that we need. So that's what we'll get there. You can see model dot summary.

4411
05:19:15,560 --> 05:19:20,040
We get the sequential model. Okay, so training. Alright, so now it's time to compile and train

4412
05:19:20,040 --> 05:19:24,120
the model, you can see I've already trained mine. What I'm going to say here is if you want to speed

4413
05:19:24,120 --> 05:19:27,480
up your training, because this will actually take a second, and we'll talk about why we pick these

4414
05:19:27,480 --> 05:19:35,640
things in a minute is go to runtime, change runtime type, and add a hardware accelerator of GPU.

4415
05:19:36,360 --> 05:19:40,280
What this will allow you to do is utilize a GPU while you're training, which should speed up your

4416
05:19:40,280 --> 05:19:45,560
training by about 10 to 20 times. So I probably should have mentioned that beforehand. But you

4417
05:19:45,560 --> 05:19:51,720
can do that. And please do for these examples. So model dot compile. Alright, so we're compiling

4418
05:19:51,720 --> 05:19:56,280
our model, we're picking the loss function as binary cross entropy. The reason we're picking

4419
05:19:56,280 --> 05:20:00,920
this is because this is going to essentially tell us how far away we are from the correct

4420
05:20:01,720 --> 05:20:06,040
probability, right, because we have two different things we could be predicting. So you know, either

4421
05:20:06,040 --> 05:20:12,200
zero or one, so positive or negative. So this will give us a correct loss for that kind of

4422
05:20:12,200 --> 05:20:16,360
problem that we've talked about before. The optimizer, we're going to use rms prop. Again,

4423
05:20:16,360 --> 05:20:19,480
I'm not going to discuss all the different optimizers, you can look them up if you care

4424
05:20:19,480 --> 05:20:24,280
that much about what they do. And we're going to use metrics as ACC. One thing I will say is

4425
05:20:24,280 --> 05:20:28,920
the optimizer is not crazy important. For this one, you could use Adam if you wanted to, and it

4426
05:20:28,920 --> 05:20:34,040
would still work fine. My usual go to is just use the atom optimizer unless you think there's a better

4427
05:20:34,040 --> 05:20:38,520
one to use. But anyways, that's something to mention. Okay, so finally, we will fit the model,

4428
05:20:38,520 --> 05:20:42,440
we've looked at the syntax a lot before. So model that fit, we'll give the training data,

4429
05:20:42,440 --> 05:20:47,720
the training labels, the epochs, and we'll do a validation split of 20%. So that's what 0.2 stands

4430
05:20:47,720 --> 05:20:53,160
for, which means that what we're going to be doing is using 20% of the training data to actually

4431
05:20:53,160 --> 05:20:57,880
evaluate and validate the model as we go through. And we can see that after training, which I've

4432
05:20:57,880 --> 05:21:01,880
already done, and you guys are welcome to obviously do on your own computer, we kind of stall at an

4433
05:21:01,880 --> 05:21:08,440
evaluation accuracy of about 88%. Whereas the model actually gets overfit to about 97 98%.

4434
05:21:09,320 --> 05:21:13,400
So what this is telling us essentially is that we don't have enough training data, and that

4435
05:21:13,400 --> 05:21:18,120
after we've even done just one epoch, we're pretty much stuck on the same validation accuracy,

4436
05:21:18,120 --> 05:21:21,480
and that there's something that needs to change in the model to make it better. But for now,

4437
05:21:21,480 --> 05:21:25,720
that's fine, we'll leave it the way that it is. Okay, so now we can look at the results. I've

4438
05:21:25,720 --> 05:21:30,760
already did the results here, just to again, speed up some time, but we'll do the evaluation on our

4439
05:21:30,760 --> 05:21:36,440
test data and test labels to get a more accurate kind of result here. And that tells us we have

4440
05:21:36,440 --> 05:21:42,600
an accuracy of about 85.5%, which you know, isn't great, but it's decent considering that we didn't

4441
05:21:42,600 --> 05:21:47,240
really write that much code to get to the point that we're at right now. Okay, so that's what

4442
05:21:47,240 --> 05:21:50,920
we're getting. The model has been trained. Again, it's not too complicated. And now we're

4443
05:21:50,920 --> 05:21:56,040
on to making predictions. So the idea is that now we've trained our model, and we want to actually

4444
05:21:56,040 --> 05:22:02,280
use it to make a prediction on some kind of movie review. So since our data was pre processed, when

4445
05:22:02,280 --> 05:22:07,160
we gave it to the model, that means we actually need to process anything we want to make a prediction

4446
05:22:07,160 --> 05:22:11,720
on in the exact same way, we need to use the same lookup table, we need to encode it, you know,

4447
05:22:11,720 --> 05:22:16,280
precisely the same. Otherwise, when we give it to the model, it's going to think that the words

4448
05:22:16,280 --> 05:22:20,760
are different, and it's not going to make an accurate prediction. So what I've done here is

4449
05:22:20,760 --> 05:22:27,960
I've made a function that will encode any text into what do you call the proper pre processed

4450
05:22:27,960 --> 05:22:33,000
kind of integers, right, just like our training data was pre processed. That's what this function

4451
05:22:33,000 --> 05:22:37,000
is going to do for us is pre processed some line of text. So what I've done is actually

4452
05:22:37,000 --> 05:22:45,160
gotten the lookup table. So essentially, the mappings from IBM, I be IMDB, I could read that

4453
05:22:45,160 --> 05:22:49,800
properly. From that data set that we loaded earlier. So let me go see if I can find where I

4454
05:22:49,800 --> 05:22:56,360
defined IMDB, you can see up here. So keras dot data sets import IMDB, just like we loaded it in,

4455
05:22:56,360 --> 05:23:01,160
we can also actually get all of the word indexes or that map, we can actually print this out if

4456
05:23:01,160 --> 05:23:05,720
we want to look at what it is after. But anyways, we have that mapping, which means that all we need

4457
05:23:05,720 --> 05:23:13,480
to do is keras dot preprocessing dot text dot text, two word sequence, what this means is give

4458
05:23:13,720 --> 05:23:19,000
in some text convert all of that text into what we call tokens, which are just the individual

4459
05:23:19,000 --> 05:23:23,800
words themselves. And then what we're going to do is just use a kind of for loop inside of here

4460
05:23:23,800 --> 05:23:30,600
that says word index at word, if word in word index, L zero for word in tokens. Now what this

4461
05:23:30,600 --> 05:23:38,040
means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary

4462
05:23:38,040 --> 05:23:44,040
of 88,000 words, then what we'll do is replace its location in the list with that specific word,

4463
05:23:44,760 --> 05:23:49,560
or with that specific integer that represents it, otherwise we'll put zero just to stand for,

4464
05:23:49,560 --> 05:23:54,840
you know, we don't know what this character is. And then what we'll do is return sequence dot pad

4465
05:23:54,840 --> 05:24:01,160
sequences, and we'll pad this token sequence, and just return actually the first index here.

4466
05:24:01,160 --> 05:24:06,200
The reason we're doing that is because this pad sequences works on a list of sequences,

4467
05:24:06,280 --> 05:24:10,760
so multiple sequences. So we need to put this inside a list, which means that this is going

4468
05:24:10,760 --> 05:24:15,480
to return to us a list of lists. So we just obviously want the first entry, because we only

4469
05:24:15,480 --> 05:24:20,120
want, you know, that one sequence that we padded. So that's how this works. Sorry, that's a bit of

4470
05:24:20,120 --> 05:24:23,480
a mouthful to explain, but you guys can run through and print the stuff out if you want to see how

4471
05:24:23,480 --> 05:24:27,960
all of it works specifically. But yeah, so we can run this cell and have a look at what this

4472
05:24:27,960 --> 05:24:32,600
actually does for us on some sample text. So that maybe was just amazing. So amazing, we can see

4473
05:24:32,600 --> 05:24:36,840
we get the output that we were kind of expecting. So integer encoded words down here, and then a

4474
05:24:36,840 --> 05:24:41,560
bunch of zeros just for all the padding. Now, while we're at it, I decided why not we why don't we

4475
05:24:41,560 --> 05:24:46,360
make a decode function so that if we have any movie review like this, that's in the integer

4476
05:24:46,360 --> 05:24:50,840
form, we can decode that into the text value. So the way we're going to do that is start by

4477
05:24:50,840 --> 05:24:55,880
reversing the word index that we just created. Now the reason for that is the word index we

4478
05:24:55,880 --> 05:25:01,720
looked at, which is this right, goes from word to integer. But we actually now want to go from

4479
05:25:01,720 --> 05:25:06,520
integer to word so that we can actually translate a sentence, right? So what I've done is made this

4480
05:25:06,520 --> 05:25:11,880
decode integers function, we've set the padding key as zero, which means that if we see zero,

4481
05:25:11,880 --> 05:25:16,040
that's really just means you know, nothing's there. We're going to create a text string,

4482
05:25:16,040 --> 05:25:21,000
which we're going to add to. And I'm just gonna say for num in integers, integers is our input,

4483
05:25:21,000 --> 05:25:25,240
which will be a list that looks something like this or an array, whatever you want to call it,

4484
05:25:25,240 --> 05:25:29,480
we're gonna say if number does not equal pad. So essentially, if the number is not zero, right,

4485
05:25:29,480 --> 05:25:34,840
it's not padding, then what we'll do is add the lookup of reverse word index num. So whatever

4486
05:25:34,840 --> 05:25:41,080
that number is, into this new string plus a space, and then just return text colon negative one,

4487
05:25:41,080 --> 05:25:45,000
which means return everything except the last space that we would have added. And then if I

4488
05:25:45,000 --> 05:25:52,280
print the decode integers, we can see that this encoded thing that we have before, which looks

4489
05:25:52,280 --> 05:25:57,880
like this gets encoded by the string that movie was just amazing. So maybe sorry, not encoded,

4490
05:25:57,880 --> 05:26:03,000
decoded, because this was the encoded form. So that's how that works. Okay, so now it's

4491
05:26:03,000 --> 05:26:07,400
time to actually make a prediction. So I've written a function here that will make a prediction on

4492
05:26:07,400 --> 05:26:12,200
some piece of text as the movie review for us. And I'll just walk us through quickly how this

4493
05:26:12,200 --> 05:26:16,360
works. And then I'll show us the actual output from our model, you know, making predictions like

4494
05:26:16,360 --> 05:26:21,640
this. So what we say is we'll take some parameter text, which will be our movie review. And we're

4495
05:26:21,640 --> 05:26:26,840
going to encode that text using the encode text function we've created above. So just this one

4496
05:26:26,840 --> 05:26:32,120
right here that essentially takes our sequence of, you know, words, we get the pre processing,

4497
05:26:32,120 --> 05:26:36,360
so turn that into a sequence, remove all the spaces, whatnot, you know, get the words,

4498
05:26:36,360 --> 05:26:42,440
then we turn those into the integers, we have that we return that. So here we have our proper

4499
05:26:42,440 --> 05:26:49,960
pre processed text. Then what we do is we create a blank NumPy array that is just a bunch of zeros

4500
05:26:50,600 --> 05:26:55,320
that's in the form one to 50 or in that shape. Now the reason I'm putting in that in that shape

4501
05:26:55,320 --> 05:27:02,440
is because the shape that our model expects is something 250, which means some number of entries,

4502
05:27:02,440 --> 05:27:08,040
and then 250 integers representing each word, right? Because that's the length of movie review

4503
05:27:08,680 --> 05:27:12,760
is what we've told the model is the length 250. So that's the length of the review.

4504
05:27:13,400 --> 05:27:18,920
Then what we do is we put pred zero. So that's what's up here, equals the encoded text. So we

4505
05:27:18,920 --> 05:27:26,520
just essentially insert our one entry into this, this array we've created. Then what we do is say

4506
05:27:26,520 --> 05:27:33,320
modeled up predict on that array, and just return and print the result zero. Now, that's pretty

4507
05:27:33,320 --> 05:27:37,480
much all there is to it. I mean, that's how it works. The reason we're doing result zero is

4508
05:27:37,480 --> 05:27:43,080
because again, model is optimized to predict on multiple things, which means like I would have

4509
05:27:43,080 --> 05:27:48,280
to do, you know, list of encoded text, which is kind of what I've done by just doing this

4510
05:27:48,280 --> 05:27:53,720
prediction lines here, which means it's going to return to me an array of arrays. So if I want

4511
05:27:53,720 --> 05:27:58,120
the first prediction, I need to index zero, because that will give me the prediction for

4512
05:27:58,120 --> 05:28:03,240
our first and only entry. Alright, so I hope that makes sense. Now we have a positive review I've

4513
05:28:03,240 --> 05:28:07,080
written and a negative review, and we're just going to compare the analysis on both of them.

4514
05:28:07,080 --> 05:28:10,360
So that movie was so awesome. I really loved it and would watch it again, because it was

4515
05:28:10,360 --> 05:28:14,040
amazingly great. And then that movie sucked, I hated it and wouldn't watch it again, was one

4516
05:28:14,120 --> 05:28:18,280
of the worst things I've ever watched. So let's look at this now. And we can see the first one

4517
05:28:18,280 --> 05:28:24,440
gets predicted at 72% positive, whereas the other one is 23% positive. So essentially what that

4518
05:28:24,440 --> 05:28:28,280
means is that, you know, if the lower the number, the more negative we're predicting it is, the

4519
05:28:28,280 --> 05:28:32,600
higher the number, the more positive we're predicting it is. If we wanted to not just print

4520
05:28:32,600 --> 05:28:37,720
out this value, and instead what we wanted to do was print out, you know, positive or negative,

4521
05:28:37,720 --> 05:28:42,600
we could just make a little if statement that says if this number is greater than 0.5, say positive,

4522
05:28:42,600 --> 05:28:48,200
otherwise say not say negative, right? And I just want to show you that changing these reviews

4523
05:28:48,200 --> 05:28:53,240
ever so slightly actually makes a big difference. So if I remove the word awesome, so that movie

4524
05:28:53,240 --> 05:28:58,680
was so and then I run this, you can see that Oh, wow, this actually increases and goes up to 84%.

4525
05:28:59,640 --> 05:29:04,120
So the presence of certain words in certain locations actually makes a big difference. And

4526
05:29:04,120 --> 05:29:09,720
especially when we have a shorter length review, right, if we have a longer length review, it

4527
05:29:09,800 --> 05:29:13,720
won't make that big of a difference. But even the removal of a few words here. And let's see,

4528
05:29:13,720 --> 05:29:19,560
so the removing the word awesome changed it by almost like 10%. Right. Now if I move, so let's

4529
05:29:19,560 --> 05:29:23,640
see if that makes a bigger difference. It makes a very little difference because it's learned,

4530
05:29:23,640 --> 05:29:28,120
at least the model, right, that the word so doesn't really make a huge impact into

4531
05:29:28,120 --> 05:29:33,080
the type of review. Whereas if I remove the word I, let's see if that makes a big impact,

4532
05:29:33,080 --> 05:29:37,320
probably not right now, it goes back up to 84. So that's cool. And that's something to play with

4533
05:29:37,320 --> 05:29:41,960
is removing certain words and seeing how much impact those actually carry. And even if I just

4534
05:29:41,960 --> 05:29:45,800
add the word great, like would great to watch it again, just in the middle of the sentence,

4535
05:29:45,800 --> 05:29:49,720
doesn't have to make any sense. Let's look at this here. Oh, boom, we increase like a little

4536
05:29:49,720 --> 05:29:54,520
bit, right. And let's say if I add this movie, you really suck. Let's see if that makes a difference.

4537
05:29:55,320 --> 05:29:59,720
No, that just reduces it like a tiny bit. So something cool, something to play with.

4538
05:29:59,720 --> 05:30:04,280
Anyways, now let's move on to the next example. So now we're on to our last and final example,

4539
05:30:04,360 --> 05:30:09,400
which is going to be creating a recurrent neural network play generator. Now, this is going to

4540
05:30:09,400 --> 05:30:13,320
be the first kind of neural network we've done, that's actually going to be creating something

4541
05:30:13,320 --> 05:30:18,680
for us. But essentially, what we're going to do is make a model that's capable of predicting the

4542
05:30:18,680 --> 05:30:23,800
next character in a sequence. So we're going to give it some sequence as an input. And what it's

4543
05:30:23,800 --> 05:30:28,200
going to do is just simply predict the most likely next character. Now, there's quite a bit

4544
05:30:28,200 --> 05:30:31,880
that's going to go into this. But the way we're going to use this to predict a play is we're

4545
05:30:31,880 --> 05:30:37,400
going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then

4546
05:30:37,400 --> 05:30:41,800
we're going to have it. So that we'll ask the model will give it some starting prompt, some

4547
05:30:41,800 --> 05:30:46,840
string to start with. And that'll be the first thing we pass to it, it will predict to us what

4548
05:30:46,840 --> 05:30:52,120
the most likely next character for that sequence is. And we'll take the output from the model

4549
05:30:52,120 --> 05:30:57,400
and feed it as the input again to the model and keep predicting sequence of characters. So

4550
05:30:57,400 --> 05:31:02,280
keep predicting the next character from the previous output as many times as we want to

4551
05:31:02,280 --> 05:31:06,760
generate an entire play. So we're going to have this neural network that's capable of predicting

4552
05:31:06,760 --> 05:31:12,920
one letter at a time actually end up generating an entire play for us by running it multiple

4553
05:31:12,920 --> 05:31:18,280
times on the previous output from the last iteration. Now, that's kind of the problem.

4554
05:31:18,280 --> 05:31:21,640
That's what we're trying to solve. So let's go ahead and get into it and talk about what's

4555
05:31:21,640 --> 05:31:25,800
involved in doing this. So the first thing we're going to do obviously is our imports. So from

4556
05:31:25,800 --> 05:31:33,080
Keras dot preprocessing import sequence, import Keras, we need TensorFlow NumPy and OS. So we'll

4557
05:31:33,080 --> 05:31:38,040
load that in. And now what we're going to do is download the file. So the data set for Romeo and

4558
05:31:38,040 --> 05:31:43,720
Juliet, which we can get by using this line here. So Keras has this utils thing, which will allow

4559
05:31:43,720 --> 05:31:48,920
us to get a file, save it as whatever we want. In this case, we're going to save it as Shakespeare

4560
05:31:48,920 --> 05:31:53,960
dot txt. And we're going to get that from this link. Now, I believe this is just some like shared

4561
05:31:53,960 --> 05:31:59,000
drive that we have access to from Keras. So we'll load that in here. And then this will

4562
05:31:59,000 --> 05:32:04,520
simply give us the path on this machine. Because remember, this is Google Collaboratory to this

4563
05:32:04,520 --> 05:32:09,480
text file. Now, if you want, you can actually load in your own text data. So we don't necessarily

4564
05:32:09,480 --> 05:32:13,720
need to use the Shakespeare play, we could use anything we want. In fact, an example that I'll

4565
05:32:13,720 --> 05:32:18,920
show later is using the B movie script. But the way you do that is run this block of code here.

4566
05:32:19,880 --> 05:32:24,280
And you'll see that it pops up this thing for choose files, just choose a file from your

4567
05:32:25,800 --> 05:32:31,400
local computer. And then what that will do is just save this on Google Collaboratory. And

4568
05:32:31,400 --> 05:32:35,400
then that will allow you to actually use that. So make sure that's a text file that you're loading

4569
05:32:35,400 --> 05:32:41,000
in there. But regardless, that should work. And then from there, you'll be good to go. So if you,

4570
05:32:41,000 --> 05:32:44,440
you know, you don't need to do that, you can just run this block of code here, if you want to load

4571
05:32:44,600 --> 05:32:49,960
in the Shakespeare txt, but otherwise, you can load in your own file. Now, after we do that,

4572
05:32:49,960 --> 05:32:54,120
what we want to do is actually open this file. So remember, that was just saving the path to it.

4573
05:32:54,120 --> 05:32:59,240
So we'll open that file in RB mode, which is read bytes mode, I believe. And then we're going to

4574
05:32:59,240 --> 05:33:04,200
say dot read, so we're going to read that in as an entire string, we're going to decode that into

4575
05:33:04,200 --> 05:33:09,160
utf a format. And then we're just printing the length of the text or the amount of characters

4576
05:33:09,240 --> 05:33:14,200
in the text. So if we do that, we can see we have the length of the text is 1.1 million

4577
05:33:14,200 --> 05:33:18,920
characters, approximately. And then we can have a look at the first 250 characters by doing this.

4578
05:33:19,720 --> 05:33:23,800
So we can see that this is kind of what the plate looks like, we have whoever's speaking,

4579
05:33:23,800 --> 05:33:29,640
colon, then some line, whoever's speaking, colon, some line, and there's all these break lines.

4580
05:33:29,640 --> 05:33:34,040
So backslash ends, which are telling us, you know, go to the next line, right? So it's going

4581
05:33:34,040 --> 05:33:37,880
to be important because we're going to hope that our neural network will be able to predict

4582
05:33:37,880 --> 05:33:43,240
things like break lines and spaces, and even this kind of format as we teach it more and get

4583
05:33:43,240 --> 05:33:49,400
further in. But now it's time to talk about encoding. So obviously, all of this text is in

4584
05:33:49,400 --> 05:33:54,520
text form, it's not pre processed for us, which means we need to pre process it and encode it

4585
05:33:54,520 --> 05:33:58,920
as integers before we can move forward. Now, fortunately, for us, this problem is actually

4586
05:33:58,920 --> 05:34:03,480
a little bit easier than the problem we discussed earlier with encoding words, because what we're

4587
05:34:03,480 --> 05:34:08,440
going to do is simply encode each character in the text with an integer. Now, you can imagine

4588
05:34:08,440 --> 05:34:13,720
why this makes this easier, because there really is a finite set of characters, whereas there's

4589
05:34:13,720 --> 05:34:19,080
kind of indefinite or, you know, I guess, infinite amount of words that could be created. So we're

4590
05:34:19,080 --> 05:34:24,360
not really going to run into the problem where, you know, two words are encoded with such different

4591
05:34:24,360 --> 05:34:28,680
or two characters are encoded with such different integers, that it makes it difficult for the model

4592
05:34:28,680 --> 05:34:34,120
to understand. Because I mean, and we can look at what the value of vocab is here, we're only

4593
05:34:34,120 --> 05:34:38,440
going to have so many characters in the text. And for characters, it just doesn't matter as much,

4594
05:34:38,440 --> 05:34:43,560
because you know, an R isn't like super meaningful compared to an A. So we can kind of encode in a

4595
05:34:43,560 --> 05:34:47,400
simple format, which is what we're going to do. So essentially, we need to figure out how many

4596
05:34:47,400 --> 05:34:52,680
unique characters are in our vocabulary. So to do that, we're going to say vocab equals sorted,

4597
05:34:52,680 --> 05:34:57,800
set text, this will sort all of the unique characters in the text. And then what we're

4598
05:34:57,800 --> 05:35:02,840
going to do is create a mapping from unique characters to indices indices. So essentially,

4599
05:35:02,840 --> 05:35:08,360
we're going to say UI, for IU in a new, a numerator vocabulary, what this will do is give us

4600
05:35:09,240 --> 05:35:14,680
essentially zero, whatever the string is, one, whatever the string is, two, whatever the string

4601
05:35:14,680 --> 05:35:19,480
is for every single letter or character in our vocabulary, which will allow us to create this

4602
05:35:19,480 --> 05:35:26,680
mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array,

4603
05:35:26,760 --> 05:35:31,960
so that we can just use the index at which a letter appears as the reverse mapping. So going

4604
05:35:31,960 --> 05:35:36,360
from index to letter, rather than lettered index, which is what this one's doing here.

4605
05:35:37,000 --> 05:35:42,680
Next, I've just written a function that takes some text and converts that to an int or the int

4606
05:35:42,680 --> 05:35:47,400
representation for it, just to make a little bit easier for us as we get later on in the tutorial.

4607
05:35:47,400 --> 05:35:52,360
So we're just going to say NP dot array of in this case, and we're just going to convert every single

4608
05:35:52,440 --> 05:35:58,600
character in our text into its integer representation by just referencing that character and putting

4609
05:35:58,600 --> 05:36:03,080
that in a list here, and then obviously converting that to NumPy array. So then if we wanted to have

4610
05:36:03,080 --> 05:36:09,480
a look at how this works, we can say text as int equals text to int text. So remember text is

4611
05:36:09,480 --> 05:36:14,520
that entire loaded file that we had above here. So we're just going to convert that to its integer

4612
05:36:14,520 --> 05:36:20,200
representation entirely using this function. And now we can look at how this works down here. So

4613
05:36:20,200 --> 05:36:28,520
we can see that the text for citizen, which is the first 13 letters is encoded by 1847 5657 581.

4614
05:36:28,520 --> 05:36:32,600
And obviously each character has its own encoding, and you can go through and kind of figure out what

4615
05:36:32,600 --> 05:36:38,040
they are based on the ones that are repeated, right? So that is how that works. Now I figured

4616
05:36:38,040 --> 05:36:41,400
while we were at it, we might as well write a function that goes the other way. So into text.

4617
05:36:42,360 --> 05:36:47,480
Reason I'm trying to convert this to a NumPy array first is just because we're going to be passing

4618
05:36:47,480 --> 05:36:52,120
in different objects potentially in here. So if it's not already a NumPy array, it needs to be

4619
05:36:52,120 --> 05:36:57,720
a NumPy array, which is kind of what this is doing. Otherwise, we're just going to pass on that we

4620
05:36:57,720 --> 05:37:02,840
don't need to convert it to NumPy array, if it already is one, we can just join all of the characters

4621
05:37:02,840 --> 05:37:09,000
from this list into here. So that's essentially what this is doing for us. It's just joining

4622
05:37:09,000 --> 05:37:15,800
into text. And then we can see if we go into text, text is in colon 13, that translates that back to

4623
05:37:15,800 --> 05:37:19,640
us for citizen, I mean, you can look more into this function if you want, but it's not that

4624
05:37:19,640 --> 05:37:25,000
complicated. Okay, so now that we have all this text encoded as integers, what we need to do is

4625
05:37:25,000 --> 05:37:30,600
create some training examples. It's not really feasible to just pass the entire you know, 1.1

4626
05:37:30,600 --> 05:37:35,640
million characters to our model at once for training, we need to split that up into something

4627
05:37:35,640 --> 05:37:40,360
that's meaningful. So what we're actually going to be doing is creating training examples where we

4628
05:37:40,360 --> 05:37:47,640
have the first, where the training input, right, so the input value is going to be some sequence

4629
05:37:47,640 --> 05:37:52,520
of some length, we'll pick the sequence length, in this case, we're actually going to pick 100.

4630
05:37:52,520 --> 05:37:57,800
And then the output or the expected output, so I guess like the label for that training example

4631
05:37:57,800 --> 05:38:03,480
is going to be the exact same sequence shifted right by one character. So essentially, I put a

4632
05:38:03,480 --> 05:38:08,600
good example here, our input will be something like hell, right? Now our output will be E L L O.

4633
05:38:08,600 --> 05:38:13,240
So what it's going to do is predict this last character, essentially. And these are what our

4634
05:38:13,240 --> 05:38:18,120
training examples are going to look like. So the entire beginning sequence, and then the output

4635
05:38:18,120 --> 05:38:23,640
sequence should be that beginning sequence minus the first letter, but tack on what the last letter

4636
05:38:23,640 --> 05:38:28,760
should be. So that this way, we can look at some input sequence and then predict that output sequence

4637
05:38:28,760 --> 05:38:34,120
that you know, plus a character, right? Okay, so that's how that works. So now we're going to do is

4638
05:38:34,120 --> 05:38:38,840
define a sequence length of 100. We're going to say the amount of examples per epoch is going to be

4639
05:38:38,840 --> 05:38:43,800
the length of the text divided by the sequence length plus one. The reason we're doing this is

4640
05:38:43,800 --> 05:38:49,480
because for every training example, we need to create a sequence input that's 100 characters long,

4641
05:38:49,480 --> 05:38:54,280
and we need to create a sequence output that's 100 characters long, which means that we need to have

4642
05:38:54,280 --> 05:39:00,760
101 characters that we use for every training example, right? Hopefully that would make sense.

4643
05:39:00,840 --> 05:39:08,440
So what this next line here is going to do is convert our entire string data set into characters.

4644
05:39:08,440 --> 05:39:12,680
And it's actually going to allow us to have a stream of characters, which means that it's

4645
05:39:12,680 --> 05:39:19,160
going to essentially contain, you know, 1.1 million characters inside of this TF dot data set

4646
05:39:19,880 --> 05:39:24,760
object from tensor slices. That's what that's doing. Next, so let's run this and make sure this

4647
05:39:24,760 --> 05:39:30,920
works. All right, what we're going to do is say sequences is equal to char data set dot batch

4648
05:39:30,920 --> 05:39:36,680
sequence length is the length of each batch. So in this case, 101 and then drop remainder means

4649
05:39:36,680 --> 05:39:43,720
let's say that we have, you know, 105 characters in our text, well, since we need sequences of

4650
05:39:43,720 --> 05:39:49,240
length 101, we'll just drop the last four characters of our text, because we can't even put those into

4651
05:39:49,240 --> 05:39:54,040
a batch. So that's what this is doing for us is going to take our entire character data set here

4652
05:39:54,120 --> 05:39:59,160
that we've created and batch it into length of 101, and then just drop the remainder. So that's

4653
05:39:59,160 --> 05:40:06,040
what we're going to do here. So sequences does now split input target. What this is going to do

4654
05:40:06,040 --> 05:40:11,400
essentially is just create those training examples that we needed. So taking this, these sequences

4655
05:40:11,400 --> 05:40:16,920
of 101 length and converting them into the input and target text, and I'll show you how they work

4656
05:40:16,920 --> 05:40:23,480
in a second, we can do this convert the sequences to that by just mapping them to this function.

4657
05:40:23,480 --> 05:40:28,280
So that's what this function does. So if we say sequences dot map, and we put this function here,

4658
05:40:28,280 --> 05:40:33,640
that means every single sequence will have this operation applied to it. And that will be stored

4659
05:40:33,640 --> 05:40:38,840
inside this data set object. Or I guess you'd say object, but we'll also just say that's it's

4660
05:40:38,840 --> 05:40:43,400
going to be, you know, the variable, right? So if we want to look at an example of how this works,

4661
05:40:44,120 --> 05:40:48,840
we can kind of see. So it just says example, the input will be first citizen, before we proceed

4662
05:40:48,840 --> 05:40:54,040
any further here, me speak, all speak, speak, first citizen, you and the output notice the first

4663
05:40:54,040 --> 05:41:00,120
character is gone, starts at I. And the last character is actually just a space here. Whereas

4664
05:41:00,120 --> 05:41:03,960
here, it didn't have a space, or you can see there's no space. Here, there is a space. That's

4665
05:41:03,960 --> 05:41:09,000
kind of what I'm trying to highlight for you. The next example, we get our all resolved rather

4666
05:41:09,000 --> 05:41:12,520
to die rather than famine, whatever it goes to here, right? And then you can see here, we omit

4667
05:41:12,600 --> 05:41:18,600
that a and the next letter is actually a K, right? That's added in there. So that's how that

4668
05:41:18,600 --> 05:41:23,800
works. Okay, so next, we need to make training batches. So we're going to say the batch size

4669
05:41:23,800 --> 05:41:29,480
equals 64. The vocabulary size is the length of the vocabulary, which if you remember all the way

4670
05:41:29,480 --> 05:41:35,160
back up to the top of the code, was the set or the sorted set of the text, which essentially

4671
05:41:35,160 --> 05:41:40,680
told us how many unique characters are in there. The embedding dimension is 256. The RNN units

4672
05:41:40,760 --> 05:41:46,680
is 1024. And the buffered size is 10,000. What we're going to do now is create a data set that

4673
05:41:46,680 --> 05:41:51,480
shuffled, we're going to switch around all these sequences, so they don't get shown in the proper

4674
05:41:51,480 --> 05:41:56,440
order, which we actually don't want. And then we're going to batch them by the batch size. So

4675
05:41:56,440 --> 05:42:00,520
if we haven't kind of gone over what batching and all this does before, I mean, you can read

4676
05:42:00,520 --> 05:42:04,360
these comments, this is straight from the TensorFlow documentation, what we want to do is

4677
05:42:04,360 --> 05:42:09,960
feed our model 64 batches of data at a time. So what we're going to do is shuffle all of the data,

4678
05:42:10,520 --> 05:42:14,440
batch it into that size, and then again, drop the remainder, if there's not enough batches,

4679
05:42:14,440 --> 05:42:18,920
which is what we'll do. We're going to define the embedding dimension, which is essentially

4680
05:42:19,480 --> 05:42:24,520
how big we want every single vector to represent our words are in the embedding layer. And then

4681
05:42:24,520 --> 05:42:29,960
the RNN units, I won't really discuss what that is right now. But that's essentially how many

4682
05:42:31,480 --> 05:42:35,000
it's hard to really just, I'm just going to omit describing at for right now, because I don't want

4683
05:42:35,000 --> 05:42:41,320
to butcher an explanation. It's not that important. Anyways, okay, so now we're going to go down to

4684
05:42:41,320 --> 05:42:45,560
building the model. So we've kind of set these parameters up here. Remember what those are,

4685
05:42:45,560 --> 05:42:49,640
we've batched and we've shuffled the data set. And again, that's how this works. You can print

4686
05:42:49,640 --> 05:42:54,680
it out if you want to see what a batch actually looks like. But essentially, it's just 64 entries

4687
05:42:54,680 --> 05:42:59,560
of those sequences, right? So 64 different training examples is what a batch that is.

4688
05:43:00,360 --> 05:43:05,880
All right. So now we go down here, we're going to say build model, we're actually making a function

4689
05:43:05,880 --> 05:43:10,520
that's going to return to us a built model. The reason for this is because

4690
05:43:11,480 --> 05:43:16,840
right now, we're going to pass the model batches of size 64 for training, right? But what we're

4691
05:43:16,840 --> 05:43:22,280
going to do later is save this model. And then we're going to patch pass it batches of one pieces

4692
05:43:22,280 --> 05:43:27,400
of, you know, training whatever data, so that it can actually make a prediction on just one

4693
05:43:28,120 --> 05:43:32,440
piece of data. Because for right now, what it's going to do is takes a batch size of 64, it's

4694
05:43:32,440 --> 05:43:37,800
going to take 64 training examples, and return to us 64 outputs. That's what this model is going

4695
05:43:37,800 --> 05:43:43,720
to be built to do the way we build it now to start. But later on, we're going to rebuild the model

4696
05:43:43,720 --> 05:43:48,120
using the same parameters that we've saved and trained for the model, but change it to just be

4697
05:43:48,120 --> 05:43:53,640
a batch size of one, so that that way we can get one prediction for one input sequence, right?

4698
05:43:54,280 --> 05:43:58,600
So that's why I'm creating this build model function. Now in here, it's going to have the

4699
05:43:58,600 --> 05:44:04,440
vocabulary sizes, first argument, the embedding dimension, which remember was 256 as a second

4700
05:44:04,440 --> 05:44:09,640
argument, but also these are the parameters up here, right? And then we're going to find the batch

4701
05:44:09,640 --> 05:44:15,880
size as you know, batch size, none, what this none means is we don't know how long the sequences

4702
05:44:15,880 --> 05:44:21,880
are going to be in each batch. All we know is that we're going to have 64 entries in each batch.

4703
05:44:21,880 --> 05:44:27,400
And then of those 64 entries, so training examples, right, we don't know how long each one

4704
05:44:27,400 --> 05:44:31,320
will be. Although in our case, we're going to use ones that are length 100. But when we actually

4705
05:44:31,320 --> 05:44:35,720
use the model to make predictions, we don't know how long the sequence is going to be that we input

4706
05:44:35,720 --> 05:44:41,000
so we leave this none. Next, we'll make an LSTM layer, which is a long short term memory RNN

4707
05:44:41,000 --> 05:44:45,800
units, which is 1024, which again, I don't really want to explain, but you can look up if you want

4708
05:44:45,880 --> 05:44:54,760
return sequences means return the intermediate stage at every step. The reason we're doing this

4709
05:44:55,800 --> 05:45:01,560
is because we want to look at what the model seeing at the intermediate steps and not just

4710
05:45:01,560 --> 05:45:06,680
the final stage. So if you leave this as false, and you don't set this to true, what happens is

4711
05:45:06,680 --> 05:45:13,480
this LSTM just returns one output that tells us what the model kind of found at the very last

4712
05:45:13,480 --> 05:45:18,600
time step. But we actually want the output at every single time step for this specific model.

4713
05:45:18,600 --> 05:45:23,880
And that's why we're setting this true, stateful, not going to talk about that one right now,

4714
05:45:23,880 --> 05:45:27,240
that's something you can look up if you want. And then recurrent initializer is just what

4715
05:45:27,240 --> 05:45:31,960
these values are going to start at in the LSTM. We're just picking this because this is what

4716
05:45:31,960 --> 05:45:37,400
TensorFlow has kind of said is a good default to pick. I won't go into more depth about that

4717
05:45:37,400 --> 05:45:42,680
again, things that you can look up more if you want. Finally, we have a dense layer, which is

4718
05:45:42,680 --> 05:45:49,000
going to contain the amount of vocabulary size nodes. The reason we're doing this is because we

4719
05:45:49,000 --> 05:45:54,200
want the final layer to have the amount of nodes in it equal to the amount of characters in the

4720
05:45:54,200 --> 05:45:59,800
vocabulary. This way, every single one of those nodes can represent a probability distribution

4721
05:45:59,800 --> 05:46:06,200
that that character comes next. So all of those nodes value some sum together should give us the

4722
05:46:06,200 --> 05:46:11,960
value of one. And that's going to allow us to look at that last layer as a predictive layer,

4723
05:46:11,960 --> 05:46:16,120
where it's telling us the probability that these characters come next, and we've discussed how

4724
05:46:16,120 --> 05:46:22,360
that's worked previously with other neural networks. So let's run this now. Name embedding

4725
05:46:22,360 --> 05:46:27,800
dim is not defined, which I mean, believes I have not ran this yet. So now we run that,

4726
05:46:27,800 --> 05:46:32,120
and we should be good. So if we look at the model summary, we can see we have our initial

4727
05:46:32,120 --> 05:46:37,400
embedding layer, we have our LSTM, and then we have our dense layer at the end. Now notice

4728
05:46:37,400 --> 05:46:43,160
64 is the batch size, right? That's the initial shape. None is the length of the sequence,

4729
05:46:43,160 --> 05:46:49,800
which we don't know. And then this is going to be just the output dimension or sorry, this is

4730
05:46:50,440 --> 05:46:56,120
the amount of values in the vector, right? So we're going to start with 256. We'll just do

4731
05:46:56,120 --> 05:47:01,160
1,024 units in the LSTM and then 65 stands for the amount of nodes, because that is the

4732
05:47:01,160 --> 05:47:06,440
length of the vocabulary. Alright, so combined, that's how many trainable parameters we get.

4733
05:47:07,080 --> 05:47:11,400
You can see each of them for each layer. And now it's time to move on to the next section.

4734
05:47:11,400 --> 05:47:15,320
Okay, so now we're moving on to the next step of the tutorial, which is creating a loss function

4735
05:47:15,320 --> 05:47:19,560
to compile our model with. Now, I'll talk about why we need to do this in a second,

4736
05:47:19,560 --> 05:47:25,560
but I first want to explore the output shape of our model. So remember, the input to our model

4737
05:47:25,560 --> 05:47:31,480
is something that is of length 64, because we're going to have batches of 64 training examples,

4738
05:47:31,480 --> 05:47:37,240
right? So every time we feed our model, we're going to give it 64 training examples. Now what

4739
05:47:37,240 --> 05:47:42,920
those training examples are, are sequences of length 100. That's what I want you to remember.

4740
05:47:42,920 --> 05:47:50,200
We're passing 64 entries that are all of length 100 into the model as its training data, right?

4741
05:47:50,840 --> 05:47:55,400
But sometimes, and when we make predictions with the model later on, we'll be passing it

4742
05:47:55,400 --> 05:48:01,400
just one entry that is of some variable length, right? And that's why we've created

4743
05:48:02,120 --> 05:48:07,400
this build model function, so that we can build this model using the parameters that we've saved

4744
05:48:07,400 --> 05:48:14,520
later on, once we train the model, and it can expect a different input shape, right? Because

4745
05:48:14,520 --> 05:48:17,560
when we're training it, it's going to be given a different shape, and we're actually testing with

4746
05:48:17,560 --> 05:48:23,800
it. Now what I want to do is explore the output of this model, though, at the current point in time.

4747
05:48:23,800 --> 05:48:30,600
So we've created a model that accepts a batch of 64 training examples that are length 100. So

4748
05:48:30,600 --> 05:48:35,640
let's just look at what the output is from the final layer. Give this a second to run.

4749
05:48:36,440 --> 05:48:44,360
We get 64, 165. And that represents the batch size, the sequence length, and the vocabulary

4750
05:48:44,360 --> 05:48:49,320
size. Now the reason for this is we have to remember that when we create a dense layer as our last

4751
05:48:49,320 --> 05:48:56,600
layer that has 65 nodes, every prediction is going to contain 65 numbers. And that's going

4752
05:48:56,600 --> 05:49:02,440
to be the probability of every one of those characters occurring, right? That's what that

4753
05:49:02,440 --> 05:49:07,480
does at the last one for us. So obviously, our last dimension is going to be 65 for the vocabulary

4754
05:49:07,480 --> 05:49:11,480
size. This is a sequence length, and that's a batch, I just want to make sure this is really

4755
05:49:11,480 --> 05:49:14,840
clear before we keep going. Otherwise, it's going to get very confusing very quickly.

4756
05:49:15,480 --> 05:49:21,160
So what I want to do now is actually look at the length of the example batch predictions,

4757
05:49:21,160 --> 05:49:25,000
and just print them out and look at what they actually are. So example batch predictions

4758
05:49:25,000 --> 05:49:31,560
is what happens when I use my model on some random input example, actually, well, the first one

4759
05:49:31,560 --> 05:49:36,840
from my data set with when it's not trained. So I can actually use my model before it's

4760
05:49:36,840 --> 05:49:43,000
trained with random weights and random biases and parameters, by simply using model, and then I can

4761
05:49:43,000 --> 05:49:47,480
put the little brackets like this and just pass in some example that I want to get a prediction

4762
05:49:47,480 --> 05:49:51,480
for. So that's what I'm going to do, I'm going to give it the first batch, and it can even it shows

4763
05:49:51,480 --> 05:49:56,200
me the shape of this batch 64 100, I'm going to pass that to the model, and it's going to give us a

4764
05:49:56,200 --> 05:50:01,960
prediction for that. And in fact, it's actually going to give us a prediction for every single

4765
05:50:01,960 --> 05:50:06,040
element in the batch, right, every single training example in the batch, it's going to give us a

4766
05:50:06,040 --> 05:50:11,640
prediction for. So let's look at what those predictions are. So this is what we get, we get

4767
05:50:11,720 --> 05:50:19,240
a length 64 tensor, right? And then inside of here, we get a list inside of a list or an array

4768
05:50:19,240 --> 05:50:24,840
inside of an array with all of these different predictions. So we'll stop there for this, like

4769
05:50:24,840 --> 05:50:29,000
explaining this aspect here. But you can see we're getting 64 different predictions because

4770
05:50:29,000 --> 05:50:35,080
there's 64 elements in the batch. Now, let's look at one prediction. So let's look at the very first

4771
05:50:35,080 --> 05:50:41,560
prediction for say the first element in the batch, right? So let's do that here. And we see now that

4772
05:50:41,560 --> 05:50:48,600
we get a length 100 tensor. And that this is what it looks like, there's still another layer inside.

4773
05:50:48,600 --> 05:50:53,240
And in fact, we can see that there's another nested layer here, right, another nested array

4774
05:50:53,240 --> 05:51:00,360
inside of this array. So the reason for this is because at every single time step, which means

4775
05:51:00,360 --> 05:51:03,640
the length of the sequence, right, because remember, our recurrent neural network is going to feed

4776
05:51:03,640 --> 05:51:08,840
one at a time every word in the sequence. In this case, our sequences are like the 100 at every

4777
05:51:08,840 --> 05:51:15,560
time step, we're actually saving that output as a, as a prediction, right? And we're passing

4778
05:51:15,560 --> 05:51:21,320
that back. So we can see that for one batch, one training, sorry, not one batch, one training

4779
05:51:21,320 --> 05:51:26,120
example, we get 100 outputs. And these outputs are in some shape, we'll talk about what those are

4780
05:51:26,120 --> 05:51:31,080
in a second. So that's something to remember that for every single training example, we get

4781
05:51:31,080 --> 05:51:35,720
whatever the length of that training example was outputs, because that's the way that this

4782
05:51:35,720 --> 05:51:41,560
model works. And then finally, we look at the prediction at just the very first time step. So

4783
05:51:41,560 --> 05:51:46,600
this is 100 different time steps. So let's look at the first time step and see what that prediction

4784
05:51:46,600 --> 05:51:53,240
is. And we can see that now we get a tensor of length 65. And this is telling us the probability

4785
05:51:53,240 --> 05:51:59,080
of every single character occurring next at the first time step. So that's what I wanted to walk

4786
05:51:59,080 --> 05:52:03,960
through is showing you what's actually outputted from the model, the current way that it works.

4787
05:52:04,040 --> 05:52:10,920
And that's why we need to actually make our own loss function to be able to determine how, you

4788
05:52:10,920 --> 05:52:15,640
know, good our models performing, when it outputs something ridiculous that looks like this, because

4789
05:52:15,640 --> 05:52:20,760
there is no just built in loss function and tensor flow that can look at a three dimensional

4790
05:52:20,760 --> 05:52:26,440
nested array of probabilities over, you know, the vocabulary size, and tell us how different the

4791
05:52:26,440 --> 05:52:31,720
two things are. So we need to make our own loss function. So if we want to determine the predicted

4792
05:52:31,720 --> 05:52:39,560
character from this array, so we'll go there now. What we can do is get the categorical, what's

4793
05:52:39,560 --> 05:52:45,800
this called, we can sample the categorical distribution. And that will tell us the predicted

4794
05:52:45,800 --> 05:52:50,280
character. So what I mean is, let's just look at this. And then we'll explain this. So since our

4795
05:52:50,280 --> 05:52:54,520
model works on random weights and biases right now, we haven't trained yet. This is actually

4796
05:52:54,520 --> 05:52:59,720
all of the predicted characters that it had. So at every time step, at the first time step,

4797
05:52:59,800 --> 05:53:05,320
it predicted h, then it predicted hyphen, then h, then g, then you, and so on so forth, you get

4798
05:53:05,320 --> 05:53:12,760
the point, right? So what we're doing to get this value is we're going to sample the prediction. So

4799
05:53:12,760 --> 05:53:17,960
at this, this is just the first time step, actually, we're sampled the prediction. Actually, no,

4800
05:53:17,960 --> 05:53:22,680
sorry, we're sampling every time stamp, my bad there. We're going to say sampled indices equals

4801
05:53:22,680 --> 05:53:27,000
NP dot reshapes, we're just reshaping this just changing the shape of it. We're going to say

4802
05:53:27,080 --> 05:53:34,200
predicted characters equals int to text sampled indices. So it's, I really, it's hard to explain

4803
05:53:34,200 --> 05:53:38,280
all this if you guys don't have a statistics kind of background a little bit to talk about why we're

4804
05:53:38,280 --> 05:53:44,120
sampling and not just taking the argument max value of like this array, because you would think

4805
05:53:44,120 --> 05:53:47,560
that what we'll do is just take the one that has the highest probability out of here, and that will

4806
05:53:47,560 --> 05:53:52,600
be the index of the next predicted character. There's some issues with doing that for the loss

4807
05:53:52,680 --> 05:53:57,960
function. Just because if we do that, then what that means is we're going to kind of get stuck in

4808
05:53:59,000 --> 05:54:03,080
an infinite loop almost where we just keep accepting the biggest character. So what we'll

4809
05:54:03,080 --> 05:54:10,040
do is pick a character based on this probability distribution, kind of, yeah, again, it's hard.

4810
05:54:10,040 --> 05:54:13,960
It's called sampling the distribution. You can look that up if you don't know what that means,

4811
05:54:13,960 --> 05:54:18,680
but sampling is just like trying to pick a character based on a probability distribution.

4812
05:54:18,680 --> 05:54:23,080
It doesn't guarantee that the character with the highest probability is going to be picked. It

4813
05:54:23,080 --> 05:54:28,520
just uses those probabilities to pick it. I hope that makes sense. I know that was like a really

4814
05:54:28,520 --> 05:54:33,880
rambly definition, but that's the best I can do. So here, we reshape the array and convert all the

4815
05:54:33,880 --> 05:54:37,880
integers to numbers to see the actual characters. So that's what these two lines are doing here.

4816
05:54:37,880 --> 05:54:42,280
And then I'm just showing the predicted characters by showing you this. And you know,

4817
05:54:42,280 --> 05:54:47,320
the character here is what was predicted at time step zero to be the next character and so on.

4818
05:54:48,120 --> 05:54:53,320
Okay, so now we can create a loss function that actually handles this for us. So this is the

4819
05:54:53,320 --> 05:54:59,160
loss function that we have. Keras has like a built in one that we can utilize, which is what

4820
05:54:59,160 --> 05:55:04,040
we're doing. But what this is going to do is take all of the labels and all of the probability

4821
05:55:04,040 --> 05:55:08,680
distributions, which is what this is, logits, I'm not going to talk about that really. And we'll

4822
05:55:08,680 --> 05:55:14,280
compute a loss on those. So how different or how similar those two things are. Remember the goal

4823
05:55:14,280 --> 05:55:18,920
of our algorithm in the neural network is to reduce the loss, right? Okay, so next,

4824
05:55:18,920 --> 05:55:22,040
we're going to compile the model, which we'll do here. So we're going to compile the model with

4825
05:55:22,040 --> 05:55:28,040
the atom optimizer and the loss function as loss, which we defined here. And now we're going to

4826
05:55:28,040 --> 05:55:31,320
set up some checkpoints. I'm not going to talk about how these work, you can kind of just read

4827
05:55:31,320 --> 05:55:38,280
through this if you want. And then we're going to train the model. Remember to start your GPU

4828
05:55:38,280 --> 05:55:44,600
hardware accelerator under runtime, change runtime type GPU, because if you do not, then this is

4829
05:55:44,600 --> 05:55:49,960
going to be very slow. But once you do that, you can train the model. I've already trained it. But

4830
05:55:49,960 --> 05:55:55,000
if we go through this training, we can see it's going to say train for 172 steps. It's going to

4831
05:55:55,000 --> 05:55:59,160
take about, you know, 30 seconds per epoch, probably maybe a little bit less than that. And

4832
05:55:59,160 --> 05:56:03,720
the more epochs you run this for, the better it will get. This is a different we're not likely

4833
05:56:03,720 --> 05:56:09,560
going to overfit here. So we can run this for like, say 100 epochs, if we wanted to. For our case,

4834
05:56:09,560 --> 05:56:14,440
let's actually start by just training this on, let's say two epochs, just to see how it does.

4835
05:56:14,440 --> 05:56:20,280
And then we'll train it on like 1020, 4050 and compare the results. But you'll notice the more

4836
05:56:20,280 --> 05:56:23,960
epochs, the better it's going to get. But just like for our case, we'll start with two, and then

4837
05:56:23,960 --> 05:56:29,400
we'll work our way up. So while that trains, we'll actually explain the next aspect of this

4838
05:56:29,480 --> 05:56:34,520
without running the code. So essentially, what we need to do, after we've trained the model,

4839
05:56:34,520 --> 05:56:41,640
we've initially the weights and biases, if we need to rebuild it using a new batch size of one.

4840
05:56:41,640 --> 05:56:47,800
So remember, the initial batch size was 64, which means that we'd have to pass it 64 inputs or

4841
05:56:47,800 --> 05:56:52,760
sequences for it to work properly. But now what I've done is I'm going to rebuild the model and

4842
05:56:52,760 --> 05:56:57,560
change it to a batch size of one, so that we can just pass it some sequence of whatever length we

4843
05:56:57,560 --> 05:57:03,080
want. And it will work. So if we run this, we've rebuilt the model with batch size one, that's

4844
05:57:03,080 --> 05:57:08,520
the only thing we've changed. And now what I can do is load the weights by saying model dot load

4845
05:57:08,520 --> 05:57:15,320
weights tf dot train dot latest checkpoint checkpoint directory, and then build the model

4846
05:57:16,680 --> 05:57:23,720
using the tensor shape one none. I know sounds strange. This is how we do this rebuild the model.

4847
05:57:23,720 --> 05:57:28,200
One none is just saying expect the input one and then none means we don't know what the next

4848
05:57:28,200 --> 05:57:35,640
dimension length will be. But here, checkpoint directory is just we've defined where on our

4849
05:57:35,640 --> 05:57:40,600
computer, we're going to save these TensorFlow checkpoints. This is just saying this is the

4850
05:57:41,400 --> 05:57:45,000
was it the prefix we're going to save the checkpoint with. So we're going to do the

4851
05:57:45,000 --> 05:57:50,040
checkpoint directory. And then checkpoint epoch where epoch will stand for obviously,

4852
05:57:50,680 --> 05:57:55,160
whatever epoch we're on. So we'll save checkpoint here, we'll save a checkpoint at epoch one,

4853
05:57:55,160 --> 05:58:00,440
a checkpoint at epoch two, to get the latest checkpoint, we do this. And then if we wanted

4854
05:58:00,440 --> 05:58:05,720
to load any intermediate checkpoint, say like checkpoint 10, which is what I've defined here,

4855
05:58:05,720 --> 05:58:10,280
we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading

4856
05:58:10,280 --> 05:58:14,680
by saying tf dot train dot load checkpoint, whereas this one just gets the most recent. So

4857
05:58:14,680 --> 05:58:18,280
we'll get the most recent, which should be checkpoint two for me. And then what we're going

4858
05:58:18,280 --> 05:58:23,640
to do is generate the text. So this function, I'll dig into it in a second, but I just want

4859
05:58:23,640 --> 05:58:27,160
to run and show you how this works, because I feel like we've done a lot of work for not

4860
05:58:27,160 --> 05:58:31,880
very many results right now. And I'm just going to type in the string Romeo, and just show you

4861
05:58:31,880 --> 05:58:36,920
that when I do this, we give it a second. And it will actually generate an output sequence

4862
05:58:36,920 --> 05:58:43,240
like this. So we have Romeo us give this is the beginning of our sequence that says lady

4863
05:58:43,240 --> 05:58:50,040
Capulet food, Marathon father, gnomes come to those shall, right? So it's like pseudo English,

4864
05:58:50,040 --> 05:58:54,360
most of it are like kind of proper words. But again, this is because we trained it on just

4865
05:58:54,360 --> 05:59:00,120
two epochs. So I'll talk about how we build this in a second. But if you wanted a better output

4866
05:59:00,120 --> 05:59:04,840
for this part, then you would train this on more epochs. So now let's talk about how I actually

4867
05:59:04,840 --> 05:59:10,360
generated that output. So we rebuilt the model to accept a batch size of one, which means that I

4868
05:59:10,360 --> 05:59:15,320
can pass it a sequence of any length. And in fact, what I start by doing is passing the

4869
05:59:15,320 --> 05:59:21,480
sequence that I've typed in here, which was Romeo, then what that does is we run this function

4870
05:59:21,480 --> 05:59:25,320
generate text, I just stole this from TensorFlow's website, like I've stolen almost all of this

4871
05:59:25,320 --> 05:59:31,800
code. And then we say the number of characters to generate is 800, the input evaluation, which

4872
05:59:31,800 --> 05:59:37,400
is now like we need to pre process this text again, so that this works properly, we could

4873
05:59:37,400 --> 05:59:42,040
use my little function, or we can just write this line of code here, which does with the function

4874
05:59:42,040 --> 05:59:47,880
that I wrote does for us. So char to IDX S for S and start string, start string is what we typed

4875
05:59:47,880 --> 05:59:53,080
in in that case, Romeo, then what we're going to do is expand the dimensions. So essentially turn

4876
05:59:53,640 --> 06:00:00,200
just a list like this that has all these numbers, nine, eight, seven into a double list like this,

4877
06:00:00,200 --> 06:00:04,520
or just a nested list, because that's what it's expecting as the input one batch, one entry.

4878
06:00:05,160 --> 06:00:09,240
Then what we do is we're going to say the string that we want to store, because we want to print

4879
06:00:09,240 --> 06:00:15,320
this out at the end, right, we'll put in this text generated list, temperature equals 1.0.

4880
06:00:15,320 --> 06:00:20,040
What this will allow us to do is if we change this value to be higher, I mean, you can read

4881
06:00:20,040 --> 06:00:24,120
the comment here, right, low temperature results in more predictable text, higher temperature results

4882
06:00:24,120 --> 06:00:27,720
in more surprising text. So this is just a parameter to mess with if you want, you don't

4883
06:00:27,720 --> 06:00:32,760
necessarily need it. And I would like, I've just left mine at one for now, we're going to start

4884
06:00:32,760 --> 06:00:37,800
by resetting the states of the model. This is because when we rebuild the model, it's going to

4885
06:00:37,800 --> 06:00:43,400
have stored the last state that it remembered when it was training. So we need to clear that

4886
06:00:43,400 --> 06:00:48,520
before we pass new input text to it. And we say for I and range num generate, which means however

4887
06:00:48,520 --> 06:00:53,080
many characters we want to generate, which is 800 here, what we're going to do is say predictions

4888
06:00:53,080 --> 06:00:58,680
equals model input a vowel, that's going to start as the start string that's encoded, right.

4889
06:00:59,480 --> 06:01:04,440
And then what we're going to do is say predictions equals TF dot squeeze prediction zero. What this

4890
06:01:04,440 --> 06:01:09,320
does is take our predictions, which is going to be in a nested list, and just removes that

4891
06:01:09,320 --> 06:01:14,040
exterior dimension. So we just have the predictions that we want, we don't have that extra dimension

4892
06:01:14,040 --> 06:01:18,040
that we need to index again. And then we're going to say using a categorical distribution to predict

4893
06:01:18,040 --> 06:01:22,440
the character returned by the model, that's what it writes here. We'll divide by the temperature,

4894
06:01:22,440 --> 06:01:28,200
if it's one, that's not going to do anything. We'll say predicted ID equals we'll sample

4895
06:01:28,200 --> 06:01:33,560
whatever the output was from the model, which is what this is doing. And then we're going to

4896
06:01:33,560 --> 06:01:40,040
take that output. So the predicted ID, and we are going to add that to the input evaluation.

4897
06:01:40,840 --> 06:01:45,080
And then what we're going to say is text generated dot append, and we're going to convert the text

4898
06:01:45,080 --> 06:01:52,360
that are integers now, back into a string, and return all of this. Now I know this seems like

4899
06:01:52,360 --> 06:01:57,160
a lot. Again, this is just given to us by TensorFlow to, you know, create this aspect,

4900
06:01:57,160 --> 06:02:00,760
you can read through the comments yourself, if you want to understand it more. But I think that

4901
06:02:00,760 --> 06:02:05,640
was a decent explanation of what this is doing. So yeah, that is how we can generate, you know,

4902
06:02:05,640 --> 06:02:11,960
sequences using recurrent neural network. Now what I'm going to do is go to my other window

4903
06:02:11,960 --> 06:02:16,120
here where I've actually typed all of the code, just in full and do a quick summary of everything

4904
06:02:16,120 --> 06:02:19,480
that we've done, just because there was a lot that went on. And then from there, I'm actually

4905
06:02:19,480 --> 06:02:23,960
going to train this on a B movie script and show you kind of how that works in comparison to the

4906
06:02:23,960 --> 06:02:29,160
Romeo and Juliet. Okay, so what I'm in now is just the exact same notebook we have before,

4907
06:02:29,160 --> 06:02:34,040
but I've just pretty much copied all the text in here. Or it's the exact same code we had before.

4908
06:02:34,040 --> 06:02:37,560
So we just don't have all that other text in between. So I can kind of do a short summary

4909
06:02:37,560 --> 06:02:42,920
of what we did, as well as show you how this worked when I trained it on the B movie script.

4910
06:02:42,920 --> 06:02:46,520
So I did mention I was going to show you that I'm not lying, I will show you can see I've

4911
06:02:46,520 --> 06:02:52,280
got B movie dot txt loaded in here. And in fact, actually, I'm going to show you this script first

4912
06:02:52,280 --> 06:02:56,920
to show you what it looks like. So this is what the B movie script looks like. You can see it

4913
06:02:56,920 --> 06:03:02,200
just like a long, you know, script of text, I just downloaded this for free off the internet.

4914
06:03:02,200 --> 06:03:06,280
And it's actually not as long as the Romeo and Juliet play. So we're not going to get as good

4915
06:03:06,280 --> 06:03:11,160
of results from our model. But it should hopefully be okay. So we just start and I'm just going to

4916
06:03:11,160 --> 06:03:14,280
do a brief summary. And then I'll show you the results from the B movie script, just so that

4917
06:03:14,280 --> 06:03:18,120
people that are confused, maybe have something that wraps it up here. We're doing our imports.

4918
06:03:18,120 --> 06:03:22,520
I don't think I need to explain that this part up here is just loading in your file. Again,

4919
06:03:22,520 --> 06:03:27,240
I don't think I need to explain that. Then we're actually going to read the file. So open it from

4920
06:03:27,240 --> 06:03:34,680
our directory, decode it into utf eight, we're going to create a vocabulary and encode all of

4921
06:03:34,680 --> 06:03:39,320
the text that's inside of this file. Then what we're going to do is turn all of that text up

4922
06:03:39,320 --> 06:03:43,480
into you know, the encoded version, we're writing a function here that goes the other way around.

4923
06:03:43,480 --> 06:03:47,960
So from int to text, not from text to int, we're going to define the sequence length that we

4924
06:03:47,960 --> 06:03:52,680
want to train with, which will be sequence length of 100. You can decrease this value if you want,

4925
06:03:52,680 --> 06:03:57,240
you go 50, go 20, it doesn't really matter. It's up to you. It just that's going to determine

4926
06:03:57,240 --> 06:04:01,400
how many training examples you're going to have right is the sequence length. Next, what we're

4927
06:04:01,400 --> 06:04:06,680
going to do is create a character data set from tensor slices from text as int. What this is going

4928
06:04:06,680 --> 06:04:13,160
to do is just convert our entire text that's now an integer array into a bunch of slices of

4929
06:04:13,160 --> 06:04:17,240
characters. And so that's what this is doing here. So or not slices, what am I saying,

4930
06:04:17,960 --> 06:04:23,240
you're just going to convert, like, split that entire array into just characters, like that's

4931
06:04:23,240 --> 06:04:27,480
pretty much what it's doing. And then what we're going to say sequences equals char data set dot

4932
06:04:27,480 --> 06:04:32,200
batch, which now is going to take all those characters and batch them in lengths of 101.

4933
06:04:32,200 --> 06:04:37,560
What we're going to do then is split all of that into the training examples. So like this, right,

4934
06:04:37,560 --> 06:04:44,120
he ll and then yellow, we're going to map this function to sequences, which means we're going

4935
06:04:44,120 --> 06:04:49,800
to apply this to every single sequence and store that in data set. Then we're going to find the

4936
06:04:49,800 --> 06:04:54,760
parameters for our initial network. We're going to shuffle the data set and batch that into now

4937
06:04:54,760 --> 06:04:59,960
64 training examples. Then we're going to make the function that builds the model, which I've

4938
06:04:59,960 --> 06:05:05,560
already discussed, we're going to actually build the model starting with a batch size of 64. We're

4939
06:05:05,560 --> 06:05:12,280
going to create our loss function, compile the model, set our checkpoints for saving, and then

4940
06:05:12,280 --> 06:05:18,120
train the model and make sure that we say checkpoint callback as the checkpoint callback

4941
06:05:18,120 --> 06:05:22,280
for the model, which means it's going to save every epoch, the weights that the model had

4942
06:05:22,280 --> 06:05:27,880
computed at that epoch. So after we do that, then our models train so we've trained the model,

4943
06:05:27,880 --> 06:05:32,360
you can see I train this on 50 epochs for the B movie script. And then what we're going to do is

4944
06:05:32,360 --> 06:05:39,000
build the model now with a batch size of one. So we can pass one example to it and get a prediction,

4945
06:05:39,000 --> 06:05:43,000
we're going to load the most recent weights into our model from the checkpoint directory

4946
06:05:43,000 --> 06:05:47,320
that we defined above. And then what we're going to do is build the model and tell it to

4947
06:05:47,320 --> 06:05:53,960
expect the shape one, none as its initial input. Now none just means we don't know what that value

4948
06:05:53,960 --> 06:05:57,640
is going to be, but we know we're going to have one entry. Alright, so now we have this generate

4949
06:05:57,720 --> 06:06:02,280
text method, or function here, which I've already kind of went through how that works. And then

4950
06:06:02,920 --> 06:06:07,640
we can see, if I type in input string, so we type, you know, input string, let's say,

4951
06:06:08,680 --> 06:06:15,480
of hello, and hit enter, we'll watch and we can see that the B movie, you know, trained model

4952
06:06:15,480 --> 06:06:20,600
comes up with its output here. Now, unfortunately, the B movie script does not work as well as Romeo

4953
06:06:20,600 --> 06:06:25,800
and Juliet. That's just because Romeo and Juliet is a much longer piece of text. It's much better

4954
06:06:26,440 --> 06:06:31,560
it's format a lot nicer and a lot more predictable. But yeah, you kind of get the idea here and it's

4955
06:06:31,560 --> 06:06:35,960
kind of cool to see how this performs on different data. So I would highly recommend that you guys

4956
06:06:35,960 --> 06:06:40,920
find some training data that you could give this other than just the Romeo and Juliet or maybe

4957
06:06:40,920 --> 06:06:45,640
even try another play or something and see what you can get out of it. Also, quick side note,

4958
06:06:45,640 --> 06:06:50,040
to make your model better, increase the amount of epochs here. Ideally, you want this loss to

4959
06:06:50,040 --> 06:06:55,240
be as low as possible, you can see mine was still actually moving down at epoch 50. You will

4960
06:06:55,240 --> 06:06:59,720
reach a point where the amount of epochs won't make a difference. Although, with models like this,

4961
06:06:59,720 --> 06:07:04,520
the more epochs typically the better, because it's difficult for it to kind of overfit, because all

4962
06:07:04,520 --> 06:07:10,680
you want it to do really is just kind of learn how the language works and then be able to replicate

4963
06:07:10,680 --> 06:07:15,560
that to you almost, right? So that's kind of the idea here. And with that being said, I'm going to

4964
06:07:15,560 --> 06:07:20,600
say that this section is probably done. Now, I know this was a long, probably confusing section

4965
06:07:20,600 --> 06:07:24,680
for a lot of you. But this is, you know, what happens when you start getting into some more

4966
06:07:24,680 --> 06:07:28,680
complex things in machine learning, it's very difficult to kind of grasp and understand all

4967
06:07:28,680 --> 06:07:33,320
these concepts in an hour of me just explaining them. What I try to do in these videos is introduce

4968
06:07:33,320 --> 06:07:37,720
you to the syntax show you how to get a working, you know, kind of prototype and hopefully give

4969
06:07:37,720 --> 06:07:41,560
you enough knowledge to the fact where if you're confused by something that I said, you can go

4970
06:07:41,560 --> 06:07:46,200
and you can look that up and you can figure out kind of the more important details for yourself,

4971
06:07:46,200 --> 06:07:50,520
because I really just I can't go into all, you know, the extremes in these videos. So anyways,

4972
06:07:50,520 --> 06:07:54,520
that has been this section. I hope you guys enjoyed doing this. I thought this was pretty cool.

4973
06:07:54,520 --> 06:07:57,400
And in the next section, we're going to be talking about reinforcement learning.

4974
06:08:00,600 --> 06:08:05,480
Hello, everyone, and welcome to the next module in this course on reinforcement learning. So what

4975
06:08:05,480 --> 06:08:08,920
we're going to be doing in this module is talking about another technique in machine learning called

4976
06:08:08,920 --> 06:08:12,920
reinforcement learning. Now, if you remember at the very beginning of this course, which I know

4977
06:08:13,000 --> 06:08:18,200
for you guys is probably at like six hours ago at this point, we did briefly discuss what reinforcement

4978
06:08:18,200 --> 06:08:22,360
learning was. Now I'll go through a recap here just to make sure everyone's clear on it. But

4979
06:08:22,360 --> 06:08:27,000
essentially, reinforcement learning is kind of the strategy in machine learning where rather

4980
06:08:27,000 --> 06:08:32,840
than feeding a ton of data and a ton of examples to our model, we let the model or in this case,

4981
06:08:32,840 --> 06:08:37,800
we're going to call it agent actually come up with these examples itself. And we do this by letting

4982
06:08:37,880 --> 06:08:43,480
an agent explore an environment. Now, essentially, the concept here is just like humans, the way that

4983
06:08:43,480 --> 06:08:48,520
we learn to do something say like play a game is by actually doing it, we get put in the environment,

4984
06:08:48,520 --> 06:08:52,440
we try to do it. And then, you know, we'll make mistakes, we'll encounter different things,

4985
06:08:52,440 --> 06:08:57,720
we'll see what goes correctly. And based on those experiences, we learn and we figure out the correct

4986
06:08:57,720 --> 06:09:03,560
things to do a very basic example is, you know, say we play a game. And when we go left, we fell

4987
06:09:03,640 --> 06:09:07,880
off a cliff or something, right? Next time we play that game, and we get to that point, we're

4988
06:09:07,880 --> 06:09:12,600
probably not going to go left, because we're going to remember the fact that that was bad, and hence

4989
06:09:12,600 --> 06:09:17,480
learned from our mistakes. So that's kind of the idea here with reinforcement learning. I'm going

4990
06:09:17,480 --> 06:09:21,880
to go through exactly how this works and give some better examples and some math behind one of the

4991
06:09:21,880 --> 06:09:25,160
implementations we're going to use. But I just want to make this clear that there's a lot of

4992
06:09:25,160 --> 06:09:29,160
different types of reinforcement learning. In this example, we're just going to be talking

4993
06:09:29,160 --> 06:09:33,080
about something called q learning. And I'm going to keep this module shorter compared to the other

4994
06:09:33,080 --> 06:09:39,000
ones. Because this field of AI machine learning is pretty complex and can get pretty difficult

4995
06:09:39,000 --> 06:09:43,640
pretty quickly. So it's something that's maybe a more advanced topic for some of you guys. Alright,

4996
06:09:43,640 --> 06:09:48,280
so anyways, now we need to define some terminology before I can even start really explaining the

4997
06:09:48,280 --> 06:09:52,760
technique we're going to use and how this works. So we have something called an environment,

4998
06:09:52,760 --> 06:09:57,640
agent, state action and reward. And I'm hoping that some of you guys will remember this from the

4999
06:09:57,640 --> 06:10:03,640
very beginning. But environment is essentially what we're trying to solve or what we're trying to

5000
06:10:03,640 --> 06:10:08,600
do. So in reinforcement learning, we have this notion of an agent. And the agent is what's going

5001
06:10:08,600 --> 06:10:13,000
to explore the environment. So if we're thinking about reinforcement learning, when it comes to

5002
06:10:13,000 --> 06:10:18,440
say training an AI to play a game, well, in that instance, say we're talking about Mario, the agent

5003
06:10:18,440 --> 06:10:23,320
would be Mario as that is the thing that's moving around and exploring our environment. And the

5004
06:10:23,400 --> 06:10:28,600
environment would be the level in which we're playing in. So you know, in another example,

5005
06:10:28,600 --> 06:10:32,040
maybe in the example we're going to use below, we're actually going to be kind of in almost a

5006
06:10:32,040 --> 06:10:37,320
maze. So the environment is going to be the maze. And the agent is going to be the character or the

5007
06:10:37,320 --> 06:10:42,040
entity or whatever you want to call it, that's exploring that maze. So it's pretty, it's usually

5008
06:10:42,040 --> 06:10:46,040
pretty intuitive to come up with what the environment and the agent are, although in some

5009
06:10:46,040 --> 06:10:50,680
more complex examples, it might not always be clear. But just understand that reinforcement

5010
06:10:50,760 --> 06:10:55,320
learning deals with an agent, something exploring an environment and a very common

5011
06:10:55,880 --> 06:10:59,960
application of reinforcement learning is in training AI is on how to play games. And it's

5012
06:10:59,960 --> 06:11:03,960
actually very interesting what they've been able to do in that field recently. Okay, so we have

5013
06:11:03,960 --> 06:11:08,360
environments and agent, hopefully that makes sense. The next thing to talk about is state. So

5014
06:11:08,360 --> 06:11:15,080
essentially, the state is where you are in the environment. So obviously, inside of the environment,

5015
06:11:15,080 --> 06:11:19,880
we can have many different states. And a state could also be associated with the, you know,

5016
06:11:19,880 --> 06:11:25,880
agent itself. So we're going to say the agent is in a specific state, whenever it is in some

5017
06:11:25,880 --> 06:11:31,160
part of the environment. Now, in the case of our game, the state that an agent would be in

5018
06:11:31,160 --> 06:11:36,680
would be their position in the level, say if they're at, you know, x y coordinates, like 1020,

5019
06:11:36,680 --> 06:11:43,320
they would be at state or in state 1020. That's kind of how we think about states. Now, obviously,

5020
06:11:43,320 --> 06:11:47,560
state could be applied in some different instances as well. We're playing say, maybe a turn based

5021
06:11:47,560 --> 06:11:52,200
game. You know, actually, that's not really a great example. I'm trying to think of something

5022
06:11:52,200 --> 06:11:56,440
where the state wouldn't necessarily be a position, maybe if you're playing a game where you have

5023
06:11:56,440 --> 06:12:01,400
like health or something like that. And part of the state might be the health of the character.

5024
06:12:01,960 --> 06:12:05,480
This can get complicated, depending on what you're trying to do. But just understand the notion

5025
06:12:05,480 --> 06:12:09,560
that for most of our example, state is simply going to be in location, although it really is

5026
06:12:09,560 --> 06:12:14,040
just kind of telling us information about where the agent is, and its status in the environment.

5027
06:12:14,840 --> 06:12:20,280
So next, we have this notion of an action. So in reinforcement learning, our agent is exploring

5028
06:12:20,280 --> 06:12:24,440
the environment, it's trying to figure out the best way or how to accomplish some kind of goal

5029
06:12:24,440 --> 06:12:28,680
in the environment. And the way that it interacts with the environment is with something called

5030
06:12:28,680 --> 06:12:34,040
actions. Now, actions could be say, moving the left arrow key, right, moving to the left in

5031
06:12:34,040 --> 06:12:38,280
the environment, moving to the right, it could be something like jumping in an action can actually

5032
06:12:38,280 --> 06:12:44,360
be not doing something at all. So when we say, you know, agent performed action, that could

5033
06:12:44,360 --> 06:12:48,680
really mean that the action and that maybe time step was that they didn't do something, right,

5034
06:12:48,680 --> 06:12:53,560
that they didn't do anything that was their action. So that's kind of the idea of action.

5035
06:12:54,120 --> 06:12:58,760
In the example of our Mario one, which I keep going back to an action would be something like

5036
06:12:58,760 --> 06:13:04,680
jumping. And typically actions will change the state of our entity or our agent, although they

5037
06:13:04,680 --> 06:13:09,800
might not necessarily do that. In fact, we will observe with a lot of the different actions that

5038
06:13:09,800 --> 06:13:14,760
we could actually be in the same state after performing that action. Alright, so now we're

5039
06:13:14,760 --> 06:13:21,240
on to the last part, which is actually the most important to understand. And this is reward. So

5040
06:13:21,240 --> 06:13:26,520
reward is actually what our agent is trying to maximize while it is in the environment. So the

5041
06:13:26,520 --> 06:13:32,600
goal of reinforcement learning is to have this agent navigate this environment, go through a

5042
06:13:32,600 --> 06:13:38,760
bunch of the different states of it and determine which actions maximize the reward at every given

5043
06:13:38,760 --> 06:13:45,320
state. So essentially, the goal of our agent is to maximize a reward. But what is a reward? Well,

5044
06:13:45,320 --> 06:13:51,320
after every action that's taken, the agent will receive a reward. Now this reward is something

5045
06:13:51,320 --> 06:13:56,520
that us as the programmer need to come up with. The reason we need to do this is because we need

5046
06:13:56,520 --> 06:14:01,000
to tell the agent when it's performing well and when it's performing poorly. And just like we

5047
06:14:01,000 --> 06:14:06,200
had like a loss function in neural networks, when we're using those before, this is almost like

5048
06:14:06,200 --> 06:14:11,480
our loss function, you know, the higher this number is, the more reward the agent gets, the

5049
06:14:11,480 --> 06:14:17,240
better, the lower the reward, you know, it's not as good, it's not doing as well. So that's how we

5050
06:14:17,240 --> 06:14:22,840
kind of monitor and assess performance for our agents is by determining the almost average amount

5051
06:14:22,840 --> 06:14:27,400
of reward that they're able to achieve. And their goal is really to, you know, it's almost an

5052
06:14:27,400 --> 06:14:32,120
optimization problem where they're trying to maximize this reward. So what we're going to do

5053
06:14:32,120 --> 06:14:36,120
in reinforcement learning is have this agent exploring the environment, going through these

5054
06:14:36,120 --> 06:14:40,760
different states and performing these different actions, trying to maximize its reward. And

5055
06:14:40,760 --> 06:14:45,080
obviously, if we're trying to get the agent to say finish a level or, you know, complete the game,

5056
06:14:45,640 --> 06:14:51,480
then the maximum maximum reward will be achieved once it's completed the level or completed the

5057
06:14:51,480 --> 06:14:56,600
game. And if it does things that we don't like, say like dying or like jumping in the wrong spot,

5058
06:14:56,680 --> 06:15:01,080
we could give it a negative reward to try to influence it to not do that. And our goal,

5059
06:15:01,080 --> 06:15:05,400
you know, when we train these agents is for them to get the most reward. And we hope that

5060
06:15:05,400 --> 06:15:08,920
they're going to learn the optimal route through a level or through some environment that will

5061
06:15:08,920 --> 06:15:13,800
maximize that reward for them. Okay, so now I'm going to talk about a technique called Q learning,

5062
06:15:13,800 --> 06:15:18,120
which is actually just an algorithm that we're going to use to implement this idea of reinforcement

5063
06:15:18,120 --> 06:15:22,520
learning. We're not going to get into anything too crazy in this last module, because this is meant

5064
06:15:22,520 --> 06:15:27,720
to be more of an introduction into the kind of field of reinforcement learning than anything else.

5065
06:15:27,720 --> 06:15:32,200
But Q learning is the most basic way to implement reinforcement learning, at least that I have

5066
06:15:32,200 --> 06:15:37,800
discovered. And essentially, what Q learning is, and I don't actually really know why they call it

5067
06:15:37,800 --> 06:15:43,880
Q, although I should probably know that is creating some kind of table or matrix likes

5068
06:15:43,880 --> 06:15:49,720
data structure, that's going to contain as the, what is it, I guess the rows, every single state,

5069
06:15:49,720 --> 06:15:54,760
and as the columns, every single action that could be taken in all of those different states.

5070
06:15:54,760 --> 06:15:59,640
So for an example here, and we'll do one on kind of the whiteboard later on, if we can get there.

5071
06:16:00,680 --> 06:16:06,280
But here, we can see that this is kind of my Q table. And what I'm saying is that we have a one,

5072
06:16:06,280 --> 06:16:12,280
a two, a three, a four, as all of the possible actions that could be performed in any given state.

5073
06:16:12,280 --> 06:16:17,720
And we have three states denoted by the fact that we have three rows. And the numbers in this,

5074
06:16:18,440 --> 06:16:23,320
this table with this Q, what do they call it, Q matrix Q table, whatever you want to call it,

5075
06:16:23,320 --> 06:16:29,720
the numbers that are present here, represent what the predicted reward will be, given that we take

5076
06:16:29,720 --> 06:16:35,720
an action, whatever this action is in this state. So I'm not sure if this is making sense to you

5077
06:16:35,720 --> 06:16:41,800
guys, but essentially, if we're saying that row zero is state zero, action two, a two, this value

5078
06:16:41,800 --> 06:16:48,840
tells us what reward we should expect to get. If we take this action while we're in this state,

5079
06:16:48,840 --> 06:16:53,400
that's what that is trying to tell us. That's what that means. Same thing here in, you know,

5080
06:16:53,400 --> 06:16:59,800
state two, we can see that the optimal action to take would be action two, because that has the

5081
06:16:59,800 --> 06:17:05,000
highest reward for this state. And that's what this table is that we're going to try to generate

5082
06:17:05,000 --> 06:17:11,160
with this technique called Q learning, a table that can tell us given any state, what the predicted

5083
06:17:11,160 --> 06:17:15,880
reward will be for any action that we take. And we're going to generate this table by exploring

5084
06:17:15,880 --> 06:17:21,400
the environment many different times, and updating these values according to what we kind of see

5085
06:17:21,400 --> 06:17:26,040
or what the agent sees in the environment and the rewards that it receives for any given action in

5086
06:17:26,040 --> 06:17:30,520
any given state. And we'll talk about how we're going to update that later. But this is the basic

5087
06:17:30,520 --> 06:17:35,480
premise. So that is kind of Q learning, we're going to hop on the whiteboard now, and we'll do a

5088
06:17:35,480 --> 06:17:39,880
more in depth example, but then we're going to talk about how we actually learned this Q table

5089
06:17:39,960 --> 06:17:44,680
that I just discussed. Okay, so I've drawn a pretty basic example right now that I'm going to try

5090
06:17:44,680 --> 06:17:50,200
to use to illustrate the idea of Q learning and talk about some problems with it and how we can

5091
06:17:50,200 --> 06:17:54,920
kind of combat those as we learn more about how Q learning works. But the idea here is that we

5092
06:17:54,920 --> 06:17:59,080
currently have three states and why, what is happening? Why was that happening up at the top?

5093
06:17:59,080 --> 06:18:04,920
I don't know. Anyways, the idea is we have three states as one s two and s three. And at each state

5094
06:18:04,920 --> 06:18:10,280
we have two possible actions that can be taken, we can either stay in this state or we can move.

5095
06:18:10,840 --> 06:18:15,960
Now, what I've done is kind of just written some integers here that represent the reward that we're

5096
06:18:15,960 --> 06:18:21,640
going to get or that the agent is going to get such that it takes that action in a given state.

5097
06:18:21,640 --> 06:18:29,080
So if we take the action here in s one, right of moving, then we will receive a reward of one

5098
06:18:29,080 --> 06:18:32,840
because that's what we've written here is the reward that we get for moving. Whereas if we

5099
06:18:32,920 --> 06:18:37,480
stay, we'll get a reward of three, you know, same concept here, if we stay, we get two,

5100
06:18:37,480 --> 06:18:42,840
if we move, we get one, and I think you understand the point. So the goal of our agent to remember

5101
06:18:42,840 --> 06:18:47,400
is to maximize its reward in the environment. And what we're going to call the environment

5102
06:18:47,400 --> 06:18:53,080
is this right here, the environment is essentially defines the number of states, the number of

5103
06:18:53,080 --> 06:18:58,920
actions, and you know, the way that the agent can interact with these states and these actions.

5104
06:18:58,920 --> 06:19:03,320
So in this case, the agent can interact with the states by taking actions that change its state,

5105
06:19:03,320 --> 06:19:07,480
right? So that's where we're getting out with this. Now, what I want to do is show you how we

5106
06:19:07,480 --> 06:19:14,680
use this queue table, or learn this queue table to come up with kind of the almost, you know,

5107
06:19:14,680 --> 06:19:19,080
the model, like the machine learning model that we're going to use. So essentially, what we would

5108
06:19:19,080 --> 06:19:27,400
want to have here is we want to have a kind of pattern in this table that allows our agent to

5109
06:19:27,480 --> 06:19:32,440
receive the maximum reward. So in this case, we're going to say that our agent will start at

5110
06:19:32,440 --> 06:19:36,760
state s one. And obviously, whenever we're doing this reinforcement learning, we need to have some

5111
06:19:36,760 --> 06:19:41,240
kind of start state that the agent will start in this could be a random state, it could change,

5112
06:19:41,240 --> 06:19:45,080
but it doesn't just start in some state. So in this case, we're going to say it starts at s one.

5113
06:19:45,640 --> 06:19:51,320
Now, when we're in s one, the agent has two things that it can do. It can stay in the current state

5114
06:19:51,320 --> 06:19:57,560
and receive a reward of three, or it can move and receive a reward of one, right? If we get to s

5115
06:19:57,560 --> 06:20:02,680
two, in this state, what can we do? We can stay, which means we receive a reward of two, or we

5116
06:20:02,680 --> 06:20:07,320
can move, which means we get a reward of one. And same thing for s three, we can stay, we get a

5117
06:20:07,320 --> 06:20:13,800
reward of four, and we can move, we get a reward of one. Now, right now, if we had just ran this

5118
06:20:14,360 --> 06:20:19,400
one time and have the agent stay in each state, like start in each unique state,

5119
06:20:19,400 --> 06:20:24,200
this is what the queue table we would get would look like. Because after looking at this, just

5120
06:20:24,200 --> 06:20:29,160
one time starting in each state, what the agent would be able to, or I guess, two times, because

5121
06:20:29,160 --> 06:20:34,680
it would have to try each action. Let's say we had the agent start in each state twice. So it started

5122
06:20:34,680 --> 06:20:39,080
an s one twice, it started s two twice, and it started an s three twice. And every time it started

5123
06:20:39,080 --> 06:20:43,480
there, it tried one of the different actions. So when it started in s one, it tried moving once,

5124
06:20:43,480 --> 06:20:47,400
and then it tried staying once, we would have a queue table that looks like this. Because what

5125
06:20:47,480 --> 06:20:52,840
would happen is we would update values in our queue table to represent the reward we received

5126
06:20:52,840 --> 06:20:58,360
when we took that action from that state. So we can see here that when we're in state s one,

5127
06:20:58,360 --> 06:21:04,520
and we decide to stay, what we did is we wrote a three inside of the stay column, because that is

5128
06:21:04,520 --> 06:21:10,840
how much reward we received when we moved, right? Same thing for state two, when we moved for state

5129
06:21:10,840 --> 06:21:16,520
two or have I guess, sorry, stayed when we stayed in state two, we received a reward of two, same

5130
06:21:16,520 --> 06:21:23,080
thing for four. Now, this is okay, right? This tells us kind of, you know, the optimal move to

5131
06:21:23,080 --> 06:21:28,120
make in any state to receive the maximum reward. But what if we introduce the idea that, you know,

5132
06:21:28,120 --> 06:21:35,160
our agent, we want it to receive the maximum total reward possible, right? So if it's in state one,

5133
06:21:35,160 --> 06:21:39,800
ideally, we'd like it to move to state two, and then move to states three, and then just stay in

5134
06:21:39,800 --> 06:21:43,800
state three, because it will receive the most amount of reward. Well, with the current table

5135
06:21:43,800 --> 06:21:48,200
that we've developed, if we just follow this, and we look at the table, we say, okay, if we want to

5136
06:21:48,200 --> 06:21:52,920
use this Q learning table now to, you know, move an agent around our level, what we'll do is we'll

5137
06:21:52,920 --> 06:21:58,600
say, okay, what state is it in? If it's in state two, we'll do stay because that's the highest reward

5138
06:21:58,600 --> 06:22:04,120
that we have in this table. If that's the approach we use, then we could see that if our, you know,

5139
06:22:04,120 --> 06:22:09,640
agent start in state one or state two, it would stay in what we call a local minima, because

5140
06:22:09,640 --> 06:22:15,320
it's not able to kind of realize from this state that it can move any further and receive a much

5141
06:22:15,320 --> 06:22:19,960
greater reward, right? And that's kind of the concept we're going to talk about as we implement and,

5142
06:22:19,960 --> 06:22:25,400
you know, discuss further how Q learning works. But hopefully this gives you a little bit of insight

5143
06:22:25,400 --> 06:22:31,560
into what we do with this table. Essentially, when we're updating these table values is when

5144
06:22:31,560 --> 06:22:36,680
we're exploring this environment. So when we explore this environment, and we start in a state,

5145
06:22:36,680 --> 06:22:41,720
when we take an action to another state, we observe the reward that we got from going there,

5146
06:22:41,720 --> 06:22:46,200
and we observe the state that we change to, right? So we observe the fact that in state one,

5147
06:22:46,200 --> 06:22:51,480
when we go to state two, we receive the reward of one. And what we do is we take that observation

5148
06:22:51,480 --> 06:22:57,640
and we use it to update this Q table. And the goal is that at the end of all of these observations,

5149
06:22:57,640 --> 06:23:03,800
and there could be millions of them, that we have a Q table that tells us the optimal action to take

5150
06:23:03,880 --> 06:23:09,880
in any single state. So we're actually hard coding, this kind of mapping that essentially

5151
06:23:09,880 --> 06:23:14,760
just tells us given any state, all you have to do is look up in this table, look at all of the

5152
06:23:14,760 --> 06:23:19,720
actions that could be taken, and just take the maximum action or the reward that's supposed to

5153
06:23:19,720 --> 06:23:24,440
give, I guess, the action that's supposed to give the maximum reward. And if we were to follow

5154
06:23:24,440 --> 06:23:28,120
that on this, we could see we get stuck in the local minima, which is why we're going to introduce

5155
06:23:28,120 --> 06:23:35,000
a lot of other concepts. So our reinforcement learning model in Q learning, we have to implement

5156
06:23:35,000 --> 06:23:41,320
the concept of being able to explore the environment, not based on previous experiences,

5157
06:23:41,320 --> 06:23:46,120
right? Because if we just tell our model, okay, what we're going to do is we're going to start in

5158
06:23:46,120 --> 06:23:50,120
all these different states, we're going to start in the start state and just start navigating around.

5159
06:23:50,120 --> 06:23:55,000
If we update our model immediately, or update our Q table immediately and put this three here for

5160
06:23:55,080 --> 06:24:01,240
stay, we can almost guarantee that since this three is here, when our model is training, right,

5161
06:24:01,240 --> 06:24:05,480
if it's using this Q table to determine what state to move to next, when it's training and

5162
06:24:05,480 --> 06:24:09,480
determining what to do, it's just always going to stay, which means we'll never get a chance to

5163
06:24:09,480 --> 06:24:15,480
even see what we could have gotten to at S three. So we need to kind of introduce some concept

5164
06:24:15,480 --> 06:24:21,560
of taking random actions, and being able to explore the environment more freely before starting

5165
06:24:21,560 --> 06:24:25,800
to look at these Q values, and use that for the training. So I'm actually going to go back

5166
06:24:26,600 --> 06:24:30,280
to my slides now to make sure I don't get lost, because I think I was starting to ramble a little

5167
06:24:30,280 --> 06:24:35,480
bit there. So we're going to now talk about learning the Q table. So essentially, I showed

5168
06:24:35,480 --> 06:24:41,320
you how we use that Q table, which is given some state, we just look that state up in the Q table,

5169
06:24:41,320 --> 06:24:45,960
and then determine what the maximum reward we could get by taking, you know, some action is and

5170
06:24:45,960 --> 06:24:50,680
then take that action. And that's how we would use the Q table later on when we're actually using the

5171
06:24:50,680 --> 06:24:56,040
model. But when we're learning the Q table, that's not necessarily what we want to do. We don't want

5172
06:24:56,040 --> 06:25:01,320
to explore the environment by just taking the maximum reward we've seen so far and just always

5173
06:25:01,320 --> 06:25:05,480
going that direction, we need to make sure that we're exploring in a different way and learning

5174
06:25:05,480 --> 06:25:10,280
the correct values for the Q table. So essentially, our agent learns by exploring the environment

5175
06:25:10,280 --> 06:25:14,280
and observing the outcome slash reward from each action it takes in a given state, which we've

5176
06:25:14,280 --> 06:25:18,520
already said. But how does it know what action to take in each state when it's learning? That's

5177
06:25:18,520 --> 06:25:23,240
the question I need to answer for you now. Well, there's two ways of doing this. Our agent can

5178
06:25:23,240 --> 06:25:28,200
essentially, you know, use the current Q table to find the best action, which is kind of what I

5179
06:25:28,200 --> 06:25:32,520
just discussed. So taking looking at the Q table, looking at the state and just taking the highest

5180
06:25:32,520 --> 06:25:38,920
reward, or it can randomly pick a valid action. And our goal is going to be when we create this Q

5181
06:25:38,920 --> 06:25:45,400
learning algorithm to have a really great balance of these two, where sometimes we use the Q table

5182
06:25:45,400 --> 06:25:52,280
to find the best action, and sometimes we take a random action. So that is one thing. But now

5183
06:25:52,280 --> 06:25:56,600
I'm just going to talk about this formula for how we actually update Q values. So obviously, what's

5184
06:25:56,600 --> 06:26:01,080
going to end up happening in our Q learning is we're going to have an agent that's going to be in

5185
06:26:01,080 --> 06:26:05,880
the learning stage, exploring the environment and having all these actions and all these rewards

5186
06:26:05,880 --> 06:26:09,480
and all these observations happening. And it's going to be moving around the environment by

5187
06:26:09,480 --> 06:26:13,480
following one of these two kind of principles, randomly picking a valid action or using the

5188
06:26:13,560 --> 06:26:18,920
current Q table to find the best action. When it gets into a net, a new state, and it, you know,

5189
06:26:18,920 --> 06:26:22,840
moves from state to state, it's going to keep updating this Q table, telling it, you know,

5190
06:26:22,840 --> 06:26:26,200
this is what I've learned about the environment, I think this is a better move, we're going to

5191
06:26:26,200 --> 06:26:30,360
update this value. But how does it do that in a way that's going to make sense? Because we can't

5192
06:26:30,360 --> 06:26:35,080
just put, you know, the maximum value we got from moving, otherwise, we're going to run into that

5193
06:26:35,080 --> 06:26:39,800
issue, which I just talked about, where we get stuck in that local maxima, right? I'm not sure

5194
06:26:39,800 --> 06:26:44,760
if I called it minimum before, but anyways, it's local maxima, where we see this high reward,

5195
06:26:44,760 --> 06:26:49,560
but that's preventing us if we keep taking that action from reaching a potentially high reward

5196
06:26:49,560 --> 06:26:55,000
in a different state. So the formula that we actually use to update the Q table is this. So Q

5197
06:26:55,000 --> 06:27:00,600
state action equals Q state action, and a state action is just referencing first the rows for the

5198
06:27:00,600 --> 06:27:07,240
state and then the action as the column, plus alpha times, and then this is all in brackets,

5199
06:27:07,240 --> 06:27:15,880
right? Reward plus, I believe this is gamma times max Q of new states minus Q state action. So what

5200
06:27:15,880 --> 06:27:19,720
the heck does this mean? What are these constants? What is all this? We're going to talk about the

5201
06:27:19,720 --> 06:27:25,160
constants in a minute. But I want to, yeah, I want to explain this formula actually. So let's,

5202
06:27:25,160 --> 06:27:29,160
okay, I guess we'll go through the constants, it's hard to go through a complicated math formula.

5203
06:27:29,160 --> 06:27:34,360
So a stands for the learning rate, and gamma stands for the discount factor. So alpha learning

5204
06:27:34,360 --> 06:27:38,520
rate, gamma discount factor. Now, what is the learning rate? Well, this is a little blurb on

5205
06:27:38,520 --> 06:27:44,360
what this is. But essentially, the learning rate ensures that we don't update our Q table too much

5206
06:27:45,320 --> 06:27:51,160
on every observation. So before, right, when I was showing you like this, if we can go back

5207
06:27:51,160 --> 06:27:56,680
to my windows ink, why is this not working? I guess I'm just not patient enough. Before when I was

5208
06:27:56,680 --> 06:28:01,880
showing you all I did when I took an action was I looked at the reward that I got from taking that

5209
06:28:01,880 --> 06:28:07,080
action. And I just put that in my Q table, right? Now, obviously, that is not an optimal approach

5210
06:28:07,080 --> 06:28:11,640
to do this, because that means that in the instance where we hit state one, well, I'm not going to

5211
06:28:11,640 --> 06:28:15,240
be able to get to this reward of four, because I'm going to throw that, you know, three in here,

5212
06:28:15,240 --> 06:28:21,800
and I'm just going to keep taking that action. We need to, you know, hopefully make this move

5213
06:28:21,800 --> 06:28:27,480
action actually have a higher value than stay. So that next time we're in state one, we consider

5214
06:28:27,480 --> 06:28:32,520
the fact that we could move to state two, and then move to state three to optimize our reward.

5215
06:28:32,520 --> 06:28:36,040
So how do we do that? Well, the learning rate is one thing that helps us kind of accomplish

5216
06:28:36,040 --> 06:28:41,080
this behavior. Essentially, what is telling us, and this is usually a decimal value, right,

5217
06:28:41,080 --> 06:28:47,640
is how much we're allowed to update every single Q value by on every single action or every single

5218
06:28:47,640 --> 06:28:52,360
observation. So if we just use the approach before, then we're only going to need to observe,

5219
06:28:52,360 --> 06:28:55,960
given the amount of states and the amount of actions, and we'll be able to completely fill

5220
06:28:55,960 --> 06:28:59,800
in the Q table. So in our case, if we had like three states and three actions, we could, you

5221
06:28:59,800 --> 06:29:04,440
know, nine iterations, we'd be able to fill the entire Q table. The learning rate means that

5222
06:29:04,440 --> 06:29:10,040
it's going to just update a little bit slower and essentially change the value in the Q table very

5223
06:29:10,040 --> 06:29:14,440
slightly. So you can see that what we're doing is taking the current value of the Q table. So

5224
06:29:14,440 --> 06:29:20,440
whatever is already there. And then what we're going to do is add some value here. And this value

5225
06:29:20,440 --> 06:29:24,920
that we add is either going to be positive or negative, essentially telling us, you know,

5226
06:29:24,920 --> 06:29:29,240
whether we should take this new action or whether we shouldn't take this new action.

5227
06:29:29,240 --> 06:29:34,920
Now, the way that this kind of value is calculated, right, is obviously our alpha is

5228
06:29:34,920 --> 06:29:41,000
multiplied this by this, but we have the reward plus, in this case, gamma, which is just going to

5229
06:29:41,000 --> 06:29:46,280
actually be the discount factor. And I'll talk about how that works in a second of the maximum

5230
06:29:46,280 --> 06:29:53,960
of the new state we moved into. Now, what this means is find the maximum reward that we could

5231
06:29:53,960 --> 06:30:01,400
receive in the new state by taking any action and multiply that by what we call the discount factor.

5232
06:30:01,400 --> 06:30:05,880
With this part of the formulas trying to do is exactly what I've kind of been talking about.

5233
06:30:06,520 --> 06:30:12,040
Try to look forward and say, okay, so I know if I take this action in this state, I receive

5234
06:30:12,040 --> 06:30:17,880
this amount of reward. But I need to factor in the reward I could receive in the next state,

5235
06:30:17,880 --> 06:30:23,800
so that I can determine the best place to move to. That's kind of what this max and this

5236
06:30:24,360 --> 06:30:28,760
gamma are trying to do for us. So this discount factor, whatever you want to call it. It's trying

5237
06:30:28,760 --> 06:30:35,160
to factor in a little bit about what we could get from the next state into this equation so that

5238
06:30:35,160 --> 06:30:41,640
hopefully our kind of agent can learn a little bit more about the transition states. So states that

5239
06:30:41,640 --> 06:30:46,200
maybe are actions that maybe don't give us an immediate reward, but lead to a larger reward

5240
06:30:46,200 --> 06:30:50,920
in the future. That's what this Y and max are trying to do. Then what we do is we subtract

5241
06:30:51,560 --> 06:30:57,560
from this, the state and action. This is just to make sure that we're adding what the difference

5242
06:30:57,560 --> 06:31:05,240
was in, you know, what we get from this versus what the current value is, and not like multiplying

5243
06:31:05,240 --> 06:31:09,640
these values crazily. I mean, you can look into more of the math here and plug in like some values

5244
06:31:09,640 --> 06:31:12,680
later, and you'll see how this kind of works. But this is the basic formula. And I feel like I

5245
06:31:12,680 --> 06:31:18,280
explain that in depth enough. Okay, so now that we've done that, and we've updated this, we've

5246
06:31:18,360 --> 06:31:24,200
learned kind of how we update the cells and how this works. I could go back to the whiteboard and

5247
06:31:24,200 --> 06:31:28,360
draw it out. But I feel like that makes enough sense. We're going to look at what the next state is,

5248
06:31:28,360 --> 06:31:32,440
we're going to factor that into our calculation, we have this learning rate, which tells us essentially

5249
06:31:32,440 --> 06:31:38,520
how much we can update each cell value by. And we have this, what do you call it here discount

5250
06:31:38,520 --> 06:31:43,880
factor, which essentially tries to kind of define the balance between finding really good rewards

5251
06:31:43,880 --> 06:31:49,960
in our current state, and finding the rewards in the future state. So the higher this value is,

5252
06:31:49,960 --> 06:31:53,960
the more we're going to look towards the future, the lower it is, the more we're going to focus

5253
06:31:53,960 --> 06:31:57,480
completely on our current reward, right? And obviously, that makes sense, because we're going

5254
06:31:57,480 --> 06:32:01,240
to add the maximum value. And if we're multiplying that by a lower number, that means we're going

5255
06:32:01,240 --> 06:32:07,480
to consider that less than if that was greater. Awesome. Okay. So now that we've kind of understand

5256
06:32:07,480 --> 06:32:11,000
that I want to move on to a Q learning example. And what we're going to do for this example is

5257
06:32:11,080 --> 06:32:16,600
actually use something called the open AI gym. I just need to throw my drawing tablet away

5258
06:32:16,600 --> 06:32:21,960
right there so that we can get started. But open AI gym is actually a really interesting kind of

5259
06:32:21,960 --> 06:32:27,240
module. I don't even actually, I don't even really know the way to describe it almost tool. There's

5260
06:32:27,240 --> 06:32:33,080
actually developed by open AI, you know, coincidentally by the name, which is founded by Elon Musk

5261
06:32:33,080 --> 06:32:38,120
and someone else. So he's actually, you know, made this kind of, I don't really don't know the word

5262
06:32:38,120 --> 06:32:43,720
to describe it. I almost want to say tool that allows programmers to work with these really cool

5263
06:32:43,720 --> 06:32:48,600
gym environments and train reinforcement learning models. So you'll see how this works in a second,

5264
06:32:48,600 --> 06:32:52,840
but essentially, there's a ton of graphical environments that have very easy interfaces

5265
06:32:52,840 --> 06:32:57,320
to use. So like moving characters around them, that you're allowed to experiment with completely

5266
06:32:57,320 --> 06:33:01,640
for free as a programmer to try to, you know, make some cool reinforcement learning models.

5267
06:33:01,640 --> 06:33:05,320
That's what open AI gym is. And you can look at it. I mean, we'll click on it here actually to see

5268
06:33:05,320 --> 06:33:09,080
what it is. You can see gym, there's all these different Atari environments, and it's just a

5269
06:33:09,080 --> 06:33:14,360
way to kind of train reinforcement learning models. All right. So now we're going to start by just

5270
06:33:14,360 --> 06:33:18,840
importing gym. If you're in Collaboratory, there's nothing you need to do here. If you're in your

5271
06:33:18,840 --> 06:33:22,840
own thing, you're going to have to pip install gym. And then what we're going to do is make this

5272
06:33:22,840 --> 06:33:28,520
frozen lake v zero gym. So essentially, what this does is just set up the environment that we're

5273
06:33:28,520 --> 06:33:32,920
going to use. Now, I'll talk more about what this environment is later, but I want to talk about how

5274
06:33:32,920 --> 06:33:37,800
gym works, because we are going to be using this throughout the thing. So the open AI gym

5275
06:33:38,440 --> 06:33:43,560
is meant for reinforcement learning. And essentially what it has is an observation space

5276
06:33:43,560 --> 06:33:48,520
and an action space for every environment. Now the observation space is what we call our

5277
06:33:48,520 --> 06:33:53,880
environment, right? And that will tell us the amount of states that exist in this environment.

5278
06:33:53,880 --> 06:33:57,080
Now, in our case, we're going to be using kind of like a maze like thing, which I'll show you in

5279
06:33:57,080 --> 06:34:01,800
a second. So you'll understand why we get the values we do. Action space tells us how many

5280
06:34:01,800 --> 06:34:07,320
actions we can take when we do the dot n, at any given state. So if we print this out,

5281
06:34:07,880 --> 06:34:12,680
we get 16 and four, representing the observation space. In other words, the number of states is

5282
06:34:12,680 --> 06:34:18,200
16. And the amount of actions we can take in every single state is four. Now in this case,

5283
06:34:18,200 --> 06:34:24,120
these actions are going to be left down up and right. But yes, now env dot reset. So essentially,

5284
06:34:24,120 --> 06:34:29,080
we have some commands that allow us to move around the environment, which are actually down here.

5285
06:34:29,080 --> 06:34:33,640
If we want to reset the environment and start back in the beginning state, then we do env

5286
06:34:33,640 --> 06:34:37,720
dot reset, you can see this actually returns to us the starting state, which obviously is going to

5287
06:34:37,720 --> 06:34:44,440
be zero. Now we also have the ability to take a random action, or select a random action from

5288
06:34:44,440 --> 06:34:49,240
the action space. So what this line does right here is say of the action space, so of all the

5289
06:34:49,240 --> 06:34:54,440
commands that are there, or all the actions we could take, pick a random one and return that.

5290
06:34:54,520 --> 06:35:01,560
So if you do that, actually, let's just print action and see what this is. You see we get zero

5291
06:35:01,560 --> 06:35:07,960
to right, it just gives us a random action that is valid from the action space. All right. Next,

5292
06:35:07,960 --> 06:35:14,600
what we have is this env dot step in action. Now what this does is take whatever action we have,

5293
06:35:14,600 --> 06:35:19,880
which in this case is three, and perform that in the environment. So tell our agent to take

5294
06:35:19,880 --> 06:35:25,160
this action in the environment and return to us a bunch of information. So the first thing is the

5295
06:35:25,160 --> 06:35:30,200
observation, which essentially means what state do we move into next? So I could call this

5296
06:35:31,640 --> 06:35:37,960
new underserved state reward is what reward did we receive by taking that action? So this will

5297
06:35:37,960 --> 06:35:43,160
be some value right in our in this case, the reward is either one or zero. But that's not

5298
06:35:43,160 --> 06:35:48,760
that important to understand. And then we have a bool of done, which tells us did we lose the game

5299
06:35:48,760 --> 06:35:54,120
or did we win the game? Yes or no. So true. So if this is true, what this means is we need to

5300
06:35:54,120 --> 06:35:59,640
reset the environment because our agent either lost or won and is no longer in a valid state in

5301
06:35:59,640 --> 06:36:04,520
the environment. Info gives us a little bit of information. It's not showing me anything here.

5302
06:36:04,520 --> 06:36:08,920
We're not going to use info throughout this, but figured I'd let you know that now in VDOT

5303
06:36:08,920 --> 06:36:13,880
render, I'll actually render this for you and show you renders a graphical user interface that

5304
06:36:13,880 --> 06:36:18,360
shows you the environment. Now, if you use this while you're training, so you actually watch

5305
06:36:18,360 --> 06:36:22,680
the agent do the training, which is what you can do with this, it slows it down drastically,

5306
06:36:22,680 --> 06:36:26,520
like probably by, you know, 10 or 20 times, because it actually needs to draw the stuff on

5307
06:36:26,520 --> 06:36:30,280
the screen. But you know, you can use it if you want. So this is what our frozen lake example

5308
06:36:30,280 --> 06:36:34,360
looks like. You can see that the highlighted square is where our agent is. And in this case,

5309
06:36:34,360 --> 06:36:42,200
we have four different blocks. We have SFH and G. So S stands for start F stands for frozen,

5310
06:36:42,200 --> 06:36:46,760
because this is a frozen lake. And the goal is to navigate to the goal without falling in one

5311
06:36:46,760 --> 06:36:52,040
of the holes, which is represented by H. And this here tells us the action that we just took. Now,

5312
06:36:52,040 --> 06:36:58,280
I guess the starting action is up because that's zero, I believe. But yes, so if we run this a

5313
06:36:58,280 --> 06:37:02,280
bunch of times, we'll see this updating. Unfortunately, this doesn't work very well in

5314
06:37:02,280 --> 06:37:07,080
Google Collaboratory, the the GUIs. But if you did this in your own command line, and you like

5315
06:37:07,080 --> 06:37:11,400
did some different steps and rounded it all out, you would see this working properly. Okay,

5316
06:37:11,480 --> 06:37:15,000
so now we're on to talking about the frozen lake environment, which is kind of what I just did.

5317
06:37:15,000 --> 06:37:18,840
So now we're just going to move to the example where we actually implement Q learning to

5318
06:37:18,840 --> 06:37:23,400
essentially solve the problem. How can we train an AI to navigate this environment and get to the

5319
06:37:23,400 --> 06:37:27,960
start to the goal? How can we do that? Well, we're going to use Q learning. So let's start. So the

5320
06:37:27,960 --> 06:37:32,680
first thing we need to do is import gym, import numpy, and then create some constants here. So

5321
06:37:32,680 --> 06:37:36,360
we'll do that. We're going to say the amount of states is equal to the line I showed you before.

5322
06:37:36,440 --> 06:37:43,240
So env dot observation, space dot n, actions is equal to env dot action space n. And then we're

5323
06:37:43,240 --> 06:37:48,680
going to say Q is equal to NP dot zeros, states and actions. So something I guess I forgot to

5324
06:37:48,680 --> 06:37:53,800
mention is when we initialize the Q table, we just initialize all blank values or zero values,

5325
06:37:53,800 --> 06:37:58,200
because obviously, at the beginning of our learning, our model or agent doesn't know

5326
06:37:58,200 --> 06:38:01,800
anything about the environment yet. So we just leave those all blank, which means we're going

5327
06:38:01,800 --> 06:38:06,520
to more likely be taking random actions at the beginning of our training, trying to explore

5328
06:38:06,520 --> 06:38:11,160
the environment space more. And then as we get further on and learn more about the environment,

5329
06:38:11,160 --> 06:38:16,840
those actions will likely be more calculated based on the Q table values. So we print this out,

5330
06:38:16,840 --> 06:38:23,000
we can see this is the array that we get, we've had to be build a 16 by four, I guess not array,

5331
06:38:23,000 --> 06:38:27,720
well, I guess this technically is an array, we'll call it matrix 16 by four. So every single row

5332
06:38:27,720 --> 06:38:31,640
represents a state, and every single column represents an action that could be taken in

5333
06:38:31,640 --> 06:38:35,880
that state. Alright, so we're going to find some constants here, which we talked about before.

5334
06:38:35,880 --> 06:38:40,760
So we have the gamma, the learning rate, the max amount of steps and the number of episodes. So the

5335
06:38:40,760 --> 06:38:46,120
number of episodes is actually, how many episodes do you want to train your agent on? So how many

5336
06:38:46,120 --> 06:38:51,560
times do you want it to run around and explore the environment? That's what episode stands for.

5337
06:38:52,280 --> 06:38:57,480
Max steps essentially says, Okay, so if we're in the environment, and we're kind of navigating

5338
06:38:57,560 --> 06:39:01,400
and moving around, and we haven't died yet, how many steps are we going to let the agent take

5339
06:39:01,400 --> 06:39:05,800
before we cut it off? Because what could happen is we could just bounce in between two different

5340
06:39:05,800 --> 06:39:10,760
states indefinitely. So we need to make sure we have a max steps so that at some point,

5341
06:39:10,760 --> 06:39:15,000
if the agent is just doing the same thing, we can, you know, end that or if it's like going in

5342
06:39:15,000 --> 06:39:21,480
circles, we can end that and start again with different, you know, Q values. Alright, so episodes,

5343
06:39:21,480 --> 06:39:24,840
yeah, we already talked about that learning rate, we know what that is gamma, we know what that is

5344
06:39:25,400 --> 06:39:29,400
mess with these values as we go through and you'll see the difference it makes in our training.

5345
06:39:29,400 --> 06:39:33,560
I've actually included a graph down below. So we'll talk about that kind of show us the outcome

5346
06:39:33,560 --> 06:39:41,400
of our training. But learning rate, the higher this is, the faster I believe that it learns. Yes,

5347
06:39:41,400 --> 06:39:45,800
so a high learning rate means that each update will introduce larger change to the current state.

5348
06:39:45,800 --> 06:39:49,480
So yeah, so that makes sense based on the equation as well. Just wanted to make sure that I wasn't

5349
06:39:49,480 --> 06:39:54,040
going crazy there. So let's run this constant block to make sure. And now we're going to talk

5350
06:39:54,040 --> 06:39:58,520
about picking an action. So remember how I said, and I actually wrote them down here,

5351
06:39:58,520 --> 06:40:04,520
there's essentially two things we can do at every, what do we call it, step, right? We can

5352
06:40:04,520 --> 06:40:09,560
randomly pick a valid action, or we can use the current Q table to find the best action. So how

5353
06:40:09,560 --> 06:40:13,480
do we actually implement that into our open AI gym? Well, I just wanted to write a little

5354
06:40:13,480 --> 06:40:18,200
code block here to show you the exact code that will do this for us. So we're going to introduce

5355
06:40:18,200 --> 06:40:25,160
this new concept or this new, I can almost call it constant, called epsilon. And I think epsilon,

5356
06:40:25,160 --> 06:40:30,520
I think I spelt this wrong, ep salon. Yeah, that should be how you spell it. So we're going to start

5357
06:40:30,520 --> 06:40:34,120
the epsilon value essentially tells us the percentage chance that we're going to pick a

5358
06:40:34,120 --> 06:40:39,480
random action. So here, we're going to use a 90% epsilon, which essentially means that every time

5359
06:40:39,480 --> 06:40:43,960
we take an action, there's going to be a 90% chance that it's random and 10% chance that we look at

5360
06:40:43,960 --> 06:40:49,720
the Q table to make that action. Now, we'll reduce this epsilon value as we train, so that

5361
06:40:49,720 --> 06:40:53,960
our model will start being able to explore, you know, as much as it possibly can in the

5362
06:40:53,960 --> 06:40:59,160
environment by just taking random actions. And then after we have enough observations,

5363
06:40:59,160 --> 06:41:02,920
and we've explored the environment enough, we'll start to slowly decrease the epsilon,

5364
06:41:02,920 --> 06:41:08,120
so that it hopefully finds a more optimal route for things to do. Now, the way we do this is we

5365
06:41:08,120 --> 06:41:12,360
save NP dot random dot uniform zero one, which essentially means pick a random value between

5366
06:41:12,360 --> 06:41:19,880
zero and one is less than epsilon and epsilon like that. I think I'm going to have to change

5367
06:41:19,880 --> 06:41:25,000
some other stuff, but we'll see, then action equals ENV dot action space dot sample. So

5368
06:41:25,000 --> 06:41:29,880
take a random action. That's what this means store what that action is in here. Otherwise,

5369
06:41:29,880 --> 06:41:37,480
we're going to take the argument max of the state row in the Q table. So what this means is find

5370
06:41:37,480 --> 06:41:41,800
the maximum value in the Q table and tell us what row it's in. So that way we know what

5371
06:41:41,800 --> 06:41:46,680
action to take. So if we're in row, I guess, not sorry, not row column for in column one,

5372
06:41:46,680 --> 06:41:50,280
you know, that's maximum value, take action one, that's what this is saying. So using the Q table

5373
06:41:50,280 --> 06:41:55,080
to pick the best action. Alright, so we don't need to run this because this is just going to be

5374
06:41:55,080 --> 06:41:59,640
which I just wrote that to show you. Now, how do we update the Q values? Well, this is just

5375
06:41:59,640 --> 06:42:04,040
following the equation that I showed above. So this is the line of code that does this, I just

5376
06:42:04,040 --> 06:42:08,040
want to write it out so you guys could see exactly what each line is doing and kind of explore it

5377
06:42:08,040 --> 06:42:11,960
for yourself. But essentially, you get the point, you know, you have your learning rate, reward,

5378
06:42:11,960 --> 06:42:17,160
gamma, take the max, so NP dot max does the same thing as a max function in Python. This is going

5379
06:42:17,160 --> 06:42:22,680
to take the max value, not the argument max from the next state, right, the new state that we moved

5380
06:42:22,680 --> 06:42:28,200
into. And then subtracting obviously Q state action. Alright, so putting it all together. So

5381
06:42:28,200 --> 06:42:32,360
now we're actually going to show how we can train and create this Q table and then use that Q table.

5382
06:42:33,000 --> 06:42:37,320
So this is the pretty much all this code that I have, we've already actually

5383
06:42:37,320 --> 06:42:41,400
written at least this block here, that's why I put it in its own block. So just all the constants,

5384
06:42:41,400 --> 06:42:45,000
I've included this render constant to tell us whether we want to draw the environment or not.

5385
06:42:45,000 --> 06:42:47,880
In this case, I'm going to leave it false, but you can make it true if you want.

5386
06:42:47,880 --> 06:42:52,360
Episodes, I've left at 1500 for this, if you want to make your model better, typically you

5387
06:42:52,360 --> 06:42:57,000
train it on more episodes, but that's up to you. And now we're going to get into the big chunk

5388
06:42:57,000 --> 06:43:02,200
of code, which I'm going to talk about. So what this is going to do, we're going to have a rewards

5389
06:43:02,200 --> 06:43:06,280
list, which is actually just going to store all the rewards we see, just so I can graph that later

5390
06:43:06,280 --> 06:43:11,080
for you guys. Then we're going to say for episode in range episodes. So this is just telling us,

5391
06:43:11,080 --> 06:43:16,200
you know, for every episode, let's do the steps I'm about to do. So maximum amount of episodes,

5392
06:43:16,200 --> 06:43:20,120
which is our training length, essentially, we're going to reset the state, obviously,

5393
06:43:20,120 --> 06:43:24,440
which makes sense. So state equals in V dot reset, which will give us the starting state.

5394
06:43:25,080 --> 06:43:29,400
We're going to say for underscore in range, max steps, which means, okay, we're going to do,

5395
06:43:29,400 --> 06:43:34,040
you know, we're going to explore the environment up to maximum steps, we do have a done here,

5396
06:43:34,120 --> 06:43:38,040
which will actually break the loop if we've reached the goal, which we'll talk about further.

5397
06:43:38,600 --> 06:43:42,280
So the first thing we're going to do is say, if render, you know, render the environment,

5398
06:43:42,280 --> 06:43:47,640
that's pretty straightforward. Otherwise, let's take an action. So for each time step, we need to

5399
06:43:47,640 --> 06:43:52,200
take an action. So epsilon, I think is spelled correctly here. Yeah, believe that's right. So

5400
06:43:52,200 --> 06:43:56,280
I'm going to say action equals in V dot action space, this is already the code we've looked at.

5401
06:43:56,280 --> 06:44:01,160
And then what we're going to say is next state reward done underscore equals in V dot step

5402
06:44:01,160 --> 06:44:05,720
action, we've put an underscore here, because we don't really care about this info value. So

5403
06:44:05,720 --> 06:44:09,400
I'm not going to store it, but we do care about what the next state will be the reward from that

5404
06:44:09,400 --> 06:44:14,840
action. And if we were done or not. So we take that action, that's what does this EMB dot step.

5405
06:44:15,480 --> 06:44:21,240
And then what we do is say Q state action, we just update the Q value using the formula that

5406
06:44:21,240 --> 06:44:25,480
we've talked about. So this is the formula, you can look at it more in depth if you want.

5407
06:44:25,480 --> 06:44:29,080
But based on whatever the reward is, you know, that's how we're going to update those Q values.

5408
06:44:29,080 --> 06:44:34,600
And after a lot of training, we should have some decent Q values in there. Alright, so then we

5409
06:44:34,600 --> 06:44:38,680
set the current state to be the next state. So that when we run this time step again,

5410
06:44:39,320 --> 06:44:43,400
now our agent is in the next state, and can start exploring the environment again,

5411
06:44:44,040 --> 06:44:49,000
in this current, you know, iteration, almost, if that makes sense. So then we say if done,

5412
06:44:49,000 --> 06:44:53,640
so essentially, if the agent died, or if they lost or whatever it was, we're going to append

5413
06:44:53,640 --> 06:45:00,600
whatever reward they got from their last step into the rewards up here. And it's worthy of

5414
06:45:00,600 --> 06:45:06,200
noting that the way the rewards work here is you get one reward, if you move to a valid block,

5415
06:45:06,200 --> 06:45:11,000
and you get zero reward, if you die. So every time we move to a valid spot, we get one,

5416
06:45:11,000 --> 06:45:15,640
otherwise we get zero. I'm pretty sure that's the way it works at least. But that's something

5417
06:45:15,640 --> 06:45:20,120
that's important to know. So then what we're going to do is reduce the epsilon if we die,

5418
06:45:20,120 --> 06:45:25,400
but just a fraction of an amount, you know, 0.001, just so we slowly start decreasing the epsilon

5419
06:45:25,400 --> 06:45:29,560
moving in the correct direction. And then we're going to break because we've reached the goals,

5420
06:45:29,560 --> 06:45:33,320
print the Q table, and then print the average reward. Now this takes a second to train,

5421
06:45:34,120 --> 06:45:39,160
like, you know, a few seconds, really. That one is pretty fast, because I've set this at

5422
06:45:39,160 --> 06:45:43,880
was it 1500. But if you want, you can set this at say 10,000, wait another, you know,

5423
06:45:43,880 --> 06:45:48,840
few minutes or whatever, and then see how much better you can do. So we can see that after that,

5424
06:45:48,920 --> 06:45:56,040
I received an average reward of 0.28886667. This is actually what the Q table values look like.

5425
06:45:56,040 --> 06:46:00,040
So all these decimal values after all these updates, I just decided to print them out.

5426
06:46:00,040 --> 06:46:04,040
And I just want to show you the average reward so that we can compare that to what we can get

5427
06:46:04,040 --> 06:46:08,120
from testing or this graph. So now I'm just going to graph this. And we're going to see this is

5428
06:46:08,120 --> 06:46:11,160
what the graph so you don't have to really understand this code if you don't want to. But

5429
06:46:11,160 --> 06:46:17,480
this is just graphing the average reward over 100 steps from the beginning to the end. So

5430
06:46:17,480 --> 06:46:22,360
essentially, I've been, I've calculated the average of every 100 episodes, and then just

5431
06:46:22,360 --> 06:46:26,840
graph this on here. We can see that we start off very poorly in terms of reward, because the

5432
06:46:26,840 --> 06:46:31,480
epsilon value is quite high, which means that we're taking, you know, random actions pretty

5433
06:46:31,480 --> 06:46:35,080
much all the time. So if we're taking a bunch of random actions, obviously, chances are,

5434
06:46:35,080 --> 06:46:38,920
we're probably going to die a lot, we're probably going to get rewards of zeros quite frequently.

5435
06:46:38,920 --> 06:46:43,560
And then after we get to about 600 episodes, you can see that six actually represents 600,

5436
06:46:43,640 --> 06:46:47,720
because this is in hundreds, we start to slowly increase. And then actually, we go on a crazy

5437
06:46:47,720 --> 06:46:53,960
increase here, when we start to take values more frequently. So the epsilon is increasing,

5438
06:46:53,960 --> 06:46:59,000
right. And then after we get here, we kind of level off. And this does show a slight decline.

5439
06:46:59,000 --> 06:47:02,840
But I guarantee you if we ran this for, you know, like 15,000, it would just go up and down and

5440
06:47:02,840 --> 06:47:07,320
bob up and down. And that's just because even though we have increased the epsilon, there is

5441
06:47:07,320 --> 06:47:11,880
still a chance that we take a random action and you know, gets your reward. So that is pretty

5442
06:47:11,880 --> 06:47:16,280
much it for this Q learning example. You know, I mean, that's pretty straightforward

5443
06:47:16,840 --> 06:47:21,560
to use the Q table. If you actually wanted to say, you know, watch the agent move around the

5444
06:47:21,560 --> 06:47:26,040
thing, I'm going to leave that to you guys, because if you can follow what I've just done in here

5445
06:47:26,040 --> 06:47:30,600
and understand this, it's actually quite easy to use the Q table. And I think as like a final,

5446
06:47:30,600 --> 06:47:35,720
almost like, you know, trust in you guys, you can figure out how to do that. The hint is essentially

5447
06:47:35,720 --> 06:47:40,600
do exactly what I've done in here, except don't update the Q table values, just use the Q table

5448
06:47:40,600 --> 06:47:45,800
values already. And that's, you know, pretty much all there is to Q learning. So this has

5449
06:47:45,800 --> 06:47:50,680
been the reinforcement learning module for this TensorFlow course, which actually is the last

5450
06:47:50,680 --> 06:47:54,760
module in this series. Now, I hope you guys have enjoyed up until this point, just an emphasis

5451
06:47:54,760 --> 06:47:59,720
again, this was really just an introduction to reinforcement learning. This technique and this

5452
06:47:59,720 --> 06:48:04,440
problem itself is not very interesting and not, you know, the best way to do things is not the

5453
06:48:04,440 --> 06:48:08,440
most powerful. It's just to get you thinking about how reinforcement learning works. And

5454
06:48:08,520 --> 06:48:12,280
potentially, if you'd like to look into that more, there's a ton of different resources and,

5455
06:48:12,280 --> 06:48:15,720
you know, things you can look at in terms of reinforcement learning. So that being said,

5456
06:48:15,720 --> 06:48:18,840
that has been this module. And now we're going to move into the conclusion, we'll talk about

5457
06:48:18,840 --> 06:48:22,920
some next steps and some more things that you guys can look at to improve your machine learning skills.

5458
06:48:26,280 --> 06:48:32,280
So finally, after about seven hours of course content, we have reached the conclusion of this

5459
06:48:32,280 --> 06:48:37,480
course. Now what I'm going to do in this last brief short section is just explain to you where

5460
06:48:37,560 --> 06:48:42,440
you can go for some next steps and some further learning with TensorFlow and machine learning

5461
06:48:42,440 --> 06:48:46,520
artificial intelligence in general. Now what I'm going to be recommending to you guys is that we

5462
06:48:46,520 --> 06:48:51,560
look at the TensorFlow website, because they have some amazing guides and resources on here. And in

5463
06:48:51,560 --> 06:48:57,320
fact, a lot of the examples that we used in our notebooks were based off of or exactly the same

5464
06:48:57,320 --> 06:49:01,960
as the original TensorFlow guide. And that's because the code that they have is just very good.

5465
06:49:01,960 --> 06:49:07,080
They're very good and easy to understand examples. And in terms of learning, I find that these

5466
06:49:07,080 --> 06:49:11,240
guides are great for people that want to get in quickly, see the examples and then go and do some

5467
06:49:11,240 --> 06:49:16,520
research on their own time and understand why they work. So if you're looking for some further steps,

5468
06:49:16,520 --> 06:49:21,720
at this point in time, you have gained a very general and broad knowledge of machine learning

5469
06:49:21,720 --> 06:49:27,240
and AI, you have some basic skills in a lot of the different areas. And hopefully this has

5470
06:49:27,240 --> 06:49:33,160
introduced you to a bunch of different concepts and the possibilities of what you are able to do

5471
06:49:33,160 --> 06:49:38,360
using modules like TensorFlow. Now what I'm going to suggest to all of you is that if you find a

5472
06:49:38,360 --> 06:49:43,640
specific area of machine learning AI that you are very interested in, that you would dial in on

5473
06:49:43,640 --> 06:49:48,840
that area and focus most of your time into learning that, that is because when you get to a point in

5474
06:49:48,840 --> 06:49:53,320
machine learning and AI, where you really get specific and pick one kind of strain or one

5475
06:49:53,320 --> 06:49:57,960
kind of area, it gets very interesting very quickly. And you can devote most of your time

5476
06:49:57,960 --> 06:50:02,120
to getting as deep as possible and not specific topic. And that's something that's really cool.

5477
06:50:02,120 --> 06:50:07,320
And most people that are experts in AI or machine learning field typically have one area of

5478
06:50:07,320 --> 06:50:11,720
specialization. Now, if you're someone who doesn't care to specialize an area or you just want to

5479
06:50:11,720 --> 06:50:16,360
play around and see some different things, the TensorFlow website is great to really get kind

5480
06:50:16,360 --> 06:50:21,160
of a general introduction to a lot of different areas and be able to kind of use this code tweak

5481
06:50:21,160 --> 06:50:25,480
it a little bit on your own, and implement it into your own projects. And in fact, the next kind

5482
06:50:25,480 --> 06:50:30,280
of steps and resources I'm going to be showing you here, and involve simply going to the TensorFlow

5483
06:50:30,360 --> 06:50:34,840
website, going to the tutorial page, this is very easy to find, I don't even need to link it,

5484
06:50:34,840 --> 06:50:39,160
you can just search TensorFlow, and you'll find this online. And looking at some more advanced

5485
06:50:39,160 --> 06:50:44,840
topics that we haven't covered. So we've covered a few of the topics and tutorials that are here,

5486
06:50:44,840 --> 06:50:49,240
I've just kind of modified their version, and thrown out in the notebook and explained it in

5487
06:50:49,240 --> 06:50:53,800
wars and video content. But if you'd like to move on to say a next step or something very cool,

5488
06:50:53,800 --> 06:50:59,320
something I would recommend is doing the deep dream in the generic generative neural network

5489
06:50:59,320 --> 06:51:03,560
section on the TensorFlow website, being able to make something like this, I think is very cool.

5490
06:51:03,560 --> 06:51:09,240
And this is an example where you can tweak this a ton by yourself and get some really cool results.

5491
06:51:09,240 --> 06:51:13,880
So some things like this are definitely next steps, there's tons and tons of guides and tutorials

5492
06:51:13,880 --> 06:51:18,360
on this website, they make it very easy for anyone to get started. And with these guides,

5493
06:51:18,360 --> 06:51:22,200
what I will say is typically what will end up happening is they just give you the code and

5494
06:51:22,200 --> 06:51:27,480
brief explanations of why things work. You should really be researching and looking up some more,

5495
06:51:27,560 --> 06:51:31,960
you know, deep level explanations of why some of these things work as you go through, if you

5496
06:51:31,960 --> 06:51:37,240
want to have a firm and great understanding of why the model performs the way that it does.

5497
06:51:37,240 --> 06:51:42,360
So with that being said, I believe I'm going to wrap up the course now. I know you guys can imagine

5498
06:51:42,360 --> 06:51:47,400
how much work I put into this. So please do leave a like, subscribe to the channel, leave a content,

5499
06:51:47,400 --> 06:51:52,360
show your support. This I believe is the largest open source machine learning course in the world

5500
06:51:52,360 --> 06:51:57,960
that deals completely with TensorFlow and Python. And I hope that this gave you a lot of knowledge.

5501
06:51:57,960 --> 06:52:01,960
So please do give me your feedback down below in the comments. With that being said, again,

5502
06:52:01,960 --> 06:52:07,400
I hope you enjoyed. And I hopefully I will see you again in another tutorial guide or series.

