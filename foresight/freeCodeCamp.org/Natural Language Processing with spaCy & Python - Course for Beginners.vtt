WEBVTT

00:00.000 --> 00:04.560
In this course, you will learn all about natural language processing and how to apply it to

00:04.560 --> 00:07.920
real-world problems using the Spacey Library.

00:07.920 --> 00:12.880
Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.

00:12.880 --> 00:14.960
Hi, and welcome to this video.

00:14.960 --> 00:21.080
My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.

00:21.080 --> 00:24.520
I come to NLP from a humanities perspective.

00:24.520 --> 00:27.520
I have my PhD in medieval history.

00:27.520 --> 00:32.040
But I use Spacey on a regular basis to do all of my NLP needs.

00:32.040 --> 00:36.120
So what you're going to get out of this video over the next few hours is a basic understanding

00:36.120 --> 00:43.600
of what natural language processing is, or NLP, and also how to apply it to domain-specific

00:43.600 --> 00:48.560
problems, or problems that exist within your own area of expertise.

00:48.560 --> 00:54.280
I happen to use this all the time to analyze historical documents or financial documents

00:54.280 --> 00:56.880
for my own personal investments.

00:56.880 --> 01:01.920
Over the next few hours, you're going to learn a lot about NLP, language as a whole,

01:01.920 --> 01:05.120
and most importantly, the Spacey Library.

01:05.120 --> 01:10.320
I like the Spacey Library because it's easy to use and easy to also implement really kind

01:10.320 --> 01:14.840
of general solutions to general problems with the off-the-shelf models that are already

01:14.840 --> 01:16.160
available to you.

01:16.160 --> 01:19.960
I'm going to walk you through, in part one of this video series, how to get the most

01:19.960 --> 01:23.040
out of Spacey with these off-the-shelf features.

01:23.040 --> 01:26.480
In part two, we're going to start tackling some of the features that don't exist in

01:26.480 --> 01:31.760
off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components

01:31.760 --> 01:37.960
in Spacey, to actually solve domain-specific problems in your own area, from the entity

01:37.960 --> 01:44.360
ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects

01:44.360 --> 01:50.440
patterns in a custom Spacey component that doesn't actually exist at the moment.

01:50.440 --> 01:54.560
I'm going to be showing you all that in part two, so that in part three, we can take the

01:54.560 --> 01:59.640
lessons that we learned in part one and part two, and actually apply them to solve a very

01:59.640 --> 02:06.560
kind of common problem that exists in an LP, and that is information extraction from financial

02:06.560 --> 02:07.560
documents.

02:07.560 --> 02:14.160
So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.

02:14.160 --> 02:19.200
If you join me over the next few hours, you will leave this lesson with a good understanding

02:19.200 --> 02:23.680
of Spacey, and also a good understanding of kind of the off-the-shelf components that

02:23.680 --> 02:30.920
are there, and a way to take the off-the-shelf components and apply them to your own domain.

02:30.920 --> 02:34.360
If you also join me in this video and you like it, please let me know in the comments

02:34.360 --> 02:38.960
down below, because I am interested in making a second part to this video that will explore

02:38.960 --> 02:44.200
not only the rules-based aspects of Spacey, but the machine learning-based aspects of

02:44.200 --> 02:45.200
Spacey.

02:45.200 --> 02:49.320
So teaching you how to train your own models to do your own things, such as training a

02:49.320 --> 02:54.320
dependency parser, training a named entity recognizer, things like this, which are not

02:54.320 --> 02:56.400
covered in this video.

02:56.400 --> 03:00.400
Nevertheless, if you join me for this one and you like it, you will find part two much

03:00.400 --> 03:02.480
easier to understand.

03:02.480 --> 03:09.400
So sit back, relax, and let's jump into what NLP is, what kind of things you can do with

03:09.400 --> 03:14.240
NLP, such as information extraction, and what the Spacey library is, and how this course

03:14.240 --> 03:15.680
will be laid out.

03:15.680 --> 03:20.800
If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital

03:20.800 --> 03:24.380
Humanities, which is linked in the description down below.

03:24.380 --> 03:29.440
Even if you're not a digital humanist like me, you will find these Python tutorials useful

03:29.440 --> 03:36.120
because they take Python and make it accessible to students of all levels, specifically those

03:36.120 --> 03:37.320
who are beginners.

03:37.320 --> 03:41.960
I walk you through not only the basics of Python, but also I walk you through step-by-step

03:41.960 --> 03:45.240
some of the more common libraries that you need.

03:45.240 --> 03:50.600
A lot of the channel deals with texts or text-based problems, but other content deals with things

03:50.600 --> 03:56.920
like machine learning and image classification and OCR, all in Python.

03:56.920 --> 04:00.840
So before we begin with Spacey, I think we should spend a little bit of time talking

04:00.840 --> 04:06.440
about what NLP or natural language processing actually is.

04:06.440 --> 04:11.600
Natural language processing is the process by which we try to get a computer system to

04:11.600 --> 04:18.480
understand and parse and extract human language, often times with raw text.

04:18.480 --> 04:21.760
There are a couple different areas of natural language processing.

04:21.760 --> 04:28.080
There's named entity recognition, part of speech tagging, syntactic parsing, text categorization,

04:28.080 --> 04:33.120
also known as text classification, co-reference resolution, machine translation.

04:33.120 --> 04:38.720
Adjacent to NLP is another kind of computational linguistics field called natural language

04:38.720 --> 04:40.980
understanding, or NLU.

04:40.980 --> 04:45.340
This is where we train computer systems to do things like relation extraction, semantic

04:45.340 --> 04:53.540
parsing, question and answering, summarization, sentiment analysis, and paraphrasing.

04:53.540 --> 04:59.400
NLP and NLU are used by a wide array of industries, from finance industry all the way through

04:59.400 --> 05:06.740
to law and academia, with researchers trying to do information extraction from texts.

05:06.740 --> 05:09.660
Within NLP, there's a couple different applications.

05:09.660 --> 05:14.620
The first and probably the most important is information extraction.

05:14.620 --> 05:19.880
This is the process by which we try to get a computer system to extract information that

05:19.880 --> 05:23.880
we find relevant to our own research or needs.

05:23.880 --> 05:28.420
So for example, as we're going to see in part three of this video, when we apply spacey

05:28.420 --> 05:35.280
to the financial sector, a person interested in finances might need NLP to go through and

05:35.280 --> 05:39.180
extract things like company names, stocks, indexes.

05:39.180 --> 05:43.340
Things that are referenced within maybe news articles, from Reuters to New York Times to

05:43.340 --> 05:45.240
Wall Street Journal.

05:45.240 --> 05:49.340
This is an example of using NLP to extract information.

05:49.340 --> 05:55.060
A good way to think about NLP's application in this area is it takes in some unstructured

05:55.060 --> 06:01.940
data, in this case raw text, and extracts structured data from it, or metadata.

06:01.940 --> 06:05.780
So it finds the things that you want it to find and extracts them for you.

06:05.780 --> 06:11.260
Now while there's ways to do this with gazetteers and list matching, using an NLP framework

06:11.260 --> 06:15.620
like spacey, which I'll talk about in just a second, has certain advantages.

06:15.620 --> 06:20.380
The main one being that you can use and leverage things that have been parsed syntactically

06:20.380 --> 06:21.660
or semantically.

06:21.660 --> 06:25.460
So things like the part of speech of a word, things like its dependencies, things like

06:25.460 --> 06:26.940
its co-reference.

06:26.940 --> 06:31.180
These are things that the spacey framework allow for you to do off the shelf and also

06:31.180 --> 06:36.700
train into machine learning models and work into pipelines with rules.

06:36.700 --> 06:40.720
So that's kind of one aspect of NLP and one way it's used.

06:40.720 --> 06:44.380
Another way it's used is to read in data and classify it.

06:44.380 --> 06:49.780
This is known as text categorization and we see that on the left hand side of this image.

06:49.780 --> 06:53.740
Text categorization or text classification, and we conclude in this sentiment analysis

06:53.740 --> 06:59.740
for the most part as well, is a way we take information into a computer system, again unstructured

06:59.780 --> 07:04.300
data, a raw text, and we classify it in some way.

07:04.300 --> 07:10.140
You've actually seen this at work for many decades now with spam detection.

07:10.140 --> 07:12.180
Spam detection is nearly perfect.

07:12.180 --> 07:16.380
It needs to be continually updated, but for the most part it is a solved problem.

07:16.380 --> 07:20.380
The reason why you have emails that automatically go to your spam folder is because there's

07:20.380 --> 07:24.380
a machine learning model that sits on the background of your, on the back end of your

07:24.380 --> 07:25.700
email server.

07:25.700 --> 07:29.780
And what it does is it actually looks at the emails, it sees if it fits the pattern for

07:29.780 --> 07:34.420
what it's seen as spam before, and it assigns it a spam label.

07:34.420 --> 07:37.140
This is known as classification.

07:37.140 --> 07:41.500
This is also used by researchers, especially in the legal industry.

07:41.500 --> 07:46.580
Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.

07:46.580 --> 07:51.780
They don't necessarily have the human time to go through and analyze every single document

07:51.780 --> 07:52.900
verbatim.

07:52.900 --> 07:57.820
It is important to kind of get a quick umbrella sense of the documents without actually having

07:57.820 --> 08:00.700
to go through and read them page by page.

08:00.700 --> 08:06.100
And so what lawyers will oftentimes do is use NLP to do classification and information

08:06.100 --> 08:07.280
extraction.

08:07.280 --> 08:12.020
They will find keywords that are relevant to their case, or they will find documents

08:12.020 --> 08:15.980
that are classified according to the relevant fields of their case.

08:15.980 --> 08:20.860
And that way they can take a million documents and reduce it down to maybe only a handful,

08:20.860 --> 08:24.180
maybe a thousand that they have to read verbatim.

08:24.180 --> 08:29.420
This is a real world application of NLP or natural language processing, and both of these

08:29.420 --> 08:33.540
tasks can be achieved through the SPACI framework.

08:33.540 --> 08:36.460
SPACI is a framework for doing NLP.

08:36.460 --> 08:41.500
Right now, as of 2021, it's only available, I believe, in Python.

08:41.500 --> 08:45.180
I think there is a community that's working on an application with R, but I don't know

08:45.180 --> 08:46.620
that for certain.

08:46.620 --> 08:51.740
But SPACI is one of many NLP frameworks that Python has available.

08:51.740 --> 08:55.340
If you're interested in looking at all of them, you can explore things like NLDK, the

08:55.340 --> 08:59.740
Natural Language Toolkit, Stanza, which I believe is coming out of the same program

08:59.740 --> 09:01.540
at Stanford.

09:01.540 --> 09:05.700
There's many out there, but I find SPACI to be the best of all of them for a couple

09:05.700 --> 09:07.180
different reasons.

09:07.180 --> 09:11.860
Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they

09:11.860 --> 09:16.700
perform very quickly, and they also have very good accuracy metrics, such as precision,

09:16.700 --> 09:17.700
recall, and f-score.

09:17.700 --> 09:21.500
And I'm not going to talk too much about the way we measure machine learning accuracy

09:21.500 --> 09:24.500
right now, but know that they are quite good.

09:24.500 --> 09:29.740
Second, SPACI has the ability to leverage current natural language processing methods,

09:29.740 --> 09:35.100
specifically, transformer models, also known usually kind of collectively as BERT models,

09:35.100 --> 09:37.820
even though that's not entirely accurate.

09:37.820 --> 09:41.780
And it allows for you to use an off-the-shelf transformer model.

09:41.780 --> 09:47.820
And third, it provides the framework for doing custom training relatively easily compared

09:47.820 --> 09:50.820
to these other NLP frameworks that are out there.

09:50.820 --> 09:56.340
Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales

09:56.340 --> 09:57.340
well.

09:57.340 --> 10:02.860
SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale.

10:02.860 --> 10:09.100
By at scale, we mean working with large quantities of documents efficiently, effectively, and

10:09.100 --> 10:10.620
accurately.

10:10.620 --> 10:15.100
SPACI scales well because it can process hundreds of thousands of documents with relative ease

10:15.100 --> 10:20.220
in a relative short period of time, especially if you stick with more rules-based pipes,

10:20.220 --> 10:24.100
which we're going to talk about in part two of this video.

10:24.100 --> 10:29.020
So those are the two things you really need to know about NLP and SPACI in general.

10:29.020 --> 10:33.580
We're going to talk about SPACI in-depth as we explore it both through this video and

10:33.580 --> 10:41.460
in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,

10:41.460 --> 10:44.900
and it should be linked in the description down below.

10:44.900 --> 10:48.460
This video and the textbook are meant to work in tandem.

10:48.460 --> 10:52.100
Some stuff that I cover in the video might not necessarily be in the textbook because

10:52.100 --> 10:57.140
it doesn't lend itself well to text representation, and the same goes for the opposite.

10:57.140 --> 11:01.580
Some stuff that I don't have the time to cover verbatim in this video I cover in a

11:01.580 --> 11:04.980
little bit more depth in the book.

11:04.980 --> 11:07.140
I think that you should try to use both of these.

11:07.140 --> 11:11.580
What I would recommend is doing one pass through this whole video, watch it in its entirety,

11:11.580 --> 11:15.420
and get an umbrella sense of everything that SPACI can do and everything that we're going

11:15.420 --> 11:16.580
to cover.

11:16.580 --> 11:22.100
I would then go back and try to replicate each stage of this process on a separate window

11:22.100 --> 11:26.380
or on a separate screen and try to kind of follow along in code, and then I would go

11:26.380 --> 11:30.660
back through a third time and try to watch the first part where I talk about what we're

11:30.660 --> 11:34.540
going to be doing and try to do it on your own without looking at the textbook or the

11:34.540 --> 11:35.540
video.

11:35.540 --> 11:39.820
If you can do that by your third pass, you'll be in very good shape to start using SPACI

11:39.820 --> 11:42.980
to solve your own domain specific problems.

11:42.980 --> 11:49.700
NLP is a complex field, and applying NLP is really complex, but fortunately frameworks

11:49.700 --> 11:54.300
like SPACI make this project and this process a lot easier.

11:54.300 --> 11:58.420
I encourage you to spend a few hours in this video, get to know SPACI, and I think you're

11:58.420 --> 12:02.740
going to find that you can do things that you didn't think possible in relative short

12:02.740 --> 12:03.900
order.

12:03.900 --> 12:08.180
So sit back, relax, and enjoy this video series on SPACI.

12:08.180 --> 12:12.820
In order to use SPACI, you're first going to have to install SPACI.

12:12.820 --> 12:17.340
Now there's a few different ways to do this depending on your environment and your operating

12:17.340 --> 12:18.340
system.

12:18.340 --> 12:25.700
I recommend going to SPACI.io backslash usage and kind of enter in the correct framework

12:25.700 --> 12:26.700
that you're working with.

12:26.700 --> 12:31.620
So if you're using Mac OS versus Windows versus Linux, you can go through and in this very

12:31.620 --> 12:36.360
handy kind of user interface, you can go through and select the different features that matter

12:36.360 --> 12:37.980
most to you.

12:37.980 --> 12:42.980
I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be

12:42.980 --> 12:47.340
doing everything on the CPU and I'm going to be working with English.

12:47.340 --> 12:50.940
So I've established all of those different parameters, and it goes through and it tells

12:50.940 --> 12:56.780
me exactly how to go through and install it using PIP in the terminal.

12:56.780 --> 13:02.020
So I encourage you to go through pause the video right now, go ahead and install Windows

13:02.020 --> 13:06.620
however you want to, I'm going to be walking through how to install it within the Jupyter

13:06.620 --> 13:09.700
notebook that we're going to be moving to in just a second.

13:09.700 --> 13:13.660
I want you to not work with the GPU at all.

13:13.660 --> 13:17.900
Working with Spacey on the GPU requires a lot more understanding about what the GPU

13:17.900 --> 13:21.660
is used for, specifically in training machine learning models.

13:21.660 --> 13:24.420
It requires you to have CUDA installed correctly.

13:24.420 --> 13:28.860
It requires a couple other things that I don't really have the time to get into in this video,

13:28.860 --> 13:33.900
but we'll be addressing in a more advanced Spacey tutorial video.

13:33.900 --> 13:39.540
So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,

13:39.540 --> 13:43.300
and then selecting CPU and since you're going to be working through this video with English

13:43.340 --> 13:49.980
texts, I encourage you to select English right now and go ahead and just install or download

13:49.980 --> 13:52.700
the Ncore Web SM model.

13:52.700 --> 13:53.700
This is the small model.

13:53.700 --> 13:56.020
I'll talk about that in just a second.

13:56.020 --> 14:02.660
So the first thing we're going to do in our Jupyter notebook is we are going to be using

14:02.660 --> 14:07.100
the exclamation mark to delineate in the cell that this is a terminal command.

14:07.100 --> 14:10.180
We're going to say PIP install Spacey.

14:10.180 --> 14:14.140
Your output when you execute this cell is going to look a little different than mine.

14:14.140 --> 14:17.260
I already have Spacey installed in this environment.

14:17.260 --> 14:20.940
And so mine kind of goes through and looks like this yours will actually go through.

14:20.940 --> 14:25.820
And instead of saying requirement already satisfied, it'll be actually passing out the the different

14:25.820 --> 14:31.420
things that it's actually installing to install Spacey and all of its dependencies.

14:31.420 --> 14:36.060
The next thing that you're going to do is you're going to again, you follow the instructions

14:36.140 --> 14:42.540
and you're going to be doing Python dash M space Spacey space download and then the model

14:42.540 --> 14:44.180
that you want to download.

14:44.180 --> 14:46.220
So let's go ahead and do that right now.

14:46.220 --> 14:52.060
So let's go ahead and say Python M Spacey download.

14:52.060 --> 14:54.540
So this is a Spacey terminal command.

14:54.540 --> 14:57.420
And we're going to download the Ncore Web SM.

14:57.420 --> 15:00.060
And again, I already have this model downloaded.

15:00.060 --> 15:05.460
So on my end, Spacey is going to look a little differently than as it's going to look on your

15:05.460 --> 15:09.100
end as it prints off on the Jupyter notebook.

15:09.100 --> 15:13.340
And if we give it a just a second, everything will go through and it says that it's collected

15:13.340 --> 15:17.900
it, it's downloading it and we are all very happy now.

15:17.900 --> 15:23.060
And so now that we've got Spacey installed correctly, and that we've got the small model

15:23.060 --> 15:28.100
downloaded correctly, we can go ahead and start actually using Spacey and make sure

15:28.100 --> 15:29.380
everything's correct.

15:29.380 --> 15:33.700
The first thing we're going to do is we're going to import the Spacey library as you

15:33.820 --> 15:35.820
would with any other Python library.

15:35.820 --> 15:41.500
If you're not familiar with this, a library is simply a set of classes and functions that

15:41.500 --> 15:46.260
you can import into a Python script so that you don't have to write a whole bunch of extra

15:46.260 --> 15:47.460
code.

15:47.460 --> 15:52.220
Libraries are massive collections of classes and functions that you can call.

15:52.220 --> 15:57.380
So when we import Spacey, we're importing the whole library of Spacey.

15:57.380 --> 16:01.860
And now that we've seen something like this, we know that Spacey has imported correctly,

16:01.860 --> 16:07.540
as long as you're not getting an error message, everything was imported fine.

16:07.540 --> 16:13.460
The next thing that we need to do is we want to make sure that our English Core Web SM,

16:13.460 --> 16:16.740
our small English model, was downloaded correctly.

16:16.740 --> 16:20.580
So the next thing that we need to do is we need to create an NLP object.

16:20.580 --> 16:24.940
I'm going to be talking a lot more about this as we move forward right now.

16:24.940 --> 16:29.420
This is just troubleshooting to make sure that we've installed Spacey correctly and

16:29.420 --> 16:32.140
we've downloaded our model correctly.

16:32.140 --> 16:35.540
So we're going to use the spacey.load command.

16:35.540 --> 16:37.460
This is going to take one argument.

16:37.460 --> 16:42.700
It's going to be a string that is going to correspond to the model that you've installed.

16:42.700 --> 16:47.820
In this case, N Core Web SM.

16:47.820 --> 16:54.100
And if you execute this cell and you have no errors, you have successfully installed

16:54.100 --> 16:59.980
Spacey correctly and you've downloaded the English Core Web SM model correctly.

16:59.980 --> 17:05.340
So go ahead, take time and get all this stuff set up, pause the video if you need to and

17:05.340 --> 17:11.180
then pop back and we're going to start actually working through the basics of Spacey.

17:11.180 --> 17:16.460
I'm now going to move into kind of an overview of kind of what's within Spacey, why it's

17:16.460 --> 17:20.940
useful and kind of some of the basic features of it that you need to be familiar with.

17:20.940 --> 17:25.420
And I'm going to be working from the Jupyter Notebook that I talked about in the introduction

17:25.420 --> 17:26.420
to this video.

17:26.420 --> 17:31.820
If we scroll down to the bottom of chapter one, the basics of Spacey and you get past

17:31.820 --> 17:35.500
the install section, you get to this section on containers.

17:35.500 --> 17:36.780
So what are containers?

17:36.780 --> 17:42.780
Well, containers within Spacey are objects that contain a large quantity of data about

17:42.780 --> 17:44.300
a text.

17:44.300 --> 17:47.780
There are several different containers that you can work with in Spacey.

17:47.780 --> 17:53.620
There's the doc, the doc bin, example, language, lexeme, span, span group and token.

17:53.620 --> 17:57.420
We're going to be dealing with the lexeme a little bit in this video series and we're

17:57.420 --> 18:01.580
going to be dealing with the language container a little bit in this video series, but really

18:01.580 --> 18:05.500
the three big things that we're going to be talking about again and again is the doc,

18:05.500 --> 18:08.420
the span and the token.

18:08.420 --> 18:12.580
And I think when you first come to Spacey, there's a little bit of a learning curve about

18:12.580 --> 18:17.260
what these things are, what they do, how they are structured hierarchically.

18:17.420 --> 18:23.140
And for that reason, I've created this, in my opinion, kind of easy to understand image

18:23.140 --> 18:25.420
of what different containers are.

18:25.420 --> 18:30.580
So if you think about what Spacey is as a pyramid, so a hierarchical system, we've

18:30.580 --> 18:36.340
got all these different containers structured around really the doc object.

18:36.340 --> 18:42.300
Your doc container or your doc object contains a whole bunch of metadata about the text

18:42.300 --> 18:46.980
that you pass to the Spacey pipeline, which we're going to see in practice.

18:46.980 --> 18:51.500
In just a few minutes, the doc object contains a bunch of different things.

18:51.500 --> 18:53.660
It contains attributes.

18:53.660 --> 18:57.100
These attributes can be things like sentences.

18:57.100 --> 19:02.140
So if you iterate over doc.sense, you can actually access all the different sentences

19:02.140 --> 19:05.100
found within that doc object.

19:05.100 --> 19:10.940
If you iterate over each individual item or index in your doc object, you can get individual

19:10.940 --> 19:12.380
tokens.

19:12.380 --> 19:16.540
Tokens are going to be things like words or punctuation marks.

19:16.580 --> 19:23.020
Anything within your sentence or text that has a self contained important value, either

19:23.020 --> 19:25.660
syntactically or semantically.

19:25.660 --> 19:30.340
So this is going to be things like words, a comma, a period, a semi colon, a quotation

19:30.340 --> 19:34.140
mark, things like this, these are all going to be your tokens.

19:34.140 --> 19:39.900
And we're going to see how tokens are a little different than just splitting words up with

19:39.900 --> 19:43.900
traditional string methods and Python.

19:43.900 --> 19:47.580
The next thing that you should be kind of familiar with are spans.

19:47.580 --> 19:54.380
So spans are important because they kind of exist within and without of the doc object.

19:54.380 --> 20:01.220
So unlike the token, which is an index of the doc object, a span can be a token itself,

20:01.220 --> 20:04.740
but it can also be a sequence of multiple tokens.

20:04.740 --> 20:06.260
We're going to see that at play.

20:06.260 --> 20:15.260
So imagine if you had a span in its category, maybe group one are our places.

20:15.260 --> 20:18.740
So a single token might be like a city like Berlin.

20:18.740 --> 20:23.220
But span group two, this could be something like full proper names.

20:23.220 --> 20:27.500
So of a people, for example, so this could be like as we're going to see Martin Luther

20:27.500 --> 20:28.500
King.

20:28.500 --> 20:33.260
This would be a sequence of tokens, a sequence of three different items in the sentence that

20:33.260 --> 20:37.180
make up one span or one self contained item.

20:37.180 --> 20:45.420
So Martin Luther King would be a person who's a collection of a sequence of individual tokens.

20:45.420 --> 20:50.620
If that doesn't make sense right now, this image will be reinforced as we go through

20:50.620 --> 20:54.740
and learn more about spacey in practice.

20:54.740 --> 21:01.500
For right now, I want you to be just understanding that the doc object is the thing around which

21:01.500 --> 21:03.420
all of spacey sits.

21:03.420 --> 21:06.280
This is going to be the object that you create.

21:06.280 --> 21:10.740
This is going to be the object that contains all the metadata that you need to access.

21:10.740 --> 21:16.740
And this is going to be the object that you try to essentially improve with different

21:16.740 --> 21:22.260
custom components, factories and pipelines as you go through and do more advanced things

21:22.260 --> 21:23.260
with spacey.

21:23.260 --> 21:29.260
We're going to now see in just a few seconds how that doc object is kind of similar to the

21:29.260 --> 21:35.860
text itself, but how it's very, very different and much more powerful.

21:35.860 --> 21:39.420
We're now going to be moving on to chapter two of this textbook, which is going to deal

21:39.420 --> 21:44.380
with kind of getting used to the in depth features of spacey.

21:44.380 --> 21:49.220
If you want to pause the video or keep this notebook or this book open up kind of separate

21:49.220 --> 21:53.980
from this video and follow along as we go through and explore it in live coding.

21:53.980 --> 21:57.420
We're going to be talking about a few different things as we explore chapter two.

21:57.780 --> 22:00.180
This will be a lot longer than chapter one.

22:00.180 --> 22:04.380
We're going to be not only importing spacey, but actually going through and loading up

22:04.380 --> 22:09.300
a model, creating a doc object around that model so that we're going to work with the

22:09.300 --> 22:11.180
doc container and practice.

22:11.180 --> 22:15.500
And then we're going to see how that doc container stores a lot of different features

22:15.500 --> 22:18.660
or metadata or attributes about the text.

22:18.660 --> 22:23.020
And while they look the same on the surface, they're actually quite different.

22:23.020 --> 22:28.700
So let's go ahead and work within our same Jupiter notebook where we've imported spacey

22:28.700 --> 22:31.940
and we have already created the NLP object.

22:31.940 --> 22:36.980
The first thing that I want to do is I want to open up a text to start working with within

22:36.980 --> 22:39.220
this repo.

22:39.220 --> 22:42.580
We've got a data folder within this data sub folder.

22:42.580 --> 22:45.260
I've got a couple of different Wikipedia openings.

22:45.260 --> 22:49.420
I've got one on MLK that we're going to be using a little later in this video and I have

22:49.420 --> 22:51.500
one on the United States.

22:51.500 --> 22:54.180
This is wiki underscore us.

22:54.180 --> 22:56.220
That's going to be what we work with right now.

22:56.220 --> 23:04.260
So let's use our with operator and open up data backslash wiki underscore us dot txt.

23:04.260 --> 23:09.300
We're going to just read that in as F and then we're going to create this text object,

23:09.300 --> 23:12.220
which is going to be equal to F dot read.

23:12.220 --> 23:16.260
And now that we've got our text object created, let's go ahead and see what this looks like.

23:16.260 --> 23:18.780
So let's print text.

23:18.780 --> 23:22.980
Then we see that it's a standard Wikipedia article kind of follows that same introductory

23:22.980 --> 23:28.420
format and it's about four or five paragraphs long with a lot of the features left in such

23:28.420 --> 23:31.900
as the brackets that delineate some kind of a footnote.

23:31.900 --> 23:35.300
We're not going to worry too much about cleaning this up right now because we're interested

23:35.300 --> 23:41.460
not with cleaning our data so much as just starting to work with the doc object in spacey.

23:41.460 --> 23:46.220
So the first thing that you want to do is you're going to want to create a doc object.

23:46.220 --> 23:50.260
It is oftentimes good practice if you're only ever working with one doc object in your

23:50.260 --> 23:54.540
script to just call your only object doc.

23:54.540 --> 23:59.620
If you're working with multiple objects, sometimes you'll say doc one doc two doc three

23:59.620 --> 24:05.100
or give it some kind of specific name so that your variables can be unique and easily identifiable

24:05.100 --> 24:06.600
later in your script.

24:06.600 --> 24:10.820
Since we're just working with one doc object right now, we're going to say doc is equal

24:10.820 --> 24:12.900
to NLP.

24:12.900 --> 24:18.660
So this is going to call our NLP model that we imported earlier in this case the English

24:18.660 --> 24:21.460
Core Web SM model.

24:21.460 --> 24:26.100
And that's going to for right now just take one argument and that's going to be the text

24:26.100 --> 24:27.100
itself.

24:27.100 --> 24:33.960
So the text object, if you execute that cell, you should have a doc object now created.

24:33.960 --> 24:38.000
Let's print off that doc object and see what it looks like.

24:38.000 --> 24:44.000
And if you scroll down, you might be thinking to yourself, this looks very, very similar

24:44.000 --> 24:47.760
if not identical to what I just saw a second ago.

24:47.760 --> 24:53.760
And in fact, on the surface, it is very similar to that text object that we gave to the NLP

24:53.760 --> 24:57.400
model or pipeline, but let's see how they're different.

24:57.400 --> 25:01.040
Let's print off the length of text.

25:01.040 --> 25:05.520
And let's print off the length of the doc object.

25:05.520 --> 25:08.160
And what we have here are two different numbers.

25:08.160 --> 25:15.240
Our text is 3525 and our doc object is 152.

25:15.240 --> 25:16.600
What is going on here?

25:16.600 --> 25:22.440
Well, let's get a sense by trying to iterate over the text object and iterating over the

25:22.440 --> 25:24.920
doc object with a simple for loop.

25:24.920 --> 25:28.800
So we're going to say for token and text, so we're going to iterate first over that

25:28.800 --> 25:32.640
text object, we're going to print off the token.

25:32.640 --> 25:37.280
So the first 10 indices, and we get individual letters as one might expect.

25:37.280 --> 25:41.480
But when we do something the same thing with the doc object, let's go ahead and start writing

25:41.480 --> 25:42.480
this out.

25:42.480 --> 25:49.000
We're going to say for token and doc, we're going to iterate over the first 10.

25:49.000 --> 25:51.120
We're going to print off the token.

25:51.120 --> 25:53.120
We see something very different.

25:53.120 --> 25:56.160
What we see here are tokens.

25:56.160 --> 26:01.880
This is why the doc object is so much more valuable and this is why the doc object has

26:01.880 --> 26:04.600
a different length than the text object.

26:04.600 --> 26:09.560
The text object is just basically counting up every instance of a character, a white

26:09.560 --> 26:11.700
space, a punctuation, etc.

26:11.700 --> 26:18.280
The doc object is counting individual tokens, so any word, any punctuation, etc.

26:18.280 --> 26:21.160
That's why they're of different length and that's why when we print them off, we see

26:21.160 --> 26:22.160
something different.

26:22.160 --> 26:25.920
So you might now already be seeing the power of spacey.

26:25.920 --> 26:30.040
It allows for you to easily on the surface with nothing else being done, easily split

26:30.040 --> 26:33.720
up your text into individual tokens without any effort at all.

26:33.720 --> 26:38.480
Now, those of you familiar with Python and different string methods might be thinking

26:38.480 --> 26:40.840
to yourself, but I've got the split method.

26:40.840 --> 26:43.560
I can just use this to split up the text.

26:43.560 --> 26:45.960
I don't need anything fancy from spacey.

26:45.960 --> 26:47.560
Well, you'd be wrong.

26:47.560 --> 26:49.320
Let me demonstrate this right now.

26:49.320 --> 26:56.880
So if I were to say for token and text.split, so I'm splitting up that text into individual

26:56.880 --> 27:01.720
and theory individual words, essentially, it's just a split method where it's splitting

27:01.720 --> 27:03.360
by individual white spaces.

27:03.360 --> 27:08.160
If I were to do that and iterate over the first 10 again.

27:08.160 --> 27:13.560
And I would just say print token, it looks good until you get down here.

27:13.560 --> 27:18.160
So until you get to USA, well, why is it a problem?

27:18.160 --> 27:20.560
The problem is quite simple.

27:20.560 --> 27:23.760
There is a parentheses mark right here.

27:23.760 --> 27:30.120
And this is where we have a huge advantage with spacey.

27:30.120 --> 27:35.920
Spacey automatically separates out these these kind of punctuation marks and removes them

27:35.920 --> 27:40.200
from individual tokens when they're not relevant to the token itself.

27:40.200 --> 27:45.160
Notice that USA has got a period within the middle of it.

27:45.160 --> 27:49.520
It's not looking at that and thinking that that is some kind of unique token, a you a

27:49.520 --> 27:53.200
period and s a period and an a in a period.

27:53.200 --> 27:57.400
It's not seeing these as four individual tokens rather it's automatically identifying them

27:57.400 --> 28:04.640
as one thing one tied together single token that's a string of characters and punctuation.

28:04.640 --> 28:09.640
This is where the power of spacey really lies just on the surface level and go ahead spend

28:09.640 --> 28:11.760
a few minutes and play around with this.

28:11.760 --> 28:16.360
And then we're going to kind of jump back here and start talking about how the doc object

28:16.360 --> 28:19.720
has a lot more than just tokens within it.

28:19.720 --> 28:22.600
It's got sentences each token has attributes.

28:22.600 --> 28:30.480
We're going to start exploring these when you pop back.

28:30.480 --> 28:34.560
If you're following along with the textbook, we're now going to be moving on to the next

28:34.560 --> 28:37.880
section, which is sentence boundary detection.

28:37.880 --> 28:45.360
An NLP sentence boundary detection is the identification of sentences within a text on the surface.

28:45.360 --> 28:46.600
This might look simple.

28:46.600 --> 28:51.520
You might be thinking to yourself, I could simply use the split function and split up

28:51.520 --> 28:53.800
a text with a simple period.

28:53.800 --> 28:55.920
And that's going to give me all my sentences.

28:55.920 --> 29:00.200
Those of you who have tried to do this might already be shaking your heads and saying no.

29:00.200 --> 29:04.560
If you do think about it, there's a really easy explanation for why this doesn't work.

29:04.560 --> 29:10.560
Were you to try to split up a text by period and make a presumption that anything that occurs

29:10.560 --> 29:14.920
with between periods is going to be an individual sentence, you would have a serious mistake

29:14.960 --> 29:20.560
when you get to things like USA, especially in Western languages, where the punctuation

29:20.560 --> 29:26.840
of a period mark is used not only to delineate the change of its sentence, rather it's used

29:26.840 --> 29:29.920
to also delineate abbreviations.

29:29.920 --> 29:36.000
So United States of America, each period represents an abbreviated word.

29:36.000 --> 29:40.880
So you could write in rules to kind of account for this, you could write in rules that could

29:40.880 --> 29:45.240
also include in other ways that sentences are created, such as question marks, such

29:45.240 --> 29:46.920
as exclamation marks.

29:46.920 --> 29:48.200
But why do that?

29:48.200 --> 29:50.200
That's a lot of effort.

29:50.200 --> 29:56.820
When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly

29:56.820 --> 29:58.320
how that works.

29:58.320 --> 30:06.800
So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing

30:06.800 --> 30:09.880
the sentence attribute of the doc object.

30:09.880 --> 30:13.080
Let's print off scent.

30:13.080 --> 30:17.520
And if you do that, you are now able to print off every individual sentence.

30:17.520 --> 30:21.880
So the entire text has been tokenized at the sentence level.

30:21.880 --> 30:27.760
In other words, spacey has used its sentence boundary detection and done all that for you

30:27.760 --> 30:29.920
and giving you all the sentences.

30:29.920 --> 30:33.600
If you work with different models of different sizes, you're going to notice that certain

30:33.600 --> 30:37.840
models the larger they get tend to do better at sentence detection.

30:37.840 --> 30:41.400
And that's because machine learning models tend to do a little bit better than heuristic

30:41.400 --> 30:42.600
approaches.

30:42.600 --> 30:47.960
The English core web SM model, while having some machine learning components in it, does

30:47.960 --> 30:50.360
not save word vectors.

30:50.360 --> 30:55.200
And so the larger you go with the models, typically the better you're going to have with regards

30:55.200 --> 30:57.440
to sentence detection.

30:57.440 --> 31:00.680
Let's go ahead and try to access one of these sentences.

31:00.680 --> 31:06.200
So let's create an object called sentence one, we're going to make that equal to doc.sense

31:06.200 --> 31:07.200
zero.

31:07.400 --> 31:14.520
We're going to try to grab that zero index and let's print off sentence one, we do this,

31:14.520 --> 31:15.920
we get an error.

31:15.920 --> 31:17.600
Why have we gotten an error?

31:17.600 --> 31:20.120
Well, it tells you why right here, it's a type air.

31:20.120 --> 31:26.720
And this means that this is not a type that can be kind of iterated over, it's not subscriptable.

31:26.720 --> 31:28.880
And it's because it is a generator.

31:28.880 --> 31:32.960
Now in Python, if you're familiar with generators, you might be thinking to yourself, there's

31:32.960 --> 31:33.960
a solution for this.

31:33.960 --> 31:35.400
And in fact, there is.

31:35.440 --> 31:40.040
If you want to work with generator objects, you need to convert them into a list.

31:40.040 --> 31:43.360
So let's say sentence one is equal to list.

31:43.360 --> 31:48.680
So using the list function to convert doc.sense into a list.

31:48.680 --> 31:53.080
And then with outside of that, we're going to grab zero, the zero index, and then we're

31:53.080 --> 31:55.960
going to print off sentence one.

31:55.960 --> 32:00.320
And we grab the first sentence of that text.

32:00.400 --> 32:06.080
This as we go deeper and deeper in spacey one by one, you're going to see the immense

32:06.080 --> 32:10.880
power that you can do with Pacea, all the immense incredible things you can use spacey

32:10.880 --> 32:14.240
for with very, very minimal code.

32:14.240 --> 32:19.160
The doc object does a lot of things for you that would take hours to actually write out

32:19.160 --> 32:22.040
and code to do with heuristic approaches.

32:22.040 --> 32:27.280
This is now a great way to segment an entire text up by sentence.

32:27.280 --> 32:33.520
And if you work with text a lot, you will already know that this has a lot of applications.

32:33.520 --> 32:37.440
As we move forward, we're going to not just talk about sentences, we're also going to

32:37.440 --> 32:43.160
be talking about token attributes, because within the doc object are individual tokens.

32:43.160 --> 32:47.920
I encourage you to pause here and go ahead and play around with the doc.sense a little

32:47.920 --> 32:52.680
bit and get familiar with how it works, what it contains, and try to convert it into a

32:52.680 --> 32:53.680
list.

32:54.680 --> 32:58.920
And we'll continue talking about tokens.

32:58.920 --> 33:02.800
This is where I really encourage you to spend a little bit of time with the textbook.

33:02.800 --> 33:08.600
Under token attributes in chapter two, I have all the different kind of major things that

33:08.600 --> 33:11.280
you're going to be using with regards to token attributes.

33:11.280 --> 33:14.680
We're going to look and see how to access them in just a second.

33:14.680 --> 33:18.640
I've provided for you kind of the most important ones that you should probably be familiar

33:18.640 --> 33:19.640
with.

33:19.640 --> 33:22.880
We're going to see this in code in just a second, and I'm going to explain with a little

33:22.880 --> 33:27.400
bit more detail than what's in the spacey documentation about what these different things

33:27.400 --> 33:30.360
are, why they're useful, and how they're used.

33:30.360 --> 33:36.480
So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.

33:36.480 --> 33:40.120
If you remember, the doc object had a sequence of tokens.

33:40.120 --> 33:43.400
So for token and doc, you could print off token.

33:43.400 --> 33:46.480
And let's just do this with the first 10.

33:46.480 --> 33:49.440
And we've got each individual token.

33:49.440 --> 33:55.880
What you don't see here is that each individual token has a bunch of metadata buried within

33:55.880 --> 33:56.880
it.

33:56.880 --> 34:02.840
These metadata are things that we call attributes or different things about that token that

34:02.840 --> 34:07.160
you can access through the spacey framework.

34:07.160 --> 34:09.680
So let's go ahead and try to do that right now.

34:09.680 --> 34:15.920
Let's just work with for right now token number two, which we're going to call sentence one,

34:15.920 --> 34:18.920
and we're going to grab from sentence one, the second index.

34:19.440 --> 34:21.960
Let's print off that word.

34:21.960 --> 34:23.360
And it should be states.

34:23.360 --> 34:26.000
And in fact, it is fantastic.

34:26.000 --> 34:30.280
So now that we've got the word states accessed, we can start kind of going through and playing

34:30.280 --> 34:33.920
around with some of the attributes that that word actually has.

34:33.920 --> 34:37.240
Now when you print it off, it looks like a regular piece of text, it looks like just

34:37.240 --> 34:42.400
a string, but it's got so much more buried within it now because it's been passed through

34:42.400 --> 34:46.440
our NLP model or pipeline from spacey.

34:46.480 --> 34:50.040
So let's go ahead and say token to dot text.

34:50.040 --> 34:52.000
And I'm going to be saying token to dot text.

34:52.000 --> 34:56.440
If you're working within an IDE like Adam, you're going to need to say print token to

34:56.440 --> 34:57.440
dot text.

34:57.440 --> 35:02.600
When we do this, we see we get a string that just is states.

35:02.600 --> 35:08.960
This is telling us that the dot text of the object, the pure text corresponds to the word

35:08.960 --> 35:09.960
states.

35:09.960 --> 35:14.200
This is really important if you need to extract the text itself from the token and not work

35:14.240 --> 35:18.160
with the token object, which has behind it a whole bunch of different metadata that we're

35:18.160 --> 35:21.000
going to go through now and start accessing.

35:21.000 --> 35:23.480
Let's use the token left edge.

35:23.480 --> 35:27.760
So we can say token to dot left underscore edge.

35:27.760 --> 35:28.760
And we can print that off.

35:28.760 --> 35:30.440
Well, what's that telling us?

35:30.440 --> 35:35.640
It's telling us that this is part of a multi word token or a token that is multiple has

35:35.640 --> 35:39.720
multiple components to make up a larger span.

35:39.720 --> 35:43.800
And that this is the leftmost token that corresponds to it.

35:43.800 --> 35:48.480
So this is going to be the word the as in the United States.

35:48.480 --> 35:51.360
Let's take a look at the right edge.

35:51.360 --> 35:57.080
We can say token to dot right underscore edge, print that off, and we get the word America.

35:57.080 --> 36:02.720
So we're able to see where this token fits within a larger span in this case a noun chunk,

36:02.720 --> 36:04.760
which we're going to explore in just a few minutes.

36:04.760 --> 36:08.600
But we also learn a lot about it, kind of the different components, so we know where

36:08.600 --> 36:12.280
to grab it from the beginning and from the very end.

36:12.280 --> 36:15.720
So that's how the left edge and the right edge work.

36:15.720 --> 36:21.120
We also have within this token to dot int type.

36:21.120 --> 36:23.360
This is going to be the type of entity.

36:23.360 --> 36:25.900
Now what you're seeing here is a integer.

36:25.900 --> 36:27.820
So this is 384.

36:27.820 --> 36:33.160
In order to actually know what 384 means, I encourage you to not really use that so much

36:33.160 --> 36:37.280
as and type with an underscore after it.

36:37.280 --> 36:41.960
This is going to give you the string corresponding to number 384.

36:41.960 --> 36:47.280
In this case, it is GPE or geopolitical entity.

36:47.280 --> 36:50.640
We're going to be working with named entity a little bit in this video, but I have a whole

36:50.640 --> 36:53.560
other book on named entity recognition.

36:53.560 --> 36:59.360
It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning

36:59.360 --> 37:02.160
and rules based in a lot more depth.

37:02.160 --> 37:06.560
Let's go ahead and keep on moving on though and looking at different entity types here

37:06.560 --> 37:07.560
as well.

37:07.560 --> 37:09.840
Not entity types, attribute types.

37:09.840 --> 37:17.920
So we're going to say token to dot int IOB, all lowercase and again underscore at the

37:17.920 --> 37:22.120
end and we get the string here, I.

37:22.120 --> 37:27.040
Now IOB is a specific kind of named entity code.

37:27.040 --> 37:31.400
B would mean that it's the beginning of an entity and I means that it's inside of an

37:31.400 --> 37:36.320
entity and O means that it's outside of an entity.

37:36.320 --> 37:43.040
The fact that we're seeing I here tells us that this word states is inside of a larger

37:43.040 --> 37:44.040
entity.

37:44.040 --> 37:47.480
In fact, we know that because we've seen the left edge and we've seen the right edge.

37:47.480 --> 37:51.280
It's inside of the United States of America.

37:51.280 --> 37:54.600
So it's part of a larger entity at hand.

37:54.600 --> 38:02.080
We can also say token to dot lima and under case again after that and we get the word

38:02.080 --> 38:03.080
states.

38:03.080 --> 38:06.720
Lima form or the root form of the word.

38:06.720 --> 38:09.760
This means that this is what the word looks like with no inflection.

38:09.760 --> 38:14.560
If we were working with a verb, in fact, let's go ahead and do that right now.

38:14.560 --> 38:16.560
Let's grab sentence.

38:16.560 --> 38:22.000
We're going to grab sentence one index 12, which should be the word no and we're going

38:22.000 --> 38:29.840
to print off the lima for the word or sorry, it's a verb and we see the verb lima as no.

38:29.840 --> 38:39.600
So if we were to print off sentence one specifically index 12, we see that its original form is

38:39.600 --> 38:40.780
known.

38:40.780 --> 38:47.640
So the lima form uninflected is the verb no K N O W.

38:47.640 --> 38:51.240
Another thing that we can access and we're going to see that have the power of this later

38:51.240 --> 38:52.240
on.

38:52.240 --> 38:55.680
This might not seem important right now, but I promise you it will be.

38:55.680 --> 39:01.400
Let's print off token that I call this again token to we're going to print that off, but

39:01.400 --> 39:04.320
we're going to print off specifically the morph.

39:04.320 --> 39:05.720
No underscore here.

39:05.720 --> 39:06.720
Just morph.

39:06.720 --> 39:12.320
What you get is what looks like a really weird output a string called noun type equal

39:12.320 --> 39:13.320
to prop.

39:13.320 --> 39:18.600
In fact, this means proper noun, a number which corresponds to sing.

39:18.600 --> 39:24.480
We're going to talk a lot more about morphological analysis later on when we try to find an extract

39:24.480 --> 39:27.320
information from our texts.

39:27.320 --> 39:32.520
But for right now, understand that what you're looking at is the output of kind of what that

39:32.520 --> 39:34.600
word is morphologically.

39:34.600 --> 39:37.640
So in this case, it's a proper noun and it's singular.

39:37.640 --> 39:45.360
If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb

39:45.360 --> 39:46.360
it is.

39:46.360 --> 39:53.040
So it's a perfect past participle known perfect past participle.

39:53.040 --> 39:56.960
For being good at NLP is also being good with language.

39:56.960 --> 40:00.240
So I encourage you to spend time and start getting familiar with those things that you

40:00.240 --> 40:04.540
might have forgotten about from like fifth grade grammar, such as perfect participles

40:04.540 --> 40:06.240
and things like that.

40:06.240 --> 40:10.620
Because when you need to start creating rules to extract information, you're going to find

40:10.620 --> 40:14.920
those pieces of information very important for writing rules.

40:14.920 --> 40:16.920
We'll talk about that in a little bit though.

40:16.920 --> 40:20.240
Let's go back to our other attributes from the token.

40:20.240 --> 40:25.680
So again, let's go to token two, and we're going to grab the POS part of speech, not

40:25.680 --> 40:27.120
what you might be thinking.

40:27.120 --> 40:33.020
So part of speech underscore POS underscore, and we output PROPN.

40:33.020 --> 40:35.760
This means that it is a proper noun.

40:35.760 --> 40:42.640
It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological

40:42.640 --> 40:49.320
detailed extraction, what kind of noun it might be with regards to in this case, singular.

40:49.320 --> 40:52.040
So that's going to be how you extract the part of speech.

40:52.040 --> 40:57.580
And the thing that you can do is you can extract the dependency relation.

40:57.580 --> 41:02.400
So in this case, we can figure out what role it plays in the sentence.

41:02.400 --> 41:05.600
In this case, the noun subject.

41:05.600 --> 41:09.800
And then finally, the last thing I really want to talk about before we move into a more

41:09.800 --> 41:17.800
detailed analysis of part of speech is going to be the token two dot lane.

41:17.800 --> 41:23.280
And what this grabs for you is the language of the doc object in this case, we're working

41:23.280 --> 41:28.680
with something from the English language, so in every language is going to have two

41:28.680 --> 41:30.320
letters that correspond to it.

41:30.320 --> 41:32.540
These are universally recognized.

41:32.540 --> 41:38.880
So that's going to be how you access different kinds of attributes that each token has.

41:38.880 --> 41:42.240
And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that

41:42.240 --> 41:43.680
I haven't covered.

41:43.680 --> 41:48.880
I gave you the ones that are the most important that I find to be used on a regular basis

41:48.880 --> 41:55.400
to solve different problems with regards to information extraction from the text.

41:55.400 --> 41:59.580
So that's going to be where we stop here with token attributes, and we're going to be moving

41:59.580 --> 42:06.200
on to part 2.5 of the book, which is part of speech tagging.

42:06.200 --> 42:12.360
I now want to move into kind of a more detailed analysis of part of speech within spacey and

42:12.360 --> 42:18.640
the dependency parser and how to actually analyze it really nicely either in a notebook

42:18.640 --> 42:20.280
or outside of a notebook.

42:20.280 --> 42:23.120
So let's work with a different text for just a few minutes.

42:23.120 --> 42:24.880
We're going to see why this is important.

42:24.880 --> 42:28.760
It's because I'm working on a zoomed in screen, and to make this sentence a little easier

42:28.760 --> 42:36.120
to understand, we're going to just use Mike and Joy's plain football, a very simple sentence.

42:36.120 --> 42:39.960
And we're going to create a new doc object, and we're going to call this doc two.

42:39.960 --> 42:42.840
That's going to be equal to NLP text.

42:42.840 --> 42:47.440
Let's print off doc two just to make sure that it was created, and in fact that we see

42:47.440 --> 42:48.440
that it was.

42:48.440 --> 42:54.200
Now that we've got it created, let's iterate over the tokens within this and say for token

42:54.200 --> 42:59.480
in text, we want to print off token dot text.

42:59.480 --> 43:01.600
We want to see what the text actually is.

43:01.600 --> 43:13.240
We want to see the token dot POS, and the token dot DEP helps if you actually iterate

43:13.240 --> 43:16.120
over the correct object over the doc to object.

43:16.120 --> 43:20.640
And we see that we've got Mike, proper noun, noun, subject, and Joy's verb.

43:20.640 --> 43:22.640
It's the root plane.

43:22.640 --> 43:24.440
In this case, it's a verb.

43:24.440 --> 43:28.280
And then we've got football, the noun, the direct object, and a period, which is the

43:28.280 --> 43:29.280
punctuation.

43:29.400 --> 43:33.640
So we can see the basic semantics of the sentence at play.

43:33.640 --> 43:37.200
What's really nice from spacey is we have a way to really visualize this information

43:37.200 --> 43:39.360
and how these words relate to one another.

43:39.360 --> 43:47.400
So we can say from spacey, import, displacey, and we're going to do displacey, displacey

43:47.400 --> 43:51.400
dot render.

43:51.400 --> 43:55.440
And this is going to take two arguments, it's going to be the text, and then it's going

43:55.440 --> 44:01.320
to be the, actually, it's going to be doc two, and then it's going to be style.

44:01.320 --> 44:05.440
In this case, we're going to be working with DEP, and we're going to print that off.

44:05.440 --> 44:10.440
And we actually see how that sentence is structured.

44:10.440 --> 44:13.720
Now in the textbook, I use a more complicated sentence.

44:13.720 --> 44:17.480
But for the reasons of this video, I've kept it a little shorter, just because I think

44:17.480 --> 44:22.320
it displays better on this screen, because you can see that this becomes a little bit

44:22.320 --> 44:25.600
more difficult to understand when you're zoomed in.

44:25.600 --> 44:29.440
But this is one sentence from that Wikipedia article.

44:29.440 --> 44:32.640
So go ahead and look at the textbook and see how elaborate this is.

44:32.640 --> 44:36.400
You can see how it's part of a compound, how it's preposition.

44:36.400 --> 44:42.680
You can see the more fine-grained aspects of the dependency parser and the part of speech

44:42.680 --> 44:46.800
tagger really at play with more complicated sentences.

44:46.800 --> 44:50.960
So that's going to be how you really access part of speech and how you can start to visualize

44:50.960 --> 44:57.240
how words in a sentence are connected to other words in a sentence with regards to their

44:57.240 --> 45:00.480
part of speech and their dependencies.

45:00.480 --> 45:01.960
That's going to be where we stop with that.

45:01.960 --> 45:07.200
In the next section, we're going to be talking about named entity recognition and how to visualize

45:07.200 --> 45:09.680
that information.

45:09.680 --> 45:13.920
So named entity recognition is a very common NLP task.

45:13.920 --> 45:19.080
It's part of kind of data extraction or information extraction from texts.

45:19.080 --> 45:23.480
It's oftentimes just called NER, named entity recognition.

45:23.480 --> 45:28.160
I have a whole book on how to do NER with Python and with Spacey, but we're not going

45:28.160 --> 45:30.120
to be talking about all the ins and outs right now.

45:30.120 --> 45:35.600
We're just going to be talking about how to access the pieces of information throughout

45:35.600 --> 45:37.080
kind of our text.

45:37.080 --> 45:42.560
And then we're going to be dealing with a lot of NER as we try to create elaborate systems

45:42.560 --> 45:47.880
to do named entity extraction for things like financial analysis.

45:47.880 --> 45:52.080
Let's go ahead and figure out how to iterate over a doc object.

45:52.080 --> 45:57.520
So we're going to say for int and doc.n, so we're going to go back to that original doc,

45:57.520 --> 46:03.800
the one that's got the text from Wikipedia on the United States.

46:03.800 --> 46:11.880
We're going to say print off int.text, so the text from it, and int.label, label underscore

46:11.880 --> 46:12.880
here.

46:12.880 --> 46:16.200
That's going to tell us what label corresponds to that text.

46:16.320 --> 46:17.320
Then we print this off.

46:17.320 --> 46:21.800
We've got a lot of GPEs, which are geopolitical entities, North America.

46:21.800 --> 46:23.280
This isn't a geopolitical entity.

46:23.280 --> 46:30.080
It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in

46:30.080 --> 46:36.920
this case, which is a national or religious political entity, quantity, the number of

46:36.920 --> 46:44.440
miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke.

46:44.480 --> 46:48.400
Then we have date being extracted, so at least 12,000 years ago.

46:48.400 --> 46:53.360
This is a small model, and it's extracting for us a lot of very important structured

46:53.360 --> 46:54.520
data.

46:54.520 --> 46:58.160
But we can see that the small model makes mistakes.

46:58.160 --> 47:02.120
So the Revolutionary War is being considered an organization.

47:02.120 --> 47:06.880
Were I to use a large model right now, which I can download separately from Spacey, and

47:06.880 --> 47:12.440
we're going to be seeing this later in this video, or were I to use the much larger transformer

47:12.440 --> 47:13.520
model.

47:13.520 --> 47:19.720
This would be correctly identified most likely as an event, not as an organization, but because

47:19.720 --> 47:24.360
this is a small model that doesn't contain word vectors, which we're going to talk about

47:24.360 --> 47:30.520
in just a little bit, it does not generalize or make predictions well on this particular

47:30.520 --> 47:31.520
data.

47:31.520 --> 47:35.680
Nevertheless, we do see really good extraction here.

47:35.680 --> 47:39.060
We have the American Civil War being extracted as an event.

47:39.060 --> 47:43.940
We have the Spanish American War, even with this encoding typographical error here.

47:43.940 --> 47:48.500
And World War being extracted as an event, World War II event, Cold War event.

47:48.500 --> 47:50.580
All of this is looking good.

47:50.580 --> 47:56.020
And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd

47:56.020 --> 47:57.020
expect to see.

47:57.020 --> 48:00.740
We even see percentages extracted correctly here.

48:00.740 --> 48:07.180
So this is how you access really vital information about your tokens, but more importantly about

48:07.180 --> 48:10.660
the entities found within your text.

48:10.660 --> 48:17.100
And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook.

48:17.100 --> 48:24.540
We can say displacie.render, we can say doc, style, we can say int.

48:24.540 --> 48:30.060
And we get this really nice visualization where each entity has its own particular color.

48:30.060 --> 48:34.140
So you can see where these entities appear within the text, as you kind of just naturally

48:34.140 --> 48:35.140
read it.

48:35.140 --> 48:39.100
And you can do this with the text as long as you want, you can even change the max length

48:39.100 --> 48:41.780
to be more than a million characters long.

48:41.780 --> 48:46.780
And again, we can see right here, org is incorrectly identified as the American Revolutionary War

48:46.780 --> 48:51.220
incorrectly identified as org, but nevertheless, we see really, really good results with a

48:51.220 --> 48:55.300
small English model without a lot of custom fine tune training.

48:55.300 --> 48:56.460
And there's a reason for this.

48:56.460 --> 49:00.860
A lot of Wikipedia data gets included into machine learning models.

49:00.860 --> 49:05.900
The machine learning models on text typically make good predictions on Wikipedia data, because

49:05.900 --> 49:08.060
it was included in their training process.

49:08.060 --> 49:10.620
Nevertheless, these are still good results.

49:10.620 --> 49:14.260
If I'm right or wrong on that, I'm not entirely certain.

49:14.260 --> 49:18.700
But that's going to be how you kind of extract important entities from your text, and most

49:18.700 --> 49:20.580
importantly visualize it.

49:20.580 --> 49:24.300
This is where chapter two of my book kind of ends.

49:24.300 --> 49:28.860
After this chapter, you have a good understanding, hopefully, of kind of what the dot container

49:28.860 --> 49:35.980
is, what tokens are, and how the doc object contains the attributes such as since and

49:35.980 --> 49:41.100
ends, which allows for you to find sentences and entities within a text.

49:41.100 --> 49:47.180
Hopefully you also have a good understanding of how to access the linguistic features of

49:47.180 --> 49:50.060
each token through token attributes.

49:50.060 --> 49:56.740
I encourage you to spend a lot of time becoming familiar with these basics, as these basics

49:56.740 --> 50:01.180
are the building block for really robust things that we're going to be getting into

50:01.180 --> 50:05.700
in the next few lessons.

50:05.700 --> 50:11.860
We're now moving into chapter three of our textbook on Spacey and Python.

50:11.860 --> 50:16.940
Now in chapter three, we're going to be continuing our theme of part one, where we're trying

50:16.940 --> 50:20.900
to understand the larger building blocks of Spacey.

50:20.900 --> 50:25.420
Even though this video is not going to deal with Spacey machine learning approaches, our

50:25.460 --> 50:30.260
custom ones, that is, it's still important to be familiar with what machine learning is

50:30.260 --> 50:36.620
and how it works, specifically with regards to language, because a lot of the Spacey models

50:36.620 --> 50:44.060
such as the medium, large and transformer models, all are machine learning models that

50:44.060 --> 50:47.620
have word vectors stored within them.

50:47.620 --> 50:54.420
This means that they're going to be larger, more accurate, and do the things a bit more

50:54.420 --> 50:58.260
slowly, depending upon its size.

50:58.260 --> 51:03.900
We're going to be working through not only what kind of machine learning is generally,

51:03.900 --> 51:08.540
but specifically how it works with regards to text.

51:08.540 --> 51:13.380
I think that this is where you're going to find this textbook to be somewhat helpful.

51:13.380 --> 51:18.820
What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did

51:18.820 --> 51:23.860
before, but this time we're going to be installing a new model.

51:23.900 --> 51:32.900
We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then

51:32.900 --> 51:37.500
we're going to download the Ncore Web MD model.

51:37.500 --> 51:39.700
This is the medium English model.

51:39.700 --> 51:43.580
This is going to take a little longer to download, and the reason why I'm having you download

51:43.580 --> 51:48.660
the medium model, and the reason why we're going to be using the medium model, is because

51:48.660 --> 51:53.620
the medium model has stored within it word vectors.

51:54.380 --> 52:01.340
Let's go ahead and talk a little bit about what word vectors are and how they're useful.

52:01.340 --> 52:04.700
So word vectors are word embeddings.

52:04.700 --> 52:12.820
So these are numerical representations of words in multi-dimensional space through matrices.

52:12.820 --> 52:15.620
That's a very compacted sentence.

52:15.620 --> 52:17.380
So let's break it down.

52:17.380 --> 52:19.260
What are word vectors used for?

52:19.260 --> 52:26.220
Well, they're used for a computer system to understand what a word actually means.

52:26.220 --> 52:29.740
So computers can't really parse text all that efficiently.

52:29.740 --> 52:31.660
They can't parse it at all.

52:31.660 --> 52:35.660
Every word needs to be converted into some kind of a number.

52:35.660 --> 52:39.540
Now for some old approaches, you would use something like a bag of words approach where

52:39.540 --> 52:43.460
each individual word would have a corresponding number to it.

52:43.460 --> 52:47.540
This would be a unique number that corresponds just to that word.

52:47.540 --> 52:53.660
There are a lot of tasks that can work, but for something like text understanding or trying

52:53.660 --> 52:59.780
to get a computer system to be able to understand how a word functions within a sentence in general,

52:59.780 --> 53:04.700
in other words, how it works in the language, how it relates to all other words, that doesn't

53:04.700 --> 53:06.860
really work for us.

53:06.860 --> 53:11.060
So what a word vector is, is it's a multi-dimensional representation.

53:11.060 --> 53:15.780
So instead of a number having just a single integer that corresponds to it, it instead

53:15.820 --> 53:22.100
has what looks like to an unsuspecting eye, essentially.

53:22.100 --> 53:28.300
It has a very complex sequence of floating numbers that are stored as an array, which

53:28.300 --> 53:34.580
is a computationally less expensive form of a list in Python or just computing in general.

53:34.580 --> 53:36.460
And this is what it looks like, a long sequence.

53:36.460 --> 53:42.700
In this case, I believe it's a 300-dimensional word that corresponds to a specific word.

53:42.700 --> 53:47.860
So this is what an array or a word vector or a word embedding looks like.

53:47.860 --> 53:53.540
What this means to a computer system is it means syntactical and semantical meaning.

53:53.540 --> 53:57.780
So the way word vectors are typically trained is, oh, there's a few different approaches,

53:57.780 --> 54:03.220
but kind of the old-school word-to-vec approach is you give a computer system a whole bunch

54:03.220 --> 54:09.260
of texts and different smaller, larger collections of texts, and what it does is it reads through

54:09.300 --> 54:15.820
all of them and figures out how words are used in relation to other words.

54:15.820 --> 54:21.060
And so what it's able to essentially do through this training process is figure out meaning.

54:21.060 --> 54:25.940
And what that meaning allows for a computer system to do is understand how a word might

54:25.940 --> 54:31.860
relate to other words within a sentence or within a language as a whole.

54:31.860 --> 54:36.060
And in order to understand this, I think it's best if we move away from this textbook and

54:36.060 --> 54:40.620
actually try to explore what word vectors look like in spacey.

54:40.620 --> 54:45.300
So you can have a better sense of specifically what they do, why they're useful, and how

54:45.300 --> 54:51.580
you, as a NLP practitioner, can go ahead and start leveraging them.

54:51.580 --> 54:54.780
So just like before, we're going to create an NLP object.

54:54.780 --> 54:59.820
This time, however, instead of loading in our Encore Web SM, we're going to load in

54:59.820 --> 55:02.780
our Encore Web MD.

55:02.780 --> 55:07.820
So the one that actually has these word vectors stored, the static vectors saved, and it's

55:07.820 --> 55:09.060
going to be a larger model.

55:09.060 --> 55:11.300
Let's go ahead and execute that cell.

55:11.300 --> 55:14.940
And while that's executing, we're going to start opening up our text.

55:14.940 --> 55:23.500
So we're going to say with open data wiki underscore us.txt, r as f, and we're going

55:23.500 --> 55:28.460
to say text is equal to f.read, so we're going to successfully load in that text file and

55:28.460 --> 55:29.460
open it up.

55:29.540 --> 55:33.980
Then we're going to create our doc object, which will be equal to NLP text.

55:33.980 --> 55:37.300
All the syntax is staying the exact same.

55:37.300 --> 55:40.620
And just like before, let's grab the first sentence.

55:40.620 --> 55:47.540
So we're going to convert our doc.sense generator into a list, and we're going to grab index

55:47.540 --> 55:48.540
zero.

55:48.540 --> 55:53.060
And let's go ahead and print off sentence one, just so you can kind of see it.

55:53.060 --> 55:54.720
And there it is.

55:54.720 --> 56:00.000
So now that we've got that kind of in memory, we can start kind of working with it a little

56:00.000 --> 56:01.000
bit.

56:01.000 --> 56:06.400
So let's go ahead and just start tackling how we can actually use word vectors with

56:06.400 --> 56:08.080
spacey.

56:08.080 --> 56:12.720
So let's kind of think about a general question right now.

56:12.720 --> 56:22.800
Let's say I wanted to know how the word, let's say country is similar to other words within

56:22.800 --> 56:25.080
our model's word embeddings.

56:25.080 --> 56:27.840
So let's create a little way we can do this.

56:27.840 --> 56:35.040
We're going to say your word, and this is going to be equal to the word country, country.

56:35.040 --> 56:36.040
There we go.

56:36.040 --> 56:40.280
And what we can do is we can say MS is equal to NLP.

56:40.280 --> 56:42.600
So we're going to go into that NLP object.

56:42.600 --> 56:50.440
We're going to grab the vocab.vectors, and we're going to say most similar.

56:50.440 --> 56:52.440
And this is a little complicated way of doing it.

56:52.440 --> 56:55.520
In fact, I'm going to go ahead and just kind of copy and paste this in.

56:55.520 --> 57:01.520
You have the code already in your textbook that you can follow along with.

57:01.520 --> 57:10.960
And I'm going to go ahead and just copy and paste it in right here and print off this.

57:10.960 --> 57:15.880
And what this is going to do is it is going to go ahead and just do this entirely.

57:15.880 --> 57:18.680
There we go.

57:19.680 --> 57:25.520
And we have to import numpy as MP.

57:25.520 --> 57:28.760
This lets us actually work with the data as a numpy array.

57:28.760 --> 57:34.040
And when we execute this cell, what we get is an output that tells us all the words

57:34.040 --> 57:37.200
that are most similar to the word country.

57:37.200 --> 57:41.520
So in this scenario, the word country, it has these kind of all these different similar

57:41.520 --> 57:47.440
words to it from the word country to the word country, capitalized nation, nation.

57:47.440 --> 57:49.960
Now it's important to understand what you're seeing here.

57:49.960 --> 57:56.440
What you're seeing is not necessarily a synonym for the word country, rather what you're seeing

57:56.440 --> 58:00.160
is are the words that are the most similar.

58:00.160 --> 58:06.560
Now this can be anything from a synonym to a variant spelling of that word to something

58:06.560 --> 58:09.720
that occurs frequently alongside of it.

58:09.720 --> 58:14.640
So for example, world, while this isn't the same, we would never consider world to be

58:14.640 --> 58:16.720
the synonym of country.

58:16.720 --> 58:21.640
But what happens is, is syntactically they're used in very similar situations.

58:21.640 --> 58:25.440
So the way you describe a country is sometimes the way you would describe your world, or

58:25.440 --> 58:30.120
maybe it's something to do with the hierarchy, so a country is found within the world.

58:30.120 --> 58:31.520
This is a good way to understand it.

58:31.520 --> 58:39.560
So it's always good to use this word as most similar, not to be something like synonym.

58:39.560 --> 58:43.520
So when you're talking about word vectors similarity, you're not talking about synonym

58:43.520 --> 58:44.520
similarity.

58:45.400 --> 58:47.200
But this is a way you can kind of quickly get a sense.

58:47.200 --> 58:49.440
So what does this do for you?

58:49.440 --> 58:51.960
Why did I go through and explain all these things about word vectors?

58:51.960 --> 58:56.240
If I'm not going to be talking about machine learning a whole bunch throughout this video.

58:56.240 --> 58:59.560
Well, I did it so that you can do one thing that's really important.

58:59.560 --> 59:03.040
And that's calculate document similarity in the spacey.

59:03.040 --> 59:05.680
So we've already got our NLP model loaded up.

59:05.680 --> 59:07.560
Let's create one object.

59:07.560 --> 59:11.400
So we're going to make doc one, we're going to make that equal to NLP.

59:11.400 --> 59:14.080
And we're going to create the text right here in this object.

59:14.080 --> 59:18.120
So let's say this is coming straight from the spacey documentation.

59:18.120 --> 59:23.600
I like salty fries and hamburgers.

59:23.600 --> 59:26.760
And we're going to say doc two is equal to NLP.

59:26.760 --> 59:33.040
And this is going to be the text fast food tastes very good.

59:33.040 --> 59:36.320
And now what we can do is let's go ahead and load those into memory.

59:36.320 --> 59:40.920
What we can do is we can actually make a calculation using spacey to find out how similar they

59:40.920 --> 59:43.600
actually are these two different sentences.

59:43.640 --> 59:49.000
We can say print off doc one, and we're going to say this again, this is coming straight

59:49.000 --> 59:53.240
from the spacey documentation doc two, so you're going to be able to see what both documents

59:53.240 --> 59:54.240
are.

59:54.240 --> 59:57.080
And then we're going to do doc one dot similarity.

59:57.080 --> 01:00:02.120
So we can go into the doc one dot similarity method and we can compare it to doc two.

01:00:02.120 --> 01:00:03.920
We can print that off.

01:00:03.920 --> 01:00:09.120
So what we're seeing here on the left is document one, this little divider thing that we printed

01:00:09.120 --> 01:00:10.240
off here.

01:00:10.240 --> 01:00:15.400
On the right, we have document two, and then we can see the degree of similarity between

01:00:15.400 --> 01:00:17.840
document one and document two.

01:00:17.840 --> 01:00:18.840
Let's create another doc object.

01:00:18.840 --> 01:00:22.920
We're going to call this NLP doc three, and we're going to make this NLP.

01:00:22.920 --> 01:00:25.240
Let's come up with a sentence that's completely different.

01:00:25.240 --> 01:00:32.840
The Empire State Building is in New York.

01:00:32.840 --> 01:00:36.640
So this is when I'm just making up off the top of my head right now.

01:00:36.640 --> 01:00:42.240
I'm going to copy and paste this down, and we're going to compare this to doc one.

01:00:42.240 --> 01:00:46.160
We're going to compare it to doc three, and we get a score of point five one.

01:00:46.160 --> 01:00:50.400
So this is less similar to than these two.

01:00:50.400 --> 01:00:52.880
So this is a way that you can take a whole bunch of documents.

01:00:52.880 --> 01:00:56.760
You can create a simple for loop, and you can find and start clustering the documents

01:00:56.760 --> 01:00:59.880
that have a lot of overlap or similarity.

01:00:59.880 --> 01:01:01.760
How is this similarity being calculated?

01:01:01.760 --> 01:01:06.080
Well, it's being calculated because what spacey is doing is it's going into its word

01:01:06.080 --> 01:01:11.360
embeddings, and even though in these two situations, we're not using the word fast

01:01:11.360 --> 01:01:14.000
food ever in this document.

01:01:14.000 --> 01:01:19.600
It's going in and it knows that salty fries and hamburgers are probably in a close cluster

01:01:19.600 --> 01:01:26.240
with the biogram or a token that's made up of two words, a biogram of fast food.

01:01:26.240 --> 01:01:31.240
So what it's doing is it's assigning a prediction that these two are still somewhat similar,

01:01:31.240 --> 01:01:36.600
more similar than these two, because of these overlapping in words.

01:01:36.600 --> 01:01:41.140
So let's try one more example, see if we can get something that's really, really close.

01:01:41.140 --> 01:01:49.840
So let's take doc four, and this is going to be equal to NLP, I enjoy oranges.

01:01:49.840 --> 01:01:55.480
And then we're going to have doc five is going to be equal to NLP, I enjoy apples.

01:01:55.480 --> 01:02:00.400
So two, I would agree, I would argue very, very syntactically similar sentences.

01:02:00.400 --> 01:02:05.200
And we're going to do doc four here, doc five here, and we're going to look and see

01:02:05.200 --> 01:02:08.000
a similarity between doc four and doc five.

01:02:08.000 --> 01:02:13.000
And if we execute this, we get a similarity of 0.96.

01:02:13.000 --> 01:02:14.600
So this is really high.

01:02:14.600 --> 01:02:18.600
This is telling me that these two sentences are very similar, and it's not just that they're

01:02:18.600 --> 01:02:25.160
similar because of the similar syntax here, that's definitely pushing the number up.

01:02:25.160 --> 01:02:29.200
It's that what the individual is liking in the scenario between these two texts, they're

01:02:29.200 --> 01:02:30.720
both fruits.

01:02:30.720 --> 01:02:31.720
Let's try something different.

01:02:31.720 --> 01:02:33.920
Let's make doc five.

01:02:33.920 --> 01:02:42.240
Let's just make doc six here, and do something like this NLP, I enjoy, what's another word

01:02:42.240 --> 01:02:44.800
we could say.

01:02:44.800 --> 01:02:49.920
Something that's different, let's say burgers, something different from a fruit.

01:02:49.920 --> 01:02:54.080
So we're going to make doc six like that, and we're going to again copy and paste this

01:02:54.080 --> 01:03:01.400
down, copy and paste this down, we're going to put doc six here.

01:03:01.400 --> 01:03:03.000
And we see this drop.

01:03:03.000 --> 01:03:08.000
So what this demonstrates, and I'm really glad this worked because I improvised this,

01:03:08.000 --> 01:03:14.840
what this demonstrates is that the similarity, the number that's given is not dependent on

01:03:14.840 --> 01:03:23.080
the contextual words, rather it's dependent upon the semantic similarity of the words.

01:03:23.080 --> 01:03:31.160
So apples and oranges are in a similar cluster around fruit because of their word embeddings.

01:03:31.160 --> 01:03:37.720
The word burgers while still being food and still being plural is different from apples

01:03:37.720 --> 01:03:38.960
and oranges.

01:03:38.960 --> 01:03:42.960
So in other words, this similarity is being calculated based on something that we humans

01:03:42.960 --> 01:03:49.000
would calculate difference in meaning based on a large understanding of a language as

01:03:49.000 --> 01:03:50.160
a whole.

01:03:50.160 --> 01:03:54.520
That's where word vectors really come into play.

01:03:54.520 --> 01:03:57.240
This allows for you to calculate other things as well.

01:03:57.240 --> 01:04:02.720
So you could even calculate the difference between salty fries and hamburgers, for example,

01:04:02.720 --> 01:04:07.840
I've got this example ready to go in the textbook, let's go ahead and try this as well.

01:04:07.840 --> 01:04:13.880
So we're going to grab doc one, and print off these few things right here.

01:04:13.880 --> 01:04:18.400
So we're going to try to calculate the similarity between french fries and burgers and what

01:04:18.400 --> 01:04:23.360
we get is a similarity of 0.73.

01:04:23.360 --> 01:04:28.880
So if we were to maybe change this up a little bit and try to calculate the similarity between

01:04:28.880 --> 01:04:37.560
maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity.

01:04:37.560 --> 01:04:42.000
So my point is, is play around with the similarity calculator, play around with the structure,

01:04:42.000 --> 01:04:47.920
the code I provided here, and get familiar with how spacey can help you kind of find

01:04:47.920 --> 01:04:52.280
a similarity, not just between documents, but between words as well.

01:04:52.280 --> 01:04:55.960
And we're going to be seeing how this is useful later on.

01:04:55.960 --> 01:04:59.840
But again, it's good to be familiar with kind of generally how machine learning kind of

01:04:59.840 --> 01:05:05.200
functions here in this context, and why these medium and large models are so much bigger.

01:05:05.200 --> 01:05:10.000
They're so much bigger because they have more word vectors that are much deeper.

01:05:10.000 --> 01:05:15.080
And the transformer model is much larger because it was trained in a completely different method

01:05:15.080 --> 01:05:17.680
than the way the medium and large models were trained.

01:05:17.680 --> 01:05:22.360
But again, that's out of the scope for this video.

01:05:22.360 --> 01:05:28.120
I now want to turn to really the last subject of this introduction to spacey part one, which

01:05:28.120 --> 01:05:32.120
is when we're taking this large umbrella view of the spacey.

01:05:32.120 --> 01:05:35.480
And in the textbook, it's going to correspond to chapter four.

01:05:35.480 --> 01:05:41.320
So what we go over in this textbook is kind of a large view of not just the dot container

01:05:41.320 --> 01:05:46.760
and the word vectors and the linguistic annotations, but really kind of the structure of the spacey

01:05:46.880 --> 01:05:50.240
framework, which comes around the pipeline.

01:05:50.240 --> 01:05:56.280
So a pipeline is a very common expression in computer science and in data science.

01:05:56.280 --> 01:05:59.680
Think of it as a traditional pipeline that you would see in a house.

01:05:59.680 --> 01:06:04.760
Now think of a pipeline being a sequence of different pipes.

01:06:04.760 --> 01:06:11.200
Each pipe in a computer system is going to perform some kind of permutation or some action

01:06:11.200 --> 01:06:16.280
on a piece of data as it goes through the pipeline.

01:06:16.280 --> 01:06:21.920
And as each pipe has a chance to act and make changes to and additions to that data,

01:06:21.920 --> 01:06:25.440
the later pipes get to benefit from those changes.

01:06:25.440 --> 01:06:28.320
So this is very common when you're thinking about logic of code.

01:06:28.320 --> 01:06:32.240
I provide it like a little image here that I think maybe might help you.

01:06:32.240 --> 01:06:38.600
So if we imagine some input sentence, right, so some input text is entering a spacey pipeline,

01:06:38.600 --> 01:06:41.440
it's going to go through a bunch of things if you're working with the medium model or

01:06:41.480 --> 01:06:47.280
the small model, that'll tokenize it and give it a word and vector for different words.

01:06:47.280 --> 01:06:53.000
It'll also find the POS, the part of speech, the dependency parser will act on it.

01:06:53.000 --> 01:07:00.040
But it might eventually get to an entity ruler, which we're going to see in just a few minutes.

01:07:00.040 --> 01:07:05.400
The entity ruler will be a series of rules-based NER named entity recognition.

01:07:05.400 --> 01:07:09.880
So it'll maybe assign a token to an entity.

01:07:09.880 --> 01:07:13.320
Might be the beginning of an entity, might be the end of an entity,

01:07:13.320 --> 01:07:15.960
might just be an individual token entity.

01:07:15.960 --> 01:07:21.960
And then what will happen is that doc object, as it kind of goes through this pipeline,

01:07:21.960 --> 01:07:25.360
will now receive a bunch of doc.ins.

01:07:25.360 --> 01:07:31.600
So it'll be, this pipe will actually add to the doc object as it goes through the pipeline,

01:07:31.600 --> 01:07:33.240
the entity component.

01:07:33.240 --> 01:07:38.120
And then the next pipeline, the entity linker, might take all those entities and try to find out

01:07:38.120 --> 01:07:39.400
which ones they are.

01:07:39.400 --> 01:07:45.120
So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number

01:07:45.120 --> 01:07:47.320
that corresponds to a specific person.

01:07:47.320 --> 01:07:52.960
So for example, if you were seeing a bunch of things like Paul something, Paul something,

01:07:52.960 --> 01:07:56.920
maybe that one Paul something might be Paul Hollywood from the Great British Bake Off,

01:07:56.920 --> 01:08:00.520
and it might have to make a connection to a specific person.

01:08:00.520 --> 01:08:05.920
So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood,

01:08:05.920 --> 01:08:07.720
depending on the context.

01:08:07.840 --> 01:08:13.120
That's out of the scope of this video series, but keep in mind that that pipe would do something

01:08:13.120 --> 01:08:17.680
else that would modify the ints that would give them greater specificity.

01:08:17.680 --> 01:08:22.200
And then what you'd be left with is the doc object on the output that not only has entities

01:08:22.200 --> 01:08:27.200
annotated, but it's also got entities linked to some generic specific data.

01:08:27.200 --> 01:08:29.360
So that's going to be how a pipeline works.

01:08:29.360 --> 01:08:31.520
And this is really what spacey is.

01:08:31.520 --> 01:08:35.640
It's a sequence of pipes that act on your data.

01:08:35.640 --> 01:08:39.640
And that's important to understand, because it means that as you add things to a spacey

01:08:39.640 --> 01:08:45.160
pipeline, you need to be very conscientious about where they're outed and in what order.

01:08:45.160 --> 01:08:48.720
As we're going to see as we move over to kind of rules based spacey, when we start talking

01:08:48.720 --> 01:08:54.560
about these different pipes, the entity ruler, the matcher custom components, regex components,

01:08:54.560 --> 01:08:56.760
you're going to need to know which order to put them in.

01:08:56.760 --> 01:08:58.800
It's going to be very important.

01:08:58.800 --> 01:09:01.000
So do please keep that in mind.

01:09:01.000 --> 01:09:05.400
Now spacey has a bunch of different attribute rulers or different pipes that you can kind

01:09:05.400 --> 01:09:06.960
of add into it.

01:09:06.960 --> 01:09:11.000
You've got dependency parsers that are going to come standard with all of your models.

01:09:11.000 --> 01:09:15.040
You've got the entity linker entity, recognizer entity ruler, you're going to have to make

01:09:15.040 --> 01:09:17.840
these yourself and add them in oftentimes.

01:09:17.840 --> 01:09:19.000
You've got a limitizer.

01:09:19.000 --> 01:09:22.440
This is going to be on most of your standard models, your morphologue, that's going to

01:09:22.440 --> 01:09:25.960
be on on there as well, sentence recognizer, synthesizer.

01:09:25.960 --> 01:09:31.520
This is what allow for you to have the doc.sense right here span categorizer.

01:09:31.520 --> 01:09:37.320
This will help categorize different spans, be them single token spans or sequence of

01:09:37.320 --> 01:09:42.560
token spans, your tagger, this will tag the different things in your text, which will

01:09:42.560 --> 01:09:45.160
help with part of speech, your text categorizer.

01:09:45.160 --> 01:09:49.200
This is when you train a machine learning model to recognize different categories of

01:09:49.200 --> 01:09:50.200
a text.

01:09:50.200 --> 01:09:55.760
So text classification, which is a very important machine learning task, tote to VEC.

01:09:55.760 --> 01:10:01.440
This is going to be what assigns word embeddings to the different words in your doc object.

01:10:01.440 --> 01:10:06.760
Organizer is what breaks that thing up and all your text into individual tokens.

01:10:06.760 --> 01:10:09.800
And you've got things like transformer and trainable pipes.

01:10:09.800 --> 01:10:13.160
Then within this, you've also got some other things called matchers.

01:10:13.160 --> 01:10:14.840
So you can do some dependency matching.

01:10:14.840 --> 01:10:17.000
We're not going to get into that in this video.

01:10:17.000 --> 01:10:20.120
You've also got the ability to use matcher and phrase matcher.

01:10:20.120 --> 01:10:24.720
These are a lot of the times can do some similar things, but they're executed a little differently

01:10:24.720 --> 01:10:25.960
to make things less confusing.

01:10:25.960 --> 01:10:29.680
I'm really only talking about the matcher of these two.

01:10:29.800 --> 01:10:33.880
If there's a need for it, I'll add into the textbook the phrase matcher at a later date,

01:10:33.880 --> 01:10:35.760
but I'm not going to cover it in this video.

01:10:35.760 --> 01:10:39.880
And if I do add in the phrase matcher, it's going to be after this matcher section here.

01:10:39.880 --> 01:10:41.440
I have it in the GitHub repo.

01:10:41.440 --> 01:10:45.200
I just haven't included in the textbook to keep things a little bit simpler, at least

01:10:45.200 --> 01:10:46.880
if you're just starting out.

01:10:46.880 --> 01:10:52.280
So a big good question is, well, how do you add pipes to a spacey pipeline?

01:10:52.280 --> 01:10:53.680
So let's go ahead and do that.

01:10:53.680 --> 01:10:57.440
We're going to make a blank spacey pipeline right now.

01:10:57.440 --> 01:11:02.800
Let's go ahead and just make, we'll just work with the same live coding notebook that we

01:11:02.800 --> 01:11:04.080
have open right now.

01:11:04.080 --> 01:11:07.520
So what we're going to do is we're going to make a blank model, and we're going to actually

01:11:07.520 --> 01:11:14.120
add in our own sentenizer to our, to our text.

01:11:14.120 --> 01:11:16.240
So let's go ahead and do that.

01:11:16.240 --> 01:11:20.680
So I'm going to say NLP is equal to a spacey dot blank.

01:11:20.680 --> 01:11:24.440
This is going to allow for me to make a blank spacey pipeline.

01:11:24.440 --> 01:11:28.640
And I'm going to say Ian so that it knows that the tokenizer that I need to use is the

01:11:28.640 --> 01:11:30.640
English tokenizer.

01:11:30.640 --> 01:11:36.040
And now if I want to add a pipe to that, I can use one of the built-in spacey features.

01:11:36.040 --> 01:11:40.880
So I can say add underscore pipe, and I can say sentenizer.

01:11:40.880 --> 01:11:43.400
So I can add in a sentenizer.

01:11:43.400 --> 01:11:49.160
This is going to allow for me to create a pipeline now that has a sequence of two different

01:11:49.160 --> 01:11:50.160
pipes.

01:11:50.160 --> 01:11:53.560
And I demonstrate in the textbook why this is important.

01:11:53.560 --> 01:11:58.960
Sometimes what you need to do is you need to just only break down a text into individual

01:11:58.960 --> 01:11:59.960
sentences.

01:11:59.960 --> 01:12:08.080
So I grabbed a massive, massive corpus from the internet, which is on MIT.edu.

01:12:08.080 --> 01:12:10.560
And it's the entire Shakespeare corpus.

01:12:10.560 --> 01:12:15.320
And I just try to calculate the, the quantity of sentences found within it.

01:12:15.320 --> 01:12:22.760
There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and

01:12:22.760 --> 01:12:26.040
count those sentences with the spacey model.

01:12:26.040 --> 01:12:32.080
Using the small model, however, it took a total amount of time of 47 minutes to actually

01:12:32.080 --> 01:12:35.520
break down all those sentences and extract them.

01:12:35.520 --> 01:12:39.600
Why is there a difference in time between 7 seconds and 47 minutes?

01:12:39.600 --> 01:12:44.320
It's because that this spacey small model has a bunch of other pipes in it that are

01:12:44.320 --> 01:12:46.520
trying to do a bunch of other things.

01:12:46.520 --> 01:12:54.600
If you just need to do one task, it's always a good idea to just activate one pipe or maybe

01:12:54.600 --> 01:12:59.240
make a blank model and just add that single pipe or the only pipes that you need to it.

01:12:59.240 --> 01:13:04.120
A great example of this is needing to tokenize a whole bunch of sentences in relatively short

01:13:04.120 --> 01:13:05.120
time.

01:13:05.120 --> 01:13:11.280
So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.

01:13:11.280 --> 01:13:12.840
However comes at a trade-off.

01:13:12.840 --> 01:13:18.640
The small model is going to be more accurate in how it finds sentence boundaries.

01:13:18.640 --> 01:13:21.080
So we have a difference in quantity here.

01:13:21.080 --> 01:13:26.240
This difference in quantity indicates that this one messed up and made some mistakes because

01:13:26.240 --> 01:13:27.960
it was just the sentenceizer.

01:13:27.960 --> 01:13:31.760
The sentenceizer didn't have extra data being fed to it.

01:13:31.760 --> 01:13:36.560
In fact, if I probably used larger models, I might even have better results.

01:13:36.560 --> 01:13:37.760
But always think about that.

01:13:37.760 --> 01:13:42.400
If time is of the essence and you don't care so much about accuracy, a great way to get

01:13:42.400 --> 01:13:46.800
the quantity of sentences or at least a ballpark is to use this method where you simply add

01:13:46.800 --> 01:13:50.120
in a sentenceizer to a blank model.

01:13:50.120 --> 01:13:56.240
So that's how you actually add in different pipes to a spacey pipeline and we're going

01:13:56.240 --> 01:14:00.760
to be reinforcing that skill as we go through, especially in part two, where we really kind

01:14:00.760 --> 01:14:03.200
of work with this in a lot of detail.

01:14:03.200 --> 01:14:08.800
Right now I'm just interested in giving you the general understanding of how this might

01:14:08.800 --> 01:14:09.800
work.

01:14:09.800 --> 01:14:16.240
Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes.

01:14:16.240 --> 01:14:20.160
We can analyze what our analyze, there we go.

01:14:20.160 --> 01:14:22.460
We can actually analyze our pipeline.

01:14:22.460 --> 01:14:27.600
If we look at the NLP object, which is our blank model with the sentenceizer, we see

01:14:27.600 --> 01:14:35.760
that our NLP pipeline ignore summary, ignore this bit here.

01:14:35.800 --> 01:14:39.840
But what you're actually able to kind of go through and see right away is that we've really

01:14:39.840 --> 01:14:42.640
just got the sentenceizer sitting in it.

01:14:42.640 --> 01:14:48.520
If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey

01:14:48.520 --> 01:14:55.920
dot load and core web SM, we're going to create that NLP two object around the small spacey

01:14:55.920 --> 01:14:59.760
English model.

01:14:59.760 --> 01:15:06.760
We can analyze the pipes again, and we see a much more elaborate pipeline.

01:15:06.760 --> 01:15:07.760
So what are we looking at?

01:15:07.760 --> 01:15:12.400
Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger

01:15:12.400 --> 01:15:18.880
after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got

01:15:18.880 --> 01:15:23.520
an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed

01:15:23.520 --> 01:15:26.400
the doc dot ends, and we keep on going down.

01:15:26.400 --> 01:15:31.240
We can see the limitizer, but we can see also a whole bunch of other things.

01:15:31.240 --> 01:15:34.840
We can see what these different things actually assign.

01:15:34.840 --> 01:15:42.000
So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually

01:15:42.000 --> 01:15:43.120
require.

01:15:43.120 --> 01:15:48.040
So if we look up here, we see that the NER pipe, so the name to the recognition pipe

01:15:48.040 --> 01:15:51.280
is responsible for assigning the doc dot ends.

01:15:51.280 --> 01:15:56.080
So that attribute of the doc object, and it's also responsible at the token level for

01:15:56.080 --> 01:16:02.480
assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes

01:16:02.480 --> 01:16:10.080
ago when we talked about the IOB being the opening beginning or out beginning inside

01:16:10.080 --> 01:16:16.720
for a different entity, it also assigns the end dot end underscore type for each token

01:16:16.720 --> 01:16:17.720
attribute.

01:16:17.720 --> 01:16:23.240
So you can see a lot of different things about your pipeline by using NLP dot analyze underscore

01:16:23.240 --> 01:16:24.880
pipes.

01:16:24.880 --> 01:16:29.800
If you've gotten to this point in the video, then I think you should by now have a good

01:16:29.800 --> 01:16:36.040
really umbrella view of what spacey is, how it works, why it's useful.

01:16:36.040 --> 01:16:39.560
And some of the basic features that it can do and how it can solve some pretty complex

01:16:39.560 --> 01:16:43.320
problems with some pretty simple lines of code.

01:16:43.320 --> 01:16:48.280
What we're going to see now moving forward is how you as a practitioner of NLP cannot

01:16:48.280 --> 01:16:52.920
just take what's given to you with spacey, but start working with it and start leveraging

01:16:52.920 --> 01:16:55.200
it for your own uses.

01:16:55.200 --> 01:16:57.280
So taking what is already available.

01:16:57.280 --> 01:17:02.120
So like these models like the English model and adding to them contributing to them.

01:17:02.120 --> 01:17:07.000
Maybe you want to make an entity ruler where you can find more entities in a text based

01:17:07.000 --> 01:17:10.120
on some cousin tier or list that you have.

01:17:10.120 --> 01:17:14.280
Maybe you want to make a matcher so you can find specific sequences within a text.

01:17:14.280 --> 01:17:16.880
Maybe that's important for information extraction.

01:17:16.880 --> 01:17:20.600
Maybe you need to add custom functions or components into your spacey pipeline.

01:17:20.600 --> 01:17:24.920
I'm going to be going through in part two rules based spacey and giving you all the

01:17:24.920 --> 01:17:31.160
basics of how to do some really robust custom things relatively quickly with within the

01:17:31.160 --> 01:17:32.920
spacey framework.

01:17:32.920 --> 01:17:36.720
All of that's going to lay the groundwork so that in part three, we can start applying

01:17:36.720 --> 01:17:40.760
all these skills and start solving some real world problems.

01:17:40.760 --> 01:17:43.680
In this case, we're going to look at financial analysis.

01:17:43.680 --> 01:17:48.440
So that's going to be where we move to next is part two.

01:17:48.440 --> 01:17:53.280
We are now moving into part two of this Jupiter book on spacey and we're going to be working

01:17:53.280 --> 01:17:55.520
with rules based spacey.

01:17:55.520 --> 01:17:58.440
Now this is really kind of the bread and butter of this video.

01:17:58.440 --> 01:18:02.800
You've gotten a sense of the umbrella structure of spacey as a framework.

01:18:02.800 --> 01:18:05.840
You've gotten a sense of what the container can contain.

01:18:05.840 --> 01:18:10.920
You've gotten a sense of the token attributes and the linguistic annotations from part one

01:18:10.920 --> 01:18:13.680
of this book and the earlier part of this video.

01:18:13.680 --> 01:18:18.880
Now we're going to move into taking those skills and really developing them into custom

01:18:18.880 --> 01:18:23.520
components and modified pipes that exist within spacey.

01:18:23.520 --> 01:18:27.960
In other words, I'm going to show you how to take what we've learned now and start really

01:18:27.960 --> 01:18:32.720
doing more robust and sophisticated things with that knowledge.

01:18:32.720 --> 01:18:36.320
So we're going to be working first with the entity ruler, then with the matcher in the

01:18:36.320 --> 01:18:39.080
next chapter, then in the components in spacey.

01:18:39.080 --> 01:18:43.160
So a custom component is a custom function that you can put into a pipeline.

01:18:43.160 --> 01:18:46.280
Then we're going to talk about regex or regular expressions.

01:18:46.280 --> 01:18:48.880
And then we're going to talk about some advanced regex with spacey.

01:18:48.880 --> 01:18:53.720
If you don't know what regex is, I'm going to cover this in chapter eight.

01:18:53.720 --> 01:18:59.000
So let's go over to our Jupiter notebook that we're going to be using for our entity ruler

01:18:59.000 --> 01:19:00.000
lesson.

01:19:00.000 --> 01:19:02.680
So let's go ahead and execute some of these cells.

01:19:02.680 --> 01:19:05.600
And then I'm going to be talking about it in just a second.

01:19:05.600 --> 01:19:10.960
First I want to take some time to explain what the entity ruler is as a pipe in spacey,

01:19:10.960 --> 01:19:15.200
what it's used for, why you'd find it useful and when to actually implement it.

01:19:15.200 --> 01:19:19.840
So there are two different ways in which you can kind of add in custom features to a spacey

01:19:19.840 --> 01:19:21.320
language pipeline.

01:19:21.320 --> 01:19:25.600
There is a rules based approach and a machine learning based approach.

01:19:25.600 --> 01:19:29.320
Rules based approaches should be used when you can think about how to generate a set

01:19:29.320 --> 01:19:35.680
of rules based on either a list of known things or a set of rules that can be generated through

01:19:35.680 --> 01:19:39.680
regex, code or linguistic features.

01:19:39.680 --> 01:19:44.160
Machine learning is when you don't know how to actually write out the rules or the rules

01:19:44.160 --> 01:19:47.800
that you would need to write out would be exceptionally complicated.

01:19:47.800 --> 01:19:51.920
A great example of a rules based approach versus a machine learning based approach and when

01:19:51.920 --> 01:19:57.080
to use them is with entity types for named entity recognition.

01:19:57.080 --> 01:20:01.760
Imagine if you wanted to extract dates from a text.

01:20:01.760 --> 01:20:06.200
There are a finite, very finite number of ways that a date can appear in a text.

01:20:06.200 --> 01:20:11.960
You could have something like January 1, 2005, you could have one January 2005, you could

01:20:11.960 --> 01:20:18.840
have one Jan 2005, you could have one slash five slash 2005, there's there's different

01:20:18.840 --> 01:20:21.320
ways that you can do this and there's a lot of them.

01:20:21.320 --> 01:20:25.400
But there really is a finite number that you could easily write a regex expression for

01:20:25.400 --> 01:20:29.600
a regular expression for to capture all of those.

01:20:29.600 --> 01:20:33.240
And in fact, those regex expressions already exist.

01:20:33.240 --> 01:20:37.360
That's why spacey is already really good at identifying dates.

01:20:37.360 --> 01:20:43.040
So dates are something that you would probably use a rules based approach for something that's

01:20:43.040 --> 01:20:47.880
a good machine learning approach for or something like names.

01:20:47.880 --> 01:20:53.880
If you wanted to capture the names of people, you would have to generate an entity ruler

01:20:53.880 --> 01:20:57.680
with a whole bunch of robust features.

01:20:57.680 --> 01:21:02.840
So you would have to have a list of all known possible first names, all known possible last

01:21:02.920 --> 01:21:09.920
names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc.

01:21:10.320 --> 01:21:12.600
And you'd have to have a list of all known suffixes.

01:21:12.600 --> 01:21:17.320
So junior, senior, the third, the fourth, etc. on down the list.

01:21:17.320 --> 01:21:22.520
This would be very, very difficult to write because first of all, the quantity of names

01:21:22.520 --> 01:21:25.440
that exist in the world are massive.

01:21:25.440 --> 01:21:29.440
The quantity of last names that exist in the world is massive.

01:21:29.440 --> 01:21:34.040
There's not a set gazetteer or set list out there of these anywhere.

01:21:34.040 --> 01:21:39.040
So for this reason, oftentimes things like people names will be worked into machine learning

01:21:39.040 --> 01:21:40.040
components.

01:21:40.040 --> 01:21:44.040
I'm going to address machine learning in another video at a later date, but right now we're

01:21:44.040 --> 01:21:47.320
going to focus on a rules based approach.

01:21:47.320 --> 01:21:54.320
So using the rules based features that spacey offers, a good NLP practitioner will be excellent

01:21:55.000 --> 01:22:00.120
at both rules based approaches and machine learning based approaches and knowing when

01:22:00.120 --> 01:22:06.920
to use which approach and when maybe maybe a task is not appropriate for machine learning

01:22:06.920 --> 01:22:10.800
when it can be worked in with rules relatively well.

01:22:10.800 --> 01:22:15.560
If you're taking a rules based approach, the approach that you take should have a high

01:22:15.560 --> 01:22:20.640
degree of confidence that the rules will always return true positives.

01:22:20.640 --> 01:22:22.400
And you need to think about that.

01:22:22.400 --> 01:22:27.640
If you are okay with your rules, maybe catching a few false positives or missing a few true

01:22:27.640 --> 01:22:32.800
positives, then maybe think about how you write the rules and allowing for those and

01:22:32.800 --> 01:22:35.040
making it known in your documentation.

01:22:35.040 --> 01:22:39.760
So that's generally what a rules based approach is in an entity ruler is a way that we can

01:22:39.760 --> 01:22:46.760
use a list or a series of features, language features to add tokens into the entity, the

01:22:47.320 --> 01:22:51.060
dot ints container within the dot container.

01:22:51.060 --> 01:22:53.940
So let's go ahead and try to do this right now.

01:22:53.940 --> 01:22:57.180
The text we're going to be working with is a kind of fun one, I think.

01:22:57.180 --> 01:23:01.820
So if you've already gotten the reference, congratulations, it's kind of obscure.

01:23:01.820 --> 01:23:04.540
But we're going to have a sentence right here that I just wrote out.

01:23:04.540 --> 01:23:08.180
West Chesterton Fieldville was referenced in Mr. Deeds.

01:23:08.180 --> 01:23:11.180
So in this context, we are going to have a few different entities.

01:23:11.180 --> 01:23:16.980
We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.

01:23:16.980 --> 01:23:19.020
It's a fake place that doesn't really exist.

01:23:19.020 --> 01:23:21.380
It was made up in the movie Mr. Deeds.

01:23:21.380 --> 01:23:25.180
And what we want is for Mr. Deeds to be grabbed as an entity as well.

01:23:25.180 --> 01:23:28.100
And this would ideally be labeled as a film.

01:23:28.100 --> 01:23:30.780
But in this case, that's probably not going to happen.

01:23:30.780 --> 01:23:32.740
Let's go ahead and see what does happen.

01:23:32.740 --> 01:23:39.580
So we're going to say for int and doc dot ints, print off int dot text, int dot label,

01:23:39.580 --> 01:23:44.020
like we learned from our NER lesson a few moments ago.

01:23:44.020 --> 01:23:46.100
And we see that the output looks like this.

01:23:46.100 --> 01:23:48.700
It's gotten almost all the entities that we wanted.

01:23:48.700 --> 01:23:50.940
Mr. was left off of Deeds.

01:23:50.940 --> 01:23:54.620
And it's grabbed the West Chesterton Fieldville and labeled it as a person.

01:23:54.620 --> 01:23:56.020
So what's gone wrong here?

01:23:56.020 --> 01:23:58.180
Well, there's a few different things that have gone wrong.

01:23:58.180 --> 01:24:02.980
The NCORE Web SM model is a machine learning model for NER.

01:24:02.980 --> 01:24:05.140
The word vectors are not saved.

01:24:05.140 --> 01:24:07.860
So the static vectors are not in it.

01:24:07.860 --> 01:24:10.020
So it's making the best prediction that it can.

01:24:10.020 --> 01:24:14.780
But even with a very robust machine learning model, unless it has seen West Chesterton

01:24:14.780 --> 01:24:21.180
Fieldville, there is not really a good way for the model to actually know that that's

01:24:21.180 --> 01:24:22.700
a place.

01:24:22.700 --> 01:24:28.740
Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer

01:24:28.740 --> 01:24:31.500
model might actually get this right.

01:24:31.500 --> 01:24:33.540
But for the most part, this is a very challenging thing.

01:24:33.540 --> 01:24:34.900
This would be challenging for a human.

01:24:34.900 --> 01:24:39.620
There's not a lot of context here to tell you what this kind of entity is, unless you

01:24:39.620 --> 01:24:48.540
knew a lot about how maybe northeastern villages and towns in North America would be called.

01:24:48.540 --> 01:24:54.020
Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.

01:24:54.020 --> 01:24:59.420
Now ideally, we would have an NER model that would label West Chesterton Fieldville as

01:24:59.420 --> 01:25:01.940
a GPE and Mr. Deeds as a film.

01:25:01.940 --> 01:25:03.660
But we've got two problems.

01:25:03.660 --> 01:25:08.460
One, the machine learning model doesn't have film as an entity type.

01:25:08.460 --> 01:25:13.980
And on top of that, West Chesterton Fieldville is not coming out correct as GPE.

01:25:13.980 --> 01:25:18.820
So our goal right now is to fix both of these problems with an entity ruler.

01:25:18.820 --> 01:25:24.300
This would be useful if I were maybe doing some text analysis on fictional places referenced

01:25:24.300 --> 01:25:25.460
in films.

01:25:25.460 --> 01:25:29.700
So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all

01:25:29.700 --> 01:25:31.820
be classified as kind of fictional places.

01:25:31.820 --> 01:25:36.660
So let's go ahead and make a ruler to correct this problem.

01:25:36.660 --> 01:25:42.100
So what we're going to do is first we're going to make a ruler by saying ruler is equal

01:25:42.100 --> 01:25:46.920
to NLP dot add pipe.

01:25:46.920 --> 01:25:50.620
And this is going to take one argument here, you're going to find out when we start working

01:25:50.620 --> 01:25:54.300
with custom components that you can have a few different arguments here, especially

01:25:54.300 --> 01:25:56.300
if you create your own custom components.

01:25:56.300 --> 01:26:00.300
But for right now, we're working with the components that come standard with spacey.

01:26:00.300 --> 01:26:01.860
There's about 18 of them.

01:26:01.860 --> 01:26:07.220
One of them is the entity underscore ruler, all lowercase.

01:26:07.220 --> 01:26:10.980
We're going to add that ruler into our NLP model.

01:26:10.980 --> 01:26:17.980
And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER

01:26:17.980 --> 01:26:25.780
model and see as we go down that the NER pipe is here and the entity ruler is now the exit,

01:26:25.780 --> 01:26:27.740
the final pipe in our pipeline.

01:26:27.740 --> 01:26:30.660
So we see that it has been successfully added.

01:26:30.660 --> 01:26:35.540
Let's go ahead now and try to add patterns into that pipeline.

01:26:35.540 --> 01:26:39.780
Patterns are the things that the spacey model is going to look for in the label that it's

01:26:39.780 --> 01:26:43.300
going to assign when it finds something that meets that pattern.

01:26:43.300 --> 01:26:46.260
This will always be a list of lists.

01:26:46.260 --> 01:26:48.140
So let's go ahead and do this right now.

01:26:48.140 --> 01:26:50.100
Sorry, a list of dictionaries.

01:26:50.100 --> 01:26:57.380
So the first pattern that we're really looking for here is going to be a dictionary.

01:26:57.380 --> 01:27:04.340
It's going to have one key of label, which is going to be equal to GPE and another label

01:27:04.340 --> 01:27:10.060
of pattern, which is going to be equal to, in this case, we want to find West Chesterton

01:27:10.060 --> 01:27:11.060
Fieldville.

01:27:11.060 --> 01:27:19.020
Let me go ahead and just copy and paste it so I don't make a mistake here.

01:27:19.020 --> 01:27:23.620
And what we want to do is we want our entity ruler to see West Chesterton Fieldville.

01:27:23.620 --> 01:27:26.380
And when it sees it, assign the label of GPE.

01:27:26.380 --> 01:27:27.940
So it's a geopolitical entity.

01:27:27.940 --> 01:27:29.740
So it's a place.

01:27:29.740 --> 01:27:31.340
So let's go ahead and execute that.

01:27:31.340 --> 01:27:32.340
Great.

01:27:32.340 --> 01:27:33.340
We've got the patterns.

01:27:33.340 --> 01:27:35.740
Now comes time to load them into the ruler.

01:27:35.740 --> 01:27:40.020
So we can say ruler.add underscore patterns.

01:27:40.020 --> 01:27:41.340
This is going to take one argument.

01:27:41.340 --> 01:27:46.380
It's going to be our list of patterns added in.

01:27:46.380 --> 01:27:47.540
Cool.

01:27:47.540 --> 01:27:49.020
Now let's create a new doc object.

01:27:49.020 --> 01:27:52.540
We're going to call this doc to that's going to be equal to NLP.

01:27:52.540 --> 01:27:55.700
We're going to pass in that same text.

01:27:55.700 --> 01:28:03.780
We're going to say for int n doc to dot ints print off int dot text and end dot label.

01:28:03.780 --> 01:28:08.940
And you're going to notice that nothing has changed.

01:28:08.940 --> 01:28:10.700
So why has nothing changed?

01:28:10.700 --> 01:28:12.660
We're still getting the same results.

01:28:12.660 --> 01:28:14.780
And we've added the correct pattern in.

01:28:14.780 --> 01:28:17.460
The answer lies into one key thing.

01:28:17.460 --> 01:28:23.340
If we look back up here, we see that our entity ruler comes after our NER.

01:28:23.340 --> 01:28:24.340
What does that mean?

01:28:24.340 --> 01:28:28.060
Well, imagine how the pipeline works that I talked about a little while ago in this

01:28:28.060 --> 01:28:29.060
video.

01:28:29.060 --> 01:28:34.420
A pipeline works by different components, adding things to an object and making changes

01:28:34.420 --> 01:28:41.460
to it, in this case, adding ints to it, and then making those things isolated from later

01:28:41.460 --> 01:28:44.940
pipes from being able to overwrite them unless specified.

01:28:44.940 --> 01:28:49.860
What this means is that when West Chesterton field bill goes through and is identified

01:28:49.860 --> 01:28:57.020
by the NER pipe as a person, it can no longer be identified as anything else.

01:28:57.020 --> 01:29:01.520
What this means is that you need to do one of two things give your ruler the ability

01:29:01.520 --> 01:29:09.160
to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline.

01:29:09.160 --> 01:29:13.220
So let's go through and solve this common problem right now.

01:29:13.220 --> 01:29:18.460
We're going to create a new NLP object called NLP to, which is going to be equal to spacey

01:29:18.460 --> 01:29:19.460
dot load.

01:29:19.460 --> 01:29:26.220
And again, we're going to load in the English core web SM's model and core web SM.

01:29:26.220 --> 01:29:28.060
Great.

01:29:28.060 --> 01:29:41.540
And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going

01:29:41.540 --> 01:29:45.420
to make that an object too.

01:29:45.420 --> 01:29:49.980
Now what we can do is we can say ruler dot add patterns, again, we're going to go through

01:29:49.980 --> 01:29:53.060
all of these steps that we just went through, we're going to add in those patterns that

01:29:53.060 --> 01:29:54.940
we created up above.

01:29:54.940 --> 01:29:58.940
And now what we're going to do is we're going to actually do one thing a little different

01:29:58.940 --> 01:30:00.380
than what we did.

01:30:00.380 --> 01:30:04.440
What we're going to do is we're going to load this up again, and we're going to do an extra

01:30:04.440 --> 01:30:05.820
keyword argument.

01:30:05.820 --> 01:30:11.620
Now we can say either after or before here, we're going to say before NER, what this is

01:30:11.620 --> 01:30:18.540
going to do is it's going to place our NER before our entity will ever for the NER component.

01:30:18.540 --> 01:30:24.820
And now when we add our patterns in, we can now create a new doc object.

01:30:24.820 --> 01:30:32.300
Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print

01:30:32.300 --> 01:30:37.340
off int dot text, and dot label.

01:30:37.460 --> 01:30:41.780
Now we notice that it is correctly labeled as a GPE.

01:30:41.780 --> 01:30:42.780
Why is this?

01:30:42.780 --> 01:30:50.540
Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will

01:30:50.540 --> 01:30:55.220
notice that our entity ruler now in the pipeline sits before the NER model.

01:30:55.220 --> 01:31:00.100
In other words, we've given primacy to our custom entity ruler, so that it's going to

01:31:00.100 --> 01:31:04.580
have the first shot at actually correctly identifying these things, but we've got another

01:31:04.580 --> 01:31:06.060
problem here.

01:31:06.060 --> 01:31:14.260
This is coming out as a person, it should be Mr. Deeds as the entire collective multi

01:31:14.260 --> 01:31:16.660
word token, and that should be a new entity.

01:31:16.660 --> 01:31:21.660
We can use the entity ruler to add in custom types of labels here.

01:31:21.660 --> 01:31:24.900
So let's go ahead and do this same thing.

01:31:24.900 --> 01:31:31.220
Let's go ahead and just copy and paste our patterns, and we're going to create one more

01:31:31.380 --> 01:31:39.300
NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM.

01:31:39.300 --> 01:31:42.020
Great, we've got that loaded up.

01:31:42.020 --> 01:31:48.260
We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to

01:31:48.260 --> 01:31:54.860
NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it

01:31:54.860 --> 01:32:01.620
before the NER pipe, NLP three, there we go.

01:32:01.620 --> 01:32:04.820
And what we need to do now is we need to copy in these patterns, and we're going to add

01:32:04.820 --> 01:32:06.620
in one more pattern.

01:32:06.620 --> 01:32:08.580
Remember this can be a list here.

01:32:08.580 --> 01:32:15.020
So this pattern, we're going to have a new label called film, and we're going to look

01:32:15.020 --> 01:32:20.180
for the sequence Mr. Deeds.

01:32:20.180 --> 01:32:23.940
And that's going to be our pattern that we want to add in to our ruler.

01:32:24.020 --> 01:32:30.060
So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember

01:32:30.060 --> 01:32:34.980
that one keyword argument, or one argument is going to be the list itself.

01:32:34.980 --> 01:32:39.380
And now we can create a new doc object, which is going to be equal to NLP three, I think

01:32:39.380 --> 01:32:48.260
I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text

01:32:48.260 --> 01:32:51.060
and and dot label.

01:32:51.060 --> 01:32:55.860
And if we execute this, we see now that not only have you gotten the entity ruler to correctly

01:32:55.860 --> 01:33:02.940
identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly,

01:33:02.940 --> 01:33:04.980
Mr. Deeds as a film.

01:33:04.980 --> 01:33:08.580
Now some of you might be realizing the problem here, this is actually a problem for machine

01:33:08.580 --> 01:33:09.580
learning models.

01:33:09.580 --> 01:33:14.180
And the reason for this is because Mr. Deeds in some instances could be the person and

01:33:14.180 --> 01:33:17.980
Mr. Deeds in other instances could be the movie itself.

01:33:17.980 --> 01:33:20.220
This is what we would call a toponym.

01:33:20.220 --> 01:33:24.020
So spelled like this, and this is a common problem in natural language processing.

01:33:24.020 --> 01:33:28.020
And it's actually one of the few problems or one of many problems really, that remain

01:33:28.020 --> 01:33:35.420
a little bit unsolved toponym resolution, spelled like this, or TR is the resolution

01:33:35.420 --> 01:33:36.420
of toponym.

01:33:36.420 --> 01:33:41.260
So things that can have multiple labels that are dependent upon context.

01:33:41.260 --> 01:33:46.860
Another example of toponym resolution is something like this, if you were to look at this word,

01:33:46.860 --> 01:33:51.460
let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology.

01:33:51.460 --> 01:33:53.940
Let's say it's only going to ever be a GPE.

01:33:53.940 --> 01:33:59.700
The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas.

01:33:59.700 --> 01:34:05.460
Toponym resolution is also the ability to resolve problems like this, when in context

01:34:05.460 --> 01:34:11.540
is Paris was kind of talking about Paris, France, when in context is it talking about

01:34:11.540 --> 01:34:15.580
Kentucky, and when in context is it talking about Texas.

01:34:15.580 --> 01:34:19.500
So that's something that you really want to think about when you're generating your

01:34:19.500 --> 01:34:25.540
rules for an entity ruler, is this ever going to be a false positive?

01:34:25.540 --> 01:34:30.180
And if the answer is that it's going to be a false positive half the time, or it's a

01:34:30.180 --> 01:34:36.100
50-50 shot, then really consider incorporating that kind of an entity into a machine learning

01:34:36.100 --> 01:34:41.740
model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as

01:34:41.740 --> 01:34:42.740
a person.

01:34:42.740 --> 01:34:48.460
And learn with word embeddings when that context means it's a film and when that context means

01:34:48.460 --> 01:34:49.460
it's a person.

01:34:49.460 --> 01:34:52.020
That's just a little toy example.

01:34:52.020 --> 01:34:55.300
What we're going to see moving forward, though, and we're going to do this with a matcher,

01:34:55.300 --> 01:34:59.620
not with the entity ruler, is that spacey can do a lot of things.

01:34:59.620 --> 01:35:05.060
You might be thinking to yourself, now I could easily just come up with a list and just check

01:35:05.060 --> 01:35:09.820
and see whenever Mr. Deeds pops up and just inject that into the doc.ins.

01:35:09.820 --> 01:35:12.180
I could do the same thing with West Chesterton Field Bill.

01:35:12.180 --> 01:35:15.500
Why do I need an NLP framework to do this?

01:35:15.500 --> 01:35:19.780
And the answer is going to come up in just a few minutes when we start realizing that

01:35:19.780 --> 01:35:25.740
spacey can do a lot more than things like regex or things like just a basic gazetteer

01:35:25.740 --> 01:35:27.820
check or a list check.

01:35:27.820 --> 01:35:32.380
What you can do with spacey is you can have the pattern not just take a sequence of characters

01:35:32.380 --> 01:35:38.180
and look for a match, but a sequence of linguistic features as well, that earlier pipes have

01:35:38.180 --> 01:35:39.180
identified.

01:35:39.180 --> 01:35:43.420
And I think it's best if we save that for just a second when we start talking about

01:35:43.420 --> 01:35:49.140
the matcher, which is, in my opinion, one of the more robust things that you can do

01:35:49.140 --> 01:35:54.860
with spacey and what sets spacey apart from things like regex or other fancier string

01:35:54.860 --> 01:35:56.860
matching approaches.

01:35:56.860 --> 01:36:03.180
Okay, we're now moving into chapter six of this book, and this is really kind of, in

01:36:03.180 --> 01:36:07.300
my opinion, one of the most important areas in this entire video.

01:36:07.300 --> 01:36:11.620
If you can master the techniques I'm going to show you for the next maybe 20 minutes

01:36:11.620 --> 01:36:16.300
or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really

01:36:16.300 --> 01:36:20.060
going to see really kind of its true power.

01:36:20.060 --> 01:36:24.380
A lot of the stuff that we talk about here in the matcher can also be implemented in

01:36:24.380 --> 01:36:28.500
the entity ruler as well with a pattern.

01:36:28.500 --> 01:36:33.660
The key difference between the entity ruler and the matcher is in how data the data is

01:36:33.660 --> 01:36:35.080
kind of extracted.

01:36:35.080 --> 01:36:39.280
So the matcher is going to store information a little differently.

01:36:39.280 --> 01:36:43.120
It's going to store it as within the vocab of the NLP model.

01:36:43.120 --> 01:36:48.920
It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk

01:36:48.920 --> 01:36:50.920
about that more in just a second.

01:36:50.920 --> 01:36:53.360
And it's not going to store it in the doc ends.

01:36:53.360 --> 01:36:57.360
So matchers don't put things in your doc.ends.

01:36:57.360 --> 01:37:01.540
So when do you want to use a matcher over an entity ruler?

01:37:01.540 --> 01:37:06.620
You want to use the entity ruler when the thing that you're trying to extract is something

01:37:06.620 --> 01:37:11.340
that is important to have a label that corresponds to it within the entities that are coming

01:37:11.340 --> 01:37:12.620
out.

01:37:12.620 --> 01:37:17.300
So in my research, I use this for anything from like, let's say stocks, if I'm working

01:37:17.300 --> 01:37:23.660
with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm

01:37:23.660 --> 01:37:31.500
a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside

01:37:31.620 --> 01:37:32.620
other entities.

01:37:32.620 --> 01:37:38.260
I'll also work in things like ships, so the names of ships, streets, things like that.

01:37:38.260 --> 01:37:43.460
When I use the the matcher, it's when I'm looking for something that is not necessarily

01:37:43.460 --> 01:37:51.060
an entity type, but something that is a structure within the text that will help me extract

01:37:51.060 --> 01:37:52.060
information.

01:37:52.060 --> 01:37:55.500
And I think that'll make more sense as we go through and I show you kind of how to improve

01:37:55.500 --> 01:38:01.060
examples going through it, we're kind of using the matcher as you would in the real world.

01:38:01.100 --> 01:38:05.980
But remember, all the patterns that I show you can also be implemented in the entity

01:38:05.980 --> 01:38:06.980
ruler.

01:38:06.980 --> 01:38:11.140
And I'm also going to talk about when we get to chapter eight, how rejects can actually

01:38:11.140 --> 01:38:14.460
be used to do similar things, but in a different way.

01:38:14.460 --> 01:38:20.180
Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic

01:38:20.180 --> 01:38:27.260
components, so the lemma of a word or the identifying if the word is a specific type

01:38:27.340 --> 01:38:32.300
of an entity, that's when you're going to want to use the matcher over rejects.

01:38:32.300 --> 01:38:36.380
And when you're going to use rejects is when you really have a complicated pattern that

01:38:36.380 --> 01:38:38.540
you need to extract.

01:38:38.540 --> 01:38:43.380
And that pattern is not dependent upon specific parts of speech, you're going to see with

01:38:43.380 --> 01:38:47.820
that how that works as we kind of go through the rest of part two, but keep that in the

01:38:47.820 --> 01:38:49.220
back of your mind.

01:38:49.220 --> 01:38:54.100
So let's go ahead and take our work over to our blank Jupiter notebook again.

01:38:54.100 --> 01:38:56.780
So what we're going to do is we're going to just set up with a basic example.

01:38:56.780 --> 01:38:58.780
We need to import spacey.

01:38:58.780 --> 01:39:04.780
And since we're working with the matcher, we also need to say from spacey dot matcher,

01:39:04.780 --> 01:39:10.020
import matcher with a capital M, very important capital M.

01:39:10.020 --> 01:39:13.900
Once we have this loaded up, we can start actually working with the matcher.

01:39:13.900 --> 01:39:20.260
And we're going to be putting the matcher in a just the small English model.

01:39:20.260 --> 01:39:24.100
And we're going to say NLP is equal to spacey dot load.

01:39:24.100 --> 01:39:29.580
And you should be getting familiar with this in core web SM, the small English model.

01:39:29.580 --> 01:39:34.780
Once we've got that loaded, and we do now, we can start actually working with the matcher.

01:39:34.780 --> 01:39:36.380
So how do you create the matcher?

01:39:36.380 --> 01:39:39.900
Well, the Pythonic way to do this and the weights in the documentation is to call the

01:39:39.900 --> 01:39:44.740
object a matcher, that's going to be equal to matcher with a capital M. So we're calling

01:39:44.740 --> 01:39:47.580
this class right here.

01:39:47.580 --> 01:39:50.980
And now what we need to do is we need to pass in one argument.

01:39:50.980 --> 01:39:54.100
This is going to be NLP dot vocab.

01:39:54.100 --> 01:39:57.500
We're going to see that we can add in some extra features here in just a little bit.

01:39:57.500 --> 01:40:00.740
I'm going to show you why you want to add in extra features at this stage, but we're

01:40:00.740 --> 01:40:02.580
going to ignore that for right now.

01:40:02.580 --> 01:40:07.220
What we're going to try to do is we're going to try to find email addresses within a text,

01:40:07.220 --> 01:40:11.300
a very simple task that's really not that difficult to do.

01:40:11.300 --> 01:40:15.380
We can do it with a very simple pattern because spacey has given us that ability.

01:40:15.380 --> 01:40:17.820
So let's create a pattern.

01:40:17.820 --> 01:40:25.980
And that's going to be equal to a list, which is going to contain a dictionary.

01:40:25.980 --> 01:40:33.860
The first item in the dictionary, or the first key, is going to be the thing that you're

01:40:33.860 --> 01:40:35.060
looking for.

01:40:35.060 --> 01:40:38.580
So in this case, we have a bunch of different things that the matcher can look for.

01:40:38.580 --> 01:40:40.540
And I'm going to be talking about all those in just a second.

01:40:40.540 --> 01:40:45.540
But one of them is very handily, this label of like email.

01:40:45.540 --> 01:40:51.700
So if the if the string or the sequence of tokens or the token is looking like an email,

01:40:51.700 --> 01:40:58.580
and that's true, then that is what we want to extract, we want to extract everything that

01:40:58.580 --> 01:40:59.900
looks like an email.

01:40:59.900 --> 01:41:04.540
And to make sure that this occurs, we're going to say matcher dot add.

01:41:04.540 --> 01:41:09.180
And then here, we're going to pass in two arguments, argument one is going to be the

01:41:09.180 --> 01:41:13.840
think of it as a label that we want to assign to it.

01:41:13.840 --> 01:41:18.480
And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll

01:41:18.480 --> 01:41:20.000
see in just a second.

01:41:20.000 --> 01:41:23.520
And the next thing is a pattern.

01:41:23.520 --> 01:41:27.200
And it's important here to note that this is a list.

01:41:27.200 --> 01:41:30.880
The argument here takes a list of lists.

01:41:30.880 --> 01:41:35.400
And because this is just one list right now, I'm making it into a list.

01:41:35.400 --> 01:41:41.680
So each one of these different patterns would be a list within a list, essentially the let's

01:41:41.680 --> 01:41:43.920
go ahead and execute that.

01:41:43.920 --> 01:41:49.160
And now we're going to say doc is equal to nlp.

01:41:49.160 --> 01:41:57.320
And I'm going to add in a text that I have in the textbook.

01:41:57.320 --> 01:42:01.600
And this is my email address w mattingly at aol.com.

01:42:01.600 --> 01:42:02.840
That might be a real email address.

01:42:02.840 --> 01:42:04.920
I don't believe it is, it's definitely not mine.

01:42:04.920 --> 01:42:07.360
So don't try and email it.

01:42:07.360 --> 01:42:11.200
And then we're going to say matches is equal to matcher doc.

01:42:11.240 --> 01:42:13.760
This is going to be how we find our matches.

01:42:13.760 --> 01:42:20.040
We pass that doc object into our matcher class.

01:42:20.040 --> 01:42:23.520
And now what we have is the ability to print off our matches.

01:42:23.520 --> 01:42:25.760
And what we get is a list.

01:42:25.760 --> 01:42:30.400
And this list is a set of tuples that will always have three indices.

01:42:30.400 --> 01:42:34.440
So index zero is going to be this very long number.

01:42:34.440 --> 01:42:40.600
What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook.

01:42:40.600 --> 01:42:44.880
And the next thing is the start token and the end token.

01:42:44.880 --> 01:42:47.600
So you might be seeing the importance here already.

01:42:47.600 --> 01:42:53.840
What we can do with this is we can actually go into the nlp vocab where this integer lies

01:42:53.840 --> 01:42:56.080
and find what it corresponds to.

01:42:56.080 --> 01:42:58.000
So this is where this is pretty cool.

01:42:58.000 --> 01:42:59.280
Check this out.

01:42:59.280 --> 01:43:02.000
So you print off nlp dot vocab.

01:43:02.000 --> 01:43:04.720
So we're going into that vocab object.

01:43:04.720 --> 01:43:08.600
We're going to index it matches zero.

01:43:08.600 --> 01:43:13.800
So this is going to be the first index, so this tuple at this point.

01:43:13.800 --> 01:43:15.960
And then we're going to grab index zero.

01:43:15.960 --> 01:43:18.240
So now we've gone into this list.

01:43:18.240 --> 01:43:24.480
We've gone to index zero, this first tuple, and now we're grabbing that first item there.

01:43:24.480 --> 01:43:30.800
Now what we need to do is we need to say dot text, you need to do it right here.

01:43:30.800 --> 01:43:37.040
And if you print this off, we get this email address, that label that we gave it up there

01:43:37.040 --> 01:43:42.440
was added into the nlp vocab with this unique lexeme that allows for us to understand what

01:43:42.440 --> 01:43:47.400
that number corresponds to within the nlp framework.

01:43:47.400 --> 01:43:54.120
So this is a very simple example of how a matcher works and how you can use it to do

01:43:54.120 --> 01:43:56.040
some pretty cool things.

01:43:56.040 --> 01:44:01.120
But let's take a moment, let's pause and let's see what we can do with this matcher.

01:44:01.120 --> 01:44:06.360
So if we go up into spacey's documentation on the matcher, we'll see that you got a couple

01:44:06.360 --> 01:44:08.000
different attributes you can work with.

01:44:08.000 --> 01:44:10.760
Now we've, we're going to be seeing this a little bit.

01:44:10.760 --> 01:44:14.560
The orth, this is the exact verbatim of a token.

01:44:14.560 --> 01:44:20.160
And we're also going to see text, the exact verbatim, text of a token.

01:44:20.160 --> 01:44:21.920
What we also have is lower.

01:44:21.920 --> 01:44:27.800
So what you can do here is you can use lower to say when the item is lowercase and it looks

01:44:27.800 --> 01:44:31.040
like and then give some lowercase pattern.

01:44:31.040 --> 01:44:35.720
This is going to be very useful for capturing things that might be at the start of a sentence.

01:44:35.720 --> 01:44:42.440
For example, if you were to look for the penguin in the text, anywhere you saw the penguin.

01:44:42.440 --> 01:44:47.760
If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the

01:44:47.760 --> 01:44:49.040
start of a sentence.

01:44:49.040 --> 01:44:51.560
It would miss it because the T would be capitalized.

01:44:51.560 --> 01:44:55.640
By using lower, you can ensure that your pattern that you're giving it is going to be looking

01:44:55.640 --> 01:45:00.360
for any pattern that matches that when the text is lowercase.

01:45:00.360 --> 01:45:07.760
If is going to be the, the length of your token text is alpha is ASCII is digit.

01:45:07.760 --> 01:45:11.560
This is when your characters are either going to be alphabetical ASCII characters.

01:45:11.560 --> 01:45:16.640
So the American standard coding initiative, I can't remember what it stands for, but it's

01:45:16.640 --> 01:45:22.920
that, I think it's 128 bit thing that America came up with when they started in coding text.

01:45:22.920 --> 01:45:27.200
It's now replaced with UTF eight and is digit is going to look for something if it is a

01:45:27.200 --> 01:45:28.200
digit.

01:45:28.200 --> 01:45:29.400
So think of each of these as a token.

01:45:29.400 --> 01:45:34.800
So if the token is a digit, then that counts in the pattern is lower is upper is title.

01:45:34.800 --> 01:45:36.360
These should be all self explanatory.

01:45:36.360 --> 01:45:40.800
If it's lowercase, if it's uppercase, if it's a title, so capitalized.

01:45:40.800 --> 01:45:44.000
And if you don't understand what all of these do right now, I'm going to be going through

01:45:44.000 --> 01:45:47.640
and showing you in just a second, just giving you an overview of different things that can

01:45:47.640 --> 01:45:52.640
be included within the, the matcher or the entity ruler here.

01:45:52.640 --> 01:45:59.240
So what we can also do is find something that if the token is actually the start of a sentence,

01:45:59.240 --> 01:46:03.400
if it's like a number, like a URL, like an email, you can extract it.

01:46:03.400 --> 01:46:07.000
And here is the main part I want to talk about because this is where you're really going

01:46:07.000 --> 01:46:12.520
to find spacey out shines any other string matching system out there.

01:46:12.520 --> 01:46:17.800
So what you can do is you can use the tokens, part of speech tag, morphological analysis,

01:46:17.800 --> 01:46:22.360
dependency label, lima and shape to actually make matches.

01:46:22.360 --> 01:46:27.380
So not just matching a sequence of characters, but matching a sequence of linguistic features.

01:46:27.380 --> 01:46:28.660
So think about this.

01:46:28.660 --> 01:46:33.900
If you wanted to capture all instances of a proper noun followed by a verb, you would

01:46:33.900 --> 01:46:36.620
not be able to do that with regex.

01:46:36.620 --> 01:46:37.720
There's not a way to do it.

01:46:37.720 --> 01:46:40.100
You can't give regex if this is a verb.

01:46:40.100 --> 01:46:42.320
Regex is just a string matching framework.

01:46:42.320 --> 01:46:46.500
It's not a framework for actually identifying linguistic features, using them and extracting

01:46:46.500 --> 01:46:47.500
them.

01:46:47.500 --> 01:46:52.220
So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological

01:46:52.220 --> 01:46:56.900
analysis, the depth, the lemma, et cetera.

01:46:57.540 --> 01:47:01.940
We can actually use all those things that have gone through the pipeline and the matcher

01:47:01.940 --> 01:47:07.460
can leverage those linguistic features and make some really cool, allow us to make really

01:47:07.460 --> 01:47:11.380
cool patterns that can match really robust and complicated things.

01:47:11.380 --> 01:47:14.700
And the final thing I'm going to talk about is right here, the OP.

01:47:14.700 --> 01:47:19.340
This is the operator or quantifier and determines how often to match a token.

01:47:19.340 --> 01:47:20.980
So there's a few different things you can use here.

01:47:20.980 --> 01:47:26.020
There's the exclamation mark, negate the pattern, requiring it to match zero times.

01:47:26.060 --> 01:47:29.260
So in this scenario, the sequence would never occur.

01:47:29.260 --> 01:47:33.980
There's the question mark, make the pattern optional, allowing it to match zero or one

01:47:33.980 --> 01:47:39.580
times require the pattern to match one or more times with the plus and the asterisk,

01:47:39.580 --> 01:47:43.500
the thing on the shift eight, allow the pattern to match zero or more times.

01:47:43.500 --> 01:47:47.620
There's other things as well that you can do to make this match or a bit more robust.

01:47:47.620 --> 01:47:51.700
But for right now, let's jump into the basics and see how we can really kind of take these

01:47:51.700 --> 01:47:55.580
and apply them in a real world question.

01:47:55.580 --> 01:48:00.580
So what I'm going to do is I'm going to work with another data set or another piece of

01:48:00.580 --> 01:48:02.980
data that I've grabbed off of Wikipedia.

01:48:02.980 --> 01:48:07.100
And this is the Wikipedia article entry on Martin Luther King, Jr.

01:48:07.100 --> 01:48:12.820
It's the opening opening few paragraphs, let's print it off and just take a quick look.

01:48:12.820 --> 01:48:13.820
And this is what it looks like.

01:48:13.820 --> 01:48:15.020
You can go through and read it.

01:48:15.020 --> 01:48:17.820
We're not too concerned about what it says right now.

01:48:17.820 --> 01:48:21.660
We're concerned about trying to extract a very specific set of patterns.

01:48:21.660 --> 01:48:25.060
What we're interested in grabbing are all proper nouns.

01:48:25.060 --> 01:48:26.540
That's the task ahead of us.

01:48:26.540 --> 01:48:32.060
Somebody has asked us to take this text in, extract all the proper nouns for me, but we're

01:48:32.060 --> 01:48:36.740
going to do a lot more and not just the proper nouns, but we want to get multi word tokens.

01:48:36.740 --> 01:48:43.300
So we want to have Martin Luther King, Jr. extracted as one token, so one export.

01:48:43.300 --> 01:48:49.020
So the other things that we want to have are these kind of structured in sequential order.

01:48:49.020 --> 01:48:54.260
So find out where they appear and extract them based on their start token.

01:48:54.260 --> 01:48:57.660
So let's go ahead and start trying to do some of these things right now.

01:48:57.660 --> 01:48:59.180
Let's scroll down here.

01:48:59.180 --> 01:49:00.220
Great.

01:49:00.220 --> 01:49:04.420
So we need to create really a new NLP object now at this point.

01:49:04.420 --> 01:49:05.420
So let's create a new one.

01:49:05.420 --> 01:49:10.760
We're going to start working with the Ncore Web SM model.

01:49:10.760 --> 01:49:15.300
If you're working with a different model, like the large or the transformer, you're

01:49:15.300 --> 01:49:17.420
going to have more accurate results.

01:49:17.420 --> 01:49:21.460
But for right now, we're just trying to do this quickly for demonstration purposes.

01:49:21.460 --> 01:49:26.780
So again, just like before, we're creating that with NLP dot vocab.

01:49:26.780 --> 01:49:29.060
And then we're going to create a pattern.

01:49:29.060 --> 01:49:31.380
So this is the pattern that we're going to work with.

01:49:31.380 --> 01:49:38.980
We want to find any occurrence of a POS part of speech that corresponds to proper noun.

01:49:38.980 --> 01:49:45.220
That's the way in which POS labels proper nouns is prop in.

01:49:45.220 --> 01:49:48.340
And we should be able to with that extract all proper nouns.

01:49:48.340 --> 01:49:54.540
So we can say matcher dot add, and we're going to say proper noun.

01:49:54.540 --> 01:49:58.060
And that's going to be our pattern.

01:49:58.060 --> 01:50:00.820
And then what we can do just like before, we're going to create the doc object.

01:50:00.820 --> 01:50:04.220
This is going to be NLP text.

01:50:04.220 --> 01:50:08.300
And then we're going to say matches is equal to matcher doc.

01:50:08.300 --> 01:50:14.420
So we're going to create the matches by passing that doc object into our matcher class.

01:50:14.420 --> 01:50:17.860
And then we're going to print off the length of the matches.

01:50:17.860 --> 01:50:22.540
So how many matches were found, and then we're going to say for match in matches.

01:50:22.540 --> 01:50:25.420
And we're just going to grab the first 10 because I've done this and there's a lot

01:50:25.420 --> 01:50:30.940
and you'll see why let's print off.

01:50:30.940 --> 01:50:33.380
Let's print off in this case, match.

01:50:33.380 --> 01:50:36.260
And then we're going to print off specifically what that text is.

01:50:36.260 --> 01:50:41.900
Remember, the output is the lexine followed by the start token and the end token, which

01:50:41.900 --> 01:50:44.420
means we can go into the doc object.

01:50:44.420 --> 01:50:46.260
And we can set up something like this.

01:50:46.260 --> 01:50:53.820
We can say match one, so index one, which is the start token and match two, which is

01:50:53.820 --> 01:50:55.020
the end token.

01:50:55.020 --> 01:50:58.620
And that'll allow us to actually index what these words are.

01:50:58.620 --> 01:51:00.900
And when we do this, we can see all these printed out.

01:51:00.900 --> 01:51:07.140
So this is the match, the lexine here, which is going to be proper down all the way down.

01:51:07.140 --> 01:51:13.140
We've got the zero here, which corresponds to the start token, the end token.

01:51:13.140 --> 01:51:15.060
And this is the the token that we extracted.

01:51:15.060 --> 01:51:21.020
Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right?

01:51:21.020 --> 01:51:23.780
So the problem should be pretty obvious right now.

01:51:23.780 --> 01:51:31.900
And the problem is that we have grabbed all proper nouns, but these proper nouns are just

01:51:31.900 --> 01:51:33.540
individual tokens.

01:51:33.540 --> 01:51:36.220
We haven't grabbed the multi word tokens.

01:51:36.220 --> 01:51:37.780
So how do we go about doing that?

01:51:37.780 --> 01:51:42.180
Well, we can solve this problem by let's go ahead and just copy and paste all this from

01:51:42.180 --> 01:51:43.180
here.

01:51:43.180 --> 01:51:47.540
And we're going to make one small adjustment here.

01:51:47.540 --> 01:51:53.940
We're going to change this to OP with a plus.

01:51:53.940 --> 01:51:55.180
So what does that mean?

01:51:55.180 --> 01:51:59.740
Well, let's pop back into our matcher under spacey and check it out.

01:51:59.740 --> 01:52:05.800
So OP members, the operator or quantifier, we're going to use the plus symbol.

01:52:05.800 --> 01:52:12.560
So it's going to look for a proper noun that occurs one or more times.

01:52:12.560 --> 01:52:16.560
So in theory, right, this should allow us to grab multi word tokens.

01:52:16.560 --> 01:52:19.600
It's going to look for a proper noun and grab as many as there are.

01:52:19.600 --> 01:52:24.440
So anything that occurs one or more times, if we run this, though, we see a problem.

01:52:24.440 --> 01:52:28.560
We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther

01:52:28.560 --> 01:52:33.000
King, Luther King, King Martin Luther King, Junior, what what is going on here?

01:52:33.000 --> 01:52:35.560
Well, you might already have figured it out.

01:52:35.560 --> 01:52:38.320
It has done exactly what we told it to do.

01:52:38.320 --> 01:52:42.800
It's grabbed all sequence of tokens that were proper nouns that occurred one or more

01:52:42.800 --> 01:52:44.440
times.

01:52:44.440 --> 01:52:46.280
Just so happens some of these overlap.

01:52:46.280 --> 01:52:50.960
So token that's doc zero to one, zero to two.

01:52:50.960 --> 01:52:56.360
So you can see the problem here is it's grabbing all of these and any combination of them.

01:52:56.360 --> 01:53:00.400
What we can do, though, is we can add an extra layer to this.

01:53:00.400 --> 01:53:04.560
So let's again, copy what we've just done because it was, it was almost there.

01:53:04.560 --> 01:53:06.720
It was good, but it wasn't great.

01:53:06.720 --> 01:53:11.080
We're going to do one new thing here when we add in the patterns, we're going to pass

01:53:11.080 --> 01:53:17.480
in the keyword argument, greedy, we're going to say longest capital, all capital letters

01:53:17.480 --> 01:53:18.480
here.

01:53:18.480 --> 01:53:22.560
And if we execute that, it's going to look for the longest token out of that mix, and

01:53:22.560 --> 01:53:26.720
it's going to give that one, make that one the only token that it extracts.

01:53:26.720 --> 01:53:33.560
We noticed that our length has changed from what was it up here, 175 to 61.

01:53:33.560 --> 01:53:34.880
So this is much better.

01:53:34.880 --> 01:53:39.720
However, we should have recognized right now, another problem.

01:53:39.720 --> 01:53:40.960
What have we done wrong?

01:53:40.960 --> 01:53:45.240
Well, what we've done wrong is these are all out of order.

01:53:45.240 --> 01:53:48.880
In fact, what happens is when you do this, I don't have evidence to support this, but

01:53:48.880 --> 01:53:51.360
I believe it's right.

01:53:51.360 --> 01:53:56.200
What will always happen is the, the greedy longest will result in all of your tokens

01:53:56.200 --> 01:54:01.360
being organized or all your matches being organized from longest to shortest.

01:54:01.360 --> 01:54:06.320
So if we were to scroll down the list and look at maybe negative one, negative, let's

01:54:06.320 --> 01:54:10.800
do negative 10 on, you'll see single word tokens.

01:54:10.800 --> 01:54:14.200
And again, this is me just guessing, but I think based on what you've just seen, that's

01:54:14.200 --> 01:54:16.080
a fairly good guess.

01:54:16.080 --> 01:54:19.080
So let's go ahead and just kind of so we can see what the output is here.

01:54:19.080 --> 01:54:22.720
So how would you go about organizing these sequentially?

01:54:22.720 --> 01:54:29.840
Well, this is where really kind of a sort comes in handy when you can pass a lambda to

01:54:29.840 --> 01:54:30.840
it.

01:54:30.840 --> 01:54:35.240
I can copy all this again, because again, we almost had this right.

01:54:35.240 --> 01:54:41.160
Here we're going to sort our matches though, we can say matches.sort, and this is going

01:54:41.160 --> 01:54:46.600
to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb

01:54:46.600 --> 01:54:53.600
is going to allow us to actually iterate over all this and find any instance where X occurs.

01:54:53.600 --> 01:54:56.320
And we're going to say to sort by X one.

01:54:56.320 --> 01:54:58.520
So what this is, it's a list of tuples.

01:54:58.520 --> 01:55:02.760
And what we're using lambda for is we're going to say sort this whole list of tuples out,

01:55:02.760 --> 01:55:07.520
but sort it by the first index, in other words, sort it by the start token.

01:55:07.520 --> 01:55:12.520
And when we execute that, we've got everything now coming out as we would expect and nor

01:55:12.520 --> 01:55:14.600
these typos that exist.

01:55:14.600 --> 01:55:17.880
We've got zero to four, six to nine.

01:55:17.880 --> 01:55:23.040
So we actually are extracting these things in sequential order as they appear in our

01:55:23.040 --> 01:55:24.040
text.

01:55:24.040 --> 01:55:30.000
You can actually go through and sort the appearance of the, of the matcher.

01:55:30.000 --> 01:55:35.600
But what if our, the person who kind of gave us this job, they were happy with this, but

01:55:35.600 --> 01:55:37.160
they came back and said, okay, that's cool.

01:55:37.160 --> 01:55:41.440
But what we're really interested in what we really want to know is every instance where

01:55:41.440 --> 01:55:48.520
a proper noun of any length, grab the multi word token still, but we want to know anytime

01:55:48.520 --> 01:55:50.560
that occurs after a verb.

01:55:50.560 --> 01:55:53.600
So anytime this proper noun is followed by a verb.

01:55:53.600 --> 01:55:56.480
So what we can do is we can add in, okay, okay, we can do this.

01:55:56.480 --> 01:55:57.800
We're going to have a comma here.

01:55:57.800 --> 01:56:00.520
So the same pattern is going to be a sequence now.

01:56:00.520 --> 01:56:02.760
It's not just going to be one thing.

01:56:02.760 --> 01:56:08.040
We're going to say token one needs to be a proper noun and grab as many of those tokens

01:56:08.040 --> 01:56:11.720
as you can zero or one to more times.

01:56:11.720 --> 01:56:17.360
And then after those are done comma, this is where the next thing has to occur POS.

01:56:17.360 --> 01:56:20.440
So the part of speech needs to be a verb.

01:56:20.440 --> 01:56:24.000
So the next thing that comes out needs to be a verb.

01:56:24.000 --> 01:56:25.680
And we want that to be the case.

01:56:25.680 --> 01:56:29.840
Well, when we do this, we can kind of go through and see the results of the first instance

01:56:29.840 --> 01:56:38.040
of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced

01:56:38.040 --> 01:56:42.280
258 director J Edgar Hoover considered.

01:56:42.280 --> 01:56:49.000
Now we're able to use those linguistic features that make Spacey amazing and actually extract

01:56:49.000 --> 01:56:51.160
some vital information.

01:56:51.160 --> 01:56:57.840
So we've been able to figure out where in this text a a proper noun is proceeded by

01:56:57.840 --> 01:56:58.840
a verb.

01:56:58.840 --> 01:57:02.480
So you can already start to probably see the implications here.

01:57:02.480 --> 01:57:06.120
And we can we can create very elaborate things with this.

01:57:06.120 --> 01:57:09.880
We can use any of these as long of a sequence as you can imagine.

01:57:09.880 --> 01:57:13.640
We're going to work with a different text and kind of demonstrate that it's a fun toy

01:57:13.640 --> 01:57:14.640
example.

01:57:14.640 --> 01:57:20.240
We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file.

01:57:20.240 --> 01:57:23.160
I'm going to load it in right now.

01:57:23.160 --> 01:57:29.400
And then I'm going to just grab the first sentence from the first chapter.

01:57:29.400 --> 01:57:32.000
And what we have here is the first sentence.

01:57:32.000 --> 01:57:33.800
So here's our scenario.

01:57:33.800 --> 01:57:41.320
Somebody has asked us to grab all the quotation marks and try to identify the person described

01:57:41.320 --> 01:57:45.320
or the person described the person who's doing the speaking or the thinking.

01:57:45.320 --> 01:57:48.560
In other words, we want to be able to grab Alice thought.

01:57:48.560 --> 01:57:54.120
Now I picked Alice in Wonderland because of the complexity of the text, not complexity

01:57:54.120 --> 01:57:59.680
in the sense of the language used children's book, but complexity and the syntax.

01:57:59.680 --> 01:58:05.720
These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly

01:58:05.720 --> 01:58:10.040
inconsistent in how we structured these kind of sequences of quotes.

01:58:10.040 --> 01:58:14.680
And the other thing I chose to do as I left in one mistake here, and that is this non

01:58:14.680 --> 01:58:16.680
standardized quotation mark.

01:58:16.680 --> 01:58:19.820
So remember, when you need to do this, things need to match perfectly.

01:58:19.820 --> 01:58:24.360
So we're going to replace this first things first is to create a cleaner text, or we do

01:58:24.360 --> 01:58:29.160
text equals text dot replace, and we're going to replace the instance of I believe it's

01:58:29.160 --> 01:58:33.400
that mark, but let's just copy and paste it in to make sure we're going to replace that

01:58:33.400 --> 01:58:39.240
with a, with a single quotation mark, and we can print off text just to make sure that

01:58:39.400 --> 01:58:40.400
was done correctly.

01:58:40.400 --> 01:58:41.400
Cool.

01:58:41.400 --> 01:58:42.400
Great.

01:58:42.400 --> 01:58:43.400
It was it's now looking good.

01:58:43.400 --> 01:58:47.880
Remember, whenever you're doing information extraction, standardize the texts as much as

01:58:47.880 --> 01:58:49.080
possible.

01:58:49.080 --> 01:58:53.720
Things like quotation marks will always throw off your data.

01:58:53.720 --> 01:59:00.480
Now that we've got that, let's go ahead and start trying to create a fairly robust pattern

01:59:00.480 --> 01:59:07.640
to try to grab all instances where there is a quotation mark, thought, something like

01:59:07.720 --> 01:59:11.480
this, and then followed by another quotation mark.

01:59:11.480 --> 01:59:15.760
So the first thing I'm going to try and do is I'm going to try to just capture all quotation

01:59:15.760 --> 01:59:17.440
marks and a text.

01:59:17.440 --> 01:59:21.520
So let's go through and try to figure out how to do that right now.

01:59:21.520 --> 01:59:25.480
So we're going to copy in a lot of the same things that we used up above, but we're going

01:59:25.480 --> 01:59:27.040
to make some modifications to it.

01:59:27.040 --> 01:59:32.640
Let's go ahead and copy and paste all that we're going to completely change our pattern.

01:59:32.640 --> 01:59:34.040
So let's get rid of this.

01:59:34.040 --> 01:59:35.400
So what are we looking for?

01:59:35.400 --> 01:59:41.240
Well, first of all, the first thing that's going to occur in this pattern is this quotation

01:59:41.240 --> 01:59:42.080
mark.

01:59:42.080 --> 01:59:47.320
So that's going to be a full text match, which is an or if you remember, and we're going

01:59:47.320 --> 01:59:51.600
to have to use double quotation marks to add in that single quotation mark.

01:59:51.600 --> 01:59:52.720
So that's what we grabbed first.

01:59:52.720 --> 01:59:57.160
We're going to look for anything that is an or and the next thing that's going to occur

01:59:57.160 --> 02:00:02.520
after that, I think this is good to probably do this now on a line by line basis.

02:00:02.520 --> 02:00:05.120
So we can keep this straight.

02:00:05.120 --> 02:00:08.400
So the next thing that's going to occur is we're looking for anything in between.

02:00:08.400 --> 02:00:14.240
So anything that is an alpha character, we're going to just grab it all.

02:00:14.240 --> 02:00:22.160
So is alpha and then we need to say true.

02:00:22.160 --> 02:00:26.960
But within this, we need to specify how many times that occurs because if we say is true,

02:00:26.960 --> 02:00:32.600
it's just going to look at the next token in this case and and then say that's the end.

02:00:32.600 --> 02:00:33.600
That's it.

02:00:33.600 --> 02:00:34.600
That's the pattern.

02:00:35.080 --> 02:00:40.880
But we want to grab not just and but and what is the use of a everything.

02:00:40.880 --> 02:00:47.280
So we need to grab not only that, but when you say OP, so our operator again.

02:00:47.280 --> 02:00:50.280
And if you said plus, you would be right here.

02:00:50.280 --> 02:00:53.960
We need to make sure that it's a plus sign, so it's grabbing everything.

02:00:53.960 --> 02:00:59.720
Now in this scenario, this is a common construct is when you have a injection here in the middle

02:00:59.720 --> 02:01:00.720
of the sentence.

02:01:00.720 --> 02:01:03.180
So thought or said, and it's the character doing it.

02:01:03.180 --> 02:01:06.020
This oftentimes got a comma right here.

02:01:06.020 --> 02:01:08.580
So we need to add in that kind of a feature.

02:01:08.580 --> 02:01:12.380
So there could be is punked.

02:01:12.380 --> 02:01:15.220
There could be a punked here.

02:01:15.220 --> 02:01:18.760
And we're going to say that that is equal to true.

02:01:18.760 --> 02:01:21.300
But that might not always be the case.

02:01:21.300 --> 02:01:23.180
There might not always be one there.

02:01:23.180 --> 02:01:28.300
So we're going to say OP is equal to a star.

02:01:28.300 --> 02:01:29.300
We go back.

02:01:29.300 --> 02:01:30.300
We'll see why.

02:01:30.300 --> 02:01:35.300
To our OP, the star allow the pattern to match zero or more time.

02:01:35.300 --> 02:01:40.120
So in this scenario, the punctuation may or may not be there.

02:01:40.120 --> 02:01:41.860
So that's the next thing that occurs.

02:01:41.860 --> 02:01:46.820
Once we've got that, the last thing that we need to match is the exact same thing that

02:01:46.820 --> 02:01:51.980
we had at the start is this or appear.

02:01:51.980 --> 02:01:53.420
And that's our sequence.

02:01:53.420 --> 02:01:57.020
So this is going to look for anything that starts with a quotation mark has a series

02:01:57.020 --> 02:02:03.700
of alpha characters has a punctuation like a comma possibly, and then closes the quotation

02:02:03.700 --> 02:02:04.700
marks.

02:02:04.700 --> 02:02:07.580
If we execute this, we succeeded.

02:02:07.580 --> 02:02:08.580
We got it.

02:02:08.580 --> 02:02:11.580
We extracted both matches from that first sentence.

02:02:11.580 --> 02:02:13.820
There are no other quotation marks in there.

02:02:13.820 --> 02:02:17.300
But our task was not just to extract this information.

02:02:17.300 --> 02:02:22.340
Our task was also to match who is the speaker.

02:02:22.340 --> 02:02:25.580
Now we can do this in a few different ways and you're going to see why this is such a

02:02:25.580 --> 02:02:28.420
complicated problem in just a second.

02:02:28.420 --> 02:02:30.260
So let's go ahead and do this.

02:02:30.260 --> 02:02:32.060
How can we make this better?

02:02:32.060 --> 02:02:36.180
Well, we're going to have this occur twice.

02:02:36.180 --> 02:02:40.380
But in the middle, we need to figure out when somebody is speaking.

02:02:40.380 --> 02:02:43.180
So one of the things that we can do is we can make a list.

02:02:43.180 --> 02:02:49.240
So let's make a list of limitized forms of our verbs.

02:02:49.240 --> 02:02:53.980
So we're going to say, let's call this speak underscore limits.

02:02:53.980 --> 02:02:56.060
This can be equal to a list.

02:02:56.060 --> 02:02:59.500
And the first thing we're going to say is think, because we know that think is in there

02:02:59.500 --> 02:03:04.720
and say this is the limitized form of thought and said.

02:03:04.720 --> 02:03:08.440
So what we can do now is after that occurs, it's adding a new thing.

02:03:08.440 --> 02:03:12.060
We're going to be able to now add in a new pattern that we're looking for.

02:03:12.060 --> 02:03:17.700
And so not just the start of a quotation mark, not just the end of a quotation mark, but

02:03:17.700 --> 02:03:20.980
also a sequence that'll be something like this.

02:03:20.980 --> 02:03:22.980
So it's going to be a part of speech.

02:03:22.980 --> 02:03:26.380
So it's going to be a verb that occurs first, right?

02:03:26.380 --> 02:03:28.620
And that's going to be a verb.

02:03:28.620 --> 02:03:41.180
But more importantly, it's going to be a lemma that is in what did I call you speak lemmas?

02:03:41.180 --> 02:03:43.740
So let's break this down.

02:03:43.740 --> 02:03:48.180
The next token needs to be a verb.

02:03:48.180 --> 02:03:57.460
And it needs to have a limitized form that is contained within the speak lemmas list.

02:03:57.460 --> 02:04:01.620
So if it's got that fantastic, let's execute this and see what happens.

02:04:01.620 --> 02:04:03.620
We should only have one hit.

02:04:03.620 --> 02:04:04.620
Cool.

02:04:04.620 --> 02:04:05.620
We do.

02:04:05.620 --> 02:04:06.620
So we've got that first hit.

02:04:06.620 --> 02:04:10.020
And the second one hasn't appeared anymore because that second quotation mark wasn't

02:04:10.020 --> 02:04:12.980
proceeded by a verb.

02:04:12.980 --> 02:04:16.660
Let's go ahead and make some modifications that we can improve this a little bit.

02:04:16.660 --> 02:04:20.860
Because we want to know not just what that person's doing.

02:04:20.860 --> 02:04:22.980
We also need to know who the speaker is.

02:04:22.980 --> 02:04:24.940
So let's grab it.

02:04:24.940 --> 02:04:26.780
Let's figure out who that speaker is.

02:04:26.780 --> 02:04:28.820
So we can use part of speech.

02:04:28.820 --> 02:04:31.020
Again, another feature here.

02:04:31.020 --> 02:04:34.780
We know that it's going to be a proper noun because oftentimes proper nouns are doing

02:04:34.780 --> 02:04:36.300
the speaking.

02:04:36.300 --> 02:04:37.660
Sometimes it might not be.

02:04:37.660 --> 02:04:41.800
Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those

02:04:41.800 --> 02:04:44.360
situations for just right now.

02:04:44.360 --> 02:04:46.000
So we're looking for a proper noun.

02:04:46.000 --> 02:04:51.480
But remember proper nouns, as we saw just a second ago, could be multiple tokens.

02:04:51.480 --> 02:04:53.840
So we're going to say OP plus.

02:04:53.840 --> 02:04:55.920
So it could be a sequence of tokens.

02:04:55.920 --> 02:04:57.680
Let's execute this.

02:04:57.680 --> 02:04:59.960
Now we've captured Alice here as well.

02:04:59.960 --> 02:05:04.000
So and is the use and what is the use of a book thought Alice.

02:05:04.000 --> 02:05:08.120
Now we know who the speaker is, but this is a partial quotation.

02:05:08.120 --> 02:05:09.240
This is not the whole thing.

02:05:09.240 --> 02:05:11.160
We need to grab the other quote.

02:05:11.160 --> 02:05:12.960
Oh, how will we ever do that?

02:05:12.960 --> 02:05:15.000
Well, we've already solved that.

02:05:15.000 --> 02:05:21.120
We can copy and paste all of this that we already have done right down here.

02:05:21.120 --> 02:05:26.400
And now we've successfully extracted that entire quote.

02:05:26.400 --> 02:05:28.640
So you might be thinking to yourself, yeah, we did it.

02:05:28.640 --> 02:05:36.600
We can now extract quotation marks and we can even extract, extract, you know, any instance

02:05:36.600 --> 02:05:39.800
where there's a quote and somebody speaking.

02:05:39.800 --> 02:05:40.800
Not so fast.

02:05:40.800 --> 02:05:42.360
We're going to try to iterate over this data.

02:05:42.360 --> 02:05:50.040
So we're going to say for text in data, zero twos, we're going to iterate over the first

02:05:50.040 --> 02:05:52.400
chapter.

02:05:52.400 --> 02:05:59.920
And we're going to go ahead and let's let's do all of this.

02:05:59.920 --> 02:06:07.600
Doc is going to be equal to that sort that out.

02:06:07.600 --> 02:06:13.040
And then again, we're going to be printing out this information, the same stuff I did

02:06:13.040 --> 02:06:16.360
before, just now it's going to be iterating over the whole chapter.

02:06:16.360 --> 02:06:23.080
And if we let this run, we've got a serious, serious problem.

02:06:23.080 --> 02:06:26.680
And it doesn't actually grab us anything.

02:06:26.680 --> 02:06:29.200
Nothing has been grabbed successfully.

02:06:29.200 --> 02:06:34.200
What is going on?

02:06:34.200 --> 02:06:35.840
We've got a problem.

02:06:35.840 --> 02:06:44.520
And that problem stems from the fact that our patterns and the problem is that we don't

02:06:44.520 --> 02:06:51.280
have our our text correctly, we're being removing the quotation mark that was the problem up

02:06:51.280 --> 02:06:52.280
above.

02:06:52.280 --> 02:06:55.640
So we're going to add this bit of code in.

02:06:55.640 --> 02:06:57.640
And we're going to be able to fix it.

02:06:57.640 --> 02:07:00.880
So now when we execute this, we see that we've only grabbed one match.

02:07:00.880 --> 02:07:03.720
Now you might be thinking to yourself, there's an issue here and there there is, let's go

02:07:03.720 --> 02:07:07.000
ahead and print off the length of matches.

02:07:07.000 --> 02:07:09.720
And we see that we've only grabbed one match.

02:07:09.720 --> 02:07:11.000
And then we haven't grabbed anything else.

02:07:11.000 --> 02:07:12.400
Well, what's the problem here?

02:07:12.400 --> 02:07:16.760
Are there are there no other instances of quotation marks in the rest of the first chapter?

02:07:16.760 --> 02:07:18.480
And the answer is no, there are.

02:07:18.480 --> 02:07:23.320
There absolutely are other quotation marks and other paragraphs from the first chapter.

02:07:23.320 --> 02:07:27.840
The problem is, is that our pattern is singular.

02:07:27.840 --> 02:07:29.280
It's not multivariate.

02:07:29.280 --> 02:07:35.560
We need to add in additional ways in which a text might be structured.

02:07:35.560 --> 02:07:40.800
So let's go ahead and try and do this with some more patterns.

02:07:40.800 --> 02:07:44.840
I'm going to go ahead and copy and paste these in from the textbook.

02:07:44.840 --> 02:07:50.000
So you'll be able to actually see them at work.

02:07:50.000 --> 02:07:54.320
And so what I've did, I've done is I've added in more patterns, pattern two and pattern

02:07:54.320 --> 02:07:58.920
three allow for instances like this, well thought Alice.

02:07:58.920 --> 02:08:03.160
So an instance where there's a punctuation, but there's no proceeding quotation after

02:08:03.160 --> 02:08:07.760
this, and then which certainly said before an instance where there's a comma followed

02:08:07.760 --> 02:08:08.760
by that.

02:08:08.760 --> 02:08:12.640
So we've been able to capture more variants and more ways in which quotation marks might

02:08:12.640 --> 02:08:14.920
exist followed by the speaker.

02:08:14.920 --> 02:08:18.320
Now this is where being a domain expert comes into play.

02:08:18.320 --> 02:08:21.440
You'd have to kind of look through and see the different ways that Louis C. Carroll

02:08:21.440 --> 02:08:25.640
structures quotation marks and write out patterns for capturing them.

02:08:25.640 --> 02:08:29.280
I'm not going to go through and try to capture everything from Alice in Wonderland because

02:08:29.280 --> 02:08:31.600
that would take a good deal of time.

02:08:31.600 --> 02:08:35.760
And it's not really in the best interest because it doesn't matter to me at all.

02:08:35.760 --> 02:08:39.400
What I encourage you to do, if this is something interesting to you is try to apply it to your

02:08:39.400 --> 02:08:44.640
own texts, different authors, structure quotation marks a little differently than what patterns

02:08:44.640 --> 02:08:47.560
that I've gotten written here are a good starting point.

02:08:47.560 --> 02:08:50.480
But I would encourage you to start playing around with them a little bit more.

02:08:50.480 --> 02:08:55.520
And what you can do is when you actually have this match extracted, you know that the

02:08:55.880 --> 02:09:01.960
instance of a proper noun that occurs between these quotation marks or after one is probably

02:09:01.960 --> 02:09:08.040
going to be the person or thing that is doing the speaking or the thinking.

02:09:08.040 --> 02:09:10.080
So that's kind of how the matcher works.

02:09:10.080 --> 02:09:15.680
It allows for you to do these things, these robust type data extractions without relying

02:09:15.680 --> 02:09:16.920
on entity ruler.

02:09:16.920 --> 02:09:21.000
And remember, you can use a lot of these same things with an entity ruler as well.

02:09:21.000 --> 02:09:25.400
But we don't want this in this case, we don't want things like this to be labeled as entities.

02:09:25.440 --> 02:09:29.320
We want them to just be separate things that we can extract outside of the of the

02:09:29.320 --> 02:09:30.960
ints dot doc dot ints.

02:09:31.320 --> 02:09:36.560
That's going to be where we conclude our chapter on on the on the matcher.

02:09:36.920 --> 02:09:40.920
In the next section of this video, we're going to be talking about custom components in

02:09:40.920 --> 02:09:46.880
spacey, which allow for us to do some pretty cool things such as add in special functions

02:09:46.880 --> 02:09:53.840
that allow for us to kind of do different custom shapes, permutations on our data with

02:09:54.520 --> 02:09:58.680
components that don't exist like an entity ruler would be a component components that

02:09:58.680 --> 02:10:00.880
don't exist within the spacey framework.

02:10:00.880 --> 02:10:06.760
So add in custom things like an entity ruler that do very specific things to your data.

02:10:09.080 --> 02:10:14.160
Hello, we're now moving into a more advanced aspect of the textbook specifically chapter

02:10:14.160 --> 02:10:14.760
seven.

02:10:14.760 --> 02:10:17.000
And that's working with custom components.

02:10:17.320 --> 02:10:21.240
A good way to think about a custom component is something that you need to do to the doc

02:10:21.240 --> 02:10:25.080
object or the doc container that spacey can't do off the shelf.

02:10:25.080 --> 02:10:27.880
You want to modify it at some point in the pipeline.

02:10:28.200 --> 02:10:32.560
So I'm going to use a basic toy example that demonstrates the power of this.

02:10:32.840 --> 02:10:35.720
Let's look at this basic example that I've already loaded into memory.

02:10:35.920 --> 02:10:38.760
It's two sentences that are in the doc object now.

02:10:39.440 --> 02:10:40.640
And that's Britain is a place.

02:10:40.840 --> 02:10:42.000
Mary is a doctor.

02:10:42.360 --> 02:10:49.080
So let's do for int and doc dot ints print off int dot text and dot label.

02:10:50.080 --> 02:10:51.520
And we see what we'd expect.

02:10:51.520 --> 02:10:54.520
Britain is GPE a geopolitical entity.

02:10:54.760 --> 02:10:56.400
Mary is a person.

02:10:57.120 --> 02:10:58.000
That's fantastic.

02:10:58.280 --> 02:11:04.560
But I've just been told by somebody higher up that they want the model to never ever

02:11:04.560 --> 02:11:11.480
give anything as GPE or maybe they want any instance of GPE to be flagged as LOC.

02:11:12.280 --> 02:11:17.640
So all the different locations all have LOC as a label or we just want to remove them

02:11:17.640 --> 02:11:18.240
entirely.

02:11:18.920 --> 02:11:20.680
So I'm going to work with that latter example.

02:11:21.000 --> 02:11:27.760
We need to create a custom pipe that removes all instances of GPE from the doc dot

02:11:27.800 --> 02:11:29.040
ints container.

02:11:29.320 --> 02:11:30.080
So how do we do that?

02:11:30.120 --> 02:11:32.880
Well, we need to use a custom component.

02:11:33.200 --> 02:11:38.720
We can do this very easily in spacey by saying from spacey dot language import

02:11:39.080 --> 02:11:41.240
language capital L very important there.

02:11:41.240 --> 02:11:44.600
Capital L now that we've got that class loaded up.

02:11:45.080 --> 02:11:46.320
Let's start working with this.

02:11:46.320 --> 02:11:49.280
What we need to do first is we need to use a flag.

02:11:49.280 --> 02:11:53.520
So the symbol and we need to say at language dot component.

02:11:54.480 --> 02:11:56.280
And we need to give that component a name.

02:11:56.720 --> 02:12:00.160
We're going to say in this case, let's say remove GPE.

02:12:01.480 --> 02:12:03.560
And now we need to create a function to do this.

02:12:03.920 --> 02:12:07.200
So we're going to call this remove GPE.

02:12:07.400 --> 02:12:09.240
I always kind of keep these as the same.

02:12:09.680 --> 02:12:11.280
That's my personal preference.

02:12:11.760 --> 02:12:15.040
And this is going to take one, one, one thing.

02:12:15.080 --> 02:12:16.400
That's going to be the doc object.

02:12:16.680 --> 02:12:19.600
So the doc object, think about how it moves through the pipeline.

02:12:19.840 --> 02:12:22.640
This component is another pipe and that pipeline.

02:12:22.640 --> 02:12:27.400
It needs to receive the doc object and send off the doc object.

02:12:27.640 --> 02:12:28.760
You could do a lot of other things.

02:12:29.080 --> 02:12:30.600
It could print off entity found.

02:12:30.600 --> 02:12:32.800
It could do really any number of things.

02:12:32.800 --> 02:12:36.440
It could add stuff to the data coming out of the pipeline.

02:12:37.040 --> 02:12:40.240
All we're concerned with right now is modifying the doc dot ints.

02:12:41.080 --> 02:12:42.400
So we can do something like this.

02:12:42.880 --> 02:12:48.120
We can say original ends is equal to a list of the doc dot ends.

02:12:48.240 --> 02:12:52.800
So remember, we have to convert the ends from a generator into a list.

02:12:53.040 --> 02:12:58.000
Now what we can do is we can say for int and doc dot ends, if the end not label.

02:12:58.000 --> 02:13:05.040
So if that label is equal to GPE, then what we want to do is we want to just

02:13:05.160 --> 02:13:06.240
we just want to remove it.

02:13:06.360 --> 02:13:11.360
So let's say original ints.remove and we're going to remove the int.

02:13:11.640 --> 02:13:12.720
Remember, it's now a list.

02:13:13.000 --> 02:13:14.560
Sorry, I executed that too soon.

02:13:14.840 --> 02:13:16.040
Remember, it's now a list.

02:13:16.200 --> 02:13:20.960
So what we can do is we can go ahead now and convert those original

02:13:20.960 --> 02:13:26.320
ends back into doc dot ends by saying doc dot ends equals original ends.

02:13:26.720 --> 02:13:30.800
And if we've done things correctly, we can return the doc object and it will

02:13:30.800 --> 02:13:32.840
have all of those things removed.

02:13:33.040 --> 02:13:35.280
So this is what we would call a custom component.

02:13:35.280 --> 02:13:39.760
Something that changes the doc object along the way in the pipeline, but

02:13:39.760 --> 02:13:41.440
we need to add it to NLP.

02:13:41.880 --> 02:13:44.120
So we can do NLP dot add pipe.

02:13:45.040 --> 02:13:47.240
We want to make sure that it comes after the NER.

02:13:47.480 --> 02:13:52.880
So we're just going to say, uh, add the pipe or move GPE corresponds

02:13:52.880 --> 02:13:54.160
to the component name.

02:13:55.880 --> 02:13:59.400
And now let's go ahead and NLP dot analyze pipes.

02:14:00.400 --> 02:14:04.880
And you'll be able to see that it sits at the end of our pipeline right there.

02:14:04.880 --> 02:14:05.880
Remove GPE.

02:14:06.360 --> 02:14:08.400
Now comes time to see if it actually works.

02:14:08.720 --> 02:14:11.800
So we're going to copy and paste our code from earlier up here.

02:14:17.040 --> 02:14:21.480
Let's go ahead and copy this.

02:14:23.680 --> 02:14:28.600
And now we're going to say for int and doc dot ends print off int dot text.

02:14:29.400 --> 02:14:30.480
And dot label.

02:14:30.920 --> 02:14:35.200
And we should see, as we would expect, just marry coming out.

02:14:35.640 --> 02:14:38.400
Our pipeline has successfully worked.

02:14:38.840 --> 02:14:42.560
Now, as we're going to see when we move into red checks, you can do a lot

02:14:42.560 --> 02:14:46.280
of really, really cool things with custom components.

02:14:46.640 --> 02:14:50.480
I'm going to kind of save the, the advanced features for, I think I've

02:14:50.480 --> 02:14:54.400
got it scheduled for chapter here, chapter nine in our textbook.

02:14:54.680 --> 02:14:59.320
This is just a very, very basic example of how you can introduce a custom

02:14:59.320 --> 02:15:01.560
component to your spacey pipeline.

02:15:01.920 --> 02:15:04.080
If you can do this, you can do a lot more.

02:15:04.400 --> 02:15:06.440
You can maybe change a different entity.

02:15:06.440 --> 02:15:07.640
So they have different labels.

02:15:07.800 --> 02:15:10.680
You can make it where GPEs and locks all agree.

02:15:10.920 --> 02:15:12.200
You can remove certain things.

02:15:12.200 --> 02:15:15.280
You can have it print off place found person found.

02:15:15.560 --> 02:15:16.400
You can do a lot.

02:15:17.240 --> 02:15:20.800
So really the sky's the limit here, but a lot of the times you're going

02:15:20.800 --> 02:15:22.600
to need to modify that doc object.

02:15:22.920 --> 02:15:26.240
And this is how you do it with a custom pipe so that you don't have to write

02:15:26.240 --> 02:15:31.880
a bunch of code for a user outside of that NLP object, the NLP object.

02:15:31.880 --> 02:15:38.560
Once you save it to disk by doing something like NLP dot to disk data,

02:15:39.200 --> 02:15:45.640
new and core web SM, it's going to actually be able to go to the disk

02:15:45.960 --> 02:15:47.640
and be saved with everything.

02:15:48.080 --> 02:15:52.440
But one thing that you should note is that the component that you have

02:15:52.440 --> 02:15:56.520
here is not automatically saved with your data.

02:15:56.920 --> 02:16:00.320
So in order for your component to actually be saved with your data,

02:16:00.640 --> 02:16:06.000
you need to store that outside of this entire script.

02:16:06.280 --> 02:16:11.360
You need to save it as a library that can be given to the model

02:16:11.400 --> 02:16:12.760
when you go to package it.

02:16:12.960 --> 02:16:15.280
That's beyond the scope of this video for right now.

02:16:15.640 --> 02:16:19.880
In order for this to work in a different Jupyter notebook, if you were to try

02:16:19.880 --> 02:16:25.320
to use this, this container, this component has to actually be in the script.

02:16:25.560 --> 02:16:29.400
When it comes time to package your model, your pipeline and distribute it,

02:16:29.680 --> 02:16:30.800
that's a different scenario.

02:16:30.800 --> 02:16:34.480
And that scenario, you're going to make sure that you've got a special my

02:16:34.480 --> 02:16:39.560
component dot pie file with this bit of code in there so that, so that spacing

02:16:39.560 --> 02:16:42.560
knows how to handle your particular data.

02:16:43.560 --> 02:16:46.800
It's now time to move on to chapter eight of this textbook.

02:16:46.800 --> 02:16:48.680
And this is where a spacey gets really interesting.

02:16:48.680 --> 02:16:53.280
You can start applying regular expressions into a spacey component

02:16:53.280 --> 02:16:57.360
like an entity ruler or a custom component, as we're going to see in just

02:16:57.360 --> 02:16:58.840
a moment with chapter nine.

02:16:59.200 --> 02:17:02.680
I'm not going to spend a good deal of time talking about regular expressions.

02:17:02.680 --> 02:17:07.000
I could spend five hours talking about regex and what all it can do.

02:17:07.120 --> 02:17:10.880
In the textbook, I go over what you really need to know, which is what regular

02:17:10.920 --> 02:17:15.920
expressions is, which is as a way to do a really robust string pattern matching.

02:17:16.280 --> 02:17:19.440
I talk about the strengths of it, the weaknesses of it, its drawbacks,

02:17:19.800 --> 02:17:23.080
how to implement it in Python and how to really work with regex.

02:17:23.240 --> 02:17:25.640
But this is a video series on spacey.

02:17:25.880 --> 02:17:29.320
What I want to talk about is how to use regex with spacey.

02:17:29.640 --> 02:17:32.400
And so let's move over to a Jupiter notebook where we actually have this

02:17:32.400 --> 02:17:34.240
code to execute and play around with.

02:17:35.000 --> 02:17:37.840
If we look here, we have the same example that we saw before.

02:17:38.160 --> 02:17:40.800
What my goal is is not to extract the whole phone number,

02:17:40.800 --> 02:17:43.200
rather try to grab this sequence here.

02:17:43.520 --> 02:17:45.760
And we do this with a regular expression pattern.

02:17:46.080 --> 02:17:49.680
What this says is it tells it to look for a sequence of tokens or sequence

02:17:49.680 --> 02:17:51.400
of characters like this.

02:17:51.680 --> 02:17:56.840
It's going to be three digits followed by a dash followed by four digits.

02:17:57.160 --> 02:18:00.440
And if I were to execute this whole code, nothing is printed out.

02:18:00.840 --> 02:18:03.400
Does that mean that I failed to write good regex?

02:18:03.400 --> 02:18:04.760
No, it does not at all.

02:18:05.120 --> 02:18:07.360
It's failed for one very important reason.

02:18:07.440 --> 02:18:11.720
And this is the whole reason why I have this chapter in here is that regex,

02:18:12.000 --> 02:18:15.320
when it comes to pattern matching, pattern matching only really works

02:18:16.160 --> 02:18:19.840
when it comes to regex for single tokens.

02:18:20.040 --> 02:18:25.480
You can't use regex across multi-word tokens, at least as of spacey 3.1.

02:18:25.920 --> 02:18:27.200
So what does that mean?

02:18:27.200 --> 02:18:30.920
Well, it means that that dash right there in our phone number is causing

02:18:30.920 --> 02:18:32.200
all kinds of problems.

02:18:32.520 --> 02:18:35.960
If we move down to our second example, it's going to be the exact same pattern.

02:18:36.280 --> 02:18:37.080
A little different.

02:18:37.080 --> 02:18:39.640
Let me go ahead and move this over so you can see it a bit better.

02:18:40.840 --> 02:18:44.920
It's going to be regex that looks like this, where we just look for a sequence

02:18:44.920 --> 02:18:49.040
of five digits, we execute that, we find it just fine.

02:18:49.080 --> 02:18:52.160
And the reason for that is because this does not have a dash.

02:18:52.520 --> 02:18:56.800
So regex, if you're familiar with it, if you've worked with it, it's very powerful.

02:18:57.000 --> 02:18:58.880
You can do a lot of cool things.

02:18:59.320 --> 02:19:04.440
When you're going to use this in Python, if you're using just the standard

02:19:04.440 --> 02:19:05.960
off the shelf components.

02:19:06.200 --> 02:19:09.680
So the entity ruler, the matcher, you're going to be using this when

02:19:09.680 --> 02:19:14.160
you want to match regex to a single token.

02:19:14.440 --> 02:19:19.480
So think about this, if you're looking for a word that starts off with a capital

02:19:19.480 --> 02:19:24.320
D, and you want to just grab all words that start with a capital D, that would

02:19:24.320 --> 02:19:28.360
be an example of when you would want to use it in a standard off the shelf component.

02:19:28.800 --> 02:19:31.520
But that's not all you can do in spacey.

02:19:31.880 --> 02:19:35.800
You can use regex to actually capture multi word tokens.

02:19:36.080 --> 02:19:38.320
So capture things like Mr.

02:19:38.400 --> 02:19:39.280
Deeds.

02:19:39.480 --> 02:19:41.200
So any instance of Mr.

02:19:41.200 --> 02:19:44.800
Period Space Name, a sequence of proper nouns.

02:19:45.680 --> 02:19:50.880
You can also use it to, but yet in order to do that, you have to actually

02:19:50.880 --> 02:19:54.520
understand how to add in a custom component for it.

02:19:54.800 --> 02:19:59.320
And we're going to be seeing that in just a second as we move on to chapter nine,

02:19:59.560 --> 02:20:01.120
which is advanced regex.

02:20:01.320 --> 02:20:05.360
If you're not familiar with regex at all, take a few minutes, read chapter eight.

02:20:05.360 --> 02:20:10.800
I encourage you to do so because I go over in detail and I talk about how to

02:20:10.800 --> 02:20:15.080
actually engage in regex and Python and its strengths and weaknesses.

02:20:15.440 --> 02:20:18.680
What I want you to really focus on though, and get away from, get from all this

02:20:18.920 --> 02:20:24.160
is how to do some really complex multi word token matching with regex.

02:20:24.160 --> 02:20:27.440
Remember, you're going to want to use regular expressions when the pattern

02:20:27.440 --> 02:20:33.840
matching that you want to do is unindependent of the, the lima, the POS,

02:20:33.840 --> 02:20:36.360
or any of the linguistic features that space is going to use.

02:20:36.680 --> 02:20:40.040
If you're working with linguistic features, you have to use the

02:20:40.040 --> 02:20:45.040
spacey pattern, pattern matching things like the morph, the orth, the lima,

02:20:45.040 --> 02:20:45.840
things like that.

02:20:45.840 --> 02:20:50.400
But if your sequence of strings is not dependent on that, so you're looking

02:20:50.400 --> 02:20:53.840
for any instance of, in this case, we're going to talk about in just a second,

02:20:54.240 --> 02:20:59.880
a, a case where Paul is followed by a capitalized letter and then a word break.

02:21:00.680 --> 02:21:03.640
Then you're going to want to use regular expressions because in this case,

02:21:03.960 --> 02:21:08.840
this is independent of any linguistic features and regular expressions

02:21:08.840 --> 02:21:12.040
allows for you to write much more robust patterns, much more quickly.

02:21:12.040 --> 02:21:14.960
If you know how to use it well, and it allows for you to do much more

02:21:14.960 --> 02:21:18.440
quick robust things within a custom component.

02:21:18.680 --> 02:21:21.000
And that's going to be where we move to now.

02:21:22.000 --> 02:21:25.400
Now that we know a little bit about regex and how it can be implemented in

02:21:25.400 --> 02:21:30.000
Python, let's go ahead and also in spacey, let's go ahead and try and see

02:21:30.200 --> 02:21:37.400
how we can get regex to actually find multi word tokens for us within spacey

02:21:37.600 --> 02:21:39.400
using everything in the spacey framework.

02:21:39.600 --> 02:21:43.000
So the first thing I'm going to do to kind of demonstrate all this is I'm going

02:21:43.000 --> 02:21:45.000
to import regex.

02:21:45.000 --> 02:21:49.400
This comes standard with Python and you can import it as RE just that way.

02:21:49.400 --> 02:21:52.400
Import RE and that's going to import regex.

02:21:52.800 --> 02:21:56.800
I'm going to work from the textbook and work with this sample text.

02:21:57.000 --> 02:22:01.800
So this is Paul Newman was an American actor, but Paul Hollywood is a British TV

02:22:02.000 --> 02:22:02.800
TV host.

02:22:03.000 --> 02:22:05.000
The name Paul is quite common.

02:22:05.200 --> 02:22:09.000
So it's going to be the text that we work with throughout this entire chapter.

02:22:09.400 --> 02:22:13.600
Now a regex pattern that I could write to capture all instances of things like

02:22:13.600 --> 02:22:17.800
Paul Newman and Paul Hollywood, which is what my goal is, could look something

02:22:18.400 --> 02:22:24.600
like this, I could say or make an R string here and say Paul, and then I'm going

02:22:24.600 --> 02:22:28.200
to grab everything that starts with a capital letter and then my grab

02:22:28.400 --> 02:22:30.200
everything until a word break.

02:22:30.400 --> 02:22:34.200
And that's going to be a pattern that I can use in regex with this formula

02:22:34.200 --> 02:22:39.100
means is find any instance of Paul proceeded by a in this case, a capital

02:22:39.100 --> 02:22:41.900
letter until the actual word break.

02:22:41.900 --> 02:22:45.300
So grab the first name Paul and then what we can make a presumption is going

02:22:45.300 --> 02:22:49.700
to be that individual's last name in the text, a simple example, but one

02:22:49.700 --> 02:22:52.400
that will demonstrate our kind of purpose right now.

02:22:52.700 --> 02:22:57.100
So how we can do this is we can create an object called matches and use regex

02:22:57.100 --> 02:23:03.200
dot find it or we can pass in the pattern and we can pass in the text.

02:23:03.400 --> 02:23:07.100
So what this is going to do is it's going to use regex to try to find this

02:23:07.100 --> 02:23:09.300
pattern within this text.

02:23:09.500 --> 02:23:11.900
And then what we can do is we can iterate over those matches.

02:23:11.900 --> 02:23:19.200
So for match and matches, we can grab and print off the match and we have

02:23:19.200 --> 02:23:21.500
something that looks like this.

02:23:23.100 --> 02:23:26.900
What we're looking at here is what we would call it a regex match object.

02:23:27.100 --> 02:23:28.800
It's got a couple of different components here.

02:23:29.000 --> 02:23:34.100
It's got a span, which tells us the start character and the end character.

02:23:35.200 --> 02:23:39.500
And then it has a match and what this match means is the actual text itself.

02:23:39.700 --> 02:23:43.300
So the match here is Paul Newman and the match here is Paul Hollywood.

02:23:43.500 --> 02:23:47.900
So we've been able to extract the two entities in the text that begin with

02:23:47.900 --> 02:23:52.500
Paul and have a proper last name structured with a capital letter.

02:23:52.700 --> 02:23:54.500
We grabbed everything up until the word break.

02:23:54.900 --> 02:23:55.500
That's great.

02:23:55.700 --> 02:23:58.500
That's going to be what you need to know kind of going forward because what

02:23:58.500 --> 02:24:03.200
we're going to do now is we're going to implement this in a custom spacey pipe.

02:24:03.400 --> 02:24:07.100
But first let's go through and write the code so that we can then easily kind

02:24:07.100 --> 02:24:08.600
of create the pipe afterwards.

02:24:09.400 --> 02:24:14.000
So what we need to do is we need to import spacey and we also need to say

02:24:14.000 --> 02:24:19.500
from spacey dot tokens import span and we're going to be importing a couple

02:24:19.500 --> 02:24:22.500
of different things as we move forward because we're going to see that we're

02:24:22.500 --> 02:24:24.300
going to make a couple of mistakes intentionally.

02:24:24.300 --> 02:24:27.400
I'm going to show you how to kind of address these common mistakes that might

02:24:27.400 --> 02:24:29.900
surface in trying to do something like this.

02:24:30.400 --> 02:24:33.900
So once we've imported those two things, we can start actually writing out our

02:24:33.900 --> 02:24:34.300
code.

02:24:34.600 --> 02:24:37.300
Again, we're going to stick with the exact same text and again, we're going

02:24:37.300 --> 02:24:41.900
to stick with the exact same pattern that we've got stored in memory up above.

02:24:42.600 --> 02:24:46.500
So what we need to do now is we need to create a blank spacey object or sorry,

02:24:46.500 --> 02:24:50.700
a blank spacey pipeline that we can kind of put all this information into.

02:24:52.200 --> 02:24:57.500
And for right now what we're going to do is we're just going to kind of go

02:24:57.500 --> 02:25:00.300
through and look at these individual entities.

02:25:08.300 --> 02:25:16.100
So again, we're going to create the doc object, which is going to be equal to

02:25:16.100 --> 02:25:21.500
nlp text and this is not going to be necessary for right now, but I'm

02:25:21.500 --> 02:25:25.300
establishing a kind of a consistent workflow for us and you're going to see

02:25:25.300 --> 02:25:28.400
how we kind of take all this and implement it inside of a pipeline.

02:25:28.700 --> 02:25:32.200
So we're going to say original ends is equal to list doc dot ends.

02:25:32.200 --> 02:25:35.500
Now in this scenario, there's not going to be any entities because we don't

02:25:35.500 --> 02:25:40.200
have an any R or an entity ruler in our blank spacey pipeline.

02:25:41.100 --> 02:25:43.800
What we're going to do next is we're going to create something called an

02:25:43.800 --> 02:25:49.000
nwt int and that's going to stand for multi word token entity.

02:25:50.000 --> 02:25:51.400
You can name this whatever you like.

02:25:51.400 --> 02:25:54.800
This is just what I kind of stick to and then we're going to do and this

02:25:54.800 --> 02:25:56.600
is straight from the spacey documentation.

02:25:56.900 --> 02:26:00.100
We're going to say for match an RE dot find it or the same thing

02:26:00.100 --> 02:26:04.200
that we saw above pattern doc dot text.

02:26:04.500 --> 02:26:07.300
So what this is going to do is it's going to take that doc object.

02:26:08.200 --> 02:26:12.000
Look at it as raw text because remember the doc object is a container

02:26:12.300 --> 02:26:16.200
that doesn't actually have raw text in it until you actually call the dot

02:26:16.200 --> 02:26:19.900
text attribute and then our goal is for each of these things.

02:26:19.900 --> 02:26:23.200
We're going to look and call in this span.

02:26:23.500 --> 02:26:29.300
So we're going to say is start and the end is equal to match dot span.

02:26:29.600 --> 02:26:33.200
So what we're doing here is we're going in and grabbing the span attribute

02:26:33.700 --> 02:26:37.000
and we're grabbing these two components the start and the end.

02:26:37.200 --> 02:26:38.100
But we have a problem.

02:26:38.400 --> 02:26:40.400
These are character spans.

02:26:40.400 --> 02:26:43.600
Remember the doc object works on a token level.

02:26:43.800 --> 02:26:47.300
So we've got to kind of figure out a way to reverse engineer this almost

02:26:47.500 --> 02:26:50.200
to actually get this into a spacey form.

02:26:50.400 --> 02:26:55.300
Fortunately the doc object also has an attribute called character span.

02:26:56.000 --> 02:27:01.500
So what we can do is we can say the span is equal to doc dot char span

02:27:02.500 --> 02:27:03.700
start and end.

02:27:03.700 --> 02:27:07.100
So what this is going to do is it's going to print off essentially for us.

02:27:07.300 --> 02:27:08.800
Let's go ahead and do that.

02:27:09.100 --> 02:27:12.000
It would print off for us where we worry to actually have an entity here.

02:27:12.600 --> 02:27:16.400
It would print off for us as we can see Paul Newman and Paul Hollywood.

02:27:16.700 --> 02:27:22.700
So what we need to do now is we need to get this span into our entities.

02:27:23.400 --> 02:27:29.100
So what we can do is instead of printing things off we can say if span is not

02:27:29.100 --> 02:27:32.100
none because in some instance instances this will be the case.

02:27:32.500 --> 02:27:35.900
You're going to say NWT ends dot append.

02:27:36.700 --> 02:27:43.600
You're going to append a tuple here span dot start span dot end span dot

02:27:43.600 --> 02:27:44.300
text.

02:27:44.400 --> 02:27:48.600
So this is going to be the start the end and the text itself.

02:27:49.100 --> 02:27:53.900
And once we've done that we've managed to get our multi word tokens into

02:27:55.100 --> 02:27:57.100
a list that looks like this.

02:27:58.100 --> 02:28:06.600
Start and Paul Newman Paul Hollywood and notice that our span dot start is

02:28:06.600 --> 02:28:09.900
aligning not with a character span.

02:28:09.900 --> 02:28:14.200
Now it's rather aligning with a token span.

02:28:14.400 --> 02:28:17.800
So what we've done is we've taken this character span here and been able to

02:28:17.800 --> 02:28:23.600
find out where they start and end within the the token sequence.

02:28:23.600 --> 02:28:24.900
So we have zero and two.

02:28:25.300 --> 02:28:28.300
So Paul Newman one this was the zero index.

02:28:28.300 --> 02:28:30.200
It goes up until the second index.

02:28:30.400 --> 02:28:34.300
So it grabs index token zero and token one and we've done the same thing

02:28:34.300 --> 02:28:35.300
with Paul Hollywood.

02:28:35.700 --> 02:28:37.000
Now we've got that data.

02:28:37.200 --> 02:28:44.900
We can actually start to inject these entities into our original entities.

02:28:44.900 --> 02:28:46.600
So let's go through and do that right now.

02:28:47.100 --> 02:28:50.200
So we can do once we've got these things appended to this list.

02:28:50.200 --> 02:28:53.200
We can start injecting them into our original entities.

02:28:53.200 --> 02:28:57.300
So we can say for end in MWT ends.

02:28:57.800 --> 02:29:01.500
What we want to do is we want to say the start the end and the name is equal

02:29:01.500 --> 02:29:05.900
to end because this is going to correspond to the tuple the start the

02:29:05.900 --> 02:29:08.700
end and the entity text.

02:29:10.000 --> 02:29:12.000
Now what we can do is we can say per end.

02:29:12.000 --> 02:29:13.600
So this is going to be the individual end.

02:29:13.600 --> 02:29:16.800
We're going to create a span object in spacey.

02:29:17.900 --> 02:29:19.000
It's going to look like this.

02:29:19.300 --> 02:29:20.700
So a capital S here.

02:29:20.700 --> 02:29:22.800
Remember we imported it right up here.

02:29:23.000 --> 02:29:27.300
This is where we're going to be working with the span class and this is

02:29:27.300 --> 02:29:31.300
going to create for us a span object that we can now safely inject into

02:29:31.300 --> 02:29:33.800
the spacey doc.ins list.

02:29:34.300 --> 02:29:39.800
So we can say doc start and label and this is going to be the label that

02:29:39.800 --> 02:29:43.300
we want to actually assign it and this is going to be person in this

02:29:43.300 --> 02:29:48.100
case because these are all people we can do now as we can go through and

02:29:48.100 --> 02:29:52.900
say doc we can inject this into the original ends.

02:29:56.100 --> 02:30:02.500
Original ins dot append and we're going to append the per end which is

02:30:02.500 --> 02:30:08.900
going to be this span object and finally what we can say is doc.ins is

02:30:08.900 --> 02:30:14.500
equal to original ends kind of like what we saw just a few moments ago

02:30:15.300 --> 02:30:17.200
and let's go ahead and print off.

02:30:23.100 --> 02:30:26.600
We've got our entities right there or we to do this up here when we first

02:30:26.600 --> 02:30:30.900
kind of create the doc object you'll see nothing an empty list but now

02:30:30.900 --> 02:30:36.300
what we've been able to do is inject these into the doc object the doc.ins

02:30:36.300 --> 02:30:40.300
attribute and we can say for end and doc.ins just like everything else

02:30:40.300 --> 02:30:45.200
and dot text and dot label and because we converted it into a span we

02:30:45.200 --> 02:30:50.900
were able to inject it into the entity attribute from the doc object kind

02:30:50.900 --> 02:30:53.400
of natively so that spacey can actually understand it.

02:30:53.800 --> 02:30:57.100
So what can we do with this well one of the things that we could do is

02:30:57.100 --> 02:31:00.200
we can use the knowledge that we just acquired about custom components

02:31:00.500 --> 02:31:04.000
and build a custom component around all of this.

02:31:04.100 --> 02:31:07.400
So how might we do that well let's go through and try it out.

02:31:08.400 --> 02:31:14.600
The first thing that we need to do is we need to import our language class so

02:31:14.600 --> 02:31:17.600
if you remember from a few moments ago whenever you need to work with a

02:31:17.600 --> 02:31:24.600
custom component you need to say from spacey dot language import language

02:31:24.600 --> 02:31:28.100
with a capital L what we're going to do now is we're going to take the code

02:31:28.100 --> 02:31:31.500
that we just wrote and we're going to try to convert that into an actual

02:31:31.500 --> 02:31:36.700
custom pipe that can fit inside of our pipeline as kind of our own custom

02:31:36.700 --> 02:31:37.900
entity ruler if you will.

02:31:38.700 --> 02:31:41.700
So what we're going to do now is we're going to call this language dot

02:31:41.700 --> 02:31:47.500
component and we're going to call this let's call this Paul NER something

02:31:47.600 --> 02:31:51.400
not too not too clever but kind of very descriptive we're going to call

02:31:51.400 --> 02:31:56.000
this Paul NER and this is going to take that single doc object because

02:31:56.000 --> 02:31:59.800
remember this pipe needs to receive the doc object and do stuff to it.

02:32:00.200 --> 02:32:03.100
So what we can do is we can take all this code that we just wrote.

02:32:06.700 --> 02:32:14.300
From here down and paste it into our function and what we have is the

02:32:14.300 --> 02:32:17.100
ability now to implement this as a custom pipe.

02:32:18.200 --> 02:32:21.500
We don't need to do this because we don't want to print things off but

02:32:21.500 --> 02:32:23.900
here we're going to return the doc object.

02:32:23.900 --> 02:32:28.500
So we have now is a custom kind of entity ruler that uses regex across

02:32:28.500 --> 02:32:29.900
multiple tokens.

02:32:30.100 --> 02:32:34.700
If you want to use regex in spacey across multiple tokens as of spacey

02:32:34.700 --> 02:32:37.500
3.1 this is the only way to implement this.

02:32:38.100 --> 02:32:45.200
So now we can take this pipe and we can actually add it to a blank custom

02:32:45.300 --> 02:32:45.800
model.

02:32:46.100 --> 02:32:52.600
So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're

02:32:52.600 --> 02:32:57.900
going to create a blank English model nlp2 dot add pipe.

02:32:59.300 --> 02:33:01.500
We're going to add in Paul NER.

02:33:05.100 --> 02:33:07.700
And now we see that we've actually created that successfully.

02:33:07.700 --> 02:33:10.400
So we have one pipe kind of sitting in all of this.

02:33:10.800 --> 02:33:14.500
Now what we can do is we can go through and we need to probably add in our

02:33:14.500 --> 02:33:19.300
pattern as well here just for good practice because this should be

02:33:19.300 --> 02:33:21.100
stored somewhat adjacent.

02:33:21.100 --> 02:33:24.100
I like to sometimes to keep it up here when I'm doing this but you can

02:33:24.100 --> 02:33:27.000
also keep it kind of inside of the function itself.

02:33:28.200 --> 02:33:31.500
Let's go ahead and just kind of save that and we're going to rerun this

02:33:32.300 --> 02:33:32.700
cool.

02:33:33.400 --> 02:33:37.200
Now what we can do is we can say doc to is equal to nlp2 we're going

02:33:37.200 --> 02:33:41.200
to go over that exact same text and we're going to print off our doc

02:33:41.300 --> 02:33:46.300
to dot ints and we've now managed to implement that as a custom

02:33:46.400 --> 02:33:49.500
spacey pipe but we've got one big problem.

02:33:49.900 --> 02:33:55.900
Let's say just hypothetically we wanted to also kind of work in really

02:33:55.900 --> 02:34:01.900
a another kind of something into our actual pipeline.

02:34:01.900 --> 02:34:06.900
We wanted this pipeline to sit on top of maybe an existing spacey model

02:34:07.300 --> 02:34:11.800
and for whatever reason we don't want Paul Hollywood to have that title.

02:34:11.800 --> 02:34:13.200
We wanted to have the title.

02:34:13.700 --> 02:34:17.000
Maybe we want to just kind of keep Paul Hollywood as a person but we

02:34:17.000 --> 02:34:21.600
also want to find maybe other cinema style entities.

02:34:21.600 --> 02:34:25.400
So we're going to create another entity here instead of all this that's

02:34:25.400 --> 02:34:30.100
going to be something like let's go ahead and make a new a new container

02:34:30.100 --> 02:34:32.200
down here a new component down here.

02:34:33.100 --> 02:34:36.600
We're going to just look for any instance of Hollywood and we're going

02:34:36.600 --> 02:34:39.300
to call that the word the label of cinema.

02:34:39.800 --> 02:34:42.000
So I want to demonstrate this because this is going to show you

02:34:42.000 --> 02:34:44.800
something that you are going to encounter when you try to implement

02:34:44.800 --> 02:34:47.200
this in the real world and I'm going to show you how to kind of

02:34:47.500 --> 02:34:49.500
address the problem that you're going to encounter.

02:34:49.900 --> 02:34:53.100
So if we had a component that looked like this now it's going to look

02:34:53.100 --> 02:34:57.200
for just instance instances of Hollywood and let's call this Holly

02:34:58.900 --> 02:35:02.900
Cinema NER and change this here as well.

02:35:03.400 --> 02:35:05.700
What we can do now is go ahead and load that up into memories.

02:35:05.700 --> 02:35:09.500
We've got this new component called Cinema NER and just like before

02:35:09.500 --> 02:35:12.200
we're going to create an LP three now this is going to be spacey dot

02:35:12.200 --> 02:35:14.400
load in core web.

02:35:15.500 --> 02:35:20.500
SM and so what this is going to do is it's going to load up the spacey

02:35:20.500 --> 02:35:26.700
small model and LP three dot add pipe and it's going to be the what did

02:35:26.700 --> 02:35:32.300
I call this again the cinema NER and if we were to go through

02:35:32.300 --> 02:35:36.900
and add that and create a new object called doc three make that

02:35:36.900 --> 02:35:39.900
equal to an LP three text.

02:35:41.700 --> 02:35:45.600
We're going to get this air and this is a common air and if you

02:35:45.600 --> 02:35:47.700
Google it you'll eventually find the right answer.

02:35:47.700 --> 02:35:49.500
I'm just going to give it to you right now.

02:35:49.800 --> 02:35:53.800
So what this is telling you is that there are spans that overlap

02:35:54.800 --> 02:36:01.200
that don't actually work because one of the spans for cinema is Hollywood

02:36:01.500 --> 02:36:07.500
and the small model is extracting not only that Hollywood as a cinema

02:36:07.500 --> 02:36:11.600
but it's also extracting Paul Hollywood as part of a longer token.

02:36:11.900 --> 02:36:17.100
So what's happened here is we're trying to assign a span to two of

02:36:17.100 --> 02:36:19.400
the same tokens and that doesn't work in spacey.

02:36:19.400 --> 02:36:21.500
It'll break so what can you do?

02:36:21.700 --> 02:36:26.300
Well a common method of solving this issue is to work with the filter

02:36:26.300 --> 02:36:28.900
spans from the spacey dot util.

02:36:29.800 --> 02:36:33.900
Let's go ahead and do this right now so you can say from spacey dot

02:36:33.900 --> 02:36:37.200
util import filter spans.

02:36:37.400 --> 02:36:42.200
What filter spans allows for you to do is to actually filter out all

02:36:42.200 --> 02:36:44.600
of the the spans that are being identified.

02:36:44.900 --> 02:36:47.700
So what we can do is we can say at this stage.

02:36:50.400 --> 02:36:55.400
Before you get to the dock dot ends you can say filtered is equal

02:36:55.400 --> 02:36:59.400
to filter spans original ends.

02:36:59.500 --> 02:37:00.500
So what does this do?

02:37:00.500 --> 02:37:03.200
Well what this does is it goes through and looks at all of the

02:37:03.200 --> 02:37:07.800
different start and end sections from all of your entities.

02:37:08.200 --> 02:37:12.400
And if there is an ever an instance where there is a an overlap

02:37:12.400 --> 02:37:16.700
of tokens so 8 to 10 and 9 to 10.

02:37:17.300 --> 02:37:22.000
Primacy and priority is going to be given to the longer token.

02:37:22.400 --> 02:37:25.800
So what we can do is we can set this now to filtered and it helps

02:37:25.800 --> 02:37:28.200
if you call it correctly filtered.

02:37:28.200 --> 02:37:28.800
There we go.

02:37:29.800 --> 02:37:32.800
We can set that to filtered instead of the original entities.

02:37:33.100 --> 02:37:34.200
Go ahead and save that.

02:37:34.500 --> 02:37:38.700
We're going to add this again and we're going to do doc 3 and

02:37:38.700 --> 02:37:43.200
we're going to say for int and doc 3 dot ends print int dot

02:37:43.200 --> 02:37:45.100
text and int dot label.

02:37:46.100 --> 02:37:50.400
And if we've done this correctly we're not going to see the

02:37:50.600 --> 02:37:54.900
cinema label come out at all because Paul Hollywood is a

02:37:54.900 --> 02:37:59.000
longer token than just Hollywood.

02:37:59.000 --> 02:38:03.400
So what we've done is we've set told spacey give the primacy

02:38:03.400 --> 02:38:07.600
to the longer tokens and assign that label by filtering out

02:38:07.600 --> 02:38:10.800
the tokens you can prevent that air from ever surfacing.

02:38:11.100 --> 02:38:14.100
But this is a very common thing that you're going to have to

02:38:14.100 --> 02:38:18.300
implement sometimes rejects really is the easiest way to

02:38:18.300 --> 02:38:20.700
inject and do pattern matching in the entity.

02:38:21.300 --> 02:38:21.600
Okay.

02:38:21.600 --> 02:38:24.300
So here's the scenario that we have before us in order to make

02:38:24.300 --> 02:38:27.500
this live this kind of live coding and applied spacey a

02:38:27.500 --> 02:38:30.300
little bit more interesting imagine in this scenario we

02:38:30.300 --> 02:38:34.100
have a client and the client is a stockbroker or somebody

02:38:34.100 --> 02:38:36.900
who's interested in investing and what they want to be able

02:38:36.900 --> 02:38:39.800
to do is look at news articles like those coming out of Reuters

02:38:40.200 --> 02:38:42.200
and they want to find the news articles that are the most

02:38:42.200 --> 02:38:46.400
relevant to what they need to actually search for and read

02:38:46.400 --> 02:38:46.900
for the day.

02:38:46.900 --> 02:38:49.000
So they want to find the ones that deal with their their

02:38:49.000 --> 02:38:52.600
personal stocks their holdings or maybe their the specific

02:38:52.600 --> 02:38:54.300
index that they're actually interested in.

02:38:54.600 --> 02:39:00.600
So what this client wants is a way to use spacey to automatically

02:39:00.600 --> 02:39:05.500
find all companies referenced within a text all stocks

02:39:05.500 --> 02:39:09.500
referenced within a text and all indexes referenced within

02:39:09.500 --> 02:39:12.600
the next text and maybe even some stock exchanges as well.

02:39:12.800 --> 02:39:15.800
Now on the actual textbook if you go through to this chapter

02:39:15.800 --> 02:39:18.900
which is number 10 you're going to find all the kind of

02:39:18.900 --> 02:39:22.200
solutions laid out for you what I'm going to do throughout

02:39:22.200 --> 02:39:26.500
the next 30 or 40 minutes is kind of walk through how I might

02:39:26.500 --> 02:39:28.800
solve this problem at least on the surface.

02:39:28.800 --> 02:39:32.300
This is going to be a rudimentary solution that demonstrates

02:39:32.300 --> 02:39:35.700
the power of spacey and how you can apply it in a very short

02:39:35.700 --> 02:39:40.100
period of time to do some pretty custom tasks such as financial

02:39:40.100 --> 02:39:44.000
analysis with that structured data that you've extracted you

02:39:44.000 --> 02:39:47.700
can then do any number of things what we're going to start off

02:39:47.700 --> 02:39:55.000
with though is importing spacey and importing pandas as PD if

02:39:55.000 --> 02:39:57.900
you're not familiar with pandas I've got a whole tutorial

02:39:57.900 --> 02:40:00.900
series on that on my channel Python tutorials for digital

02:40:00.900 --> 02:40:03.700
humanities even though it has digital humanities in the title

02:40:03.700 --> 02:40:06.900
it's for kind of everyone but go through if you're not familiar

02:40:06.900 --> 02:40:09.500
with pandas and check that out you're not really going to need

02:40:09.500 --> 02:40:13.000
it for for this video here you're going to just need to

02:40:13.000 --> 02:40:17.000
understand that I'm using pandas to access and grab the data

02:40:17.000 --> 02:40:21.200
that I need from a couple CSV files or comma separated value

02:40:21.200 --> 02:40:22.200
files that I have.

02:40:23.400 --> 02:40:25.400
So the first thing that we need to do is we need to create

02:40:25.400 --> 02:40:28.400
what's known as a pandas data frame and this is going to be

02:40:28.400 --> 02:40:32.700
equal to PD dot read CSV and I actually have these stored in

02:40:32.700 --> 02:40:36.700
the data sub folder in the repo you have free access to these

02:40:36.700 --> 02:40:39.500
they're a little tiny data sets that I cultivated pretty

02:40:39.500 --> 02:40:42.300
quickly they're not perfect but they're good enough for our

02:40:42.300 --> 02:40:45.900
purposes and we're going to use the separator keyword argument

02:40:45.900 --> 02:40:49.500
which is going to say to separate everything out by tab

02:40:49.500 --> 02:40:53.400
because these are CSV files tab separated value files and we

02:40:53.400 --> 02:40:56.100
have something that looks like this so what this stocks dot

02:40:56.100 --> 02:41:00.100
CSV file is is it's all the symbols company names industry

02:41:00.100 --> 02:41:03.300
and market caps for I think it's around five thousand seven

02:41:03.300 --> 02:41:05.700
hundred different stocks five thousand eight hundred and seventy

02:41:05.700 --> 02:41:08.900
nine and so what we're going to use this for is as a way to

02:41:08.900 --> 02:41:13.300
start working into an entity ruler all these different symbols

02:41:13.300 --> 02:41:16.400
and company names what we want to do is we want to use these

02:41:16.400 --> 02:41:19.900
symbols to work into a model as a way to grab stocks that might

02:41:19.900 --> 02:41:22.400
be referenced and you can already probably start to see a

02:41:22.400 --> 02:41:25.400
problem with this capital a here we're going to get to that

02:41:25.400 --> 02:41:27.900
in a little bit and we want to grab all the company names

02:41:27.900 --> 02:41:30.300
so we can maybe create two different entity types from

02:41:30.300 --> 02:41:35.900
this data set stock and company so let's go through and make

02:41:35.900 --> 02:41:39.100
these into lists so they're a little bit more so let's go

02:41:39.100 --> 02:41:41.500
through and make these into lists so they're a little bit

02:41:41.500 --> 02:41:46.100
more manageable what we need to do is we need to create a list

02:41:46.100 --> 02:41:50.500
of symbols and that's going to be equal to DF dot symbol dot

02:41:50.500 --> 02:41:53.900
two list this is a great way to do it and pandas so you can

02:41:53.900 --> 02:41:57.300
kind of easily convert all these different columns into

02:41:58.500 --> 02:42:01.400
different lists that you can work with in Python so companies

02:42:01.400 --> 02:42:05.100
is going to be equal to DF dot company and name I believe

02:42:05.100 --> 02:42:08.800
the name was two list and just to demonstrate how this works

02:42:08.800 --> 02:42:12.700
let's print off symbols we're going to print up to 10 and

02:42:12.700 --> 02:42:15.400
you can kind of see we've managed to take these columns now

02:42:15.400 --> 02:42:19.100
and kind of build them into a simple Python list so what can

02:42:19.100 --> 02:42:22.100
we do with that well one of the things that we can do is we

02:42:22.100 --> 02:42:25.900
can use that information to start cultivating an entity

02:42:25.900 --> 02:42:29.700
ruler but remember we want more things than just one or two

02:42:29.700 --> 02:42:32.700
kind of in our entity ruler we don't just want stocks and we

02:42:32.700 --> 02:42:35.900
don't just want companies we also want things like indexes

02:42:35.900 --> 02:42:37.800
we're going to get to that in just a second though for right

02:42:37.800 --> 02:42:41.600
now let's try to work these two things into an entity ruler

02:42:41.600 --> 02:42:45.300
how might we go about doing that well as you might expect

02:42:45.300 --> 02:42:49.000
we're going to create a fairly simple entity ruler so we're

02:42:49.000 --> 02:42:51.800
going to say is nlp is going to be equal to spacey dot blank

02:42:51.800 --> 02:42:54.500
we don't need a lot of fancy features here we're just going

02:42:54.500 --> 02:42:58.200
to have a blank model that's just going to hose host an

02:42:58.700 --> 02:43:02.900
single entity ruler that's going to be equal to nlp dot add

02:43:03.500 --> 02:43:08.100
underscore pipe and this is going to be entity ruler and now

02:43:08.100 --> 02:43:10.700
what we need to do is we need to come up with a way to go

02:43:10.700 --> 02:43:14.800
through all of these different symbols and add them in so we

02:43:14.800 --> 02:43:20.200
can say for symbol and symbols we want to say patterns dot

02:43:20.200 --> 02:43:23.700
append and we're going to make a an empty list of patterns

02:43:23.700 --> 02:43:29.100
up here and what we're going to append is that dictionary that

02:43:29.100 --> 02:43:32.100
you met when we talked about the entity ruler and I believe

02:43:32.100 --> 02:43:36.100
it was chapter five yeah and what this is going to have

02:43:36.100 --> 02:43:39.000
there are two things label which is going to correspond to

02:43:39.100 --> 02:43:44.600
stock in this case and it's going to have a pattern and

02:43:44.600 --> 02:43:49.000
that's going to correspond to the pattern of the symbol so

02:43:49.000 --> 02:43:52.500
we're going to say symbol and what that lets us do is kind of

02:43:52.500 --> 02:43:56.100
go through and easily create and add these patterns and and

02:43:56.100 --> 02:43:59.500
we can do the same thing for company remember it's never a

02:43:59.500 --> 02:44:03.200
good idea to copy and paste in your code I am simply doing

02:44:03.200 --> 02:44:06.200
it for demonstration purposes right now this is not polished

02:44:06.200 --> 02:44:09.600
code by any stretch of the imagination and what we can do

02:44:09.600 --> 02:44:12.000
here now is we can do the same thing loop over the different

02:44:12.000 --> 02:44:15.600
companies and add each company and so what this is doing is

02:44:15.600 --> 02:44:18.500
it's creating a large list of different patterns that the

02:44:18.500 --> 02:44:22.500
entity ruler will use to then go through and as we create the

02:44:22.500 --> 02:44:27.100
a doc object over that sample Reuters text I just showed you

02:44:27.100 --> 02:44:30.200
a second ago which we should probably just go ahead and

02:44:30.200 --> 02:44:33.100
pull up right now I'm going to copy and paste it straight

02:44:33.100 --> 02:44:35.100
from the textbook.

02:44:38.600 --> 02:44:41.600
Let's go ahead and execute that cell and we're going to add

02:44:41.600 --> 02:44:45.000
in this text here it is a little lengthy but it'll be all

02:44:45.000 --> 02:44:47.800
right and what we're going to do now is we're going to iterate

02:44:47.800 --> 02:44:51.000
over create a doc object to iterate over all of that.

02:44:54.000 --> 02:44:57.800
And our goal here is going to be able to say for int and doc

02:44:57.800 --> 02:45:01.800
dot ends we want to have extracted all of these different

02:45:02.100 --> 02:45:08.100
entities so we can say print off and dot text and dot label

02:45:10.000 --> 02:45:11.300
and let's see if we succeeded.

02:45:15.400 --> 02:45:19.000
And we have to add in our patterns to our entity ruler so

02:45:19.000 --> 02:45:22.600
remember we can do this by saying ruler dot add patterns.

02:45:24.900 --> 02:45:26.000
Patterns there we go.

02:45:27.000 --> 02:45:32.000
That's what this error actually means and now when we do it

02:45:32.000 --> 02:45:36.500
we see that we've been able to extract Apple as a company

02:45:36.500 --> 02:45:40.700
Apple as a company Nasdaq everything's looking pretty good

02:45:40.700 --> 02:45:43.900
but I notice really quickly that I wasn't actually able to

02:45:43.900 --> 02:45:48.800
extract Apple as a stock and I've also got another problem

02:45:49.100 --> 02:45:54.700
I've extracted to the lowercase TWO as a stock as well why

02:45:54.700 --> 02:45:57.900
have these two things are as a company. Well it turns out in

02:45:57.900 --> 02:46:03.500
our data set we've got to TWO that is a company name that's

02:46:03.500 --> 02:46:07.200
almost always going to be a false positive and we know that

02:46:07.200 --> 02:46:10.100
that kind of thing might be better off worked into a machine

02:46:10.100 --> 02:46:13.100
learning model for right now though we're going to work

02:46:13.100 --> 02:46:15.700
under the presumption that anytime we encounter this kind

02:46:15.700 --> 02:46:19.500
of obscure company TWO as a lowercase it's going to be a

02:46:19.500 --> 02:46:22.900
false positive. I also have another problem I know for a

02:46:22.900 --> 02:46:28.500
fact that Apple the stock is referenced within this text to

02:46:28.500 --> 02:46:31.400
make it a little easier. Let's see it right here and notice

02:46:31.400 --> 02:46:33.900
that it didn't find it to make this a little easier to

02:46:33.900 --> 02:46:37.300
display. Let's go ahead and display what we're looking at

02:46:37.500 --> 02:46:41.700
as the splacy render so what we can do is we can use that the

02:46:41.700 --> 02:46:45.200
splacy render that we met a little bit ago in this video.

02:46:46.700 --> 02:46:49.700
So in order to import this if you remember we need to say

02:46:50.700 --> 02:46:56.800
from spacey import display see and that's going to allow us

02:46:56.800 --> 02:47:00.200
to actually display our entities. Let's go ahead and put

02:47:00.200 --> 02:47:03.400
this however on a different cell just so we don't have to

02:47:03.400 --> 02:47:09.400
execute that every time and we're going to say at splacy render

02:47:09.600 --> 02:47:13.500
and we're going to render the doc object with a style that's

02:47:13.500 --> 02:47:17.800
equal to ENT and we can see that we've got our text now

02:47:17.800 --> 02:47:21.300
popping out with our things labeled and you can see pretty

02:47:21.300 --> 02:47:24.100
quickly where we've made some mistakes where we need to

02:47:24.100 --> 02:47:28.300
incorporate some things into our entity ruler. So for example

02:47:28.300 --> 02:47:31.500
if I'm scrolling through this is gray little ugly we can change

02:47:31.500 --> 02:47:34.500
the colors that's beyond the scope of this video though but

02:47:34.500 --> 02:47:37.900
let's keep on going down we notice that we have Apple dot

02:47:37.900 --> 02:47:41.600
IO and yet this has been missed by our entity ruler. Why has

02:47:41.600 --> 02:47:46.400
this been missed well. Spacey as a tokenizer is seeing this

02:47:46.400 --> 02:47:52.200
as a single token so Apple dot Oh the letter Oh capital letter

02:47:52.200 --> 02:47:55.900
Oh why is that well I didn't know about this but apparently

02:47:55.900 --> 02:47:59.800
it does has to deal with kind of the way in which stock indices

02:47:59.800 --> 02:48:03.500
are I think it's on the NASDAQ kind of structure things so

02:48:03.500 --> 02:48:06.200
what can we do well we've got a couple different options here

02:48:06.600 --> 02:48:09.400
I know that these go through all different letters from A to

02:48:09.400 --> 02:48:13.000
Z so we can either work with the string library or we can do

02:48:13.000 --> 02:48:16.500
is we can import a quick list that I've already written out

02:48:16.800 --> 02:48:20.000
of all the different letters of the alphabet and iterate

02:48:20.000 --> 02:48:23.800
through those with our ruler up here.

02:48:26.000 --> 02:48:29.100
Let's go ahead and add these letters right there and we can

02:48:29.100 --> 02:48:31.100
kind of iterate through those and whenever a stock kind of

02:48:31.100 --> 02:48:35.800
pops out with that kind of symbol plus any occurrence where

02:48:35.800 --> 02:48:40.400
it's got a period followed by a letter in those scenarios we

02:48:40.400 --> 02:48:44.600
want that to be flagged as a stock as well so what we can do

02:48:44.600 --> 02:48:47.400
is we can add in another thing right here add in another

02:48:47.400 --> 02:48:51.800
pattern and this is now going to be symbol plus we're going

02:48:51.800 --> 02:48:55.900
to add in F string right here a formatted string any occurrence

02:48:55.900 --> 02:49:05.300
of L we can set up a loop to say for L and letters do this

02:49:05.700 --> 02:49:09.400
and what this is going to allow us to do is to look for any

02:49:09.400 --> 02:49:14.900
instance where there is a symbol followed by a period

02:49:14.900 --> 02:49:18.400
followed by one of these capitalized letters that I just

02:49:18.400 --> 02:49:22.400
copied and pasted in so if we do that we can execute that cell

02:49:24.400 --> 02:49:27.600
and we can scroll down and we can now do the exact same thing

02:49:27.600 --> 02:49:31.800
that we just did a second ago and actually display this

02:49:32.800 --> 02:49:40.000
and now we're finding these stocks highlighted as stock so

02:49:40.000 --> 02:49:42.800
we're successfully getting these stocks and extracting them

02:49:42.800 --> 02:49:45.800
we've got a few different things that our client wants to

02:49:45.800 --> 02:49:49.000
also extract though they don't want to just extract companies

02:49:49.000 --> 02:49:53.200
and they don't want to just extract stock and they want to

02:49:53.200 --> 02:49:57.200
also extract stock exchanges and indexes but we have one

02:49:57.200 --> 02:50:00.500
other problem and go ahead and get rid of this as the display

02:50:00.500 --> 02:50:03.600
mode and switch back to just our set of entities because

02:50:03.600 --> 02:50:06.400
it's a little easier to read for this example we've got

02:50:06.400 --> 02:50:08.400
another problem and we see we have a couple other stocks

02:50:08.400 --> 02:50:12.600
popping out we now know that Kroger stock is here the n i

02:50:12.600 --> 02:50:16.000
o dot n stock is in this text as well now we're starting to

02:50:16.000 --> 02:50:19.700
see a greater degree of specificity for right now I'm

02:50:19.700 --> 02:50:24.000
going to include two as a set of a stop technical term would

02:50:24.000 --> 02:50:26.500
be like a stop or something that I don't want to be included

02:50:26.500 --> 02:50:29.700
into the model so I'm going to make a list of stops and

02:50:29.700 --> 02:50:32.400
we're just going to include two in that and we're going to

02:50:32.400 --> 02:50:40.200
say for company and companies do all this if company not in

02:50:40.200 --> 02:50:44.900
stops we want this to occur what this means now is that our

02:50:44.900 --> 02:50:47.900
our pipeline while going through and having all of these

02:50:47.900 --> 02:50:50.900
different things all these different rules it's also going

02:50:50.900 --> 02:50:54.200
to have another rule that looks to see if there's a stop or

02:50:54.200 --> 02:50:58.400
if this company name is this stop and if it is then we want

02:50:58.400 --> 02:51:03.000
it to just kind of skip over and ignore it and if we go

02:51:03.000 --> 02:51:05.800
through we notice that now we've successfully eliminated

02:51:05.800 --> 02:51:10.300
this what we would presume to be a consistent false positive

02:51:10.300 --> 02:51:13.100
something that's going to come up again and again as a false

02:51:13.100 --> 02:51:17.100
positive great so we've been able to get this where it works

02:51:17.100 --> 02:51:20.300
now pretty well what I also want to work into this model if

02:51:20.300 --> 02:51:23.600
you remember though are things like indexes fortunately

02:51:23.600 --> 02:51:28.200
I've also provided for us a list of all different indexes that

02:51:28.200 --> 02:51:31.500
are available from I believe it's like everything like the

02:51:31.500 --> 02:51:35.700
Dow Jones is about 13 or 14 of them let's go ahead and import

02:51:35.700 --> 02:51:41.300
those up above and let's do that right here in this cell so

02:51:41.300 --> 02:51:43.800
it kind of goes in sequential order that follows better with

02:51:43.800 --> 02:51:46.900
the textbook to so it's a new data frame object this is going

02:51:46.900 --> 02:51:50.800
to be equal to P a PD dot read CSV we're going to read in that

02:51:50.800 --> 02:51:53.700
data file that I've given us and that's going to be the indexes

02:51:53.700 --> 02:51:59.900
dot T SV with a separator that's equal to a tab let's see what

02:51:59.900 --> 02:52:04.000
that looks like and this is what it looks like so all these

02:52:04.000 --> 02:52:06.900
different indices now I know I'm going to have a problem right

02:52:06.900 --> 02:52:11.100
out of the gate and that's going to be that sometimes you're

02:52:11.100 --> 02:52:14.600
going to see things referenced as SNP 500 I don't know a lot

02:52:14.600 --> 02:52:17.100
about finances but I know that you don't always see it as

02:52:17.100 --> 02:52:22.200
SNP 500 index but I do think that these index symbols are

02:52:22.200 --> 02:52:25.700
also going to be useful so like I did before I'm going to convert

02:52:25.700 --> 02:52:28.500
these things into a list so it's a little easier for me to work

02:52:28.500 --> 02:52:33.900
with in a for loop and I'm going to say indexes is equal to

02:52:33.900 --> 02:52:41.800
DF2 dot index name so grabbing that column to list and index

02:52:41.800 --> 02:52:48.400
symbols is equal to DF2 dot index symbol dot to list and

02:52:48.400 --> 02:52:51.500
both of these are going to be different and they're both going

02:52:51.500 --> 02:52:56.200
to have the same exact entity label which is going to be an

02:52:56.200 --> 02:53:00.200
index and so let's go ahead and iterate over these and add them

02:53:00.200 --> 02:53:03.300
in as well so I'm going to go ahead and do that right now

02:53:04.000 --> 02:53:13.700
for indexes and indexes we want this label to be index we

02:53:13.700 --> 02:53:17.700
want this to be index here so it's going to allow us to kind

02:53:17.700 --> 02:53:20.200
of go through and grab all those and we want to do the same

02:53:20.200 --> 02:53:26.000
thing with index symbols keep these a little separated here

02:53:26.000 --> 02:53:32.300
index symbols and that allows for us to do that and let's go

02:53:32.300 --> 02:53:35.200
ahead and without making any adjustments let's see let's see

02:53:35.200 --> 02:53:37.900
how this does with these new patterns that we've added in

02:53:38.400 --> 02:53:41.100
and because we've already got this text loaded into memory

02:53:41.100 --> 02:53:44.500
I'm going to go ahead and put this right here doc is going to

02:53:44.500 --> 02:53:52.900
be equal to nlp text for int and doc and print off and dot

02:53:52.900 --> 02:53:58.400
text and dot label and we can kind of go through and we're

02:53:58.400 --> 02:54:02.200
actually now able to extract some indexes and I believe when

02:54:02.200 --> 02:54:05.200
I was looking at this text really quickly though I noticed

02:54:05.200 --> 02:54:09.600
that there was one instance at least where we had not only

02:54:09.600 --> 02:54:15.300
the index referenced but also a name like S&P 500 right here

02:54:15.300 --> 02:54:18.700
S&P 500 notice that it isn't found because it doesn't have

02:54:18.700 --> 02:54:21.800
the name index after it and notice also that none of our

02:54:22.300 --> 02:54:25.000
our symbols are being found because they all seem to be

02:54:25.000 --> 02:54:31.500
preceded by a dot so in this case a dot J a DJI and so that's

02:54:31.500 --> 02:54:33.600
something else that I have to work into this model and the

02:54:33.600 --> 02:54:37.000
list I gave the data set that's not there so I need to collect

02:54:37.100 --> 02:54:40.600
a list of these different names and work those into an entity

02:54:40.600 --> 02:54:43.500
ruler as well but for right now let's ignore that and focus

02:54:43.500 --> 02:54:48.500
on including this S&P 500 so how can I get the S&P 500 in

02:54:48.500 --> 02:54:51.800
there from the list I already gave it well what I can do is

02:54:51.800 --> 02:54:56.800
I can say okay so under these indices not only do I want to

02:54:56.800 --> 02:55:00.300
add that specific pattern let's go ahead and break these things

02:55:00.300 --> 02:55:03.700
up into different words and so I'm going to have the words is

02:55:03.700 --> 02:55:07.400
equal to index dot split and then I'm going to make a

02:55:07.400 --> 02:55:13.600
presumption that the the first two words so the S&P 500 the

02:55:13.600 --> 02:55:18.800
S&P 400 are sometimes going to be referenced by themselves so

02:55:18.800 --> 02:55:21.400
what I want to do is I want to work that into the model as

02:55:21.400 --> 02:55:26.600
well and I want to say we're going to say patterns dot

02:55:26.600 --> 02:55:34.700
append copy this as well we can say something like dot join

02:55:36.300 --> 02:55:41.800
words up until the second index and let's go ahead and work

02:55:41.800 --> 02:55:45.700
that into our model in our patterns or pipeline and print

02:55:45.700 --> 02:55:50.300
off our NLP again and you'll find that we've now been able to

02:55:50.300 --> 02:55:56.300
capture things like S&P 500 that aren't proceeded by the word

02:55:56.300 --> 02:56:00.700
index and we see that we in fact have S&P 500 is now popping

02:56:00.700 --> 02:56:04.100
out time and again that's fantastic I'm pretty happy with

02:56:04.100 --> 02:56:06.800
that now we're we're getting a deeper sense of what this

02:56:06.800 --> 02:56:09.700
text is about without actually having to read it we know that

02:56:09.700 --> 02:56:12.700
it's going to deal heavily with Apple and we know that it's

02:56:12.700 --> 02:56:15.000
also going to tangentially deal with some of these other

02:56:15.000 --> 02:56:19.200
things as well but I also want to include into this into this

02:56:19.200 --> 02:56:23.200
pipeline the ability for the entity ruler to not just find

02:56:23.200 --> 02:56:26.200
these things but I also wanted to be able to find different

02:56:26.200 --> 02:56:30.000
stock exchanges so I've got a list I cultivated for different

02:56:30.000 --> 02:56:33.800
stock exchanges which are things like NYSE things like that

02:56:33.800 --> 02:56:40.500
so I can say DS3 is going to be equal to PD dot read CSV data

02:56:40.500 --> 02:56:47.800
backslash stock exchanges dot TSV and then the separator is

02:56:47.800 --> 02:56:51.300
going to be again a tab and let's take a look at what this

02:56:51.300 --> 02:56:52.000
looks like.

02:56:54.700 --> 02:56:56.000
Stanges there we go.

02:57:00.600 --> 02:57:03.300
There we are and we have something that looks like this

02:57:03.300 --> 02:57:09.600
a pretty a pretty large CSV file CSV file sorry that's got a

02:57:09.600 --> 02:57:13.100
bunch of different rows the ones I'm most interested in well

02:57:13.100 --> 02:57:16.700
there's a couple actually I'm interested in specifically the

02:57:16.700 --> 02:57:20.900
Google Prefix and this description the description has

02:57:20.900 --> 02:57:24.400
the actual name and the Prefix has this really nice abbreviation

02:57:24.400 --> 02:57:27.900
that I've seen pop out a few different times such as Nasdaq

02:57:27.900 --> 02:57:31.100
here if we keep on going down we would see different things

02:57:31.100 --> 02:57:35.200
as well NYSE these are kind of different stock exchanges.

02:57:36.000 --> 02:57:40.600
So let's pop back down here and let's go ahead and convert

02:57:40.600 --> 02:57:44.600
those two things into individual lists as well so we're going

02:57:44.600 --> 02:57:49.200
to say exchanges it's going to be equal to DF3 dot ISO Mike

02:57:49.900 --> 02:57:56.800
dot to list and then I'm also going to grab the F3 dot sorry

02:57:56.800 --> 02:58:00.200
Google I have to do this as a dictionary because it's the

02:58:00.200 --> 02:58:03.700
way the data sets cultivated it's got a space in the middle

02:58:03.700 --> 02:58:07.900
this is a common problem that you run into and then I also

02:58:07.900 --> 02:58:12.100
want to know grab all of these exchanges as well so I'm going

02:58:12.100 --> 02:58:20.400
to say also on top of that DF3 dot description to list so I'm

02:58:20.400 --> 02:58:26.700
making a large list exchanges and I get this here because it

02:58:26.700 --> 02:58:30.400
says Google Prefix isn't an actual thing and in fact it's

02:58:30.400 --> 02:58:34.500
prefix with an I and now we actually are able to get all

02:58:34.500 --> 02:58:39.400
these things extracted so what I want to do now is I want to

02:58:39.400 --> 02:58:43.300
work all these different symbols and descriptions into into

02:58:43.300 --> 02:58:46.500
the model as well or into the pipeline as well so I can say

02:58:46.500 --> 02:58:54.500
for for E and exchanges I want to say patterns dot append

02:58:57.700 --> 02:59:01.700
and I want to do a label that's going to be let's do stock

02:59:02.400 --> 02:59:07.300
exchange and then the next thing I want to do is a pattern

02:59:07.600 --> 02:59:11.100
and that's going to be equal to in this case E as we're going

02:59:11.100 --> 02:59:14.600
to see this is not adequate enough we need to do a few

02:59:14.600 --> 02:59:18.100
different things to really kind of work this out but it's going

02:59:18.100 --> 02:59:20.200
to be a good enough to at least get started

02:59:25.200 --> 02:59:26.600
and it's going to take it just a second

02:59:32.000 --> 02:59:34.200
and the main thing that's happening right now are these

02:59:34.200 --> 02:59:38.200
different for loops so if we keep on going down we now see

02:59:38.200 --> 02:59:41.800
that we were able to extract the NYSE stock exchange so we've

02:59:41.800 --> 02:59:44.600
not only been able to work into a pipeline in a very short

02:59:44.600 --> 02:59:47.200
order maybe about 20 30 minutes we've been able to work

02:59:47.200 --> 02:59:50.600
into a pipeline all of these different things that are coming

02:59:50.600 --> 02:59:54.000
out we do however see a couple problems and this is where I'm

02:59:54.000 --> 02:59:56.000
going to leave it though because you've got the basic

02:59:56.000 --> 02:59:59.600
mechanics down now comes time for you being a domain expert

02:59:59.800 --> 03:00:02.100
to work out and come up with rules to solve some of these

03:00:02.100 --> 03:00:06.300
problems Nasdaq is not a company so there's a problem with

03:00:06.300 --> 03:00:10.100
the data set or Nasdaq is listed as a company name and one of

03:00:10.100 --> 03:00:13.500
the data sets we need to work that out where Nasdaq is never

03:00:13.500 --> 03:00:17.100
referenced as a company we have the S&P and is now being

03:00:17.100 --> 03:00:20.200
coming out correctly as S&P 500 there might be instances

03:00:20.200 --> 03:00:23.600
where just S&P is referenced which I think in that context

03:00:23.600 --> 03:00:27.100
would probably be the S&P 500 but nevertheless we've been

03:00:27.100 --> 03:00:33.200
able to actually extract these things sometimes the Dow Jones

03:00:33.200 --> 03:00:37.800
Industrial Average might just be referenced to Dow Jones so

03:00:37.800 --> 03:00:40.400
this index might just be these first two words I know that's

03:00:40.400 --> 03:00:42.900
a common occurrence we've also seen that we weren't able to

03:00:42.900 --> 03:00:45.700
extract some of those things that were a period followed by

03:00:45.700 --> 03:00:50.100
a symbol that referenced the actual index itself nevertheless

03:00:50.100 --> 03:00:52.600
this is a really good starting point and you can see how just

03:00:52.600 --> 03:00:55.200
in a few minutes you're able to generate this thing that can

03:00:55.200 --> 03:00:59.700
extract information from unstructured text at the end of

03:00:59.700 --> 03:01:02.100
the day like I said in the introduction to this entire

03:01:02.100 --> 03:01:07.100
video that's one of the essential tasks of NLP designing

03:01:07.100 --> 03:01:10.600
this and implementing it is pretty quick and easy perfecting

03:01:10.600 --> 03:01:14.200
it is where the time really is to get this financial analysis

03:01:14.800 --> 03:01:19.000
entity ruler working really well where it has almost no false

03:01:19.000 --> 03:01:24.600
positives and almost never misses a true a true positive it

03:01:24.600 --> 03:01:27.100
would take maybe a few more hours of just some kind of working

03:01:27.100 --> 03:01:29.300
and eventually there are certain things you might find that

03:01:29.300 --> 03:01:32.600
would work better in a machine learning model nevertheless

03:01:32.600 --> 03:01:36.000
you can see the degree to which rules based approaches in

03:01:36.000 --> 03:01:39.600
Spacey can really accomplish some pretty robust tasks with

03:01:39.600 --> 03:01:43.400
minimal minimal amount of code so long as you have access to

03:01:43.400 --> 03:01:46.400
or have already cultivated the data sets required.

03:01:49.100 --> 03:01:52.900
Thank you so much for watching this video series on Spacey

03:01:52.900 --> 03:01:55.700
an introduction to basic concepts of natural language

03:01:55.700 --> 03:02:00.700
processing linguistic annotations in Spacey vectors pipelines

03:02:00.700 --> 03:02:03.800
and kind of rules based Spacey you've enjoyed this video

03:02:03.800 --> 03:02:07.200
please like and subscribe down below and if you've also found

03:02:07.200 --> 03:02:10.600
this video useful consider joining me on my channel Python

03:02:10.600 --> 03:02:13.700
tutorials for digital humanities if you have like this and

03:02:13.700 --> 03:02:17.300
found this video useful I'm envisioning a second part to

03:02:17.300 --> 03:02:21.500
this video where I go with the machine learning aspects of

03:02:21.500 --> 03:02:24.000
Spacey if you're interested in that let me know in the

03:02:24.000 --> 03:02:26.700
comments down below and I'll make a second video that

03:02:26.700 --> 03:02:28.000
corresponds to this one.

03:02:28.700 --> 03:02:30.600
Thank you for watching and have a great day.

