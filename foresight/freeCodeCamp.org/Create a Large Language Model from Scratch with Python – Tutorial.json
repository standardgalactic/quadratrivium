{"text": " Learn how to build your own large language model from scratch. This course goes into the data handling, math and transformers behind large language models. Elliot Arledge created this course. He will help you gain a deep understanding of how LLMs work and how they can be used in various applications. So let's get started. Welcome to Intro to Language Modeling. In this course, you're going to learn a lot of crazy stuff. Okay, I'm just going to give you a heads up. It's going to be a lot of crazy stuff we learn here. However, it will not be insanely hard. I don't expect you have any any experience in calculus or linear algebra. A lot of courses out there do assume that, but I will not. We're going to build up from square one. We're going to take baby steps when it comes to new fundamental concepts in math and machine learning. And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out. That way we don't take forever just taking baby steps through every little concept. This course is inspired by Andre Karpathy's. Building a GPT from scratch lecture. So shout out to him. And yeah, we don't assume you have any experience, maybe three months of Python experience. Just so the syntax is sort of familiar and you can. You're able to follow along that way, but no matter how smart you are, how quick you learn. The willingness to put in the hours is the most important because this is material that you won't normally come across. So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water. Water is very important. And yeah, hopefully you can make it to the end of this. You can do it. Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing. We'll be scaling the data to about 45 gigabytes for the entire training data set. So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45. So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine. You can just download a different data set and sort of follow the same data pipeline that I do in this video. Through the course, you may see me switch between Mac OS and Windows. The code still works all the same, both operating systems, and I'll be using a tool called SSH. It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook. So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording. So why am I creating this course? Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space. So I intend to build up from baby steps and then larger steps when things are fairly simple to work with. And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you. So with that being said, let's go ahead and jump in to the good stuff. So in order to develop this project step by step, we're going to use something called Jupyter notebooks. And you can sort of play with these in the Anaconda prompt or at least launch them from here. So Anaconda prompt is just great for anything machine learning related. So make sure to have this installed. I will link a video in the description so that you can sort of set this up and install it step by step guide in there. So we can do from this point is sort of just set up our project and initialize everything. So I'm going to do is just head over into my directory that I want to be Python testing. We're going to make a directory free code camp GPT course. And then from this point, we're going to go and make a virtual environment. So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around. And what the virtual environment does is it sort of separates that. So you have this isolated environment over here, and you can just play around with this however you want. And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system. When you're not in a virtual environment, if that makes sense. So we're going to go ahead and set that up right now by using Python dash M. And then we're going to go V and V for virtual V and V and then CUDA. So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs. GPUs are going to accelerate this a ton. And basically CUDA is just that little feature in the GPU that lets us do that. So we're going to make an environment called CUDA. I'm going to go and press enter. It's going to do that for us going to take a few seconds. So now that's done, we can go ahead and do CUDA. And we're just going to basically activate this environment so we can start developing in it. I'm going to go backslash, we're going to go scripts, and then activate. So now you can see it says CUDA base. So we're in CUDA and then secondary base. So it's going to prioritize CUDA. So from this point, we can actually start installing some stuff, some libraries here. So we can go pip three install Matt plot lib numpy. We're going to use p y l m z a l z m a. And then what are some other ones? We're going to do IPY kernel. This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks. So that's why that's important. And then just the actual Jupyter notebook feature. So go and press enter. Those are going to install. That's going to take a few seconds to do. So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm. And don't quote me on this, but I'm pretty sure it's based in C plus plus. So you actually need some build tools for this. And you can get that with visual studio build tools. So what you're you might see, you might see a little error and basically go to that website. You're going to get this right here. So just go ahead and download build tools. What's going to download here, you're going to click on that. It's going to, it's going to set up. And then you're going to go ahead and click continue. And then at this point, you can go ahead and click modify if you see this here. And then you might get to a little workloads section here. So once you're at workloads, that's good. What you're going to make sure is that you have these two checked off right here. Just make sure that you have these two. I'm not sure what desktop particularly does. It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects. So just get these two for now. That'll be good. And then you can click modify over here if you wanted to modify just like that. And then you should be good to rerun that command. So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch. We're not going to do it like this. What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch. So it's going to install the CUDA extension, which will allow us to utilize the GPU. So it's just this command right here. And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started. And then you'll be able to see this right here. So we have stable windows pip Python and CUDA 11.7 or 11.8. So I just clicked on this. And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch. And then with this index URL for the CUDA 11.8. So that's pretty much all we're doing there to install CUDA. That's part of our torch. So we can go ahead and click enter on this. So great. We've installed a lot of things, a lot of libraries, a lot of setup has been done already. What I want to check now is just to make sure that our Python version is what we want. So I've done version 3.10.9. That's great. If you're between 3.9, 3.10, 3.11, that's perfect. So if you're in between those, it should be fine. At this point, we can just jump right into our Jupyter Notebook. So the command for that is just Jupyter Notebook. It's about like that. Click enter. It's going to send us into here. And I've created this little biogram.ipynb here in my VS Code. So pretty much you need to actually type some stuff in it. And you need to make sure that it has the ipynb extension or else it won't work. So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason. So just make sure you type some stuff in it. Open that in VS Code. Type, I don't know, a equals 3 or str equals banana. I don't care. At this point, let's go ahead and pop into here. So this is what our notebook is going to look like. And we're going to be working with this quite a bit throughout this course. So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook. And make sure that we can interact with it from this kernel rather than just through the command prompt. So we're going to go ahead and check here. And I have a virtual environment here. You may not, but all we're going to do is basically go into here. We're going to end this. And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install. User, you'll see why we're doing this in the second user name equals CUDA. This is from the virtual environment we initialized before. So that's the name of the virtual environment. And then the display name, how it's actually going to look in the terminal is going to be display name. We'll just call it CUDA GPT. I don't know. That sounds like a cool name. And I'm going to press enter. It's going to make this environment for us great installed. Good. So we can go and run our notebook again and we'll see if this changes. So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT. Let's click that. Sweet. So now we can actually start doing more and just sort of experimenting with how the notebooks work and actually how we can build up this bi-gram model and sort of learning how language models work from scratch. So let's go ahead and do that. Now that we jump into this actual code here, what I want to do is delete all of these. Good. So now what I'm going to do is just get a small little data set, just very small for us to work with that we can sort of try to make a bi-gram out of, something very small. So what we can do is go to this website called Project Gutenberg and they basically just have a bunch of free books that are licensed under Creative Commons. So we can use all of these for free. So let's use the Wizard of Oz. Put it at the end of Wizard of Oz. Great. So what we're going to want to do is just click on plain text here. Great. So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore of underscore Oz. Good. So now what I'm going to do is we should probably drag this into, we should drag this into our folder here. I'm just going to pop that into there. Good stuff. Did that work? Sweet. So now we have our Wizard of Oz text in here, we can open that. What we can do is start of this book. Okay. So we can go ahead and go down to when it starts. Sweet. So maybe we'll just cut it here. That'd be a good place to start. Just like that. I'll put a few spaces. Good. So now we have this book. We go to the bottom here just to get rid of some of this other licensing stuff, which might get in the way with our predictions in the context of the entire book. So let's just go down to when that starts. End of the book. Okay. So we've gotten all that. That is done. Get rid of the illustration there. Perfect. So now we have this Wizard of Oz text that we can work with. Let's close that up. 233 kilobytes. Awesome. Very small size. We can work with this. This is great. So we have this wizard of Oz dot txt file. And what are we going to do with that? Well, we're going to try to train a transformer or at least a background language model on this text. So in order to do that, we need to sort of learn how to manage this text file, how to open it, et cetera. So we're going to go ahead and open this and do wizard of Oz. Like that. And we're going to open in read mode. And then we're going to use the encoding utf 8 just like that. So this is the file mode that you're going to open in. There's read mode. There's write mode. There's read binary. There's write binary. And those are really the only ones we're going to be worrying about for this video. The other ones you can look into in your spare time if you'd like to. I've already seen using those four for now. And then the encoding is just what type of character coding are we using? That's pretty much it. We can just open this as F short for file. I'm going to go text equals f dot read. I'm going to read this file stored in a string variable. And then we can print some stuff about it. So we can go print the length of this text. Run that. We get the length of the text. We could print the first 200 characters of the text. Sure. So you have the first 200 characters. Great. So now we know how to, you know, just play with characters. At least just see what the characters actually look like. So now we can do a little bit more from this point, which is going to be encoders. And before we get into that, what I'm going to do is put these into a little vocabulary list that we can work with. So all I'm going to do is I'm going to say we're going to make a charge variable. So the charge is going to be all the charge or all the characters in this text piece. So we're going to make a sorted set of text here, and we're going to just print out charge. So look at that. We have a giant array of all these characters. So now we can, what we can do is we can use something called a tokenizer and a tokenizer consists of an encoder and a decoder. What an encoder does is it's actually going to convert each character or sorry, each element of this array to an integer. So maybe this would be a zero. This would be a one, right? So a new, a new line or an enter would be a zero, a space would be a one exclamation mark would be a two, et cetera, right? All the way to the length of them. And then what we could do is we could even, we could even print the length of these characters. So you can see how many there actually are. So there's 81 characters in the entire, in the entire Wizard of Oz book. So I've written some code here that is going to do that job for us, the job of tokenizers. So what we do is we just use a little generator, some generator for loops here, a generator for loops rather, and we make a little mapping from strings to integers and integers to strings, given the vocabulary. So we just enumerate through each of these. We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right? That's basically all we're doing here. And we have an encoder and a decoder. So let's say we wanted to convert the string hello to integers. So we go encode, and we could do hello, just like that. And then we could go ahead and print this out. Perfect. Let's go ahead and run that. Boom. So now we have a conversion from characters to integers. And then if we wanted to maybe convert this back, so decode it, sort this in a little, maybe decoded hello equals that. And then we could go or encoded rather encoded hello. And then we could go decoded. Hello is equal to we go decode and we can use the encoded hello. So we're going to go ahead and encode this into integers. And then we're going to decode the integers back to a character format. So let's go ahead and print that out. We're going to go ahead and print the decoded hello. Perfect. So now we get that. So I'm going to fill you in on a little background information about these tokenizers. So right now we're using the character level tokenizer, which takes basically each character and converts it to an integer equivalent. So we have a very small vocabulary and a very large amount of tokens to convert. So if we have 40,000 individual characters, it means we have a small vocabulary to work with, but a lot of characters to encode and decode, right? If we have, if we work with maybe a word level tokenizer, that means we have a ton, like every single word in the English language, I mean, if you're working with multiple languages, this could be like, you know, a lot, very large amount of tokens. So you're going to have like maybe millions or billions or trillions if you're, if you're doing something weird. But in that case, you're going to have a way smaller set to work with. So you're going to have very large vocabulary, but a very small amount to encode and decode. So if you have a subword tokenizer, that means you're going to be somewhere in between a character level and a word level tokenizer, if that makes sense. So in the context of language models, it's really important that we're efficient with our data and just having a giant string might not work the best. And we're going to be using a machine learning framework called pi torch or torch. So I've imported this right here. And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well. A lot of the linear algebra, which involves a type of data structure called tensors. So tensors are pretty much matrices. If you're not familiar with those, that's fine. We'll go over them more in the course. But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with. So I'm going to go ahead and delete these here. And all we can do is just make our data element. We could this is going to be the entire text data of the entire Wizard of Oz. So we could go ahead and make this data equals and we're going to go torch tensor. And then we're going to go and code. We're going to put the text inside of that. So we're going to go ahead and encode this text right here. And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long. This basically means we're just going to have this as a super long sequence of integers. And yeah, let's go see what we can do with this torch tensor element right here. So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data. So it's pretty much the same thing in terms of working with arrays. It's just a different type of data structure in the context of pi torch sort of easier to work within that way. Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot. But we're going to go over some of this stuff later in the course just about how to do all this math. We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff. So next I'm going to talk about is something called validation and training splits. So why don't we just, you know, use the entire text document and only train on that entire text corpus? Why don't we train on that? Well, the reason we actually split into training and validation sets, I'm going to show you right here. So we have this giant text corpus. It's a super long text file. Think of it as a, you know, an essay, but a lot of pages. So this is our entire corpus and we make our training set, you know, 80% of it. So maybe this much. And then the other validation is this 20% right here. Okay. So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out. It would have it in the entire thing memorized. It wouldn't really get anything useful out of that. You would only know this document. But what the purpose of language modeling is, is to generate text that's like the training data. And this is exactly why we put into splits. So if we, if we run our training split right here, it's only going to know 80% of that entire corpus. And it's only going to generate on that 80% instead of the entire thing. And then we have our other 20%, which only knows 20% of the entire corpus. So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document. We're trying to generate text that's like the document. Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare. And the point is to generate Shakespearean like text, but not exactly what it looked like. Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right? We're trying to generate text that's like it. So that's the entire reason, or at least that's most of the reason why we use train and vowel splits. So you might be wondering, you know, like, why is this even called the bi-gram language model? I'm actually going to show you how that works right now. So if we go back to our whiteboard here, I've drawn a little sketch. So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now. We're just working with characters. Pretty much we have two, right? So by means to the by prefix means two. So we're going to, we're going to have a bi-gram. So given maybe, I mean, there's nothing before an H in this content. So we just assume that's the start of content, and then that's going to point to an H. So H is the most likely to come after the start. And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L, we're going to have another L, and then L leads to O, right? So maybe there's going to be some probabilities associated with these. So that's pretty much how it's how it's going to predict right now. It's only going to consider the previous character to predict the next. So we have given this one, we predict the next. So there's two, which is why it's called bi-gram language model. So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want, how we can actually implement this into a neural network, an artificial neural network, and train it. So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here, just a small snippet, and we're going to make some predictions and we're going to make some targets out of that. So our block size is just a bunch of encoded characters or integers that we have predictions and targets. So let's say we take a small little size of maybe block size of five, okay? So we have this tiny little tensor of five integers and these are our predictions. So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one. So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside. So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one. So we're going to be accessing the same indices. So at index zero, it's going to be five, index zero is going to be 67, right? So 67 is following five in the bi-gram language model. So that's pretty much all we do. So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target. And then we can optimize for reducing that error. So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here. We would take a little snippet random. It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size. So five. Ignore my terrible writing again. And then this one would just be it would just be one up to block size or five plus one. So we'll be up to six, right? And that's that's pretty much all we do. This is exactly what it's going to look like in the code. So I've written some code here that does exactly what we just talked about in Python. So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger. And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one. So we make it position one up to block size plus one little offset there. And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are. So this is what this looks like in Python, right, we can do predictions. But this isn't really scalable yet. This is sequential right sequential. It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly. That only happens sequentially it's this one and this task and this task and this task, right. But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel. So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them. So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot. So I'm going to illustrate that for you right now. So let's just say we have a block. Okay, block looks like this. And we have some we have some integers in between here. So this is a block. Okay. Now, if we want to make multiple of these, we're just going to stack them. So we're going to make another one. Another one. Another one. So let's say we have four batches. Okay. Or sorry, four blocks. So we have four different blocks that are just stacked on top of each other. And we can represent this as a new hyper parameter called batch size. This is going to tell us how many of these sequences can we actually process in parallel. So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time. So this is a really good way to scale language models. And without these, you can't really expect any fast training or good performance at all. So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process. And we can, it just takes one line to do this actually. So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation. And if it's available, like it says it's available, we'll set the device to CUDA else CPU. So we're going to go and print out the device here. So that's going to run and we get CUDA. So that means we can use the GPU for a lot of our processing here. And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size. And then we're going to use batch size, which is how many blocks we're doing in parallel. And we're just going to make this four for now. So these are our two hyper parameters that are very, very important for training. And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it. And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first. It'll shut down and you just go up Jupyter notebook again and then enter. And then after this is done, you should be able to just restart that and it will work. Hopefully. There we go. So I go in restart and clear outputs. And we can run that. See, we get boo. So awesome. Now, let's try to do some actual cool pie torch stop. So we're going to go in and import torch here. And then let's go ahead and try this random feature. So you go random. We'll do equals torch dot random. And then let's say we go minus 100 to 100. And then in brackets, we go six, just like that. So if we want to print this out here, or we could just go random like that. Run this block first. Good. And boom. So we get a tensor type. And all these numbers are we have we have six of them. So 123456. And they're between negative 100 and 100. So we're going to have to keep this in mind right here when we're getting our random batches from this giant text corpus. So let's try out a new one. Let's just try. We can make we can make tensors. We've done this before. So you do tensor equals torch dot tensor. So if you go 0.1, 1.2, here, I'll just copy and paste one right here. So we do this. And we can just do tensor and we'll get exactly this. So we get a three by two matrix. Now we're going to try a different one called zeros. So zeros is just torch dot zeros. And then inside of here, we could just do the dimensions or the shape of this. So two by three, and then we could just do zeros. And then go ahead and run that. So we get a two by three of zeros. And these are all floating point numbers, by the way. Maybe we could try ones. Now I know ones is pretty fun ones. So we both torch dot ones. It's pretty much the same as zeros. We could just do like maybe three by four and then print that ones out. So we have a three by four of ones. Sweet. So what if we do input equals torch dot empty. We can make this two by three. So these are interesting. These are pretty much a bunch of very either very large or very small numbers. I haven't particularly found a use case for this yet, but just another feature that PyTorch has. We have a range. So we go arrange equals torch dot arrange. I could do like five, for example, just do range. So now we have a tensor just sorted zero or rather starting at zero up to four. So five, just just like that. Line space equals torch dot line line space. Spelling is weird to three, 10, and then steps, for example, equals five. So it makes sense in a second here, go run. And we got a line space of steps equals five. So we have five different ones, boom, boom, boom, boom, boom. And we go all the way from three to 10. So pretty much getting all of the constant increments from three all the way up to 10 over five steps. So you're doing, you're basically adding the same amount every time. So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right? So just over five steps, we want to find what that constant increment is. So that's a pretty cool one. And then we have, we'll do log space, which is interesting. Log space equals torch dot log space. And then we'll go start, start equals negative 10 and equals 10. These are both start and end. You can either put these here. You can either put the start with them, start equals, or you don't have to. It's honestly up to you. And then we can put our steps again. So steps equals every five. Let's go ahead and run that. Oops, need to put log space there. So we get that. So we start at one of the negative 10. And then we just do this little increments here. So it goes 10, negative five, zero plus five times. Just over five steps. So that's pretty cool. What else do we have here? So we have I, torch dot I. I just have all these on my second screen here. So a bunch of examples just written out and we're just kind of visualizing what these can do. And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do. So we're just kind of experimenting with these. What we can do with the basics of pytorch and some of the very basic functions. So first I will print this out here. So we get pretty much just a diagonal line and it's in five. So you get a five by five matrix and pretty much just reduced row each long form. I don't know how to pronounce it, but that's pretty much what it looks like. So pretty cool stuff. Let's see what else we have. We have empty like. We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty. And then we can go two by three and then data type torch dot int 64 64 bit integers. And then let's see what happens here empty. So that's pretty cool. What else do we have? Yes, we can do timing as well. So I'm just going to erase all of these. You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit. But yeah, I'm actually going to delete all of these here. And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment. Cuda if torch dot Cuda underscore is dot Cuda is available. And then else you print out our device here and run this Cuda suite. So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality. We're just doing different experiments with a bunch of different tensors. So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time. So this comes with the operating system or sorry with with Python. You don't have to actually install this manually. So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now. So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds. So if we subtract end time, start time will get a three second difference and that would be the total elapsed time. And then this little number here, this four will be just how many decimal places we have. So I can go ahead and run this here. Time is not defined. Let's run that first. It's going to take, you know, almost no time at all. So we can actually increase this if we want to 10 and then run that again. Again, it's, you know, we're making up pretty much a one by one matrix. So just a just a zero. So we're not really going to get anything significant from that. But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network. That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem. It'll take maybe a few seconds at most like maybe 30 seconds to do that. And that's not going to be any time at all in the entire training process. So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU. So I have some code for that right here. So we're going to initialize a bunch of matrices here. So our sorry, tensors, and we have just basically random ones. So we have a 10,000 by 10,000, all random, all random floating point numbers. And then we're going to push these to the GPU. And we have two of these and then same thing for NumPy. So in order to actually multiply matrices with PyTorch, we need to use this at symbol here. So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply. So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds. And then we have, see, look at that. So for the GPU, it took a little while to do that. And then for the CPU, it didn't take as long. So this is because there's the shape of these matrices are not really that big. They're just two dimensional, right? So it's see, this is something that the CPU can do very quickly because there's not that much to do. But let's say we want to bump it up a notch. So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there. Hopefully that works. And then we can do, we'll just do the same thing. So just paste this. Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did. And this is because there's, you know, a lot more going on here. There's a lot more simple multiplication to do. So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model, we're not going to be doing very complex operations between all these tensors. They're going to be very similar to what we saw in here. The dimensionality and shape is going to be very similar to what we're seeing right now. You know, maybe three or four dimensions. And it's going to be very easy for a GPU to do this. They're not complex tasks that we need the CPU to do. They're not very hard at all. So when we give this task to parallel processing, it's going to be a ton quicker. So you're going to see why this matters later in the course. You're going to see this with some of the hyper parameters we're going to use, which I'm not going to get into quite yet, but over the next little bit, you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process. So this is great. Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency. So there's actually another term that we can use called a percentage percentage time. I don't know if that's exactly how you're supposed to call it, but that's what it is. And pretty much what it'll do is time how long it takes to execute a block. So we can see here there's CPU times zero nanoseconds. The end is for nano billionth of a second is a nanosecond and then wall time. So CPU time is how long it takes to execute on the CPU. The time that it's doing operations for and then the wall time would be how long it actually takes like in real time. How long do you have to wait? Do you have to wait until it's finished? So the only thing that the CPU CPU time doesn't include is waiting. So in an entire process, there's going to be some operations and there's going to be some waiting. Wall time is going to have both of those and CPU time is just the execution. So let's go ahead and continue with some of the basic PyTorch functions. So I've written some stuff down here. So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu. I don't think that's how you pronounce it, but we'll get into that more. Transposing, linear, concatenating, and the softmax function. So let's first start off here with the Torch.multinomial. So this is essentially a probability distribution based on the index that you give it. So we have probabilities here. We say 0.1 and 0.9. These numbers have to add up to one to make 100%. 100% is one, one whole. So I have 10% and 90%. This is an index zero. So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one. So if I go ahead and run these up here. Give this a second to do its thing. So you can see that in the end we have our numSample set to 10. So it's going to give us 10 of these. 1, 2, 3, 4, 5, 6, 7, 9, 10. And all of them are ones. If we run it again, we make it slightly different results. So now we have some zeros in there. But the zeros have very low probability of happening. As a matter of fact, exactly a 10% probability of happening. So we're going to use this later in predicting what word is going to come next. Let's move on to Torch.cat or short for Torch.concatenate. So this will essentially concatenate two tensors into one. So I initialize this tensor here, torch.tensor, 1, 2, 3, 4. It's one dimensional. And we have another tensor here that just contains five. So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5. We just combine them together and this is what will come out in the end. So we run that 1, 2, 3, 4, 5. Perfect. So we're going to actually use this when we're generating. When we're generating text given a context. So it's going to start from zero. We're going to use our probability distribution to pick the first one. And then based on the first one, we're going to, you know, we're going to predict the next character. And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted. So we have this, maybe like 100 characters over here. And then the next character that we're predicting is over here. We just concatenate these. And by the end, we will have all of the integers that we've predicted. So next up, we have torch.trill. And what this stands for, what the trail stands for is a triangle lower. So it's going to be in a sort of a triangle formation like this diagonal. It's going to be going from top left to bottom right. And so you're going to see a little bit more why later in this course. But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history. We're trying to predict the future. So giving the answers in the future isn't what we want to do at all. So maybe we've just predicted one and the rest of them we haven't predicted yet. So we set all these to zero. And then we predicted another one. And these are still zero. So these are talking to each other in history. And as and as our predictions add up, we have more and more history to look back to and less future, right? Basically, the premise of this is just making sure we can't communicate with the answer. We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet. They don't give you the answer sheet. So you have to know based on your history of knowledge, which answers to predict. And that's all that's going on here. And we have, I mean, you could probably guess this triangle upper. So we have all the upper ones. These are, you know, lower on the lower side and then these are on the upper side. So same concept there. And then we have a masked fill. So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here. So if you exponentiate zero, if you exponentiate zero, it'll become one. If you exponentiate negative infinity, it'll become zero. All that's going on here is we're doing approximately 2.71. And this is a constant that we use in the dot exp function. And then we're putting this to whatever power is in that current slot. So we have a zero here. So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71. And then 2.71 to the negative infinity is, of course, zero. So that's pretty much how we get from this to this. And we're just, we're simply just masking these over. So that's great. And I sort of showcase what the exp does. And we're just using this one right here. We're using this output and we're just plugging it into here. So it'll go from negative infinity to zero and then zero to one. So that's how we get from here to here. Now we have transposing. So transposing is when we sort of flip or swap the dimensions of a tensor. So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four. And we can use the transpose function to essentially flip any dimensions that we want. So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension, but we're pretty much swapping the zero with position with the second. So zero, one, two, we're swapping this one with this one. So the end result, like you would probably guess the shape of this is going to be 432 instead of 234. So you kind of just take a look at this and see, you know, which ones are being flipped. And those are the dimensions and that's the output. So hopefully that makes sense. Next up, we have torch dot stack. And this is where we're actually going to go. We're going to we're going to do more of this. We're actually going to use torch dot stack stack very shortly here when we're getting our batches. So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant, a giant length of integers or tokens. And all we're doing is we're just stacking them together in blocks or to make a batch. So that's pretty much what we're going to end up doing. And that's what torch dot stack does. We can take something that's one dimensional and then we can stack it to make it two dimensional. We can take something that's two dimensional and stack it a bunch of times to make it three dimensional. Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other. Now it's four dimensional. So hopefully that makes sense. All we're doing is we're just passing in each tensor that we're going to stack in order. So this is our little output here and that's pretty much all it is. The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish. It's really important. It's called the nn dot linear function. So it is a pretty much a function of the nn dot module. And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters. So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false. But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better. And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output. So pretty much anything with nn dot linear is going to be very important and it's going to be learnable. So we can see over here. This is the tors.nn little site here on the docs. So we have containers, a bunch of different layers like activations, layers, pretty much just layers. That's all it is. And so these are these are important. We're going to, we're basically going to learn from these. And you're going to see why we're going to use something called keys and values, keys, values and queers later on. You'll see why those are important. But if that doesn't make sense yet, help me, let me illustrate value for you right now. So I drew this out here. So if we look back at our examples, we have a, we make, we initialize a term. We make, we initialize a tensor. It's 10, 10 and 10. What we're going to do is we're going to do a linear transformation. This linear stands for linear transformation. So pretty much we're just going to apply a weight and a bias through each of these layers here. So we have an input and we have an output x is our input, y is our output. And this is of size three and this is of size three. So pretty much we just need to make sure that these are lining up. And for more context, the nn.sequential is sort of built off nn.linear. So if we go ahead and search that up right now, this will make sense in a second here. This is also some good prerequisite knowledge in general for machine learning. So let's see nn.sequential doesn't show it here, but pretty much. If you have, let's say, two, you have two input neurons and maybe you have one output neuron. Okay, you have a bunch of hidden layers in between here. Let's say we have one, two, three, four, and then one, two, three. So pretty much you need to make sure that the inputs aligns with this hidden layer. This hidden layer aligns with this one and this one aligns with this one. So you're going to have a transformation of two to four. So two, four, and then this one's going to be four to three, four to three, and then you're going to have a final one. This is two to four right here, four to three here, and then this final one. It's going to be three to one. So you pretty much just need to make sure that these are lining up. So we can see that we have two, four, and then this four is carried on from this output here. And pretty much this will just make sure that our shapes are consistent. And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work. So we need to make sure that our shapes are consistent. If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets, but if you're really interested, you could use chatGPT, of course. And that's a really good learning resource, chatGPT, going on to get up discussions, maybe, or just looking at documentation. And if you're not good at reading documentation, then you could take maybe some little keywords from here, like a sequential container. Well, what is a sequential container? You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation and figure things out step by step. It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on. You don't need to memorize them. But while you're working with them, it's important to understand what they're really doing behind the scenes, especially if you want to make an efficient and popular working neural net. So that's that. And pretty much what's going to happen here with these linear layers is we're just going to simply transform from one to the other input to output, no hidden layers. And we're just going to be able to learn best parameters for doing that. You're going to see why that's useful later. Now we have the softmax function. So that sounds scary. And the softmax function isn't actually what it sounds like at all. Let me illustrate that for you right now. So let's go ahead and change the color here. So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc. Right, floating points, whatever. So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these. And we're going to divide them by the sum of all of these exponentiated. So pretty much what's going to happen, let's say we exponentiate one. So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp. And I think I talked about this up here. This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating. So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71. And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two. Okay, so we're going to get 7.34. I'm going to get 7.34. Gorg my writing, it's terrible. 2.71 to 3 cubed. So 19.9. So pretty much what's going to happen is we can rearrange this in a new array. 7.34 and 19.9. So if we add all these up together, we add all these up together, we're going to get 2.71 plus this. Let's do this math real quick. I'm just going to walk you through this to help you understand what the softmax function is doing. 7.34 plus 19.9. That's going to give us a total of 29.95. Great. 29.95. So all we do is we just divide each of these elements by the total. So 2.71 divided by this is going to give us maybe x. And we do 7.34 divided by this is going to give us y. And then we have 19.9 divided by this is going to give us z. So pretty much you're going to exponentiate all of these. You're going to add them together to create a total. And then you're going to divide each of those exponentiate elements by the exponentiated total. So after that, this x right here is just, we're just going to wrap these again. And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z. That's all it's doing. And yeah, it's not really crazy. There's a weird formula for it. Softmax, softmax function. So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here and a lot of math that's above the high school level. But yeah, like this formula here, I believe this is what it is. Or standard unit, softmax function, there you go. So pretty much this is what it does. And there's your easy explanation of what it does. So you're going to see why this is useful later, but it's just important to know what's going on so that you won't lag behind later in the course when this background knowledge becomes important. So if we go over a little example of that, of the softmax function in code, it looks like this right here. So we import torsha and n dot functional as f, f short for functional. And we pretty much just do f dot softmax and then plug in a tensor. And what we want the dimension to be the output dimension. So if we plug this into here and we print it out, we go and print it out. It's going to take a second. Torch is not defined. So let's run this from the top here. Boom. And let's try that again. Boom. There we go. So if you took all those values, let's actually do this again from scratch. So we do 2.71, 2.71 divided by 29.95. We get 0.09, 0.09. Good. And then if we do 7.34 divided by 29.95, we get 0.245. So 0.245. Well, it's kind of close. Really close actually. And then 66.52. So if we go, what was that last one there? 19.9. So we do 19.9 divided by 29.95. 66.4. So 66.5. It's pretty close. Again, we're rounding, so it's not perfectly accurate. As you can see, they're very close and for only having two decimal places, we did pretty good. So that's just sort of illustrating what the softmax function does and what it looks like in code. We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line. It's just like that. So now we're going to go over embeddings. And I'm not actually, I don't have any code for this yet. We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning. So if we pop in a chat GPT here, we say, what is an end dot embedding? And then dots. Let me type in a non-bedding class in the PyTorch library. Okay, actual language processing max maps each discrete input to a dense vector representation. Okay, how does this work? Let's see. So we have some vocab. So that's probably our vocabulary size. I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set. That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter. So let's see. This doesn't quite make sense to me yet. So maybe I want to learn what does this actually look like? Can you explain this to a, maybe an eighth grader and provide a visualization? Certainly. Okay. Little secret codes that represent the meaning of the words. Okay, that helps. So if we have cat, okay, so cat, cat's a word. So maybe we want to know what it would look like on a character level. What about on a character level instead of word level? So it's probably going to look very similar. We have this little vector here storing some information about whatever this is. So a, it means this here. Okay, so as your point to, and this is really useful. So we've pretty much just learned what embedding vectors does. And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character. And we don't even know what each of these elements mean. We don't know what they mean. This could be maybe positivity or should be the start of a word or it could be any piece of information, maybe something we can't even comprehend yet. But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before, nn.embedding right here is a part of the nn.module. So these are learnable parameters, which is great. So it's actually going to learn the importance of each letter, and it's going to be able to produce some amazing results. So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter. In our case, it's character level, not subword, not word, it's character level. So it's going to represent some meaning about those. So that's what embedding vectors are. Let's go figure out how they work in code. We have this little character level embedding vector and it contains a list. There's five elements in here, one, two, three, four, five, and it's by the vocab size. So we have all of our vocabulary by the length of each embedding vector. So this actually makes sense because our vocab size by the embedding dimension, which is how much information is actually being stored in each of these characters. So this now is very easy to understand. I'm just going to copy this code from here and I'm going to paste it down here. And let's just get rid of the torch because we already initialized that above. So if we just run this, actually, let's turn that down to maybe a thousand characters. Let's try that out. And it's not defined. We did not initialize it. So let's go back down here and look at that. So this dot shape is going to essentially show the shape of it this much by this much. So it's four by a hundred. And yeah, so we can we can work with these and we can store stuff about characters in them. And you're going to see this in the next lecture, how we actually use embedding vectors. So no need to worry if a lot of this doesn't make sense yet. That's fine. You're going to learn a little bit more about how we use these over the course. You're going to get more confident with using them even in your own projects. So don't don't stress about it too much right now. Embeddings are pretty tricky at first to learn. So don't worry about that too much. But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks. So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say. But that's the architecture of this multilayer perceptron. But pretty much what's happening is we have a little input here and we have a white matrix. So white matrix is looks like this. It's like this and we have some, we have some values in between X1, Y1 and maybe Z1. So a bunch of weights and maybe biases to that we add to it. So the tricky part is how do we actually multiply our input by this white matrix? We're just doing one matrix times another. Well, that's called matrix multiplication. And I'm going to show you how to do that right now. So first off, we have to learn something called dot products. So dot products are actually pretty easy and you might have actually done them before. So let's say we go ahead and take, we go ahead and take this right here we go. One, two, three. That's going to be what A is. And then we have four, five, six. So if we want to find the dot product between these two, all we have to do is simply take the index of both of these, the first ones and the second ones and third ones, multiply them together and then add. So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five, and then add it to three times six. So one times four is four, two times five is ten, three times six is eighteen. So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two. So the dot product of this is going to be thirty-two. And that's pretty much how simple dot products are. It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up. That's a dot product. So we actually need dot products for matrix multiplication. So let's go ahead and jump into that right now. So I'm just going to create two matrices that are going to be pretty easy to work with. So let's say we have A and have one matrix over here. It's going to be one, two, three, four, five and six. This is going to be equal to A and then B is going to be another matrix. So we're going to have seven, eight, nine, ten, eleven, twelve. Ignore my terrible writing. Pretty much what we do is to multiply these together. First we need to make sure that they can multiply together. So we need to take a look at the amount of rows and columns at this half. So this one right here is three rows, one, two, three. Three rows and two columns. So this is going to be a three by two matrix. And this one has two rows and three columns. So it's a two by three matrix. So all we have to make sure that if we're multiplying A dot product with B, and this is the PyTorch syntax for multiplying matrices, if we're multiplying A by B, then we have to make sure the following is true. So if we use three by two and then dot product with two times three, we have to make sure that these two inner values are the same. So two is equal to two, so we cross these out, and then the ones that we have left over are three by three. So the resulting matrix would be A three by three. Or if you had like a three by four times A five by five by one, that doesn't work because these values aren't the same. So these two matrices couldn't multiply. And sometimes you actually have to flip these to make them work. So maybe we change this value here to A three. We change this value to a three. In this order, they do not multiply. But if we switch them around, we have a three by five with A five by three, sorry, five by three with A three by four. So these two numbers are the same. That works. The resulting matrix is a five by four. So that's how you make sure that two matrices are compatible. So now to actually multiply these together, what we're going to do, I'm going to make a new line here. So we're going to rewrite these. Now we don't have to rewrite them. Let's just cross that out here. So pretty much what we have to do is we have to take these two and dot product with these two. And then once we're done that, we do the same with these and these, these and these. So we start with the first, the first row in the A matrix. And we iterate through all of the columns in the B matrix. And then after we're done that, we just go to the next row in the A matrix and then et cetera, right? So let's go ahead and do this right now. That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here. So we have our one times, our one times seven plus two times 10. So one times seven plus two times 10. And this is equal to 27. So that's the first dot product of one and two and seven and 10. So what this is actually going to look like in our new matrix, I'm going to write this out here. So this is our new matrix here. This 27 is going to go right here. Let's continue. So next up, we're going to do one and two and then eight and 11. So we're going to go one, one times eight plus two, or sorry, two and 11. So one times eight is eight and then two times 11 is 22. So our result here is 30 and 30 is just going to go right here. So 27, 30, and you can see how this is going to work, right? So in our first row of A, we're going to get the first row of this resulting matrix. So let's go ahead and do the rest here. So we have one and two and then nine and 12. One times nine, two times 12. One times nine is nine, two times 12 is 24. So if we do, that's like 33, I believe. So 33 and we can go ahead and write that here. So now let's move on to the next. We have three and four, three, three and four dot product with seven and 10. So three will multiply seven. And then we're going to go ahead and add that to four times 10. Three times seven, three times seven is 21, and then four times 10 is 40. So we're going to get 47. So I'll put there so we can go in and write 47 right there. And our next one is going to be three and four dot product with eight and 11. So eight plus four times 11. Perfect. So we get three times eight is 24 and then plus 44. So 24 plus 44, that's 68. So we get 68 and we can go in and write that here. So next up, we have three and four and nine and 12. So three times nine is 27. And then four times 12. So let's just, let's just do that. I'm not doing that in my head. 27 plus was four times 12. So that's 48. 27 plus 48 gives us 75. Let's go ahead and write our 75 here. Then we can go ahead and slide down to this row since we're done, since we're done that. And then we go five, five and six dot product was seven and 10. So our result from this five times seven is 35 and then six times 10 is 60. So we're going to get 95. We can go in and write our 95 here. And then five and six dot product with eight and 11. So five times eight is 40 and then six times 11 is 66. So we get 104. And then the last one, so five and six dot product with nine and 12. So five, five times nine is 45. And then six times 12 is what six times 12, 72, I think. So six times 12, 72. Yeah. So 45 plus 72, 117. And that is how you do a three by two matrix and a two by three matrix multiplying them together. So the result would be C equals that. So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far. So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU. So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training. But pretty much that's how you do dot products and matrix multiplication. So I actually realized I messed up a little bit on the math there. So this 104, that's actually 106. So I messed up there if you caught that. Good job. But pretty much this is what this looks like in three lines of code. So all of this up here that we just covered all of this is in three lines. So we initialize an A tensor and a B tensor. Each one of these is a row. Each one of these is a row and it'll pretty much multiply these together. So this at symbol, this is a shorthand how you multiply two matrices in pytorch together. Another way to do this is to use the torch dot matrix multiply function or math mall for short. And then you can do A and B. So these will print literally the same thing. Look at that. So I'm not too sure on the differences between them. I use A at B for short. But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that. But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks. So there's our network here if I go up. Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number. So if it's like a one, it would be like one dot zero or something or just like a one dot. That's what it would look like as a floating point number. But if it were an integer, say B is full of ones with integers, it would just be a one. There wouldn't be any decimal zero zero center, right? It would just be one. So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type. So I showcase this right here. We have an int 64. So type of it is an integer and a float 32, 64 and 32 don't mean anything. All we have to know is an integer and floating point number. So I've initialized a torch.randint, I covered above and set above here. And maybe not. Anyways, this pretty much does torch.randint is going the first parameter here is anything. It's pretty much your range. So I could do like zero to five, or I could just do like one. So it'll do zero up to one, and then your shape of the matrix that it generates. So I said it's a random int. So that means it's going to generate a tensor with the data type integer 64. So we have a three by two, and then I initialize another random key detail here. We don't have the int suffix. So this just generates floating point numbers. And if we actually return the types of each of these. So five print int 64 dot d type, and then float 32 dot d type. Save that. I'm going to comment this out for now. We get a in 64 and float 32. So if we just try to multiply these together, try to multiply these together. Expected scalar type long above found float. So long is pretty much when you have a sequence of integers. And float is, of course, you have the decimal place. So you can actually multiply this together. So pretty much what you can do is cast the float method on this. If you just do dot float, and then parentheses, and then run this, it'll actually work. So you can cast integers to floats. And then I think there's a way you can cast floats to integers, but it has some rounding in there. So probably not the best for input and weights, matrix multiplication. But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise. So you want to make sure that they have sort of room to float around. So that's pretty much how you avoid that error. Let's move on. So congratulations. You probably made it further than quite a few people already. So congratulations on that. That was one of the most comprehensive parts of this entire course. Understanding the math is going on behind the scenes. For some people, it's very hard to grasp if you're not very fluent with math. But yeah, let's continue the biogram language model and let's pump out some code here. So to recap, we're using CUDA to accelerate the training process. We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel. Two hyperparameters. We open our text. We make a vocabulary out of it. We initialize our encoder and decoder. We get our data encoding all this text, and then we get our train and bow splits. And then this next function here, get batch. So before I jump into this, go ahead and run this here. So this is pretty much just taking the first little, I don't know, we have eight characters. So it's taking the first eight characters and then index one all the way to index nine. And we can pretty much use this to show what the current input is and then what the target would be. So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right? So this is the premise of the biogram language model. Given this character, we're going to predict the next. It doesn't know anything else in the entire history. It just knows what's before it or just knows what the current character is. And based on that, we're going to predict the next one. So we have this get batch function here, and this part right here is the most important piece of code. This is going to work a little bit more later with our train and bow splits, making sure that, you know, I'll try to explain this in a different way with our training bow splits. So imagine you take a course, as you take a math course, okay? And 90% of all your work is done just learning how the course works, learning all about the math. So that's like 90% of data you get from it. And then maybe another 10%. Another 10% at the end is that final exam, which might have some questions you've never seen before. So the point is in that first 90%, you're tested on based on what you know. And then this other 10% is what you don't know. And this pretty much means you can't memorize everything and then just start generating based on your memory. You generate something that's alike or something that's close based on what you already know and the patterns you captured in that 90% of the course. So you can write your final exam successfully. So that's pretty much what's going on here. The training is the course, learning everything about it and then validation is validating the final exam. So pretty much what we're doing here is initialize IX and that'll take a random manager between zero and then length of the length of the entire text minus block size. So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data. So that's kind of how that works. And if we print this out here, it'll just give us this right here. So we get some random integers. These are some random indices in the entire text that we can start generating from. So print this out and then torch.stack. We covered this before. Pretty much what this does, it's going to stack them in batches. This is the entire point of batches. So that's what we do there. We get X and then Y is just the same thing, but offset by one like this. So that's what happens there. And let's get into actually, I'm going to add something here. This is going to be very important. We're going to go X and Y is equal to model dot. We're going to go X dot to device. So notice how, no, we didn't do it up here. Okay, we'll cover this later, but pretty much you're going to see what this does in a second here. We return these and you can see that the device changed. So now we're actually on CUDA. And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU. They're no longer going to be processed sequentially, but rather in our batches in parallel. So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here. So now we can go ahead and actually initialize our neural net. So what I'm going to do is I'm going to go back up here and we're going to import some more stuff. So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second. I'm going to explain this here. I just want to get some code out first. And down here we can initialize this. So it's a class. We're going to make it a by-gram language model subclass of nn.module. And the reason why we do nn.module here is because it's going to take an nn.module. I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters. So I'm going to go ahead and look at the documentation here so you can sort of understand this better. We go to nn. So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before. So we have nn.linear. So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable. So that weight matrix will be changed through gradient descent. And actually, I think I should probably cover gradient descent right now. So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better. So I'm going to go ahead and set up a little graph for that right now. So I'm going to be using a little tool called Desmos. Desmos is actually great. It acts as a graphing calculator. So you can plug in formulas and move things around. You sort of visualize how math functions work. So I've written some functions out here that will basically calculate the derivative of a sine wave. So if I move A around, you'll see that changes. So before I get into what's really going on here, I need to first tell you what the loss actually is. If you're not familiar with the loss, let's say we have 80 characters in our vocabulary. And we have just started our model, no training at all, completely random weights. And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully. So how we can measure the loss of this is by taking the negative log likelihood. So the likelihood is one out of 80. We take the log of that and then negative. So if we plug this in here, we'll get 4.38. So that's a terrible loss. Obviously, that's one out of 80. So it's like, you know, not even 2% chance. So that's not great. So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss. And that's how we train our network. So how does this actually work? How does this actually work out in code, you ask? So pretty much, let's say we have a loss here, okay? Start off with a loss of 2, just arbitrary loss, whatever. And what we're trying to do is decrease it. So over time, it's going to become smaller and smaller if we move in this direction. So how do we know if we're moving in the right direction? Well, we take the derivative of what the current point is at right now, and then we try moving it in a different direction. So if we move it this way, sure, it'll go down. That's great. We can hit the local bottom over there, or we can move to this side. And then we can see that the slope is increasing in a negative direction. So we're going to keep adjusting the parameters in favor of this direction. So that's pretty much what gradient descent is. We're descending with the gradient. So pretty self-explanatory. That's what the loss function does. And gradient descent is an optimizer. So it's an optimizer for the network. Optimizes our parameters, our weight, matrices, etc. So these are some common optimizers that are used. And this is just by going to torch.optim, short for optimizer. And these are just a list of a bunch of optimizers that PyTorch provides. So what we're going to be using is something called AdamW. And what AdamW is, is it pretty much... I'm just going to read off my little script here, because I can't memorize every optimizer that exists. So Adam, without Adam, just Adam, not AdamW, Adam is a popular optimization algorithm that combines ideas of momentum. And it uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. And the learning rate is something that we should also go over. So let's say I figure out I need to move in this direction. I move, I take a step like that. Okay, that's a very big step that I say, okay, we need to keep moving in that direction. So what happens is I go like this, and then I end up there. And it's like, whoa, we're going up now, what happened? So that's because you have a very high learning rate. If you have a lower learning rate, what will happen is you'll start here. It'll take little one-pixel steps or very, very small steps. Okay, that's good. That's better. It's even better. Keep going in this direction. This is great. And you keep going down. You're like, okay, this is good. We're descending. And it's starting to flatten out. So we know that we're hitting a local bottom here. And then we stop because it starts ascending again. So that means this is our best set of parameters because of what that loss is or what the derivative is of that particular point. So pretty much this is what the learning rate is. So you want to have a small learning rate so that you don't take too large steps so that the parameters don't change dramatically and end up messing you up. So you want to make them small enough so that you can still have efficient training. You don't want to be moving in a millionth of one or something. That would be ridiculous. You'd have to do so many iterations to even get this far. So maybe you'd make it decently high but not too high that it'll go like that, right? So that's what the learning rate is, just how fast it learns pretty much. And yeah, so AtomW is a modification of the Atom Optimizer. And it adds weight to K. So pretty much there's just some features that you add on to gradient descent and then AtomW is the same thing except that has weight to K. And what this pretty much means is it generalizes the parameters more. So instead of having very high level of performance or very low level, it takes a little generalized in between. So the weight significance will actually shrink as it flans out. So this will pretty much make sure that certain parameters in your network, certain parameters in your weight matrices aren't affecting the output of this model drastically. That could be in a positive or negative direction. You can have insanely high performance from some lucky parameters in your weight matrices. So pretty much the point is to minimize those, to decay those values. That's what weight to K is, to prevent it from having that insane or super low performance. That's what weight to K is. So that's a little background on gradient descent and optimizers. Let's go ahead and finish typing this out. So next up, we actually, we need to initialize some things. So we have our init self, of course, since it's a class, vocab size. I want to make sure that's correct, vocabulary size. I might actually shrink this just a vocab size because it sounds way easier to type out. And vocab size, good. So we're going to pump out some R code here. And this is just assuming that you have some sort of a background in Python. If not, it's all good. Just understanding the premise of what's going on here. So we're going to make something called an embedding table. And I'm going to explain this to you in a second here, why the embedding table is really important. Notice that we use the nn. We use the nn module in this. So that means this is going to be a learnable parameter, the init.embedding. So we're going to make this vocab size by vocab size. So let's say you have all eight characters here and you have all eight characters here. I'm going to actually show you what this looks like in a second here and why this is really important. So first off, we're going to finish typing out this background language model. So we're going to define our forward pass here. So the reason why we type this forward pass out, instead of just using what it offers by default, is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task. This is a really good practice because we want to actually know what's going on behind the scenes in our model. We want to know exactly what's going on. We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug. So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch? Well, like I said, understanding the process, what are all the transformations that are actually going on, all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output? Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that. So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up. So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here. Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right? So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right? And optimization, of course. So that's pretty much why we write out the forward pass from scratch. It's also just best practice. So it's never really a good idea to not write this. But let's continue. So self, and it will do index and targets. So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here. Before we do logits, I'm going to explain to you this embedding table here. Paste that in. Return logits. You're going to see why we return logits in a second here. So this an end on embedding here is pretty much just a lookup table. So what we're going to have, I'm actually going to pull up my notebook here. So we have a giant sort of grid of what the predictions are going to look like. It's going to look, can I drag it in here? No. So go ahead and download this full screen. Boom. This is my notion here, but pretty much this is what it looks like. And I took this picture from Andrei Karpathy's lecture. But what this is, is it has start tokens and end tokens. So start is at the start of the block, and end tokens are at the end of the block. And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character. So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here. So if we just add up all these, if we normalize them, and we get a little probability of this happening, I don't know, if we add up all these together, I don't know what that is. It's some crazy number, maybe 20,000 or something, something crazy. Pretty much that percentage is the percentage of the end token coming after the character A. And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind. That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that. To that entire row. So this one's pretty significant in proportion to the others. So this one's going to be a fairly high probability of coming next. A lot of the times you're going to have an I coming after an R. And that's pretty much what that is. That's the embedding table. So that's why we make it vocab size by vocab size. So that's a little background on what we're doing here. So let's continue with the term logits. So what exactly are the logits? You're probably asking that. So let's actually go back to a little notebook I had over here. So remember our softmax function, right? Our softmax right here. So we exponentiated each of these values and then we normalized them. Normalized. We took its contribution to the sum of everything. That's what normalizing is. So you can think of logits as just a bunch of floating point numbers that are normalized, right? So you have a total, I'll write this out. So let's say we have, that's a terrible line. Let's draw a new one. So let's say we have 2, 4, and 6. And we want to normalize these. So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12. So 2 divided by 12. We take the percentage of that. 2 out of 12 is 0.16 something, okay? So 0.16, we'll just do 1.167. And then 4 out of 12 would be double that. So 4 out of 12 would be 33, 33%. And then 6 out of 12, that's 50. So 0.5. So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution. So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d. We know that from this distribution, a followed by d is most likely to come next. So this is what the logits are. They're pretty much a probability distribution of what we want to predict. So given that, let's hop back into here. We're going to mess around with these a little bit. So we have this embedding table, and I already showed you what that looked like. It looked like this right here. This is our embedding table. So let's use something called, we're going to use a function called dot view. So this is going to help us sort of reshape what our logits look like. And I'm going to go over an example of what this looks like in a second here. I'm just going to pump out some code. So we have our batch by our time. So the time is, you can think of time as that sequence of integers. That's the time dimension, right? You start from here. Maybe through the generating process, we don't know what's here next. We don't know what's on the, we don't know what the next token is. So that's why we say it's time because there's some we don't know yet and there's some that we already do know. That's what we call the time dimension. And then channels would just be, how many different channels are, what's the vocabulary size? Channels is the vocabulary size. So we can make this the logits dot shape. This is what logits going to return here is B by T by C. That's the shape of it. And then our targets do, actually, no, we won't do that yet. We'll do do logits equals logits dot view. And then we'll, this is very important, B by T. So because we're particularly paying attention to the channels, the vocabulary, the batch and time, they, I mean, they're not as important here. So we can sort of blend these together. And as long as the logits and the targets have the same batch and time, we should be all right. So we're going to do B, B times T by C. And then we can go to initialize our targets. It's going to be targets dot view. And it's going to be just a B by T. And then we can make our loss, remember the loss function, right? We do the functional of cross entropy, just a way of measuring the loss. And we basically take where there's two parameters here. So we have the logits and the targets. So I'm going to go over exactly what's going on here in a second. But first, you might be asking, what does this view mean? What exactly does this do? So I'm going to show you that right now. There's some code here that initializes a random tensor of shape 2 by 3 by 5. And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape. So shape takes the, you know, it takes the 2 by 3 by 5. We get x equals 2, y equals 3, and z equals 5. So then we can do dot view, and that'll pretty much make that tensor again with those dimensions. So then we can just print that out afterwards. We go, we could print out, I don't know, print x, y, z. We have 2, 3, 5. Print, print a dot shape. And actually, I'll print out a dot shape right here first so you can see that this actually does line up. A dot shape. And then down here as well. Same exact thing. This also view does, basically allows us to unpack with the dot shape, and then we can use view to put them back together into a tensor. So you might be asking, why in this notebook did we, did we have to reshape these? Why do we do that? Well, the answer sort of falls into what the shape needs to be here with cross entropy. What does it expect? What does PyTorch expect the actual shape to be? So I looked at the documentation here, and it pretty much says that we want either one dimension, which is channels, or 2, which is n, which I believe n is also the batch. So you have n, n different blocks or batches. And then you have some other dimensions here. So pretty much what it's expecting is a b by c by t instead of a b by t by c, which is precisely what we get out of here. It's the logits dot shape is b by t by c. We want it in a b by c by t. So pretty much what we're doing is we're just putting this into, we're just making this one parameter by multiplying those. That's what's going on here. And then that means the second one is going to be c. So you get like a b times t equals n, and then c, just the way that it expects it, right? Just like that. So that's pretty much what we're doing there. And a lot of the times you might get errors from passing it into a functional function in PyTorch. So it's important to pay attention to how PyTorch expects the shapes to be, because you're going to get errors from that. And I mean, it's not very hard to reshape them. You just use the dot view and dot shape and you unpack them, reshape them together. It's overall pretty simple for a beginner to intermediate level projects. So it shouldn't really be a trouble there, but just watch out for that, because it will come back and get you if you're not aware at some point. So I've added a new function here called generate, and this is pretty much going to generate tokens for us. So we pass an index, which is the current index or the context, and then we have max new tokens, and this is passed in through here. So we have our context, we make it a single zero, just the next line character. And then we generate based on that, and then our max new tokens, second parameter, we just make it 500 second parameter. So cool. What do we do inside of here? We have a little loop that pretty much it generates based on the range of the max new tokens. So we're going to generate max new tokens, tokens, if that makes sense. Pretty much what we do is we call forward pass based on the current state of the model parameters. And I want to be explicit here and say self dot forward, rather than just self index, it will call self dot forward when we do this. But let's just be explicit and say self dot forward here. So we get the logic and the loss from this. We focus on the last time step. That's the only one we care about diagram language model. We only care about the single previous character, only one doesn't have context before. And then we apply the softmax to get probability distribution. And we already went over the softmax function before. The reason why we use negative one here is because we're focusing on the last dimension. And in case you aren't familiar with negative indexing, which is what this is here and same with here, is imagine you have a little number line. It starts at index zero, one, two, three, four, five, et cetera. So if you go before zero, it's just going to loop to the very end of that array. So when we call negative one, it's going to do the last element, negative two, second last element, negative three, third last element, et cetera. So that's pretty much all this is here. And you can do this for anything in Python. Negative indexing is quite common. So that's what we do here. We apply softmax to the last dimension. And then we sample from the distribution. So we already went over torch dot monomial, we get one sample. And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate. It concatenates the previous context or the previous tokens with the newly generated one. And then we just combine them together. So they're one thing. And we do this on a B by T plus one. And if that doesn't make sense, let me help you out here. So we have this time dimension, let's say we have, you know, maybe just one element here. So we have something in the zero position. And then whenever we generate a token, we're going to take the information from the zero position. And then we're going to add one to it. So it becomes a B by T. Since there was only one element, the length of that was one, it is now two. Then we have this two, we make it three. And then we have this three, we make it four. So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it. And then we, you know, after this loop, we just return the index. So this is all the generated tokens for max new tokens. And that's pretty much what that does. Model up to device here, this is just going to push our parameters to the GPU for more efficient training. I'm not sure if this makes a huge difference right now because we're only doing background language modeling. But yeah, it's handy to have this here. And then, I mean, this is, this is pretty self explanatory here. We generate based on a context. This is the context, which is a single zero or a next line character. We pass in our max new tokens. And then we pretty much just decode this. So that's how that works. Let's move on to the optimizer and the training loop, the actual training process. So I actually skipped something and probably left you a little bit confused. But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here? Because the logits only returns two dimensions, right? You have a B by T, or you have a B times T by C. So how exactly does this work? Well, when we call this forward pass, all we're passing in is the index here. So that means targets defaults to none. So because targets is none, the loss is none, and this code does not execute. And it just uses this logits here, which is three dimensional. So that's how that works. And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there. And that will make sure targets is not none. So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits. So that's just a little clarification there, if that was confusing to anybody. Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type. So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes. So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference between float and int is float has decimals, it's a floating point number. And then integers just, just a single integer doesn't, it's not really anything more than that. It can just be bigger based on the amount of bits that occupies. So that's just a overview on tors dot long. It's the exact same thing as in 64. So that's that. Now we have this, we have this training loop here. So we define our optimizer. And I already meant over optimizers previously, Adam W, which is Adam weight decay. So we have weight decay in here. And then all of our model parameters, and then our learning rates. So I actually wrote to learning rate up here. So I would add this and then just rerun this part of the code here if you're typing along. So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop. And the learning rate is special, because sometimes you're learning what will be too high. And some said, sometimes it'll be too low. So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time. So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip. So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly. You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that. So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs. So now we have this training loop here, which is going to iterate over the max iterations. We just give each iteration the term iter. And I don't think we use this yet, but we will later for just reporting on the loss over time. But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training. This is the training loop, we don't care about validation. So we're going to call train on this, we're going to get some x inputs and some y targets. So we go in and do a model dot forward here, we got our logits and our loss. And then we're going to do our optimizer dot zero grad and I'll explain this in the second here. It's a little bit confusing. But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops. I know I can go by this a little bit quickly. But this is the standard training loop architecture for basic models. And this is what it'll usually look like. So you'll, you know, you'll get your data, get your inputs or outputs, whatever. You'll do a forward pass. You'll define some thing about the optimizer here. In our case, it's your grad. And then you'll have a loss dot backward, which is backward pass. And the optimizer dot step, which lets gradient descent work its magic. So back to optimizer dot zero grad. So by default, PyTorch will accumulate the gradients over time via adding them. And what we do by by putting a zero grad is we make sure that they do not add over time. So the previous gradients do not affect the current one. And the reason we don't want this is because previous gradients are from previous data. And the data is, you know, kind of weird sometimes, sometimes it's biased. And we don't want that determining, you know, how much like what our error is, right? So we only want to decide, we only want to optimize based on the current gradient of our current data. And this little parameter in here, we go set to none. This pretty much means we're going to set, we're going to set the gradients instead of zero, instead of zero gradient, we're going to set it to none. And the reason why we set it to none is because none occupies a lot less space. It just, yeah, just occupies a lot less space when you have a zero. That's, that's probably an int 64 or something that's going to take up space. And because, you know, we might have a lot of these accumulating that takes up space over time. So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to. And that's pretty much what that does. It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets, which need to understand previous context because they're recurrent. I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation. We'll simply take an average of all the accumulation steps and just averages the gradients together. So you get a more effective, maybe block size, right? You get more context that way. And you can have the same batch size. So just little neat tricks like that. We'll talk about gradient accumulation more later in the course, but pretty much what's going on here. We define an optimizer, Adam W. We iterate over max editors. We get a batch training split. We do a forward pass, zero grad, backward pass, and then we get a step in the right direction. So we're gradient descent works as magic. And at the end, we could just print out the loss here. So I've run this a few times. And over time, I've gotten the loss of 2.55, which is okay. And if we generate based on that loss, we get still pretty garbage tokens. But then again, this is a background language model. So actually, I might need to retrain this here. It's not trained yet. So what I'm actually going to do is run this, run this, run this, boom. And then what I'll do, oh, it looks like we're printing out a lot of stuff here. So that's coming from our get batch. So I'll just comment that. Or we can just delete it overall. Cool. And now if we run this again. Give it a second. Perfect. So I don't know why it's still doing that. If we run it again, let's see. Where are we printing stuff? No. Ah, yes. We have to run this again after changing it. Silly me. And of course, 10,000 steps is a lot. So it takes a little while. It takes a few seconds, which is actually quite quick. So after the first one, we get a loss of 3.15. We can generate from that. And we get something that is less garbage. You know, it has some next line characters. It understands a little bit more to, you know, space things out and whatnot. So that's like slightly less garbage than before. But yeah, this, this is pretty good. So I lied. There aren't actually any lectures previously where I talked about optimizers. So might as well talk about it now. So a bunch of common ones. And honestly, you don't really need to know anything more than the common ones because most of them are just built off of these. So you have your mean squared error, common loss function using regression, regression problems, where it's like, you know, you have a bunch of data points, find the best fit line, right? That's a common regression problem. Goals to predict a continuous output and measures the average squared difference between the predicted and actual values, often used to train neural networks for regression tasks. So cool. That's the most basic one. You can look into that more if you'd like, but that's our most basic optimizer. Gradient descent is a step up from that. It's used to minimize the loss function in a model, measures how well the model, the gradient measures how well the model is able to predict the target variable based on the input features. So we have some input X, we have some weights and biases maybe, WX plus B. And all we're trying to do is make sure that the inputs or make sure that we make the inputs become the desired outputs and based on how far it is away from the desired outputs, we can change the parameters of the model. So we went over gradient descent recently or previously, but that's pretty much what's going on here. And momentum is just a little extension of gradient descent that adds the momentum term. So it helps smooth out the training and allows it to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. It's particularly useful for training deep neural nets. So momentum is when you have, you know, you consider some of the other gradients. So you have something that's like maybe passed on from here and then it might include a little bit of the current one. So like 90%, like a good momentum coefficient would be like 90% previous gradients and then 10% of the current one. So it kind of like lags behind and makes it converge sort of smoothly. That makes sense. Arm as prop, I've never used this, but it's an algorithm that use the moving average of the squared gradient to adapt learning rates of each parameter, helps to avoid oscillations in the parameter updates and can move and can improve convergence in some cases. So you can look more into that if you'd like. Adam, very popular, combines the ideas of momentum and arm as prop. He uses a moving average, both the gradient and its squared value to adapt learning rate of each parameter. So often uses the default optimizer for deep learning models. And in our case, when we continue to build this out, it's going to be quite a deep net. And Adam W is just a modification of the item optimizer that adds weight decay to the parameter updates. So helps to regularize and improve generalization performance. Using this optimizer as it best suits the properties of the model we'll train in this video. So, of course, I'm reading off the script here. There's no really other better way to say how these optimizers work. But, yeah, if you want to look more into, you know, concepts like momentum or weight decay or, you know, oscillations and just some statistic stuff, you can. But honestly, the only thing that really matters is just knowing which optimizers are used for certain things. So, like, what is the momentum used for? What is Adam W great for? What is MSC good for, right? Just knowing what the differences and similarities are, as well as when is the best case to use the optimizer. So, yeah, you can find more information about that at torch.optim. So when we develop language models, something really important in language modeling, data science, machine learning, at all, is just being able to report a loss or get an idea of how well our model is performing over, you know, the first 1,000 iterations and then the first 2,000 iterations and 4,000 iterations, right? So we want to get a general idea of how our model is converging over time. But we don't want to just print every single step of this. That wouldn't make sense. So what we actually could do is print every, you know, 200 iterations, 500. We could print every 10,000 iterations if you're running a crazy big language model if you wanted to. And that's exactly what we're going to implement right here. So, actually, this doesn't require an insane amount of Python syntax. This is just, I'm actually just going to add it into our for loop here. And what this is going to do is it's going to do what I just said, is print every, you know, every certain number of iterations. So we can add a new hyper parameter up here called eval-itters. And I'm going to make this 250 just for, just to make things sort of easy here. And we're going to go ahead and add this in here. So I'm going to go if-iter. And we're going to do the module operator. You can look more into this if you want later. And we're going to do eval-itters equals equals zero. What this is going to do is it's going to check if the current iteration divided by, or sorry, if the remainder of the current iteration divided by our eval-itters parameter, if the remainder of that is zero, then we continue with it. So hopefully that made sense. If you want to, you could just ask GPT4 or GPT3.5, whatever you have, just this module operator, and you should get a good general understanding of what it does. Cool. So all we can do now is we'll just say, we'll just have a filler statement here. We'll just do print, we've been f-string, and then we'll go losses, losses, maybe that. Or actually, I'm going to change this here. We can go step-iter. Add a little colon in there. And then I'll go split. Actually, I'll just go loss, and then losses like that. And then we'll have some sort of put in here. Something soon. I don't know. And all I've done is I've actually added a little function in here behind the scenes. You guys didn't see me do this yet. But pretty much, I'm not going to go through the actual function itself, but what is important is that you know this decorator right here. This probably isn't very common to you. This is torch.nograt. And what this is going to do is it's going to make sure that PyTorch doesn't use gradients at all in here. That'll reduce computation. It'll reduce memory usage. It's just overall better for performance. And because we're just reporting a loss, we don't really need to do any optimizing or gradient computation here. We're just getting losses. We're feeding some stuff into the model. We're getting a loss out of it, and we're going from there. So that's pretty much what's happening with this torch.nograt. And, you know, for things like, I don't know, if you have other classes or other outside functions, like, I mean, get batched by default isn't using this because it doesn't have the model thing passed into it. But estimate loss does have model pass into it right here. So we just kind of want to make sure that it's not using any gradients. We're going to reduce computation that way. So anyways, if you want, you can just take a quick readover of this, and it should overall make sense. Terms like .item.me are pretty common. A lot of the other things here, like model, X and Y, we get our logits and our loss. This stuff should make sense. It should be pretty straightforward. And only two other things I want to touch on is model.eval and model.train, because you probably have not seen these yet. So model.train essentially puts the model in the training mode. The model learns from the data, meaning the weights and biases, if we have, well, sometimes you only have weights, sometimes you, you know, sometimes you have weights and biases, whatever it is, those are updated during this phase. And then some layers of the model, like dropout and batch normalization, which you may not be familiar with yet, operate differently in training mode. For example, dropout is active, and what dropout does is this little hyperparameter that we add up here. It'll look like this. Dropout would be like 0.2. So pretty much what dropout does is it's going to drop out random neurons in the network so that we don't overfit. And this is actually disabled in validation mode, or eval mode. So this will just help our model sort of learn better when it has little, like, pieces of noise and when things aren't in quite the right place so that you don't have, you know, certain neurons in the network taking priority and just making a lot of the heavy decisions. We don't want that. So dropout will just sort of help our model train better by taking 20% of the neurons out, 0.2, at random. And that's all dropout does. So I'm just going to delete that for now. And then, yeah, model that train. Well, dropout is active during this phase, during training, randomly turning off, random neurons in the network. And this is to prevent overfitting. We went over overfitting earlier, I believe. And as for evaluation mode, evaluation mode is used when the model's being evaluated or tested just like it sounds. It's being trained. What the other mode is being validated or tested. And layers like dropout and batch normalization behave differently in this mode. Like dropout is turned off in the evaluation, right? Because what we're actually doing is we're using the entire network. We want everything to be working sort of together. And we want to actually see how well does it perform. Training mode is when we're just, you know, sampling, doing weird things to try to challenge the network as we're training it. And then evaluating or validation would be when we just get the network in its optimal form and we're trying to see how good of results it produces. So that's what a val is. And the reason we switched into a val here is just because, well, we are testing the model. We want to see, you know, how well it does with any given set of data from a get batch. And we don't actually need to train here. If there was no training, this would not be here because we would not be using any gradients. So we would be using gradients if training was on. Anyways, that's estimate loss for you. This function is, you know, just generally good to have a data science. Your train and validation splits, whatnot. And yeah, good for reporting. You know how it is. And we can go ahead and add this down here. So there's something soon. We'll go losses is equal to estimates loss. And then we can go ahead and put a... Yeah, we don't actually have to put anything in here. Cool. So now let's go ahead and run this. Let me run from the start here. Boom, boom, boom, boom, boom, boom. Perfect. I'm running for 10,000 iterations. That's interesting. Okay. So, yes. So what I'm going to do actually here is you can see this loss part is weird. So I'm actually going to change this up. And I'm just going to switch it to... We're going to go train loss. And we're going to go losses. And we're going to do the train split. And then we're going to go over here and just do the validation loss. We can do validation or just val for short. And I'm going to make it consistent here. So we have a colon there, a colon here. And then we just go losses and do val. Cool. So I'm going to reduce these max editors up here to only 1,000. Run that. Run this. Oh, somebody did a match. Okay. Okay. Okay. Yes. So what actually happened here was when we were doing these little ticks, what was happening is these were matching up with these. And it was telling us, oh, you can't do that. You can't start here and then end there and have all this weird stuff. Like, you can't do that. So pretty much we just need to make sure that these are different. So I'm going to do a double quote instead of single and then double quote to finish it off. And as you can see, this worked out here. So I'll just run that again so you guys can see what this looks like. Okay. Because we have, you know, a lot of decimal places. So what we can actually do here is we can add in a little format or a little decimal place reducer, if you call it, just for, you know, so you can read it. So it's not like some weird decimal number and you're like, oh, does this eight matter? Probably not. Just like the first three digits, maybe. So all we can do here is just add in, I believe this is how it goes. I don't think it's the other way. We'll find out. Some stuff in Python is extremely confusing to me. But there we go. So I got it right. Go on and then period. And as you can see, we have those digits reduced. So I can actually put this down to 3F. Wonderful. So we have our train loss and our validation loss. Great job you made it this far. This is absolutely amazing. This is insane. You've gotten this far in the video. We've covered all the basics, everything you need to know about background language models, optimizers, training loops, reporting losses. I can't even name everything we've done because it's so much. So congratulations that you made it this far. You should go take a quick break. Give yourself a pat on the back and get ready for the next part here because it's going to be absolutely insane. We're going to dig into literally state of the art language models and how we can build them from scratch, or at least how we can pre-train them. And some of these terms are going to seem a little bit out there, but I can ensure you by the end of this next section here, you're going to have a pretty good understanding about the state of language models right now. So go take a quick break and I'll see you back in a little bit. So there's something I'd like to clear up and actually sort of lied to you a little bit, a little while back in this course about what normalizing is. So I recall we were talking about the softmax function and normalizing vectors. So the softmax is definitely a form of normalization, but there are many forms. There are not just a few or like there's not just one or two normalizations. There are actually many of them and I have them on my second monitor here, but I don't want to just dump that library of information on your head because that's not how you learn. So what we're going to do is we're going to plug this into GPT-4. I'm going to say, can you list all the forms of normalizing in machine learning? And how are they different from one another? GPT-4 is a great tool. If you don't already use it, I highly suggest you use it, or even GPT-3.5, which is the free version. But yeah, it's a great tool for just quickly learning anything and then have it give you example practice questions with answers so you can learn topics in literally minutes that would take you several lectures to learn in a university course. But anyways, there's a few here. So min-max normalization, yep. z-score, decimal scaling, mean normalization, unit vector, or layer 2, robust scaling, power transformations. Okay. So yeah, and then softmax would be another one. What about softmax? It is in data type normalization, but it's not typically using from normalizing input data. It's commonly used in the output layer. So softmax is a type of normalization, but it's not used for normalizing input data. And honestly, we've proved that here by actually producing some probabilities. So this isn't something we used in our forward pass. This is something we used in our generate function to get a bunch of probabilities from our logits. So this is, yeah, interesting. It's good to just figure little things like these out for, you know, just to be, put you on the edge a little bit more for the future when it comes to engineering these kind of things. All right, great. So the next thing I want to touch on is activation functions. And activation functions are extremely important in offering new ways of changing our inputs that are not linear. So, for example, if we were to have a bunch of linear layers, a bunch of, let me erase this, if we were to have a bunch of, you know, nn.linears in a row, what would actually happen is they would all just, you know, they would all squeeze together and essentially apply one transformation that sums up all of them kind of. They all sort of multiply together and it gives us one transformation that is kind of just a waste of computation because let's say you have 100 of these nn.linear layers and nothing else. You're essentially going from inputs to outputs, but you're doing 100 times the computation for just one multiplication. That doesn't really make sense. So what can we do to actually make these deep neural networks important and what can we offer that's more than just linear transformations? Well, that's where activation functions come in and I'm going to go over these in a quick second here. So let's go navigate over to the PyTorch docs. So the three activation functions I'm going to cover in this little part of the video are the relu, the sigmoid, and the tanh activation functions. So let's start off with the relu or rectified linear unit. So we're going to use functional relu and the reason why we're not just going to use torch.n is because we're not doing any forward passes here. I'm just going to add these into our, I'm going to add these, let me clear this, clear this output. That's fine. I'm actually going to add these into here and there's no forward pass. We're just going to simply run them through a function and get an output just so we can see what it looks like. So I've actually added this up here from torch.n and import functional as capital F. It's just kind of a common PyTorch practice, capital F. And let's go ahead and start off with the relu here. So we can go, I don't know, x equals torch.tensor and then we'll make it a negative 0.05, for example. And then we'll go dtype equals torch.flurp32 and we can go y equals f.relu of x. And then we'll go ahead and print y. It has no attribute relu. Okay, let's try nn then. Let's try nn and see if that works. Okay, well that didn't work and that's fine because we can simply take a look at this and it'll help us understand. We don't actually need to, we don't need to write this out in code as long as it sort of makes sense. We don't need to write this in the forward pass, really. You're not going to use it anywhere else. So yeah, I'm not going to be too discouraged that that does not work in the functional library. But yeah, so pretty much what this does is if a number is below, if a number is 0 or below 0, it will turn that number into 0. And then if it's above 0, it'll stay the same. So this graph sort of helps you visualize that. There's a little function here. That might make sense to some people. I don't really care about the functions too much as long as I can sort of visualize what the function means, what it does, what are some applications it can be used. That usually covers enough for like any function at all. So that's the Relu function. Pretty cool. It simply offers a non-linearity to our linear networks. So if you have 100 layers deep and every, I don't know, every second step you put a Relu, that network is going to learn a lot more things. It's going to learn a lot more linearity, non-linearity. Then if you were to just have 100 layers multiplying all into one transformation. So that's what that is. That's the Relu. Now let's go over to Sigmoid. So here we can actually use the functional library. And all Sigmoid does is we go 1 over 1 plus exponentiated of negative x. So I'm going to add that here. We could, yeah, why not do that? Negative 0.05 float 32. Sure. We'll go f dot Sigmoid. And then we'll just go x and then we'll print y. Cool. So we get a tensor 0.4875. Interesting. So this little negative 0.05 here is essentially being plugged into this negative x. So 1 over 1 plus 2.71 to the power of negative 0.05. So it's essentially, if we do 2.71, 2.71 to the power of negative 0.05, we're just going to get positive. So 1.05 and then 1 plus that. So that's 2.05. We just do 1 over that. 2.05. So we get about 0.487. And what do we get here? 0.4 at 7. Cool. So that's interesting. And let's actually look, is there a graph here? Let's look at the Sigmoid activation function. Wikipedia. Don't get too scared by this math here. I don't like it either, but I like the graphs they're cool to look at. So this is pretty much what it's doing here. So yeah, it's just a little curve. Kind of looks like a, it's kind of just like a wave, but it's cool looking. That's what the Sigmoid function does. It's used to just generalize over this line. And yeah, Sigmoid function is pretty cool. So now let's move on to the tanh. The tanh function. Google Bing is, or Microsoft Bing is giving me a nice description of that. Cool. Perfect. E to the negative x. I like that. So tanh is a little bit different. There's a lot more exponentiating going on here. So you have, I'll just say expo or exp of x minus exp of negative x divided by exp of x plus exp of negative x. There's a lot of positives and negatives in here. Positive, positive, negative, negative, negative, positive. So that's interesting. Let's go ahead and put this into code here. So I'll go torch dot examples, or torch examples. This is our file here. And I'll just go tanh. Cool. So negative 0.05. Cool. What if we do a one? What will that produce? Oh, 0.76. What if we do a 10? 1.0. Interesting. So this is sort of similar to the sigmoid, except it's, you know, it's actually asked to attach a BT what the difference is. When would you use tanh over sigmoid? Let's see here. Sigmoid function and hyperbolic tangent or tanh function are activations functions used in neural networks. They have a similar s-shaped curve, but have different ranges. So sigmoid output values between a 0 and a 1 while tanh is between a negative 1 and a 1. So if you're, you know, if you're rating maybe the, maybe if you're getting a probability distribution, for example, you want it to be between 0 and 1, meaning percentages or decimal places. So like a 0.5 would be 50%, 0.87 would be 87%. And that's what the sigmoid function does. It's quite close to the softmax function, actually. Except the softmax just, you know, it prioritizes the bigger values and puts the smaller values to our priority. That's all the softmax says. It's kind of a sigmoid on steroids. And the tanh outputs between negative 1 and 1. So, yeah, you could maybe even start theorycrafting and thinking of some ways you could use even the tanh function and sigmoid in different use cases. So that's kind of a general overview on those. So biogram language models are finished. All of this we finished here is now done. You're back from your break. If you took one, if you didn't, that's fine too. But pretty much we're going to dig into the transformer architecture now. And we're actually going to build it from scratch. So there was recently a paper proposed called the transformer model. And this uses a mechanism called self-attention. Self-attention is used in these multi-head attention, little bricks here. And there's a lot that happens. So there's something I want to clarify before we jump right into this architecture and just dump a bunch of information on your poor little brain right now. But a lot of these networks, at first, can be extremely confusing to beginners. So I want to make it clear. It's perfectly okay if you don't understand this at first. I'm going to try to explain this in the best way possible. Believe me, I've seen tons of videos on people explaining the transformer architecture. And all of them have been, to some degree, a bit confusing to me as well. So I'm going to try to clarify all those little pieces of confusion. Like what does that mean? You didn't cover that piece. I don't know what's going on here. I'm going to cover all those little bits and make sure that nothing is left behind. So you're going to want to sit tight and pay attention for this next part here. So yeah, let's go ahead and dive into just the general transformer architecture and why it's important. So in the transformer network, you have a lot of computation going on. You have some adding and normalizing. You have some multi-hat attention. You have some feed forward networks. There's a lot going on here. There's a lot of computation, a lot of multiplying, there's a lot going on. So the question I actually had at first was, well, if you're just multiplying these inputs by a bunch of different things along, you should just end up with some random value at the end that maybe doesn't really mean that much of the initial input. And that's actually correct. For the first few iterations, the model has absolutely no context as to what's going on. It is clueless. It is going in random directions and it's just trying to find the best way to converge. So this is what machine learning and deep learning is actually all about, is having all these little parameters in, you know, the adding and normalizing, the feed forward networks, even multi-hat attention. We're trying to optimize the parameters for producing an output that is meaningful that will actually help us produce almost perfectly like English text. And so this is the entire process of pre-training. You send a bunch of inputs into a transformer and you get some output probabilities that you used to generate from. And what attention does is it sets little different scores to, you know, each little token in a sentence. For tokens you have character, subword, and word-level tokens. So you're pretty much just mapping bits of attention to each of these, as well as, you know, what is the position also mean as well. So you could have two words that are right next to each other, but then if you don't actually, you know, positionally encode them, it doesn't really mean much, because it's like, oh, these could be like 4,000 characters apart. So that's why you need both to put attention scores on these tokens and to positionally encode them. And that's what's happening here. So what we do is we get to our inputs. We got our inputs. So, I mean, we went over this with diagram language models. We feed our X and Y, so X would be our inputs, Y would be our targets or outputs. And what we're going to do is give these little embeddings. So I believe we went over embeddings a little while ago, and pretty much what those mean is it's going to have a little row for each token on that table, and that's going to store, you know, some vector as to what that token means. So let's say you had, like, you know, the character E, for example, the sentiment or the vector of the character E is probably going to be vastly different than the sentiment of Z, right? Because E is a very common vowel, and Z is one of the most uncommon, if not the most uncommon letter in the English language. So these embeddings are learned. We have these both for our inputs and our outputs. We give them positional encodings like I was talking about, and there's ways we can do that. We can actually use learnable parameters to assign these encodings. A lot of these are learnable parameters, by the way, and you'll see that as you, you know, delve more and more into transformers. But, yeah, so after we've given these inputs, embeddings, and positional encodings, and same thing with the outputs, which are essentially just shifted right, you have, you know, I up to block size for inputs, and then I plus one up to block size plus one, right? Or whatever little thing we employed here in our background language models. Quite what it was. Or even if we did that at all. No. I'm just speaking gibberish right now, but that's fine because it's going to make sense in a little bit here. So what I'm going to actually do is I'm not going to read off of this right here because this is really confusing. So I'm going to switch over to a little, like a little sketch that I drew out. And this is pretty much the entire transformer with a lot of other things considered that this initial image does not really put into perspective. So let's go ahead and jump into sort of what's going on in here from the ground up. So like I was talking about before, we have some inputs and we have some outputs which are shifted right, and we give each of them some embedding vectors and positional encodings. So from here, let's say we have n layers. This is going to make sense in a second. n layers is set to four. So the amount of layers we have is set to four. So you can see we have an encoder, encoder. Like we have four of these and we have four decoders. So four is actually the amount of encoders and decoders we have. We always have the same amount of each. So if we have, you know, ten layers, that means we have ten encoders and ten decoders. And pretty much what would happen is after this input, embedding and positional embedding, we feed that into the first encoder layer and then the next, and then next, and then right as soon as we hit the last one, we feed these into each of these decoders here, each of these decoder layers. So only the last encoder will feed into these decoders. And pretty much these decoders will all run. They'll all learn different things. And then they'll turn what they learned. They'll do, they'll apply a linear transformation at the end of it. This is not in the decoder function. This is actually after the last decoder. It'll apply a linear transformation to pretty much sort of simplify or give a summary of what it learned. And then we apply a softmax on that new, you know, tensor to get some probabilities to sample from, like we talked about in the generate function in our biogram. And then once we get these probabilities, we can then sample from them and generate tokens. And that's kind of like the first little step here. That's what's going on. We have some encoders. We have some decoders. We do a transformation to summarize. We have a softmax to get probabilities. And then we generate based on those probabilities. Cool. Next up, in the encoder, in each of these encoders, this is what it's going to look like. So we have multi-hat attention, which I'm going to dub into a second here. So after this multi-hat attention, we have a residual connection. So in case you aren't familiar with residual connections, I might have went over this before. But pretty much what they do is it's a little connector. So I don't know. Let's say you get some inputs X, you have some inputs X down here, and you put them into some sort of function here, some sort of like feedforward network, whatever it is. A feedforward network is essentially just a linear, a RELU, and then a linear. That's all feedforward network is right here. Linear, really, really linear. And all you do is you wrap those inputs around so you don't actually put them into that feedforward network. You actually wrap them around, and then you can add them to the output. So you had some X values here, go through the RELU, and then you had some wrap around. And then right here, you simply add them together and you normalize them using some encod layer norm, which we're going to cover in a little bit. And the reason why residual connections are so useful in transformers is because when you have a really deep neural network, a lot of the information is actually forgotten in the first steps. So if you have your first view encoder layers and your first view decoder layers, a lot of the information here is going to be forgotten because it's not being carried through. The first steps of it aren't explicitly being carried through and sort of skipped through the functions. And yeah, you can sort of see how they would just be forgotten. So residual connections are sort of just a cheat for getting around that, for not having deep neural networks forget things from the beginning, and having them all sort of work together to the same degree. So residual connections are great that way. And then, you know, at the end there, you would add them together and then normalize. And there's two different ways that you can do this add a norm. There's add a norm and then norm and add. So these are two different separate architectures that you can do in transformers. And both of these are sort of like meta architectures. But pretty much pre-norm is the normalize then add, and then post-norm is add then normalize. So in this attention is all you need paper proposed by a bunch of research scientists was initially you want to add these, you want to add these together and then normalize them. So that is what we call the post-norm architecture. And then pre-norm is just flip them around. So I've actually done some testing with pre-norm and post-norm and the original transformer paper turned out to be quite actually a lot better, at least for training very small language models. If you're training bigger ones, it might be different, but essentially we're just going to go by the rules that we use in here. So add a norm. We're not going to do norm and add. Add a norm in this video specifically because it works better and we just don't want to break any of the rules and go outside of it because then that starts to get confusing. And actually if you watch the Andre Carpathi lecture on building GPTs from scratch, he actually implemented it in the pre-norm way. So normalize then add. So yeah, based on my experience, what I've done on my computer here is the post-norm architecture works quite better. So that's why we're going to use it. We're going to do add then normalize. So then we essentially feed this into a feedforward network which we covered earlier. And then how did it go? So we're encoder. We do a residual connection from here to here and then another residual connection from outside of our feedforward network. So each time we're doing some other things like some, you know, some computation blocks in here, we're going to have a rest connection. Same with our feedforward rest connection. And then of course the output from here, just when it exits, it's going to feed into the next encoder block if it's not the last encoder. So this one is going to do all this. It's going to feed into that one. It's going to do the same thing. Feed into this one. Going to feed into that one. And then the output of this is going to feed into each of these decoders, all the same information. And yeah, so that's a little bit scoped in as to what these encoders look like. So now that you know what the encoder looks like, what the feedforward looks like, we're going to go into multi-head attention, sort of the premise, sort of the highlight of the transformer architecture and why it's so important. So multi-head attention, we call it multi-head attention because there are a bunch of these different heads learning different semantic info from a unique perspective. So let's say you have 10 different people looking at the same book. If you have 10 different people, let's say they're all reading the same Harry Potter book. These different people, they might have different cognitive abilities. They might have different IQs. They might have been raised in different ways. So they might interpret things differently. They might look at little things in that book and their mind will, they'll imagine different scenarios, different environments from the book. And essentially why this is so valuable is because we don't just want to have one person, just one perspective on this. We want to have a bunch of different heads in parallel looking at this same piece of data because they're all going to capture different things about it. And keep in mind each of these heads, each of these heads in parallel, these different perspectives, they have different learnable parameters. So they're not all the same one looking at this piece of data. They're actually, they all have different learnable parameters. So you have a bunch of these at the same time learning different things and that's why it's so powerful. So this scale.product attention runs in parallel, which means we can scale that to the GPU, which is very useful. It's good to touch on that. Anything with the GPU that you can accelerate is just an automatic win because parallelism is great in machine learning. Why not have parallelism, right? If it's just going to be running the CPU, what's the point? That's why we love GPUs. Anyways, yeah. So you're going to have these different, you're going to have these things that are called keys, queries and values. I'll touch on those in a second here because keys, queries and values sort of point to self-attention, which is literally the entire point of the transformer. Transformer wouldn't really mean anything without self-attention. So I'll touch on those in a second here and we'll actually delve deeper as we hit this sort of block. But yeah, you have these keys, queries and values. They go into scale.product attention. So a bunch of these running in parallel and then you concatenate the results from all these different heads running in parallel. You have all these different people. You concatenate all of them, you generalize it, and then you apply a transformation to a linear transformation to pretty much summarize that and then do your add a norm, then pay for a network. So that's what's going on in multi-head attention. You're just doing a bunch of self-attentions in parallel, concatenating, and then continuing on with this part. So scale.product attention. What is that? So let's just start from the ground up here. We'll just go from left to right. So you have your keys, queries and values. What do your keys do? Well, a key is let's just say you have a token and a sentence. Okay? So if you have let me just roll down here to a good example. So self-attention uses keys, queries and values. Self-attention helps identify which of these tokens in a sentence in any given sentence are more important and how much attention you should pay to each of those characters or words, whatever you're using. We'll just use words to make it easier to understand for the purpose of this video. But essentially imagine you have these two sentences here. So you have let me bring out my little piece of text. So you have that didn't work. So imagine you have server, can I have the check? And then you have and you have looks like I crashed the server. So I mean, both of these have the word server in them, but they mean different things. Server meaning like the waiter or the waitress or whoever is billing you at the end of your restaurant visit. And then looks like I crashed the server is like, oh, there's actually a server running in the cloud, not like a person that's billing me, but an actual server. That's maybe running a video game. And these are two different things. So what attention can do is it can actually identify which words would get attention here. So it can say server, can I have the check? Can I have? So it's maybe you're looking for something you're looking for the check and then server is like, oh, well in this in this particular sequence or in this in the sentiment of this sentence here server is specifically tied to this one meaning, maybe a human someone at a restaurant and then crash the server crash is going to get a very high attention score because you don't normally crash a server at a restaurant that doesn't particularly make sense. So if you have different words like this what self-attention will do is it will learn which words in the sentence are actually more important and which words should pay more attention to. So that's really all that's going on here and the key is essentially going to emit a different it's going to emit a little tensor here saying what do I contain and then query is going to say what am I looking for? So what's going to happen is if these, let's say server, it's going to look for things like check or crashed so if it sees crashed then that means the key and the query are going to multiply and it's going to get a very high attention score but if you have something like it's like there's literally almost any sentence so that doesn't mean much. We're not going to pay attention to those words so that's going to get a very low attention score and all attention is you're just dot-producting these vectors together. So you get a key and a query, you dot-product them we already went over dot-products in this course before and then this is a little bit of a confusing part is you just scale by one over the square root of the length of a row in the keys or queries matrix otherwise known as DK. So let's say we have our key and our query these are all going to be the same length by the way. Let's say our keys is maybe our keys is going to be like 10 characters long our keys are going to be 10 characters long as well so it's going to do one over the square root of 10 if that makes sense and so that's just essentially a way of preventing these dot-products from exploding we want to scale them because as we have as the length of it increases so will the ending dot-product because there's more of these to multiply so we pretty much just want to scale it by using an inverse square root and that will just help us with scaling make sure nothing explodes in unnecessary ways and then the next little important part is using tort.trill which I imagine we went over in our examples here trill yeah so you can see that it's a diagonal it's a left triangular matrix of ones and these aren't going to be ones in our self-attention here in our tort.trill or masking what this is going to be is the scores at each time step the combination of scores at each time step so if we've only gone if we're only looking at the first time step we should not have access to the rest of things or else that would be cheating we shouldn't be allowed to look ahead because we haven't actually produced these yet we need to produce these before we can put them into perspective and put a weight on them so we're going to set all these to zero and then we go to the next time step so now we've just generated this one we haven't generated these yet so we can't look at them and then as we go more and more as the time step increases we know more and more context about all of these tokens so that's all that's doing mask attention is pretty much just saying we don't want to look into the future we want to only guess with what we currently know in our current time step and everything before it you can't jump into the future look at what happened in the past and do stuff based on that same thing applies to life you can't really skip to the future and say hey if you do this you're going to be a billionaire no that would be cheating you're not allowed to do that you can only look at the mistakes you made and say how can I become a billionaire based on all these other mistakes that I made how can I become as close to perfect as possible which no one I can ever be perfect but that's my little analogy for the day so that's mask attention pretty much just not letting us skip time steps so that's fun let's continue two more little things I want to touch on before I jump forward here so these keys, queries and values each of these are learned through a linear transformation just an end dot linear is applied and that's how we get our keys, queries and values so that's just a little touching there if you're wondering how do we get those it's just an end dot linear transformation and then as for our masking we don't actually apply this all the time you might have seen right here we have multi-head attention multi-head attention and then mask multi-head attention so this masked attention isn't used all the time it's only used actually one out of the three attentions we have per layer so I'll give you a little bit more information about that as we progress more and more into the architecture as we learn more about it I'm not going to dive into that quite yet though so let's just continue on with what's going on so we have a softmax and why softmax important well I actually mentioned earlier softmax is not commonly used as a normalization method but here we're actually using softmax to normalize so when you have all of these when you have all of these attention scores essentially what the softmax is doing is it's going to exponentiate and normalize all of these so all of the attention scores that have scored high like maybe 50 to 90% or whatever it is those are going to take a massive effect in that entire attention I guess tensor if you want to call it that and that's important it might not seem important but it's essentially just giving the model more confidence as to which tokens matter more so for example if we just did a normalization we would have words like server and crash and then server and check and then you would just know a decent amount about those those would pay attention to a decent amount because they multiply together quite well but if you softmax those then it's like those are almost the only characters that matter so it's looking at the context of those two and then we're sort of filling in like we're learning about the rest of the sentence based on just the sentiment of those attention scores because they're so high priority because they multiply together to such a high degree we want to emphasize them basically let the model learn more about which words matter more together so that's pretty much just what the softmax does it increases our confidence in attention and then a matrix multiply we go back to our V here and this is a value so essentially what this is is just a linear transformation and we apply this on our we apply this on our inputs and we have some value about you know what exactly those tokens are and after we've gotten all of our attention our softmax everything done it's just going to multiply the original values by everything we've gotten so far just so that you don't have any information that's really lost or we don't have anything scrambled just that we have like a general idea of okay these are actually all the tokens we have and then these are we found interesting the attention scores so we have an output which is a blend of input vector values and attention placed on each token and that's pretty much what's happening in scaled dot product attention in parallel so we have a bunch of these that are just happening at the same time many of these happening at the same time and yeah so that's what attention is that's what feedforward networks are residual connections are and yeah and then so after this after we've fed these into our decoders we get an output we apply linear transformation to summarize softmax probabilities and then we generate based on that based on everything that we learned and actually what I didn't quite write a lot about was the decoder so what I'm actually going to talk about next is something I didn't fill in yet which is why why the heck do we use mass attention here but not in these places so why the heck do we have a multi attention here all that attention here but mass attention here so why is this well the purpose of the encoder is to pretty much learn the present past and future and put that into a vector representation for the decoder that's what the encoder does so it's okay if we look into the future and understand tokens that way because we're technically not cheating we're just learning the different attention scores and yeah we're just using that to help us predict based on what the sentence looks like but not explicitly giving it away just giving it an idea of what to look for type of thing and then we use mass attention here because well we don't want to look ahead we want to look at the present and the past and later on we're not giving anything explicit here we're not giving anything yet so we want to make some raw guesses they're not going to be very good guesses at first we want to make some raw guesses and then later on we can feed these the added and normalized guesses into this next multi attention which isn't masked and then we can use this max multi head attention with the vector representation given by the encoder and then we can sort of do more useful things with that rather than just being forced to guess raw attention scores and then being judged for that we can sort of introduce more more and more elements in this decoder block to help us learn more meaningful things so we start off with making this mass multi head attention and then combining that with our then afterwards we do a multi head attention with the vector representation from the encoder and then we can make decisions on that so that's kind of why that works this way if you don't think I explain it like amazingly well you can totally just ask GPT4 or GPT3.5 and get a pretty decent answer but that's how that works and another thing I kind of wanted to point out here is these linear transformations that you see I mean there's a lot of them in the scaled dot project attention so you have your linears for your value or key value and key query and values so as well as the one up here linears are great for just expanding or shrinking a bunch of important info into something easier to work with so if you have a bunch of if you have a large vector containing a bunch of info learned from this scaled dot project attention you can you can sort of just compress that into something more manageable through a linear transformation and it's essentially what's just happening here with Softmax as well as in our scaled dot project attention here for these linear transformations from our inputs to quick keys, queries and values that's all that's happening if you want to read more about linear transformations the importance of them you can totally go out of your way to do that but that's just sort of a brief summary as to why they're important just shrinking or expanding so that's sort of a brief overview on how transformers work however in this course we will not be building the transformer architecture we'll be building something called a GPT which you're probably familiar with and GPT stands for Generatively Pre-Trained Transformer or Generative Pre-Trained Transformer one of the two and pretty much what this is it's pretty close to the transformer this architecture here except it only adopts the decoder blocks and it takes away this multi-head attention here so all we're doing is we're removing the encoder as well as what the encoder plugs into so all we have left is just some inputs our max multi-head attention our post-norm architecture and then right after this we're not going to a non-mass multi-head attention but rather to a feed forward network and then a post-norm so that's all it is, it's just 1, 2, 3, 4 that's all it's going to look like that's all the blocks are going to be it is still important to understand the transformer architecture itself because you might need that in the future and it is sort of a good practice in language modeling to have a grasp on and to understand you know why we use mass multi-head attention in the decoder and why we don't use it in the encoder and stuff like that so anyways we're going to go ahead and build this if you need to look back if something wasn't quite clear definitely skip back a few seconds or a few minutes through the video and just make sure you clarify everything up to this point but yeah I'm going to go over some more math on the side here and just some other little little widgets we're going to need for building the decoder GPT architecture so let's go ahead and do that we're going to jump into building the transformer rather than building the GPT from scratch what I want to do is linger on self-attention for a little bit or rather just the attention mechanism and the matrix multiplication behind it and why it works so I'm going to use whiteboard to illustrate this so we're going to go ahead and draw out a we'll just use maybe a four token sequence here of words okay so we're going to highlight which words are probably going to end up correlating together or the attention mechanism is going to multiply them together to a high amount based on what it learns about those tokens this is what this is so I'm going to help us illustrate that and what the GPT is going to see sort of from the inside what it looks like from the inside so I'm going to go ahead and draw this out here just make a table here we'll give it four of these and draw a little line through the middle my drawing might not be perfect but it's definitely better than on paper so cool we have this we have my I'm going to go here dog has please and then my my dog so I delete that my dog has please cool so to what degree are these going to interact well my and my I mean it doesn't really give away that much it's only just the start so maybe this will interact to a low amount and then you have my and dog these might interact to a medium amount because it's like your dog so we might go we might go medium like that and then my and has well that doesn't give away too much so maybe that'll be low and then my and please it's like oh that doesn't really mean much my please that doesn't really make sense maybe we'll have it interact to a low amount and then these would be the same thing so my and dog so be medium and then has and has would be low and then my and please would also be low and then you have dog and dog so these might interact to a low amount they're the same word so we'll just forget about that and then we have a dog has so these might interact to a medium amount dog has the dog has something and then dog and please these might interact to a high amount because they're associating the dog with something else meaning please we have has and dog these would interact to the same amount so medium and then has and has be probably to a low amount and then we could do low for we could do what was it high for this one as well please and dog so these will interact to a high amount and then we have has and please so these could interact maybe a medium amount medium and then please and please which would be low so what you get I'll just highlight this in I'll just highlight this in green here so you get all the medium and high attention scores you'd have your medium here medium here high medium medium high medium and medium so you can see that these are sort of symmetrical and this is what the attention map will look like of course there's going to be some scaling going on here based on the amount of actual attention's heads we have running in parallel but that's besides the point really what's going on here is the network is going to learn how to place the right attention scores because attention is simply being used to generate tokens that's that's how the that's how the GPT works it's using attention to generate tokens so we can make those sort of attention scores how they're placed we can make those learnable through all of the like embeddings like everything we have in the entire network can make sure that we place effective attention scores and to make sure that they're measured properly so obviously I didn't quantify these very well like not with floating point numbers but this is sort of the premise of how it works and how we want the model to look at different tokens and how they relate to one another so that's what the attention mechanism looks like under the hood so this is what the actual GPT or decoder only transformer architecture looks like and so I'm just going to go through this step by step here and then we can hopefully jump into some of the math and code behind how this works so we have our inputs embeddings and positional encodings we have only decoder blocks and then some linear transformation and then pretty much just we do some softmax probability distribution we sample from those and then we start just generating some output and then we compare those to our inputs and see how off they were, optimized from that in each of these decoder blocks we have our all data attention, res connections feedforward network consists of a linear, real linear border and then another res connection in each of these multi-attentions we have multiple heads running in parallel and each of these heads is going to take a key, query and value these are all learnable linear transformations and we're going to basically dot product the key and query together concatenate these results and do a little transformation to sort of summarize it afterwards and then what actually goes on in the dot product attention is just the dot product meaning of the key and query the scaling to prevent these values from exploding to prevent the vanishing gradient problem and then we have our masking to make sure that these, to make sure the model isn't looking ahead and cheating and then softmax matrix multiply we output that and then kind of fill in the blank there, so cool this is a little bit pretty much the transform architecture a little bit dumb down a little smaller in complexity to actually understand but that's kind of the premise of what's going on here so still implements a self-attention mechanism so as you can see now I am currently on my macbook M2 chip, I'm not going to go into the specs of why it's important but really quick, I'm just going to show you how I SSH onto my other PC so I go SSH just like that and then I type in my ipv4 address and then I just get a simple password here, password that I've never had is cool so now I'm on my desktop computer and this is the command prompt that I use for it so awesome I'm going to go ahead and go into the free code camp little directory I have so cd desktop cd python testing and then here I'm actually going to activate my CUDA virtual environment oops, not accelerate I'm going to go CUDA activate cool and then I'm going to go cd into free code camp gbt course, awesome so now, if I actually do code on here like this to open up my VS code, it doesn't do that so there's another little way I have to do this and you have to go into VS code go into a little remote explorer here and then you can simply connect so I'm just going to connect to the current window itself there's an extension you need for this called open SSH server, I think it's what it's called and it's simply the same password I used in the command prompt I can type it correctly awesome so now it's SSH into my computer upstairs and I'm just going to open the little editor in here nice, so you can see that it looks just like that, that's wonderful so now I'm going to open this in a Jupyter notebook actually cd into desktop here cd python cd python testing CUDA scripts activate cd free code camp gbt course and then code like that and it will open perfect how wonderful is that and I've already done a little bit of this here but we're going to jump into exactly how we can build up this transformer or gbt architecture in the code itself so I'm going to pop over to my Jupyter notebook in here cool and now this little address I'm going to paste that into my browser awesome so we have this gbt v1 Jupyter notebook so what I've actually done is I've done some importations here so I've imported all of these python importations all the hyper parameters that we used from before I've imported the data loader I've imported the tokenizer the train and bell splits they get batch function estimate loss, just everything that we're going to need and it's all in neatly organized little code blocks so awesome now what? well let's go ahead and continue here with the actual upgrading from the very top level so I remember I actually showed and you can skip back to this I actually showed the architecture of the gbt sort of lined out in I guess a little sketch a little sketch that I did and all we're going to do is pretty much build up from the high level the high high level general architecture down to the technical stuff down to the very root dot product attention that we're going to be doing here so I'm going to go ahead and start off with this gbt language model which I just renamed I replaced bygram with gbt here so that's all we're doing and we're going to add some little code bits and just walk through step by step what we're doing so let's do that so great we're going to next we're going to talk about these positional encodings so I go back to the paper here rather this architecture we initially have our tokenize inputs and then we give we give them embedding so token embeddings and then a positional encoding so this positional encoding going back to the attention paper is right here so all it does is every even token index we apply this function and then every odd token index we apply this function you don't really need to know what it's doing other than the fact that these are the different sine and cosine functions that it uses to apply positional encodings to the tokenized inputs so every so on our first index or whatever let's say we have hello world okay there's five characters here h will be index zero so it'll get an even encoding function and then e will be odd since it's index one so it'll get this one and then l will get this the next l will get this and then or I don't know if I messed up that order but essentially it just iterates and it goes back and forth between those applying these fixed functions and the thing is with fixed functions is that they don't actually learn about the data at all because they're fixed so another way we could do this would be using nn.embedding which is what we use for the token embedding so I'm going to go ahead and implement this here in our gbtv one script so I'm going to go ahead and add on this line self dot positional self dot position embedding table nn.embedding block size so the block size is the length or the sequence length which in our case it's going to be 8 so there's going to be 8 tokens and this means we're going to have 8 different indices and each one is going to be of size nn.embed and this is a new parameter I actually want to add here so nn.embed will not only be used in positional embedding but it will also be used in our token embedding because when we actually store information about the tokens we want that to be in a very large vector so not necessarily a probability distribution or what we were using before in the bi-gram language model but rather a really large vector or a list you could think about it as a bunch of different attributes that are about a character so maybe you know A and E would be pretty close but both vowels versus like E and Z would be very different because Z is not a very common letter and E is the most common letter in the alphabet so we pretty much just want to have vectors to differentiate these tokens to place some semantic meaning on them and anyways that's a little talk about what token embedding table is going to do when we add n.embed and then positional embedding table is just the same thing but instead of each character having its own thing each letter index in the input is going to have its own embedding so I can go and add this up here the n.embed and we can just make this maybe 384 so 384 is quite huge and it's maybe a little too big for your PC but we'll see in a second so what this is going to do is it's going to have a giant vector it's going to be like we could say like embedding embedding vector and then it would be like this and you would have a bunch of different attributes so like 0.1 0.2 0.8 1.1 right? except instead of 4 this is 384 elements long and each of these is just going to store a tiny little attribute about that token so let's say we maybe had like a two dimensional and we were using a word so if we had sad versus happy sad might be sad might be 0.1 and then 0.8 or 0.8 whereas happy sad would be maybe the positivity of what it's saying and then 0.8 would be is it showing some sort of emotion which is a lot right? it's 80% emotion and 0.1 of maybe positive sentiment and then if we had 0.9 would be happy because it's happy it's very good and then 0.8 is emotional because they're sort of the same emotional level but yeah so this is what our embedding vectors are pretty much describing and all this hyperparameter is concerned with is how long that vector actually is so anyways let's continue with the GPT language model class so the next bit I like to talk about is how many decoder layers we have so in here let's just say we have four decoder layers so we have four of these it's going to go through this one and then this one and then this one then this one this is all happening sequentially so we could actually make a little sequential neural network with four decoder layers so I'm actually going to add this in and then a little bit of extra code which I'll explain in a second here so this self dot blocks is how many decoder blocks we have running sequentially or layers blocks and layers can be used interchangeably in this context but yeah we have an end dot sequential and this asterisk is pretty much saying we're going to repeat this right here for how many end layer is and end layer is another hyperparameter we're going to add we go end underscore layer we go equals four so end underscore layer equals four that means it's going to make four of these I guess blocks or layers sequentially it's going to make four of them and this little block thing we're going to build on top of this in a second here we're going to make an actual block class and I'm going to explain what that does but for now this is going to be some temporary code as long as you understand that this is what this is how we create our four layers our four decoder layers that's all you need to know for now I'm going to move more into this block later as for this self dot layer norm final this is the final layer norm all this is going to do is we're just simply going to add this to the end of our network here just simply at the end here and all this is going to do is just going to help the model converge better layer norms are super useful and yeah so you'll see more how that works I'll actually remove it later on and we'll actually compare and see how good it actually does and you can totally go out of your way to experiment with different normalizations and see how well the layer norm helps the model perform or how well the loss sort of converges over time when you put the layer norm in different places so let's go back here and now we have this end here which is the language I believe this is the language modeling head or something again this is what Andrey Karpathy used I'm assuming that means language modeling head but pretty much all we're doing is we're just projecting we're doing this final transformation here this final little linear layer here from all of these sequential decoder outputs and we're just going to transform that to something that the softmax can work with so we have our layer norm afterwards to sort of normalize help the model converge after all these after all this computation we're going to feed that into a linear layer to make it I guess softmax workable so the softmax can work with it and yeah so we're just simply projecting it from an embed which is the vector length that we get from our decoder and and this vocab size so the vocab size is going to essentially give up a little probability distribution on each token that we have or the vocabulary so anyways I'm going to make this back to normal here and we're going to just apply this to the forward pass so a little thing I wanted to add on to this positional embedding or rather just the idea of embeddings versus the fixed definite function of the sinusoidal functions and the cosine functions that we used here these are both actually used in practice the reason I said we're going to use embeddings is because we just want it to be more oriented around our data however in practice sinusoidal encodings are used in base transformer models whereas learned embeddings what we're using are used in variants like GBT and we are building a GBT so we're probably going to find out a performance from learning about embeddings and this is just summing up the experts do it's a little practice that experts do when they're building transformer models versus variants like GBTs so that's just a little background on why we're using learnable embeddings so now let's continue with the forward pass here so I'm going to paste in some more code and let me just make sure this is formatted properly cool so we have this token embedding which is our token embedding table we take an IDX token embedding here then what we do with this positional embedding table so we have this torch.arrange we make sure this is on the CUDA device the GPU device so it's in parallel and all this is going to do is it's going to look at how long is T and let's say T is our block size so T is going to be 8 so all it's going to do is give us 8 indices it's going to be like 0, 1, 2, 3, 4, 5, 6, 7 8 of those and we're essentially just going to give each of those each of those indices a different a different end embedding vector for each of those indices just a little lookup table and that's what that is so all we do now is it's actually quite simple and this is a very efficient way to do it is you just add these two together broadcasting rules which you might want to look into I'll actually search that up right now torch broadcasting semantics pie torch broadcasting I cannot spell broadcasting semantics so these are a little bit funky when you look at them the first time but pretty much these are just rules about how you can do arithmetic operations and just operations in general to tensors so tensors are like you think of matrices where it's like a 2x2 tensors can be the same thing but they could be like a 2x2x2 or a 2x2x2x2x2 whatever dimension you want to have there and pretty much it's just rules about how you can have two of those weirdly shaped tensors and do things to them so just some rules here I would advise you familiarize yourself with these even play around with it if you want just for a few minutes and just get an idea for which, like just try to multiply tensors together and see which ones throw errors and which ones don't so it's a good idea to understand how broadcasting rules work obviously this term is a little fancy and it's like that's like a crazy advanced term not really it's pretty much just some rules about how you're multiplying these really weirdly shaped tensors so yeah anyways if we go back to here we are allowed to broadcast these we're allowed to actually add them together so the positional embedding and the token embedding we get X from this B by T by C shape so now what we can do with these is we can actually feed it into the GPT or I guess sort of a transformer network if you want to say that so we have these embeddings and positional encodings we add these together and then we feed them into our sequential network so how are we doing this well we go self dot blocks which is up here and we essentially just feed an X which is literally exactly what happens here we have our tokenized inputs we got our embeddings and our positional encodings through learnable embeddings we add them together and then we feed them into the network directly so that's all that's happening here and that's how we're feeding an X which is the output of these then after this is like way after we've gotten through all of these GPT layers or blocks we do this final layer norm and then this linear transformation to get it to a softmax to get it to essentially probabilities that we can feed into our softmax function and then other than that this forward pass is exactly the same other than this little block of code here so if this makes sense so far that is absolutely amazing let's continue I'm actually going to add a little bit of in practice some little weight initializations that we should be using in our language model and in module subclass so I'm going to go over a little bit of math here but this is just really important for practice and to make sure that your model does not fail in the training process this is very important it's going to be a little funky on the conceptualizing but bring out some pen and paper and do some math with me we've built up some of these initial GPT language model architecture and before we continue building more of it and the other functions some of the math stuff that's going on the parallelization that's going on in the script I want to show you some of the math that we're going to use to initialize the weights of the model to help it train and converge better so there's this new thing that I want to introduce called standard deviation and this is used in intermediate level mathematics the symbol essentially looks like this population standard deviation so n the size so it's just going to be an array the length of the array and then xi we iterate over each value so xf position 0 xf position 1 xf position 2 and then this u here is the mean so we iterate over each element we're going to subtract it by the mean we're going to square that and then keep adding all these squared results together and then once we get the sum of that we're going to subtract or we're going to divide this by the number of elements there are and then once we get this result we're going to square root that so this symbol here might also look a little bit unfamiliar and I'll illustrate this out for you so we go to our whiteboard and this e looks like looks like that let's just say we were to put in x i like that and our array let's just say for instance our array is 0.1 0.2, 0.3 so what would the result of this be well if we look at each element iteratively add them together so 0.1 plus 0.2 plus 0.3 well we get 0.6 from that so this would essentially be equal to 0.6 that's what that equals we just add each of these up together or we do whatever this is iteratively whatever this element is we iterate over the number of elements we have in the arbitrary array or vector or list or whatever you want to call it and then we just sort of look at what's going on here and we can do some basic arithmetic stuff so let's walk through a few examples just to illustrate to you what the results look like based on the inputs here so I'm going to go back to my whiteboard we're going to draw a little line here just to separate this so I want to calculate the standard deviation do standard deviation of and then we'll just make some random array negative 0.38 negative 0.38 0.52 and then 2.48 cool so we have this array this is three elements so that means n is going to be equal to three let me drag this over here so n is the number of elements so n is going to be equal to three our mean well our mean is just we add all these up together and then we average them so our mean is going to be equal to let's just say 0.38 plus 0.52 plus 2.48 and then divided by three and the answer to this I did the math ahead of time is literally 0.873 repeated but we're just going to put 0.87 for simplicity's sake cool so the mean of this is 0.87 and n is equal to three now we can start doing some of the other math so we have this O has a cool line and we do square root one over n which is equal to three and then we multiply this by sigma that's what this symbol is that's sigma that's the name for it and then we go X I minus and then our mean of 0.87 apologies for the sloppy writing and then we square that so let me drag this out awesome so let's just do this step by step here so the first one is going to be 0.38 0. negative 0.38 and we're going to do minus the mean here so minus 0.87 and I'm just going to wrap all this in brackets so that we don't miss anything wrap it in brackets and then just square it and see what we get after so I'm just going to write all these out then we can do the calculations so next up we have 0.52 minus 0.87 we'll square that and then next up we have 2.48 minus 0.87 and then we square that as well so awesome what is the result of this the result of negative 0.38 minus 0.87 squared is 1.57 the result of this line is 0.12 again these are all approximations they're not super spot on we're just doing this to understand what's going on here just to overview the function not for precision then the next one is going to be 2.59 and you can double check all these calculations if you'd like I have done these preemptively so that is that and now from here what we have to do is add each of these together so 1.57 plus 0.12 plus 2.59 divided by 3 is 1.57 plus 0.12 plus 2.59 all that divided by 3 is going to be equal to 1.42 keep in mind we also have to square root this so the square root of that is going to be 1.19 approximately we'll just add this guy ahead of it so that's what the standard deviation of this array is negative 0.38 0.52, 2.48 standard deviation is 1.19 let's do another example so let's say we want to do the standard deviation of 0.48 0.5 0.50 I guess 0.52 so there's a little pattern here just goes up by 0.02 each time and you're going to see why this is vastly different than the other example so let's walk through this so first of all we have N N is equal to 3 cool what does our mean our mean well if you do our mean our mean is 0.5 0.48 plus this plus that that's going to be 0.5 and if you're good with numbers you'll probably already be able to do this in your head but that's okay if not next up we're going to do this in the formula so what do these iterations look like so 0. let's just do these in brackets the old way 0.5 squared the next one is 0.5 minus 0.5 squared which we already know is 0 and this one is 0.52 minus 0.5 squared so the result of 0.48 minus 0.5 squared and what's right equals here is going to be approximately 0.02 squared so that would be 0.004 like that so I'll make this not actually overlap 0.004 and then this one we obviously know would be 0 because 0.5 minus 0.5 that's 0 then you square 0 still the same thing and then this one is 0.0004 as well so when we add these two together we're going to get 0.0008 just like that and then if we divide them by 3 or whatever n is then we end up getting 0.00026 repeating so I'll just write 266 like that and so all we have to do at this point is do the square root of this and we'll do square root of 0.00026 approximately and that's going to be equal to about 0.0163 so that is our standard deviation of both of these arrays here so 0.048 and then 0.52 our standard deviation is 0.0163 so very small and then we have negative 0.38, 0.52 and 2.48 we get a standard deviation of 1.19 so you can see that these numbers are vastly different one is like one is literally 100 times greater than the other so the reason for this is because these numbers are super diverse I guess another way you could think of them is that they stretch out very far from the mean this essentially means when you're initializing your parameters that if you have some outliers then your network is going to be funky because it's the learning process just messed up because you have outliers and it's not just learning the right way it's supposed to whereas if you had way too small of a standard deviation from your initial parameters like in here but maybe even smaller so let's say they were all 0.5 then all of your neurons would effectively be the same and they would all learn the same pattern so then you would have no learning done so one would either be you're learning a super super unstable and you have outliers that are just learning very distinct things and not really not really not really letting other neurons get opportunities to learn or rather other parameters to learn if you have a lot of diversity you just have outliers and then if you have no diversity at all then essentially nothing is learned and your network is useless so all we want to do is make sure that our standard deviation is balanced and stable so that the training process can learn effective things so each neuron can learn a little bit so you can see here this would probably be an okay standard deviation if these were some parameters because they're a little bit different than each other they're not all like super super close to the same and yeah so essentially what this looks like in code here is the following so you don't actually need to memorize what this does as it's just used in practice by professionals but essentially what this does is it initializes our weights around certain standard deviations so here we set it to 0.02 which is pretty much the same as what we had in here so point point this one's a little bit off in the standard deviation set here but essentially we're just making sure that our weights are initialized properly and you don't have to memorize this at all it's just used in practice and it's going to help our training converge better so as long as you understand that we can apply some initializations on our weights that's all that really matters, so cool let's move on to the next part of our GBT architecture so awesome, we finished this GBT language class, everything's pretty much done here we did our knit we did some weight initializations and we did our forward pass, so awesome that's all done, now let's move on to the next which is the block class so what is block? well, if we go back to this diagram each of these decoder blocks is a block so we're pretty much just going to fill in this gap here our GBT language model has these two where we get our tokenized inputs and then we do some transformations and the softmax after and essentially we're just filling in this gap here and then we're going to build out and just sort of branch out until it's completely built so let's go ahead and build these blocks here what does this look like? that's what this does so we have our knit, we have a forward pass as per usual and knit and a forward pass as seen in the GBT language model class which is going to look like this forward and an init so the init is going to just initialize some things it's going to initialize some transformations and some things that we're going to do in the forward pass that's all it's doing so what do we do first? well we have this new head size parameter introduced so head size is the number of features that each head will be capturing in our multi-head attention so all the heads in parallel features are each of them capturing so we do that by dividing n embed by n head so n head is the number of heads we have and n embed is the number of features we have where we're capturing so 384 features divided by 4 heads so each head is going to be capturing 96 features hence head size so next up we have self.sa which is just short for self-attention we do a multi-head attention we pass in our n head and our head size and you'll see how these parameters fit in later once we build up this multi-head attention class so cool now we have a feed forward which is as explained just in the diagram here our feed forward is just this which we're actually going to build out next and we have two layer norms and these are just for the post norm pre norm architecture that we could implement here in this case it's going to be post norm just because I found that it converges better for this for this course and the data that we're using and just the model parameters and what not it just works better so also that is the original architecture that we use in the attention paper so you might have seen that they do an add a norm rather than a norm and add anyways we've initialized all of these so we have head size, self attention feed forward and then two layer norms so in our forward pass we do our self attention first let's actually go back to here so we do our self attention then add a norm then a feed forward and then add a norm again so what does this look like self attention, add a norm feed forward, add a norm cool so we're doing an add so we're going x plus the previous answer which is adding them together and then we're just applying a layer norm to this so cool if you want to look up more into what layer norm does and everything and why it's so useful you can totally go out of your way to do that but layer norm is essentially just going to help smoothen out our features here so and honestly there's not much else to that we just return this final value here and that's pretty much the output of our blocks so next up I'm going to add a new little code block here which is going to be our feed forward so let's go ahead and do that so feed forward, it's just going to look exactly like this it's actually quite simple so all we do is we make an nn dot sequential torch dot nn we make this a sequential network of linear linear, relu, and then linear so in our linear we have to pay attention to the shapes here so we have n embed and then n embed times 4 and then the relu will just essentially what the relu will do is it looks like this let me illustrate this for you guys so essentially you have this graph here and let's just make this a whole plane actually so all of these values that are below 0 all these values that are below 0 on the x axis and equal to 0 will be changed just to 0 like that so you have all these values that look like this and then everything that is above 0 just stays the same so you essentially just have this funny looking shape it's like straight and then diagonal that's what the relu function does it looks at a number sees if it's equal to or less than 0 if that's true we give that number 0 and if it's not then we just leave the number alone so cool very cool non-linearity function you can read papers on that if you like but essentially the shape of this just doesn't matter all we're doing is we're just making sure that we're just converting some values if they're equal to or below 0 that's all this is doing and then we essentially are multiplying this we're doing this linear transformation times this one so we have to make sure that these inner we have to make sure that these inner dimensions line up so 4 times N embed and 4 times N embed those are equal to each other so our output shape should be N embed by N embed cool so now we have our dropout and in case you don't know what dropout is it pretty much just makes a certain percentage of our neurons just dropout and become 0 this is used to prevent overfitting and some other little details that I'm sure you could you could figure out through experimenting so all this actually looks like in a parameter form is just dropout dropout equals we'll just say 0.2 for the same so 0.2 means 20% or 0.2 is going to yeah so 0.2 in percentage form is just going to dropout 20% of our neurons turn them to 0 to prevent overfitting that's what that's doing so cool we have our feedforward network we dropout after to prevent overfitting and then we just call it forward on this sequential network so cool feedforward pretty self-explanatory we're going to add the multi-head attention class so we've built all these decoder blocks we've built inside of the decoder blocks we've built the feedforward and our res connections and now all we have to do left in this block is the multi-head attention so it's going to look exactly like this here we're going to ignore the keys and queers for now and save this for dot product attention so we're going to essentially just make a bunch of these multiple heads and we're going to concatenate results and do a linear transformation so what does this look like in code well let's go ahead and add this here all that attention cool so multiple heads of attention in parallel I explained this earlier so I'm not going to jump into too much detail on that but we have our knit we have our forward and what are we doing in here so our self dot heads is just a module list and module list is kind of funky I'll dive into it a little bit later but essentially what we're doing is we're having a bunch of these heads essentially in parallel for each head so num heads let's say our num heads is set to our num heads is set to maybe four in this block we do multi-head attention we do n heads and then head size so and heads and then head size so num heads essentially what it is so for the number of heads that we have which is four we're going to pretty much make one head running in parallel so four heads running in parallel is what this does here then we have this projection which is essentially just going to project the head size times the number of heads to an embed and you might ask well that's weird because num heads times this is literally equal to an embedding if you go back to the math we did here and the purpose of this is just to be super hackable so that if you actually do want to change these around it won't be throwing you dimensionality errors so that's what we're doing just a little projection from our whatever these values are up to this constant feature length of an embed so then we just follow that with a drop out dropping out 20% of the networks neurons now let's go into this forward here so forward torch dot concatenate or torch dot cat we do four h and self dot heads so we're going to concatenate each head together along the last dimension and the last dimension in this case is the b batch by time by we just say feature dimension or channel dimension the channel dimension here is the last one so we're going to concatenate along this feature dimension and let me just help you illustrate what exactly this looks like so when we concatenate along these we have this b by t and then we'll just say our features are going to be h1 like each of our heads here another h1 h1 h1 and these are all just features of head one and then our next would be h2 h2 h2 h2 and then let's just say we have a third head go h3 h3 h3 h3 h3 like that so we have maybe four features per head and there's three heads so essentially all we're doing when we do this concatenate is we're just concatenating these along the last dimension so to convert this like ugly list format of just each head features sequentially in order which is like really hard to process we're just concatenating these so they're easier to process so that's what that does and then we just follow this with a dropout self dot projection and then just follow that with a dropout so cool if that didn't totally make sense you can totally just plug this code into chat gbt and get a detailed explanation on how it works if something wasn't particularly clear but essentially that's the premise you have your batch by time batch by sequence length or time use interchangeably and then you have your features which are all just in this weird list format of each feature just listed after another so cool that's what multi head attention looks like let's go ahead and implement dot product attention or scale dot product attention so a little something I'd like to cover before we go into our next scaled dot product attention was just this linear transformation here and you might think well what's the point if we're just transforming an embed to an embed right we're to have the match like that and essentially what this does is it just adds in another learnable parameter for us so it has a weight and a bias if we set bias to false like that then it wouldn't have a bias but it does have a bias so another just wx plus b if you will a weight times x plus a bias so it just adds more learnable parameters to help our network learn more about this text so cool I'm going to go ahead and add in this last but not least scale dot product attention or head class so there's going to be a bunch of these heads hence class head running in parallel and inside of here we're going to do some scale dot product attention so there's a lot of code in here don't get too overwhelmed by this but I'm going to walk through this step by step so we have our in it we have our forward awesome so what do we do in our architecture here so we have a key a query and a value the keys and the queries dot product together they get scaled by one over the square root of length of a row in the keys or queries matrix so we'll just say maybe keys for example the row of keys the length of a row in keys and then we just do our masking to make sure the network does not look ahead and cheat and then we do a softmax and a matrix multiply to essentially add this value weight on top of it so cool we do this keep in mind this initialization is not actually doing any calculations but just rather initializing linear transformations that we will do in the forward pass so this self dot key is just going to transform and embed to head size bias false and then I mean the rest of these are just the same and embed to head size because each head will have 96 features rather than 384 so we kind of already went over that but that's just what that's doing cool that's just a linear transformation that's happening to convert from 384 to 96 features then we have this self dot register buffer well what does this do you might ask register buffer is essentially just going to register this no look ahead masking in the model state so instead of having to re-initialize this every single head for every single forward and backward pass we're just going to add this to the model state so it's going to save us a lot of computation that way on our training so our training times can be reduced just because we're registering this yeah so it's just going to prevent some of that overhead computation of having to redo this over and over again you could still do training without this it would just take longer so that's what that's doing yeah so now we have this drop-out of course and then in our forward pass let's break this down step by step here so we have a b by t by c so batch by time by channel is our shape we just unpack those numbers and then we have a key which is just calling this linear transformation here on an input x and then a query which is also calling the same transformation but a different learnable transformation on x as well so what we get is this instead of b by t by c we get b by t by head size hence this transformation from 384 to 96 so that's what that is that's how these turn out here so now we can actually compute the attention scores so what do we do we'll just say weights is our attention weights are I guess you could say that we have our queries dot product matrix multiply with the keys transposed so what does this actually look like and I want to help you guys sort of understand what transposing does here so let's go back to here and draw out what this is going to look like so essentially what transposing is going to do is it is just going to make sure let me draw this out first so let's say you had I don't know maybe a b c d and you have a b c and d cool let's draw some lines to separate these so awesome so essentially what this does is the transposing puts it into this form so if we didn't have transposed then this would be in a different order it wouldn't be a b c d in both from like top to bottom left to right type of thing it would be in a different order but essentially not allow us to multiply them the same way so when we do a by a a times b it's like sort of a direct multiply if you will I don't know if you remember times tables at all from elementary school but that's pretty much what it is we're just setting up in a times table form and we're computing attention scores that way so that's what that is that's what this transposing is doing all this does is it just flips the second last dimension with the last dimension so in our case our second last is t and our last is head size so it just swaps these two so we get b by t by head size and then b by head size by t we dot product these together also keeping in mind our scaling here which is taking this we're just taking this scaling one over the square root of length of a row in the keys if we look at this here now there's little analogy I'd like to provide for this scaling right here so imagine in a room with a group of people and you're trying to understand the overall conversation if everyone is talking at once it might be challenging to keep track of what's being said it would be more manageable if you could focus on time right? so that's similar to how a multi head attention in a transformer works so each of these heads divides the original problem of understanding the entire conversation i.e. the entire input sequence into smaller more manageable sub problems each of these sub problems is a head so the head size is the number of these sub problems now consider what happens when each person talks louder or quieter if someone speaks too loudly or the values and the vectors are very large it might drown out the others this could make it difficult to understand the conversation because you're only hearing one voice or most of one voice to prevent this we want to control how loud or how quiet each person is talking so we can hear everyone evenly the dot product of the query and key vectors in the attention mechanism we want to check how loud each of voices if the vectors are very large or high dimensional or many people are talking the dot product can be very large to control this volume by scaling down the dot product using the square root of the head size this scaling helps ensure that no single voice is too dominant allowing us to hear all the voices evenly this is why we don't scale by the number of heads time steps they don't directly affect how loud each voice is so in sum multi head attention allows us to focus on different parts of the conversation and scaling helps us to hear all parts of the conversation evenly allowing us to understand the overall conversation better so hopefully that helps you understand exactly what this scaling is doing so now let's go into the rest of this here so we have this scaling applied for our head size our head size dimension we're doing this dot product matrix multiplication here we get our B by T by T and then what is this masked fill doing so let me help you illustrate this here so mask fill is essentially we'll say block size is 3 here alright so we have initially like a 1 a 0.6 and then like a 0.4 then our next one is yeah we'll just say all of these are the same so essentially in our first one we want to mask out everything except for the first time step and then when we advance one so let's just change this here back to 0 when we go on to the next time step we want to expose the next piece so 0.6 I believe it was and then a 0 again and then when we expose the next time step after that we want to expose all of them so just kind of what this means is as we as the time step advances in this sort of I guess vertical part is every time this steps 1 we just want to expose one more token or one more and then we'll use sort of in like a staircase format so essentially what this mask fill is doing is it's making this T by T so block size by block size and for each of these values we're going to set them to negative infinity so for each value that's 0 we're going to make that the float value negative infinity so it's going to look like this negative infinity negative infinity just like that so essentially what happens after this is our softmax is going to take these values and it's going to exponentiate normalize them we already went over the soft softmax previously but essentially what this is going to do this this last dimension here concatenate or not concatenate rather apply the softmax along the last dimension is it's going to do that in this sort of horizontal here so this last this last T it's like blocks it's like block size by block size so it's like we'll say T1 and T2 each of these being like the block size we're just going to do it to this last T2 here and this horizontal is T2 so hopefully that makes sense and essentially what this exponentiation is going to do is it's going to turn these values to 0 and this one is obviously going to remain a 1 and then it's going to turn these into 0 and it's going to probably sharpen this 1 here so this 1 is going to be more significant it's going to grow more than the 0.6 because we're exponentiating and then same here so this 1 is going to be very, very sharp compared to 0.6 or 0.4 that's what the softmax does essentially the point of the softmax function is to make the values stand out more it's to make the model more confident in highlighting attention scores so when you have one value that's like very big but not too big, not exploding because of our scaling, right? we want to keep a minor scaling but when a value is big, when a score or attention score is very big we want the model to put a lot of focus on that and to say this the entire sentence or the entire thing of tokens and we just want it to learn the most from that so essentially that's what softmax is doing instead of just a normal normalizing mechanism it's just doing some exponentiation to that to make the model more confident in its predictions so this will help us score better in the long run if we just highlight what tokens and what attention scores are more important in the sequence and then after this softmax here we just apply a simple dropout on this way variable this new calculated way scale.product.attention masked and then softmaxed we apply a dropout on that and then we perform our final weighted aggregation so this v multiplied by the output of the softmax cool so we get this v self.value of x so we just multiply that a little pointer I wanted to add to this module list module list here and then our go yes our sequential network here so we have this sequential number of blocks here for n layers and we have our module list so what really is the difference here well module list is not the same as n and not sequential in terms of the asterisk usage that we see in the language model class module list doesn't run one layer or head after another but rather each is isolated and gets its own unique perspective sequential processing is where one block depends on another to synchronously complete so that means we're waiting on one to finish before we move on to the next so they're not completing asynchronously or in parallel so the multiple heads in a transformer model operate independently and their computations can be processed in parallel however this parallel parallelism isn't due to the module list that stores the heads instead it's because of how the computation are structured to take advantage of the GPU's capabilities for simultaneous computation and this is also how the deep learning framework PyTorch interfaces with the GPU so this isn't particularly something we have to worry about too much but you could supposedly think that these are sort of running in parallel yeah so if you want to get into hardware then that's like your whole realm there but this is PyTorch, this is software not hardware at all I don't expect you have to have any hardware knowledge about GPU, CPU anything like that anyways that's just kind of a background of what's going on there so cool so let's actually go over what is going on from the ground up here so we have this GPT language model we got our token embeddings, positional embeddings we have these sequential blocks initialize our weights for each of these blocks we have a this class block so we get a head size parameter which is n embedded of 384 divided by n heads which is 4 so we get 96 from that that's the number of features we're capturing self-attention we do a feed forward to layer norms self-attention, layer norm feed forward layer norm in the post norm architecture then we do a feed forward just a linear followed by a relu followed by a linear and then dropping that out and then we have our multi-head attention which just sort of structured these attention heads running in parallel and then concatenates the results and then for each of these heads we have our keys, queries and values we register a model state to prevent overhead computation excessively then we just do our scale dot product attention in this line, we do our mast field to prevent look ahead we do our softmax to make our values sharper and to make some of them stand out and then we do a drop out finally on that and just some weighted aggregation we do our weights this final weight variable multiplied by our weighted value from this initially this linear transformation so cool, that's what's happening step by step in this GBT architecture amazing give yourself a good pat on the back go grab some coffee, do whatever you need to do even get some sleep and get ready for the next section so there's actually another hyper parameter I forgot to add which is n layer and n layer is essentially equal to 4 n layer is essentially equal to the number of decoder blocks we have so instead of n block we just say n layers doesn't really matter what it's called but that's what it means and then number of heads is how many heads we have running theoretically in parallel and then n embed is the number of total dimensions we want to capture from all the heads concatenated together type of thing, we already went over that, so cool hyper parameters block size, sequence length batch sizes, how many of these do we want at the same time max itters is just training how many iterations we want to do learning rate is what we covered that in actually the Desmos calculator that I showed a little while back just showing how we update the model weights based on the derivative of the loss function and then validators which was just reporting the loss and then lastly the dropout which is dropping out 0.2 or 20% of the total neurons so awesome that's pretty cool, let's go ahead and jump into some data stuff I'm going to pull out a paper here so let's just make sure everything works here and then we're actually going to download our data so I want to try to run some iterations and just make sure that our, actually I made some changes pretty much this was weird and didn't work so I just changed this around to making our characters empty opening this text file opening it storing it in a variable format and then just making our vocab this sorted list set of our text and then just making the vocab size the length of that so let's go ahead and actually run this through, I did change the block size to 64 batch size 128 some other hype parameters here so honestly the block size and batch size will depend on your computational resources so just experiment with these I'm just going to try these out first just to show you guys what this looks like okay so it looks like we're getting idx is not defined where could that be okay yep so this is we could just change that it's just saying idx is not defined we're using index here idx there so that should work now and we're getting local variable t reference before assignment okay so we have some we have t here and then we initialize t there so let's just bring up up to there cool now let's try and run this oh shape is invalid for input size of okay let's see what we got it turns out we don't actually need two token embedding tables a little bit of a selling mistake but we don't need two of those so I'll just delete that and then what I'm going to do is go ahead and run this again let's see a new error local variable t reference before assignment okay so our t is referenced here and well how can we initialize this what we can do is we could take this index here of shape b by t because it goes b by t plus 1 etc and just keeps growing so we could actually unpack that so we could go b b and t is going to be index that shape just unpack that so cool so now we're going to run this training loop and it looks like it's working so far so that's amazing super cool step 0 train last 4.4 that's actually a pretty good training loss overall so we'll come back after this is done I've set it to train for 3,000 iterations printing every 500 iterations so we'll just see the loss six times over this entire training process or we should I don't know why it's going to 100 eval itters eval itters estimate loss is okay so we don't actually need eval interval we'll just make this sure why not 100 we'll keep that and it's just going to keep going here we'll see our loss over time it's going to get smaller so I'll come back when that's done as for the data we're going to be using the open web text corpus and let's just go down here so this is a paper called survey survey of large language models so I'll just go back to open web text where that is up it's just fine so open web text this is consisted of a bunch of reddit links or just reddit upvotes so if you go and reddit and you see a bunch of those posts that are highly upvoted or downvoted they're pretty much those pieces of text are valuable and they contain things that we can train them so pretty much web text is just a corpus of all these upvoted links but it's not publicly available so somebody created an open source version called open web text hence open and it's pretty much as an open version of this so we're going to download that for a here like common crawl which is really really big so like petabyte scale data volume you have a bunch of books so this is a good paper to read over it's just called a survey of large language models you can search this up and it'll come up you can just download the pdf for so this is a really nice paper read over that if you'd like but anyways this is a download link for this open web text corpus so just go to this link I have it in the github repo and you just go to download and it'll bring you to this drive so you can go in and right click this and just hit download it'll say 12 gigabytes exceeds maximum file size that it can scan so it's like this might have a virus don't worry it doesn't have a virus this is actually created by a researcher so not really bad people are in charge of creating text corpora so go in and download anyway I've actually already downloaded this so yeah I'll come back when our training is actually done here so I'm actually going to stop here iteration 2000 because we're not actually getting that much amazing progress and the reason for this is because our hyper parameters so batch size and block size I mean these are okay but we might want to change up as our learning rate so some combinations of learning rates that are really useful is like 3e to the negative 3 you go 3e to the negative 4 you go 1e to the negative 3 1e 1e to the negative 4 so these are all learning rates that I like to play around with these are just sort of common ones it's up to you if you want to use them or not but what I might do actually is just downgrade to 3e to the negative 4 and we'll retest it as well I'm going to bump up the the number of heads and the number of layers so that we can capture more complex relationships in the text thus having it learn more so I'm going to change each of these to 8 go 8 actually kernel will go restart now we'll just run this from the top and and we'll run that cool so let's see what we actually start off with and what our loss looks like over time cool so we got step 1 4.5 about the same as last time it's like 0.2 off or something so it's pretty close let's see the next iteration here that's wonderful so before we were getting like 3.1 ish or something around that range 3.15 now we're getting 2.2 so you can see that as we change hyper parameters we can actually see a significant change in our loss this is amazing this is just to sort of prove how cool hyper parameters are and what they do for you so let's start changing around some data stuff this right here is the Wizard of Oz text just a simple text file it's the size isn't super large so we can actually open it all into ram at once but if we were to use the open web text we cannot actually read you know 45 gigabytes of utfa text in ram at once just can't do that unless you have like maybe 64 or 128 gigabytes of ram this is really just not feasible at all so we're going to do some data pre-processing here some data cleaning and then just a way to simply load data into the GPT so let's go ahead and do that so the model has actually gotten really good at predicting the next token as you can see the train loss here is 1.01 so let's actually find what the prediction accuracy of that is so I might just go into GPT-4 here and just ask it what is the prediction accuracy of loss 1.01 the loss value comes with a loss function during the pre-process okay so let's see cross entropy loss doesn't mean the model is 99% accurate okay so that pretty much means that the model is really accurate but I want to find a value here so if the we'll go to Wolfram alpha and just we'll just guess some values here so negative ln of let's say 0.9 okay so probably not that 0.3 0.2 0.4 0.35 yep so the model has about a 35% chance of guessing the next token as of right now so that's actually pretty good so 1 in every 3 tokens are spot on so that is wonderful this is converging even more we're getting 0.89 so now it's getting like every 40% are being guessed properly our validation is not doing amazing though but we'll linger on that a little bit here and you'll see sort of how this changes as we scale our data but so I've installed this webtext.tar file tar file is interesting so in order to actually extract these you simply just right click on them you go extract to and then it'll just make a new file here so it'll process this you have to make sure you have WinRAR or else this might not work to the fullest extent and yeah so we'll just wait for this to finish up here we should end up with something that looks like this so open webtext and inside of here you have a bunch of xz files cool so there's actually 20,000 of these so we're gonna have to do a lot of there's definitely gonna be some for loops in here for sure so let's just handle this step by step in this data extract file so first off we're gonna need to import some python modules we're gonna use OS for interacting with the operating system LZMA for handling xz files which are a type of compressed file like 7zip for example and then TQDM for displaying a progress bar so you see a progress bar left to right in the terminal and that's pretty much gonna show us how quick we are at executing the script so next up we're gonna define a function called xz files in dir it takes a directory as an input returns a list of all of the xz file names in that directory it's gonna use os.listdir to get all the file names and os path as file to check if each one is a file and not a directory or symbolic link if a file name ends with .xz and it's a file it'll be added to the list so we just have a bunch of these files each element is just the title of each file in there so that's pretty much what that does and next up here we'll set up some variables folder path it's just gonna be where our xz files are located so I'm actually gonna change this here because that's an incorrect file path but yes just like that you have to make sure that these slashes are actually forward slashes or else you might get bytecode errors so when it actually tries to read the string it doesn't think that these are separated the backward slashes do weird things so you could either do a one forward slash or two backward slashes that should work just make sure you get forward slashes and you should be good so folder path is where all these files are located all these xz files are located as you saw output file is the pattern for output file names in case we want to have more than one of them so if you want to have 200 output files instead of one then it'll just be like output 0, output 1, output 2 etc and then vocab file is where we want to save our vocabulary keep in mind in this giant corpus you can't push it on to ram at once so what we're gonna do is as we're reading these little compressed files 20,000 of them we're gonna take all of the new characters from them and just push them into some vocab file containing all of the different characters that we have so that way we can handle this later and just pretty much sort it into some list containing all of our vocabulary split files how many files do we want to split this into so pretty much this it ties back to output file and just these curly braces here how many do we want to have if we want to have more than one then we would this would take effect so cool now we'll use our x files in dir to get a list of file names and store them in this variable we'll count the number of total xd files simply the length of our file names now in here we'll calculate the number of files to process for each output file if the user is requested more than one output file request more than one output file this is the total number of files divided by the number output files rounded down so if the user only wants one output file max count is the same as total files and that's how that works so next up we'll just create a set to store a vocabulary when we start appending these new characters into it a set is a collection of unique items in case you did not know entirely what a set was now this is where it gets interesting we're ready to process our .xz files for each output file we'll process max count files for each file we'll open it read its contents and write the contents to the current output file and then add any unique characters to our vocabulary set after processing max count files remove them from our list of files and then finally we'll write all of our vocabulary to this file so we pretty much just open we just write all of these characters in the vocab to this vocab file which is here vocab.txt so awesome now honestly we could just go ahead and run this so let's go ahead and go in here I'm going to go cls to clear that we'll go python data extract .py let's see this work it's magic how many files would you like to split this into we'll go one then we get a progress bar 20,000 files and we'll just let that load I'll come back to you in about 30 minutes to check up on this okay so there's not a little one thing we want to consider for and it's actually quite important is our splits for train and file splits it would be really inefficient to just get blocks and then creating train and file splits as we go every new batch we get so in turn what we might be better off doing is just creating an output train file and an output file file so just two of them instead of one train is 90% of our data file is 10% of our data if that makes sense so pretty much what I did is I got the output line for how many files do you want so you can see I got quite a bit of files produced here by not doing that correctly so don't do that and yeah essentially we're just we're pretty much just doing that so we're processing some training files we're separating 90% of the names on the left side and then 10% of the names we're just separating those in the two different arrays, file names and then we're just processing each of those arrays based on the file names so I took away that little bit that was asking how many files per split do you want so I took that away and this is effectively the same code just a little bit of tweaks and yeah so I'm going to go ahead and run this data extract cool so we got an output train and then after this it's going to do the output validation set so I'll come back after this is done so awesome I have just downloaded both or I've both got both these splits output train and val train so just to confirm that they're actually the right size got 38.9 and then 4.27 so if we do this divided by 9 so about 30 8.9 divided by 9 4.32 and it's pretty close to 4.27 so we can confirm that these are pretty much the length that we expect them to be so awesome we have this vocab.txt file wonderful so now we have to focus on is getting this into our batches so when we call our get batch function actually cd out of this open this in a Jupyter notebook copy my desktop paste it over here and perfect so this open when web text folder with these files awesome and our GPTV one so this get batch function is going to have to change also these are going to have to change as well and this one too these are probably not going to be here but pretty much let's go ahead and first of all get this vocab.txt in so what I'm going to do I'm just going to go we're going to go open web text slash vocab.txt cool so that's our vocab right there text read vocab size the length of that nice so that's what our vocab is and then what we're going to do next is change this get batch function around so first of all I'm going to go ahead and get rid of this here and then I've actually produced some code specifically for this so I'm just going to go back to my I'm just going to find this folder okay so I've actually produced some code here I produced this off camera but pretty much what this is going to do it's going to let us call a split okay so we have our get batch function all of this down here is the same as our GPTV one file and then this data is just going to get a random chunk of text with giant block of text and the way that we get it is actually pretty interesting so the way that we get this text is something called memory mapping so memory mapping is a way to look at disk files or to open them and look at pieces of them without opening the entire thing at once so memory mapping I'm not a hardware guy so I can't really talk about that memory mapping is pretty cool and allows us to look at little chunks at a time in very large text files so that's essentially what we're doing here we're passing this split split file name is equal to train split this is just an example text file if the split is equal to train then this is our file name else file split and then we're going to open this file name in binary mode this has to be in binary mode it's also a lot more efficient in binary mode and then we're going to open this with a mem map so I don't expect you to memorize all the mem map syntax you can look at the docs if you would like but I'm just going to explain logically what's happening so we're going to open this with the mem map library and we're going to open this as mm so the file size is literally just the length of it so determining the file size and all we're doing from this point is we're just finding a position so we're using the random library and we're finding a position between 0 and the file size minus block size times batch size so pretty much we have this giant text file we could either what we want to do is we want to start from 0 and go up to like just before the end because if we actually sample that last piece then it's still going to have some wiggle room to reach further into the file if we just made it from like the first the very start of the file to the very end then it would want to do is it would want to look past the end because it would want to look at more tokens from that and then we would just get errors because you can't read more than the file size if that makes sense so that's why I'm just making this little threshold here and yeah so that's what that does that's the starting position could be a random number between the start and a little bit a little margin from the end here so next up we have this seek function so seek is going to go to the start position and then block is going to read we're going to go up to the start position it's going to seek up to there that's where it's going to start it's going to go up to it and then the read function is going to find a block of text that is block size times batch size so it's going to find a little snippet of text in there at the starting position and it's going to be of size it's going to have this the same amount of I guess bytes as block size time times batch size then all that minus one just so that it fits into this start position we don't get errors here that's why I put the minus one but yeah so we'll get a pretty we'll get a pretty decent text amount I guess you could say it's going to be enough to work with you could you could of course increases if you wanted to you could do like times eight if you wanted times eight and then times eight up here but we're not going to do that based on my experience this is performed pretty well so we're going to stick with this method here and then we just decode this bit of text the reason we decode it is it's it's because it's we read it in binary form so once we have this block of text we actually have to decode this to UFA format or UTF format and then any like bytecode errors we get we're just going to ignore that this is something you learn through practice is when you start dealing with like really weird data or if it has like corruptions in it you'll get errors so all you want to do is all this does is it pretty much says okay we're just going to ignore this bit of text and we're just going to sample everything around it and not include that part and plus since we're doing so many iterations it won't actually interfere that much so we should be all right and then for this replace go function here I was noticing I got errors about this slash R so all this does is it just replaces that with an empty string and then finally we have all this we have all this decoded data so all we're going to do is just encode this into the tokenized form so it's all in it's all in the tokenized form integers or torch.longs data type and we just that's what our data is instead of a bunch of characters it's just a bunch of numbers and then we return that into our get batch and this is what our data is so that's pretty cool we can get either train or a valve split and that's sort of what it looks like in practice that's how we sample from very large text files at a smaller scale bit by bit so let's go ahead and implement this here and go grab this entire thing and pop over to here we're just going to replace that so get random chunk, get batch cool so now we can actually go ahead and perhaps run this actually before we run this there's a little something we need to add in here so I have this train split.txt and a valve split.txt so I actually need to change these so let's go rename we'll go train split.txt and then a valve split.txt cool and then we could just go open web text forward slash and then same thing for here cool let's go ahead and run this now and we're getting errors mem map is not defined so that's another thing we need to probably add in then so I'm actually just going to stop this process from running here we're going to go pip install mem map mem map is not defined we don't actually need to install this by default comes with the operating system so what we actually need to do is just close this gptv1 awesome everything is good nothing is broken so what I actually need to do up here is import this so I need to go import mem map just like that and should be good to start running this script name random is not defined again another importation we have to make import import random and we should start seeing some progress going here so once we see the first iteration I'm going to stop it come back at the last iteration and then we'll start adding some little bits and pieces onto our script here to make it better so we're already about 600 iterations in and you can see how the training loss is actually done really well so far it's gone from 10.5 drop all the way to 2.38 and we can actually see that we might be able to actually get a val loss that is lower than the train because keep in mind in train mode the dropout takes effect but in val in eval mode let me just scroll up to this here yes so model about eval what this does is it turns off the dropout so we don't lose any of the neurons and they're all sort of showing the same features and giving all the information that they're supposed to because they're all active but in train mode 20% of them are off so once you actually see in eval mode it does better that means that the network has started to form a sense of completeness in its learning so it's just adjusting things a little bit once it hits that point and we might see this happen momentarily but this is really good progress so far a loss of 1.8 is amazing so in the meantime I'm just going to add some little tweaks here and there to improve this script so I've actually stopped the iteration process but we've gotten to 700 steps and we can already see that val loss is becoming a less than train loss which is showing that the model is actually converging and doing very well so this architecture is amazing we've pretty much covered every architectural, math, pie torch part that this script has to offer the only thing I want to add actually a few things I want to add one of them being torch.load and torch.save so one thing that's going to be really important when you start to scale up your iterations is you don't just want to run a script that executes a training loop with an architecture and that's it. You won't have some way to store those learning parameters so that's what torch.load and torch.save does save some file right and you can pretty much you could put it into like a serialized format when you save it you take your initial architecture in our case it would actually be the GPT language model so you would save this because it contains everything all these other classes as well they're all inside of GPT language model would save that architecture and you essentially serialize it into some pickled file that would have the file extension .pkl so essentially instead of using torch we're just going to use a library called pickle because they're essentially the same thing pickle is a little bit easier to use or at least a little bit easier to understand there's less to it pickle will only work on one GPU so if you have like 8 GPUs at the same time you're going to want to learn a little bit more about hardware stuff and some PyTorch docs but pretty much if we want to save this after training what we're going to do is we're going to use a little library called pickle and this comes pre-installed with windows import pickle okay so what we want to do is implement this after the training loop after all these parameters have been updated and learned to the fullest extent so after this training loop we're simply going to open we're going to do with open and we could just go model 01 like that and then just that .pkl is the file extension for it and then since we're writing to it we're going to go write binary F and then in order to actually save this we just go pickle.dump and then we can use model and then just F like that so if I start recording this it's going to make if I start recording this training process it's going to make my clip like so I'm going to come back to this after we've done let's just say about 100 iterations we're going to do 100 editors and I'm going to come back and show you guys what the model file looks like what I actually did is I changed some of the model hyper parameters because it was taking way too long to perform what we wanted it to so I changed and head to one and layer to one and I had half batch size all the way down from 64 to 32 so what I'm actually going to add here is just to make sure I like to print this out at the beginning of this make sure that the device is CUDA let's go back down so it did in fact train the model so we got all this done and yeah so I don't know why I did 2.54 whatever that that was just the entire loss so model saved awesome what does this actually look like here so this model.pkl 106 megabytes isn't that wonderful so this is our model file this is what they look like it's just a serialized pretty much the entire architecture all the parameters of the model the state everything that it contains and we just compress that into a little pkl file take that out decompress it and then just use it again with all those same parameters so awesome and all this really took was we just open as this we do a pickle.dump to make sure that actually save I just like to add a little print statement there cool so next what I'd like to add is a little wait for us to instead of just doing all of our training at once and then saving the model being able to train multiple times so I'm gonna go up here to our GPT language model here and let's just see what I'm gonna do with open and we're gonna go model 01 pkl and we're gonna go read binary so actually gonna read it we're gonna load this into our script here so we're gonna go as f and then I believe it's pickle.load you just go yeah model equals pickle.load and then we'll just essentially dump that right in there go print loading model parameters dot dot dot and then just put f in there and then once it is loaded we'll do print loaded successfully cool so I'm actually gonna try this out now go do that boom and boom okay so loading model parameters loaded successfully and we'll actually see this start to work on its own now so is it going to begin or is it not going to begin let's run that okay perfect so now we should take the loss that we had before which was about 2.54 I believe something around those, something along those lines you can see that our training process is greatly accelerated so we had 100 now it's just gonna do an estimate loss cool and we're almost done 1.96 awesome and the model saved so essentially what we can do with this is we can now save models and then we can load them and then iterate further so if you wanted to you could create a super cool GPT language model script here and you could essentially give it like 10,000 or 20,000 iterations to run overnight you'd be able to save it and then import that into say a chat bot if you want so that's pretty cool and that's just kind of a good thing good little, it's kind of essential for language modeling because what's the point in having a machine learning model if you can't actually use it and deploy it so you need to save for this stuff to work alright now let's move on to a little something in this task manager here which I'd like to go over so this shared GPU memory here and this dedicated GPU memory so dedicated means how much VRAM, video RAM does your GPU actually have on the card so on the card it's going to be very quick memory because it doesn't have to the electrons don't have to travel as quickly that's kind of the logic of it the electrons don't have to travel they don't have to travel as far because the little RAM chip is right there so dedicated GPU memory is a lot faster shared GPU memory is essentially if this gets overloaded it'll use some of the RAM on your computer instead so this will typically be about half of your computer's RAM I have 32 gigabytes of RAM on my computer so 16.0 makes sense half 32 and yeah so you want to make sure you're only using dedicated GPU memory having your shared GPU memory go up is not usually a good thing a little bit is fine but dedicated GPU memory is the fastest and you want everything to stick on there just try to make sure all of your parameters sort of fit around this whatever your max capacity is maybe it's 4, maybe it's 8 maybe it's 48 who knows and a good way to figure out what you can use on your GPU without it getting memory errors or using shared memory is to actually play around with these parameters up here so block size and batch size actually let me switch those around these are not supposed to be in that order but all good we'll make our batch size 64 that's 128 okay so batch size and block size are very big contributors to how much memory you're going to use learning rate is not max iterations is not evaluators is not but these three will the amount of features that you store the amount of heads you have running in parallel and then also layers so some of these will not affect you as much because they're more sort of restrained to computation how quickly you can do operations if something is sequential so N layer won't strain you as much as something like batch and block size but those are just good little things to sort of tweak and play around with so I found the optimal sort of set of hyper parameters for my PC that happens to be 8, 8, 3, 8, 4 learning rates is the same and then 64, 128 for this so that happened to be the optimal hyper parameters for my computer it'll probably be different for yours if you don't have 8 gigabytes of RAM on your GPU so anyways that's a little something you have to pay attention to to make sure you don't run out of errors and a technique you can use which I'm not actually going to show you in this course but it's quite useful is something called auto tuning and what auto tuning does is it pretty much runs a bunch of these a bunch of models with different sets of hyper parameters so to run like batch size 64 batch size 32, batch size 16 batch size maybe 256 we'll be like okay which ones are throwing errors and which ones aren't so what it'll do if you properly if you properly set up an auto tuning script is you will be able to find the most optimal set of parameters for your computer most optimal set of hyper parameters that is possible so auto tuning is cool you can definitely look more into that there's tons of research on it and yeah so auto tuning is cool let's dig into the next part the next little trick we use in practice especially by machine learning engineers it's a little something called arguments so you pass an argument into not necessarily a function but into the command line so this is what it'll look like this is just a basic example of what arg parsing will look like so just go python, arg parsing because that's a script's name I go dash llms because that's what it says right here this is what the argument is and then we can just pass in a string say hello the provided whatever is hello cool you can add little arguments to this and I'm even going to change this around I could say batch size and then let's go like that batch batch size please provide a batch size I can do the same thing again and see it says following arguments required are batch size so that obviously didn't work and if we actually tried the correct way our parsing.py then we go dash, batch size we can make it 32 oops that's because it's not a string so what we need to actually do is it's bs somewhere okay so args parse args so we need to change this to bs like that let me go batch size batch size is 32 okay so even I'm a little bit new to arguments as well but this is something that comes in very handy when you're trying to know each time you're trying to change some parameters if you add new gpu or whatever and you're like oh I want to double my batch size it's like sure you can easily do that so a lot of the times it won't just have one but you'll have like many meaning like maybe a dozen or so of these of these little arguments so that is what this looks like and we're going to go ahead and implement this into our little script here so I'm just going to pop over to gpt1 I'm going to pull this up on my second monitor here and in terms of these I'm just going to start off making a importation arg arg parser or arg parse rather that's what it's called and then we go parser is equal to I'll just copy and paste this entire thing and why not cool okay so we get a batch size or something and then we'll add in the second part here so args parse the arguments here and the little scope of batch size like that our batch size is equal to whatever that was and we'll just go args dot args dot batch size so cool we're going to run this and not defined so I got a little not defined thing here and pretty much all I missed was that we're doing this so essentially this should be equal to this right here so I'm just going to go ahead and copy that and boot parse args except we don't have a parse args function so what do we need to do instead well it actually that might just work on it so let's try it out okay so it looks like it's actually expecting some input here in code so that's probably working and if we ported this into a script then it would simply ask us for some input so I believe we're doing this correctly let's go ahead and actually switch over and pour all of this into some code so I'm going to make a training file and a chat file the training file is going to be all of our parameters whatever all of our architecture and then the actual training loop itself we're going to have some arguments in there and then the chat bot is going to be pretty much just a question-answer thing that just reproduces text so it'll just be like prompt, completion type of thing and yeah so let's go ahead and implement that here so in our GPT course here I'm going to go training.py and we're going to go chatbot.py just like that so in training let's go ahead and drag everything in here I'm just going to move this over to the second screen and just copy and paste everything in order here so next up we have our characters and then we have our tokenizer and then our getRandomChunk and getBatches suite our estimateLoss function and then this giant piece of code containing most of the architecture we built up we're just going to add that in there we're not getting any warnings and then the training loop and the optimizer awesome then after this we would simply have this context but the point of this is that we want to have this in our chatbot script so what I'm going to do is in this training.py I'm going to keep all of these the same I'm going to keep this entire thing the same get rid of this little block of code and we're going to go into the chatbot here so loadingMile parameters good we want to load some in train some more and then dump it chatbot is not going to dump anything it's just going to save so I'm going to take all of our training here and instead of dumping take that away we'll also take away the training loop as well I don't believe we have anything else to actually bring in we don't need our getBatch we do not need our getRandomChunks so awesome we're just importing these parameters by default like that awesome so from this point we have imported we've imported our model cool so let's go ahead and port in our little chatbot here this little end piece which is going to allow us to essentially chat with the model this is what it looks like a little wild loop we have a prompt we just input something prompt next line that should be fairly self explanatory and we have this tensor we're going to encode this prompt into a bunch of integers or torch.long data types on the GPU devices CUDA and then after after we've actually generated these so model.generate we're going to unsqueeze these remember it's a torch.tensor so it's going to be in the matrices form so it's going to look like this it's going to look like this or whatever that's essentially what the shape is so all we're doing when we unsqueeze it is we're just taking away this wrapping around it so awesome we're just going to do some tokens for example 150 here and then to a list format and then we can just print these out as January characters awesome so we're just going to ask this prompt and then do some compute give us a completion so on and so forth so that's what this is doing here and another thing I wanted to point out is actually when we load these parameters in at least on training it's going to initially give us errors from we're going to get errors from that because the model will just not be anything and we won't be able to import stuff so that's going to give you errors first of all another thing you want to pay attention to is to make sure that when you've actually trained this initial model that it matches all of the architectural stuff and the hyper parameters that you used that when you're using to load up again so when you're running your forward pass and whatnot you just want to make sure that this architecture sort of lines up with it just so that you don't get any architectural errors those can be really confusing to debug so yeah and the way we can do this is actually just commenting it out here awesome, we're able to save load models and we're able to use a little loop to create a sort of chat-up that's not really helpful because we haven't trained it an insane amount on data that actually is useful so another little detail that's very important is to actually make sure that you have nn-module in all of these classes and subclasses nn.module basically works as a tracker for all of your parameters it makes make sure that all of your nn extensions run correctly and just overall a cornerstone for PyTorch like you need it so make sure you have nn-module in all of these classes I know that block sort of comes out of GPT language model and so on and so forth but just all of these classes with nn or any learnable parameters you will need it in it's overall just a good practice to have nn-module in all of your classes overall just to sort of avoid those errors so cool I didn't explicitly go over that at the beginning but that's just a heads up you always want to make sure nn-module is inside of these so cool now something I'd like to highlight is a little error that we get we try to generate when we have max new tokens above block size so let me show you that right now you just go python, chat bot and then batch size 32 so we could say we could say hello for example okay so it's going to give us some errors here and what exactly does this error mean well when we try to generate 150 new tokens what it's doing is it's taking the previous you know H-E-L-L-O exclamation mark 6 tokens and it's pretty much adding up 150 on top of that so we have 156 tokens that we're now trying to fit inside of block size which in our case is 128 so of course 156 does not fit into 128 and that's why we get some errors here so all we have to do is make sure that we essentially what we could do is make sure that max new tokens is small enough and then be sort of paying attention when we make prompts or we could actually make a little cropping cropping tool here so what this will do is it will pretty much crop through the last block size tokens and this is super useful because it pretty much doesn't make us have to pay attention to max new tokens all the time and it just essentially crops it around that 128 limit so I'm going to go ahead and replace index here with index con or index condition and we go ahead and run this again so I could say hello and we get a successful completion awesome we can keep asking new prompts like this right and awesome so yeah we're not really getting any of these dimensionality like architecture fitting type errors if you want to call them if you want to make it super fancy that way but yeah not really that much else to do yeah there's a few points I want to go over including fine tuning so I'm going to go over a little illustrative example as to what fine tuning actually looks like in practice so in pre-training which is what this course is based off of in pre-training you have this giant text corpus right you have this giant corpus here some text in it and essentially what you do is you take out little snippets these are called blocks or batches or chunks you could say you take out little batches of these you sample random little blocks and you take multiple batches of them and you essentially have this let's just say H E L L O and maybe the next predict maybe the outputs or the targets rather or the L L O exclamation mark so it's just shifted over by one and so given this sequence of characters you want to predict this which is just the input shifted by one that's what pre-training is and keep in mind that these are the same size this is one, two, three, four, and five same thing here these are both five characters long fine tuning however is not completely the same so I could have hello and then maybe like a question mark and it would respond you know the model might respond L R U maybe that's just a a response that it gives us we can obviously see that hello does not have the same amount of characters with the same amount of indices as how are you so this is essentially the difference between fine tuning and pre-training with fine tuning you just have to add a little bit of different things in your generate function to compensate for not having the same amount of indices in your inputs and targets and rather just generate until you receive an end token so what they don't explicitly say here is at the end of this question there's actually a little end token which we usually do looks like this like that or like this these are end tokens and then you typically have the same for start tokens like an s or start like that, pretty simple and essentially you would just append them and a start token the start token doesn't matter as much as we essentially just are looking at what this does and then we start generating the start doesn't really matter because we don't really need to know when to start generating it just happens but the end token is important because we don't want to just generate an infinite number of tokens because these aren't the same size it could theoretically generate a really really long completion so all we want to make sure is that it's not generating an infinite amount of tokens consuming an infinite amount of computation and just to prevent that loop so that's why we append this end token to the end here we have this little end bit and essentially once this end token is sampled you would end the generation simple as that and we don't actually sample from the token itself but rather the actual the I guess you could say index or the miracle value the encoded version of end which is usually just going to be the length of your vocab size plus one so if your vocab size in our case is like maybe 32,000 your end token would be at index 32,001 so that way when you sample when you sample an end token when you sample that 32,001 token you actually just end the sequence and of course when you train your model you're always appending this end token to the end so you get your initial inputs and then inside of either your training data or when you actually are processing it and feeding it into that transformer you have some sort of function that's just appending that little 32,001 token index to it so that's pretty much what fine tuning is it comes up fine tuning and the whole process of creating these giant language models is to of course help people and there's no better way to do that than to literally have all the information that humans have ever known meaning like common crawl open web text or Wikipedia and even research papers pre-training on all of that so just doing again the same size and then shift over for targets and then after you've iterated on that many many times you switch over to fine tuning where you have these specifically picked out prompt and completion pairs and you just train on those for a really long time until you are satisfied with your result and yeah that's what language modeling is there are a few key pointers I want to leave you with before you head on your way to research and development and machine learning so first things first there's a little something called efficiency testing or just finding out how quickly certain operations takes we'll just call this efficiency testing and I'll show you exactly how to do this right here efficiency yeah I don't know if I spelled that correctly I don't know what it's doing now anyways we'll just pop into code here and essentially we'll just do I don't know I'm testing import time and essentially all we're going to do is just time how long operations take so in here you can go you can go start time equals time dot time and essentially what this function does is it just takes a look at the current time right now the current like millisecond very precise and we can do some little operation like I don't know 4 I in range we'll just go 10,000 go print I print I times 2 and then we can just end the time here so go end time equals time dot time again calling the current time so we're doing right now versus back then and that little difference is how long it took to execute so all we can do is just do we can say total time we can say total time equals end time minus start time and we'll just go print end time or I'm taking let's go total total time like that just execute this Python time testing cool time taken 1.32 seconds so you can essentially time every single operation you do with this method and you can see even in your I encourage you to actually try this out I'm not going to but I encourage you to try out how long the model actually takes to do certain things like how long does it take to load a model how does it take to save a model how long does it take to estimate the loss right play around with hyperparameters see how long things take and maybe you'll figure out something new who knows but this is a little something we use to pretty much test how long something takes how efficient it is and then to also see if it's worth investigating a new way of approaching something in case it takes ridiculous amount of time so that's time testing and efficiency testing for you the next little bit I want to cover is the history I'm not going to go over the entire history of AI and LLMs but essentially we originated with something called RNNs okay RNNs are called recurrent neural networks and they're really inefficient at least for scaled AI systems so RNNs are a little essentially think of it as a little loop keeps learning and learning and this is sequential right it does this and then this and then this has to wait for each completion synchronous you can't have multiple of them at once because they're complex GPUs cannot run complex things they're only designed for just matrix multiplication and very simple math like that so RNNs are essentially a little bit dumber than transformers and they are run on the CPU so RNNs was where we last sort of stopped at and what I encourage you to do is look into more of the language modeling and AI history and research that has led up to this point so you can have an idea as to how researchers have been able to quickly innovate given all these historical innovations so you have like all these things leading up to the transformer well how did they all philosophize up to that point and yeah it's just something good to sort of be confident in is innovating as both a researcher and engineer and a business person so cool RNNs were where we sort of finished off and now it's transformers and GPTs that's the current state of AI next up I would like to go over something called quantization so quantization is essentially a way to reduce the memory usage by your parameters so there's actually a paper here called QLaura Efficient Fine Tuning of Quantized LLMs so all this does in simple form is pretty much instead of using 32 bit floating point numbers it goes not only to 16 bit of half precision but all the way down to 4 so what this actually looks like is in binary code or in bytecode it will look here there's some array of numbers that it uses okay I can't find it but pretty much what it is it is a bunch of it's a bunch of floating point numbers and they're all between negative one and one and there are 16 of them if you have a 4 bit number that means it can hold 16 different values 0 through 15 which is 16 values and all you pretty much do is you have this array of floating point numbers you use the bytecode of that 4 bit number to look up the index in that array and that is your weight that is the weight they use in your model so this way instead of using 32 bit having these super long numbers that are super precise you can have super precise numbers that are just generally good parameters to have that just perform decently they're just sort of well spread out and experimented on and they just happen to work and you have 16 of them instead of a lot so that's another cool little thing that's going on right now is 4 bit quantizations it's a little bit harder to implement I would encourage you to experiment with half precision meaning 16 bit floating point numbers so that means it occupies 16 on and off switches or capacitors on your GPU and so quantization is cool to sort of scale down the memory so that way you can scale up all of your hyper parameters and have a more complex model with these yeah just essentially to have bigger models with less space take it up so that is quantization and this is the paper for it it's a little link you can search out if you want to get more familiar with this see sort of performance standards and what not the next thing I'd like to cover is gradient accumulation so you might have heard of this you might not have heard of this gradient accumulation will what gradient accumulation does is it will accumulate gradients over say we just set a variable x so every x iterations it'll just accumulate those iterations, average them and what this allows you to do is instead of updating each iteration you're updating every x iterations so that allows you to fit more parameters and more info or generalization into this one piece so that way when you update your parameters it's able to generalize more over maybe a higher batch size or a higher block size so when you distribute this over many iterations and average them you can fit more into each iteration because it's sort of calculating all of them combined so yeah that's a cool little trick you can use if your GPU maybe isn't as big if it doesn't have as much VRAM on it so gradient accumulation is wonderful and it's used lots in practice the final thing I'd like to leave you guys off with is something called hugging face and you've probably heard a lot about this so far but let me just guide you through and show you how absolutely explosive hugging face is for machine learning so you have a bunch of models, data sets spaces, docs, etc and let's go to models for example so let's just showcase how cool this is you have multimodal AIs which could be like image and text or video etc you have multiple different modes so it's not just text or not just video it's many different ones at the same time so you have multimodal models you have computer vision you have natural language processing and we're actually doing natural language processing in this course we have audio, a tabular and reinforcement learning so this is really cool and you can actually just download these models and host them on your own computer that is really cool you also have data sets which are even cooler and these are pretty much just really high quality data sets of prompt and answer completions at least for our purpose if you want to use those so you have question answering or conversational work data set for example as 9000 downloads 500 likes it has a bunch of IDs, system prompts so you're an AI assistant or whatever and then you have the cool stuff which is you'll be given a definition of a task first and some input of the task etc and then the response it's like oh we just gave it an input and asked it to answer in a format and actually did that correctly so you could pretty much train these on a bunch of prompts that you would be able to feed into GPT-4 and try to make your model perform that way and this actually has 4.23 million rows in the training split which is amazing so data sets are wonderful and you can find the best ones at least the best fine tuning data sets on OpenORCA really good as for pre-training I believe I mentioned this earlier in this survey of large language models that we just put down through Reddit links yep so you could use like OpenWebText you could use CommonCrawl you could use Books you could use Wikipedia these are all pre-training data sources so yeah hopefully that leaves you with a better understanding on how to create GPTs, transformers and pretty good large language models from scratch with your own data that you scraped or that you downloaded and yeah that's it, thanks for watching so you've learned a ton in this course about language modeling how to use data, how to create architecture from scratch maybe even how to look at research papers so if you really enjoy this content I would encourage you to maybe subscribe and like on my YouTube channel which is in the description I make many videos about AI and computer science in general so you could totally feel free to subscribe there if you don't want to subscribe, that's fine you could always unsubscribe later if you want to it's completely free but yeah, also have a GitHub repo in the description for all the code that we used not the data because it's way too big but all of the code and the Wizard of Oz Text file so that's all in the GitHub repo in the description thanks for watching", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Learn how to build your own large language model from scratch.", "tokens": [50364, 17216, 577, 281, 1322, 428, 1065, 2416, 2856, 2316, 490, 8459, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 1, "seek": 0, "start": 4.36, "end": 9.84, "text": " This course goes into the data handling, math and transformers behind large language models.", "tokens": [50582, 639, 1164, 1709, 666, 264, 1412, 13175, 11, 5221, 293, 4088, 433, 2261, 2416, 2856, 5245, 13, 50856], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 2, "seek": 0, "start": 9.96, "end": 11.8, "text": " Elliot Arledge created this course.", "tokens": [50862, 38986, 1587, 3042, 2942, 341, 1164, 13, 50954], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 3, "seek": 0, "start": 12.040000000000001, "end": 18.64, "text": " He will help you gain a deep understanding of how LLMs work and how they can be used in various applications.", "tokens": [50966, 634, 486, 854, 291, 6052, 257, 2452, 3701, 295, 577, 441, 43, 26386, 589, 293, 577, 436, 393, 312, 1143, 294, 3683, 5821, 13, 51296], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 4, "seek": 0, "start": 19.04, "end": 20.6, "text": " So let's get started.", "tokens": [51316, 407, 718, 311, 483, 1409, 13, 51394], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 5, "seek": 0, "start": 21.0, "end": 22.88, "text": " Welcome to Intro to Language Modeling.", "tokens": [51414, 4027, 281, 47406, 281, 24445, 6583, 11031, 13, 51508], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 6, "seek": 0, "start": 22.92, "end": 25.28, "text": " In this course, you're going to learn a lot of crazy stuff.", "tokens": [51510, 682, 341, 1164, 11, 291, 434, 516, 281, 1466, 257, 688, 295, 3219, 1507, 13, 51628], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 7, "seek": 0, "start": 25.32, "end": 27.080000000000002, "text": " Okay, I'm just going to give you a heads up.", "tokens": [51630, 1033, 11, 286, 478, 445, 516, 281, 976, 291, 257, 8050, 493, 13, 51718], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 8, "seek": 0, "start": 27.12, "end": 28.96, "text": " It's going to be a lot of crazy stuff we learn here.", "tokens": [51720, 467, 311, 516, 281, 312, 257, 688, 295, 3219, 1507, 321, 1466, 510, 13, 51812], "temperature": 0.0, "avg_logprob": -0.21639049754423254, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0275562796741724}, {"id": 9, "seek": 2896, "start": 29.240000000000002, "end": 31.400000000000002, "text": " However, it will not be insanely hard.", "tokens": [50378, 2908, 11, 309, 486, 406, 312, 40965, 1152, 13, 50486], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 10, "seek": 2896, "start": 31.64, "end": 35.72, "text": " I don't expect you have any any experience in calculus or linear algebra.", "tokens": [50498, 286, 500, 380, 2066, 291, 362, 604, 604, 1752, 294, 33400, 420, 8213, 21989, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 11, "seek": 2896, "start": 36.64, "end": 39.04, "text": " A lot of courses out there do assume that, but I will not.", "tokens": [50748, 316, 688, 295, 7712, 484, 456, 360, 6552, 300, 11, 457, 286, 486, 406, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 12, "seek": 2896, "start": 39.92, "end": 41.2, "text": " We're going to build up from square one.", "tokens": [50912, 492, 434, 516, 281, 1322, 493, 490, 3732, 472, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 13, "seek": 2896, "start": 41.32, "end": 46.24, "text": " We're going to take baby steps when it comes to new fundamental concepts in math and machine learning.", "tokens": [50982, 492, 434, 516, 281, 747, 3186, 4439, 562, 309, 1487, 281, 777, 8088, 10392, 294, 5221, 293, 3479, 2539, 13, 51228], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 14, "seek": 2896, "start": 46.6, "end": 51.24, "text": " And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out.", "tokens": [51246, 400, 321, 434, 516, 281, 747, 257, 4833, 4439, 1564, 721, 366, 6457, 1850, 293, 436, 434, 1333, 295, 1858, 281, 2573, 484, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 15, "seek": 2896, "start": 51.92, "end": 55.52, "text": " That way we don't take forever just taking baby steps through every little concept.", "tokens": [51512, 663, 636, 321, 500, 380, 747, 5680, 445, 1940, 3186, 4439, 807, 633, 707, 3410, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 16, "seek": 2896, "start": 56.0, "end": 58.56, "text": " This course is inspired by Andre Karpathy's.", "tokens": [51716, 639, 1164, 307, 7547, 538, 20667, 591, 6529, 9527, 311, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1596379384507228, "compression_ratio": 1.725, "no_speech_prob": 0.006482828874140978}, {"id": 17, "seek": 5896, "start": 59.4, "end": 61.32, "text": " Building a GPT from scratch lecture.", "tokens": [50386, 18974, 257, 26039, 51, 490, 8459, 7991, 13, 50482], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 18, "seek": 5896, "start": 62.24, "end": 63.24, "text": " So shout out to him.", "tokens": [50528, 407, 8043, 484, 281, 796, 13, 50578], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 19, "seek": 5896, "start": 63.88, "end": 69.68, "text": " And yeah, we don't assume you have any experience, maybe three months of Python experience.", "tokens": [50610, 400, 1338, 11, 321, 500, 380, 6552, 291, 362, 604, 1752, 11, 1310, 1045, 2493, 295, 15329, 1752, 13, 50900], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 20, "seek": 5896, "start": 70.48, "end": 73.0, "text": " Just so the syntax is sort of familiar and you can.", "tokens": [50940, 1449, 370, 264, 28431, 307, 1333, 295, 4963, 293, 291, 393, 13, 51066], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 21, "seek": 5896, "start": 74.08, "end": 79.12, "text": " You're able to follow along that way, but no matter how smart you are, how quick you learn.", "tokens": [51120, 509, 434, 1075, 281, 1524, 2051, 300, 636, 11, 457, 572, 1871, 577, 4069, 291, 366, 11, 577, 1702, 291, 1466, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 22, "seek": 5896, "start": 80.08, "end": 85.24000000000001, "text": " The willingness to put in the hours is the most important because this is material that you won't normally come across.", "tokens": [51420, 440, 25069, 281, 829, 294, 264, 2496, 307, 264, 881, 1021, 570, 341, 307, 2527, 300, 291, 1582, 380, 5646, 808, 2108, 13, 51678], "temperature": 0.0, "avg_logprob": -0.1847672462463379, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.002630633534863591}, {"id": 23, "seek": 8524, "start": 86.19999999999999, "end": 95.8, "text": " So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water.", "tokens": [50412, 407, 382, 938, 382, 291, 434, 1075, 281, 829, 294, 300, 5754, 4630, 11, 2944, 807, 613, 16564, 11, 754, 498, 309, 311, 1152, 11, 747, 257, 1702, 1821, 11, 4444, 257, 13288, 11, 2035, 291, 643, 281, 360, 11, 4444, 512, 1281, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14895503634498233, "compression_ratio": 1.578740157480315, "no_speech_prob": 0.0011512440396472812}, {"id": 24, "seek": 8524, "start": 96.24, "end": 97.28, "text": " Water is very important.", "tokens": [50914, 8772, 307, 588, 1021, 13, 50966], "temperature": 0.0, "avg_logprob": -0.14895503634498233, "compression_ratio": 1.578740157480315, "no_speech_prob": 0.0011512440396472812}, {"id": 25, "seek": 8524, "start": 97.75999999999999, "end": 101.44, "text": " And yeah, hopefully you can make it to the end of this.", "tokens": [50990, 400, 1338, 11, 4696, 291, 393, 652, 309, 281, 264, 917, 295, 341, 13, 51174], "temperature": 0.0, "avg_logprob": -0.14895503634498233, "compression_ratio": 1.578740157480315, "no_speech_prob": 0.0011512440396472812}, {"id": 26, "seek": 8524, "start": 102.39999999999999, "end": 103.03999999999999, "text": " You can do it.", "tokens": [51222, 509, 393, 360, 309, 13, 51254], "temperature": 0.0, "avg_logprob": -0.14895503634498233, "compression_ratio": 1.578740157480315, "no_speech_prob": 0.0011512440396472812}, {"id": 27, "seek": 8524, "start": 104.28, "end": 111.64, "text": " Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing.", "tokens": [51316, 4162, 309, 311, 1737, 3089, 2255, 11, 1203, 486, 312, 2654, 24903, 11, 1825, 294, 264, 15355, 295, 4835, 1412, 6352, 420, 4588, 15866, 13, 51684], "temperature": 0.0, "avg_logprob": -0.14895503634498233, "compression_ratio": 1.578740157480315, "no_speech_prob": 0.0011512440396472812}, {"id": 28, "seek": 11164, "start": 112.28, "end": 117.6, "text": " We'll be scaling the data to about 45 gigabytes for the entire training data set.", "tokens": [50396, 492, 603, 312, 21589, 264, 1412, 281, 466, 6905, 42741, 337, 264, 2302, 3097, 1412, 992, 13, 50662], "temperature": 0.0, "avg_logprob": -0.2456550807743282, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0098580252379179}, {"id": 29, "seek": 11164, "start": 118.16, "end": 123.56, "text": " So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45.", "tokens": [50690, 407, 362, 4289, 24819, 370, 321, 393, 5484, 264, 5883, 6905, 293, 550, 7620, 309, 281, 364, 3571, 281, 589, 365, 6905, 13, 50960], "temperature": 0.0, "avg_logprob": -0.2456550807743282, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0098580252379179}, {"id": 30, "seek": 11164, "start": 124.44, "end": 129.24, "text": " So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine.", "tokens": [51004, 407, 1338, 11, 498, 291, 500, 380, 767, 362, 4289, 42741, 24819, 11, 300, 311, 3879, 2489, 13, 51244], "temperature": 0.0, "avg_logprob": -0.2456550807743282, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0098580252379179}, {"id": 31, "seek": 11164, "start": 129.24, "end": 136.64, "text": " You can just download a different data set and sort of follow the same data pipeline that I do in this video.", "tokens": [51244, 509, 393, 445, 5484, 257, 819, 1412, 992, 293, 1333, 295, 1524, 264, 912, 1412, 15517, 300, 286, 360, 294, 341, 960, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2456550807743282, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0098580252379179}, {"id": 32, "seek": 13664, "start": 137.64, "end": 141.04, "text": " Through the course, you may see me switch between Mac OS and Windows.", "tokens": [50414, 8927, 264, 1164, 11, 291, 815, 536, 385, 3679, 1296, 5707, 12731, 293, 8591, 13, 50584], "temperature": 0.0, "avg_logprob": -0.25596919812654195, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.053380291908979416}, {"id": 33, "seek": 13664, "start": 141.44, "end": 146.64, "text": " The code still works all the same, both operating systems, and I'll be using a tool called SSH.", "tokens": [50604, 440, 3089, 920, 1985, 439, 264, 912, 11, 1293, 7447, 3652, 11, 293, 286, 603, 312, 1228, 257, 2290, 1219, 12238, 39, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25596919812654195, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.053380291908979416}, {"id": 34, "seek": 13664, "start": 146.64, "end": 162.64, "text": " It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook.", "tokens": [50864, 467, 311, 257, 7154, 300, 286, 393, 1745, 490, 452, 31737, 281, 452, 8591, 6465, 300, 286, 478, 6613, 322, 558, 586, 11, 293, 300, 486, 2089, 385, 281, 14483, 11, 1190, 11, 1322, 11, 2035, 11, 360, 1340, 17720, 4077, 11, 5622, 12391, 4077, 322, 452, 31737, 13, 51664], "temperature": 0.0, "avg_logprob": -0.25596919812654195, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.053380291908979416}, {"id": 35, "seek": 16264, "start": 163.64, "end": 169.64, "text": " So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording.", "tokens": [50414, 407, 286, 603, 312, 1075, 281, 360, 1203, 322, 456, 300, 286, 393, 452, 8591, 3820, 11, 309, 603, 445, 574, 257, 707, 857, 819, 337, 264, 6613, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1041169666108631, "compression_ratio": 1.597902097902098, "no_speech_prob": 0.01971394754946232}, {"id": 36, "seek": 16264, "start": 170.64, "end": 173.64, "text": " So why am I creating this course?", "tokens": [50764, 407, 983, 669, 286, 4084, 341, 1164, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1041169666108631, "compression_ratio": 1.597902097902098, "no_speech_prob": 0.01971394754946232}, {"id": 37, "seek": 16264, "start": 173.64, "end": 183.64, "text": " Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space.", "tokens": [50914, 1042, 11, 411, 286, 848, 949, 11, 257, 688, 295, 26992, 11, 436, 500, 380, 362, 264, 8088, 3601, 411, 33400, 8213, 21989, 281, 854, 552, 483, 1409, 420, 21341, 641, 2539, 294, 341, 1901, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1041169666108631, "compression_ratio": 1.597902097902098, "no_speech_prob": 0.01971394754946232}, {"id": 38, "seek": 16264, "start": 183.64, "end": 189.64, "text": " So I intend to build up from baby steps and then larger steps when things are fairly simple to work with.", "tokens": [51414, 407, 286, 19759, 281, 1322, 493, 490, 3186, 4439, 293, 550, 4833, 4439, 562, 721, 366, 6457, 2199, 281, 589, 365, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1041169666108631, "compression_ratio": 1.597902097902098, "no_speech_prob": 0.01971394754946232}, {"id": 39, "seek": 18964, "start": 190.64, "end": 199.64, "text": " And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you.", "tokens": [50414, 400, 286, 603, 764, 9952, 16660, 530, 293, 1823, 538, 1823, 5110, 281, 854, 3410, 24106, 1125, 2831, 813, 445, 3507, 9131, 295, 8513, 68, 412, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09482811465121732, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.020017065107822418}, {"id": 40, "seek": 18964, "start": 199.64, "end": 204.64, "text": " So with that being said, let's go ahead and jump in to the good stuff.", "tokens": [50864, 407, 365, 300, 885, 848, 11, 718, 311, 352, 2286, 293, 3012, 294, 281, 264, 665, 1507, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09482811465121732, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.020017065107822418}, {"id": 41, "seek": 18964, "start": 204.64, "end": 209.64, "text": " So in order to develop this project step by step, we're going to use something called Jupyter notebooks.", "tokens": [51114, 407, 294, 1668, 281, 1499, 341, 1716, 1823, 538, 1823, 11, 321, 434, 516, 281, 764, 746, 1219, 22125, 88, 391, 43782, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09482811465121732, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.020017065107822418}, {"id": 42, "seek": 18964, "start": 209.64, "end": 214.64, "text": " And you can sort of play with these in the Anaconda prompt or at least launch them from here.", "tokens": [51364, 400, 291, 393, 1333, 295, 862, 365, 613, 294, 264, 1107, 326, 12233, 12391, 420, 412, 1935, 4025, 552, 490, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09482811465121732, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.020017065107822418}, {"id": 43, "seek": 21464, "start": 214.64, "end": 218.64, "text": " So Anaconda prompt is just great for anything machine learning related.", "tokens": [50364, 407, 1107, 326, 12233, 12391, 307, 445, 869, 337, 1340, 3479, 2539, 4077, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0776061951359616, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.09006202220916748}, {"id": 44, "seek": 21464, "start": 218.64, "end": 220.64, "text": " So make sure to have this installed.", "tokens": [50564, 407, 652, 988, 281, 362, 341, 8899, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0776061951359616, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.09006202220916748}, {"id": 45, "seek": 21464, "start": 220.64, "end": 227.64, "text": " I will link a video in the description so that you can sort of set this up and install it step by step guide in there.", "tokens": [50664, 286, 486, 2113, 257, 960, 294, 264, 3855, 370, 300, 291, 393, 1333, 295, 992, 341, 493, 293, 3625, 309, 1823, 538, 1823, 5934, 294, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0776061951359616, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.09006202220916748}, {"id": 46, "seek": 21464, "start": 227.64, "end": 233.64, "text": " So we can do from this point is sort of just set up our project and initialize everything.", "tokens": [51014, 407, 321, 393, 360, 490, 341, 935, 307, 1333, 295, 445, 992, 493, 527, 1716, 293, 5883, 1125, 1203, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0776061951359616, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.09006202220916748}, {"id": 47, "seek": 23364, "start": 233.64, "end": 243.64, "text": " So I'm going to do is just head over into my directory that I want to be Python testing.", "tokens": [50364, 407, 286, 478, 516, 281, 360, 307, 445, 1378, 670, 666, 452, 21120, 300, 286, 528, 281, 312, 15329, 4997, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13701089223225912, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.4802871644496918}, {"id": 48, "seek": 23364, "start": 243.64, "end": 250.64, "text": " We're going to make a directory free code camp GPT course.", "tokens": [50864, 492, 434, 516, 281, 652, 257, 21120, 1737, 3089, 2255, 26039, 51, 1164, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13701089223225912, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.4802871644496918}, {"id": 49, "seek": 23364, "start": 250.64, "end": 254.64, "text": " And then from this point, we're going to go and make a virtual environment.", "tokens": [51214, 400, 550, 490, 341, 935, 11, 321, 434, 516, 281, 352, 293, 652, 257, 6374, 2823, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13701089223225912, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.4802871644496918}, {"id": 50, "seek": 25464, "start": 254.64, "end": 266.64, "text": " So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around.", "tokens": [50364, 407, 6374, 2823, 11, 309, 486, 9105, 294, 428, 14502, 11, 291, 486, 362, 445, 439, 295, 428, 15329, 15148, 11, 439, 428, 36606, 456, 445, 12607, 926, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08838950886445887, "compression_ratio": 1.859375, "no_speech_prob": 0.6922701001167297}, {"id": 51, "seek": 25464, "start": 266.64, "end": 269.64, "text": " And what the virtual environment does is it sort of separates that.", "tokens": [50964, 400, 437, 264, 6374, 2823, 775, 307, 309, 1333, 295, 34149, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08838950886445887, "compression_ratio": 1.859375, "no_speech_prob": 0.6922701001167297}, {"id": 52, "seek": 25464, "start": 269.64, "end": 274.64, "text": " So you have this isolated environment over here, and you can just play around with this however you want.", "tokens": [51114, 407, 291, 362, 341, 14621, 2823, 670, 510, 11, 293, 291, 393, 445, 862, 926, 365, 341, 4461, 291, 528, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08838950886445887, "compression_ratio": 1.859375, "no_speech_prob": 0.6922701001167297}, {"id": 53, "seek": 25464, "start": 274.64, "end": 283.64, "text": " And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system.", "tokens": [51364, 400, 309, 311, 2584, 4994, 370, 300, 1582, 380, 534, 3278, 365, 439, 295, 264, 4338, 15148, 300, 291, 362, 439, 264, 2306, 300, 445, 3345, 264, 1185, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08838950886445887, "compression_ratio": 1.859375, "no_speech_prob": 0.6922701001167297}, {"id": 54, "seek": 28364, "start": 283.64, "end": 286.64, "text": " When you're not in a virtual environment, if that makes sense.", "tokens": [50364, 1133, 291, 434, 406, 294, 257, 6374, 2823, 11, 498, 300, 1669, 2020, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11186768593044456, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.11747259646654129}, {"id": 55, "seek": 28364, "start": 286.64, "end": 291.64, "text": " So we're going to go ahead and set that up right now by using Python dash M.", "tokens": [50514, 407, 321, 434, 516, 281, 352, 2286, 293, 992, 300, 493, 558, 586, 538, 1228, 15329, 8240, 376, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11186768593044456, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.11747259646654129}, {"id": 56, "seek": 28364, "start": 291.64, "end": 295.64, "text": " And then we're going to go V and V for virtual V and V and then CUDA.", "tokens": [50764, 400, 550, 321, 434, 516, 281, 352, 691, 293, 691, 337, 6374, 691, 293, 691, 293, 550, 29777, 7509, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11186768593044456, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.11747259646654129}, {"id": 57, "seek": 28364, "start": 295.64, "end": 306.64, "text": " So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs.", "tokens": [50964, 407, 264, 1778, 983, 321, 584, 29777, 7509, 510, 307, 570, 1780, 562, 321, 853, 281, 21341, 527, 2539, 420, 264, 5245, 2539, 11, 321, 434, 516, 281, 643, 281, 764, 18407, 82, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11186768593044456, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.11747259646654129}, {"id": 58, "seek": 28364, "start": 306.64, "end": 309.64, "text": " GPUs are going to accelerate this a ton.", "tokens": [51514, 18407, 82, 366, 516, 281, 21341, 341, 257, 2952, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11186768593044456, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.11747259646654129}, {"id": 59, "seek": 30964, "start": 309.64, "end": 313.64, "text": " And basically CUDA is just that little feature in the GPU that lets us do that.", "tokens": [50364, 400, 1936, 29777, 7509, 307, 445, 300, 707, 4111, 294, 264, 18407, 300, 6653, 505, 360, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 60, "seek": 30964, "start": 313.64, "end": 316.64, "text": " So we're going to make an environment called CUDA.", "tokens": [50564, 407, 321, 434, 516, 281, 652, 364, 2823, 1219, 29777, 7509, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 61, "seek": 30964, "start": 316.64, "end": 317.64, "text": " I'm going to go and press enter.", "tokens": [50714, 286, 478, 516, 281, 352, 293, 1886, 3242, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 62, "seek": 30964, "start": 317.64, "end": 320.64, "text": " It's going to do that for us going to take a few seconds.", "tokens": [50764, 467, 311, 516, 281, 360, 300, 337, 505, 516, 281, 747, 257, 1326, 3949, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 63, "seek": 30964, "start": 320.64, "end": 323.64, "text": " So now that's done, we can go ahead and do CUDA.", "tokens": [50914, 407, 586, 300, 311, 1096, 11, 321, 393, 352, 2286, 293, 360, 29777, 7509, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 64, "seek": 30964, "start": 323.64, "end": 328.64, "text": " And we're just going to basically activate this environment so we can start developing in it.", "tokens": [51064, 400, 321, 434, 445, 516, 281, 1936, 13615, 341, 2823, 370, 321, 393, 722, 6416, 294, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 65, "seek": 30964, "start": 328.64, "end": 333.64, "text": " I'm going to go backslash, we're going to go scripts, and then activate.", "tokens": [51314, 286, 478, 516, 281, 352, 646, 10418, 1299, 11, 321, 434, 516, 281, 352, 23294, 11, 293, 550, 13615, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12339211285598879, "compression_ratio": 1.8361344537815125, "no_speech_prob": 0.1383601278066635}, {"id": 66, "seek": 33364, "start": 333.64, "end": 335.64, "text": " So now you can see it says CUDA base.", "tokens": [50364, 407, 586, 291, 393, 536, 309, 1619, 29777, 7509, 3096, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1640867729709573, "compression_ratio": 1.4678362573099415, "no_speech_prob": 0.03845582157373428}, {"id": 67, "seek": 33364, "start": 335.64, "end": 339.64, "text": " So we're in CUDA and then secondary base.", "tokens": [50464, 407, 321, 434, 294, 29777, 7509, 293, 550, 11396, 3096, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1640867729709573, "compression_ratio": 1.4678362573099415, "no_speech_prob": 0.03845582157373428}, {"id": 68, "seek": 33364, "start": 339.64, "end": 341.64, "text": " So it's going to prioritize CUDA.", "tokens": [50664, 407, 309, 311, 516, 281, 25164, 29777, 7509, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1640867729709573, "compression_ratio": 1.4678362573099415, "no_speech_prob": 0.03845582157373428}, {"id": 69, "seek": 33364, "start": 341.64, "end": 346.64, "text": " So from this point, we can actually start installing some stuff, some libraries here.", "tokens": [50764, 407, 490, 341, 935, 11, 321, 393, 767, 722, 20762, 512, 1507, 11, 512, 15148, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1640867729709573, "compression_ratio": 1.4678362573099415, "no_speech_prob": 0.03845582157373428}, {"id": 70, "seek": 33364, "start": 346.64, "end": 353.64, "text": " So we can go pip three install Matt plot lib numpy.", "tokens": [51014, 407, 321, 393, 352, 8489, 1045, 3625, 7397, 7542, 22854, 1031, 8200, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1640867729709573, "compression_ratio": 1.4678362573099415, "no_speech_prob": 0.03845582157373428}, {"id": 71, "seek": 35364, "start": 354.64, "end": 361.64, "text": " We're going to use p y l m z a l z m a.", "tokens": [50414, 492, 434, 516, 281, 764, 280, 288, 287, 275, 710, 257, 287, 710, 275, 257, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 72, "seek": 35364, "start": 361.64, "end": 364.64, "text": " And then what are some other ones?", "tokens": [50764, 400, 550, 437, 366, 512, 661, 2306, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 73, "seek": 35364, "start": 364.64, "end": 367.64, "text": " We're going to do IPY kernel.", "tokens": [50914, 492, 434, 516, 281, 360, 8671, 56, 28256, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 74, "seek": 35364, "start": 367.64, "end": 374.64, "text": " This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks.", "tokens": [51064, 639, 307, 337, 264, 3539, 22125, 88, 391, 43782, 293, 885, 1075, 281, 1565, 264, 29777, 7509, 6374, 2823, 666, 729, 43782, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 75, "seek": 35364, "start": 374.64, "end": 376.64, "text": " So that's why that's important.", "tokens": [51414, 407, 300, 311, 983, 300, 311, 1021, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 76, "seek": 35364, "start": 376.64, "end": 379.64, "text": " And then just the actual Jupyter notebook feature.", "tokens": [51514, 400, 550, 445, 264, 3539, 22125, 88, 391, 21060, 4111, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 77, "seek": 35364, "start": 379.64, "end": 381.64, "text": " So go and press enter.", "tokens": [51664, 407, 352, 293, 1886, 3242, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1602352968196279, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.22527457773685455}, {"id": 78, "seek": 38164, "start": 381.64, "end": 382.64, "text": " Those are going to install.", "tokens": [50364, 3950, 366, 516, 281, 3625, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 79, "seek": 38164, "start": 382.64, "end": 385.64, "text": " That's going to take a few seconds to do.", "tokens": [50414, 663, 311, 516, 281, 747, 257, 1326, 3949, 281, 360, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 80, "seek": 38164, "start": 385.64, "end": 393.64, "text": " So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm.", "tokens": [50564, 407, 437, 1062, 767, 1051, 307, 291, 603, 483, 257, 1322, 6713, 365, 280, 288, 287, 710, 275, 257, 11, 597, 307, 257, 19355, 9284, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 81, "seek": 38164, "start": 393.64, "end": 397.64, "text": " And don't quote me on this, but I'm pretty sure it's based in C plus plus.", "tokens": [50964, 400, 500, 380, 6513, 385, 322, 341, 11, 457, 286, 478, 1238, 988, 309, 311, 2361, 294, 383, 1804, 1804, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 82, "seek": 38164, "start": 397.64, "end": 400.64, "text": " So you actually need some build tools for this.", "tokens": [51164, 407, 291, 767, 643, 512, 1322, 3873, 337, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 83, "seek": 38164, "start": 400.64, "end": 405.64, "text": " And you can get that with visual studio build tools.", "tokens": [51314, 400, 291, 393, 483, 300, 365, 5056, 6811, 1322, 3873, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 84, "seek": 38164, "start": 405.64, "end": 410.64, "text": " So what you're you might see, you might see a little error and basically go to that website.", "tokens": [51564, 407, 437, 291, 434, 291, 1062, 536, 11, 291, 1062, 536, 257, 707, 6713, 293, 1936, 352, 281, 300, 3144, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12018767341238554, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.020016182214021683}, {"id": 85, "seek": 41064, "start": 410.64, "end": 411.64, "text": " You're going to get this right here.", "tokens": [50364, 509, 434, 516, 281, 483, 341, 558, 510, 13, 50414], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 86, "seek": 41064, "start": 411.64, "end": 414.64, "text": " So just go ahead and download build tools.", "tokens": [50414, 407, 445, 352, 2286, 293, 5484, 1322, 3873, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 87, "seek": 41064, "start": 414.64, "end": 417.64, "text": " What's going to download here, you're going to click on that.", "tokens": [50564, 708, 311, 516, 281, 5484, 510, 11, 291, 434, 516, 281, 2052, 322, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 88, "seek": 41064, "start": 417.64, "end": 419.64, "text": " It's going to, it's going to set up.", "tokens": [50714, 467, 311, 516, 281, 11, 309, 311, 516, 281, 992, 493, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 89, "seek": 41064, "start": 419.64, "end": 425.64, "text": " And then you're going to go ahead and click continue.", "tokens": [50814, 400, 550, 291, 434, 516, 281, 352, 2286, 293, 2052, 2354, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 90, "seek": 41064, "start": 425.64, "end": 429.64, "text": " And then at this point, you can go ahead and click modify if you see this here.", "tokens": [51114, 400, 550, 412, 341, 935, 11, 291, 393, 352, 2286, 293, 2052, 16927, 498, 291, 536, 341, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 91, "seek": 41064, "start": 429.64, "end": 434.64, "text": " And then you might get to a little workloads section here.", "tokens": [51314, 400, 550, 291, 1062, 483, 281, 257, 707, 32452, 3541, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 92, "seek": 41064, "start": 434.64, "end": 436.64, "text": " So once you're at workloads, that's good.", "tokens": [51564, 407, 1564, 291, 434, 412, 32452, 11, 300, 311, 665, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10667987562652327, "compression_ratio": 2.004854368932039, "no_speech_prob": 0.03903330862522125}, {"id": 93, "seek": 43664, "start": 436.64, "end": 441.64, "text": " What you're going to make sure is that you have these two checked off right here.", "tokens": [50364, 708, 291, 434, 516, 281, 652, 988, 307, 300, 291, 362, 613, 732, 10033, 766, 558, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 94, "seek": 43664, "start": 441.64, "end": 443.64, "text": " Just make sure that you have these two.", "tokens": [50614, 1449, 652, 988, 300, 291, 362, 613, 732, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 95, "seek": 43664, "start": 443.64, "end": 445.64, "text": " I'm not sure what desktop particularly does.", "tokens": [50714, 286, 478, 406, 988, 437, 14502, 4098, 775, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 96, "seek": 43664, "start": 445.64, "end": 452.64, "text": " It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects.", "tokens": [50814, 467, 1062, 854, 11, 457, 309, 311, 445, 733, 295, 665, 281, 362, 512, 295, 613, 1322, 3873, 322, 428, 6465, 13448, 11, 754, 337, 2027, 4455, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 97, "seek": 43664, "start": 452.64, "end": 455.64, "text": " So just get these two for now.", "tokens": [51164, 407, 445, 483, 613, 732, 337, 586, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 98, "seek": 43664, "start": 455.64, "end": 456.64, "text": " That'll be good.", "tokens": [51314, 663, 603, 312, 665, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 99, "seek": 43664, "start": 456.64, "end": 460.64, "text": " And then you can click modify over here if you wanted to modify just like that.", "tokens": [51364, 400, 550, 291, 393, 2052, 16927, 670, 510, 498, 291, 1415, 281, 16927, 445, 411, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 100, "seek": 43664, "start": 460.64, "end": 465.64, "text": " And then you should be good to rerun that command.", "tokens": [51564, 400, 550, 291, 820, 312, 665, 281, 43819, 409, 300, 5622, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06320275029828472, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.026348479092121124}, {"id": 101, "seek": 46564, "start": 465.64, "end": 477.64, "text": " So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch.", "tokens": [50364, 407, 490, 341, 935, 11, 437, 321, 393, 767, 360, 307, 321, 434, 516, 281, 3625, 27822, 293, 321, 434, 767, 516, 281, 360, 309, 538, 1228, 8489, 3625, 1045, 3625, 27822, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0795859864779881, "compression_ratio": 2.0046296296296298, "no_speech_prob": 0.012051078490912914}, {"id": 102, "seek": 46564, "start": 477.64, "end": 479.64, "text": " We're not going to do it like this.", "tokens": [50964, 492, 434, 406, 516, 281, 360, 309, 411, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0795859864779881, "compression_ratio": 2.0046296296296298, "no_speech_prob": 0.012051078490912914}, {"id": 103, "seek": 46564, "start": 479.64, "end": 487.64, "text": " What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch.", "tokens": [51064, 708, 321, 434, 767, 516, 281, 360, 307, 321, 434, 516, 281, 764, 257, 4994, 5622, 293, 341, 307, 516, 281, 3625, 29777, 7509, 365, 527, 27822, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0795859864779881, "compression_ratio": 2.0046296296296298, "no_speech_prob": 0.012051078490912914}, {"id": 104, "seek": 46564, "start": 487.64, "end": 492.64, "text": " So it's going to install the CUDA extension, which will allow us to utilize the GPU.", "tokens": [51464, 407, 309, 311, 516, 281, 3625, 264, 29777, 7509, 10320, 11, 597, 486, 2089, 505, 281, 16117, 264, 18407, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0795859864779881, "compression_ratio": 2.0046296296296298, "no_speech_prob": 0.012051078490912914}, {"id": 105, "seek": 46564, "start": 492.64, "end": 494.64, "text": " So it's just this command right here.", "tokens": [51714, 407, 309, 311, 445, 341, 5622, 558, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0795859864779881, "compression_ratio": 2.0046296296296298, "no_speech_prob": 0.012051078490912914}, {"id": 106, "seek": 49464, "start": 494.64, "end": 505.64, "text": " And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started.", "tokens": [50364, 400, 498, 291, 528, 281, 915, 411, 257, 665, 5622, 281, 764, 11, 437, 291, 393, 360, 307, 352, 281, 264, 1730, 27822, 45623, 11, 445, 352, 281, 352, 281, 483, 1409, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12409454921506485, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.04021819680929184}, {"id": 107, "seek": 49464, "start": 505.64, "end": 509.64, "text": " And then you'll be able to see this right here.", "tokens": [50914, 400, 550, 291, 603, 312, 1075, 281, 536, 341, 558, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12409454921506485, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.04021819680929184}, {"id": 108, "seek": 49464, "start": 509.64, "end": 515.64, "text": " So we have stable windows pip Python and CUDA 11.7 or 11.8.", "tokens": [51114, 407, 321, 362, 8351, 9309, 8489, 15329, 293, 29777, 7509, 2975, 13, 22, 420, 2975, 13, 23, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12409454921506485, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.04021819680929184}, {"id": 109, "seek": 49464, "start": 515.64, "end": 516.64, "text": " So I just clicked on this.", "tokens": [51414, 407, 286, 445, 23370, 322, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12409454921506485, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.04021819680929184}, {"id": 110, "seek": 49464, "start": 516.64, "end": 523.64, "text": " And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch.", "tokens": [51464, 400, 1670, 321, 3212, 380, 516, 281, 312, 1228, 27822, 5201, 420, 27822, 6278, 11, 286, 1936, 445, 630, 8489, 1045, 3625, 27822, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12409454921506485, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.04021819680929184}, {"id": 111, "seek": 52364, "start": 523.64, "end": 529.64, "text": " And then with this index URL for the CUDA 11.8.", "tokens": [50364, 400, 550, 365, 341, 8186, 12905, 337, 264, 29777, 7509, 2975, 13, 23, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 112, "seek": 52364, "start": 529.64, "end": 533.64, "text": " So that's pretty much all we're doing there to install CUDA.", "tokens": [50664, 407, 300, 311, 1238, 709, 439, 321, 434, 884, 456, 281, 3625, 29777, 7509, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 113, "seek": 52364, "start": 533.64, "end": 534.64, "text": " That's part of our torch.", "tokens": [50864, 663, 311, 644, 295, 527, 27822, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 114, "seek": 52364, "start": 534.64, "end": 538.64, "text": " So we can go ahead and click enter on this.", "tokens": [50914, 407, 321, 393, 352, 2286, 293, 2052, 3242, 322, 341, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 115, "seek": 52364, "start": 538.64, "end": 539.64, "text": " So great.", "tokens": [51114, 407, 869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 116, "seek": 52364, "start": 539.64, "end": 544.64, "text": " We've installed a lot of things, a lot of libraries, a lot of setup has been done already.", "tokens": [51164, 492, 600, 8899, 257, 688, 295, 721, 11, 257, 688, 295, 15148, 11, 257, 688, 295, 8657, 575, 668, 1096, 1217, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 117, "seek": 52364, "start": 544.64, "end": 548.64, "text": " What I want to check now is just to make sure that our Python version is what we want.", "tokens": [51414, 708, 286, 528, 281, 1520, 586, 307, 445, 281, 652, 988, 300, 527, 15329, 3037, 307, 437, 321, 528, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 118, "seek": 52364, "start": 548.64, "end": 551.64, "text": " So I've done version 3.10.9.", "tokens": [51614, 407, 286, 600, 1096, 3037, 805, 13, 3279, 13, 24, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 119, "seek": 52364, "start": 551.64, "end": 552.64, "text": " That's great.", "tokens": [51764, 663, 311, 869, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10749751564085953, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.05029946565628052}, {"id": 120, "seek": 55264, "start": 552.64, "end": 557.64, "text": " If you're between 3.9, 3.10, 3.11, that's perfect.", "tokens": [50364, 759, 291, 434, 1296, 805, 13, 24, 11, 805, 13, 3279, 11, 805, 13, 5348, 11, 300, 311, 2176, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 121, "seek": 55264, "start": 557.64, "end": 560.64, "text": " So if you're in between those, it should be fine.", "tokens": [50614, 407, 498, 291, 434, 294, 1296, 729, 11, 309, 820, 312, 2489, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 122, "seek": 55264, "start": 560.64, "end": 564.64, "text": " At this point, we can just jump right into our Jupyter Notebook.", "tokens": [50764, 1711, 341, 935, 11, 321, 393, 445, 3012, 558, 666, 527, 22125, 88, 391, 11633, 2939, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 123, "seek": 55264, "start": 564.64, "end": 567.64, "text": " So the command for that is just Jupyter Notebook.", "tokens": [50964, 407, 264, 5622, 337, 300, 307, 445, 22125, 88, 391, 11633, 2939, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 124, "seek": 55264, "start": 567.64, "end": 568.64, "text": " It's about like that.", "tokens": [51114, 467, 311, 466, 411, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 125, "seek": 55264, "start": 568.64, "end": 571.64, "text": " Click enter.", "tokens": [51164, 8230, 3242, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 126, "seek": 55264, "start": 571.64, "end": 574.64, "text": " It's going to send us into here.", "tokens": [51314, 467, 311, 516, 281, 2845, 505, 666, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13248774931602872, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.006288459990173578}, {"id": 127, "seek": 57464, "start": 575.64, "end": 582.64, "text": " And I've created this little biogram.ipynb here in my VS Code.", "tokens": [50414, 400, 286, 600, 2942, 341, 707, 3228, 12820, 13, 647, 2534, 65, 510, 294, 452, 25091, 15549, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11977135733272253, "compression_ratio": 1.4975369458128078, "no_speech_prob": 0.6181391477584839}, {"id": 128, "seek": 57464, "start": 582.64, "end": 586.64, "text": " So pretty much you need to actually type some stuff in it.", "tokens": [50764, 407, 1238, 709, 291, 643, 281, 767, 2010, 512, 1507, 294, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11977135733272253, "compression_ratio": 1.4975369458128078, "no_speech_prob": 0.6181391477584839}, {"id": 129, "seek": 57464, "start": 586.64, "end": 592.64, "text": " And you need to make sure that it has the ipynb extension or else it won't work.", "tokens": [50964, 400, 291, 643, 281, 652, 988, 300, 309, 575, 264, 28501, 2534, 65, 10320, 420, 1646, 309, 1582, 380, 589, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11977135733272253, "compression_ratio": 1.4975369458128078, "no_speech_prob": 0.6181391477584839}, {"id": 130, "seek": 57464, "start": 592.64, "end": 598.64, "text": " So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason.", "tokens": [51264, 407, 498, 309, 311, 445, 28501, 2534, 65, 293, 1177, 380, 362, 1340, 294, 309, 11, 286, 393, 380, 534, 1401, 300, 3991, 337, 512, 1778, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11977135733272253, "compression_ratio": 1.4975369458128078, "no_speech_prob": 0.6181391477584839}, {"id": 131, "seek": 59864, "start": 598.64, "end": 600.64, "text": " So just make sure you type some stuff in it.", "tokens": [50364, 407, 445, 652, 988, 291, 2010, 512, 1507, 294, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 132, "seek": 59864, "start": 600.64, "end": 601.64, "text": " Open that in VS Code.", "tokens": [50464, 7238, 300, 294, 25091, 15549, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 133, "seek": 59864, "start": 601.64, "end": 605.64, "text": " Type, I don't know, a equals 3 or str equals banana.", "tokens": [50514, 15576, 11, 286, 500, 380, 458, 11, 257, 6915, 805, 420, 1056, 6915, 14194, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 134, "seek": 59864, "start": 605.64, "end": 607.64, "text": " I don't care.", "tokens": [50714, 286, 500, 380, 1127, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 135, "seek": 59864, "start": 607.64, "end": 612.64, "text": " At this point, let's go ahead and pop into here.", "tokens": [50814, 1711, 341, 935, 11, 718, 311, 352, 2286, 293, 1665, 666, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 136, "seek": 59864, "start": 612.64, "end": 614.64, "text": " So this is what our notebook is going to look like.", "tokens": [51064, 407, 341, 307, 437, 527, 21060, 307, 516, 281, 574, 411, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 137, "seek": 59864, "start": 614.64, "end": 619.64, "text": " And we're going to be working with this quite a bit throughout this course.", "tokens": [51164, 400, 321, 434, 516, 281, 312, 1364, 365, 341, 1596, 257, 857, 3710, 341, 1164, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 138, "seek": 59864, "start": 619.64, "end": 627.64, "text": " So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook.", "tokens": [51414, 407, 437, 321, 434, 516, 281, 643, 281, 360, 958, 510, 307, 652, 988, 300, 527, 6374, 2823, 307, 767, 1854, 295, 527, 21060, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09738580845604258, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.02095973677933216}, {"id": 139, "seek": 62764, "start": 627.64, "end": 633.64, "text": " And make sure that we can interact with it from this kernel rather than just through the command prompt.", "tokens": [50364, 400, 652, 988, 300, 321, 393, 4648, 365, 309, 490, 341, 28256, 2831, 813, 445, 807, 264, 5622, 12391, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 140, "seek": 62764, "start": 633.64, "end": 635.64, "text": " So we're going to go ahead and check here.", "tokens": [50664, 407, 321, 434, 516, 281, 352, 2286, 293, 1520, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 141, "seek": 62764, "start": 635.64, "end": 637.64, "text": " And I have a virtual environment here.", "tokens": [50764, 400, 286, 362, 257, 6374, 2823, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 142, "seek": 62764, "start": 637.64, "end": 642.64, "text": " You may not, but all we're going to do is basically go into here.", "tokens": [50864, 509, 815, 406, 11, 457, 439, 321, 434, 516, 281, 360, 307, 1936, 352, 666, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 143, "seek": 62764, "start": 642.64, "end": 644.64, "text": " We're going to end this.", "tokens": [51114, 492, 434, 516, 281, 917, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 144, "seek": 62764, "start": 644.64, "end": 656.64, "text": " And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install.", "tokens": [51214, 400, 439, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 352, 2286, 293, 360, 15329, 8240, 376, 293, 550, 28501, 88, 28256, 3625, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0985838140098794, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.016397252678871155}, {"id": 145, "seek": 65664, "start": 657.64, "end": 664.64, "text": " User, you'll see why we're doing this in the second user name equals CUDA.", "tokens": [50414, 32127, 11, 291, 603, 536, 983, 321, 434, 884, 341, 294, 264, 1150, 4195, 1315, 6915, 29777, 7509, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11260410204325637, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0518229715526104}, {"id": 146, "seek": 65664, "start": 664.64, "end": 667.64, "text": " This is from the virtual environment we initialized before.", "tokens": [50764, 639, 307, 490, 264, 6374, 2823, 321, 5883, 1602, 949, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11260410204325637, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0518229715526104}, {"id": 147, "seek": 65664, "start": 667.64, "end": 669.64, "text": " So that's the name of the virtual environment.", "tokens": [50914, 407, 300, 311, 264, 1315, 295, 264, 6374, 2823, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11260410204325637, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0518229715526104}, {"id": 148, "seek": 65664, "start": 669.64, "end": 680.64, "text": " And then the display name, how it's actually going to look in the terminal is going to be display name.", "tokens": [51014, 400, 550, 264, 4674, 1315, 11, 577, 309, 311, 767, 516, 281, 574, 294, 264, 14709, 307, 516, 281, 312, 4674, 1315, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11260410204325637, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0518229715526104}, {"id": 149, "seek": 68064, "start": 681.64, "end": 688.64, "text": " We'll just call it CUDA GPT.", "tokens": [50414, 492, 603, 445, 818, 309, 29777, 7509, 26039, 51, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 150, "seek": 68064, "start": 688.64, "end": 689.64, "text": " I don't know.", "tokens": [50764, 286, 500, 380, 458, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 151, "seek": 68064, "start": 689.64, "end": 690.64, "text": " That sounds like a cool name.", "tokens": [50814, 663, 3263, 411, 257, 1627, 1315, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 152, "seek": 68064, "start": 690.64, "end": 692.64, "text": " And I'm going to press enter.", "tokens": [50864, 400, 286, 478, 516, 281, 1886, 3242, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 153, "seek": 68064, "start": 692.64, "end": 697.64, "text": " It's going to make this environment for us great installed.", "tokens": [50964, 467, 311, 516, 281, 652, 341, 2823, 337, 505, 869, 8899, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 154, "seek": 68064, "start": 697.64, "end": 698.64, "text": " Good.", "tokens": [51214, 2205, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 155, "seek": 68064, "start": 698.64, "end": 705.64, "text": " So we can go and run our notebook again and we'll see if this changes.", "tokens": [51264, 407, 321, 393, 352, 293, 1190, 527, 21060, 797, 293, 321, 603, 536, 498, 341, 2962, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14979794737580535, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.030667243525385857}, {"id": 156, "seek": 70564, "start": 705.64, "end": 712.64, "text": " So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT.", "tokens": [50364, 407, 321, 393, 352, 2286, 293, 1665, 666, 527, 3228, 12, 1342, 797, 11, 28256, 11, 1319, 28256, 11, 9351, 11, 29777, 7509, 26039, 51, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 157, "seek": 70564, "start": 712.64, "end": 714.64, "text": " Let's click that.", "tokens": [50714, 961, 311, 2052, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 158, "seek": 70564, "start": 714.64, "end": 715.64, "text": " Sweet.", "tokens": [50814, 14653, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 159, "seek": 70564, "start": 715.64, "end": 721.64, "text": " So now we can actually start doing more and just sort of experimenting with how the notebooks", "tokens": [50864, 407, 586, 321, 393, 767, 722, 884, 544, 293, 445, 1333, 295, 29070, 365, 577, 264, 43782, 51164], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 160, "seek": 70564, "start": 721.64, "end": 728.64, "text": " work and actually how we can build up this bi-gram model and sort of learning how language", "tokens": [51164, 589, 293, 767, 577, 321, 393, 1322, 493, 341, 3228, 12, 1342, 2316, 293, 1333, 295, 2539, 577, 2856, 51514], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 161, "seek": 70564, "start": 728.64, "end": 730.64, "text": " models work from scratch.", "tokens": [51514, 5245, 589, 490, 8459, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 162, "seek": 70564, "start": 730.64, "end": 731.64, "text": " So let's go ahead and do that.", "tokens": [51614, 407, 718, 311, 352, 2286, 293, 360, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15728398525353635, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.009266351349651814}, {"id": 163, "seek": 73164, "start": 731.64, "end": 739.64, "text": " Now that we jump into this actual code here, what I want to do is delete all of these.", "tokens": [50364, 823, 300, 321, 3012, 666, 341, 3539, 3089, 510, 11, 437, 286, 528, 281, 360, 307, 12097, 439, 295, 613, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12120625407425399, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.3377881348133087}, {"id": 164, "seek": 73164, "start": 739.64, "end": 740.64, "text": " Good.", "tokens": [50764, 2205, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12120625407425399, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.3377881348133087}, {"id": 165, "seek": 73164, "start": 740.64, "end": 745.64, "text": " So now what I'm going to do is just get a small little data set, just very small for us to", "tokens": [50814, 407, 586, 437, 286, 478, 516, 281, 360, 307, 445, 483, 257, 1359, 707, 1412, 992, 11, 445, 588, 1359, 337, 505, 281, 51064], "temperature": 0.0, "avg_logprob": -0.12120625407425399, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.3377881348133087}, {"id": 166, "seek": 73164, "start": 745.64, "end": 750.64, "text": " work with that we can sort of try to make a bi-gram out of, something very small.", "tokens": [51064, 589, 365, 300, 321, 393, 1333, 295, 853, 281, 652, 257, 3228, 12, 1342, 484, 295, 11, 746, 588, 1359, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12120625407425399, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.3377881348133087}, {"id": 167, "seek": 73164, "start": 750.64, "end": 756.64, "text": " So what we can do is go to this website called Project Gutenberg and they basically just", "tokens": [51314, 407, 437, 321, 393, 360, 307, 352, 281, 341, 3144, 1219, 9849, 42833, 6873, 293, 436, 1936, 445, 51614], "temperature": 0.0, "avg_logprob": -0.12120625407425399, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.3377881348133087}, {"id": 168, "seek": 75664, "start": 756.64, "end": 761.64, "text": " have a bunch of free books that are licensed under Creative Commons.", "tokens": [50364, 362, 257, 3840, 295, 1737, 3642, 300, 366, 25225, 833, 26598, 34894, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 169, "seek": 75664, "start": 761.64, "end": 765.64, "text": " So we can use all of these for free.", "tokens": [50614, 407, 321, 393, 764, 439, 295, 613, 337, 1737, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 170, "seek": 75664, "start": 765.64, "end": 769.64, "text": " So let's use the Wizard of Oz.", "tokens": [50814, 407, 718, 311, 764, 264, 37449, 295, 29843, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 171, "seek": 75664, "start": 769.64, "end": 773.64, "text": " Put it at the end of Wizard of Oz.", "tokens": [51014, 4935, 309, 412, 264, 917, 295, 37449, 295, 29843, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 172, "seek": 75664, "start": 773.64, "end": 775.64, "text": " Great.", "tokens": [51214, 3769, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 173, "seek": 75664, "start": 775.64, "end": 778.64, "text": " So what we're going to want to do is just click on plain text here.", "tokens": [51314, 407, 437, 321, 434, 516, 281, 528, 281, 360, 307, 445, 2052, 322, 11121, 2487, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 174, "seek": 75664, "start": 778.64, "end": 779.64, "text": " Great.", "tokens": [51464, 3769, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1377099797695498, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.26274433732032776}, {"id": 175, "seek": 77964, "start": 779.64, "end": 789.64, "text": " So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore", "tokens": [50364, 407, 586, 321, 393, 352, 35233, 318, 281, 3155, 341, 293, 550, 321, 727, 445, 352, 37449, 295, 29843, 11, 37449, 37556, 50864], "temperature": 0.0, "avg_logprob": -0.1303912823016827, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.01590491458773613}, {"id": 176, "seek": 77964, "start": 789.64, "end": 791.64, "text": " of underscore Oz.", "tokens": [50864, 295, 37556, 29843, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1303912823016827, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.01590491458773613}, {"id": 177, "seek": 77964, "start": 791.64, "end": 792.64, "text": " Good.", "tokens": [50964, 2205, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1303912823016827, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.01590491458773613}, {"id": 178, "seek": 77964, "start": 792.64, "end": 804.64, "text": " So now what I'm going to do is we should probably drag this into, we should drag this into our", "tokens": [51014, 407, 586, 437, 286, 478, 516, 281, 360, 307, 321, 820, 1391, 5286, 341, 666, 11, 321, 820, 5286, 341, 666, 527, 51614], "temperature": 0.0, "avg_logprob": -0.1303912823016827, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.01590491458773613}, {"id": 179, "seek": 77964, "start": 804.64, "end": 805.64, "text": " folder here.", "tokens": [51614, 10820, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1303912823016827, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.01590491458773613}, {"id": 180, "seek": 80564, "start": 805.64, "end": 810.64, "text": " I'm just going to pop that into there.", "tokens": [50364, 286, 478, 445, 516, 281, 1665, 300, 666, 456, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 181, "seek": 80564, "start": 810.64, "end": 811.64, "text": " Good stuff.", "tokens": [50614, 2205, 1507, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 182, "seek": 80564, "start": 811.64, "end": 813.64, "text": " Did that work?", "tokens": [50664, 2589, 300, 589, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 183, "seek": 80564, "start": 813.64, "end": 814.64, "text": " Sweet.", "tokens": [50764, 14653, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 184, "seek": 80564, "start": 814.64, "end": 817.64, "text": " So now we have our Wizard of Oz text in here, we can open that.", "tokens": [50814, 407, 586, 321, 362, 527, 37449, 295, 29843, 2487, 294, 510, 11, 321, 393, 1269, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 185, "seek": 80564, "start": 817.64, "end": 821.64, "text": " What we can do is start of this book.", "tokens": [50964, 708, 321, 393, 360, 307, 722, 295, 341, 1446, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 186, "seek": 80564, "start": 821.64, "end": 822.64, "text": " Okay.", "tokens": [51164, 1033, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1335323303937912, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.03731865435838699}, {"id": 187, "seek": 82264, "start": 822.64, "end": 829.64, "text": " So we can go ahead and go down to when it starts.", "tokens": [50364, 407, 321, 393, 352, 2286, 293, 352, 760, 281, 562, 309, 3719, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 188, "seek": 82264, "start": 829.64, "end": 838.64, "text": " Sweet.", "tokens": [50714, 14653, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 189, "seek": 82264, "start": 838.64, "end": 841.64, "text": " So maybe we'll just cut it here.", "tokens": [51164, 407, 1310, 321, 603, 445, 1723, 309, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 190, "seek": 82264, "start": 841.64, "end": 843.64, "text": " That'd be a good place to start.", "tokens": [51314, 663, 1116, 312, 257, 665, 1081, 281, 722, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 191, "seek": 82264, "start": 843.64, "end": 844.64, "text": " Just like that.", "tokens": [51414, 1449, 411, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 192, "seek": 82264, "start": 844.64, "end": 847.64, "text": " I'll put a few spaces.", "tokens": [51464, 286, 603, 829, 257, 1326, 7673, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 193, "seek": 82264, "start": 847.64, "end": 848.64, "text": " Good.", "tokens": [51614, 2205, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 194, "seek": 82264, "start": 848.64, "end": 850.64, "text": " So now we have this book.", "tokens": [51664, 407, 586, 321, 362, 341, 1446, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16626308333705847, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.02064361423254013}, {"id": 195, "seek": 85064, "start": 850.64, "end": 857.64, "text": " We go to the bottom here just to get rid of some of this other licensing stuff, which", "tokens": [50364, 492, 352, 281, 264, 2767, 510, 445, 281, 483, 3973, 295, 512, 295, 341, 661, 29759, 1507, 11, 597, 50714], "temperature": 0.0, "avg_logprob": -0.155575444148137, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.019119668751955032}, {"id": 196, "seek": 85064, "start": 857.64, "end": 863.64, "text": " might get in the way with our predictions in the context of the entire book.", "tokens": [50714, 1062, 483, 294, 264, 636, 365, 527, 21264, 294, 264, 4319, 295, 264, 2302, 1446, 13, 51014], "temperature": 0.0, "avg_logprob": -0.155575444148137, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.019119668751955032}, {"id": 197, "seek": 85064, "start": 863.64, "end": 866.64, "text": " So let's just go down to when that starts.", "tokens": [51014, 407, 718, 311, 445, 352, 760, 281, 562, 300, 3719, 13, 51164], "temperature": 0.0, "avg_logprob": -0.155575444148137, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.019119668751955032}, {"id": 198, "seek": 85064, "start": 866.64, "end": 869.64, "text": " End of the book.", "tokens": [51164, 6967, 295, 264, 1446, 13, 51314], "temperature": 0.0, "avg_logprob": -0.155575444148137, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.019119668751955032}, {"id": 199, "seek": 85064, "start": 869.64, "end": 871.64, "text": " Okay.", "tokens": [51314, 1033, 13, 51414], "temperature": 0.0, "avg_logprob": -0.155575444148137, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.019119668751955032}, {"id": 200, "seek": 87164, "start": 871.64, "end": 877.64, "text": " So we've gotten all that.", "tokens": [50364, 407, 321, 600, 5768, 439, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 201, "seek": 87164, "start": 877.64, "end": 878.64, "text": " That is done.", "tokens": [50664, 663, 307, 1096, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 202, "seek": 87164, "start": 878.64, "end": 881.64, "text": " Get rid of the illustration there.", "tokens": [50714, 3240, 3973, 295, 264, 22645, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 203, "seek": 87164, "start": 881.64, "end": 882.64, "text": " Perfect.", "tokens": [50864, 10246, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 204, "seek": 87164, "start": 882.64, "end": 885.64, "text": " So now we have this Wizard of Oz text that we can work with.", "tokens": [50914, 407, 586, 321, 362, 341, 37449, 295, 29843, 2487, 300, 321, 393, 589, 365, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 205, "seek": 87164, "start": 885.64, "end": 887.64, "text": " Let's close that up.", "tokens": [51064, 961, 311, 1998, 300, 493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 206, "seek": 87164, "start": 887.64, "end": 888.64, "text": " 233 kilobytes.", "tokens": [51164, 6673, 18, 5128, 996, 43673, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 207, "seek": 87164, "start": 888.64, "end": 889.64, "text": " Awesome.", "tokens": [51214, 10391, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 208, "seek": 87164, "start": 889.64, "end": 890.64, "text": " Very small size.", "tokens": [51264, 4372, 1359, 2744, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 209, "seek": 87164, "start": 890.64, "end": 891.64, "text": " We can work with this.", "tokens": [51314, 492, 393, 589, 365, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 210, "seek": 87164, "start": 891.64, "end": 892.64, "text": " This is great.", "tokens": [51364, 639, 307, 869, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 211, "seek": 87164, "start": 892.64, "end": 894.64, "text": " So we have this wizard of Oz dot txt file.", "tokens": [51414, 407, 321, 362, 341, 25807, 295, 29843, 5893, 256, 734, 3991, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 212, "seek": 87164, "start": 894.64, "end": 895.64, "text": " And what are we going to do with that?", "tokens": [51514, 400, 437, 366, 321, 516, 281, 360, 365, 300, 30, 51564], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 213, "seek": 87164, "start": 895.64, "end": 900.64, "text": " Well, we're going to try to train a transformer or at least a background language model on", "tokens": [51564, 1042, 11, 321, 434, 516, 281, 853, 281, 3847, 257, 31782, 420, 412, 1935, 257, 3678, 2856, 2316, 322, 51814], "temperature": 0.0, "avg_logprob": -0.16593174436199132, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.005910778883844614}, {"id": 214, "seek": 90064, "start": 900.64, "end": 901.64, "text": " this text.", "tokens": [50364, 341, 2487, 13, 50414], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 215, "seek": 90064, "start": 901.64, "end": 905.64, "text": " So in order to do that, we need to sort of learn how to manage this text file, how to", "tokens": [50414, 407, 294, 1668, 281, 360, 300, 11, 321, 643, 281, 1333, 295, 1466, 577, 281, 3067, 341, 2487, 3991, 11, 577, 281, 50614], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 216, "seek": 90064, "start": 905.64, "end": 907.64, "text": " open it, et cetera.", "tokens": [50614, 1269, 309, 11, 1030, 11458, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 217, "seek": 90064, "start": 907.64, "end": 915.64, "text": " So we're going to go ahead and open this and do wizard of Oz.", "tokens": [50714, 407, 321, 434, 516, 281, 352, 2286, 293, 1269, 341, 293, 360, 25807, 295, 29843, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 218, "seek": 90064, "start": 915.64, "end": 916.64, "text": " Like that.", "tokens": [51114, 1743, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 219, "seek": 90064, "start": 916.64, "end": 917.64, "text": " And we're going to open in read mode.", "tokens": [51164, 400, 321, 434, 516, 281, 1269, 294, 1401, 4391, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 220, "seek": 90064, "start": 917.64, "end": 922.64, "text": " And then we're going to use the encoding utf 8 just like that.", "tokens": [51214, 400, 550, 321, 434, 516, 281, 764, 264, 43430, 2839, 69, 1649, 445, 411, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 221, "seek": 90064, "start": 922.64, "end": 925.64, "text": " So this is the file mode that you're going to open in.", "tokens": [51464, 407, 341, 307, 264, 3991, 4391, 300, 291, 434, 516, 281, 1269, 294, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 222, "seek": 90064, "start": 925.64, "end": 926.64, "text": " There's read mode.", "tokens": [51614, 821, 311, 1401, 4391, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 223, "seek": 90064, "start": 926.64, "end": 927.64, "text": " There's write mode.", "tokens": [51664, 821, 311, 2464, 4391, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 224, "seek": 90064, "start": 927.64, "end": 928.64, "text": " There's read binary.", "tokens": [51714, 821, 311, 1401, 17434, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13321610384209212, "compression_ratio": 1.9014084507042253, "no_speech_prob": 0.014059723354876041}, {"id": 225, "seek": 92864, "start": 928.64, "end": 929.64, "text": " There's write binary.", "tokens": [50364, 821, 311, 2464, 17434, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 226, "seek": 92864, "start": 929.64, "end": 936.64, "text": " And those are really the only ones we're going to be worrying about for this video.", "tokens": [50414, 400, 729, 366, 534, 264, 787, 2306, 321, 434, 516, 281, 312, 18788, 466, 337, 341, 960, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 227, "seek": 92864, "start": 936.64, "end": 940.64, "text": " The other ones you can look into in your spare time if you'd like to.", "tokens": [50764, 440, 661, 2306, 291, 393, 574, 666, 294, 428, 13798, 565, 498, 291, 1116, 411, 281, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 228, "seek": 92864, "start": 940.64, "end": 943.64, "text": " I've already seen using those four for now.", "tokens": [50964, 286, 600, 1217, 1612, 1228, 729, 1451, 337, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 229, "seek": 92864, "start": 943.64, "end": 947.64, "text": " And then the encoding is just what type of character coding are we using?", "tokens": [51114, 400, 550, 264, 43430, 307, 445, 437, 2010, 295, 2517, 17720, 366, 321, 1228, 30, 51314], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 230, "seek": 92864, "start": 947.64, "end": 948.64, "text": " That's pretty much it.", "tokens": [51314, 663, 311, 1238, 709, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 231, "seek": 92864, "start": 948.64, "end": 951.64, "text": " We can just open this as F short for file.", "tokens": [51364, 492, 393, 445, 1269, 341, 382, 479, 2099, 337, 3991, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 232, "seek": 92864, "start": 951.64, "end": 954.64, "text": " I'm going to go text equals f dot read.", "tokens": [51514, 286, 478, 516, 281, 352, 2487, 6915, 283, 5893, 1401, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1567036568581521, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.028429323807358742}, {"id": 233, "seek": 95464, "start": 955.64, "end": 958.64, "text": " I'm going to read this file stored in a string variable.", "tokens": [50414, 286, 478, 516, 281, 1401, 341, 3991, 12187, 294, 257, 6798, 7006, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 234, "seek": 95464, "start": 958.64, "end": 961.64, "text": " And then we can print some stuff about it.", "tokens": [50564, 400, 550, 321, 393, 4482, 512, 1507, 466, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 235, "seek": 95464, "start": 961.64, "end": 965.64, "text": " So we can go print the length of this text.", "tokens": [50714, 407, 321, 393, 352, 4482, 264, 4641, 295, 341, 2487, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 236, "seek": 95464, "start": 965.64, "end": 966.64, "text": " Run that.", "tokens": [50914, 8950, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 237, "seek": 95464, "start": 966.64, "end": 970.64, "text": " We get the length of the text.", "tokens": [50964, 492, 483, 264, 4641, 295, 264, 2487, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 238, "seek": 95464, "start": 970.64, "end": 976.64, "text": " We could print the first 200 characters of the text.", "tokens": [51164, 492, 727, 4482, 264, 700, 2331, 4342, 295, 264, 2487, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 239, "seek": 95464, "start": 976.64, "end": 977.64, "text": " Sure.", "tokens": [51464, 4894, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 240, "seek": 95464, "start": 977.64, "end": 979.64, "text": " So you have the first 200 characters.", "tokens": [51514, 407, 291, 362, 264, 700, 2331, 4342, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 241, "seek": 95464, "start": 979.64, "end": 980.64, "text": " Great.", "tokens": [51614, 3769, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14510724761269309, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.03461148589849472}, {"id": 242, "seek": 98064, "start": 981.64, "end": 984.64, "text": " So now we know how to, you know, just play with characters.", "tokens": [50414, 407, 586, 321, 458, 577, 281, 11, 291, 458, 11, 445, 862, 365, 4342, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 243, "seek": 98064, "start": 984.64, "end": 988.64, "text": " At least just see what the characters actually look like.", "tokens": [50564, 1711, 1935, 445, 536, 437, 264, 4342, 767, 574, 411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 244, "seek": 98064, "start": 988.64, "end": 993.64, "text": " So now we can do a little bit more from this point, which is going to be encoders.", "tokens": [50764, 407, 586, 321, 393, 360, 257, 707, 857, 544, 490, 341, 935, 11, 597, 307, 516, 281, 312, 2058, 378, 433, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 245, "seek": 98064, "start": 993.64, "end": 1000.64, "text": " And before we get into that, what I'm going to do is put these into a little vocabulary", "tokens": [51014, 400, 949, 321, 483, 666, 300, 11, 437, 286, 478, 516, 281, 360, 307, 829, 613, 666, 257, 707, 19864, 51364], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 246, "seek": 98064, "start": 1000.64, "end": 1002.64, "text": " list that we can work with.", "tokens": [51364, 1329, 300, 321, 393, 589, 365, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 247, "seek": 98064, "start": 1002.64, "end": 1008.64, "text": " So all I'm going to do is I'm going to say we're going to make a charge variable.", "tokens": [51464, 407, 439, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 584, 321, 434, 516, 281, 652, 257, 4602, 7006, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06730722307084917, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.002082664519548416}, {"id": 248, "seek": 100864, "start": 1008.64, "end": 1014.64, "text": " So the charge is going to be all the charge or all the characters in this text piece.", "tokens": [50364, 407, 264, 4602, 307, 516, 281, 312, 439, 264, 4602, 420, 439, 264, 4342, 294, 341, 2487, 2522, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10079992351247304, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.0019265977898612618}, {"id": 249, "seek": 100864, "start": 1014.64, "end": 1028.6399999999999, "text": " So we're going to make a sorted set of text here, and we're going to just print out charge.", "tokens": [50664, 407, 321, 434, 516, 281, 652, 257, 25462, 992, 295, 2487, 510, 11, 293, 321, 434, 516, 281, 445, 4482, 484, 4602, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10079992351247304, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.0019265977898612618}, {"id": 250, "seek": 100864, "start": 1028.6399999999999, "end": 1029.6399999999999, "text": " So look at that.", "tokens": [51364, 407, 574, 412, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10079992351247304, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.0019265977898612618}, {"id": 251, "seek": 100864, "start": 1029.6399999999999, "end": 1033.6399999999999, "text": " We have a giant array of all these characters.", "tokens": [51414, 492, 362, 257, 7410, 10225, 295, 439, 613, 4342, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10079992351247304, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.0019265977898612618}, {"id": 252, "seek": 103364, "start": 1033.64, "end": 1039.64, "text": " So now we can, what we can do is we can use something called a tokenizer and a tokenizer", "tokens": [50364, 407, 586, 321, 393, 11, 437, 321, 393, 360, 307, 321, 393, 764, 746, 1219, 257, 14862, 6545, 293, 257, 14862, 6545, 50664], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 253, "seek": 103364, "start": 1039.64, "end": 1042.64, "text": " consists of an encoder and a decoder.", "tokens": [50664, 14689, 295, 364, 2058, 19866, 293, 257, 979, 19866, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 254, "seek": 103364, "start": 1042.64, "end": 1048.64, "text": " What an encoder does is it's actually going to convert each character or sorry, each element", "tokens": [50814, 708, 364, 2058, 19866, 775, 307, 309, 311, 767, 516, 281, 7620, 1184, 2517, 420, 2597, 11, 1184, 4478, 51114], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 255, "seek": 103364, "start": 1048.64, "end": 1051.64, "text": " of this array to an integer.", "tokens": [51114, 295, 341, 10225, 281, 364, 24922, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 256, "seek": 103364, "start": 1051.64, "end": 1054.64, "text": " So maybe this would be a zero.", "tokens": [51264, 407, 1310, 341, 576, 312, 257, 4018, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 257, "seek": 103364, "start": 1054.64, "end": 1056.64, "text": " This would be a one, right?", "tokens": [51414, 639, 576, 312, 257, 472, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 258, "seek": 103364, "start": 1056.64, "end": 1062.64, "text": " So a new, a new line or an enter would be a zero, a space would be a one exclamation", "tokens": [51514, 407, 257, 777, 11, 257, 777, 1622, 420, 364, 3242, 576, 312, 257, 4018, 11, 257, 1901, 576, 312, 257, 472, 1624, 43233, 51814], "temperature": 0.0, "avg_logprob": -0.09898686198006688, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.1601799726486206}, {"id": 259, "seek": 106264, "start": 1062.64, "end": 1064.64, "text": " mark would be a two, et cetera, right?", "tokens": [50364, 1491, 576, 312, 257, 732, 11, 1030, 11458, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 260, "seek": 106264, "start": 1064.64, "end": 1066.64, "text": " All the way to the length of them.", "tokens": [50464, 1057, 264, 636, 281, 264, 4641, 295, 552, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 261, "seek": 106264, "start": 1066.64, "end": 1071.64, "text": " And then what we could do is we could even, we could even print the length of these characters.", "tokens": [50564, 400, 550, 437, 321, 727, 360, 307, 321, 727, 754, 11, 321, 727, 754, 4482, 264, 4641, 295, 613, 4342, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 262, "seek": 106264, "start": 1071.64, "end": 1073.64, "text": " So you can see how many there actually are.", "tokens": [50814, 407, 291, 393, 536, 577, 867, 456, 767, 366, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 263, "seek": 106264, "start": 1073.64, "end": 1078.64, "text": " So there's 81 characters in the entire, in the entire Wizard of Oz book.", "tokens": [50914, 407, 456, 311, 30827, 4342, 294, 264, 2302, 11, 294, 264, 2302, 37449, 295, 29843, 1446, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 264, "seek": 106264, "start": 1078.64, "end": 1082.64, "text": " So I've written some code here that is going to do that job for us, the job of tokenizers.", "tokens": [51164, 407, 286, 600, 3720, 512, 3089, 510, 300, 307, 516, 281, 360, 300, 1691, 337, 505, 11, 264, 1691, 295, 14862, 22525, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 265, "seek": 106264, "start": 1082.64, "end": 1087.64, "text": " So what we do is we just use a little generator, some generator for loops here,", "tokens": [51364, 407, 437, 321, 360, 307, 321, 445, 764, 257, 707, 19265, 11, 512, 19265, 337, 16121, 510, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09627347564697265, "compression_ratio": 1.78515625, "no_speech_prob": 0.010650711134076118}, {"id": 266, "seek": 108764, "start": 1087.64, "end": 1093.64, "text": " a generator for loops rather, and we make a little mapping from strings to integers", "tokens": [50364, 257, 19265, 337, 16121, 2831, 11, 293, 321, 652, 257, 707, 18350, 490, 13985, 281, 41674, 50664], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 267, "seek": 108764, "start": 1093.64, "end": 1096.64, "text": " and integers to strings, given the vocabulary.", "tokens": [50664, 293, 41674, 281, 13985, 11, 2212, 264, 19864, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 268, "seek": 108764, "start": 1096.64, "end": 1099.64, "text": " So we just enumerate through each of these.", "tokens": [50814, 407, 321, 445, 465, 15583, 473, 807, 1184, 295, 613, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 269, "seek": 108764, "start": 1099.64, "end": 1104.64, "text": " We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right?", "tokens": [50964, 492, 362, 472, 15187, 11, 700, 4478, 13279, 281, 257, 472, 11, 1150, 13279, 281, 257, 732, 11, 1030, 11458, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 270, "seek": 108764, "start": 1104.64, "end": 1106.64, "text": " That's basically all we're doing here.", "tokens": [51214, 663, 311, 1936, 439, 321, 434, 884, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 271, "seek": 108764, "start": 1106.64, "end": 1108.64, "text": " And we have an encoder and a decoder.", "tokens": [51314, 400, 321, 362, 364, 2058, 19866, 293, 257, 979, 19866, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 272, "seek": 108764, "start": 1108.64, "end": 1114.64, "text": " So let's say we wanted to convert the string hello to integers.", "tokens": [51414, 407, 718, 311, 584, 321, 1415, 281, 7620, 264, 6798, 7751, 281, 41674, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15370977909193126, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.03731841966509819}, {"id": 273, "seek": 111464, "start": 1114.64, "end": 1120.64, "text": " So we go encode, and we could do hello, just like that.", "tokens": [50364, 407, 321, 352, 2058, 1429, 11, 293, 321, 727, 360, 7751, 11, 445, 411, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 274, "seek": 111464, "start": 1120.64, "end": 1125.64, "text": " And then we could go ahead and print this out.", "tokens": [50664, 400, 550, 321, 727, 352, 2286, 293, 4482, 341, 484, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 275, "seek": 111464, "start": 1125.64, "end": 1126.64, "text": " Perfect.", "tokens": [50914, 10246, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 276, "seek": 111464, "start": 1126.64, "end": 1128.64, "text": " Let's go ahead and run that.", "tokens": [50964, 961, 311, 352, 2286, 293, 1190, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 277, "seek": 111464, "start": 1128.64, "end": 1129.64, "text": " Boom.", "tokens": [51064, 15523, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 278, "seek": 111464, "start": 1129.64, "end": 1133.64, "text": " So now we have a conversion from characters to integers.", "tokens": [51114, 407, 586, 321, 362, 257, 14298, 490, 4342, 281, 41674, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 279, "seek": 111464, "start": 1133.64, "end": 1139.64, "text": " And then if we wanted to maybe convert this back, so decode it,", "tokens": [51314, 400, 550, 498, 321, 1415, 281, 1310, 7620, 341, 646, 11, 370, 979, 1429, 309, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09472101490672041, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0027148271910846233}, {"id": 280, "seek": 113964, "start": 1139.64, "end": 1145.64, "text": " sort this in a little, maybe decoded hello equals that.", "tokens": [50364, 1333, 341, 294, 257, 707, 11, 1310, 979, 12340, 7751, 6915, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1536032689082158, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.06007581576704979}, {"id": 281, "seek": 113964, "start": 1145.64, "end": 1153.64, "text": " And then we could go or encoded rather encoded hello.", "tokens": [50664, 400, 550, 321, 727, 352, 420, 2058, 12340, 2831, 2058, 12340, 7751, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1536032689082158, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.06007581576704979}, {"id": 282, "seek": 113964, "start": 1153.64, "end": 1158.64, "text": " And then we could go decoded.", "tokens": [51064, 400, 550, 321, 727, 352, 979, 12340, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1536032689082158, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.06007581576704979}, {"id": 283, "seek": 113964, "start": 1158.64, "end": 1164.64, "text": " Hello is equal to we go decode and we can use the encoded hello.", "tokens": [51314, 2425, 307, 2681, 281, 321, 352, 979, 1429, 293, 321, 393, 764, 264, 2058, 12340, 7751, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1536032689082158, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.06007581576704979}, {"id": 284, "seek": 113964, "start": 1164.64, "end": 1168.64, "text": " So we're going to go ahead and encode this into integers.", "tokens": [51614, 407, 321, 434, 516, 281, 352, 2286, 293, 2058, 1429, 341, 666, 41674, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1536032689082158, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.06007581576704979}, {"id": 285, "seek": 116864, "start": 1168.64, "end": 1173.64, "text": " And then we're going to decode the integers back to a character format.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 979, 1429, 264, 41674, 646, 281, 257, 2517, 7877, 13, 50614], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 286, "seek": 116864, "start": 1173.64, "end": 1176.64, "text": " So let's go ahead and print that out.", "tokens": [50614, 407, 718, 311, 352, 2286, 293, 4482, 300, 484, 13, 50764], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 287, "seek": 116864, "start": 1176.64, "end": 1181.64, "text": " We're going to go ahead and print the decoded hello.", "tokens": [50764, 492, 434, 516, 281, 352, 2286, 293, 4482, 264, 979, 12340, 7751, 13, 51014], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 288, "seek": 116864, "start": 1181.64, "end": 1182.64, "text": " Perfect.", "tokens": [51014, 10246, 13, 51064], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 289, "seek": 116864, "start": 1182.64, "end": 1184.64, "text": " So now we get that.", "tokens": [51064, 407, 586, 321, 483, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 290, "seek": 116864, "start": 1184.64, "end": 1188.64, "text": " So I'm going to fill you in on a little background information about these tokenizers.", "tokens": [51164, 407, 286, 478, 516, 281, 2836, 291, 294, 322, 257, 707, 3678, 1589, 466, 613, 14862, 22525, 13, 51364], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 291, "seek": 116864, "start": 1188.64, "end": 1193.64, "text": " So right now we're using the character level tokenizer, which takes basically each character", "tokens": [51364, 407, 558, 586, 321, 434, 1228, 264, 2517, 1496, 14862, 6545, 11, 597, 2516, 1936, 1184, 2517, 51614], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 292, "seek": 116864, "start": 1193.64, "end": 1196.64, "text": " and converts it to an integer equivalent.", "tokens": [51614, 293, 38874, 309, 281, 364, 24922, 10344, 13, 51764], "temperature": 0.0, "avg_logprob": -0.04142551951938205, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.0015485661569982767}, {"id": 293, "seek": 119664, "start": 1196.64, "end": 1203.64, "text": " So we have a very small vocabulary and a very large amount of tokens to convert.", "tokens": [50364, 407, 321, 362, 257, 588, 1359, 19864, 293, 257, 588, 2416, 2372, 295, 22667, 281, 7620, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06512065653531056, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01614205725491047}, {"id": 294, "seek": 119664, "start": 1203.64, "end": 1208.64, "text": " So if we have 40,000 individual characters, it means we have a small vocabulary to work", "tokens": [50714, 407, 498, 321, 362, 3356, 11, 1360, 2609, 4342, 11, 309, 1355, 321, 362, 257, 1359, 19864, 281, 589, 50964], "temperature": 0.0, "avg_logprob": -0.06512065653531056, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01614205725491047}, {"id": 295, "seek": 119664, "start": 1208.64, "end": 1212.64, "text": " with, but a lot of characters to encode and decode, right?", "tokens": [50964, 365, 11, 457, 257, 688, 295, 4342, 281, 2058, 1429, 293, 979, 1429, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06512065653531056, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01614205725491047}, {"id": 296, "seek": 119664, "start": 1212.64, "end": 1219.64, "text": " If we have, if we work with maybe a word level tokenizer, that means we have a ton, like", "tokens": [51164, 759, 321, 362, 11, 498, 321, 589, 365, 1310, 257, 1349, 1496, 14862, 6545, 11, 300, 1355, 321, 362, 257, 2952, 11, 411, 51514], "temperature": 0.0, "avg_logprob": -0.06512065653531056, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01614205725491047}, {"id": 297, "seek": 119664, "start": 1219.64, "end": 1224.64, "text": " every single word in the English language, I mean, if you're working with multiple languages,", "tokens": [51514, 633, 2167, 1349, 294, 264, 3669, 2856, 11, 286, 914, 11, 498, 291, 434, 1364, 365, 3866, 8650, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06512065653531056, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01614205725491047}, {"id": 298, "seek": 122464, "start": 1224.64, "end": 1230.64, "text": " this could be like, you know, a lot, very large amount of tokens.", "tokens": [50364, 341, 727, 312, 411, 11, 291, 458, 11, 257, 688, 11, 588, 2416, 2372, 295, 22667, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06586027145385742, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.03513319045305252}, {"id": 299, "seek": 122464, "start": 1230.64, "end": 1235.64, "text": " So you're going to have like maybe millions or billions or trillions if you're, if you're", "tokens": [50664, 407, 291, 434, 516, 281, 362, 411, 1310, 6803, 420, 17375, 420, 504, 46279, 498, 291, 434, 11, 498, 291, 434, 50914], "temperature": 0.0, "avg_logprob": -0.06586027145385742, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.03513319045305252}, {"id": 300, "seek": 122464, "start": 1235.64, "end": 1236.64, "text": " doing something weird.", "tokens": [50914, 884, 746, 3657, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06586027145385742, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.03513319045305252}, {"id": 301, "seek": 122464, "start": 1236.64, "end": 1242.64, "text": " But in that case, you're going to have a way smaller set to work with.", "tokens": [50964, 583, 294, 300, 1389, 11, 291, 434, 516, 281, 362, 257, 636, 4356, 992, 281, 589, 365, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06586027145385742, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.03513319045305252}, {"id": 302, "seek": 122464, "start": 1242.64, "end": 1248.64, "text": " So you're going to have very large vocabulary, but a very small amount to encode and decode.", "tokens": [51264, 407, 291, 434, 516, 281, 362, 588, 2416, 19864, 11, 457, 257, 588, 1359, 2372, 281, 2058, 1429, 293, 979, 1429, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06586027145385742, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.03513319045305252}, {"id": 303, "seek": 124864, "start": 1248.64, "end": 1254.64, "text": " So if you have a subword tokenizer, that means you're going to be somewhere in between a character", "tokens": [50364, 407, 498, 291, 362, 257, 1422, 7462, 14862, 6545, 11, 300, 1355, 291, 434, 516, 281, 312, 4079, 294, 1296, 257, 2517, 50664], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 304, "seek": 124864, "start": 1254.64, "end": 1257.64, "text": " level and a word level tokenizer, if that makes sense.", "tokens": [50664, 1496, 293, 257, 1349, 1496, 14862, 6545, 11, 498, 300, 1669, 2020, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 305, "seek": 124864, "start": 1257.64, "end": 1262.64, "text": " So in the context of language models, it's really important that we're efficient with our data", "tokens": [50814, 407, 294, 264, 4319, 295, 2856, 5245, 11, 309, 311, 534, 1021, 300, 321, 434, 7148, 365, 527, 1412, 51064], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 306, "seek": 124864, "start": 1262.64, "end": 1265.64, "text": " and just having a giant string might not work the best.", "tokens": [51064, 293, 445, 1419, 257, 7410, 6798, 1062, 406, 589, 264, 1151, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 307, "seek": 124864, "start": 1265.64, "end": 1270.64, "text": " And we're going to be using a machine learning framework called pi torch or torch.", "tokens": [51214, 400, 321, 434, 516, 281, 312, 1228, 257, 3479, 2539, 8388, 1219, 3895, 27822, 420, 27822, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 308, "seek": 124864, "start": 1270.64, "end": 1273.64, "text": " So I've imported this right here.", "tokens": [51464, 407, 286, 600, 25524, 341, 558, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07148384820847284, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.02517288364470005}, {"id": 309, "seek": 127364, "start": 1274.64, "end": 1281.64, "text": " And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well.", "tokens": [50414, 400, 1238, 709, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 4813, 257, 688, 295, 264, 5221, 11, 257, 688, 295, 264, 33400, 337, 505, 382, 731, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 310, "seek": 127364, "start": 1281.64, "end": 1288.64, "text": " A lot of the linear algebra, which involves a type of data structure called tensors.", "tokens": [50764, 316, 688, 295, 264, 8213, 21989, 11, 597, 11626, 257, 2010, 295, 1412, 3877, 1219, 10688, 830, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 311, "seek": 127364, "start": 1288.64, "end": 1290.64, "text": " So tensors are pretty much matrices.", "tokens": [51114, 407, 10688, 830, 366, 1238, 709, 32284, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 312, "seek": 127364, "start": 1290.64, "end": 1292.64, "text": " If you're not familiar with those, that's fine.", "tokens": [51214, 759, 291, 434, 406, 4963, 365, 729, 11, 300, 311, 2489, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 313, "seek": 127364, "start": 1292.64, "end": 1294.64, "text": " We'll go over them more in the course.", "tokens": [51314, 492, 603, 352, 670, 552, 544, 294, 264, 1164, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 314, "seek": 127364, "start": 1294.64, "end": 1301.64, "text": " But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with.", "tokens": [51414, 583, 1238, 709, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 445, 829, 1203, 1854, 295, 257, 40863, 370, 300, 309, 311, 3571, 337, 3895, 27822, 281, 589, 365, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09949428316146608, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.13653020560741425}, {"id": 315, "seek": 130164, "start": 1301.64, "end": 1303.64, "text": " So I'm going to go ahead and delete these here.", "tokens": [50364, 407, 286, 478, 516, 281, 352, 2286, 293, 12097, 613, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 316, "seek": 130164, "start": 1303.64, "end": 1309.64, "text": " And all we can do is just make our data element.", "tokens": [50464, 400, 439, 321, 393, 360, 307, 445, 652, 527, 1412, 4478, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 317, "seek": 130164, "start": 1309.64, "end": 1313.64, "text": " We could this is going to be the entire text data of the entire Wizard of Oz.", "tokens": [50764, 492, 727, 341, 307, 516, 281, 312, 264, 2302, 2487, 1412, 295, 264, 2302, 37449, 295, 29843, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 318, "seek": 130164, "start": 1313.64, "end": 1323.64, "text": " So we could go ahead and make this data equals and we're going to go torch tensor.", "tokens": [50964, 407, 321, 727, 352, 2286, 293, 652, 341, 1412, 6915, 293, 321, 434, 516, 281, 352, 27822, 40863, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 319, "seek": 130164, "start": 1323.64, "end": 1326.64, "text": " And then we're going to go and code.", "tokens": [51464, 400, 550, 321, 434, 516, 281, 352, 293, 3089, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 320, "seek": 130164, "start": 1326.64, "end": 1328.64, "text": " We're going to put the text inside of that.", "tokens": [51614, 492, 434, 516, 281, 829, 264, 2487, 1854, 295, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1146022379398346, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004754580091685057}, {"id": 321, "seek": 132864, "start": 1328.64, "end": 1331.64, "text": " So we're going to go ahead and encode this text right here.", "tokens": [50364, 407, 321, 434, 516, 281, 352, 2286, 293, 2058, 1429, 341, 2487, 558, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07261988851759169, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0070109921507537365}, {"id": 322, "seek": 132864, "start": 1331.64, "end": 1344.64, "text": " And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long.", "tokens": [50514, 400, 321, 434, 516, 281, 652, 988, 300, 321, 362, 264, 558, 1412, 2010, 11, 597, 307, 257, 27822, 5893, 938, 1412, 2010, 6915, 27822, 5893, 938, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07261988851759169, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0070109921507537365}, {"id": 323, "seek": 132864, "start": 1344.64, "end": 1350.64, "text": " This basically means we're just going to have this as a super long sequence of integers.", "tokens": [51164, 639, 1936, 1355, 321, 434, 445, 516, 281, 362, 341, 382, 257, 1687, 938, 8310, 295, 41674, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07261988851759169, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0070109921507537365}, {"id": 324, "seek": 132864, "start": 1350.64, "end": 1357.64, "text": " And yeah, let's go see what we can do with this torch tensor element right here.", "tokens": [51464, 400, 1338, 11, 718, 311, 352, 536, 437, 321, 393, 360, 365, 341, 27822, 40863, 4478, 558, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07261988851759169, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0070109921507537365}, {"id": 325, "seek": 135764, "start": 1357.64, "end": 1364.64, "text": " So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data.", "tokens": [50364, 407, 286, 600, 445, 3720, 257, 707, 4482, 5629, 689, 321, 393, 445, 4482, 484, 264, 700, 2319, 4342, 420, 2319, 41674, 295, 341, 1412, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08202544956991117, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.003375997068360448}, {"id": 326, "seek": 135764, "start": 1364.64, "end": 1368.64, "text": " So it's pretty much the same thing in terms of working with arrays.", "tokens": [50714, 407, 309, 311, 1238, 709, 264, 912, 551, 294, 2115, 295, 1364, 365, 41011, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08202544956991117, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.003375997068360448}, {"id": 327, "seek": 135764, "start": 1368.64, "end": 1379.64, "text": " It's just a different type of data structure in the context of pi torch sort of easier to work within that way.", "tokens": [50914, 467, 311, 445, 257, 819, 2010, 295, 1412, 3877, 294, 264, 4319, 295, 3895, 27822, 1333, 295, 3571, 281, 589, 1951, 300, 636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08202544956991117, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.003375997068360448}, {"id": 328, "seek": 137964, "start": 1379.64, "end": 1391.64, "text": " Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot.", "tokens": [50364, 17741, 27822, 307, 445, 10029, 16908, 937, 926, 10688, 830, 293, 42626, 552, 11, 725, 71, 569, 278, 11, 4473, 10139, 1860, 11, 30955, 11, 884, 5893, 3383, 11, 597, 300, 3263, 411, 257, 688, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13167166053701979, "compression_ratio": 1.76953125, "no_speech_prob": 0.8019447326660156}, {"id": 329, "seek": 137964, "start": 1391.64, "end": 1396.64, "text": " But we're going to go over some of this stuff later in the course just about how to do all this math.", "tokens": [50964, 583, 321, 434, 516, 281, 352, 670, 512, 295, 341, 1507, 1780, 294, 264, 1164, 445, 466, 577, 281, 360, 439, 341, 5221, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13167166053701979, "compression_ratio": 1.76953125, "no_speech_prob": 0.8019447326660156}, {"id": 330, "seek": 137964, "start": 1396.64, "end": 1408.64, "text": " We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff.", "tokens": [51214, 492, 434, 516, 281, 767, 352, 670, 5110, 322, 577, 281, 12972, 341, 8141, 538, 341, 8141, 11, 754, 498, 436, 434, 406, 264, 912, 3909, 293, 754, 5893, 15792, 3584, 11, 5893, 1674, 278, 11, 300, 733, 295, 1507, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13167166053701979, "compression_ratio": 1.76953125, "no_speech_prob": 0.8019447326660156}, {"id": 331, "seek": 140864, "start": 1408.64, "end": 1413.64, "text": " So next I'm going to talk about is something called validation and training splits.", "tokens": [50364, 407, 958, 286, 478, 516, 281, 751, 466, 307, 746, 1219, 24071, 293, 3097, 37741, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 332, "seek": 140864, "start": 1413.64, "end": 1420.64, "text": " So why don't we just, you know, use the entire text document and only train on that entire text corpus?", "tokens": [50614, 407, 983, 500, 380, 321, 445, 11, 291, 458, 11, 764, 264, 2302, 2487, 4166, 293, 787, 3847, 322, 300, 2302, 2487, 1181, 31624, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 333, "seek": 140864, "start": 1420.64, "end": 1421.64, "text": " Why don't we train on that?", "tokens": [50964, 1545, 500, 380, 321, 3847, 322, 300, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 334, "seek": 140864, "start": 1421.64, "end": 1427.64, "text": " Well, the reason we actually split into training and validation sets, I'm going to show you right here.", "tokens": [51014, 1042, 11, 264, 1778, 321, 767, 7472, 666, 3097, 293, 24071, 6352, 11, 286, 478, 516, 281, 855, 291, 558, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 335, "seek": 140864, "start": 1427.64, "end": 1429.64, "text": " So we have this giant text corpus.", "tokens": [51314, 407, 321, 362, 341, 7410, 2487, 1181, 31624, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 336, "seek": 140864, "start": 1429.64, "end": 1430.64, "text": " It's a super long text file.", "tokens": [51414, 467, 311, 257, 1687, 938, 2487, 3991, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 337, "seek": 140864, "start": 1430.64, "end": 1434.64, "text": " Think of it as a, you know, an essay, but a lot of pages.", "tokens": [51464, 6557, 295, 309, 382, 257, 11, 291, 458, 11, 364, 16238, 11, 457, 257, 688, 295, 7183, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07006700703355133, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.022279275581240654}, {"id": 338, "seek": 143464, "start": 1434.64, "end": 1440.64, "text": " So this is our entire corpus and we make our training set, you know, 80% of it.", "tokens": [50364, 407, 341, 307, 527, 2302, 1181, 31624, 293, 321, 652, 527, 3097, 992, 11, 291, 458, 11, 4688, 4, 295, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 339, "seek": 143464, "start": 1440.64, "end": 1441.64, "text": " So maybe this much.", "tokens": [50664, 407, 1310, 341, 709, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 340, "seek": 143464, "start": 1441.64, "end": 1445.64, "text": " And then the other validation is this 20% right here.", "tokens": [50714, 400, 550, 264, 661, 24071, 307, 341, 945, 4, 558, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 341, "seek": 143464, "start": 1445.64, "end": 1446.64, "text": " Okay.", "tokens": [50914, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 342, "seek": 143464, "start": 1446.64, "end": 1457.64, "text": " So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out.", "tokens": [50964, 407, 498, 321, 645, 281, 445, 3847, 322, 264, 2302, 551, 11, 934, 257, 1629, 1230, 295, 36540, 11, 309, 576, 445, 27478, 264, 2302, 2487, 2522, 293, 309, 576, 312, 1075, 281, 11, 291, 458, 11, 2935, 2464, 309, 11, 445, 2464, 309, 484, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 343, "seek": 143464, "start": 1457.64, "end": 1459.64, "text": " It would have it in the entire thing memorized.", "tokens": [51514, 467, 576, 362, 309, 294, 264, 2302, 551, 46677, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 344, "seek": 143464, "start": 1459.64, "end": 1461.64, "text": " It wouldn't really get anything useful out of that.", "tokens": [51614, 467, 2759, 380, 534, 483, 1340, 4420, 484, 295, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07824948526197864, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011685417965054512}, {"id": 345, "seek": 146164, "start": 1461.64, "end": 1464.64, "text": " You would only know this document.", "tokens": [50364, 509, 576, 787, 458, 341, 4166, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 346, "seek": 146164, "start": 1464.64, "end": 1469.64, "text": " But what the purpose of language modeling is, is to generate text that's like the training data.", "tokens": [50514, 583, 437, 264, 4334, 295, 2856, 15983, 307, 11, 307, 281, 8460, 2487, 300, 311, 411, 264, 3097, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 347, "seek": 146164, "start": 1469.64, "end": 1472.64, "text": " And this is exactly why we put into splits.", "tokens": [50764, 400, 341, 307, 2293, 983, 321, 829, 666, 37741, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 348, "seek": 146164, "start": 1472.64, "end": 1480.64, "text": " So if we, if we run our training split right here, it's only going to know 80% of that entire corpus.", "tokens": [50914, 407, 498, 321, 11, 498, 321, 1190, 527, 3097, 7472, 558, 510, 11, 309, 311, 787, 516, 281, 458, 4688, 4, 295, 300, 2302, 1181, 31624, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 349, "seek": 146164, "start": 1480.64, "end": 1484.64, "text": " And it's only going to generate on that 80% instead of the entire thing.", "tokens": [51314, 400, 309, 311, 787, 516, 281, 8460, 322, 300, 4688, 4, 2602, 295, 264, 2302, 551, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 350, "seek": 146164, "start": 1484.64, "end": 1489.64, "text": " And then we have our other 20%, which only knows 20% of the entire corpus.", "tokens": [51514, 400, 550, 321, 362, 527, 661, 945, 8923, 597, 787, 3255, 945, 4, 295, 264, 2302, 1181, 31624, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05147592226664225, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.031132856383919716}, {"id": 351, "seek": 148964, "start": 1489.64, "end": 1496.64, "text": " So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document.", "tokens": [50364, 407, 264, 1778, 983, 321, 360, 341, 307, 281, 652, 988, 300, 264, 10593, 366, 3845, 293, 406, 364, 1900, 5055, 295, 264, 3539, 4166, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11414358350965711, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.029298605397343636}, {"id": 352, "seek": 148964, "start": 1496.64, "end": 1500.64, "text": " We're trying to generate text that's like the document.", "tokens": [50714, 492, 434, 1382, 281, 8460, 2487, 300, 311, 411, 264, 4166, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11414358350965711, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.029298605397343636}, {"id": 353, "seek": 148964, "start": 1500.64, "end": 1508.64, "text": " Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare.", "tokens": [50914, 1743, 11, 337, 1365, 11, 294, 20667, 2741, 31852, 72, 311, 7991, 11, 415, 16329, 322, 22825, 282, 2487, 11, 364, 2302, 2522, 295, 22825, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11414358350965711, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.029298605397343636}, {"id": 354, "seek": 150864, "start": 1508.64, "end": 1515.64, "text": " And the point is to generate Shakespearean like text, but not exactly what it looked like.", "tokens": [50364, 400, 264, 935, 307, 281, 8460, 22825, 282, 411, 2487, 11, 457, 406, 2293, 437, 309, 2956, 411, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 355, "seek": 150864, "start": 1515.64, "end": 1521.64, "text": " Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right?", "tokens": [50714, 1726, 300, 1900, 11, 291, 458, 11, 3356, 11, 1360, 3876, 420, 411, 257, 1326, 4714, 3876, 295, 300, 2302, 1181, 31624, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 356, "seek": 150864, "start": 1521.64, "end": 1523.64, "text": " We're trying to generate text that's like it.", "tokens": [51014, 492, 434, 1382, 281, 8460, 2487, 300, 311, 411, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 357, "seek": 150864, "start": 1523.64, "end": 1529.64, "text": " So that's the entire reason, or at least that's most of the reason why we use train and vowel splits.", "tokens": [51114, 407, 300, 311, 264, 2302, 1778, 11, 420, 412, 1935, 300, 311, 881, 295, 264, 1778, 983, 321, 764, 3847, 293, 29410, 37741, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 358, "seek": 150864, "start": 1529.64, "end": 1533.64, "text": " So you might be wondering, you know, like, why is this even called the bi-gram language model?", "tokens": [51414, 407, 291, 1062, 312, 6359, 11, 291, 458, 11, 411, 11, 983, 307, 341, 754, 1219, 264, 3228, 12, 1342, 2856, 2316, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 359, "seek": 150864, "start": 1533.64, "end": 1536.64, "text": " I'm actually going to show you how that works right now.", "tokens": [51614, 286, 478, 767, 516, 281, 855, 291, 577, 300, 1985, 558, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09053941475328549, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3139156401157379}, {"id": 360, "seek": 153664, "start": 1536.64, "end": 1540.64, "text": " So if we go back to our whiteboard here, I've drawn a little sketch.", "tokens": [50364, 407, 498, 321, 352, 646, 281, 527, 2418, 3787, 510, 11, 286, 600, 10117, 257, 707, 12325, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 361, "seek": 153664, "start": 1540.64, "end": 1547.64, "text": " So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now.", "tokens": [50564, 407, 498, 321, 362, 341, 2522, 295, 2701, 11, 264, 1349, 7751, 11, 718, 311, 445, 584, 309, 11, 321, 500, 380, 362, 281, 2058, 1429, 309, 382, 604, 41674, 558, 586, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 362, "seek": 153664, "start": 1547.64, "end": 1549.64, "text": " We're just working with characters.", "tokens": [50914, 492, 434, 445, 1364, 365, 4342, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 363, "seek": 153664, "start": 1549.64, "end": 1552.64, "text": " Pretty much we have two, right?", "tokens": [51014, 10693, 709, 321, 362, 732, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 364, "seek": 153664, "start": 1552.64, "end": 1555.64, "text": " So by means to the by prefix means two.", "tokens": [51164, 407, 538, 1355, 281, 264, 538, 46969, 1355, 732, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 365, "seek": 153664, "start": 1555.64, "end": 1558.64, "text": " So we're going to, we're going to have a bi-gram.", "tokens": [51314, 407, 321, 434, 516, 281, 11, 321, 434, 516, 281, 362, 257, 3228, 12, 1342, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 366, "seek": 153664, "start": 1558.64, "end": 1563.64, "text": " So given maybe, I mean, there's nothing before an H in this content.", "tokens": [51464, 407, 2212, 1310, 11, 286, 914, 11, 456, 311, 1825, 949, 364, 389, 294, 341, 2701, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11969573974609375, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.01798209175467491}, {"id": 367, "seek": 156364, "start": 1563.64, "end": 1568.64, "text": " So we just assume that's the start of content, and then that's going to point to an H.", "tokens": [50364, 407, 321, 445, 6552, 300, 311, 264, 722, 295, 2701, 11, 293, 550, 300, 311, 516, 281, 935, 281, 364, 389, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 368, "seek": 156364, "start": 1568.64, "end": 1572.64, "text": " So H is the most likely to come after the start.", "tokens": [50614, 407, 389, 307, 264, 881, 3700, 281, 808, 934, 264, 722, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 369, "seek": 156364, "start": 1572.64, "end": 1579.64, "text": " And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L,", "tokens": [50814, 400, 550, 1310, 2212, 364, 389, 11, 321, 434, 516, 281, 362, 364, 462, 11, 550, 2212, 364, 462, 11, 321, 434, 516, 281, 362, 364, 441, 11, 550, 2212, 364, 441, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 370, "seek": 156364, "start": 1579.64, "end": 1582.64, "text": " we're going to have another L, and then L leads to O, right?", "tokens": [51164, 321, 434, 516, 281, 362, 1071, 441, 11, 293, 550, 441, 6689, 281, 422, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 371, "seek": 156364, "start": 1582.64, "end": 1588.64, "text": " So maybe there's going to be some probabilities associated with these.", "tokens": [51314, 407, 1310, 456, 311, 516, 281, 312, 512, 33783, 6615, 365, 613, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 372, "seek": 156364, "start": 1588.64, "end": 1591.64, "text": " So that's pretty much how it's how it's going to predict right now.", "tokens": [51614, 407, 300, 311, 1238, 709, 577, 309, 311, 577, 309, 311, 516, 281, 6069, 558, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07569590210914612, "compression_ratio": 1.9478260869565218, "no_speech_prob": 0.0023230293300002813}, {"id": 373, "seek": 159164, "start": 1591.64, "end": 1595.64, "text": " It's only going to consider the previous character to predict the next.", "tokens": [50364, 467, 311, 787, 516, 281, 1949, 264, 3894, 2517, 281, 6069, 264, 958, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08720569967109466, "compression_ratio": 1.8434782608695652, "no_speech_prob": 0.04207449406385422}, {"id": 374, "seek": 159164, "start": 1595.64, "end": 1598.64, "text": " So we have given this one, we predict the next.", "tokens": [50564, 407, 321, 362, 2212, 341, 472, 11, 321, 6069, 264, 958, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08720569967109466, "compression_ratio": 1.8434782608695652, "no_speech_prob": 0.04207449406385422}, {"id": 375, "seek": 159164, "start": 1598.64, "end": 1601.64, "text": " So there's two, which is why it's called bi-gram language model.", "tokens": [50714, 407, 456, 311, 732, 11, 597, 307, 983, 309, 311, 1219, 3228, 12, 1342, 2856, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08720569967109466, "compression_ratio": 1.8434782608695652, "no_speech_prob": 0.04207449406385422}, {"id": 376, "seek": 159164, "start": 1601.64, "end": 1608.64, "text": " So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want,", "tokens": [50864, 407, 286, 11200, 452, 6237, 3579, 510, 11, 457, 321, 434, 767, 516, 281, 352, 666, 577, 321, 393, 3847, 264, 3228, 12, 1342, 2856, 2316, 281, 360, 437, 321, 528, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08720569967109466, "compression_ratio": 1.8434782608695652, "no_speech_prob": 0.04207449406385422}, {"id": 377, "seek": 159164, "start": 1608.64, "end": 1615.64, "text": " how we can actually implement this into a neural network, an artificial neural network, and train it.", "tokens": [51214, 577, 321, 393, 767, 4445, 341, 666, 257, 18161, 3209, 11, 364, 11677, 18161, 3209, 11, 293, 3847, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08720569967109466, "compression_ratio": 1.8434782608695652, "no_speech_prob": 0.04207449406385422}, {"id": 378, "seek": 161564, "start": 1615.64, "end": 1624.64, "text": " So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here,", "tokens": [50364, 407, 321, 434, 516, 281, 483, 666, 746, 1219, 3461, 2744, 11, 597, 307, 1238, 709, 445, 1940, 257, 4974, 35623, 302, 484, 295, 341, 2302, 2487, 1181, 31624, 510, 11, 50814], "temperature": 0.0, "avg_logprob": -0.06642460154595776, "compression_ratio": 1.8516949152542372, "no_speech_prob": 0.13468942046165466}, {"id": 379, "seek": 161564, "start": 1624.64, "end": 1631.64, "text": " just a small snippet, and we're going to make some predictions and we're going to make some targets out of that.", "tokens": [50814, 445, 257, 1359, 35623, 302, 11, 293, 321, 434, 516, 281, 652, 512, 21264, 293, 321, 434, 516, 281, 652, 512, 12911, 484, 295, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06642460154595776, "compression_ratio": 1.8516949152542372, "no_speech_prob": 0.13468942046165466}, {"id": 380, "seek": 161564, "start": 1631.64, "end": 1638.64, "text": " So our block size is just a bunch of encoded characters or integers that we have predictions and targets.", "tokens": [51164, 407, 527, 3461, 2744, 307, 445, 257, 3840, 295, 2058, 12340, 4342, 420, 41674, 300, 321, 362, 21264, 293, 12911, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06642460154595776, "compression_ratio": 1.8516949152542372, "no_speech_prob": 0.13468942046165466}, {"id": 381, "seek": 161564, "start": 1638.64, "end": 1644.64, "text": " So let's say we take a small little size of maybe block size of five, okay?", "tokens": [51514, 407, 718, 311, 584, 321, 747, 257, 1359, 707, 2744, 295, 1310, 3461, 2744, 295, 1732, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.06642460154595776, "compression_ratio": 1.8516949152542372, "no_speech_prob": 0.13468942046165466}, {"id": 382, "seek": 164464, "start": 1644.64, "end": 1650.64, "text": " So we have this tiny little tensor of five integers and these are our predictions.", "tokens": [50364, 407, 321, 362, 341, 5870, 707, 40863, 295, 1732, 41674, 293, 613, 366, 527, 21264, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07588713765144348, "compression_ratio": 1.743455497382199, "no_speech_prob": 0.004330548457801342}, {"id": 383, "seek": 164464, "start": 1650.64, "end": 1662.64, "text": " So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one.", "tokens": [50664, 407, 2212, 512, 4319, 558, 510, 11, 321, 434, 516, 281, 312, 32884, 613, 293, 550, 321, 362, 527, 12911, 11, 597, 576, 312, 18687, 538, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07588713765144348, "compression_ratio": 1.743455497382199, "no_speech_prob": 0.004330548457801342}, {"id": 384, "seek": 164464, "start": 1662.64, "end": 1668.64, "text": " So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside.", "tokens": [51264, 407, 3449, 577, 510, 321, 362, 257, 1732, 293, 550, 510, 264, 1732, 307, 2380, 293, 550, 341, 6976, 307, 2380, 510, 293, 586, 309, 311, 1854, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07588713765144348, "compression_ratio": 1.743455497382199, "no_speech_prob": 0.004330548457801342}, {"id": 385, "seek": 166864, "start": 1668.64, "end": 1676.64, "text": " So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one.", "tokens": [50364, 407, 439, 321, 434, 884, 307, 445, 1940, 300, 3461, 490, 264, 21264, 293, 294, 1668, 281, 483, 264, 12911, 11, 321, 445, 18687, 300, 538, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09464186618202611, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.04601558297872543}, {"id": 386, "seek": 166864, "start": 1676.64, "end": 1679.64, "text": " So we're going to be accessing the same indices.", "tokens": [50764, 407, 321, 434, 516, 281, 312, 26440, 264, 912, 43840, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09464186618202611, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.04601558297872543}, {"id": 387, "seek": 166864, "start": 1679.64, "end": 1683.64, "text": " So at index zero, it's going to be five, index zero is going to be 67, right?", "tokens": [50914, 407, 412, 8186, 4018, 11, 309, 311, 516, 281, 312, 1732, 11, 8186, 4018, 307, 516, 281, 312, 23879, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.09464186618202611, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.04601558297872543}, {"id": 388, "seek": 166864, "start": 1683.64, "end": 1687.64, "text": " So 67 is following five in the bi-gram language model.", "tokens": [51114, 407, 23879, 307, 3480, 1732, 294, 264, 3228, 12, 1342, 2856, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09464186618202611, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.04601558297872543}, {"id": 389, "seek": 166864, "start": 1687.64, "end": 1690.64, "text": " So that's pretty much all we do.", "tokens": [51314, 407, 300, 311, 1238, 709, 439, 321, 360, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09464186618202611, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.04601558297872543}, {"id": 390, "seek": 169064, "start": 1690.64, "end": 1700.64, "text": " So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target.", "tokens": [50364, 407, 718, 311, 445, 574, 412, 577, 709, 295, 257, 2649, 307, 300, 3779, 1314, 490, 420, 577, 709, 1400, 307, 264, 17630, 1314, 490, 264, 3779, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11316049421155774, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.5883069634437561}, {"id": 391, "seek": 169064, "start": 1700.64, "end": 1705.64, "text": " And then we can optimize for reducing that error.", "tokens": [50864, 400, 550, 321, 393, 19719, 337, 12245, 300, 6713, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11316049421155774, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.5883069634437561}, {"id": 392, "seek": 169064, "start": 1705.64, "end": 1719.64, "text": " So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here.", "tokens": [51114, 407, 264, 881, 3875, 15329, 11420, 295, 341, 294, 264, 2517, 1496, 14862, 22525, 420, 264, 2517, 1496, 22667, 2831, 576, 312, 445, 2935, 341, 558, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11316049421155774, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.5883069634437561}, {"id": 393, "seek": 171964, "start": 1719.64, "end": 1723.64, "text": " We would take a little snippet random.", "tokens": [50364, 492, 576, 747, 257, 707, 35623, 302, 4974, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14658735593159994, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.04270395264029503}, {"id": 394, "seek": 171964, "start": 1723.64, "end": 1738.64, "text": " It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size.", "tokens": [50564, 467, 576, 312, 1238, 709, 445, 490, 264, 722, 420, 512, 2035, 445, 512, 35623, 302, 439, 264, 636, 490, 264, 722, 295, 264, 35623, 302, 493, 281, 3461, 2744, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14658735593159994, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.04270395264029503}, {"id": 395, "seek": 171964, "start": 1738.64, "end": 1743.64, "text": " So five.", "tokens": [51314, 407, 1732, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14658735593159994, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.04270395264029503}, {"id": 396, "seek": 171964, "start": 1743.64, "end": 1746.64, "text": " Ignore my terrible writing again.", "tokens": [51564, 24754, 418, 452, 6237, 3579, 797, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14658735593159994, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.04270395264029503}, {"id": 397, "seek": 174664, "start": 1746.64, "end": 1762.64, "text": " And then this one would just be it would just be one up to block size or five plus one.", "tokens": [50364, 400, 550, 341, 472, 576, 445, 312, 309, 576, 445, 312, 472, 493, 281, 3461, 2744, 420, 1732, 1804, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0884516904520434, "compression_ratio": 1.5729166666666667, "no_speech_prob": 0.006903044879436493}, {"id": 398, "seek": 174664, "start": 1762.64, "end": 1764.64, "text": " So we'll be up to six, right?", "tokens": [51164, 407, 321, 603, 312, 493, 281, 2309, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.0884516904520434, "compression_ratio": 1.5729166666666667, "no_speech_prob": 0.006903044879436493}, {"id": 399, "seek": 174664, "start": 1764.64, "end": 1766.64, "text": " And that's that's pretty much all we do.", "tokens": [51264, 400, 300, 311, 300, 311, 1238, 709, 439, 321, 360, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0884516904520434, "compression_ratio": 1.5729166666666667, "no_speech_prob": 0.006903044879436493}, {"id": 400, "seek": 174664, "start": 1766.64, "end": 1768.64, "text": " This is exactly what it's going to look like in the code.", "tokens": [51364, 639, 307, 2293, 437, 309, 311, 516, 281, 574, 411, 294, 264, 3089, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0884516904520434, "compression_ratio": 1.5729166666666667, "no_speech_prob": 0.006903044879436493}, {"id": 401, "seek": 174664, "start": 1768.64, "end": 1773.64, "text": " So I've written some code here that does exactly what we just talked about in Python.", "tokens": [51464, 407, 286, 600, 3720, 512, 3089, 510, 300, 775, 2293, 437, 321, 445, 2825, 466, 294, 15329, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0884516904520434, "compression_ratio": 1.5729166666666667, "no_speech_prob": 0.006903044879436493}, {"id": 402, "seek": 177364, "start": 1773.64, "end": 1781.64, "text": " So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger.", "tokens": [50364, 407, 286, 6964, 341, 3461, 2744, 2681, 281, 3180, 445, 370, 291, 393, 733, 295, 536, 437, 341, 1542, 411, 322, 257, 4833, 4373, 11, 257, 707, 857, 4833, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1238964494452419, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.07915999740362167}, {"id": 403, "seek": 177364, "start": 1781.64, "end": 1791.64, "text": " And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one.", "tokens": [50764, 400, 445, 437, 321, 4114, 558, 456, 294, 264, 22125, 88, 391, 21060, 341, 2535, 4018, 493, 281, 3461, 493, 281, 3461, 2744, 293, 550, 18687, 538, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1238964494452419, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.07915999740362167}, {"id": 404, "seek": 177364, "start": 1791.64, "end": 1796.64, "text": " So we make it position one up to block size plus one little offset there.", "tokens": [51264, 407, 321, 652, 309, 2535, 472, 493, 281, 3461, 2744, 1804, 472, 707, 18687, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1238964494452419, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.07915999740362167}, {"id": 405, "seek": 179664, "start": 1796.64, "end": 1808.64, "text": " And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are.", "tokens": [50364, 400, 321, 1238, 709, 445, 4114, 760, 510, 1783, 382, 527, 21264, 382, 293, 983, 382, 527, 12911, 11, 293, 550, 445, 257, 707, 337, 6367, 281, 855, 437, 264, 17630, 293, 437, 264, 12911, 366, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12266044805545619, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.62580806016922}, {"id": 406, "seek": 179664, "start": 1808.64, "end": 1812.64, "text": " So this is what this looks like in Python, right, we can do predictions.", "tokens": [50964, 407, 341, 307, 437, 341, 1542, 411, 294, 15329, 11, 558, 11, 321, 393, 360, 21264, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12266044805545619, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.62580806016922}, {"id": 407, "seek": 179664, "start": 1812.64, "end": 1815.64, "text": " But this isn't really scalable yet.", "tokens": [51164, 583, 341, 1943, 380, 534, 38481, 1939, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12266044805545619, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.62580806016922}, {"id": 408, "seek": 179664, "start": 1815.64, "end": 1818.64, "text": " This is sequential right sequential.", "tokens": [51314, 639, 307, 42881, 558, 42881, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12266044805545619, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.62580806016922}, {"id": 409, "seek": 179664, "start": 1818.64, "end": 1825.64, "text": " It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly.", "tokens": [51464, 467, 307, 1071, 636, 295, 16141, 437, 264, 13199, 775, 13199, 393, 360, 257, 688, 295, 3997, 7705, 588, 2661, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12266044805545619, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.62580806016922}, {"id": 410, "seek": 182564, "start": 1825.64, "end": 1830.64, "text": " That only happens sequentially it's this one and this task and this task and this task, right.", "tokens": [50364, 663, 787, 2314, 5123, 3137, 309, 311, 341, 472, 293, 341, 5633, 293, 341, 5633, 293, 341, 5633, 11, 558, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11676529778374566, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.022969765588641167}, {"id": 411, "seek": 182564, "start": 1830.64, "end": 1837.64, "text": " But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel.", "tokens": [50614, 583, 365, 18407, 82, 11, 291, 393, 360, 257, 707, 857, 544, 18587, 9608, 11, 457, 588, 11, 588, 2661, 11, 420, 294, 8952, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11676529778374566, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.022969765588641167}, {"id": 412, "seek": 182564, "start": 1837.64, "end": 1852.64, "text": " So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them.", "tokens": [50964, 407, 321, 393, 360, 257, 3840, 295, 588, 1359, 420, 406, 24903, 379, 3997, 24903, 11, 293, 257, 3840, 295, 819, 707, 27751, 300, 3212, 380, 382, 665, 11, 457, 456, 311, 9131, 295, 552, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11676529778374566, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.022969765588641167}, {"id": 413, "seek": 185264, "start": 1852.64, "end": 1865.64, "text": " So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot.", "tokens": [50364, 407, 1238, 709, 437, 321, 393, 360, 307, 321, 393, 747, 1184, 295, 613, 707, 8474, 11, 293, 550, 321, 393, 8630, 552, 293, 2944, 613, 281, 264, 18407, 281, 4373, 527, 3097, 257, 688, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08571420626693897, "compression_ratio": 1.5825242718446602, "no_speech_prob": 0.22800280153751373}, {"id": 414, "seek": 185264, "start": 1865.64, "end": 1868.64, "text": " So I'm going to illustrate that for you right now.", "tokens": [51014, 407, 286, 478, 516, 281, 23221, 300, 337, 291, 558, 586, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08571420626693897, "compression_ratio": 1.5825242718446602, "no_speech_prob": 0.22800280153751373}, {"id": 415, "seek": 185264, "start": 1868.64, "end": 1870.64, "text": " So let's just say we have a block.", "tokens": [51164, 407, 718, 311, 445, 584, 321, 362, 257, 3461, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08571420626693897, "compression_ratio": 1.5825242718446602, "no_speech_prob": 0.22800280153751373}, {"id": 416, "seek": 185264, "start": 1870.64, "end": 1872.64, "text": " Okay, block looks like this.", "tokens": [51264, 1033, 11, 3461, 1542, 411, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08571420626693897, "compression_ratio": 1.5825242718446602, "no_speech_prob": 0.22800280153751373}, {"id": 417, "seek": 185264, "start": 1872.64, "end": 1879.64, "text": " And we have some we have some integers in between here.", "tokens": [51364, 400, 321, 362, 512, 321, 362, 512, 41674, 294, 1296, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08571420626693897, "compression_ratio": 1.5825242718446602, "no_speech_prob": 0.22800280153751373}, {"id": 418, "seek": 187964, "start": 1879.64, "end": 1882.64, "text": " So this is a block.", "tokens": [50364, 407, 341, 307, 257, 3461, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 419, "seek": 187964, "start": 1882.64, "end": 1884.64, "text": " Okay.", "tokens": [50514, 1033, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 420, "seek": 187964, "start": 1884.64, "end": 1888.64, "text": " Now, if we want to make multiple of these, we're just going to stack them.", "tokens": [50614, 823, 11, 498, 321, 528, 281, 652, 3866, 295, 613, 11, 321, 434, 445, 516, 281, 8630, 552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 421, "seek": 187964, "start": 1888.64, "end": 1892.64, "text": " So we're going to make another one.", "tokens": [50814, 407, 321, 434, 516, 281, 652, 1071, 472, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 422, "seek": 187964, "start": 1892.64, "end": 1895.64, "text": " Another one.", "tokens": [51014, 3996, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 423, "seek": 187964, "start": 1895.64, "end": 1896.64, "text": " Another one.", "tokens": [51164, 3996, 472, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 424, "seek": 187964, "start": 1896.64, "end": 1898.64, "text": " So let's say we have four batches.", "tokens": [51214, 407, 718, 311, 584, 321, 362, 1451, 15245, 279, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 425, "seek": 187964, "start": 1898.64, "end": 1899.64, "text": " Okay.", "tokens": [51314, 1033, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 426, "seek": 187964, "start": 1899.64, "end": 1900.64, "text": " Or sorry, four blocks.", "tokens": [51364, 1610, 2597, 11, 1451, 8474, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 427, "seek": 187964, "start": 1900.64, "end": 1905.64, "text": " So we have four different blocks that are just stacked on top of each other.", "tokens": [51414, 407, 321, 362, 1451, 819, 8474, 300, 366, 445, 28867, 322, 1192, 295, 1184, 661, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09414821741532306, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.006191825494170189}, {"id": 428, "seek": 190564, "start": 1905.64, "end": 1910.64, "text": " And we can represent this as a new hyper parameter called batch size.", "tokens": [50364, 400, 321, 393, 2906, 341, 382, 257, 777, 9848, 13075, 1219, 15245, 2744, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05095746216264743, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13833901286125183}, {"id": 429, "seek": 190564, "start": 1910.64, "end": 1915.64, "text": " This is going to tell us how many of these sequences can we actually process in parallel.", "tokens": [50614, 639, 307, 516, 281, 980, 505, 577, 867, 295, 613, 22978, 393, 321, 767, 1399, 294, 8952, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05095746216264743, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13833901286125183}, {"id": 430, "seek": 190564, "start": 1915.64, "end": 1923.64, "text": " So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time.", "tokens": [50864, 407, 264, 3461, 2744, 307, 264, 4641, 295, 1184, 8310, 11, 293, 264, 15245, 2744, 307, 577, 867, 295, 613, 366, 321, 767, 884, 412, 264, 912, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.05095746216264743, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13833901286125183}, {"id": 431, "seek": 190564, "start": 1923.64, "end": 1927.64, "text": " So this is a really good way to scale language models.", "tokens": [51264, 407, 341, 307, 257, 534, 665, 636, 281, 4373, 2856, 5245, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05095746216264743, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13833901286125183}, {"id": 432, "seek": 190564, "start": 1927.64, "end": 1932.64, "text": " And without these, you can't really expect any fast training or good performance at all.", "tokens": [51464, 400, 1553, 613, 11, 291, 393, 380, 534, 2066, 604, 2370, 3097, 420, 665, 3389, 412, 439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.05095746216264743, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13833901286125183}, {"id": 433, "seek": 193264, "start": 1932.64, "end": 1939.64, "text": " So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process.", "tokens": [50364, 407, 321, 445, 1437, 670, 577, 321, 393, 767, 483, 15245, 279, 420, 2831, 577, 321, 393, 764, 15245, 279, 281, 21341, 264, 3097, 1399, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11159983548251065, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.036207910627126694}, {"id": 434, "seek": 193264, "start": 1939.64, "end": 1944.64, "text": " And we can, it just takes one line to do this actually.", "tokens": [50714, 400, 321, 393, 11, 309, 445, 2516, 472, 1622, 281, 360, 341, 767, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11159983548251065, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.036207910627126694}, {"id": 435, "seek": 193264, "start": 1944.64, "end": 1957.64, "text": " So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation.", "tokens": [50964, 407, 439, 321, 362, 281, 360, 307, 818, 341, 707, 2445, 510, 1566, 498, 29777, 7509, 5893, 27822, 5893, 29777, 7509, 307, 2435, 11, 321, 603, 445, 1520, 498, 264, 26039, 390, 2435, 2361, 322, 428, 29777, 7509, 13260, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11159983548251065, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.036207910627126694}, {"id": 436, "seek": 195764, "start": 1958.64, "end": 1964.64, "text": " And if it's available, like it says it's available, we'll set the device to CUDA else CPU.", "tokens": [50414, 400, 498, 309, 311, 2435, 11, 411, 309, 1619, 309, 311, 2435, 11, 321, 603, 992, 264, 4302, 281, 29777, 7509, 1646, 13199, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08178464104147519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.3958492875099182}, {"id": 437, "seek": 195764, "start": 1964.64, "end": 1967.64, "text": " So we're going to go and print out the device here.", "tokens": [50714, 407, 321, 434, 516, 281, 352, 293, 4482, 484, 264, 4302, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08178464104147519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.3958492875099182}, {"id": 438, "seek": 195764, "start": 1967.64, "end": 1970.64, "text": " So that's going to run and we get CUDA.", "tokens": [50864, 407, 300, 311, 516, 281, 1190, 293, 321, 483, 29777, 7509, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08178464104147519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.3958492875099182}, {"id": 439, "seek": 195764, "start": 1970.64, "end": 1975.64, "text": " So that means we can use the GPU for a lot of our processing here.", "tokens": [51014, 407, 300, 1355, 321, 393, 764, 264, 18407, 337, 257, 688, 295, 527, 9007, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08178464104147519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.3958492875099182}, {"id": 440, "seek": 195764, "start": 1975.64, "end": 1982.64, "text": " And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size.", "tokens": [51264, 400, 1339, 321, 434, 510, 11, 286, 478, 767, 516, 281, 1286, 493, 341, 9848, 13075, 3461, 2744, 493, 281, 264, 1192, 3461, 2744, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08178464104147519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.3958492875099182}, {"id": 441, "seek": 198264, "start": 1982.64, "end": 1987.64, "text": " And then we're going to use batch size, which is how many blocks we're doing in parallel.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 764, 15245, 2744, 11, 597, 307, 577, 867, 8474, 321, 434, 884, 294, 8952, 13, 50614], "temperature": 0.0, "avg_logprob": -0.062352447509765624, "compression_ratio": 1.692, "no_speech_prob": 0.07474129647016525}, {"id": 442, "seek": 198264, "start": 1987.64, "end": 1990.64, "text": " And we're just going to make this four for now.", "tokens": [50614, 400, 321, 434, 445, 516, 281, 652, 341, 1451, 337, 586, 13, 50764], "temperature": 0.0, "avg_logprob": -0.062352447509765624, "compression_ratio": 1.692, "no_speech_prob": 0.07474129647016525}, {"id": 443, "seek": 198264, "start": 1990.64, "end": 1994.64, "text": " So these are our two hyper parameters that are very, very important for training.", "tokens": [50764, 407, 613, 366, 527, 732, 9848, 9834, 300, 366, 588, 11, 588, 1021, 337, 3097, 13, 50964], "temperature": 0.0, "avg_logprob": -0.062352447509765624, "compression_ratio": 1.692, "no_speech_prob": 0.07474129647016525}, {"id": 444, "seek": 198264, "start": 1994.64, "end": 2008.64, "text": " And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it.", "tokens": [50964, 400, 291, 603, 536, 300, 983, 613, 1813, 709, 544, 1021, 1780, 562, 321, 4373, 493, 264, 1412, 293, 764, 544, 3997, 15902, 281, 3847, 293, 1466, 264, 8294, 295, 264, 2856, 2361, 322, 264, 2487, 300, 321, 976, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.062352447509765624, "compression_ratio": 1.692, "no_speech_prob": 0.07474129647016525}, {"id": 445, "seek": 200864, "start": 2008.64, "end": 2019.64, "text": " And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first.", "tokens": [50364, 400, 498, 309, 1177, 380, 589, 558, 1314, 11, 498, 309, 311, 257, 777, 22125, 88, 391, 21060, 1177, 380, 589, 558, 1314, 11, 286, 1116, 2748, 445, 8850, 1969, 383, 281, 10373, 341, 2045, 309, 257, 1326, 1413, 1062, 406, 589, 264, 700, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14947716552432222, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.19421230256557465}, {"id": 446, "seek": 200864, "start": 2019.64, "end": 2023.64, "text": " It'll shut down and you just go up Jupyter notebook again and then enter.", "tokens": [50914, 467, 603, 5309, 760, 293, 291, 445, 352, 493, 22125, 88, 391, 21060, 797, 293, 550, 3242, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14947716552432222, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.19421230256557465}, {"id": 447, "seek": 200864, "start": 2023.64, "end": 2032.64, "text": " And then after this is done, you should be able to just restart that and it will work.", "tokens": [51114, 400, 550, 934, 341, 307, 1096, 11, 291, 820, 312, 1075, 281, 445, 21022, 300, 293, 309, 486, 589, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14947716552432222, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.19421230256557465}, {"id": 448, "seek": 200864, "start": 2032.64, "end": 2033.64, "text": " Hopefully.", "tokens": [51564, 10429, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14947716552432222, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.19421230256557465}, {"id": 449, "seek": 200864, "start": 2033.64, "end": 2035.64, "text": " There we go.", "tokens": [51614, 821, 321, 352, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14947716552432222, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.19421230256557465}, {"id": 450, "seek": 203564, "start": 2035.64, "end": 2040.64, "text": " So I go in restart and clear outputs.", "tokens": [50364, 407, 286, 352, 294, 21022, 293, 1850, 23930, 13, 50614], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 451, "seek": 203564, "start": 2040.64, "end": 2042.64, "text": " And we can run that.", "tokens": [50614, 400, 321, 393, 1190, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 452, "seek": 203564, "start": 2042.64, "end": 2043.64, "text": " See, we get boo.", "tokens": [50714, 3008, 11, 321, 483, 23113, 13, 50764], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 453, "seek": 203564, "start": 2043.64, "end": 2045.64, "text": " So awesome.", "tokens": [50764, 407, 3476, 13, 50864], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 454, "seek": 203564, "start": 2045.64, "end": 2050.6400000000003, "text": " Now, let's try to do some actual cool pie torch stop.", "tokens": [50864, 823, 11, 718, 311, 853, 281, 360, 512, 3539, 1627, 1730, 27822, 1590, 13, 51114], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 455, "seek": 203564, "start": 2050.6400000000003, "end": 2054.6400000000003, "text": " So we're going to go in and import torch here.", "tokens": [51114, 407, 321, 434, 516, 281, 352, 294, 293, 974, 27822, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 456, "seek": 203564, "start": 2054.6400000000003, "end": 2059.6400000000003, "text": " And then let's go ahead and try this random feature.", "tokens": [51314, 400, 550, 718, 311, 352, 2286, 293, 853, 341, 4974, 4111, 13, 51564], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 457, "seek": 203564, "start": 2059.6400000000003, "end": 2061.6400000000003, "text": " So you go random.", "tokens": [51564, 407, 291, 352, 4974, 13, 51664], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 458, "seek": 203564, "start": 2061.6400000000003, "end": 2064.6400000000003, "text": " We'll do equals torch dot random.", "tokens": [51664, 492, 603, 360, 6915, 27822, 5893, 4974, 13, 51814], "temperature": 0.0, "avg_logprob": -0.284937888697574, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.032086487859487534}, {"id": 459, "seek": 206464, "start": 2064.64, "end": 2070.64, "text": " And then let's say we go minus 100 to 100.", "tokens": [50364, 400, 550, 718, 311, 584, 321, 352, 3175, 2319, 281, 2319, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 460, "seek": 206464, "start": 2070.64, "end": 2074.64, "text": " And then in brackets, we go six, just like that.", "tokens": [50664, 400, 550, 294, 26179, 11, 321, 352, 2309, 11, 445, 411, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 461, "seek": 206464, "start": 2074.64, "end": 2080.64, "text": " So if we want to print this out here, or we could just go random like that.", "tokens": [50864, 407, 498, 321, 528, 281, 4482, 341, 484, 510, 11, 420, 321, 727, 445, 352, 4974, 411, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 462, "seek": 206464, "start": 2080.64, "end": 2082.64, "text": " Run this block first.", "tokens": [51164, 8950, 341, 3461, 700, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 463, "seek": 206464, "start": 2082.64, "end": 2083.64, "text": " Good.", "tokens": [51264, 2205, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 464, "seek": 206464, "start": 2083.64, "end": 2084.64, "text": " And boom.", "tokens": [51314, 400, 9351, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 465, "seek": 206464, "start": 2084.64, "end": 2087.64, "text": " So we get a tensor type.", "tokens": [51364, 407, 321, 483, 257, 40863, 2010, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 466, "seek": 206464, "start": 2087.64, "end": 2091.64, "text": " And all these numbers are we have we have six of them.", "tokens": [51514, 400, 439, 613, 3547, 366, 321, 362, 321, 362, 2309, 295, 552, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 467, "seek": 206464, "start": 2091.64, "end": 2093.64, "text": " So 123456.", "tokens": [51714, 407, 34466, 8465, 21, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16896444378477155, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0194123312830925}, {"id": 468, "seek": 209364, "start": 2093.64, "end": 2096.64, "text": " And they're between negative 100 and 100.", "tokens": [50364, 400, 436, 434, 1296, 3671, 2319, 293, 2319, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 469, "seek": 209364, "start": 2096.64, "end": 2102.64, "text": " So we're going to have to keep this in mind right here when we're getting our random", "tokens": [50514, 407, 321, 434, 516, 281, 362, 281, 1066, 341, 294, 1575, 558, 510, 562, 321, 434, 1242, 527, 4974, 50814], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 470, "seek": 209364, "start": 2102.64, "end": 2105.64, "text": " batches from this giant text corpus.", "tokens": [50814, 15245, 279, 490, 341, 7410, 2487, 1181, 31624, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 471, "seek": 209364, "start": 2105.64, "end": 2107.64, "text": " So let's try out a new one.", "tokens": [50964, 407, 718, 311, 853, 484, 257, 777, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 472, "seek": 209364, "start": 2107.64, "end": 2109.64, "text": " Let's just try.", "tokens": [51064, 961, 311, 445, 853, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 473, "seek": 209364, "start": 2109.64, "end": 2111.64, "text": " We can make we can make tensors.", "tokens": [51164, 492, 393, 652, 321, 393, 652, 10688, 830, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 474, "seek": 209364, "start": 2111.64, "end": 2112.64, "text": " We've done this before.", "tokens": [51264, 492, 600, 1096, 341, 949, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 475, "seek": 209364, "start": 2112.64, "end": 2117.64, "text": " So you do tensor equals torch dot tensor.", "tokens": [51314, 407, 291, 360, 40863, 6915, 27822, 5893, 40863, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12454126214468351, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.009556618519127369}, {"id": 476, "seek": 211764, "start": 2117.64, "end": 2130.64, "text": " So if you go 0.1, 1.2, here, I'll just copy and paste one right here.", "tokens": [50364, 407, 498, 291, 352, 1958, 13, 16, 11, 502, 13, 17, 11, 510, 11, 286, 603, 445, 5055, 293, 9163, 472, 558, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22133778744056576, "compression_ratio": 1.3255813953488371, "no_speech_prob": 0.02096143178641796}, {"id": 477, "seek": 211764, "start": 2130.64, "end": 2132.64, "text": " So we do this.", "tokens": [51014, 407, 321, 360, 341, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22133778744056576, "compression_ratio": 1.3255813953488371, "no_speech_prob": 0.02096143178641796}, {"id": 478, "seek": 211764, "start": 2132.64, "end": 2138.64, "text": " And we can just do tensor and we'll get exactly this.", "tokens": [51114, 400, 321, 393, 445, 360, 40863, 293, 321, 603, 483, 2293, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22133778744056576, "compression_ratio": 1.3255813953488371, "no_speech_prob": 0.02096143178641796}, {"id": 479, "seek": 211764, "start": 2138.64, "end": 2145.64, "text": " So we get a three by two matrix.", "tokens": [51414, 407, 321, 483, 257, 1045, 538, 732, 8141, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22133778744056576, "compression_ratio": 1.3255813953488371, "no_speech_prob": 0.02096143178641796}, {"id": 480, "seek": 214564, "start": 2145.64, "end": 2148.64, "text": " Now we're going to try a different one called zeros.", "tokens": [50364, 823, 321, 434, 516, 281, 853, 257, 819, 472, 1219, 35193, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 481, "seek": 214564, "start": 2148.64, "end": 2151.64, "text": " So zeros is just torch dot zeros.", "tokens": [50514, 407, 35193, 307, 445, 27822, 5893, 35193, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 482, "seek": 214564, "start": 2151.64, "end": 2156.64, "text": " And then inside of here, we could just do the dimensions or the shape of this.", "tokens": [50664, 400, 550, 1854, 295, 510, 11, 321, 727, 445, 360, 264, 12819, 420, 264, 3909, 295, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 483, "seek": 214564, "start": 2156.64, "end": 2160.64, "text": " So two by three, and then we could just do zeros.", "tokens": [50914, 407, 732, 538, 1045, 11, 293, 550, 321, 727, 445, 360, 35193, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 484, "seek": 214564, "start": 2160.64, "end": 2162.64, "text": " And then go ahead and run that.", "tokens": [51114, 400, 550, 352, 2286, 293, 1190, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 485, "seek": 214564, "start": 2162.64, "end": 2166.64, "text": " So we get a two by three of zeros.", "tokens": [51214, 407, 321, 483, 257, 732, 538, 1045, 295, 35193, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 486, "seek": 214564, "start": 2166.64, "end": 2170.64, "text": " And these are all floating point numbers, by the way.", "tokens": [51414, 400, 613, 366, 439, 12607, 935, 3547, 11, 538, 264, 636, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11528155729942716, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.04466802626848221}, {"id": 487, "seek": 217064, "start": 2170.64, "end": 2171.64, "text": " Maybe we could try ones.", "tokens": [50364, 2704, 321, 727, 853, 2306, 13, 50414], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 488, "seek": 217064, "start": 2171.64, "end": 2173.64, "text": " Now I know ones is pretty fun ones.", "tokens": [50414, 823, 286, 458, 2306, 307, 1238, 1019, 2306, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 489, "seek": 217064, "start": 2173.64, "end": 2177.64, "text": " So we both torch dot ones.", "tokens": [50514, 407, 321, 1293, 27822, 5893, 2306, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 490, "seek": 217064, "start": 2177.64, "end": 2179.64, "text": " It's pretty much the same as zeros.", "tokens": [50714, 467, 311, 1238, 709, 264, 912, 382, 35193, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 491, "seek": 217064, "start": 2179.64, "end": 2185.64, "text": " We could just do like maybe three by four and then print that ones out.", "tokens": [50814, 492, 727, 445, 360, 411, 1310, 1045, 538, 1451, 293, 550, 4482, 300, 2306, 484, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 492, "seek": 217064, "start": 2185.64, "end": 2188.64, "text": " So we have a three by four of ones.", "tokens": [51114, 407, 321, 362, 257, 1045, 538, 1451, 295, 2306, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 493, "seek": 217064, "start": 2188.64, "end": 2189.64, "text": " Sweet.", "tokens": [51264, 14653, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 494, "seek": 217064, "start": 2189.64, "end": 2197.64, "text": " So what if we do input equals torch dot empty.", "tokens": [51314, 407, 437, 498, 321, 360, 4846, 6915, 27822, 5893, 6707, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18780560926957565, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.05183291435241699}, {"id": 495, "seek": 219764, "start": 2197.64, "end": 2205.64, "text": " We can make this two by three.", "tokens": [50364, 492, 393, 652, 341, 732, 538, 1045, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 496, "seek": 219764, "start": 2205.64, "end": 2208.64, "text": " So these are interesting.", "tokens": [50764, 407, 613, 366, 1880, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 497, "seek": 219764, "start": 2208.64, "end": 2215.64, "text": " These are pretty much a bunch of very either very large or very small numbers.", "tokens": [50914, 1981, 366, 1238, 709, 257, 3840, 295, 588, 2139, 588, 2416, 420, 588, 1359, 3547, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 498, "seek": 219764, "start": 2215.64, "end": 2218.64, "text": " I haven't particularly found a use case for this yet,", "tokens": [51264, 286, 2378, 380, 4098, 1352, 257, 764, 1389, 337, 341, 1939, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 499, "seek": 219764, "start": 2218.64, "end": 2221.64, "text": " but just another feature that PyTorch has.", "tokens": [51414, 457, 445, 1071, 4111, 300, 9953, 51, 284, 339, 575, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 500, "seek": 219764, "start": 2221.64, "end": 2223.64, "text": " We have a range.", "tokens": [51564, 492, 362, 257, 3613, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1690954826247524, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.005384483840316534}, {"id": 501, "seek": 222364, "start": 2223.64, "end": 2229.64, "text": " So we go arrange equals torch dot arrange.", "tokens": [50364, 407, 321, 352, 9424, 6915, 27822, 5893, 9424, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2638736593312231, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.007695010397583246}, {"id": 502, "seek": 222364, "start": 2229.64, "end": 2234.64, "text": " I could do like five, for example, just do range.", "tokens": [50664, 286, 727, 360, 411, 1732, 11, 337, 1365, 11, 445, 360, 3613, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2638736593312231, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.007695010397583246}, {"id": 503, "seek": 222364, "start": 2234.64, "end": 2242.64, "text": " So now we have a tensor just sorted zero or rather starting at zero up to four.", "tokens": [50914, 407, 586, 321, 362, 257, 40863, 445, 25462, 4018, 420, 2831, 2891, 412, 4018, 493, 281, 1451, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2638736593312231, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.007695010397583246}, {"id": 504, "seek": 222364, "start": 2242.64, "end": 2247.64, "text": " So five, just just like that.", "tokens": [51314, 407, 1732, 11, 445, 445, 411, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2638736593312231, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.007695010397583246}, {"id": 505, "seek": 224764, "start": 2247.64, "end": 2256.64, "text": " Line space equals torch dot line line space.", "tokens": [50364, 14670, 1901, 6915, 27822, 5893, 1622, 1622, 1901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 506, "seek": 224764, "start": 2256.64, "end": 2264.64, "text": " Spelling is weird to three, 10, and then steps, for example, equals five.", "tokens": [50814, 3550, 2669, 307, 3657, 281, 1045, 11, 1266, 11, 293, 550, 4439, 11, 337, 1365, 11, 6915, 1732, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 507, "seek": 224764, "start": 2264.64, "end": 2267.64, "text": " So it makes sense in a second here, go run.", "tokens": [51214, 407, 309, 1669, 2020, 294, 257, 1150, 510, 11, 352, 1190, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 508, "seek": 224764, "start": 2267.64, "end": 2271.64, "text": " And we got a line space of steps equals five.", "tokens": [51364, 400, 321, 658, 257, 1622, 1901, 295, 4439, 6915, 1732, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 509, "seek": 224764, "start": 2271.64, "end": 2274.64, "text": " So we have five different ones, boom, boom, boom, boom, boom.", "tokens": [51564, 407, 321, 362, 1732, 819, 2306, 11, 9351, 11, 9351, 11, 9351, 11, 9351, 11, 9351, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 510, "seek": 224764, "start": 2274.64, "end": 2276.64, "text": " And we go all the way from three to 10.", "tokens": [51714, 400, 321, 352, 439, 264, 636, 490, 1045, 281, 1266, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1864625253985005, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.07919157296419144}, {"id": 511, "seek": 227664, "start": 2276.64, "end": 2283.64, "text": " So pretty much getting all of the constant increments from three all the way up to 10 over five steps.", "tokens": [50364, 407, 1238, 709, 1242, 439, 295, 264, 5754, 1946, 1117, 490, 1045, 439, 264, 636, 493, 281, 1266, 670, 1732, 4439, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1135482880675677, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.03019796498119831}, {"id": 512, "seek": 227664, "start": 2283.64, "end": 2287.64, "text": " So you're doing, you're basically adding the same amount every time.", "tokens": [50714, 407, 291, 434, 884, 11, 291, 434, 1936, 5127, 264, 912, 2372, 633, 565, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1135482880675677, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.03019796498119831}, {"id": 513, "seek": 227664, "start": 2287.64, "end": 2295.64, "text": " So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right?", "tokens": [50914, 407, 1045, 1804, 502, 13, 11901, 307, 1017, 13, 11901, 1804, 1071, 502, 13, 11901, 307, 1386, 13, 20, 293, 550, 1649, 13, 6074, 293, 550, 1266, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.1135482880675677, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.03019796498119831}, {"id": 514, "seek": 227664, "start": 2295.64, "end": 2299.64, "text": " So just over five steps, we want to find what that constant increment is.", "tokens": [51314, 407, 445, 670, 1732, 4439, 11, 321, 528, 281, 915, 437, 300, 5754, 26200, 307, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1135482880675677, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.03019796498119831}, {"id": 515, "seek": 227664, "start": 2299.64, "end": 2302.64, "text": " So that's a pretty cool one.", "tokens": [51514, 407, 300, 311, 257, 1238, 1627, 472, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1135482880675677, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.03019796498119831}, {"id": 516, "seek": 230264, "start": 2302.64, "end": 2307.64, "text": " And then we have, we'll do log space, which is interesting.", "tokens": [50364, 400, 550, 321, 362, 11, 321, 603, 360, 3565, 1901, 11, 597, 307, 1880, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1614828109741211, "compression_ratio": 1.5, "no_speech_prob": 0.02595200203359127}, {"id": 517, "seek": 230264, "start": 2307.64, "end": 2311.64, "text": " Log space equals torch dot log space.", "tokens": [50614, 10824, 1901, 6915, 27822, 5893, 3565, 1901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1614828109741211, "compression_ratio": 1.5, "no_speech_prob": 0.02595200203359127}, {"id": 518, "seek": 230264, "start": 2311.64, "end": 2328.64, "text": " And then we'll go start, start equals negative 10 and equals 10.", "tokens": [50814, 400, 550, 321, 603, 352, 722, 11, 722, 6915, 3671, 1266, 293, 6915, 1266, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1614828109741211, "compression_ratio": 1.5, "no_speech_prob": 0.02595200203359127}, {"id": 519, "seek": 230264, "start": 2328.64, "end": 2330.64, "text": " These are both start and end.", "tokens": [51664, 1981, 366, 1293, 722, 293, 917, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1614828109741211, "compression_ratio": 1.5, "no_speech_prob": 0.02595200203359127}, {"id": 520, "seek": 233064, "start": 2330.64, "end": 2331.64, "text": " You can either put these here.", "tokens": [50364, 509, 393, 2139, 829, 613, 510, 13, 50414], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 521, "seek": 233064, "start": 2331.64, "end": 2335.64, "text": " You can either put the start with them, start equals, or you don't have to.", "tokens": [50414, 509, 393, 2139, 829, 264, 722, 365, 552, 11, 722, 6915, 11, 420, 291, 500, 380, 362, 281, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 522, "seek": 233064, "start": 2335.64, "end": 2337.64, "text": " It's honestly up to you.", "tokens": [50614, 467, 311, 6095, 493, 281, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 523, "seek": 233064, "start": 2337.64, "end": 2340.64, "text": " And then we can put our steps again.", "tokens": [50714, 400, 550, 321, 393, 829, 527, 4439, 797, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 524, "seek": 233064, "start": 2340.64, "end": 2343.64, "text": " So steps equals every five.", "tokens": [50864, 407, 4439, 6915, 633, 1732, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 525, "seek": 233064, "start": 2343.64, "end": 2345.64, "text": " Let's go ahead and run that.", "tokens": [51014, 961, 311, 352, 2286, 293, 1190, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 526, "seek": 233064, "start": 2345.64, "end": 2350.64, "text": " Oops, need to put log space there.", "tokens": [51114, 21726, 11, 643, 281, 829, 3565, 1901, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 527, "seek": 233064, "start": 2350.64, "end": 2351.64, "text": " So we get that.", "tokens": [51364, 407, 321, 483, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 528, "seek": 233064, "start": 2351.64, "end": 2355.64, "text": " So we start at one of the negative 10.", "tokens": [51414, 407, 321, 722, 412, 472, 295, 264, 3671, 1266, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 529, "seek": 233064, "start": 2355.64, "end": 2357.64, "text": " And then we just do this little increments here.", "tokens": [51614, 400, 550, 321, 445, 360, 341, 707, 1946, 1117, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 530, "seek": 233064, "start": 2357.64, "end": 2359.64, "text": " So it goes 10, negative five, zero plus five times.", "tokens": [51714, 407, 309, 1709, 1266, 11, 3671, 1732, 11, 4018, 1804, 1732, 1413, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15927401185035706, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.04741174727678299}, {"id": 531, "seek": 235964, "start": 2359.64, "end": 2360.64, "text": " Just over five steps.", "tokens": [50364, 1449, 670, 1732, 4439, 13, 50414], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 532, "seek": 235964, "start": 2360.64, "end": 2362.64, "text": " So that's pretty cool.", "tokens": [50414, 407, 300, 311, 1238, 1627, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 533, "seek": 235964, "start": 2362.64, "end": 2364.64, "text": " What else do we have here?", "tokens": [50514, 708, 1646, 360, 321, 362, 510, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 534, "seek": 235964, "start": 2364.64, "end": 2368.64, "text": " So we have I, torch dot I.", "tokens": [50614, 407, 321, 362, 286, 11, 27822, 5893, 286, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 535, "seek": 235964, "start": 2368.64, "end": 2370.64, "text": " I just have all these on my second screen here.", "tokens": [50814, 286, 445, 362, 439, 613, 322, 452, 1150, 2568, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 536, "seek": 235964, "start": 2370.64, "end": 2376.64, "text": " So a bunch of examples just written out and we're just kind of visualizing what these can do.", "tokens": [50914, 407, 257, 3840, 295, 5110, 445, 3720, 484, 293, 321, 434, 445, 733, 295, 5056, 3319, 437, 613, 393, 360, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 537, "seek": 235964, "start": 2376.64, "end": 2387.64, "text": " And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do.", "tokens": [51214, 400, 1310, 291, 1062, 754, 362, 428, 1065, 5880, 707, 44102, 295, 1194, 300, 291, 434, 516, 281, 1310, 915, 746, 1646, 300, 291, 393, 764, 613, 337, 337, 428, 1065, 2973, 4455, 420, 2035, 291, 528, 281, 360, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14201933819314708, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.00970669835805893}, {"id": 538, "seek": 238764, "start": 2387.64, "end": 2390.64, "text": " So we're just kind of experimenting with these.", "tokens": [50364, 407, 321, 434, 445, 733, 295, 29070, 365, 613, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19236487434023902, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.01406154129654169}, {"id": 539, "seek": 238764, "start": 2390.64, "end": 2395.64, "text": " What we can do with the basics of pytorch and some of the very basic functions.", "tokens": [50514, 708, 321, 393, 360, 365, 264, 14688, 295, 25878, 284, 339, 293, 512, 295, 264, 588, 3875, 6828, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19236487434023902, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.01406154129654169}, {"id": 540, "seek": 238764, "start": 2395.64, "end": 2401.64, "text": " So first I will print this out here.", "tokens": [50764, 407, 700, 286, 486, 4482, 341, 484, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19236487434023902, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.01406154129654169}, {"id": 541, "seek": 238764, "start": 2401.64, "end": 2409.64, "text": " So we get pretty much just a diagonal line and it's in five.", "tokens": [51064, 407, 321, 483, 1238, 709, 445, 257, 21539, 1622, 293, 309, 311, 294, 1732, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19236487434023902, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.01406154129654169}, {"id": 542, "seek": 240964, "start": 2409.64, "end": 2419.64, "text": " So you get a five by five matrix and pretty much just reduced row each long form.", "tokens": [50364, 407, 291, 483, 257, 1732, 538, 1732, 8141, 293, 1238, 709, 445, 9212, 5386, 1184, 938, 1254, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11440241514746823, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.19427205622196198}, {"id": 543, "seek": 240964, "start": 2419.64, "end": 2423.64, "text": " I don't know how to pronounce it, but that's pretty much what it looks like.", "tokens": [50864, 286, 500, 380, 458, 577, 281, 19567, 309, 11, 457, 300, 311, 1238, 709, 437, 309, 1542, 411, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11440241514746823, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.19427205622196198}, {"id": 544, "seek": 240964, "start": 2423.64, "end": 2426.64, "text": " So pretty cool stuff.", "tokens": [51064, 407, 1238, 1627, 1507, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11440241514746823, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.19427205622196198}, {"id": 545, "seek": 240964, "start": 2426.64, "end": 2429.64, "text": " Let's see what else we have.", "tokens": [51214, 961, 311, 536, 437, 1646, 321, 362, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11440241514746823, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.19427205622196198}, {"id": 546, "seek": 240964, "start": 2429.64, "end": 2435.64, "text": " We have empty like.", "tokens": [51364, 492, 362, 6707, 411, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11440241514746823, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.19427205622196198}, {"id": 547, "seek": 243564, "start": 2435.64, "end": 2453.64, "text": " We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty.", "tokens": [50364, 492, 362, 6707, 411, 27822, 5893, 6707, 411, 257, 293, 550, 321, 603, 445, 584, 1310, 652, 257, 2681, 281, 652, 309, 257, 27822, 5893, 6707, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2714310307656565, "compression_ratio": 1.3411764705882352, "no_speech_prob": 0.020021885633468628}, {"id": 548, "seek": 245364, "start": 2453.64, "end": 2469.64, "text": " And then we can go two by three and then data type torch dot int 64 64 bit integers.", "tokens": [50364, 400, 550, 321, 393, 352, 732, 538, 1045, 293, 550, 1412, 2010, 27822, 5893, 560, 12145, 12145, 857, 41674, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14954631145183855, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.04271635785698891}, {"id": 549, "seek": 245364, "start": 2469.64, "end": 2477.64, "text": " And then let's see what happens here empty.", "tokens": [51164, 400, 550, 718, 311, 536, 437, 2314, 510, 6707, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14954631145183855, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.04271635785698891}, {"id": 550, "seek": 245364, "start": 2477.64, "end": 2480.64, "text": " So that's pretty cool.", "tokens": [51564, 407, 300, 311, 1238, 1627, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14954631145183855, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.04271635785698891}, {"id": 551, "seek": 245364, "start": 2480.64, "end": 2481.64, "text": " What else do we have?", "tokens": [51714, 708, 1646, 360, 321, 362, 30, 51764], "temperature": 0.0, "avg_logprob": -0.14954631145183855, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.04271635785698891}, {"id": 552, "seek": 248164, "start": 2481.64, "end": 2483.64, "text": " Yes, we can do timing as well.", "tokens": [50364, 1079, 11, 321, 393, 360, 10822, 382, 731, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11912532025072949, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.06461828947067261}, {"id": 553, "seek": 248164, "start": 2483.64, "end": 2487.64, "text": " So I'm just going to erase all of these.", "tokens": [50464, 407, 286, 478, 445, 516, 281, 23525, 439, 295, 613, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11912532025072949, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.06461828947067261}, {"id": 554, "seek": 248164, "start": 2487.64, "end": 2498.64, "text": " You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit.", "tokens": [50664, 509, 393, 11369, 646, 294, 264, 960, 445, 574, 293, 1310, 5120, 365, 613, 257, 707, 857, 11, 853, 257, 707, 857, 544, 813, 445, 437, 286, 600, 1096, 365, 552, 11, 1310, 16927, 552, 257, 707, 857, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11912532025072949, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.06461828947067261}, {"id": 555, "seek": 248164, "start": 2498.64, "end": 2501.64, "text": " But yeah, I'm actually going to delete all of these here.", "tokens": [51214, 583, 1338, 11, 286, 478, 767, 516, 281, 12097, 439, 295, 613, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11912532025072949, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.06461828947067261}, {"id": 556, "seek": 250164, "start": 2501.64, "end": 2517.64, "text": " And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment.", "tokens": [50364, 400, 550, 321, 393, 352, 2286, 293, 360, 264, 4302, 6915, 383, 11152, 293, 321, 434, 516, 281, 352, 2286, 293, 3679, 341, 670, 281, 264, 383, 11152, 26039, 51, 2823, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2676132520039876, "compression_ratio": 1.2815533980582525, "no_speech_prob": 0.7655329704284668}, {"id": 557, "seek": 251764, "start": 2517.64, "end": 2536.64, "text": " Cuda if torch dot Cuda underscore is dot Cuda is available.", "tokens": [50364, 383, 11152, 498, 27822, 5893, 383, 11152, 37556, 307, 5893, 383, 11152, 307, 2435, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2290893604880885, "compression_ratio": 1.0727272727272728, "no_speech_prob": 0.7019736766815186}, {"id": 558, "seek": 253664, "start": 2536.64, "end": 2548.64, "text": " And then else you print out our device here and run this Cuda suite.", "tokens": [50364, 400, 550, 1646, 291, 4482, 484, 527, 4302, 510, 293, 1190, 341, 383, 11152, 14205, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14587455477033343, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.40322211384773254}, {"id": 559, "seek": 253664, "start": 2548.64, "end": 2564.64, "text": " So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality.", "tokens": [50964, 407, 321, 434, 516, 281, 853, 281, 360, 1507, 365, 264, 18407, 586, 5347, 281, 264, 13199, 293, 534, 536, 577, 709, 295, 257, 2649, 383, 11152, 420, 264, 18407, 307, 516, 281, 652, 294, 9660, 281, 264, 13199, 562, 321, 1319, 264, 3909, 293, 10139, 1860, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14587455477033343, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.40322211384773254}, {"id": 560, "seek": 256464, "start": 2564.64, "end": 2569.64, "text": " We're just doing different experiments with a bunch of different tensors.", "tokens": [50364, 492, 434, 445, 884, 819, 12050, 365, 257, 3840, 295, 819, 10688, 830, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10363178934369768, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.3377038240432739}, {"id": 561, "seek": 256464, "start": 2569.64, "end": 2576.64, "text": " So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time.", "tokens": [50614, 407, 294, 1668, 281, 767, 3481, 264, 2649, 1296, 264, 18407, 293, 264, 13199, 11, 286, 445, 25524, 257, 6405, 1219, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10363178934369768, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.3377038240432739}, {"id": 562, "seek": 256464, "start": 2576.64, "end": 2580.64, "text": " So this comes with the operating system or sorry with with Python.", "tokens": [50964, 407, 341, 1487, 365, 264, 7447, 1185, 420, 2597, 365, 365, 15329, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10363178934369768, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.3377038240432739}, {"id": 563, "seek": 256464, "start": 2580.64, "end": 2583.64, "text": " You don't have to actually install this manually.", "tokens": [51164, 509, 500, 380, 362, 281, 767, 3625, 341, 16945, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10363178934369768, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.3377038240432739}, {"id": 564, "seek": 258364, "start": 2583.64, "end": 2593.64, "text": " So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now.", "tokens": [50364, 407, 1936, 437, 321, 360, 307, 321, 5699, 321, 818, 565, 5893, 565, 293, 550, 34153, 11, 309, 486, 445, 747, 264, 2190, 565, 35623, 302, 558, 586, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15154198237827846, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.8534088730812073}, {"id": 565, "seek": 258364, "start": 2593.64, "end": 2600.64, "text": " So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds.", "tokens": [50864, 407, 722, 565, 486, 312, 411, 558, 586, 293, 550, 917, 565, 1310, 1045, 3949, 1780, 486, 312, 11, 291, 458, 11, 558, 586, 1804, 1045, 3949, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15154198237827846, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.8534088730812073}, {"id": 566, "seek": 258364, "start": 2600.64, "end": 2606.64, "text": " So if we subtract end time, start time will get a three second difference and that would be the total elapsed time.", "tokens": [51214, 407, 498, 321, 16390, 917, 565, 11, 722, 565, 486, 483, 257, 1045, 1150, 2649, 293, 300, 576, 312, 264, 3217, 806, 2382, 292, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15154198237827846, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.8534088730812073}, {"id": 567, "seek": 260664, "start": 2606.64, "end": 2614.64, "text": " And then this little number here, this four will be just how many decimal places we have.", "tokens": [50364, 400, 550, 341, 707, 1230, 510, 11, 341, 1451, 486, 312, 445, 577, 867, 26601, 3190, 321, 362, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 568, "seek": 260664, "start": 2614.64, "end": 2616.64, "text": " So I can go ahead and run this here.", "tokens": [50764, 407, 286, 393, 352, 2286, 293, 1190, 341, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 569, "seek": 260664, "start": 2616.64, "end": 2620.64, "text": " Time is not defined. Let's run that first.", "tokens": [50864, 6161, 307, 406, 7642, 13, 961, 311, 1190, 300, 700, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 570, "seek": 260664, "start": 2620.64, "end": 2625.64, "text": " It's going to take, you know, almost no time at all.", "tokens": [51064, 467, 311, 516, 281, 747, 11, 291, 458, 11, 1920, 572, 565, 412, 439, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 571, "seek": 260664, "start": 2625.64, "end": 2629.64, "text": " So we can actually increase this if we want to 10 and then run that again.", "tokens": [51314, 407, 321, 393, 767, 3488, 341, 498, 321, 528, 281, 1266, 293, 550, 1190, 300, 797, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 572, "seek": 260664, "start": 2629.64, "end": 2633.64, "text": " Again, it's, you know, we're making up pretty much a one by one matrix.", "tokens": [51514, 3764, 11, 309, 311, 11, 291, 458, 11, 321, 434, 1455, 493, 1238, 709, 257, 472, 538, 472, 8141, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10869267301739387, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.4453626573085785}, {"id": 573, "seek": 263364, "start": 2633.64, "end": 2635.64, "text": " So just a just a zero.", "tokens": [50364, 407, 445, 257, 445, 257, 4018, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11750577843707541, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.052598994225263596}, {"id": 574, "seek": 263364, "start": 2635.64, "end": 2641.64, "text": " So we're not really going to get anything significant from that.", "tokens": [50464, 407, 321, 434, 406, 534, 516, 281, 483, 1340, 4776, 490, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11750577843707541, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.052598994225263596}, {"id": 575, "seek": 263364, "start": 2641.64, "end": 2657.64, "text": " But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network.", "tokens": [50764, 583, 13448, 11, 337, 337, 767, 4997, 264, 2649, 1296, 264, 18407, 293, 264, 13199, 11, 437, 321, 434, 516, 281, 3292, 466, 307, 300, 17138, 1166, 1399, 11, 264, 1399, 295, 2128, 1320, 293, 646, 38377, 807, 264, 3209, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11750577843707541, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.052598994225263596}, {"id": 576, "seek": 265764, "start": 2657.64, "end": 2667.64, "text": " That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem.", "tokens": [50364, 663, 311, 10029, 437, 321, 434, 1382, 281, 19719, 337, 767, 7380, 439, 613, 9834, 293, 439, 613, 2316, 17443, 281, 264, 18407, 1943, 380, 534, 516, 281, 312, 264, 1154, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0696891579672555, "compression_ratio": 1.6332046332046333, "no_speech_prob": 0.5734365582466125}, {"id": 577, "seek": 265764, "start": 2667.64, "end": 2671.64, "text": " It'll take maybe a few seconds at most like maybe 30 seconds to do that.", "tokens": [50864, 467, 603, 747, 1310, 257, 1326, 3949, 412, 881, 411, 1310, 2217, 3949, 281, 360, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0696891579672555, "compression_ratio": 1.6332046332046333, "no_speech_prob": 0.5734365582466125}, {"id": 578, "seek": 265764, "start": 2671.64, "end": 2675.64, "text": " And that's not going to be any time at all in the entire training process.", "tokens": [51064, 400, 300, 311, 406, 516, 281, 312, 604, 565, 412, 439, 294, 264, 2302, 3097, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0696891579672555, "compression_ratio": 1.6332046332046333, "no_speech_prob": 0.5734365582466125}, {"id": 579, "seek": 265764, "start": 2675.64, "end": 2686.64, "text": " So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU.", "tokens": [51264, 407, 437, 321, 528, 281, 360, 307, 445, 536, 11, 291, 458, 11, 597, 307, 1101, 22592, 47, 88, 322, 264, 13199, 420, 27822, 1228, 29777, 7509, 322, 264, 18407, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0696891579672555, "compression_ratio": 1.6332046332046333, "no_speech_prob": 0.5734365582466125}, {"id": 580, "seek": 268664, "start": 2686.64, "end": 2688.64, "text": " So I have some code for that right here.", "tokens": [50364, 407, 286, 362, 512, 3089, 337, 300, 558, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 581, "seek": 268664, "start": 2688.64, "end": 2692.64, "text": " So we're going to initialize a bunch of matrices here.", "tokens": [50464, 407, 321, 434, 516, 281, 5883, 1125, 257, 3840, 295, 32284, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 582, "seek": 268664, "start": 2692.64, "end": 2698.64, "text": " So our sorry, tensors, and we have just basically random ones.", "tokens": [50664, 407, 527, 2597, 11, 10688, 830, 11, 293, 321, 362, 445, 1936, 4974, 2306, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 583, "seek": 268664, "start": 2698.64, "end": 2703.64, "text": " So we have a 10,000 by 10,000, all random, all random floating point numbers.", "tokens": [50964, 407, 321, 362, 257, 1266, 11, 1360, 538, 1266, 11, 1360, 11, 439, 4974, 11, 439, 4974, 12607, 935, 3547, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 584, "seek": 268664, "start": 2703.64, "end": 2705.64, "text": " And then we're going to push these to the GPU.", "tokens": [51214, 400, 550, 321, 434, 516, 281, 2944, 613, 281, 264, 18407, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 585, "seek": 268664, "start": 2705.64, "end": 2709.64, "text": " And we have two of these and then same thing for NumPy.", "tokens": [51314, 400, 321, 362, 732, 295, 613, 293, 550, 912, 551, 337, 22592, 47, 88, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08509218215942382, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.05106731876730919}, {"id": 586, "seek": 270964, "start": 2710.64, "end": 2717.64, "text": " So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.", "tokens": [50414, 407, 294, 1668, 281, 767, 12972, 32284, 365, 9953, 51, 284, 339, 11, 321, 643, 281, 764, 341, 412, 5986, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08262862781486889, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.11753562092781067}, {"id": 587, "seek": 270964, "start": 2717.64, "end": 2729.64, "text": " So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply.", "tokens": [50764, 407, 321, 12972, 613, 293, 321, 483, 341, 777, 11, 321, 483, 341, 777, 4974, 40863, 293, 550, 321, 1590, 309, 293, 550, 321, 360, 264, 912, 551, 670, 510, 11, 3993, 321, 764, 22592, 47, 88, 13, 76, 723, 647, 356, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08262862781486889, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.11753562092781067}, {"id": 588, "seek": 270964, "start": 2729.64, "end": 2737.64, "text": " So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds.", "tokens": [51364, 407, 498, 286, 352, 2286, 293, 1190, 613, 11, 309, 311, 516, 281, 747, 257, 1326, 3949, 281, 5883, 1125, 613, 293, 406, 754, 257, 1326, 3949, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08262862781486889, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.11753562092781067}, {"id": 589, "seek": 273764, "start": 2737.64, "end": 2740.64, "text": " And then we have, see, look at that.", "tokens": [50364, 400, 550, 321, 362, 11, 536, 11, 574, 412, 300, 13, 50514], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 590, "seek": 273764, "start": 2740.64, "end": 2745.64, "text": " So for the GPU, it took a little while to do that.", "tokens": [50514, 407, 337, 264, 18407, 11, 309, 1890, 257, 707, 1339, 281, 360, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 591, "seek": 273764, "start": 2745.64, "end": 2749.64, "text": " And then for the CPU, it didn't take as long.", "tokens": [50764, 400, 550, 337, 264, 13199, 11, 309, 994, 380, 747, 382, 938, 13, 50964], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 592, "seek": 273764, "start": 2749.64, "end": 2756.64, "text": " So this is because there's the shape of these matrices are not really that big.", "tokens": [50964, 407, 341, 307, 570, 456, 311, 264, 3909, 295, 613, 32284, 366, 406, 534, 300, 955, 13, 51314], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 593, "seek": 273764, "start": 2756.64, "end": 2758.64, "text": " They're just two dimensional, right?", "tokens": [51314, 814, 434, 445, 732, 18795, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 594, "seek": 273764, "start": 2758.64, "end": 2764.64, "text": " So it's see, this is something that the CPU can do very quickly because there's not that much to do.", "tokens": [51414, 407, 309, 311, 536, 11, 341, 307, 746, 300, 264, 13199, 393, 360, 588, 2661, 570, 456, 311, 406, 300, 709, 281, 360, 13, 51714], "temperature": 0.0, "avg_logprob": -0.078348178674679, "compression_ratio": 1.625, "no_speech_prob": 0.020327787846326828}, {"id": 595, "seek": 276464, "start": 2764.64, "end": 2767.64, "text": " But let's say we want to bump it up a notch.", "tokens": [50364, 583, 718, 311, 584, 321, 528, 281, 9961, 309, 493, 257, 26109, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11599875858851842, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.005640975199639797}, {"id": 596, "seek": 276464, "start": 2767.64, "end": 2775.64, "text": " So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.", "tokens": [50514, 407, 498, 321, 352, 281, 2319, 11, 2319, 11, 2319, 11, 293, 550, 1310, 321, 603, 3507, 294, 1071, 2319, 456, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11599875858851842, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.005640975199639797}, {"id": 597, "seek": 276464, "start": 2775.64, "end": 2776.64, "text": " Hopefully that works.", "tokens": [50914, 10429, 300, 1985, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11599875858851842, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.005640975199639797}, {"id": 598, "seek": 276464, "start": 2776.64, "end": 2779.64, "text": " And then we can do, we'll just do the same thing.", "tokens": [50964, 400, 550, 321, 393, 360, 11, 321, 603, 445, 360, 264, 912, 551, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11599875858851842, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.005640975199639797}, {"id": 599, "seek": 276464, "start": 2779.64, "end": 2781.64, "text": " So just paste this.", "tokens": [51114, 407, 445, 9163, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11599875858851842, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.005640975199639797}, {"id": 600, "seek": 278164, "start": 2782.64, "end": 2798.64, "text": " Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did.", "tokens": [50414, 823, 498, 321, 853, 281, 1190, 341, 797, 11, 291, 603, 536, 300, 264, 18407, 767, 1890, 1570, 813, 1922, 264, 565, 300, 264, 13199, 630, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08156183458143665, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.0030750203877687454}, {"id": 601, "seek": 278164, "start": 2798.64, "end": 2803.64, "text": " And this is because there's, you know, a lot more going on here.", "tokens": [51214, 400, 341, 307, 570, 456, 311, 11, 291, 458, 11, 257, 688, 544, 516, 322, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08156183458143665, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.0030750203877687454}, {"id": 602, "seek": 278164, "start": 2803.64, "end": 2807.64, "text": " There's a lot more simple multiplication to do.", "tokens": [51464, 821, 311, 257, 688, 544, 2199, 27290, 281, 360, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08156183458143665, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.0030750203877687454}, {"id": 603, "seek": 280764, "start": 2807.64, "end": 2815.64, "text": " So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model,", "tokens": [50364, 407, 264, 1778, 983, 341, 307, 370, 4776, 307, 570, 562, 321, 362, 11, 291, 458, 11, 6803, 420, 17375, 295, 9834, 294, 527, 2856, 2316, 11, 50764], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 604, "seek": 280764, "start": 2815.64, "end": 2821.64, "text": " we're not going to be doing very complex operations between all these tensors.", "tokens": [50764, 321, 434, 406, 516, 281, 312, 884, 588, 3997, 7705, 1296, 439, 613, 10688, 830, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 605, "seek": 280764, "start": 2821.64, "end": 2824.64, "text": " They're going to be very similar to what we saw in here.", "tokens": [51064, 814, 434, 516, 281, 312, 588, 2531, 281, 437, 321, 1866, 294, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 606, "seek": 280764, "start": 2824.64, "end": 2829.64, "text": " The dimensionality and shape is going to be very similar to what we're seeing right now.", "tokens": [51214, 440, 10139, 1860, 293, 3909, 307, 516, 281, 312, 588, 2531, 281, 437, 321, 434, 2577, 558, 586, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 607, "seek": 280764, "start": 2829.64, "end": 2831.64, "text": " You know, maybe three or four dimensions.", "tokens": [51464, 509, 458, 11, 1310, 1045, 420, 1451, 12819, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 608, "seek": 280764, "start": 2831.64, "end": 2835.64, "text": " And it's going to be very easy for a GPU to do this.", "tokens": [51564, 400, 309, 311, 516, 281, 312, 588, 1858, 337, 257, 18407, 281, 360, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05701889908104612, "compression_ratio": 1.76953125, "no_speech_prob": 0.12074581533670425}, {"id": 609, "seek": 283564, "start": 2835.64, "end": 2838.64, "text": " They're not complex tasks that we need the CPU to do.", "tokens": [50364, 814, 434, 406, 3997, 9608, 300, 321, 643, 264, 13199, 281, 360, 13, 50514], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 610, "seek": 283564, "start": 2838.64, "end": 2840.64, "text": " They're not very hard at all.", "tokens": [50514, 814, 434, 406, 588, 1152, 412, 439, 13, 50614], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 611, "seek": 283564, "start": 2840.64, "end": 2848.64, "text": " So when we give this task to parallel processing, it's going to be a ton quicker.", "tokens": [50614, 407, 562, 321, 976, 341, 5633, 281, 8952, 9007, 11, 309, 311, 516, 281, 312, 257, 2952, 16255, 13, 51014], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 612, "seek": 283564, "start": 2848.64, "end": 2851.64, "text": " So you're going to see why this matters later in the course.", "tokens": [51014, 407, 291, 434, 516, 281, 536, 983, 341, 7001, 1780, 294, 264, 1164, 13, 51164], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 613, "seek": 283564, "start": 2851.64, "end": 2855.64, "text": " You're going to see this with some of the hyper parameters we're going to use,", "tokens": [51164, 509, 434, 516, 281, 536, 341, 365, 512, 295, 264, 9848, 9834, 321, 434, 516, 281, 764, 11, 51364], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 614, "seek": 283564, "start": 2855.64, "end": 2860.64, "text": " which I'm not going to get into quite yet, but over the next little bit,", "tokens": [51364, 597, 286, 478, 406, 516, 281, 483, 666, 1596, 1939, 11, 457, 670, 264, 958, 707, 857, 11, 51614], "temperature": 0.0, "avg_logprob": -0.04384916562300462, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.07155708968639374}, {"id": 615, "seek": 286064, "start": 2860.64, "end": 2868.64, "text": " you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process.", "tokens": [50364, 291, 434, 516, 281, 536, 983, 264, 18407, 307, 516, 281, 1871, 257, 688, 337, 5662, 264, 10493, 295, 300, 17138, 1166, 1399, 13, 50764], "temperature": 0.0, "avg_logprob": -0.049487666328354635, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.06950713694095612}, {"id": 616, "seek": 286064, "start": 2868.64, "end": 2869.64, "text": " So this is great.", "tokens": [50764, 407, 341, 307, 869, 13, 50814], "temperature": 0.0, "avg_logprob": -0.049487666328354635, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.06950713694095612}, {"id": 617, "seek": 286064, "start": 2869.64, "end": 2877.64, "text": " Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency.", "tokens": [50814, 823, 291, 458, 257, 707, 857, 544, 466, 983, 321, 764, 264, 18407, 2602, 295, 264, 13199, 337, 3097, 10493, 13, 51214], "temperature": 0.0, "avg_logprob": -0.049487666328354635, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.06950713694095612}, {"id": 618, "seek": 286064, "start": 2877.64, "end": 2883.64, "text": " So there's actually another term that we can use called a percentage percentage time.", "tokens": [51214, 407, 456, 311, 767, 1071, 1433, 300, 321, 393, 764, 1219, 257, 9668, 9668, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.049487666328354635, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.06950713694095612}, {"id": 619, "seek": 286064, "start": 2883.64, "end": 2887.64, "text": " I don't know if that's exactly how you're supposed to call it, but that's what it is.", "tokens": [51514, 286, 500, 380, 458, 498, 300, 311, 2293, 577, 291, 434, 3442, 281, 818, 309, 11, 457, 300, 311, 437, 309, 307, 13, 51714], "temperature": 0.0, "avg_logprob": -0.049487666328354635, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.06950713694095612}, {"id": 620, "seek": 288764, "start": 2887.64, "end": 2892.64, "text": " And pretty much what it'll do is time how long it takes to execute a block.", "tokens": [50364, 400, 1238, 709, 437, 309, 603, 360, 307, 565, 577, 938, 309, 2516, 281, 14483, 257, 3461, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0986807369491429, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.07365430146455765}, {"id": 621, "seek": 288764, "start": 2892.64, "end": 2897.64, "text": " So we can see here there's CPU times zero nanoseconds.", "tokens": [50614, 407, 321, 393, 536, 510, 456, 311, 13199, 1413, 4018, 14067, 541, 28750, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0986807369491429, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.07365430146455765}, {"id": 622, "seek": 288764, "start": 2897.64, "end": 2902.64, "text": " The end is for nano billionth of a second is a nanosecond and then wall time.", "tokens": [50864, 440, 917, 307, 337, 30129, 5218, 392, 295, 257, 1150, 307, 257, 14067, 541, 18882, 293, 550, 2929, 565, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0986807369491429, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.07365430146455765}, {"id": 623, "seek": 288764, "start": 2902.64, "end": 2908.64, "text": " So CPU time is how long it takes to execute on the CPU.", "tokens": [51114, 407, 13199, 565, 307, 577, 938, 309, 2516, 281, 14483, 322, 264, 13199, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0986807369491429, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.07365430146455765}, {"id": 624, "seek": 288764, "start": 2908.64, "end": 2916.64, "text": " The time that it's doing operations for and then the wall time would be how long it actually takes like in real time.", "tokens": [51414, 440, 565, 300, 309, 311, 884, 7705, 337, 293, 550, 264, 2929, 565, 576, 312, 577, 938, 309, 767, 2516, 411, 294, 957, 565, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0986807369491429, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.07365430146455765}, {"id": 625, "seek": 291664, "start": 2916.64, "end": 2920.64, "text": " How long do you have to wait? Do you have to wait until it's finished?", "tokens": [50364, 1012, 938, 360, 291, 362, 281, 1699, 30, 1144, 291, 362, 281, 1699, 1826, 309, 311, 4335, 30, 50564], "temperature": 0.0, "avg_logprob": -0.0733792220844942, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.002800634130835533}, {"id": 626, "seek": 291664, "start": 2920.64, "end": 2925.64, "text": " So the only thing that the CPU CPU time doesn't include is waiting.", "tokens": [50564, 407, 264, 787, 551, 300, 264, 13199, 13199, 565, 1177, 380, 4090, 307, 3806, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0733792220844942, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.002800634130835533}, {"id": 627, "seek": 291664, "start": 2925.64, "end": 2930.64, "text": " So in an entire process, there's going to be some operations and there's going to be some waiting.", "tokens": [50814, 407, 294, 364, 2302, 1399, 11, 456, 311, 516, 281, 312, 512, 7705, 293, 456, 311, 516, 281, 312, 512, 3806, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0733792220844942, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.002800634130835533}, {"id": 628, "seek": 291664, "start": 2930.64, "end": 2936.64, "text": " Wall time is going to have both of those and CPU time is just the execution.", "tokens": [51064, 9551, 565, 307, 516, 281, 362, 1293, 295, 729, 293, 13199, 565, 307, 445, 264, 15058, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0733792220844942, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.002800634130835533}, {"id": 629, "seek": 291664, "start": 2936.64, "end": 2942.64, "text": " So let's go ahead and continue with some of the basic PyTorch functions.", "tokens": [51364, 407, 718, 311, 352, 2286, 293, 2354, 365, 512, 295, 264, 3875, 9953, 51, 284, 339, 6828, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0733792220844942, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.002800634130835533}, {"id": 630, "seek": 294264, "start": 2942.64, "end": 2946.64, "text": " So I've written some stuff down here.", "tokens": [50364, 407, 286, 600, 3720, 512, 1507, 760, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12033625482355506, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.06751713901758194}, {"id": 631, "seek": 294264, "start": 2946.64, "end": 2954.64, "text": " So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu.", "tokens": [50564, 407, 321, 434, 516, 281, 352, 670, 7160, 339, 13, 372, 501, 11, 7160, 339, 13, 76, 723, 259, 47429, 11, 7160, 339, 13, 6903, 373, 11, 10931, 84, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12033625482355506, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.06751713901758194}, {"id": 632, "seek": 294264, "start": 2954.64, "end": 2957.64, "text": " I don't think that's how you pronounce it, but we'll get into that more.", "tokens": [50964, 286, 500, 380, 519, 300, 311, 577, 291, 19567, 309, 11, 457, 321, 603, 483, 666, 300, 544, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12033625482355506, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.06751713901758194}, {"id": 633, "seek": 294264, "start": 2957.64, "end": 2964.64, "text": " Transposing, linear, concatenating, and the softmax function.", "tokens": [51114, 6531, 79, 6110, 11, 8213, 11, 1588, 7186, 990, 11, 293, 264, 2787, 41167, 2445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12033625482355506, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.06751713901758194}, {"id": 634, "seek": 294264, "start": 2964.64, "end": 2968.64, "text": " So let's first start off here with the Torch.multinomial.", "tokens": [51464, 407, 718, 311, 700, 722, 766, 510, 365, 264, 7160, 339, 13, 76, 723, 259, 47429, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12033625482355506, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.06751713901758194}, {"id": 635, "seek": 296864, "start": 2968.64, "end": 2974.64, "text": " So this is essentially a probability distribution based on the index that you give it.", "tokens": [50364, 407, 341, 307, 4476, 257, 8482, 7316, 2361, 322, 264, 8186, 300, 291, 976, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 636, "seek": 296864, "start": 2974.64, "end": 2976.64, "text": " So we have probabilities here.", "tokens": [50664, 407, 321, 362, 33783, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 637, "seek": 296864, "start": 2976.64, "end": 2979.64, "text": " We say 0.1 and 0.9.", "tokens": [50764, 492, 584, 1958, 13, 16, 293, 1958, 13, 24, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 638, "seek": 296864, "start": 2979.64, "end": 2982.64, "text": " These numbers have to add up to one to make 100%.", "tokens": [50914, 1981, 3547, 362, 281, 909, 493, 281, 472, 281, 652, 2319, 6856, 51064], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 639, "seek": 296864, "start": 2982.64, "end": 2984.64, "text": " 100% is one, one whole.", "tokens": [51064, 2319, 4, 307, 472, 11, 472, 1379, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 640, "seek": 296864, "start": 2984.64, "end": 2987.64, "text": " So I have 10% and 90%.", "tokens": [51164, 407, 286, 362, 1266, 4, 293, 4289, 6856, 51314], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 641, "seek": 296864, "start": 2987.64, "end": 2989.64, "text": " This is an index zero.", "tokens": [51314, 639, 307, 364, 8186, 4018, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 642, "seek": 296864, "start": 2989.64, "end": 2995.64, "text": " So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one.", "tokens": [51414, 407, 456, 311, 257, 1266, 4, 2931, 300, 321, 434, 516, 281, 483, 257, 4018, 293, 257, 4289, 4, 2931, 300, 321, 434, 516, 281, 483, 257, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07510318587311601, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.07365193217992783}, {"id": 643, "seek": 299564, "start": 2995.64, "end": 3005.64, "text": " So if I go ahead and run these up here.", "tokens": [50364, 407, 498, 286, 352, 2286, 293, 1190, 613, 493, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 644, "seek": 299564, "start": 3005.64, "end": 3007.64, "text": " Give this a second to do its thing.", "tokens": [50864, 5303, 341, 257, 1150, 281, 360, 1080, 551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 645, "seek": 299564, "start": 3007.64, "end": 3011.64, "text": " So you can see that in the end we have our numSample set to 10.", "tokens": [50964, 407, 291, 393, 536, 300, 294, 264, 917, 321, 362, 527, 1031, 28743, 781, 992, 281, 1266, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 646, "seek": 299564, "start": 3011.64, "end": 3013.64, "text": " So it's going to give us 10 of these.", "tokens": [51164, 407, 309, 311, 516, 281, 976, 505, 1266, 295, 613, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 647, "seek": 299564, "start": 3013.64, "end": 3015.64, "text": " 1, 2, 3, 4, 5, 6, 7, 9, 10.", "tokens": [51264, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1722, 11, 1266, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 648, "seek": 299564, "start": 3015.64, "end": 3017.64, "text": " And all of them are ones.", "tokens": [51364, 400, 439, 295, 552, 366, 2306, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 649, "seek": 299564, "start": 3017.64, "end": 3020.64, "text": " If we run it again, we make it slightly different results.", "tokens": [51464, 759, 321, 1190, 309, 797, 11, 321, 652, 309, 4748, 819, 3542, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10888200593226165, "compression_ratio": 1.45, "no_speech_prob": 0.019120361655950546}, {"id": 650, "seek": 302064, "start": 3020.64, "end": 3022.64, "text": " So now we have some zeros in there.", "tokens": [50364, 407, 586, 321, 362, 512, 35193, 294, 456, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 651, "seek": 302064, "start": 3022.64, "end": 3025.64, "text": " But the zeros have very low probability of happening.", "tokens": [50464, 583, 264, 35193, 362, 588, 2295, 8482, 295, 2737, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 652, "seek": 302064, "start": 3025.64, "end": 3029.64, "text": " As a matter of fact, exactly a 10% probability of happening.", "tokens": [50614, 1018, 257, 1871, 295, 1186, 11, 2293, 257, 1266, 4, 8482, 295, 2737, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 653, "seek": 302064, "start": 3029.64, "end": 3036.64, "text": " So we're going to use this later in predicting what word is going to come next.", "tokens": [50814, 407, 321, 434, 516, 281, 764, 341, 1780, 294, 32884, 437, 1349, 307, 516, 281, 808, 958, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 654, "seek": 302064, "start": 3036.64, "end": 3042.64, "text": " Let's move on to Torch.cat or short for Torch.concatenate.", "tokens": [51164, 961, 311, 1286, 322, 281, 7160, 339, 13, 18035, 420, 2099, 337, 7160, 339, 13, 1671, 66, 7186, 473, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 655, "seek": 302064, "start": 3042.64, "end": 3046.64, "text": " So this will essentially concatenate two tensors into one.", "tokens": [51464, 407, 341, 486, 4476, 1588, 7186, 473, 732, 10688, 830, 666, 472, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10066647432288345, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0980442687869072}, {"id": 656, "seek": 304664, "start": 3046.64, "end": 3050.64, "text": " So I initialize this tensor here, torch.tensor, 1, 2, 3, 4.", "tokens": [50364, 407, 286, 5883, 1125, 341, 40863, 510, 11, 3930, 339, 13, 83, 23153, 11, 502, 11, 568, 11, 805, 11, 1017, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 657, "seek": 304664, "start": 3050.64, "end": 3052.64, "text": " It's one dimensional.", "tokens": [50564, 467, 311, 472, 18795, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 658, "seek": 304664, "start": 3052.64, "end": 3056.64, "text": " And we have another tensor here that just contains five.", "tokens": [50664, 400, 321, 362, 1071, 40863, 510, 300, 445, 8306, 1732, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 659, "seek": 304664, "start": 3056.64, "end": 3063.64, "text": " So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5.", "tokens": [50864, 407, 498, 321, 1588, 7186, 473, 502, 11, 568, 11, 805, 11, 1017, 293, 1025, 11, 550, 321, 483, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 660, "seek": 304664, "start": 3063.64, "end": 3068.64, "text": " We just combine them together and this is what will come out in the end.", "tokens": [51214, 492, 445, 10432, 552, 1214, 293, 341, 307, 437, 486, 808, 484, 294, 264, 917, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 661, "seek": 304664, "start": 3068.64, "end": 3070.64, "text": " So we run that 1, 2, 3, 4, 5.", "tokens": [51464, 407, 321, 1190, 300, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 662, "seek": 304664, "start": 3070.64, "end": 3072.64, "text": " Perfect.", "tokens": [51564, 10246, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 663, "seek": 304664, "start": 3072.64, "end": 3075.64, "text": " So we're going to actually use this when we're generating.", "tokens": [51664, 407, 321, 434, 516, 281, 767, 764, 341, 562, 321, 434, 17746, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1091472185575045, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.042070869356393814}, {"id": 664, "seek": 307564, "start": 3075.64, "end": 3078.64, "text": " When we're generating text given a context.", "tokens": [50364, 1133, 321, 434, 17746, 2487, 2212, 257, 4319, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06807863712310791, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.013840166851878166}, {"id": 665, "seek": 307564, "start": 3078.64, "end": 3082.64, "text": " So it's going to start from zero.", "tokens": [50514, 407, 309, 311, 516, 281, 722, 490, 4018, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06807863712310791, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.013840166851878166}, {"id": 666, "seek": 307564, "start": 3082.64, "end": 3085.64, "text": " We're going to use our probability distribution to pick the first one.", "tokens": [50714, 492, 434, 516, 281, 764, 527, 8482, 7316, 281, 1888, 264, 700, 472, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06807863712310791, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.013840166851878166}, {"id": 667, "seek": 307564, "start": 3085.64, "end": 3093.64, "text": " And then based on the first one, we're going to, you know, we're going to predict the next character.", "tokens": [50864, 400, 550, 2361, 322, 264, 700, 472, 11, 321, 434, 516, 281, 11, 291, 458, 11, 321, 434, 516, 281, 6069, 264, 958, 2517, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06807863712310791, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.013840166851878166}, {"id": 668, "seek": 307564, "start": 3093.64, "end": 3101.64, "text": " And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted.", "tokens": [51264, 400, 550, 1564, 321, 362, 19147, 300, 11, 321, 434, 516, 281, 1588, 7186, 473, 264, 777, 472, 365, 264, 2306, 300, 321, 600, 1217, 19147, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06807863712310791, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.013840166851878166}, {"id": 669, "seek": 310164, "start": 3101.64, "end": 3104.64, "text": " So we have this, maybe like 100 characters over here.", "tokens": [50364, 407, 321, 362, 341, 11, 1310, 411, 2319, 4342, 670, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 670, "seek": 310164, "start": 3104.64, "end": 3106.64, "text": " And then the next character that we're predicting is over here.", "tokens": [50514, 400, 550, 264, 958, 2517, 300, 321, 434, 32884, 307, 670, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 671, "seek": 310164, "start": 3106.64, "end": 3108.64, "text": " We just concatenate these.", "tokens": [50614, 492, 445, 1588, 7186, 473, 613, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 672, "seek": 310164, "start": 3108.64, "end": 3114.64, "text": " And by the end, we will have all of the integers that we've predicted.", "tokens": [50714, 400, 538, 264, 917, 11, 321, 486, 362, 439, 295, 264, 41674, 300, 321, 600, 19147, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 673, "seek": 310164, "start": 3114.64, "end": 3117.64, "text": " So next up, we have torch.trill.", "tokens": [51014, 407, 958, 493, 11, 321, 362, 3930, 339, 13, 6903, 373, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 674, "seek": 310164, "start": 3117.64, "end": 3123.64, "text": " And what this stands for, what the trail stands for is a triangle lower.", "tokens": [51164, 400, 437, 341, 7382, 337, 11, 437, 264, 9924, 7382, 337, 307, 257, 13369, 3126, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 675, "seek": 310164, "start": 3123.64, "end": 3127.64, "text": " So it's going to be in a sort of a triangle formation like this diagonal.", "tokens": [51464, 407, 309, 311, 516, 281, 312, 294, 257, 1333, 295, 257, 13369, 11723, 411, 341, 21539, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10899551564996893, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.02228209748864174}, {"id": 676, "seek": 312764, "start": 3127.64, "end": 3132.64, "text": " It's going to be going from top left to bottom right.", "tokens": [50364, 467, 311, 516, 281, 312, 516, 490, 1192, 1411, 281, 2767, 558, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08246302840733293, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.034084875136613846}, {"id": 677, "seek": 312764, "start": 3132.64, "end": 3136.64, "text": " And so you're going to see a little bit more why later in this course.", "tokens": [50614, 400, 370, 291, 434, 516, 281, 536, 257, 707, 857, 544, 983, 1780, 294, 341, 1164, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08246302840733293, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.034084875136613846}, {"id": 678, "seek": 312764, "start": 3136.64, "end": 3150.64, "text": " But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history.", "tokens": [50814, 583, 341, 307, 1021, 570, 562, 291, 434, 767, 1382, 281, 6069, 41674, 420, 257, 958, 22667, 294, 264, 8310, 11, 291, 362, 11, 291, 787, 458, 437, 311, 294, 264, 2190, 2503, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08246302840733293, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.034084875136613846}, {"id": 679, "seek": 312764, "start": 3150.64, "end": 3152.64, "text": " We're trying to predict the future.", "tokens": [51514, 492, 434, 1382, 281, 6069, 264, 2027, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08246302840733293, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.034084875136613846}, {"id": 680, "seek": 312764, "start": 3152.64, "end": 3156.64, "text": " So giving the answers in the future isn't what we want to do at all.", "tokens": [51614, 407, 2902, 264, 6338, 294, 264, 2027, 1943, 380, 437, 321, 528, 281, 360, 412, 439, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08246302840733293, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.034084875136613846}, {"id": 681, "seek": 315664, "start": 3156.64, "end": 3160.64, "text": " So maybe we've just predicted one and the rest of them we haven't predicted yet.", "tokens": [50364, 407, 1310, 321, 600, 445, 19147, 472, 293, 264, 1472, 295, 552, 321, 2378, 380, 19147, 1939, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 682, "seek": 315664, "start": 3160.64, "end": 3162.64, "text": " So we set all these to zero.", "tokens": [50564, 407, 321, 992, 439, 613, 281, 4018, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 683, "seek": 315664, "start": 3162.64, "end": 3164.64, "text": " And then we predicted another one.", "tokens": [50664, 400, 550, 321, 19147, 1071, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 684, "seek": 315664, "start": 3164.64, "end": 3165.64, "text": " And these are still zero.", "tokens": [50764, 400, 613, 366, 920, 4018, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 685, "seek": 315664, "start": 3165.64, "end": 3167.64, "text": " So these are talking to each other in history.", "tokens": [50814, 407, 613, 366, 1417, 281, 1184, 661, 294, 2503, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 686, "seek": 315664, "start": 3167.64, "end": 3178.64, "text": " And as and as our predictions add up, we have more and more history to look back to and less future, right?", "tokens": [50914, 400, 382, 293, 382, 527, 21264, 909, 493, 11, 321, 362, 544, 293, 544, 2503, 281, 574, 646, 281, 293, 1570, 2027, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 687, "seek": 315664, "start": 3178.64, "end": 3184.64, "text": " Basically, the premise of this is just making sure we can't communicate with the answer.", "tokens": [51464, 8537, 11, 264, 22045, 295, 341, 307, 445, 1455, 988, 321, 393, 380, 7890, 365, 264, 1867, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08298798843666359, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.008844025433063507}, {"id": 688, "seek": 318464, "start": 3184.64, "end": 3191.64, "text": " We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet.", "tokens": [50364, 492, 393, 380, 6069, 1339, 5276, 437, 264, 1867, 307, 445, 411, 562, 291, 2464, 364, 1139, 11, 291, 393, 380, 764, 264, 1867, 8193, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 689, "seek": 318464, "start": 3191.64, "end": 3193.64, "text": " They don't give you the answer sheet.", "tokens": [50714, 814, 500, 380, 976, 291, 264, 1867, 8193, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 690, "seek": 318464, "start": 3193.64, "end": 3199.64, "text": " So you have to know based on your history of knowledge, which answers to predict.", "tokens": [50814, 407, 291, 362, 281, 458, 2361, 322, 428, 2503, 295, 3601, 11, 597, 6338, 281, 6069, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 691, "seek": 318464, "start": 3199.64, "end": 3202.64, "text": " And that's all that's going on here.", "tokens": [51114, 400, 300, 311, 439, 300, 311, 516, 322, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 692, "seek": 318464, "start": 3202.64, "end": 3206.64, "text": " And we have, I mean, you could probably guess this triangle upper.", "tokens": [51264, 400, 321, 362, 11, 286, 914, 11, 291, 727, 1391, 2041, 341, 13369, 6597, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 693, "seek": 318464, "start": 3206.64, "end": 3208.64, "text": " So we have all the upper ones.", "tokens": [51464, 407, 321, 362, 439, 264, 6597, 2306, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 694, "seek": 318464, "start": 3208.64, "end": 3212.64, "text": " These are, you know, lower on the lower side and then these are on the upper side.", "tokens": [51564, 1981, 366, 11, 291, 458, 11, 3126, 322, 264, 3126, 1252, 293, 550, 613, 366, 322, 264, 6597, 1252, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08138480068238313, "compression_ratio": 1.8489795918367347, "no_speech_prob": 0.11429930478334427}, {"id": 695, "seek": 321264, "start": 3212.64, "end": 3214.64, "text": " So same concept there.", "tokens": [50364, 407, 912, 3410, 456, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08892516114495018, "compression_ratio": 1.7552083333333333, "no_speech_prob": 0.18459975719451904}, {"id": 696, "seek": 321264, "start": 3214.64, "end": 3217.64, "text": " And then we have a masked fill.", "tokens": [50464, 400, 550, 321, 362, 257, 45249, 2836, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08892516114495018, "compression_ratio": 1.7552083333333333, "no_speech_prob": 0.18459975719451904}, {"id": 697, "seek": 321264, "start": 3217.64, "end": 3228.64, "text": " So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here.", "tokens": [50614, 407, 341, 472, 311, 516, 281, 312, 588, 1021, 1780, 570, 294, 1668, 281, 767, 483, 281, 341, 935, 11, 439, 321, 360, 307, 321, 445, 37871, 13024, 633, 4478, 294, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08892516114495018, "compression_ratio": 1.7552083333333333, "no_speech_prob": 0.18459975719451904}, {"id": 698, "seek": 321264, "start": 3228.64, "end": 3233.64, "text": " So if you exponentiate zero, if you exponentiate zero, it'll become one.", "tokens": [51164, 407, 498, 291, 37871, 13024, 4018, 11, 498, 291, 37871, 13024, 4018, 11, 309, 603, 1813, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08892516114495018, "compression_ratio": 1.7552083333333333, "no_speech_prob": 0.18459975719451904}, {"id": 699, "seek": 321264, "start": 3233.64, "end": 3237.64, "text": " If you exponentiate negative infinity, it'll become zero.", "tokens": [51414, 759, 291, 37871, 13024, 3671, 13202, 11, 309, 603, 1813, 4018, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08892516114495018, "compression_ratio": 1.7552083333333333, "no_speech_prob": 0.18459975719451904}, {"id": 700, "seek": 323764, "start": 3237.64, "end": 3243.64, "text": " All that's going on here is we're doing approximately 2.71.", "tokens": [50364, 1057, 300, 311, 516, 322, 510, 307, 321, 434, 884, 10447, 568, 13, 29985, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09190795065342695, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.27810174226760864}, {"id": 701, "seek": 323764, "start": 3243.64, "end": 3248.64, "text": " And this is a constant that we use in the dot exp function.", "tokens": [50664, 400, 341, 307, 257, 5754, 300, 321, 764, 294, 264, 5893, 1278, 2445, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09190795065342695, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.27810174226760864}, {"id": 702, "seek": 323764, "start": 3248.64, "end": 3254.64, "text": " And then we're putting this to whatever power is in that current slot.", "tokens": [50914, 400, 550, 321, 434, 3372, 341, 281, 2035, 1347, 307, 294, 300, 2190, 14747, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09190795065342695, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.27810174226760864}, {"id": 703, "seek": 323764, "start": 3254.64, "end": 3256.64, "text": " So we have a zero here.", "tokens": [51214, 407, 321, 362, 257, 4018, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09190795065342695, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.27810174226760864}, {"id": 704, "seek": 323764, "start": 3256.64, "end": 3266.64, "text": " So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71.", "tokens": [51314, 407, 568, 13, 29985, 281, 264, 4018, 307, 2681, 281, 472, 568, 13, 29985, 281, 264, 472, 307, 2681, 281, 568, 13, 29985, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09190795065342695, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.27810174226760864}, {"id": 705, "seek": 326664, "start": 3267.64, "end": 3280.64, "text": " And then 2.71 to the negative infinity is, of course, zero.", "tokens": [50414, 400, 550, 568, 13, 29985, 281, 264, 3671, 13202, 307, 11, 295, 1164, 11, 4018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07024615052817525, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0018100769957527518}, {"id": 706, "seek": 326664, "start": 3280.64, "end": 3283.64, "text": " So that's pretty much how we get from this to this.", "tokens": [51064, 407, 300, 311, 1238, 709, 577, 321, 483, 490, 341, 281, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07024615052817525, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0018100769957527518}, {"id": 707, "seek": 326664, "start": 3283.64, "end": 3288.64, "text": " And we're just, we're simply just masking these over.", "tokens": [51214, 400, 321, 434, 445, 11, 321, 434, 2935, 445, 31226, 613, 670, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07024615052817525, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0018100769957527518}, {"id": 708, "seek": 326664, "start": 3288.64, "end": 3290.64, "text": " So that's great.", "tokens": [51464, 407, 300, 311, 869, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07024615052817525, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0018100769957527518}, {"id": 709, "seek": 326664, "start": 3290.64, "end": 3294.64, "text": " And I sort of showcase what the exp does.", "tokens": [51564, 400, 286, 1333, 295, 20388, 437, 264, 1278, 775, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07024615052817525, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0018100769957527518}, {"id": 710, "seek": 329464, "start": 3294.64, "end": 3296.64, "text": " And we're just using this one right here.", "tokens": [50364, 400, 321, 434, 445, 1228, 341, 472, 558, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 711, "seek": 329464, "start": 3296.64, "end": 3300.64, "text": " We're using this output and we're just plugging it into here.", "tokens": [50464, 492, 434, 1228, 341, 5598, 293, 321, 434, 445, 42975, 309, 666, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 712, "seek": 329464, "start": 3300.64, "end": 3305.64, "text": " So it'll go from negative infinity to zero and then zero to one.", "tokens": [50664, 407, 309, 603, 352, 490, 3671, 13202, 281, 4018, 293, 550, 4018, 281, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 713, "seek": 329464, "start": 3305.64, "end": 3308.64, "text": " So that's how we get from here to here.", "tokens": [50914, 407, 300, 311, 577, 321, 483, 490, 510, 281, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 714, "seek": 329464, "start": 3308.64, "end": 3311.64, "text": " Now we have transposing.", "tokens": [51064, 823, 321, 362, 7132, 6110, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 715, "seek": 329464, "start": 3311.64, "end": 3316.64, "text": " So transposing is when we sort of flip or swap the dimensions of a tensor.", "tokens": [51214, 407, 7132, 6110, 307, 562, 321, 1333, 295, 7929, 420, 18135, 264, 12819, 295, 257, 40863, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 716, "seek": 329464, "start": 3316.64, "end": 3323.64, "text": " So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four.", "tokens": [51464, 407, 294, 341, 1389, 11, 286, 5883, 1125, 257, 27822, 5893, 35193, 40863, 365, 12819, 732, 538, 1045, 538, 1451, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07087624723261053, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.00406960304826498}, {"id": 717, "seek": 332364, "start": 3323.64, "end": 3330.64, "text": " And we can use the transpose function to essentially flip any dimensions that we want.", "tokens": [50364, 400, 321, 393, 764, 264, 25167, 2445, 281, 4476, 7929, 604, 12819, 300, 321, 528, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09548732086464211, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0009109792299568653}, {"id": 718, "seek": 332364, "start": 3330.64, "end": 3337.64, "text": " So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension,", "tokens": [50714, 407, 437, 321, 434, 884, 307, 321, 434, 1237, 412, 264, 4018, 365, 382, 309, 3263, 3657, 281, 406, 584, 700, 10139, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09548732086464211, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0009109792299568653}, {"id": 719, "seek": 332364, "start": 3337.64, "end": 3341.64, "text": " but we're pretty much swapping the zero with position with the second.", "tokens": [51064, 457, 321, 434, 1238, 709, 1693, 10534, 264, 4018, 365, 2535, 365, 264, 1150, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09548732086464211, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0009109792299568653}, {"id": 720, "seek": 332364, "start": 3341.64, "end": 3345.64, "text": " So zero, one, two, we're swapping this one with this one.", "tokens": [51264, 407, 4018, 11, 472, 11, 732, 11, 321, 434, 1693, 10534, 341, 472, 365, 341, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09548732086464211, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0009109792299568653}, {"id": 721, "seek": 332364, "start": 3345.64, "end": 3351.64, "text": " So the end result, like you would probably guess the shape of this is going to be 432 instead of 234.", "tokens": [51464, 407, 264, 917, 1874, 11, 411, 291, 576, 1391, 2041, 264, 3909, 295, 341, 307, 516, 281, 312, 1017, 11440, 2602, 295, 6673, 19, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09548732086464211, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0009109792299568653}, {"id": 722, "seek": 335164, "start": 3351.64, "end": 3355.64, "text": " So you kind of just take a look at this and see, you know, which ones are being flipped.", "tokens": [50364, 407, 291, 733, 295, 445, 747, 257, 574, 412, 341, 293, 536, 11, 291, 458, 11, 597, 2306, 366, 885, 26273, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 723, "seek": 335164, "start": 3355.64, "end": 3359.64, "text": " And those are the dimensions and that's the output.", "tokens": [50564, 400, 729, 366, 264, 12819, 293, 300, 311, 264, 5598, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 724, "seek": 335164, "start": 3359.64, "end": 3361.64, "text": " So hopefully that makes sense.", "tokens": [50764, 407, 4696, 300, 1669, 2020, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 725, "seek": 335164, "start": 3361.64, "end": 3363.64, "text": " Next up, we have torch dot stack.", "tokens": [50864, 3087, 493, 11, 321, 362, 27822, 5893, 8630, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 726, "seek": 335164, "start": 3363.64, "end": 3366.64, "text": " And this is where we're actually going to go.", "tokens": [50964, 400, 341, 307, 689, 321, 434, 767, 516, 281, 352, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 727, "seek": 335164, "start": 3366.64, "end": 3368.64, "text": " We're going to we're going to do more of this.", "tokens": [51114, 492, 434, 516, 281, 321, 434, 516, 281, 360, 544, 295, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 728, "seek": 335164, "start": 3368.64, "end": 3375.64, "text": " We're actually going to use torch dot stack stack very shortly here when we're getting our batches.", "tokens": [51214, 492, 434, 767, 516, 281, 764, 27822, 5893, 8630, 8630, 588, 13392, 510, 562, 321, 434, 1242, 527, 15245, 279, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10240928404921786, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.00255071884021163}, {"id": 729, "seek": 337564, "start": 3375.64, "end": 3383.64, "text": " So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant,", "tokens": [50364, 407, 1604, 949, 562, 286, 390, 1417, 466, 15245, 2744, 293, 577, 321, 747, 257, 3840, 295, 613, 8474, 1214, 293, 321, 445, 8630, 552, 7410, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08840513759189182, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.011157209053635597}, {"id": 730, "seek": 337564, "start": 3383.64, "end": 3388.64, "text": " a giant length of integers or tokens.", "tokens": [50764, 257, 7410, 4641, 295, 41674, 420, 22667, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08840513759189182, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.011157209053635597}, {"id": 731, "seek": 337564, "start": 3388.64, "end": 3394.64, "text": " And all we're doing is we're just stacking them together in blocks or to make a batch.", "tokens": [51014, 400, 439, 321, 434, 884, 307, 321, 434, 445, 41376, 552, 1214, 294, 8474, 420, 281, 652, 257, 15245, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08840513759189182, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.011157209053635597}, {"id": 732, "seek": 337564, "start": 3394.64, "end": 3397.64, "text": " So that's pretty much what we're going to end up doing.", "tokens": [51314, 407, 300, 311, 1238, 709, 437, 321, 434, 516, 281, 917, 493, 884, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08840513759189182, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.011157209053635597}, {"id": 733, "seek": 337564, "start": 3397.64, "end": 3399.64, "text": " And that's what torch dot stack does.", "tokens": [51464, 400, 300, 311, 437, 27822, 5893, 8630, 775, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08840513759189182, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.011157209053635597}, {"id": 734, "seek": 339964, "start": 3399.64, "end": 3406.64, "text": " We can take something that's one dimensional and then we can stack it to make it two dimensional.", "tokens": [50364, 492, 393, 747, 746, 300, 311, 472, 18795, 293, 550, 321, 393, 8630, 309, 281, 652, 309, 732, 18795, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 735, "seek": 339964, "start": 3406.64, "end": 3412.64, "text": " We can take something that's two dimensional and stack it a bunch of times to make it three dimensional.", "tokens": [50714, 492, 393, 747, 746, 300, 311, 732, 18795, 293, 8630, 309, 257, 3840, 295, 1413, 281, 652, 309, 1045, 18795, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 736, "seek": 339964, "start": 3412.64, "end": 3418.64, "text": " Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other.", "tokens": [51014, 1610, 321, 393, 584, 1045, 18795, 11, 337, 1365, 11, 321, 362, 257, 3840, 295, 25415, 293, 321, 8630, 729, 322, 1192, 295, 1184, 661, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 737, "seek": 339964, "start": 3418.64, "end": 3419.64, "text": " Now it's four dimensional.", "tokens": [51314, 823, 309, 311, 1451, 18795, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 738, "seek": 339964, "start": 3419.64, "end": 3421.64, "text": " So hopefully that makes sense.", "tokens": [51364, 407, 4696, 300, 1669, 2020, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 739, "seek": 339964, "start": 3421.64, "end": 3425.64, "text": " All we're doing is we're just passing in each tensor that we're going to stack in order.", "tokens": [51464, 1057, 321, 434, 884, 307, 321, 434, 445, 8437, 294, 1184, 40863, 300, 321, 434, 516, 281, 8630, 294, 1668, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07821756496764067, "compression_ratio": 2.0043478260869567, "no_speech_prob": 0.4997759759426117}, {"id": 740, "seek": 342564, "start": 3425.64, "end": 3429.64, "text": " So this is our little output here and that's pretty much all it is.", "tokens": [50364, 407, 341, 307, 527, 707, 5598, 510, 293, 300, 311, 1238, 709, 439, 309, 307, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09732789006726496, "compression_ratio": 1.766304347826087, "no_speech_prob": 0.023680325597524643}, {"id": 741, "seek": 342564, "start": 3429.64, "end": 3437.64, "text": " The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish.", "tokens": [50564, 440, 958, 2445, 300, 311, 516, 281, 312, 534, 1021, 337, 527, 2316, 293, 321, 434, 516, 281, 312, 1228, 341, 264, 2302, 565, 490, 722, 281, 2413, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09732789006726496, "compression_ratio": 1.766304347826087, "no_speech_prob": 0.023680325597524643}, {"id": 742, "seek": 342564, "start": 3437.64, "end": 3438.64, "text": " It's really important.", "tokens": [50964, 467, 311, 534, 1021, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09732789006726496, "compression_ratio": 1.766304347826087, "no_speech_prob": 0.023680325597524643}, {"id": 743, "seek": 342564, "start": 3438.64, "end": 3440.64, "text": " It's called the nn dot linear function.", "tokens": [51014, 467, 311, 1219, 264, 297, 77, 5893, 8213, 2445, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09732789006726496, "compression_ratio": 1.766304347826087, "no_speech_prob": 0.023680325597524643}, {"id": 744, "seek": 342564, "start": 3440.64, "end": 3445.64, "text": " So it is a pretty much a function of the nn dot module.", "tokens": [51114, 407, 309, 307, 257, 1238, 709, 257, 2445, 295, 264, 297, 77, 5893, 10088, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09732789006726496, "compression_ratio": 1.766304347826087, "no_speech_prob": 0.023680325597524643}, {"id": 745, "seek": 344564, "start": 3445.64, "end": 3455.64, "text": " And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters.", "tokens": [50364, 400, 341, 307, 534, 1021, 570, 291, 434, 516, 281, 536, 1780, 322, 297, 77, 5893, 10088, 307, 309, 8306, 1340, 300, 575, 1466, 712, 9834, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10922561897026313, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.622114360332489}, {"id": 746, "seek": 344564, "start": 3455.64, "end": 3461.64, "text": " So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false.", "tokens": [50864, 407, 562, 321, 360, 257, 9887, 281, 746, 11, 562, 291, 3079, 257, 3364, 293, 257, 12577, 11, 294, 341, 1389, 11, 309, 603, 312, 7908, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10922561897026313, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.622114360332489}, {"id": 747, "seek": 344564, "start": 3461.64, "end": 3470.64, "text": " But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better.", "tokens": [51164, 583, 1238, 709, 562, 321, 3079, 257, 3364, 420, 257, 12577, 833, 297, 77, 5893, 10088, 11, 309, 486, 1466, 729, 293, 309, 603, 1813, 1101, 293, 1101, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10922561897026313, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.622114360332489}, {"id": 748, "seek": 347064, "start": 3470.64, "end": 3479.64, "text": " And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output.", "tokens": [50364, 400, 309, 603, 1936, 3847, 2361, 322, 577, 8559, 729, 366, 293, 577, 1998, 1629, 9834, 1565, 309, 281, 264, 14721, 5598, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08755021936753217, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.10083381831645966}, {"id": 749, "seek": 347064, "start": 3479.64, "end": 3486.64, "text": " So pretty much anything with nn dot linear is going to be very important and it's going to be learnable.", "tokens": [50814, 407, 1238, 709, 1340, 365, 297, 77, 5893, 8213, 307, 516, 281, 312, 588, 1021, 293, 309, 311, 516, 281, 312, 1466, 712, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08755021936753217, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.10083381831645966}, {"id": 750, "seek": 347064, "start": 3486.64, "end": 3488.64, "text": " So we can see over here.", "tokens": [51164, 407, 321, 393, 536, 670, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08755021936753217, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.10083381831645966}, {"id": 751, "seek": 347064, "start": 3488.64, "end": 3492.64, "text": " This is the tors.nn little site here on the docs.", "tokens": [51264, 639, 307, 264, 3930, 82, 13, 77, 77, 707, 3621, 510, 322, 264, 45623, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08755021936753217, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.10083381831645966}, {"id": 752, "seek": 347064, "start": 3492.64, "end": 3499.64, "text": " So we have containers, a bunch of different layers like activations, layers, pretty much just layers.", "tokens": [51464, 407, 321, 362, 17089, 11, 257, 3840, 295, 819, 7914, 411, 2430, 763, 11, 7914, 11, 1238, 709, 445, 7914, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08755021936753217, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.10083381831645966}, {"id": 753, "seek": 349964, "start": 3499.64, "end": 3500.64, "text": " That's all it is.", "tokens": [50364, 663, 311, 439, 309, 307, 13, 50414], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 754, "seek": 349964, "start": 3500.64, "end": 3503.64, "text": " And so these are these are important.", "tokens": [50414, 400, 370, 613, 366, 613, 366, 1021, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 755, "seek": 349964, "start": 3503.64, "end": 3506.64, "text": " We're going to, we're basically going to learn from these.", "tokens": [50564, 492, 434, 516, 281, 11, 321, 434, 1936, 516, 281, 1466, 490, 613, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 756, "seek": 349964, "start": 3506.64, "end": 3512.64, "text": " And you're going to see why we're going to use something called keys and values, keys, values and queers later on.", "tokens": [50714, 400, 291, 434, 516, 281, 536, 983, 321, 434, 516, 281, 764, 746, 1219, 9317, 293, 4190, 11, 9317, 11, 4190, 293, 631, 433, 1780, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 757, "seek": 349964, "start": 3512.64, "end": 3513.64, "text": " You'll see why those are important.", "tokens": [51014, 509, 603, 536, 983, 729, 366, 1021, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 758, "seek": 349964, "start": 3513.64, "end": 3518.64, "text": " But if that doesn't make sense yet, help me, let me illustrate value for you right now.", "tokens": [51064, 583, 498, 300, 1177, 380, 652, 2020, 1939, 11, 854, 385, 11, 718, 385, 23221, 2158, 337, 291, 558, 586, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 759, "seek": 349964, "start": 3518.64, "end": 3520.64, "text": " So I drew this out here.", "tokens": [51314, 407, 286, 12804, 341, 484, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 760, "seek": 349964, "start": 3520.64, "end": 3528.64, "text": " So if we look back at our examples, we have a, we make, we initialize a term.", "tokens": [51414, 407, 498, 321, 574, 646, 412, 527, 5110, 11, 321, 362, 257, 11, 321, 652, 11, 321, 5883, 1125, 257, 1433, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17787451416481542, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.015420762822031975}, {"id": 761, "seek": 352864, "start": 3528.64, "end": 3531.64, "text": " We make, we initialize a tensor.", "tokens": [50364, 492, 652, 11, 321, 5883, 1125, 257, 40863, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 762, "seek": 352864, "start": 3531.64, "end": 3533.64, "text": " It's 10, 10 and 10.", "tokens": [50514, 467, 311, 1266, 11, 1266, 293, 1266, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 763, "seek": 352864, "start": 3533.64, "end": 3536.64, "text": " What we're going to do is we're going to do a linear transformation.", "tokens": [50614, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 360, 257, 8213, 9887, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 764, "seek": 352864, "start": 3536.64, "end": 3538.64, "text": " This linear stands for linear transformation.", "tokens": [50764, 639, 8213, 7382, 337, 8213, 9887, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 765, "seek": 352864, "start": 3538.64, "end": 3544.64, "text": " So pretty much we're just going to apply a weight and a bias through each of these layers here.", "tokens": [50864, 407, 1238, 709, 321, 434, 445, 516, 281, 3079, 257, 3364, 293, 257, 12577, 807, 1184, 295, 613, 7914, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 766, "seek": 352864, "start": 3544.64, "end": 3549.64, "text": " So we have an input and we have an output x is our input, y is our output.", "tokens": [51164, 407, 321, 362, 364, 4846, 293, 321, 362, 364, 5598, 2031, 307, 527, 4846, 11, 288, 307, 527, 5598, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 767, "seek": 352864, "start": 3549.64, "end": 3553.64, "text": " And this is of size three and this is of size three.", "tokens": [51414, 400, 341, 307, 295, 2744, 1045, 293, 341, 307, 295, 2744, 1045, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 768, "seek": 352864, "start": 3553.64, "end": 3556.64, "text": " So pretty much we just need to make sure that these are lining up.", "tokens": [51614, 407, 1238, 709, 321, 445, 643, 281, 652, 988, 300, 613, 366, 19628, 493, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10763457440954494, "compression_ratio": 1.900414937759336, "no_speech_prob": 0.0031723324209451675}, {"id": 769, "seek": 355664, "start": 3556.64, "end": 3566.64, "text": " And for more context, the nn.sequential is sort of built off nn.linear.", "tokens": [50364, 400, 337, 544, 4319, 11, 264, 297, 77, 13, 11834, 2549, 307, 1333, 295, 3094, 766, 297, 77, 13, 28263, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1246627919814166, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.014499875716865063}, {"id": 770, "seek": 355664, "start": 3566.64, "end": 3572.64, "text": " So if we go ahead and search that up right now, this will make sense in a second here.", "tokens": [50864, 407, 498, 321, 352, 2286, 293, 3164, 300, 493, 558, 586, 11, 341, 486, 652, 2020, 294, 257, 1150, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1246627919814166, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.014499875716865063}, {"id": 771, "seek": 355664, "start": 3572.64, "end": 3577.64, "text": " This is also some good prerequisite knowledge in general for machine learning.", "tokens": [51164, 639, 307, 611, 512, 665, 38333, 34152, 3601, 294, 2674, 337, 3479, 2539, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1246627919814166, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.014499875716865063}, {"id": 772, "seek": 355664, "start": 3577.64, "end": 3585.64, "text": " So let's see nn.sequential doesn't show it here, but pretty much.", "tokens": [51414, 407, 718, 311, 536, 297, 77, 13, 11834, 2549, 1177, 380, 855, 309, 510, 11, 457, 1238, 709, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1246627919814166, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.014499875716865063}, {"id": 773, "seek": 358564, "start": 3585.64, "end": 3593.64, "text": " If you have, let's say, two, you have two input neurons and maybe you have one output neuron.", "tokens": [50364, 759, 291, 362, 11, 718, 311, 584, 11, 732, 11, 291, 362, 732, 4846, 22027, 293, 1310, 291, 362, 472, 5598, 34090, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09974498932178204, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.01854051649570465}, {"id": 774, "seek": 358564, "start": 3593.64, "end": 3595.64, "text": " Okay, you have a bunch of hidden layers in between here.", "tokens": [50764, 1033, 11, 291, 362, 257, 3840, 295, 7633, 7914, 294, 1296, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09974498932178204, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.01854051649570465}, {"id": 775, "seek": 358564, "start": 3595.64, "end": 3602.64, "text": " Let's say we have one, two, three, four, and then one, two, three.", "tokens": [50864, 961, 311, 584, 321, 362, 472, 11, 732, 11, 1045, 11, 1451, 11, 293, 550, 472, 11, 732, 11, 1045, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09974498932178204, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.01854051649570465}, {"id": 776, "seek": 358564, "start": 3602.64, "end": 3609.64, "text": " So pretty much you need to make sure that the inputs aligns with this hidden layer.", "tokens": [51214, 407, 1238, 709, 291, 643, 281, 652, 988, 300, 264, 15743, 7975, 82, 365, 341, 7633, 4583, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09974498932178204, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.01854051649570465}, {"id": 777, "seek": 358564, "start": 3609.64, "end": 3612.64, "text": " This hidden layer aligns with this one and this one aligns with this one.", "tokens": [51564, 639, 7633, 4583, 7975, 82, 365, 341, 472, 293, 341, 472, 7975, 82, 365, 341, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09974498932178204, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.01854051649570465}, {"id": 778, "seek": 361264, "start": 3612.64, "end": 3617.64, "text": " So you're going to have a transformation of two to four.", "tokens": [50364, 407, 291, 434, 516, 281, 362, 257, 9887, 295, 732, 281, 1451, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 779, "seek": 361264, "start": 3617.64, "end": 3625.64, "text": " So two, four, and then this one's going to be four to three, four to three,", "tokens": [50614, 407, 732, 11, 1451, 11, 293, 550, 341, 472, 311, 516, 281, 312, 1451, 281, 1045, 11, 1451, 281, 1045, 11, 51014], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 780, "seek": 361264, "start": 3625.64, "end": 3627.64, "text": " and then you're going to have a final one.", "tokens": [51014, 293, 550, 291, 434, 516, 281, 362, 257, 2572, 472, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 781, "seek": 361264, "start": 3627.64, "end": 3631.64, "text": " This is two to four right here, four to three here, and then this final one.", "tokens": [51114, 639, 307, 732, 281, 1451, 558, 510, 11, 1451, 281, 1045, 510, 11, 293, 550, 341, 2572, 472, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 782, "seek": 361264, "start": 3631.64, "end": 3633.64, "text": " It's going to be three to one.", "tokens": [51314, 467, 311, 516, 281, 312, 1045, 281, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 783, "seek": 361264, "start": 3633.64, "end": 3637.64, "text": " So you pretty much just need to make sure that these are lining up.", "tokens": [51414, 407, 291, 1238, 709, 445, 643, 281, 652, 988, 300, 613, 366, 19628, 493, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08313684837490905, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.07258190214633942}, {"id": 784, "seek": 363764, "start": 3637.64, "end": 3643.64, "text": " So we can see that we have two, four, and then this four is carried on from this output here.", "tokens": [50364, 407, 321, 393, 536, 300, 321, 362, 732, 11, 1451, 11, 293, 550, 341, 1451, 307, 9094, 322, 490, 341, 5598, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 785, "seek": 363764, "start": 3643.64, "end": 3647.64, "text": " And pretty much this will just make sure that our shapes are consistent.", "tokens": [50664, 400, 1238, 709, 341, 486, 445, 652, 988, 300, 527, 10854, 366, 8398, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 786, "seek": 363764, "start": 3647.64, "end": 3652.64, "text": " And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work.", "tokens": [50864, 400, 295, 1164, 11, 498, 436, 3212, 380, 8398, 11, 498, 264, 10854, 500, 380, 589, 484, 11, 264, 5221, 2935, 1582, 380, 589, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 787, "seek": 363764, "start": 3652.64, "end": 3654.64, "text": " So we need to make sure that our shapes are consistent.", "tokens": [51114, 407, 321, 643, 281, 652, 988, 300, 527, 10854, 366, 8398, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 788, "seek": 363764, "start": 3654.64, "end": 3659.64, "text": " If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets,", "tokens": [51214, 759, 300, 994, 380, 652, 2020, 11, 286, 458, 286, 478, 406, 411, 1687, 869, 412, 13468, 9482, 295, 18161, 36170, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 789, "seek": 363764, "start": 3659.64, "end": 3664.64, "text": " but if you're really interested, you could use chatGPT, of course.", "tokens": [51464, 457, 498, 291, 434, 534, 3102, 11, 291, 727, 764, 5081, 38, 47, 51, 11, 295, 1164, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1145191565155983, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.19175629317760468}, {"id": 790, "seek": 366464, "start": 3664.64, "end": 3669.64, "text": " And that's a really good learning resource, chatGPT, going on to get up discussions, maybe,", "tokens": [50364, 400, 300, 311, 257, 534, 665, 2539, 7684, 11, 5081, 38, 47, 51, 11, 516, 322, 281, 483, 493, 11088, 11, 1310, 11, 50614], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 791, "seek": 366464, "start": 3669.64, "end": 3672.64, "text": " or just looking at documentation.", "tokens": [50614, 420, 445, 1237, 412, 14333, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 792, "seek": 366464, "start": 3672.64, "end": 3679.64, "text": " And if you're not good at reading documentation, then you could take maybe some little keywords from here,", "tokens": [50764, 400, 498, 291, 434, 406, 665, 412, 3760, 14333, 11, 550, 291, 727, 747, 1310, 512, 707, 21009, 490, 510, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 793, "seek": 366464, "start": 3679.64, "end": 3682.64, "text": " like a sequential container.", "tokens": [51114, 411, 257, 42881, 10129, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 794, "seek": 366464, "start": 3682.64, "end": 3684.64, "text": " Well, what is a sequential container?", "tokens": [51264, 1042, 11, 437, 307, 257, 42881, 10129, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 795, "seek": 366464, "start": 3684.64, "end": 3689.64, "text": " You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation", "tokens": [51364, 509, 393, 1029, 5081, 38, 47, 51, 729, 3467, 295, 1651, 293, 445, 1333, 295, 257, 6374, 11403, 264, 14333, 51614], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 796, "seek": 366464, "start": 3689.64, "end": 3691.64, "text": " and figure things out step by step.", "tokens": [51614, 293, 2573, 721, 484, 1823, 538, 1823, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11015585426972291, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.05031261593103409}, {"id": 797, "seek": 369164, "start": 3691.64, "end": 3698.64, "text": " It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on.", "tokens": [50364, 467, 311, 534, 1152, 281, 458, 437, 291, 434, 884, 498, 291, 500, 380, 458, 439, 295, 264, 5221, 293, 439, 295, 264, 6828, 300, 366, 516, 322, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 798, "seek": 369164, "start": 3698.64, "end": 3700.64, "text": " You don't need to memorize them.", "tokens": [50714, 509, 500, 380, 643, 281, 27478, 552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 799, "seek": 369164, "start": 3700.64, "end": 3704.64, "text": " But while you're working with them, it's important to understand what they're really doing behind the scenes,", "tokens": [50814, 583, 1339, 291, 434, 1364, 365, 552, 11, 309, 311, 1021, 281, 1223, 437, 436, 434, 534, 884, 2261, 264, 8026, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 800, "seek": 369164, "start": 3704.64, "end": 3710.64, "text": " especially if you want to make an efficient and popular working neural net.", "tokens": [51014, 2318, 498, 291, 528, 281, 652, 364, 7148, 293, 3743, 1364, 18161, 2533, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 801, "seek": 369164, "start": 3710.64, "end": 3713.64, "text": " So that's that.", "tokens": [51314, 407, 300, 311, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 802, "seek": 369164, "start": 3713.64, "end": 3720.64, "text": " And pretty much what's going to happen here with these linear layers is we're just going to simply transform", "tokens": [51464, 400, 1238, 709, 437, 311, 516, 281, 1051, 510, 365, 613, 8213, 7914, 307, 321, 434, 445, 516, 281, 2935, 4088, 51814], "temperature": 0.0, "avg_logprob": -0.07693597067773869, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.022279798984527588}, {"id": 803, "seek": 372064, "start": 3720.64, "end": 3723.64, "text": " from one to the other input to output, no hidden layers.", "tokens": [50364, 490, 472, 281, 264, 661, 4846, 281, 5598, 11, 572, 7633, 7914, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 804, "seek": 372064, "start": 3723.64, "end": 3726.64, "text": " And we're just going to be able to learn best parameters for doing that.", "tokens": [50514, 400, 321, 434, 445, 516, 281, 312, 1075, 281, 1466, 1151, 9834, 337, 884, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 805, "seek": 372064, "start": 3726.64, "end": 3730.64, "text": " You're going to see why that's useful later.", "tokens": [50664, 509, 434, 516, 281, 536, 983, 300, 311, 4420, 1780, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 806, "seek": 372064, "start": 3730.64, "end": 3732.64, "text": " Now we have the softmax function.", "tokens": [50864, 823, 321, 362, 264, 2787, 41167, 2445, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 807, "seek": 372064, "start": 3732.64, "end": 3734.64, "text": " So that sounds scary.", "tokens": [50964, 407, 300, 3263, 6958, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 808, "seek": 372064, "start": 3734.64, "end": 3738.64, "text": " And the softmax function isn't actually what it sounds like at all.", "tokens": [51064, 400, 264, 2787, 41167, 2445, 1943, 380, 767, 437, 309, 3263, 411, 412, 439, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 809, "seek": 372064, "start": 3738.64, "end": 3740.64, "text": " Let me illustrate that for you right now.", "tokens": [51264, 961, 385, 23221, 300, 337, 291, 558, 586, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 810, "seek": 372064, "start": 3740.64, "end": 3745.64, "text": " So let's go ahead and change the color here.", "tokens": [51364, 407, 718, 311, 352, 2286, 293, 1319, 264, 2017, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08638220013312574, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0010648441966623068}, {"id": 811, "seek": 374564, "start": 3745.64, "end": 3755.64, "text": " So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc.", "tokens": [50364, 407, 718, 311, 584, 321, 362, 257, 10225, 11, 321, 362, 257, 472, 11, 732, 11, 1045, 11, 718, 311, 1286, 486, 652, 552, 12607, 935, 3547, 568, 13, 15, 11, 805, 13, 15, 11, 5183, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1524675809420072, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0033762301318347454}, {"id": 812, "seek": 374564, "start": 3755.64, "end": 3757.64, "text": " Right, floating points, whatever.", "tokens": [50864, 1779, 11, 12607, 2793, 11, 2035, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1524675809420072, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0033762301318347454}, {"id": 813, "seek": 374564, "start": 3757.64, "end": 3768.64, "text": " So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these.", "tokens": [50964, 407, 1238, 709, 498, 321, 829, 498, 321, 829, 341, 666, 264, 2787, 41167, 2445, 11, 437, 311, 516, 281, 1051, 307, 321, 434, 516, 281, 37871, 13024, 1184, 295, 613, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1524675809420072, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0033762301318347454}, {"id": 814, "seek": 374564, "start": 3768.64, "end": 3773.64, "text": " And we're going to divide them by the sum of all of these exponentiated.", "tokens": [51514, 400, 321, 434, 516, 281, 9845, 552, 538, 264, 2408, 295, 439, 295, 613, 37871, 72, 770, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1524675809420072, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0033762301318347454}, {"id": 815, "seek": 377364, "start": 3773.64, "end": 3777.64, "text": " So pretty much what's going to happen, let's say we exponentiate one.", "tokens": [50364, 407, 1238, 709, 437, 311, 516, 281, 1051, 11, 718, 311, 584, 321, 37871, 13024, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10429760782342208, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.010326110757887363}, {"id": 816, "seek": 377364, "start": 3777.64, "end": 3785.64, "text": " So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp.", "tokens": [50564, 407, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 360, 11, 341, 307, 437, 309, 311, 516, 281, 574, 411, 294, 3089, 11, 309, 311, 516, 281, 352, 472, 5893, 1278, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10429760782342208, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.010326110757887363}, {"id": 817, "seek": 377364, "start": 3785.64, "end": 3788.64, "text": " And I think I talked about this up here.", "tokens": [50964, 400, 286, 519, 286, 2825, 466, 341, 493, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10429760782342208, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.010326110757887363}, {"id": 818, "seek": 377364, "start": 3788.64, "end": 3795.64, "text": " This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating.", "tokens": [51114, 639, 307, 37871, 72, 990, 562, 321, 362, 568, 13, 29985, 281, 264, 1347, 295, 2035, 1230, 321, 434, 37871, 72, 990, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10429760782342208, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.010326110757887363}, {"id": 819, "seek": 379564, "start": 3795.64, "end": 3803.64, "text": " So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71.", "tokens": [50364, 407, 498, 321, 362, 341, 472, 11, 321, 434, 516, 281, 37871, 13024, 300, 293, 300, 311, 516, 281, 976, 505, 11, 309, 311, 516, 281, 976, 505, 568, 13, 29985, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17868663043510624, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.1992235779762268}, {"id": 820, "seek": 379564, "start": 3803.64, "end": 3815.64, "text": " And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two.", "tokens": [50764, 400, 321, 362, 341, 732, 510, 11, 293, 300, 311, 516, 281, 976, 505, 2035, 11, 2035, 732, 307, 37871, 72, 770, 568, 13, 29985, 11, 1347, 295, 732, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17868663043510624, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.1992235779762268}, {"id": 821, "seek": 379564, "start": 3815.64, "end": 3817.64, "text": " Okay, so we're going to get 7.34.", "tokens": [51364, 1033, 11, 370, 321, 434, 516, 281, 483, 1614, 13, 12249, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17868663043510624, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.1992235779762268}, {"id": 822, "seek": 381764, "start": 3818.64, "end": 3820.64, "text": " I'm going to get 7.34.", "tokens": [50414, 286, 478, 516, 281, 483, 1614, 13, 12249, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 823, "seek": 381764, "start": 3820.64, "end": 3822.64, "text": " Gorg my writing, it's terrible.", "tokens": [50514, 460, 4646, 452, 3579, 11, 309, 311, 6237, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 824, "seek": 381764, "start": 3822.64, "end": 3827.64, "text": " 2.71 to 3 cubed.", "tokens": [50614, 568, 13, 29985, 281, 805, 36510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 825, "seek": 381764, "start": 3827.64, "end": 3828.64, "text": " So 19.9.", "tokens": [50864, 407, 1294, 13, 24, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 826, "seek": 381764, "start": 3831.64, "end": 3835.64, "text": " So pretty much what's going to happen is we can rearrange this in a new array.", "tokens": [51064, 407, 1238, 709, 437, 311, 516, 281, 1051, 307, 321, 393, 39568, 341, 294, 257, 777, 10225, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 827, "seek": 381764, "start": 3835.64, "end": 3840.64, "text": " 7.34 and 19.9.", "tokens": [51264, 1614, 13, 12249, 293, 1294, 13, 24, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 828, "seek": 381764, "start": 3840.64, "end": 3846.64, "text": " So if we add all these up together, we add all these up together, we're going to get 2.71 plus this.", "tokens": [51514, 407, 498, 321, 909, 439, 613, 493, 1214, 11, 321, 909, 439, 613, 493, 1214, 11, 321, 434, 516, 281, 483, 568, 13, 29985, 1804, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1917179570053563, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.057478174567222595}, {"id": 829, "seek": 384664, "start": 3846.64, "end": 3848.64, "text": " Let's do this math real quick.", "tokens": [50364, 961, 311, 360, 341, 5221, 957, 1702, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 830, "seek": 384664, "start": 3848.64, "end": 3852.64, "text": " I'm just going to walk you through this to help you understand what the softmax function is doing.", "tokens": [50464, 286, 478, 445, 516, 281, 1792, 291, 807, 341, 281, 854, 291, 1223, 437, 264, 2787, 41167, 2445, 307, 884, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 831, "seek": 384664, "start": 3852.64, "end": 3860.64, "text": " 7.34 plus 19.9.", "tokens": [50664, 1614, 13, 12249, 1804, 1294, 13, 24, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 832, "seek": 384664, "start": 3860.64, "end": 3863.64, "text": " That's going to give us a total of 29.95.", "tokens": [51064, 663, 311, 516, 281, 976, 505, 257, 3217, 295, 9413, 13, 15718, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 833, "seek": 384664, "start": 3863.64, "end": 3864.64, "text": " Great.", "tokens": [51214, 3769, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 834, "seek": 384664, "start": 3864.64, "end": 3869.64, "text": " 29.95.", "tokens": [51264, 9413, 13, 15718, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11155407769339425, "compression_ratio": 1.3137254901960784, "no_speech_prob": 0.0014549816260114312}, {"id": 835, "seek": 386964, "start": 3869.64, "end": 3876.64, "text": " So all we do is we just divide each of these elements by the total.", "tokens": [50364, 407, 439, 321, 360, 307, 321, 445, 9845, 1184, 295, 613, 4959, 538, 264, 3217, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09811923209201084, "compression_ratio": 1.808641975308642, "no_speech_prob": 0.026348289102315903}, {"id": 836, "seek": 386964, "start": 3876.64, "end": 3880.64, "text": " So 2.71 divided by this is going to give us maybe x.", "tokens": [50714, 407, 568, 13, 29985, 6666, 538, 341, 307, 516, 281, 976, 505, 1310, 2031, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09811923209201084, "compression_ratio": 1.808641975308642, "no_speech_prob": 0.026348289102315903}, {"id": 837, "seek": 386964, "start": 3880.64, "end": 3884.64, "text": " And we do 7.34 divided by this is going to give us y.", "tokens": [50914, 400, 321, 360, 1614, 13, 12249, 6666, 538, 341, 307, 516, 281, 976, 505, 288, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09811923209201084, "compression_ratio": 1.808641975308642, "no_speech_prob": 0.026348289102315903}, {"id": 838, "seek": 386964, "start": 3884.64, "end": 3889.64, "text": " And then we have 19.9 divided by this is going to give us z.", "tokens": [51114, 400, 550, 321, 362, 1294, 13, 24, 6666, 538, 341, 307, 516, 281, 976, 505, 710, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09811923209201084, "compression_ratio": 1.808641975308642, "no_speech_prob": 0.026348289102315903}, {"id": 839, "seek": 386964, "start": 3889.64, "end": 3894.64, "text": " So pretty much you're going to exponentiate all of these.", "tokens": [51364, 407, 1238, 709, 291, 434, 516, 281, 37871, 13024, 439, 295, 613, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09811923209201084, "compression_ratio": 1.808641975308642, "no_speech_prob": 0.026348289102315903}, {"id": 840, "seek": 389464, "start": 3894.64, "end": 3897.64, "text": " You're going to add them together to create a total.", "tokens": [50364, 509, 434, 516, 281, 909, 552, 1214, 281, 1884, 257, 3217, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 841, "seek": 389464, "start": 3897.64, "end": 3902.64, "text": " And then you're going to divide each of those exponentiate elements by the exponentiated total.", "tokens": [50514, 400, 550, 291, 434, 516, 281, 9845, 1184, 295, 729, 37871, 13024, 4959, 538, 264, 37871, 72, 770, 3217, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 842, "seek": 389464, "start": 3902.64, "end": 3909.64, "text": " So after that, this x right here is just, we're just going to wrap these again.", "tokens": [50764, 407, 934, 300, 11, 341, 2031, 558, 510, 307, 445, 11, 321, 434, 445, 516, 281, 7019, 613, 797, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 843, "seek": 389464, "start": 3909.64, "end": 3917.64, "text": " And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z.", "tokens": [51114, 400, 439, 341, 2787, 41167, 2445, 307, 884, 307, 309, 311, 29942, 341, 502, 11, 568, 11, 805, 281, 2031, 11, 288, 11, 710, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 844, "seek": 389464, "start": 3917.64, "end": 3919.64, "text": " That's all it's doing.", "tokens": [51514, 663, 311, 439, 309, 311, 884, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 845, "seek": 389464, "start": 3919.64, "end": 3922.64, "text": " And yeah, it's not really crazy.", "tokens": [51614, 400, 1338, 11, 309, 311, 406, 534, 3219, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10927348270594517, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.010650994256138802}, {"id": 846, "seek": 392264, "start": 3922.64, "end": 3925.64, "text": " There's a weird formula for it.", "tokens": [50364, 821, 311, 257, 3657, 8513, 337, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 847, "seek": 392264, "start": 3925.64, "end": 3930.64, "text": " Softmax, softmax function.", "tokens": [50514, 16985, 41167, 11, 2787, 41167, 2445, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 848, "seek": 392264, "start": 3930.64, "end": 3935.64, "text": " So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here", "tokens": [50764, 407, 498, 291, 434, 294, 28999, 11, 291, 434, 516, 281, 12426, 1803, 570, 456, 311, 257, 688, 295, 2115, 294, 510, 51014], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 849, "seek": 392264, "start": 3935.64, "end": 3939.64, "text": " and a lot of math that's above the high school level.", "tokens": [51014, 293, 257, 688, 295, 5221, 300, 311, 3673, 264, 1090, 1395, 1496, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 850, "seek": 392264, "start": 3939.64, "end": 3943.64, "text": " But yeah, like this formula here, I believe this is what it is.", "tokens": [51214, 583, 1338, 11, 411, 341, 8513, 510, 11, 286, 1697, 341, 307, 437, 309, 307, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 851, "seek": 392264, "start": 3943.64, "end": 3946.64, "text": " Or standard unit, softmax function, there you go.", "tokens": [51414, 1610, 3832, 4985, 11, 2787, 41167, 2445, 11, 456, 291, 352, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 852, "seek": 392264, "start": 3946.64, "end": 3948.64, "text": " So pretty much this is what it does.", "tokens": [51564, 407, 1238, 709, 341, 307, 437, 309, 775, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 853, "seek": 392264, "start": 3948.64, "end": 3951.64, "text": " And there's your easy explanation of what it does.", "tokens": [51664, 400, 456, 311, 428, 1858, 10835, 295, 437, 309, 775, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1269251067062904, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.0035932441242039204}, {"id": 854, "seek": 395164, "start": 3951.64, "end": 3956.64, "text": " So you're going to see why this is useful later, but it's just important to know what's going on", "tokens": [50364, 407, 291, 434, 516, 281, 536, 983, 341, 307, 4420, 1780, 11, 457, 309, 311, 445, 1021, 281, 458, 437, 311, 516, 322, 50614], "temperature": 0.0, "avg_logprob": -0.11165827199032433, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.002050509676337242}, {"id": 855, "seek": 395164, "start": 3956.64, "end": 3962.64, "text": " so that you won't lag behind later in the course when this background knowledge becomes important.", "tokens": [50614, 370, 300, 291, 1582, 380, 8953, 2261, 1780, 294, 264, 1164, 562, 341, 3678, 3601, 3643, 1021, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11165827199032433, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.002050509676337242}, {"id": 856, "seek": 395164, "start": 3962.64, "end": 3968.64, "text": " So if we go over a little example of that, of the softmax function in code, it looks like this right here.", "tokens": [50914, 407, 498, 321, 352, 670, 257, 707, 1365, 295, 300, 11, 295, 264, 2787, 41167, 2445, 294, 3089, 11, 309, 1542, 411, 341, 558, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11165827199032433, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.002050509676337242}, {"id": 857, "seek": 395164, "start": 3968.64, "end": 3973.64, "text": " So we import torsha and n dot functional as f, f short for functional.", "tokens": [51214, 407, 321, 974, 3930, 82, 1641, 293, 297, 5893, 11745, 382, 283, 11, 283, 2099, 337, 11745, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11165827199032433, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.002050509676337242}, {"id": 858, "seek": 395164, "start": 3973.64, "end": 3977.64, "text": " And we pretty much just do f dot softmax and then plug in a tensor.", "tokens": [51464, 400, 321, 1238, 709, 445, 360, 283, 5893, 2787, 41167, 293, 550, 5452, 294, 257, 40863, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11165827199032433, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.002050509676337242}, {"id": 859, "seek": 397764, "start": 3977.64, "end": 3983.64, "text": " And what we want the dimension to be the output dimension.", "tokens": [50364, 400, 437, 321, 528, 264, 10139, 281, 312, 264, 5598, 10139, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 860, "seek": 397764, "start": 3983.64, "end": 3989.64, "text": " So if we plug this into here and we print it out, we go and print it out.", "tokens": [50664, 407, 498, 321, 5452, 341, 666, 510, 293, 321, 4482, 309, 484, 11, 321, 352, 293, 4482, 309, 484, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 861, "seek": 397764, "start": 3989.64, "end": 3995.64, "text": " It's going to take a second.", "tokens": [50964, 467, 311, 516, 281, 747, 257, 1150, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 862, "seek": 397764, "start": 3995.64, "end": 3999.64, "text": " Torch is not defined. So let's run this from the top here.", "tokens": [51264, 7160, 339, 307, 406, 7642, 13, 407, 718, 311, 1190, 341, 490, 264, 1192, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 863, "seek": 397764, "start": 3999.64, "end": 4002.64, "text": " Boom.", "tokens": [51464, 15523, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 864, "seek": 397764, "start": 4002.64, "end": 4004.64, "text": " And let's try that again. Boom. There we go.", "tokens": [51614, 400, 718, 311, 853, 300, 797, 13, 15523, 13, 821, 321, 352, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1748708163990694, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.051827527582645416}, {"id": 865, "seek": 400464, "start": 4004.64, "end": 4008.64, "text": " So if you took all those values, let's actually do this again from scratch.", "tokens": [50364, 407, 498, 291, 1890, 439, 729, 4190, 11, 718, 311, 767, 360, 341, 797, 490, 8459, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13943483954981753, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.04022425040602684}, {"id": 866, "seek": 400464, "start": 4008.64, "end": 4016.64, "text": " So we do 2.71, 2.71 divided by 29.95.", "tokens": [50564, 407, 321, 360, 568, 13, 29985, 11, 568, 13, 29985, 6666, 538, 9413, 13, 15718, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13943483954981753, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.04022425040602684}, {"id": 867, "seek": 400464, "start": 4016.64, "end": 4022.64, "text": " We get 0.09, 0.09. Good.", "tokens": [50964, 492, 483, 1958, 13, 13811, 11, 1958, 13, 13811, 13, 2205, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13943483954981753, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.04022425040602684}, {"id": 868, "seek": 400464, "start": 4022.64, "end": 4032.64, "text": " And then if we do 7.34 divided by 29.95, we get 0.245.", "tokens": [51264, 400, 550, 498, 321, 360, 1614, 13, 12249, 6666, 538, 9413, 13, 15718, 11, 321, 483, 1958, 13, 7911, 20, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13943483954981753, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.04022425040602684}, {"id": 869, "seek": 403264, "start": 4032.64, "end": 4036.64, "text": " So 0.245. Well, it's kind of close.", "tokens": [50364, 407, 1958, 13, 7911, 20, 13, 1042, 11, 309, 311, 733, 295, 1998, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17992868528261288, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.00638751033693552}, {"id": 870, "seek": 403264, "start": 4036.64, "end": 4042.64, "text": " Really close actually. And then 66.52. So if we go, what was that last one there?", "tokens": [50564, 4083, 1998, 767, 13, 400, 550, 21126, 13, 17602, 13, 407, 498, 321, 352, 11, 437, 390, 300, 1036, 472, 456, 30, 50864], "temperature": 0.0, "avg_logprob": -0.17992868528261288, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.00638751033693552}, {"id": 871, "seek": 403264, "start": 4042.64, "end": 4050.64, "text": " 19.9. So we do 19.9 divided by 29.95.", "tokens": [50864, 1294, 13, 24, 13, 407, 321, 360, 1294, 13, 24, 6666, 538, 9413, 13, 15718, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17992868528261288, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.00638751033693552}, {"id": 872, "seek": 403264, "start": 4050.64, "end": 4055.64, "text": " 66.4. So 66.5. It's pretty close.", "tokens": [51264, 21126, 13, 19, 13, 407, 21126, 13, 20, 13, 467, 311, 1238, 1998, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17992868528261288, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.00638751033693552}, {"id": 873, "seek": 403264, "start": 4055.64, "end": 4061.64, "text": " Again, we're rounding, so it's not perfectly accurate.", "tokens": [51514, 3764, 11, 321, 434, 48237, 11, 370, 309, 311, 406, 6239, 8559, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17992868528261288, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.00638751033693552}, {"id": 874, "seek": 406164, "start": 4061.64, "end": 4067.64, "text": " As you can see, they're very close and for only having two decimal places, we did pretty good.", "tokens": [50364, 1018, 291, 393, 536, 11, 436, 434, 588, 1998, 293, 337, 787, 1419, 732, 26601, 3190, 11, 321, 630, 1238, 665, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11902324676513672, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.009706169366836548}, {"id": 875, "seek": 406164, "start": 4067.64, "end": 4072.64, "text": " So that's just sort of illustrating what the softmax function does and what it looks like in code.", "tokens": [50664, 407, 300, 311, 445, 1333, 295, 8490, 8754, 437, 264, 2787, 41167, 2445, 775, 293, 437, 309, 1542, 411, 294, 3089, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11902324676513672, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.009706169366836548}, {"id": 876, "seek": 406164, "start": 4072.64, "end": 4081.64, "text": " We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line.", "tokens": [50914, 492, 362, 341, 1333, 295, 3909, 510, 13, 17182, 12819, 1355, 321, 445, 747, 11, 291, 458, 11, 309, 311, 445, 733, 295, 257, 2997, 1622, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11902324676513672, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.009706169366836548}, {"id": 877, "seek": 406164, "start": 4081.64, "end": 4084.64, "text": " It's just like that.", "tokens": [51364, 467, 311, 445, 411, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11902324676513672, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.009706169366836548}, {"id": 878, "seek": 406164, "start": 4084.64, "end": 4087.64, "text": " So now we're going to go over embeddings.", "tokens": [51514, 407, 586, 321, 434, 516, 281, 352, 670, 12240, 29432, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11902324676513672, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.009706169366836548}, {"id": 879, "seek": 408764, "start": 4087.64, "end": 4090.64, "text": " And I'm not actually, I don't have any code for this yet.", "tokens": [50364, 400, 286, 478, 406, 767, 11, 286, 500, 380, 362, 604, 3089, 337, 341, 1939, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14422587758487032, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.0609583742916584}, {"id": 880, "seek": 408764, "start": 4090.64, "end": 4096.639999999999, "text": " We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills", "tokens": [50514, 492, 434, 516, 281, 2573, 341, 484, 1823, 538, 1823, 365, 5081, 26039, 51, 11, 570, 286, 528, 281, 855, 291, 1074, 1333, 295, 264, 3942, 50814], "temperature": 0.0, "avg_logprob": -0.14422587758487032, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.0609583742916584}, {"id": 881, "seek": 408764, "start": 4096.639999999999, "end": 4103.639999999999, "text": " and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning.", "tokens": [50814, 293, 437, 309, 2516, 281, 9943, 11403, 364, 1558, 420, 2445, 420, 445, 1223, 577, 746, 1985, 294, 2674, 294, 3479, 2539, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14422587758487032, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.0609583742916584}, {"id": 882, "seek": 408764, "start": 4103.639999999999, "end": 4113.639999999999, "text": " So if we pop in a chat GPT here, we say, what is an end dot embedding?", "tokens": [51164, 407, 498, 321, 1665, 294, 257, 5081, 26039, 51, 510, 11, 321, 584, 11, 437, 307, 364, 917, 5893, 12240, 3584, 30, 51664], "temperature": 0.0, "avg_logprob": -0.14422587758487032, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.0609583742916584}, {"id": 883, "seek": 411364, "start": 4113.64, "end": 4125.64, "text": " And then dots. Let me type in a non-bedding class in the PyTorch library.", "tokens": [50364, 400, 550, 15026, 13, 961, 385, 2010, 294, 257, 2107, 12, 2883, 3584, 1508, 294, 264, 9953, 51, 284, 339, 6405, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2645574002652555, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.0022517116740345955}, {"id": 884, "seek": 411364, "start": 4125.64, "end": 4131.64, "text": " Okay, actual language processing max maps each discrete input to a dense vector representation.", "tokens": [50964, 1033, 11, 3539, 2856, 9007, 11469, 11317, 1184, 27706, 4846, 281, 257, 18011, 8062, 10290, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2645574002652555, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.0022517116740345955}, {"id": 885, "seek": 411364, "start": 4131.64, "end": 4134.64, "text": " Okay, how does this work? Let's see.", "tokens": [51264, 1033, 11, 577, 775, 341, 589, 30, 961, 311, 536, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2645574002652555, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.0022517116740345955}, {"id": 886, "seek": 411364, "start": 4134.64, "end": 4138.64, "text": " So we have some vocab. So that's probably our vocabulary size.", "tokens": [51414, 407, 321, 362, 512, 2329, 455, 13, 407, 300, 311, 1391, 527, 19864, 2744, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2645574002652555, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.0022517116740345955}, {"id": 887, "seek": 413864, "start": 4138.64, "end": 4145.64, "text": " I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set.", "tokens": [50364, 286, 519, 321, 2825, 466, 300, 3071, 11, 19864, 2744, 11, 577, 867, 4342, 11, 577, 867, 3845, 4342, 366, 767, 294, 527, 1412, 992, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11418625286647252, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.04467495158314705}, {"id": 888, "seek": 413864, "start": 4145.64, "end": 4151.64, "text": " That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter.", "tokens": [50714, 663, 311, 264, 19864, 2744, 13, 400, 550, 512, 12240, 3584, 10139, 510, 11, 597, 307, 257, 9848, 13075, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11418625286647252, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.04467495158314705}, {"id": 889, "seek": 413864, "start": 4151.64, "end": 4156.64, "text": " So let's see. This doesn't quite make sense to me yet.", "tokens": [51014, 407, 718, 311, 536, 13, 639, 1177, 380, 1596, 652, 2020, 281, 385, 1939, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11418625286647252, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.04467495158314705}, {"id": 890, "seek": 413864, "start": 4156.64, "end": 4159.64, "text": " So maybe I want to learn what does this actually look like?", "tokens": [51264, 407, 1310, 286, 528, 281, 1466, 437, 775, 341, 767, 574, 411, 30, 51414], "temperature": 0.0, "avg_logprob": -0.11418625286647252, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.04467495158314705}, {"id": 891, "seek": 415964, "start": 4159.64, "end": 4173.64, "text": " Can you explain this to a, maybe an eighth grader and provide a visualization?", "tokens": [50364, 1664, 291, 2903, 341, 281, 257, 11, 1310, 364, 19495, 2771, 260, 293, 2893, 257, 5056, 2144, 30, 51064], "temperature": 0.0, "avg_logprob": -0.20302273333072662, "compression_ratio": 1.389937106918239, "no_speech_prob": 0.15193332731723785}, {"id": 892, "seek": 415964, "start": 4173.64, "end": 4177.64, "text": " Certainly. Okay.", "tokens": [51064, 16628, 13, 1033, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20302273333072662, "compression_ratio": 1.389937106918239, "no_speech_prob": 0.15193332731723785}, {"id": 893, "seek": 415964, "start": 4177.64, "end": 4182.64, "text": " Little secret codes that represent the meaning of the words. Okay, that helps.", "tokens": [51264, 8022, 4054, 14211, 300, 2906, 264, 3620, 295, 264, 2283, 13, 1033, 11, 300, 3665, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20302273333072662, "compression_ratio": 1.389937106918239, "no_speech_prob": 0.15193332731723785}, {"id": 894, "seek": 415964, "start": 4182.64, "end": 4187.64, "text": " So if we have cat, okay, so cat, cat's a word.", "tokens": [51514, 407, 498, 321, 362, 3857, 11, 1392, 11, 370, 3857, 11, 3857, 311, 257, 1349, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20302273333072662, "compression_ratio": 1.389937106918239, "no_speech_prob": 0.15193332731723785}, {"id": 895, "seek": 418764, "start": 4187.64, "end": 4193.64, "text": " So maybe we want to know what it would look like on a character level.", "tokens": [50364, 407, 1310, 321, 528, 281, 458, 437, 309, 576, 574, 411, 322, 257, 2517, 1496, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09854753681870758, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.004330703988671303}, {"id": 896, "seek": 418764, "start": 4193.64, "end": 4203.64, "text": " What about on a character level instead of word level?", "tokens": [50664, 708, 466, 322, 257, 2517, 1496, 2602, 295, 1349, 1496, 30, 51164], "temperature": 0.0, "avg_logprob": -0.09854753681870758, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.004330703988671303}, {"id": 897, "seek": 418764, "start": 4203.64, "end": 4205.64, "text": " So it's probably going to look very similar.", "tokens": [51164, 407, 309, 311, 1391, 516, 281, 574, 588, 2531, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09854753681870758, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.004330703988671303}, {"id": 898, "seek": 418764, "start": 4205.64, "end": 4210.64, "text": " We have this little vector here storing some information about whatever this is.", "tokens": [51264, 492, 362, 341, 707, 8062, 510, 26085, 512, 1589, 466, 2035, 341, 307, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09854753681870758, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.004330703988671303}, {"id": 899, "seek": 421064, "start": 4210.64, "end": 4218.64, "text": " So a, it means this here. Okay, so as your point to, and this is really useful.", "tokens": [50364, 407, 257, 11, 309, 1355, 341, 510, 13, 1033, 11, 370, 382, 428, 935, 281, 11, 293, 341, 307, 534, 4420, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1008479396502177, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.06850025802850723}, {"id": 900, "seek": 421064, "start": 4218.64, "end": 4221.64, "text": " So we've pretty much just learned what embedding vectors does.", "tokens": [50764, 407, 321, 600, 1238, 709, 445, 3264, 437, 12240, 3584, 18875, 775, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1008479396502177, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.06850025802850723}, {"id": 901, "seek": 421064, "start": 4221.64, "end": 4230.64, "text": " And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character.", "tokens": [50914, 400, 498, 291, 2378, 380, 4305, 493, 365, 341, 11, 1238, 709, 437, 436, 603, 360, 307, 436, 603, 3531, 512, 8062, 295, 1589, 466, 341, 2517, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1008479396502177, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.06850025802850723}, {"id": 902, "seek": 421064, "start": 4230.64, "end": 4234.64, "text": " And we don't even know what each of these elements mean.", "tokens": [51364, 400, 321, 500, 380, 754, 458, 437, 1184, 295, 613, 4959, 914, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1008479396502177, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.06850025802850723}, {"id": 903, "seek": 421064, "start": 4234.64, "end": 4235.64, "text": " We don't know what they mean.", "tokens": [51564, 492, 500, 380, 458, 437, 436, 914, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1008479396502177, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.06850025802850723}, {"id": 904, "seek": 423564, "start": 4235.64, "end": 4241.64, "text": " This could be maybe positivity or should be the start of a word or it could be any piece of information,", "tokens": [50364, 639, 727, 312, 1310, 35198, 420, 820, 312, 264, 722, 295, 257, 1349, 420, 309, 727, 312, 604, 2522, 295, 1589, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10610757872115734, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.2843253016471863}, {"id": 905, "seek": 423564, "start": 4241.64, "end": 4244.64, "text": " maybe something we can't even comprehend yet.", "tokens": [50664, 1310, 746, 321, 393, 380, 754, 38183, 1939, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10610757872115734, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.2843253016471863}, {"id": 906, "seek": 423564, "start": 4244.64, "end": 4255.64, "text": " But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before,", "tokens": [50814, 583, 264, 935, 307, 11, 498, 321, 767, 976, 552, 18875, 293, 321, 3154, 613, 666, 257, 3209, 293, 1466, 570, 382, 321, 1866, 949, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10610757872115734, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.2843253016471863}, {"id": 907, "seek": 423564, "start": 4255.64, "end": 4262.64, "text": " nn.embedding right here is a part of the nn.module.", "tokens": [51364, 297, 77, 13, 443, 2883, 3584, 558, 510, 307, 257, 644, 295, 264, 297, 77, 13, 8014, 2271, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10610757872115734, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.2843253016471863}, {"id": 908, "seek": 426264, "start": 4262.64, "end": 4265.64, "text": " So these are learnable parameters, which is great.", "tokens": [50364, 407, 613, 366, 1466, 712, 9834, 11, 597, 307, 869, 13, 50514], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 909, "seek": 426264, "start": 4265.64, "end": 4268.64, "text": " So it's actually going to learn the importance of each letter,", "tokens": [50514, 407, 309, 311, 767, 516, 281, 1466, 264, 7379, 295, 1184, 5063, 11, 50664], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 910, "seek": 426264, "start": 4268.64, "end": 4271.64, "text": " and it's going to be able to produce some amazing results.", "tokens": [50664, 293, 309, 311, 516, 281, 312, 1075, 281, 5258, 512, 2243, 3542, 13, 50814], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 911, "seek": 426264, "start": 4271.64, "end": 4281.64, "text": " So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter.", "tokens": [50814, 407, 294, 2099, 11, 264, 12240, 3584, 18875, 366, 4476, 257, 8062, 420, 257, 29054, 10290, 295, 264, 16149, 295, 257, 5063, 13, 51314], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 912, "seek": 426264, "start": 4281.64, "end": 4285.64, "text": " In our case, it's character level, not subword, not word, it's character level.", "tokens": [51314, 682, 527, 1389, 11, 309, 311, 2517, 1496, 11, 406, 1422, 7462, 11, 406, 1349, 11, 309, 311, 2517, 1496, 13, 51514], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 913, "seek": 426264, "start": 4285.64, "end": 4288.64, "text": " So it's going to represent some meaning about those.", "tokens": [51514, 407, 309, 311, 516, 281, 2906, 512, 3620, 466, 729, 13, 51664], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 914, "seek": 426264, "start": 4288.64, "end": 4290.64, "text": " So that's what embedding vectors are.", "tokens": [51664, 407, 300, 311, 437, 12240, 3584, 18875, 366, 13, 51764], "temperature": 0.0, "avg_logprob": -0.057379033830430776, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.011685524135828018}, {"id": 915, "seek": 429064, "start": 4290.64, "end": 4292.64, "text": " Let's go figure out how they work in code.", "tokens": [50364, 961, 311, 352, 2573, 484, 577, 436, 589, 294, 3089, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06998353738051194, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01825883239507675}, {"id": 916, "seek": 429064, "start": 4292.64, "end": 4298.64, "text": " We have this little character level embedding vector and it contains a list.", "tokens": [50464, 492, 362, 341, 707, 2517, 1496, 12240, 3584, 8062, 293, 309, 8306, 257, 1329, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06998353738051194, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01825883239507675}, {"id": 917, "seek": 429064, "start": 4298.64, "end": 4305.64, "text": " There's five elements in here, one, two, three, four, five, and it's by the vocab size.", "tokens": [50764, 821, 311, 1732, 4959, 294, 510, 11, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 293, 309, 311, 538, 264, 2329, 455, 2744, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06998353738051194, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01825883239507675}, {"id": 918, "seek": 429064, "start": 4305.64, "end": 4311.64, "text": " So we have all of our vocabulary by the length of each embedding vector.", "tokens": [51114, 407, 321, 362, 439, 295, 527, 19864, 538, 264, 4641, 295, 1184, 12240, 3584, 8062, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06998353738051194, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01825883239507675}, {"id": 919, "seek": 431164, "start": 4311.64, "end": 4316.64, "text": " So this actually makes sense because our vocab size by the embedding dimension,", "tokens": [50364, 407, 341, 767, 1669, 2020, 570, 527, 2329, 455, 2744, 538, 264, 12240, 3584, 10139, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10548630189359857, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.4412956237792969}, {"id": 920, "seek": 431164, "start": 4316.64, "end": 4322.64, "text": " which is how much information is actually being stored in each of these characters.", "tokens": [50614, 597, 307, 577, 709, 1589, 307, 767, 885, 12187, 294, 1184, 295, 613, 4342, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10548630189359857, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.4412956237792969}, {"id": 921, "seek": 431164, "start": 4322.64, "end": 4324.64, "text": " So this now is very easy to understand.", "tokens": [50914, 407, 341, 586, 307, 588, 1858, 281, 1223, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10548630189359857, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.4412956237792969}, {"id": 922, "seek": 431164, "start": 4324.64, "end": 4329.64, "text": " I'm just going to copy this code from here and I'm going to paste it down here.", "tokens": [51014, 286, 478, 445, 516, 281, 5055, 341, 3089, 490, 510, 293, 286, 478, 516, 281, 9163, 309, 760, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10548630189359857, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.4412956237792969}, {"id": 923, "seek": 431164, "start": 4329.64, "end": 4336.64, "text": " And let's just get rid of the torch because we already initialized that above.", "tokens": [51264, 400, 718, 311, 445, 483, 3973, 295, 264, 27822, 570, 321, 1217, 5883, 1602, 300, 3673, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10548630189359857, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.4412956237792969}, {"id": 924, "seek": 433664, "start": 4337.64, "end": 4342.64, "text": " So if we just run this, actually, let's turn that down to maybe a thousand characters.", "tokens": [50414, 407, 498, 321, 445, 1190, 341, 11, 767, 11, 718, 311, 1261, 300, 760, 281, 1310, 257, 4714, 4342, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1411618883647616, "compression_ratio": 1.3916083916083917, "no_speech_prob": 0.020019739866256714}, {"id": 925, "seek": 433664, "start": 4342.64, "end": 4344.64, "text": " Let's try that out.", "tokens": [50664, 961, 311, 853, 300, 484, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1411618883647616, "compression_ratio": 1.3916083916083917, "no_speech_prob": 0.020019739866256714}, {"id": 926, "seek": 433664, "start": 4345.64, "end": 4347.64, "text": " And it's not defined.", "tokens": [50814, 400, 309, 311, 406, 7642, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1411618883647616, "compression_ratio": 1.3916083916083917, "no_speech_prob": 0.020019739866256714}, {"id": 927, "seek": 433664, "start": 4347.64, "end": 4349.64, "text": " We did not initialize it.", "tokens": [50914, 492, 630, 406, 5883, 1125, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1411618883647616, "compression_ratio": 1.3916083916083917, "no_speech_prob": 0.020019739866256714}, {"id": 928, "seek": 433664, "start": 4360.64, "end": 4363.64, "text": " So let's go back down here and look at that.", "tokens": [51564, 407, 718, 311, 352, 646, 760, 510, 293, 574, 412, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1411618883647616, "compression_ratio": 1.3916083916083917, "no_speech_prob": 0.020019739866256714}, {"id": 929, "seek": 436364, "start": 4363.64, "end": 4370.64, "text": " So this dot shape is going to essentially show the shape of it this much by this much.", "tokens": [50364, 407, 341, 5893, 3909, 307, 516, 281, 4476, 855, 264, 3909, 295, 309, 341, 709, 538, 341, 709, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 930, "seek": 436364, "start": 4370.64, "end": 4372.64, "text": " So it's four by a hundred.", "tokens": [50714, 407, 309, 311, 1451, 538, 257, 3262, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 931, "seek": 436364, "start": 4372.64, "end": 4379.64, "text": " And yeah, so we can we can work with these and we can store stuff about characters in them.", "tokens": [50814, 400, 1338, 11, 370, 321, 393, 321, 393, 589, 365, 613, 293, 321, 393, 3531, 1507, 466, 4342, 294, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 932, "seek": 436364, "start": 4379.64, "end": 4385.64, "text": " And you're going to see this in the next lecture, how we actually use embedding vectors.", "tokens": [51164, 400, 291, 434, 516, 281, 536, 341, 294, 264, 958, 7991, 11, 577, 321, 767, 764, 12240, 3584, 18875, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 933, "seek": 436364, "start": 4385.64, "end": 4389.64, "text": " So no need to worry if a lot of this doesn't make sense yet.", "tokens": [51464, 407, 572, 643, 281, 3292, 498, 257, 688, 295, 341, 1177, 380, 652, 2020, 1939, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 934, "seek": 436364, "start": 4389.64, "end": 4390.64, "text": " That's fine.", "tokens": [51664, 663, 311, 2489, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09765892402798522, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.013013914227485657}, {"id": 935, "seek": 439064, "start": 4390.64, "end": 4393.64, "text": " You're going to learn a little bit more about how we use these over the course.", "tokens": [50364, 509, 434, 516, 281, 1466, 257, 707, 857, 544, 466, 577, 321, 764, 613, 670, 264, 1164, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 936, "seek": 439064, "start": 4393.64, "end": 4397.64, "text": " You're going to get more confident with using them even in your own projects.", "tokens": [50514, 509, 434, 516, 281, 483, 544, 6679, 365, 1228, 552, 754, 294, 428, 1065, 4455, 13, 50714], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 937, "seek": 439064, "start": 4397.64, "end": 4399.64, "text": " So don't don't stress about it too much right now.", "tokens": [50714, 407, 500, 380, 500, 380, 4244, 466, 309, 886, 709, 558, 586, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 938, "seek": 439064, "start": 4399.64, "end": 4403.64, "text": " Embeddings are pretty tricky at first to learn.", "tokens": [50814, 24234, 292, 29432, 366, 1238, 12414, 412, 700, 281, 1466, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 939, "seek": 439064, "start": 4403.64, "end": 4405.64, "text": " So don't worry about that too much.", "tokens": [51014, 407, 500, 380, 3292, 466, 300, 886, 709, 13, 51114], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 940, "seek": 439064, "start": 4405.64, "end": 4417.64, "text": " But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks.", "tokens": [51114, 583, 456, 366, 257, 1326, 544, 721, 286, 528, 281, 352, 670, 445, 281, 483, 505, 4927, 337, 512, 295, 264, 8213, 21989, 293, 8141, 27290, 294, 1729, 300, 321, 434, 516, 281, 312, 884, 294, 18161, 9590, 13, 51714], "temperature": 0.0, "avg_logprob": -0.05944387118021647, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.024042820557951927}, {"id": 941, "seek": 441764, "start": 4417.64, "end": 4429.64, "text": " So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say.", "tokens": [50364, 407, 498, 321, 362, 11, 286, 1604, 949, 321, 7373, 484, 341, 707, 12325, 295, 341, 307, 767, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 457, 561, 411, 281, 818, 309, 257, 18161, 3209, 570, 309, 311, 3571, 281, 584, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14116436602121377, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.08626728504896164}, {"id": 942, "seek": 441764, "start": 4429.64, "end": 4433.64, "text": " But that's the architecture of this multilayer perceptron.", "tokens": [50964, 583, 300, 311, 264, 9482, 295, 341, 2120, 388, 11167, 43276, 2044, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14116436602121377, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.08626728504896164}, {"id": 943, "seek": 441764, "start": 4433.64, "end": 4438.64, "text": " But pretty much what's happening is we have a little input here and we have a white matrix.", "tokens": [51164, 583, 1238, 709, 437, 311, 2737, 307, 321, 362, 257, 707, 4846, 510, 293, 321, 362, 257, 2418, 8141, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14116436602121377, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.08626728504896164}, {"id": 944, "seek": 443864, "start": 4438.64, "end": 4441.64, "text": " So white matrix is looks like this.", "tokens": [50364, 407, 2418, 8141, 307, 1542, 411, 341, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18953137065089026, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.5114027857780457}, {"id": 945, "seek": 443864, "start": 4441.64, "end": 4451.64, "text": " It's like this and we have some, we have some values in between X1, Y1 and maybe Z1.", "tokens": [50514, 467, 311, 411, 341, 293, 321, 362, 512, 11, 321, 362, 512, 4190, 294, 1296, 1783, 16, 11, 398, 16, 293, 1310, 1176, 16, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18953137065089026, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.5114027857780457}, {"id": 946, "seek": 443864, "start": 4451.64, "end": 4457.64, "text": " So a bunch of weights and maybe biases to that we add to it.", "tokens": [51014, 407, 257, 3840, 295, 17443, 293, 1310, 32152, 281, 300, 321, 909, 281, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18953137065089026, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.5114027857780457}, {"id": 947, "seek": 443864, "start": 4457.64, "end": 4463.64, "text": " So the tricky part is how do we actually multiply our input by this white matrix?", "tokens": [51314, 407, 264, 12414, 644, 307, 577, 360, 321, 767, 12972, 527, 4846, 538, 341, 2418, 8141, 30, 51614], "temperature": 0.0, "avg_logprob": -0.18953137065089026, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.5114027857780457}, {"id": 948, "seek": 443864, "start": 4463.64, "end": 4465.64, "text": " We're just doing one matrix times another.", "tokens": [51614, 492, 434, 445, 884, 472, 8141, 1413, 1071, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18953137065089026, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.5114027857780457}, {"id": 949, "seek": 446564, "start": 4465.64, "end": 4467.64, "text": " Well, that's called matrix multiplication.", "tokens": [50364, 1042, 11, 300, 311, 1219, 8141, 27290, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08783977372305733, "compression_ratio": 1.6512820512820512, "no_speech_prob": 0.05663489177823067}, {"id": 950, "seek": 446564, "start": 4467.64, "end": 4470.64, "text": " And I'm going to show you how to do that right now.", "tokens": [50464, 400, 286, 478, 516, 281, 855, 291, 577, 281, 360, 300, 558, 586, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08783977372305733, "compression_ratio": 1.6512820512820512, "no_speech_prob": 0.05663489177823067}, {"id": 951, "seek": 446564, "start": 4470.64, "end": 4474.64, "text": " So first off, we have to learn something called dot products.", "tokens": [50614, 407, 700, 766, 11, 321, 362, 281, 1466, 746, 1219, 5893, 3383, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08783977372305733, "compression_ratio": 1.6512820512820512, "no_speech_prob": 0.05663489177823067}, {"id": 952, "seek": 446564, "start": 4474.64, "end": 4480.64, "text": " So dot products are actually pretty easy and you might have actually done them before.", "tokens": [50814, 407, 5893, 3383, 366, 767, 1238, 1858, 293, 291, 1062, 362, 767, 1096, 552, 949, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08783977372305733, "compression_ratio": 1.6512820512820512, "no_speech_prob": 0.05663489177823067}, {"id": 953, "seek": 446564, "start": 4480.64, "end": 4486.64, "text": " So let's say we go ahead and take, we go ahead and take this right here we go.", "tokens": [51114, 407, 718, 311, 584, 321, 352, 2286, 293, 747, 11, 321, 352, 2286, 293, 747, 341, 558, 510, 321, 352, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08783977372305733, "compression_ratio": 1.6512820512820512, "no_speech_prob": 0.05663489177823067}, {"id": 954, "seek": 448664, "start": 4487.64, "end": 4489.64, "text": " One, two, three.", "tokens": [50414, 1485, 11, 732, 11, 1045, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10573289480554052, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.3105476200580597}, {"id": 955, "seek": 448664, "start": 4489.64, "end": 4491.64, "text": " That's going to be what A is.", "tokens": [50514, 663, 311, 516, 281, 312, 437, 316, 307, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10573289480554052, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.3105476200580597}, {"id": 956, "seek": 448664, "start": 4491.64, "end": 4495.64, "text": " And then we have four, five, six.", "tokens": [50614, 400, 550, 321, 362, 1451, 11, 1732, 11, 2309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10573289480554052, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.3105476200580597}, {"id": 957, "seek": 448664, "start": 4495.64, "end": 4504.64, "text": " So if we want to find the dot product between these two, all we have to do is simply take the index of both of these,", "tokens": [50814, 407, 498, 321, 528, 281, 915, 264, 5893, 1674, 1296, 613, 732, 11, 439, 321, 362, 281, 360, 307, 2935, 747, 264, 8186, 295, 1293, 295, 613, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10573289480554052, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.3105476200580597}, {"id": 958, "seek": 448664, "start": 4504.64, "end": 4508.64, "text": " the first ones and the second ones and third ones, multiply them together and then add.", "tokens": [51264, 264, 700, 2306, 293, 264, 1150, 2306, 293, 2636, 2306, 11, 12972, 552, 1214, 293, 550, 909, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10573289480554052, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.3105476200580597}, {"id": 959, "seek": 450864, "start": 4508.64, "end": 4520.64, "text": " So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five,", "tokens": [50364, 407, 321, 434, 516, 281, 352, 2286, 293, 360, 472, 11, 12972, 1451, 11, 472, 1413, 1451, 11, 293, 550, 909, 309, 281, 732, 1413, 1732, 11, 50964], "temperature": 0.0, "avg_logprob": -0.137805970509847, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.07918571680784225}, {"id": 960, "seek": 450864, "start": 4520.64, "end": 4524.64, "text": " and then add it to three times six.", "tokens": [50964, 293, 550, 909, 309, 281, 1045, 1413, 2309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.137805970509847, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.07918571680784225}, {"id": 961, "seek": 450864, "start": 4526.64, "end": 4532.64, "text": " So one times four is four, two times five is ten, three times six is eighteen.", "tokens": [51264, 407, 472, 1413, 1451, 307, 1451, 11, 732, 1413, 1732, 307, 2064, 11, 1045, 1413, 2309, 307, 31755, 13, 51564], "temperature": 0.0, "avg_logprob": -0.137805970509847, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.07918571680784225}, {"id": 962, "seek": 450864, "start": 4532.64, "end": 4536.64, "text": " So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two.", "tokens": [51564, 407, 321, 434, 516, 281, 352, 2286, 293, 909, 613, 493, 11, 321, 483, 32253, 1804, 31755, 11, 286, 1697, 307, 11790, 12, 20534, 13, 51764], "temperature": 0.0, "avg_logprob": -0.137805970509847, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.07918571680784225}, {"id": 963, "seek": 453664, "start": 4536.64, "end": 4542.64, "text": " So the dot product of this is going to be thirty-two.", "tokens": [50364, 407, 264, 5893, 1674, 295, 341, 307, 516, 281, 312, 11790, 12, 20534, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 964, "seek": 453664, "start": 4542.64, "end": 4545.64, "text": " And that's pretty much how simple dot products are.", "tokens": [50664, 400, 300, 311, 1238, 709, 577, 2199, 5893, 3383, 366, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 965, "seek": 453664, "start": 4545.64, "end": 4554.64, "text": " It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up.", "tokens": [50814, 467, 311, 445, 1940, 1184, 8186, 295, 1293, 295, 613, 41011, 11, 30955, 552, 1214, 293, 550, 5127, 439, 295, 613, 3383, 493, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 966, "seek": 453664, "start": 4554.64, "end": 4555.64, "text": " That's a dot product.", "tokens": [51264, 663, 311, 257, 5893, 1674, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 967, "seek": 453664, "start": 4555.64, "end": 4559.64, "text": " So we actually need dot products for matrix multiplication.", "tokens": [51314, 407, 321, 767, 643, 5893, 3383, 337, 8141, 27290, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 968, "seek": 453664, "start": 4559.64, "end": 4561.64, "text": " So let's go ahead and jump into that right now.", "tokens": [51514, 407, 718, 311, 352, 2286, 293, 3012, 666, 300, 558, 586, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0818315652700571, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0011694146087393165}, {"id": 969, "seek": 456164, "start": 4561.64, "end": 4566.64, "text": " So I'm just going to create two matrices that are going to be pretty easy to work with.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 1884, 732, 32284, 300, 366, 516, 281, 312, 1238, 1858, 281, 589, 365, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0841327558589887, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.014500864781439304}, {"id": 970, "seek": 456164, "start": 4566.64, "end": 4572.64, "text": " So let's say we have A and have one matrix over here.", "tokens": [50614, 407, 718, 311, 584, 321, 362, 316, 293, 362, 472, 8141, 670, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0841327558589887, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.014500864781439304}, {"id": 971, "seek": 456164, "start": 4572.64, "end": 4580.64, "text": " It's going to be one, two, three, four, five and six.", "tokens": [50914, 467, 311, 516, 281, 312, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 293, 2309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0841327558589887, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.014500864781439304}, {"id": 972, "seek": 456164, "start": 4580.64, "end": 4586.64, "text": " This is going to be equal to A and then B is going to be another matrix.", "tokens": [51314, 639, 307, 516, 281, 312, 2681, 281, 316, 293, 550, 363, 307, 516, 281, 312, 1071, 8141, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0841327558589887, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.014500864781439304}, {"id": 973, "seek": 458664, "start": 4586.64, "end": 4595.64, "text": " So we're going to have seven, eight, nine, ten, eleven, twelve.", "tokens": [50364, 407, 321, 434, 516, 281, 362, 3407, 11, 3180, 11, 4949, 11, 2064, 11, 21090, 11, 14390, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 974, "seek": 458664, "start": 4595.64, "end": 4597.64, "text": " Ignore my terrible writing.", "tokens": [50814, 24754, 418, 452, 6237, 3579, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 975, "seek": 458664, "start": 4597.64, "end": 4601.64, "text": " Pretty much what we do is to multiply these together.", "tokens": [50914, 10693, 709, 437, 321, 360, 307, 281, 12972, 613, 1214, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 976, "seek": 458664, "start": 4601.64, "end": 4605.64, "text": " First we need to make sure that they can multiply together.", "tokens": [51114, 2386, 321, 643, 281, 652, 988, 300, 436, 393, 12972, 1214, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 977, "seek": 458664, "start": 4605.64, "end": 4609.64, "text": " So we need to take a look at the amount of rows and columns at this half.", "tokens": [51314, 407, 321, 643, 281, 747, 257, 574, 412, 264, 2372, 295, 13241, 293, 13766, 412, 341, 1922, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 978, "seek": 458664, "start": 4609.64, "end": 4612.64, "text": " So this one right here is three rows, one, two, three.", "tokens": [51514, 407, 341, 472, 558, 510, 307, 1045, 13241, 11, 472, 11, 732, 11, 1045, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 979, "seek": 458664, "start": 4612.64, "end": 4614.64, "text": " Three rows and two columns.", "tokens": [51664, 6244, 13241, 293, 732, 13766, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07088566761390835, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.016151469200849533}, {"id": 980, "seek": 461464, "start": 4614.64, "end": 4617.64, "text": " So this is going to be a three by two matrix.", "tokens": [50364, 407, 341, 307, 516, 281, 312, 257, 1045, 538, 732, 8141, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 981, "seek": 461464, "start": 4617.64, "end": 4621.64, "text": " And this one has two rows and three columns.", "tokens": [50514, 400, 341, 472, 575, 732, 13241, 293, 1045, 13766, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 982, "seek": 461464, "start": 4621.64, "end": 4624.64, "text": " So it's a two by three matrix.", "tokens": [50714, 407, 309, 311, 257, 732, 538, 1045, 8141, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 983, "seek": 461464, "start": 4624.64, "end": 4631.64, "text": " So all we have to make sure that if we're multiplying A dot product with B,", "tokens": [50864, 407, 439, 321, 362, 281, 652, 988, 300, 498, 321, 434, 30955, 316, 5893, 1674, 365, 363, 11, 51214], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 984, "seek": 461464, "start": 4631.64, "end": 4635.64, "text": " and this is the PyTorch syntax for multiplying matrices,", "tokens": [51214, 293, 341, 307, 264, 9953, 51, 284, 339, 28431, 337, 30955, 32284, 11, 51414], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 985, "seek": 461464, "start": 4635.64, "end": 4641.64, "text": " if we're multiplying A by B, then we have to make sure the following is true.", "tokens": [51414, 498, 321, 434, 30955, 316, 538, 363, 11, 550, 321, 362, 281, 652, 988, 264, 3480, 307, 2074, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06844125546907123, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.002844534581527114}, {"id": 986, "seek": 464164, "start": 4641.64, "end": 4649.64, "text": " So if we use three by two and then dot product with two times three,", "tokens": [50364, 407, 498, 321, 764, 1045, 538, 732, 293, 550, 5893, 1674, 365, 732, 1413, 1045, 11, 50764], "temperature": 0.0, "avg_logprob": -0.06581473350524902, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.000896894431207329}, {"id": 987, "seek": 464164, "start": 4649.64, "end": 4654.64, "text": " we have to make sure that these two inner values are the same.", "tokens": [50764, 321, 362, 281, 652, 988, 300, 613, 732, 7284, 4190, 366, 264, 912, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06581473350524902, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.000896894431207329}, {"id": 988, "seek": 464164, "start": 4654.64, "end": 4657.64, "text": " So two is equal to two, so we cross these out,", "tokens": [51014, 407, 732, 307, 2681, 281, 732, 11, 370, 321, 3278, 613, 484, 11, 51164], "temperature": 0.0, "avg_logprob": -0.06581473350524902, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.000896894431207329}, {"id": 989, "seek": 464164, "start": 4657.64, "end": 4660.64, "text": " and then the ones that we have left over are three by three.", "tokens": [51164, 293, 550, 264, 2306, 300, 321, 362, 1411, 670, 366, 1045, 538, 1045, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06581473350524902, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.000896894431207329}, {"id": 990, "seek": 464164, "start": 4660.64, "end": 4664.64, "text": " So the resulting matrix would be A three by three.", "tokens": [51314, 407, 264, 16505, 8141, 576, 312, 316, 1045, 538, 1045, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06581473350524902, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.000896894431207329}, {"id": 991, "seek": 466464, "start": 4664.64, "end": 4672.64, "text": " Or if you had like a three by four times A five by five by one,", "tokens": [50364, 1610, 498, 291, 632, 411, 257, 1045, 538, 1451, 1413, 316, 1732, 538, 1732, 538, 472, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 992, "seek": 466464, "start": 4672.64, "end": 4675.64, "text": " that doesn't work because these values aren't the same.", "tokens": [50764, 300, 1177, 380, 589, 570, 613, 4190, 3212, 380, 264, 912, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 993, "seek": 466464, "start": 4675.64, "end": 4678.64, "text": " So these two matrices couldn't multiply.", "tokens": [50914, 407, 613, 732, 32284, 2809, 380, 12972, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 994, "seek": 466464, "start": 4678.64, "end": 4682.64, "text": " And sometimes you actually have to flip these to make them work.", "tokens": [51064, 400, 2171, 291, 767, 362, 281, 7929, 613, 281, 652, 552, 589, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 995, "seek": 466464, "start": 4682.64, "end": 4687.64, "text": " So maybe we change this value here to A three.", "tokens": [51264, 407, 1310, 321, 1319, 341, 2158, 510, 281, 316, 1045, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 996, "seek": 466464, "start": 4687.64, "end": 4689.64, "text": " We change this value to a three.", "tokens": [51514, 492, 1319, 341, 2158, 281, 257, 1045, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 997, "seek": 466464, "start": 4689.64, "end": 4692.64, "text": " In this order, they do not multiply.", "tokens": [51614, 682, 341, 1668, 11, 436, 360, 406, 12972, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10769434178129156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.2597154676914215}, {"id": 998, "seek": 469264, "start": 4692.64, "end": 4705.64, "text": " But if we switch them around, we have a three by five with A five by three,", "tokens": [50364, 583, 498, 321, 3679, 552, 926, 11, 321, 362, 257, 1045, 538, 1732, 365, 316, 1732, 538, 1045, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 999, "seek": 469264, "start": 4705.64, "end": 4709.64, "text": " sorry, five by three with A three by four.", "tokens": [51014, 2597, 11, 1732, 538, 1045, 365, 316, 1045, 538, 1451, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1000, "seek": 469264, "start": 4709.64, "end": 4711.64, "text": " So these two numbers are the same.", "tokens": [51214, 407, 613, 732, 3547, 366, 264, 912, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1001, "seek": 469264, "start": 4711.64, "end": 4712.64, "text": " That works.", "tokens": [51314, 663, 1985, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1002, "seek": 469264, "start": 4712.64, "end": 4714.64, "text": " The resulting matrix is a five by four.", "tokens": [51364, 440, 16505, 8141, 307, 257, 1732, 538, 1451, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1003, "seek": 469264, "start": 4714.64, "end": 4718.64, "text": " So that's how you make sure that two matrices are compatible.", "tokens": [51464, 407, 300, 311, 577, 291, 652, 988, 300, 732, 32284, 366, 18218, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1004, "seek": 469264, "start": 4718.64, "end": 4721.64, "text": " So now to actually multiply these together,", "tokens": [51664, 407, 586, 281, 767, 12972, 613, 1214, 11, 51814], "temperature": 0.0, "avg_logprob": -0.12267413084534393, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0018674670718610287}, {"id": 1005, "seek": 472164, "start": 4721.64, "end": 4723.64, "text": " what we're going to do, I'm going to make a new line here.", "tokens": [50364, 437, 321, 434, 516, 281, 360, 11, 286, 478, 516, 281, 652, 257, 777, 1622, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1006, "seek": 472164, "start": 4723.64, "end": 4727.64, "text": " So we're going to rewrite these.", "tokens": [50464, 407, 321, 434, 516, 281, 28132, 613, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1007, "seek": 472164, "start": 4727.64, "end": 4729.64, "text": " Now we don't have to rewrite them.", "tokens": [50664, 823, 321, 500, 380, 362, 281, 28132, 552, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1008, "seek": 472164, "start": 4729.64, "end": 4731.64, "text": " Let's just cross that out here.", "tokens": [50764, 961, 311, 445, 3278, 300, 484, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1009, "seek": 472164, "start": 4731.64, "end": 4741.64, "text": " So pretty much what we have to do is we have to take these two and dot product with these two.", "tokens": [50864, 407, 1238, 709, 437, 321, 362, 281, 360, 307, 321, 362, 281, 747, 613, 732, 293, 5893, 1674, 365, 613, 732, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1010, "seek": 472164, "start": 4741.64, "end": 4748.64, "text": " And then once we're done that, we do the same with these and these, these and these.", "tokens": [51364, 400, 550, 1564, 321, 434, 1096, 300, 11, 321, 360, 264, 912, 365, 613, 293, 613, 11, 613, 293, 613, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08443928728199969, "compression_ratio": 1.888268156424581, "no_speech_prob": 0.007344277575612068}, {"id": 1011, "seek": 474864, "start": 4748.64, "end": 4754.64, "text": " So we start with the first, the first row in the A matrix.", "tokens": [50364, 407, 321, 722, 365, 264, 700, 11, 264, 700, 5386, 294, 264, 316, 8141, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1244597483162928, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.03160663694143295}, {"id": 1012, "seek": 474864, "start": 4754.64, "end": 4758.64, "text": " And we iterate through all of the columns in the B matrix.", "tokens": [50664, 400, 321, 44497, 807, 439, 295, 264, 13766, 294, 264, 363, 8141, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1244597483162928, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.03160663694143295}, {"id": 1013, "seek": 474864, "start": 4758.64, "end": 4764.64, "text": " And then after we're done that, we just go to the next row in the A matrix and then et cetera, right?", "tokens": [50864, 400, 550, 934, 321, 434, 1096, 300, 11, 321, 445, 352, 281, 264, 958, 5386, 294, 264, 316, 8141, 293, 550, 1030, 11458, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1244597483162928, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.03160663694143295}, {"id": 1014, "seek": 474864, "start": 4764.64, "end": 4766.64, "text": " So let's go ahead and do this right now.", "tokens": [51164, 407, 718, 311, 352, 2286, 293, 360, 341, 558, 586, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1244597483162928, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.03160663694143295}, {"id": 1015, "seek": 474864, "start": 4766.64, "end": 4770.64, "text": " That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here.", "tokens": [51264, 663, 1391, 3263, 13181, 281, 722, 11, 457, 718, 385, 445, 23221, 341, 11, 577, 341, 1333, 295, 1985, 558, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1244597483162928, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.03160663694143295}, {"id": 1016, "seek": 477064, "start": 4770.64, "end": 4780.64, "text": " So we have our one times, our one times seven plus two times 10.", "tokens": [50364, 407, 321, 362, 527, 472, 1413, 11, 527, 472, 1413, 3407, 1804, 732, 1413, 1266, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10359335767811742, "compression_ratio": 1.6, "no_speech_prob": 0.42222750186920166}, {"id": 1017, "seek": 477064, "start": 4780.64, "end": 4787.64, "text": " So one times seven plus two times 10.", "tokens": [50864, 407, 472, 1413, 3407, 1804, 732, 1413, 1266, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10359335767811742, "compression_ratio": 1.6, "no_speech_prob": 0.42222750186920166}, {"id": 1018, "seek": 477064, "start": 4787.64, "end": 4790.64, "text": " And this is equal to 27.", "tokens": [51214, 400, 341, 307, 2681, 281, 7634, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10359335767811742, "compression_ratio": 1.6, "no_speech_prob": 0.42222750186920166}, {"id": 1019, "seek": 477064, "start": 4790.64, "end": 4796.64, "text": " So that's the first dot product of one and two and seven and 10.", "tokens": [51364, 407, 300, 311, 264, 700, 5893, 1674, 295, 472, 293, 732, 293, 3407, 293, 1266, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10359335767811742, "compression_ratio": 1.6, "no_speech_prob": 0.42222750186920166}, {"id": 1020, "seek": 479664, "start": 4796.64, "end": 4801.64, "text": " So what this is actually going to look like in our new matrix, I'm going to write this out here.", "tokens": [50364, 407, 437, 341, 307, 767, 516, 281, 574, 411, 294, 527, 777, 8141, 11, 286, 478, 516, 281, 2464, 341, 484, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09231296101131954, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.007345095276832581}, {"id": 1021, "seek": 479664, "start": 4801.64, "end": 4804.64, "text": " So this is our new matrix here.", "tokens": [50614, 407, 341, 307, 527, 777, 8141, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09231296101131954, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.007345095276832581}, {"id": 1022, "seek": 479664, "start": 4804.64, "end": 4808.64, "text": " This 27 is going to go right here.", "tokens": [50764, 639, 7634, 307, 516, 281, 352, 558, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09231296101131954, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.007345095276832581}, {"id": 1023, "seek": 479664, "start": 4808.64, "end": 4810.64, "text": " Let's continue.", "tokens": [50964, 961, 311, 2354, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09231296101131954, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.007345095276832581}, {"id": 1024, "seek": 479664, "start": 4810.64, "end": 4816.64, "text": " So next up, we're going to do one and two and then eight and 11.", "tokens": [51064, 407, 958, 493, 11, 321, 434, 516, 281, 360, 472, 293, 732, 293, 550, 3180, 293, 2975, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09231296101131954, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.007345095276832581}, {"id": 1025, "seek": 481664, "start": 4816.64, "end": 4832.64, "text": " So we're going to go one, one times eight plus two, or sorry, two and 11.", "tokens": [50364, 407, 321, 434, 516, 281, 352, 472, 11, 472, 1413, 3180, 1804, 732, 11, 420, 2597, 11, 732, 293, 2975, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16653144965737554, "compression_ratio": 1.552, "no_speech_prob": 0.024794554337859154}, {"id": 1026, "seek": 481664, "start": 4832.64, "end": 4835.64, "text": " So one times eight is eight and then two times 11 is 22.", "tokens": [51164, 407, 472, 1413, 3180, 307, 3180, 293, 550, 732, 1413, 2975, 307, 5853, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16653144965737554, "compression_ratio": 1.552, "no_speech_prob": 0.024794554337859154}, {"id": 1027, "seek": 481664, "start": 4835.64, "end": 4841.64, "text": " So our result here is 30 and 30 is just going to go right here.", "tokens": [51314, 407, 527, 1874, 510, 307, 2217, 293, 2217, 307, 445, 516, 281, 352, 558, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16653144965737554, "compression_ratio": 1.552, "no_speech_prob": 0.024794554337859154}, {"id": 1028, "seek": 484164, "start": 4841.64, "end": 4845.64, "text": " So 27, 30, and you can see how this is going to work, right?", "tokens": [50364, 407, 7634, 11, 2217, 11, 293, 291, 393, 536, 577, 341, 307, 516, 281, 589, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.12115413386647295, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.030209559947252274}, {"id": 1029, "seek": 484164, "start": 4845.64, "end": 4853.64, "text": " So in our first row of A, we're going to get the first row of this resulting matrix.", "tokens": [50564, 407, 294, 527, 700, 5386, 295, 316, 11, 321, 434, 516, 281, 483, 264, 700, 5386, 295, 341, 16505, 8141, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12115413386647295, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.030209559947252274}, {"id": 1030, "seek": 484164, "start": 4853.64, "end": 4858.64, "text": " So let's go ahead and do the rest here.", "tokens": [50964, 407, 718, 311, 352, 2286, 293, 360, 264, 1472, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12115413386647295, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.030209559947252274}, {"id": 1031, "seek": 484164, "start": 4858.64, "end": 4863.64, "text": " So we have one and two and then nine and 12.", "tokens": [51214, 407, 321, 362, 472, 293, 732, 293, 550, 4949, 293, 2272, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12115413386647295, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.030209559947252274}, {"id": 1032, "seek": 484164, "start": 4863.64, "end": 4870.64, "text": " One times nine, two times 12.", "tokens": [51464, 1485, 1413, 4949, 11, 732, 1413, 2272, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12115413386647295, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.030209559947252274}, {"id": 1033, "seek": 487064, "start": 4870.64, "end": 4873.64, "text": " One times nine is nine, two times 12 is 24.", "tokens": [50364, 1485, 1413, 4949, 307, 4949, 11, 732, 1413, 2272, 307, 4022, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1034, "seek": 487064, "start": 4873.64, "end": 4877.64, "text": " So if we do, that's like 33, I believe.", "tokens": [50514, 407, 498, 321, 360, 11, 300, 311, 411, 11816, 11, 286, 1697, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1035, "seek": 487064, "start": 4877.64, "end": 4882.64, "text": " So 33 and we can go ahead and write that here.", "tokens": [50714, 407, 11816, 293, 321, 393, 352, 2286, 293, 2464, 300, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1036, "seek": 487064, "start": 4882.64, "end": 4885.64, "text": " So now let's move on to the next.", "tokens": [50964, 407, 586, 718, 311, 1286, 322, 281, 264, 958, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1037, "seek": 487064, "start": 4885.64, "end": 4894.64, "text": " We have three and four, three, three and four dot product with seven and 10.", "tokens": [51114, 492, 362, 1045, 293, 1451, 11, 1045, 11, 1045, 293, 1451, 5893, 1674, 365, 3407, 293, 1266, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1038, "seek": 487064, "start": 4894.64, "end": 4899.64, "text": " So three will multiply seven.", "tokens": [51564, 407, 1045, 486, 12972, 3407, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16019233254825369, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00884601753205061}, {"id": 1039, "seek": 489964, "start": 4899.64, "end": 4908.64, "text": " And then we're going to go ahead and add that to four times 10.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 352, 2286, 293, 909, 300, 281, 1451, 1413, 1266, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1264022518606747, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.004981583449989557}, {"id": 1040, "seek": 489964, "start": 4908.64, "end": 4914.64, "text": " Three times seven, three times seven is 21, and then four times 10 is 40.", "tokens": [50814, 6244, 1413, 3407, 11, 1045, 1413, 3407, 307, 5080, 11, 293, 550, 1451, 1413, 1266, 307, 3356, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1264022518606747, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.004981583449989557}, {"id": 1041, "seek": 489964, "start": 4914.64, "end": 4916.64, "text": " So we're going to get 47.", "tokens": [51114, 407, 321, 434, 516, 281, 483, 16953, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1264022518606747, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.004981583449989557}, {"id": 1042, "seek": 489964, "start": 4916.64, "end": 4922.64, "text": " So I'll put there so we can go in and write 47 right there.", "tokens": [51214, 407, 286, 603, 829, 456, 370, 321, 393, 352, 294, 293, 2464, 16953, 558, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1264022518606747, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.004981583449989557}, {"id": 1043, "seek": 492264, "start": 4922.64, "end": 4929.64, "text": " And our next one is going to be three and four dot product with eight and 11.", "tokens": [50364, 400, 527, 958, 472, 307, 516, 281, 312, 1045, 293, 1451, 5893, 1674, 365, 3180, 293, 2975, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14312570571899413, "compression_ratio": 1.4396551724137931, "no_speech_prob": 0.02096107229590416}, {"id": 1044, "seek": 492264, "start": 4929.64, "end": 4940.64, "text": " So eight plus four times 11.", "tokens": [50714, 407, 3180, 1804, 1451, 1413, 2975, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14312570571899413, "compression_ratio": 1.4396551724137931, "no_speech_prob": 0.02096107229590416}, {"id": 1045, "seek": 492264, "start": 4940.64, "end": 4941.64, "text": " Perfect.", "tokens": [51264, 10246, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14312570571899413, "compression_ratio": 1.4396551724137931, "no_speech_prob": 0.02096107229590416}, {"id": 1046, "seek": 492264, "start": 4941.64, "end": 4947.64, "text": " So we get three times eight is 24 and then plus 44.", "tokens": [51314, 407, 321, 483, 1045, 1413, 3180, 307, 4022, 293, 550, 1804, 16408, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14312570571899413, "compression_ratio": 1.4396551724137931, "no_speech_prob": 0.02096107229590416}, {"id": 1047, "seek": 494764, "start": 4947.64, "end": 4952.64, "text": " So 24 plus 44, that's 68.", "tokens": [50364, 407, 4022, 1804, 16408, 11, 300, 311, 23317, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11718291176689996, "compression_ratio": 1.28, "no_speech_prob": 0.00818751472979784}, {"id": 1048, "seek": 494764, "start": 4952.64, "end": 4959.64, "text": " So we get 68 and we can go in and write that here.", "tokens": [50614, 407, 321, 483, 23317, 293, 321, 393, 352, 294, 293, 2464, 300, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11718291176689996, "compression_ratio": 1.28, "no_speech_prob": 0.00818751472979784}, {"id": 1049, "seek": 494764, "start": 4959.64, "end": 4973.64, "text": " So next up, we have three and four and nine and 12.", "tokens": [50964, 407, 958, 493, 11, 321, 362, 1045, 293, 1451, 293, 4949, 293, 2272, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11718291176689996, "compression_ratio": 1.28, "no_speech_prob": 0.00818751472979784}, {"id": 1050, "seek": 497364, "start": 4973.64, "end": 4976.64, "text": " So three times nine is 27.", "tokens": [50364, 407, 1045, 1413, 4949, 307, 7634, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1051, "seek": 497364, "start": 4976.64, "end": 4977.64, "text": " And then four times 12.", "tokens": [50514, 400, 550, 1451, 1413, 2272, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1052, "seek": 497364, "start": 4977.64, "end": 4979.64, "text": " So let's just, let's just do that.", "tokens": [50564, 407, 718, 311, 445, 11, 718, 311, 445, 360, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1053, "seek": 497364, "start": 4979.64, "end": 4981.64, "text": " I'm not doing that in my head.", "tokens": [50664, 286, 478, 406, 884, 300, 294, 452, 1378, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1054, "seek": 497364, "start": 4981.64, "end": 4983.64, "text": " 27 plus was four times 12.", "tokens": [50764, 7634, 1804, 390, 1451, 1413, 2272, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1055, "seek": 497364, "start": 4983.64, "end": 4984.64, "text": " So that's 48.", "tokens": [50864, 407, 300, 311, 11174, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1056, "seek": 497364, "start": 4984.64, "end": 4990.64, "text": " 27 plus 48 gives us 75.", "tokens": [50914, 7634, 1804, 11174, 2709, 505, 9562, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1057, "seek": 497364, "start": 4990.64, "end": 4994.64, "text": " Let's go ahead and write our 75 here.", "tokens": [51214, 961, 311, 352, 2286, 293, 2464, 527, 9562, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1058, "seek": 497364, "start": 4994.64, "end": 4998.64, "text": " Then we can go ahead and slide down to this row since we're done, since we're done that.", "tokens": [51414, 1396, 321, 393, 352, 2286, 293, 4137, 760, 281, 341, 5386, 1670, 321, 434, 1096, 11, 1670, 321, 434, 1096, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15768093329209548, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.030211463570594788}, {"id": 1059, "seek": 499864, "start": 4998.64, "end": 5016.64, "text": " And then we go five, five and six dot product was seven and 10.", "tokens": [50364, 400, 550, 321, 352, 1732, 11, 1732, 293, 2309, 5893, 1674, 390, 3407, 293, 1266, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11097716331481934, "compression_ratio": 1.32, "no_speech_prob": 0.2876491844654083}, {"id": 1060, "seek": 499864, "start": 5016.64, "end": 5022.64, "text": " So our result from this five times seven is 35 and then six times 10 is 60.", "tokens": [51264, 407, 527, 1874, 490, 341, 1732, 1413, 3407, 307, 6976, 293, 550, 2309, 1413, 1266, 307, 4060, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11097716331481934, "compression_ratio": 1.32, "no_speech_prob": 0.2876491844654083}, {"id": 1061, "seek": 499864, "start": 5022.64, "end": 5024.64, "text": " So we're going to get 95.", "tokens": [51564, 407, 321, 434, 516, 281, 483, 13420, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11097716331481934, "compression_ratio": 1.32, "no_speech_prob": 0.2876491844654083}, {"id": 1062, "seek": 502464, "start": 5024.64, "end": 5029.64, "text": " We can go in and write our 95 here.", "tokens": [50364, 492, 393, 352, 294, 293, 2464, 527, 13420, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14255118370056152, "compression_ratio": 1.311926605504587, "no_speech_prob": 0.044671040028333664}, {"id": 1063, "seek": 502464, "start": 5029.64, "end": 5046.64, "text": " And then five and six dot product with eight and 11.", "tokens": [50614, 400, 550, 1732, 293, 2309, 5893, 1674, 365, 3180, 293, 2975, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14255118370056152, "compression_ratio": 1.311926605504587, "no_speech_prob": 0.044671040028333664}, {"id": 1064, "seek": 502464, "start": 5046.64, "end": 5050.64, "text": " So five times eight is 40 and then six times 11 is 66.", "tokens": [51464, 407, 1732, 1413, 3180, 307, 3356, 293, 550, 2309, 1413, 2975, 307, 21126, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14255118370056152, "compression_ratio": 1.311926605504587, "no_speech_prob": 0.044671040028333664}, {"id": 1065, "seek": 505064, "start": 5050.64, "end": 5059.64, "text": " So we get 104.", "tokens": [50364, 407, 321, 483, 47757, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10265763316835676, "compression_ratio": 1.0246913580246915, "no_speech_prob": 0.08033598959445953}, {"id": 1066, "seek": 505064, "start": 5059.64, "end": 5075.64, "text": " And then the last one, so five and six dot product with nine and 12.", "tokens": [50814, 400, 550, 264, 1036, 472, 11, 370, 1732, 293, 2309, 5893, 1674, 365, 4949, 293, 2272, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10265763316835676, "compression_ratio": 1.0246913580246915, "no_speech_prob": 0.08033598959445953}, {"id": 1067, "seek": 507564, "start": 5075.64, "end": 5078.64, "text": " So five, five times nine is 45.", "tokens": [50364, 407, 1732, 11, 1732, 1413, 4949, 307, 6905, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1068, "seek": 507564, "start": 5078.64, "end": 5084.64, "text": " And then six times 12 is what six times 12, 72, I think.", "tokens": [50514, 400, 550, 2309, 1413, 2272, 307, 437, 2309, 1413, 2272, 11, 18731, 11, 286, 519, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1069, "seek": 507564, "start": 5084.64, "end": 5087.64, "text": " So six times 12, 72.", "tokens": [50814, 407, 2309, 1413, 2272, 11, 18731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1070, "seek": 507564, "start": 5087.64, "end": 5088.64, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1071, "seek": 507564, "start": 5088.64, "end": 5096.64, "text": " So 45 plus 72, 117.", "tokens": [51014, 407, 6905, 1804, 18731, 11, 2975, 22, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1072, "seek": 507564, "start": 5096.64, "end": 5104.64, "text": " And that is how you do a three by two matrix and a two by three matrix multiplying them together.", "tokens": [51414, 400, 300, 307, 577, 291, 360, 257, 1045, 538, 732, 8141, 293, 257, 732, 538, 1045, 8141, 30955, 552, 1214, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16789332303133878, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.1311185508966446}, {"id": 1073, "seek": 510464, "start": 5104.64, "end": 5112.64, "text": " So the result would be C equals that.", "tokens": [50364, 407, 264, 1874, 576, 312, 383, 6915, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05339921712875366, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09391923248767853}, {"id": 1074, "seek": 510464, "start": 5112.64, "end": 5121.64, "text": " So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far.", "tokens": [50764, 407, 382, 291, 393, 536, 11, 309, 2516, 257, 688, 295, 4439, 300, 1890, 767, 1596, 257, 857, 295, 565, 5347, 281, 257, 688, 295, 264, 661, 1507, 286, 600, 5343, 294, 341, 960, 370, 1400, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05339921712875366, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09391923248767853}, {"id": 1075, "seek": 510464, "start": 5121.64, "end": 5130.64, "text": " So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU.", "tokens": [51214, 407, 291, 393, 536, 577, 309, 311, 534, 1021, 281, 483, 10807, 281, 360, 341, 337, 505, 293, 2318, 281, 4373, 341, 322, 257, 18407, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05339921712875366, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09391923248767853}, {"id": 1076, "seek": 513064, "start": 5130.64, "end": 5136.64, "text": " So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training.", "tokens": [50364, 407, 286, 478, 516, 281, 1066, 45550, 300, 935, 544, 293, 544, 307, 577, 264, 18407, 307, 588, 1021, 337, 21589, 428, 3097, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1077, "seek": 513064, "start": 5136.64, "end": 5141.64, "text": " But pretty much that's how you do dot products and matrix multiplication.", "tokens": [50664, 583, 1238, 709, 300, 311, 577, 291, 360, 5893, 3383, 293, 8141, 27290, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1078, "seek": 513064, "start": 5141.64, "end": 5144.64, "text": " So I actually realized I messed up a little bit on the math there.", "tokens": [50914, 407, 286, 767, 5334, 286, 16507, 493, 257, 707, 857, 322, 264, 5221, 456, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1079, "seek": 513064, "start": 5144.64, "end": 5148.64, "text": " So this 104, that's actually 106.", "tokens": [51064, 407, 341, 47757, 11, 300, 311, 767, 1266, 21, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1080, "seek": 513064, "start": 5148.64, "end": 5152.64, "text": " So I messed up there if you caught that.", "tokens": [51264, 407, 286, 16507, 493, 456, 498, 291, 5415, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1081, "seek": 513064, "start": 5152.64, "end": 5153.64, "text": " Good job.", "tokens": [51464, 2205, 1691, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1082, "seek": 513064, "start": 5153.64, "end": 5158.64, "text": " But pretty much this is what this looks like in three lines of code.", "tokens": [51514, 583, 1238, 709, 341, 307, 437, 341, 1542, 411, 294, 1045, 3876, 295, 3089, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08878472586658513, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.2971382141113281}, {"id": 1083, "seek": 515864, "start": 5158.64, "end": 5165.64, "text": " So all of this up here that we just covered all of this is in three lines.", "tokens": [50364, 407, 439, 295, 341, 493, 510, 300, 321, 445, 5343, 439, 295, 341, 307, 294, 1045, 3876, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12082384933124889, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.036198947578668594}, {"id": 1084, "seek": 515864, "start": 5165.64, "end": 5169.64, "text": " So we initialize an A tensor and a B tensor.", "tokens": [50714, 407, 321, 5883, 1125, 364, 316, 40863, 293, 257, 363, 40863, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12082384933124889, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.036198947578668594}, {"id": 1085, "seek": 515864, "start": 5169.64, "end": 5171.64, "text": " Each one of these is a row.", "tokens": [50914, 6947, 472, 295, 613, 307, 257, 5386, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12082384933124889, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.036198947578668594}, {"id": 1086, "seek": 515864, "start": 5171.64, "end": 5176.64, "text": " Each one of these is a row and it'll pretty much multiply these together.", "tokens": [51014, 6947, 472, 295, 613, 307, 257, 5386, 293, 309, 603, 1238, 709, 12972, 613, 1214, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12082384933124889, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.036198947578668594}, {"id": 1087, "seek": 515864, "start": 5176.64, "end": 5183.64, "text": " So this at symbol, this is a shorthand how you multiply two matrices in pytorch together.", "tokens": [51264, 407, 341, 412, 5986, 11, 341, 307, 257, 402, 2652, 474, 577, 291, 12972, 732, 32284, 294, 25878, 284, 339, 1214, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12082384933124889, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.036198947578668594}, {"id": 1088, "seek": 518364, "start": 5183.64, "end": 5191.64, "text": " Another way to do this is to use the torch dot matrix multiply function or math mall for short.", "tokens": [50364, 3996, 636, 281, 360, 341, 307, 281, 764, 264, 27822, 5893, 8141, 12972, 2445, 420, 5221, 16026, 337, 2099, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1089, "seek": 518364, "start": 5191.64, "end": 5194.64, "text": " And then you can do A and B.", "tokens": [50764, 400, 550, 291, 393, 360, 316, 293, 363, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1090, "seek": 518364, "start": 5194.64, "end": 5198.64, "text": " So these will print literally the same thing.", "tokens": [50914, 407, 613, 486, 4482, 3736, 264, 912, 551, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1091, "seek": 518364, "start": 5198.64, "end": 5199.64, "text": " Look at that.", "tokens": [51114, 2053, 412, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1092, "seek": 518364, "start": 5199.64, "end": 5203.64, "text": " So I'm not too sure on the differences between them.", "tokens": [51164, 407, 286, 478, 406, 886, 988, 322, 264, 7300, 1296, 552, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1093, "seek": 518364, "start": 5203.64, "end": 5207.64, "text": " I use A at B for short.", "tokens": [51364, 286, 764, 316, 412, 363, 337, 2099, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14813935129266037, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0747423842549324}, {"id": 1094, "seek": 520764, "start": 5207.64, "end": 5216.64, "text": " But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that.", "tokens": [50364, 583, 498, 291, 534, 528, 281, 458, 445, 11, 291, 458, 11, 747, 257, 574, 412, 264, 14333, 420, 575, 281, 362, 22431, 51, 472, 295, 264, 732, 293, 820, 312, 1075, 281, 483, 364, 1867, 490, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1504800182649459, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.06950297206640244}, {"id": 1095, "seek": 520764, "start": 5216.64, "end": 5226.64, "text": " But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks.", "tokens": [50814, 583, 286, 478, 516, 281, 1286, 322, 281, 746, 300, 321, 528, 281, 1159, 484, 337, 11, 2318, 562, 321, 434, 884, 527, 8141, 27290, 294, 527, 9590, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1504800182649459, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.06950297206640244}, {"id": 1096, "seek": 520764, "start": 5226.64, "end": 5229.64, "text": " So there's our network here if I go up.", "tokens": [51314, 407, 456, 311, 527, 3209, 510, 498, 286, 352, 493, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1504800182649459, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.06950297206640244}, {"id": 1097, "seek": 522964, "start": 5229.64, "end": 5239.64, "text": " Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number.", "tokens": [50364, 11739, 321, 362, 11, 321, 362, 512, 8141, 11, 512, 8141, 316, 11, 293, 633, 4478, 294, 341, 8141, 307, 257, 12607, 935, 1230, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1098, "seek": 522964, "start": 5239.64, "end": 5245.64, "text": " So if it's like a one, it would be like one dot zero or something or just like a one dot.", "tokens": [50864, 407, 498, 309, 311, 411, 257, 472, 11, 309, 576, 312, 411, 472, 5893, 4018, 420, 746, 420, 445, 411, 257, 472, 5893, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1099, "seek": 522964, "start": 5245.64, "end": 5247.64, "text": " That's what it would look like as a floating point number.", "tokens": [51164, 663, 311, 437, 309, 576, 574, 411, 382, 257, 12607, 935, 1230, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1100, "seek": 522964, "start": 5247.64, "end": 5252.64, "text": " But if it were an integer, say B is full of ones with integers, it would just be a one.", "tokens": [51264, 583, 498, 309, 645, 364, 24922, 11, 584, 363, 307, 1577, 295, 2306, 365, 41674, 11, 309, 576, 445, 312, 257, 472, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1101, "seek": 522964, "start": 5252.64, "end": 5255.64, "text": " There wouldn't be any decimal zero zero center, right?", "tokens": [51514, 821, 2759, 380, 312, 604, 26601, 4018, 4018, 3056, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1102, "seek": 522964, "start": 5255.64, "end": 5257.64, "text": " It would just be one.", "tokens": [51664, 467, 576, 445, 312, 472, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11175255897717598, "compression_ratio": 1.8728070175438596, "no_speech_prob": 0.0939469039440155}, {"id": 1103, "seek": 525764, "start": 5257.64, "end": 5265.64, "text": " So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type.", "tokens": [50364, 407, 294, 9953, 51, 284, 339, 11, 291, 2644, 767, 12972, 41674, 293, 12607, 935, 3547, 570, 436, 434, 406, 264, 912, 1412, 2010, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11525457451142461, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002396475989371538}, {"id": 1104, "seek": 525764, "start": 5265.64, "end": 5268.64, "text": " So I showcase this right here.", "tokens": [50764, 407, 286, 20388, 341, 558, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11525457451142461, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002396475989371538}, {"id": 1105, "seek": 525764, "start": 5268.64, "end": 5270.64, "text": " We have an int 64.", "tokens": [50914, 492, 362, 364, 560, 12145, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11525457451142461, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002396475989371538}, {"id": 1106, "seek": 525764, "start": 5270.64, "end": 5276.64, "text": " So type of it is an integer and a float 32, 64 and 32 don't mean anything.", "tokens": [51014, 407, 2010, 295, 309, 307, 364, 24922, 293, 257, 15706, 8858, 11, 12145, 293, 8858, 500, 380, 914, 1340, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11525457451142461, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002396475989371538}, {"id": 1107, "seek": 525764, "start": 5276.64, "end": 5279.64, "text": " All we have to know is an integer and floating point number.", "tokens": [51314, 1057, 321, 362, 281, 458, 307, 364, 24922, 293, 12607, 935, 1230, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11525457451142461, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002396475989371538}, {"id": 1108, "seek": 527964, "start": 5279.64, "end": 5292.64, "text": " So I've initialized a torch.randint, I covered above and set above here.", "tokens": [50364, 407, 286, 600, 5883, 1602, 257, 3930, 339, 13, 3699, 686, 11, 286, 5343, 3673, 293, 992, 3673, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2949962799365704, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.18225830793380737}, {"id": 1109, "seek": 527964, "start": 5292.64, "end": 5299.64, "text": " And maybe not.", "tokens": [51014, 400, 1310, 406, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2949962799365704, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.18225830793380737}, {"id": 1110, "seek": 527964, "start": 5299.64, "end": 5307.64, "text": " Anyways, this pretty much does torch.randint is going the first parameter here is anything.", "tokens": [51364, 15585, 11, 341, 1238, 709, 775, 3930, 339, 13, 3699, 686, 307, 516, 264, 700, 13075, 510, 307, 1340, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2949962799365704, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.18225830793380737}, {"id": 1111, "seek": 530764, "start": 5307.64, "end": 5309.64, "text": " It's pretty much your range.", "tokens": [50364, 467, 311, 1238, 709, 428, 3613, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08042851890005716, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.01405902300029993}, {"id": 1112, "seek": 530764, "start": 5309.64, "end": 5314.64, "text": " So I could do like zero to five, or I could just do like one.", "tokens": [50464, 407, 286, 727, 360, 411, 4018, 281, 1732, 11, 420, 286, 727, 445, 360, 411, 472, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08042851890005716, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.01405902300029993}, {"id": 1113, "seek": 530764, "start": 5314.64, "end": 5320.64, "text": " So it'll do zero up to one, and then your shape of the matrix that it generates.", "tokens": [50714, 407, 309, 603, 360, 4018, 493, 281, 472, 11, 293, 550, 428, 3909, 295, 264, 8141, 300, 309, 23815, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08042851890005716, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.01405902300029993}, {"id": 1114, "seek": 530764, "start": 5320.64, "end": 5323.64, "text": " So I said it's a random int.", "tokens": [51014, 407, 286, 848, 309, 311, 257, 4974, 560, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08042851890005716, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.01405902300029993}, {"id": 1115, "seek": 530764, "start": 5323.64, "end": 5329.64, "text": " So that means it's going to generate a tensor with the data type integer 64.", "tokens": [51164, 407, 300, 1355, 309, 311, 516, 281, 8460, 257, 40863, 365, 264, 1412, 2010, 24922, 12145, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08042851890005716, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.01405902300029993}, {"id": 1116, "seek": 532964, "start": 5329.64, "end": 5338.64, "text": " So we have a three by two, and then I initialize another random key detail here.", "tokens": [50364, 407, 321, 362, 257, 1045, 538, 732, 11, 293, 550, 286, 5883, 1125, 1071, 4974, 2141, 2607, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08700619072749696, "compression_ratio": 1.37012987012987, "no_speech_prob": 0.0685051754117012}, {"id": 1117, "seek": 532964, "start": 5338.64, "end": 5341.64, "text": " We don't have the int suffix.", "tokens": [50814, 492, 500, 380, 362, 264, 560, 3889, 970, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08700619072749696, "compression_ratio": 1.37012987012987, "no_speech_prob": 0.0685051754117012}, {"id": 1118, "seek": 532964, "start": 5341.64, "end": 5345.64, "text": " So this just generates floating point numbers.", "tokens": [50964, 407, 341, 445, 23815, 12607, 935, 3547, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08700619072749696, "compression_ratio": 1.37012987012987, "no_speech_prob": 0.0685051754117012}, {"id": 1119, "seek": 532964, "start": 5345.64, "end": 5348.64, "text": " And if we actually return the types of each of these.", "tokens": [51164, 400, 498, 321, 767, 2736, 264, 3467, 295, 1184, 295, 613, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08700619072749696, "compression_ratio": 1.37012987012987, "no_speech_prob": 0.0685051754117012}, {"id": 1120, "seek": 534864, "start": 5348.64, "end": 5360.64, "text": " So five print int 64 dot d type, and then float 32 dot d type.", "tokens": [50364, 407, 1732, 4482, 560, 12145, 5893, 274, 2010, 11, 293, 550, 15706, 8858, 5893, 274, 2010, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21340246342900973, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.30384987592697144}, {"id": 1121, "seek": 534864, "start": 5360.64, "end": 5361.64, "text": " Save that.", "tokens": [50964, 15541, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.21340246342900973, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.30384987592697144}, {"id": 1122, "seek": 534864, "start": 5361.64, "end": 5365.64, "text": " I'm going to comment this out for now.", "tokens": [51014, 286, 478, 516, 281, 2871, 341, 484, 337, 586, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21340246342900973, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.30384987592697144}, {"id": 1123, "seek": 534864, "start": 5365.64, "end": 5368.64, "text": " We get a in 64 and float 32.", "tokens": [51214, 492, 483, 257, 294, 12145, 293, 15706, 8858, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21340246342900973, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.30384987592697144}, {"id": 1124, "seek": 534864, "start": 5368.64, "end": 5377.64, "text": " So if we just try to multiply these together, try to multiply these together.", "tokens": [51364, 407, 498, 321, 445, 853, 281, 12972, 613, 1214, 11, 853, 281, 12972, 613, 1214, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21340246342900973, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.30384987592697144}, {"id": 1125, "seek": 537764, "start": 5377.64, "end": 5380.64, "text": " Expected scalar type long above found float.", "tokens": [50364, 2111, 10729, 39684, 2010, 938, 3673, 1352, 15706, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1126, "seek": 537764, "start": 5380.64, "end": 5383.64, "text": " So long is pretty much when you have a sequence of integers.", "tokens": [50514, 407, 938, 307, 1238, 709, 562, 291, 362, 257, 8310, 295, 41674, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1127, "seek": 537764, "start": 5383.64, "end": 5387.64, "text": " And float is, of course, you have the decimal place.", "tokens": [50664, 400, 15706, 307, 11, 295, 1164, 11, 291, 362, 264, 26601, 1081, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1128, "seek": 537764, "start": 5387.64, "end": 5389.64, "text": " So you can actually multiply this together.", "tokens": [50864, 407, 291, 393, 767, 12972, 341, 1214, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1129, "seek": 537764, "start": 5389.64, "end": 5397.64, "text": " So pretty much what you can do is cast the float method on this.", "tokens": [50964, 407, 1238, 709, 437, 291, 393, 360, 307, 4193, 264, 15706, 3170, 322, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1130, "seek": 537764, "start": 5397.64, "end": 5404.64, "text": " If you just do dot float, and then parentheses, and then run this, it'll actually work.", "tokens": [51364, 759, 291, 445, 360, 5893, 15706, 11, 293, 550, 34153, 11, 293, 550, 1190, 341, 11, 309, 603, 767, 589, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11283899122668851, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.004069627728313208}, {"id": 1131, "seek": 540464, "start": 5404.64, "end": 5407.64, "text": " So you can cast integers to floats.", "tokens": [50364, 407, 291, 393, 4193, 41674, 281, 37878, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09021755365224984, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.026753460988402367}, {"id": 1132, "seek": 540464, "start": 5407.64, "end": 5411.64, "text": " And then I think there's a way you can cast floats to integers, but it has some rounding in there.", "tokens": [50514, 400, 550, 286, 519, 456, 311, 257, 636, 291, 393, 4193, 37878, 281, 41674, 11, 457, 309, 575, 512, 48237, 294, 456, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09021755365224984, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.026753460988402367}, {"id": 1133, "seek": 540464, "start": 5411.64, "end": 5417.64, "text": " So probably not the best for input and weights, matrix multiplication.", "tokens": [50714, 407, 1391, 406, 264, 1151, 337, 4846, 293, 17443, 11, 8141, 27290, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09021755365224984, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.026753460988402367}, {"id": 1134, "seek": 540464, "start": 5417.64, "end": 5426.64, "text": " But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise.", "tokens": [51014, 583, 1338, 11, 1238, 709, 498, 291, 434, 884, 604, 636, 281, 8141, 27290, 11, 309, 311, 516, 281, 312, 1228, 12607, 935, 3547, 570, 264, 17443, 486, 483, 4664, 13600, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09021755365224984, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.026753460988402367}, {"id": 1135, "seek": 540464, "start": 5426.64, "end": 5430.64, "text": " So you want to make sure that they have sort of room to float around.", "tokens": [51464, 407, 291, 528, 281, 652, 988, 300, 436, 362, 1333, 295, 1808, 281, 15706, 926, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09021755365224984, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.026753460988402367}, {"id": 1136, "seek": 543064, "start": 5430.64, "end": 5433.64, "text": " So that's pretty much how you avoid that error.", "tokens": [50364, 407, 300, 311, 1238, 709, 577, 291, 5042, 300, 6713, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1137, "seek": 543064, "start": 5433.64, "end": 5434.64, "text": " Let's move on.", "tokens": [50514, 961, 311, 1286, 322, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1138, "seek": 543064, "start": 5434.64, "end": 5435.64, "text": " So congratulations.", "tokens": [50564, 407, 13568, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1139, "seek": 543064, "start": 5435.64, "end": 5438.64, "text": " You probably made it further than quite a few people already.", "tokens": [50614, 509, 1391, 1027, 309, 3052, 813, 1596, 257, 1326, 561, 1217, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1140, "seek": 543064, "start": 5438.64, "end": 5440.64, "text": " So congratulations on that.", "tokens": [50764, 407, 13568, 322, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1141, "seek": 543064, "start": 5440.64, "end": 5444.64, "text": " That was one of the most comprehensive parts of this entire course.", "tokens": [50864, 663, 390, 472, 295, 264, 881, 13914, 3166, 295, 341, 2302, 1164, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1142, "seek": 543064, "start": 5444.64, "end": 5448.64, "text": " Understanding the math is going on behind the scenes.", "tokens": [51064, 36858, 264, 5221, 307, 516, 322, 2261, 264, 8026, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1143, "seek": 543064, "start": 5448.64, "end": 5452.64, "text": " For some people, it's very hard to grasp if you're not very fluent with math.", "tokens": [51264, 1171, 512, 561, 11, 309, 311, 588, 1152, 281, 21743, 498, 291, 434, 406, 588, 40799, 365, 5221, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1144, "seek": 543064, "start": 5452.64, "end": 5458.64, "text": " But yeah, let's continue the biogram language model and let's pump out some code here.", "tokens": [51464, 583, 1338, 11, 718, 311, 2354, 264, 3228, 12820, 2856, 2316, 293, 718, 311, 5889, 484, 512, 3089, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09454454001733813, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.025167521089315414}, {"id": 1145, "seek": 545864, "start": 5458.64, "end": 5463.64, "text": " So to recap, we're using CUDA to accelerate the training process.", "tokens": [50364, 407, 281, 20928, 11, 321, 434, 1228, 29777, 7509, 281, 21341, 264, 3097, 1399, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1146, "seek": 545864, "start": 5463.64, "end": 5470.64, "text": " We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel.", "tokens": [50614, 492, 362, 732, 9848, 2181, 335, 6202, 11, 3461, 2744, 337, 264, 4641, 295, 41674, 11, 293, 15245, 337, 577, 867, 295, 729, 366, 2614, 294, 8952, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1147, "seek": 545864, "start": 5470.64, "end": 5472.64, "text": " Two hyperparameters.", "tokens": [50964, 4453, 9848, 2181, 335, 6202, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1148, "seek": 545864, "start": 5472.64, "end": 5474.64, "text": " We open our text.", "tokens": [51064, 492, 1269, 527, 2487, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1149, "seek": 545864, "start": 5474.64, "end": 5476.64, "text": " We make a vocabulary out of it.", "tokens": [51164, 492, 652, 257, 19864, 484, 295, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1150, "seek": 545864, "start": 5476.64, "end": 5479.64, "text": " We initialize our encoder and decoder.", "tokens": [51264, 492, 5883, 1125, 527, 2058, 19866, 293, 979, 19866, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1151, "seek": 545864, "start": 5479.64, "end": 5486.64, "text": " We get our data encoding all this text, and then we get our train and bow splits.", "tokens": [51414, 492, 483, 527, 1412, 43430, 439, 341, 2487, 11, 293, 550, 321, 483, 527, 3847, 293, 4503, 37741, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10043949055894513, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.019411729648709297}, {"id": 1152, "seek": 548664, "start": 5486.64, "end": 5488.64, "text": " And then this next function here, get batch.", "tokens": [50364, 400, 550, 341, 958, 2445, 510, 11, 483, 15245, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10493331342130094, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.013842973858118057}, {"id": 1153, "seek": 548664, "start": 5488.64, "end": 5494.64, "text": " So before I jump into this, go ahead and run this here.", "tokens": [50464, 407, 949, 286, 3012, 666, 341, 11, 352, 2286, 293, 1190, 341, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10493331342130094, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.013842973858118057}, {"id": 1154, "seek": 548664, "start": 5494.64, "end": 5502.64, "text": " So this is pretty much just taking the first little, I don't know, we have eight characters.", "tokens": [50764, 407, 341, 307, 1238, 709, 445, 1940, 264, 700, 707, 11, 286, 500, 380, 458, 11, 321, 362, 3180, 4342, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10493331342130094, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.013842973858118057}, {"id": 1155, "seek": 548664, "start": 5502.64, "end": 5509.64, "text": " So it's taking the first eight characters and then index one all the way to index nine.", "tokens": [51164, 407, 309, 311, 1940, 264, 700, 3180, 4342, 293, 550, 8186, 472, 439, 264, 636, 281, 8186, 4949, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10493331342130094, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.013842973858118057}, {"id": 1156, "seek": 550964, "start": 5510.64, "end": 5518.64, "text": " And we can pretty much use this to show what the current input is and then what the target would be.", "tokens": [50414, 400, 321, 393, 1238, 709, 764, 341, 281, 855, 437, 264, 2190, 4846, 307, 293, 550, 437, 264, 3779, 576, 312, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1157, "seek": 550964, "start": 5518.64, "end": 5526.64, "text": " So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right?", "tokens": [50814, 407, 498, 321, 362, 4688, 11, 3779, 307, 472, 11, 4688, 293, 472, 11, 3779, 307, 472, 11, 4688, 293, 472, 11, 3779, 307, 7562, 11, 1030, 11458, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1158, "seek": 550964, "start": 5526.64, "end": 5529.64, "text": " So this is the premise of the biogram language model.", "tokens": [51214, 407, 341, 307, 264, 22045, 295, 264, 3228, 12820, 2856, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1159, "seek": 550964, "start": 5529.64, "end": 5531.64, "text": " Given this character, we're going to predict the next.", "tokens": [51364, 18600, 341, 2517, 11, 321, 434, 516, 281, 6069, 264, 958, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1160, "seek": 550964, "start": 5531.64, "end": 5534.64, "text": " It doesn't know anything else in the entire history.", "tokens": [51464, 467, 1177, 380, 458, 1340, 1646, 294, 264, 2302, 2503, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1161, "seek": 550964, "start": 5534.64, "end": 5538.64, "text": " It just knows what's before it or just knows what the current character is.", "tokens": [51614, 467, 445, 3255, 437, 311, 949, 309, 420, 445, 3255, 437, 264, 2190, 2517, 307, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1415758779493429, "compression_ratio": 1.768, "no_speech_prob": 0.54606693983078}, {"id": 1162, "seek": 553864, "start": 5538.64, "end": 5542.64, "text": " And based on that, we're going to predict the next one.", "tokens": [50364, 400, 2361, 322, 300, 11, 321, 434, 516, 281, 6069, 264, 958, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11588749518761268, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.010324507020413876}, {"id": 1163, "seek": 553864, "start": 5542.64, "end": 5548.64, "text": " So we have this get batch function here, and this part right here is the most important piece of code.", "tokens": [50564, 407, 321, 362, 341, 483, 15245, 2445, 510, 11, 293, 341, 644, 558, 510, 307, 264, 881, 1021, 2522, 295, 3089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11588749518761268, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.010324507020413876}, {"id": 1164, "seek": 553864, "start": 5548.64, "end": 5555.64, "text": " This is going to work a little bit more later with our train and bow splits, making sure that, you know,", "tokens": [50864, 639, 307, 516, 281, 589, 257, 707, 857, 544, 1780, 365, 527, 3847, 293, 4503, 37741, 11, 1455, 988, 300, 11, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11588749518761268, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.010324507020413876}, {"id": 1165, "seek": 553864, "start": 5555.64, "end": 5558.64, "text": " I'll try to explain this in a different way with our training bow splits.", "tokens": [51214, 286, 603, 853, 281, 2903, 341, 294, 257, 819, 636, 365, 527, 3097, 4503, 37741, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11588749518761268, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.010324507020413876}, {"id": 1166, "seek": 553864, "start": 5558.64, "end": 5562.64, "text": " So imagine you take a course, as you take a math course, okay?", "tokens": [51364, 407, 3811, 291, 747, 257, 1164, 11, 382, 291, 747, 257, 5221, 1164, 11, 1392, 30, 51564], "temperature": 0.0, "avg_logprob": -0.11588749518761268, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.010324507020413876}, {"id": 1167, "seek": 556264, "start": 5562.64, "end": 5569.64, "text": " And 90% of all your work is done just learning how the course works, learning all about the math.", "tokens": [50364, 400, 4289, 4, 295, 439, 428, 589, 307, 1096, 445, 2539, 577, 264, 1164, 1985, 11, 2539, 439, 466, 264, 5221, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1168, "seek": 556264, "start": 5569.64, "end": 5572.64, "text": " So that's like 90% of data you get from it.", "tokens": [50714, 407, 300, 311, 411, 4289, 4, 295, 1412, 291, 483, 490, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1169, "seek": 556264, "start": 5572.64, "end": 5574.64, "text": " And then maybe another 10%.", "tokens": [50864, 400, 550, 1310, 1071, 1266, 6856, 50964], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1170, "seek": 556264, "start": 5574.64, "end": 5580.64, "text": " Another 10% at the end is that final exam, which might have some questions you've never seen before.", "tokens": [50964, 3996, 1266, 4, 412, 264, 917, 307, 300, 2572, 1139, 11, 597, 1062, 362, 512, 1651, 291, 600, 1128, 1612, 949, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1171, "seek": 556264, "start": 5580.64, "end": 5585.64, "text": " So the point is in that first 90%, you're tested on based on what you know.", "tokens": [51264, 407, 264, 935, 307, 294, 300, 700, 4289, 8923, 291, 434, 8246, 322, 2361, 322, 437, 291, 458, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1172, "seek": 556264, "start": 5585.64, "end": 5589.64, "text": " And then this other 10% is what you don't know.", "tokens": [51514, 400, 550, 341, 661, 1266, 4, 307, 437, 291, 500, 380, 458, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06605104131436129, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.19182565808296204}, {"id": 1173, "seek": 558964, "start": 5589.64, "end": 5595.64, "text": " And this pretty much means you can't memorize everything and then just start generating based on your memory.", "tokens": [50364, 400, 341, 1238, 709, 1355, 291, 393, 380, 27478, 1203, 293, 550, 445, 722, 17746, 2361, 322, 428, 4675, 13, 50664], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1174, "seek": 558964, "start": 5595.64, "end": 5601.64, "text": " You generate something that's alike or something that's close based on what you already know and the patterns you captured", "tokens": [50664, 509, 8460, 746, 300, 311, 20025, 420, 746, 300, 311, 1998, 2361, 322, 437, 291, 1217, 458, 293, 264, 8294, 291, 11828, 50964], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1175, "seek": 558964, "start": 5601.64, "end": 5604.64, "text": " in that 90% of the course.", "tokens": [50964, 294, 300, 4289, 4, 295, 264, 1164, 13, 51114], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1176, "seek": 558964, "start": 5604.64, "end": 5606.64, "text": " So you can write your final exam successfully.", "tokens": [51114, 407, 291, 393, 2464, 428, 2572, 1139, 10727, 13, 51214], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1177, "seek": 558964, "start": 5606.64, "end": 5608.64, "text": " So that's pretty much what's going on here.", "tokens": [51214, 407, 300, 311, 1238, 709, 437, 311, 516, 322, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1178, "seek": 558964, "start": 5608.64, "end": 5616.64, "text": " The training is the course, learning everything about it and then validation is validating the final exam.", "tokens": [51314, 440, 3097, 307, 264, 1164, 11, 2539, 1203, 466, 309, 293, 550, 24071, 307, 7363, 990, 264, 2572, 1139, 13, 51714], "temperature": 0.0, "avg_logprob": -0.078498886181758, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.0033761507365852594}, {"id": 1179, "seek": 561664, "start": 5616.64, "end": 5629.64, "text": " So pretty much what we're doing here is initialize IX and that'll take a random manager between zero", "tokens": [50364, 407, 1238, 709, 437, 321, 434, 884, 510, 307, 5883, 1125, 286, 55, 293, 300, 603, 747, 257, 4974, 6598, 1296, 4018, 51014], "temperature": 0.0, "avg_logprob": -0.1262063588181587, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.005219401326030493}, {"id": 1180, "seek": 561664, "start": 5629.64, "end": 5634.64, "text": " and then length of the length of the entire text minus block size.", "tokens": [51014, 293, 550, 4641, 295, 264, 4641, 295, 264, 2302, 2487, 3175, 3461, 2744, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1262063588181587, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.005219401326030493}, {"id": 1181, "seek": 561664, "start": 5634.64, "end": 5644.64, "text": " So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data.", "tokens": [51264, 407, 498, 291, 483, 264, 8186, 300, 311, 412, 4641, 295, 1412, 3175, 3461, 2744, 11, 291, 603, 920, 483, 264, 4342, 493, 281, 264, 4641, 295, 1412, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1262063588181587, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.005219401326030493}, {"id": 1182, "seek": 564464, "start": 5644.64, "end": 5646.64, "text": " So that's kind of how that works.", "tokens": [50364, 407, 300, 311, 733, 295, 577, 300, 1985, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1183, "seek": 564464, "start": 5646.64, "end": 5650.64, "text": " And if we print this out here, it'll just give us this right here.", "tokens": [50464, 400, 498, 321, 4482, 341, 484, 510, 11, 309, 603, 445, 976, 505, 341, 558, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1184, "seek": 564464, "start": 5650.64, "end": 5651.64, "text": " So we get some random integers.", "tokens": [50664, 407, 321, 483, 512, 4974, 41674, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1185, "seek": 564464, "start": 5651.64, "end": 5660.64, "text": " These are some random indices in the entire text that we can start generating from.", "tokens": [50714, 1981, 366, 512, 4974, 43840, 294, 264, 2302, 2487, 300, 321, 393, 722, 17746, 490, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1186, "seek": 564464, "start": 5660.64, "end": 5664.64, "text": " So print this out and then torch.stack.", "tokens": [51164, 407, 4482, 341, 484, 293, 550, 27822, 13, 372, 501, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1187, "seek": 564464, "start": 5664.64, "end": 5665.64, "text": " We covered this before.", "tokens": [51364, 492, 5343, 341, 949, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1188, "seek": 564464, "start": 5665.64, "end": 5668.64, "text": " Pretty much what this does, it's going to stack them in batches.", "tokens": [51414, 10693, 709, 437, 341, 775, 11, 309, 311, 516, 281, 8630, 552, 294, 15245, 279, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1189, "seek": 564464, "start": 5668.64, "end": 5670.64, "text": " This is the entire point of batches.", "tokens": [51564, 639, 307, 264, 2302, 935, 295, 15245, 279, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08122679039284035, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.012238088063895702}, {"id": 1190, "seek": 567064, "start": 5670.64, "end": 5673.64, "text": " So that's what we do there.", "tokens": [50364, 407, 300, 311, 437, 321, 360, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10126476011414459, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.05259634554386139}, {"id": 1191, "seek": 567064, "start": 5673.64, "end": 5679.64, "text": " We get X and then Y is just the same thing, but offset by one like this.", "tokens": [50514, 492, 483, 1783, 293, 550, 398, 307, 445, 264, 912, 551, 11, 457, 18687, 538, 472, 411, 341, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10126476011414459, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.05259634554386139}, {"id": 1192, "seek": 567064, "start": 5679.64, "end": 5682.64, "text": " So that's what happens there.", "tokens": [50814, 407, 300, 311, 437, 2314, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10126476011414459, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.05259634554386139}, {"id": 1193, "seek": 567064, "start": 5682.64, "end": 5687.64, "text": " And let's get into actually, I'm going to add something here.", "tokens": [50964, 400, 718, 311, 483, 666, 767, 11, 286, 478, 516, 281, 909, 746, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10126476011414459, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.05259634554386139}, {"id": 1194, "seek": 567064, "start": 5687.64, "end": 5688.64, "text": " This is going to be very important.", "tokens": [51214, 639, 307, 516, 281, 312, 588, 1021, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10126476011414459, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.05259634554386139}, {"id": 1195, "seek": 568864, "start": 5688.64, "end": 5696.64, "text": " We're going to go X and Y is equal to model dot.", "tokens": [50364, 492, 434, 516, 281, 352, 1783, 293, 398, 307, 2681, 281, 2316, 5893, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1836194329791599, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.46853429079055786}, {"id": 1196, "seek": 568864, "start": 5696.64, "end": 5702.64, "text": " We're going to go X dot to device.", "tokens": [50764, 492, 434, 516, 281, 352, 1783, 5893, 281, 4302, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1836194329791599, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.46853429079055786}, {"id": 1197, "seek": 568864, "start": 5702.64, "end": 5706.64, "text": " So notice how, no, we didn't do it up here.", "tokens": [51064, 407, 3449, 577, 11, 572, 11, 321, 994, 380, 360, 309, 493, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1836194329791599, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.46853429079055786}, {"id": 1198, "seek": 568864, "start": 5706.64, "end": 5715.64, "text": " Okay, we'll cover this later, but pretty much you're going to see what this does in a second here.", "tokens": [51264, 1033, 11, 321, 603, 2060, 341, 1780, 11, 457, 1238, 709, 291, 434, 516, 281, 536, 437, 341, 775, 294, 257, 1150, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1836194329791599, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.46853429079055786}, {"id": 1199, "seek": 571564, "start": 5716.64, "end": 5723.64, "text": " We return these and you can see that the device changed.", "tokens": [50414, 492, 2736, 613, 293, 291, 393, 536, 300, 264, 4302, 3105, 13, 50764], "temperature": 0.0, "avg_logprob": -0.137438786657233, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.014500116929411888}, {"id": 1200, "seek": 571564, "start": 5723.64, "end": 5724.64, "text": " So now we're actually on CUDA.", "tokens": [50764, 407, 586, 321, 434, 767, 322, 29777, 7509, 13, 50814], "temperature": 0.0, "avg_logprob": -0.137438786657233, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.014500116929411888}, {"id": 1201, "seek": 571564, "start": 5724.64, "end": 5733.64, "text": " And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU.", "tokens": [50814, 400, 341, 307, 534, 665, 570, 613, 732, 3755, 295, 1412, 510, 11, 264, 15743, 293, 264, 12911, 366, 572, 2854, 322, 264, 13199, 13, 51264], "temperature": 0.0, "avg_logprob": -0.137438786657233, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.014500116929411888}, {"id": 1202, "seek": 571564, "start": 5733.64, "end": 5739.64, "text": " They're no longer going to be processed sequentially, but rather in our batches in parallel.", "tokens": [51264, 814, 434, 572, 2854, 516, 281, 312, 18846, 5123, 3137, 11, 457, 2831, 294, 527, 15245, 279, 294, 8952, 13, 51564], "temperature": 0.0, "avg_logprob": -0.137438786657233, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.014500116929411888}, {"id": 1203, "seek": 573964, "start": 5739.64, "end": 5748.64, "text": " So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here.", "tokens": [50364, 407, 300, 311, 1238, 709, 577, 291, 2944, 604, 2522, 295, 1412, 420, 9834, 281, 264, 18407, 307, 445, 5893, 281, 293, 550, 264, 4302, 597, 291, 5883, 1602, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09800254242329658, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.011156941764056683}, {"id": 1204, "seek": 573964, "start": 5748.64, "end": 5752.64, "text": " So now we can go ahead and actually initialize our neural net.", "tokens": [50814, 407, 586, 321, 393, 352, 2286, 293, 767, 5883, 1125, 527, 18161, 2533, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09800254242329658, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.011156941764056683}, {"id": 1205, "seek": 573964, "start": 5752.64, "end": 5759.64, "text": " So what I'm going to do is I'm going to go back up here and we're going to import some more stuff.", "tokens": [51014, 407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 352, 646, 493, 510, 293, 321, 434, 516, 281, 974, 512, 544, 1507, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09800254242329658, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.011156941764056683}, {"id": 1206, "seek": 575964, "start": 5759.64, "end": 5767.64, "text": " So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second.", "tokens": [50364, 407, 286, 478, 516, 281, 974, 5893, 297, 77, 382, 297, 77, 293, 291, 434, 516, 281, 536, 983, 257, 688, 295, 341, 307, 1021, 294, 257, 1150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18830883719704367, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.021612269803881645}, {"id": 1207, "seek": 575964, "start": 5767.64, "end": 5769.64, "text": " I'm going to explain this here.", "tokens": [50764, 286, 478, 516, 281, 2903, 341, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18830883719704367, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.021612269803881645}, {"id": 1208, "seek": 575964, "start": 5769.64, "end": 5780.64, "text": " I just want to get some code out first.", "tokens": [50864, 286, 445, 528, 281, 483, 512, 3089, 484, 700, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18830883719704367, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.021612269803881645}, {"id": 1209, "seek": 578064, "start": 5780.64, "end": 5782.64, "text": " And down here we can initialize this.", "tokens": [50364, 400, 760, 510, 321, 393, 5883, 1125, 341, 13, 50464], "temperature": 0.0, "avg_logprob": -0.20936567253536648, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.11119872331619263}, {"id": 1210, "seek": 578064, "start": 5782.64, "end": 5783.64, "text": " So it's a class.", "tokens": [50464, 407, 309, 311, 257, 1508, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20936567253536648, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.11119872331619263}, {"id": 1211, "seek": 578064, "start": 5783.64, "end": 5793.64, "text": " We're going to make it a by-gram language model subclass of nn.module.", "tokens": [50514, 492, 434, 516, 281, 652, 309, 257, 538, 12, 1342, 2856, 2316, 1422, 11665, 295, 297, 77, 13, 8014, 2271, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20936567253536648, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.11119872331619263}, {"id": 1212, "seek": 578064, "start": 5793.64, "end": 5802.64, "text": " And the reason why we do nn.module here is because it's going to take an nn.module.", "tokens": [51014, 400, 264, 1778, 983, 321, 360, 297, 77, 13, 8014, 2271, 510, 307, 570, 309, 311, 516, 281, 747, 364, 297, 77, 13, 8014, 2271, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20936567253536648, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.11119872331619263}, {"id": 1213, "seek": 580264, "start": 5802.64, "end": 5817.64, "text": " I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters.", "tokens": [50364, 286, 500, 380, 458, 577, 281, 2903, 341, 411, 31762, 11, 457, 1238, 709, 562, 321, 764, 264, 297, 77, 13, 8014, 2271, 6828, 294, 9953, 51, 284, 339, 293, 309, 311, 1854, 295, 257, 297, 77, 13, 8014, 2271, 1422, 11665, 11, 436, 434, 439, 1466, 712, 9834, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09001975793104905, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.29403916001319885}, {"id": 1214, "seek": 580264, "start": 5817.64, "end": 5822.64, "text": " So I'm going to go ahead and look at the documentation here so you can sort of understand this better.", "tokens": [51114, 407, 286, 478, 516, 281, 352, 2286, 293, 574, 412, 264, 14333, 510, 370, 291, 393, 1333, 295, 1223, 341, 1101, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09001975793104905, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.29403916001319885}, {"id": 1215, "seek": 582264, "start": 5823.64, "end": 5825.64, "text": " We go to nn.", "tokens": [50414, 492, 352, 281, 297, 77, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1232395822351629, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.5073651671409607}, {"id": 1216, "seek": 582264, "start": 5825.64, "end": 5836.64, "text": " So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before.", "tokens": [50514, 407, 1238, 709, 439, 295, 613, 45216, 304, 7914, 11, 18680, 1753, 7914, 11, 31782, 11, 8213, 11, 411, 321, 2956, 412, 8213, 7914, 949, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1232395822351629, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.5073651671409607}, {"id": 1217, "seek": 582264, "start": 5836.64, "end": 5838.64, "text": " So we have nn.linear.", "tokens": [51064, 407, 321, 362, 297, 77, 13, 28263, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1232395822351629, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.5073651671409607}, {"id": 1218, "seek": 582264, "start": 5838.64, "end": 5846.64, "text": " So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable.", "tokens": [51164, 407, 498, 321, 764, 297, 77, 13, 28263, 1854, 295, 341, 11, 300, 1355, 300, 264, 297, 77, 13, 28263, 9834, 366, 1466, 712, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1232395822351629, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.5073651671409607}, {"id": 1219, "seek": 582264, "start": 5846.64, "end": 5851.64, "text": " So that weight matrix will be changed through gradient descent.", "tokens": [51564, 407, 300, 3364, 8141, 486, 312, 3105, 807, 16235, 23475, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1232395822351629, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.5073651671409607}, {"id": 1220, "seek": 585164, "start": 5851.64, "end": 5854.64, "text": " And actually, I think I should probably cover gradient descent right now.", "tokens": [50364, 400, 767, 11, 286, 519, 286, 820, 1391, 2060, 16235, 23475, 558, 586, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1221, "seek": 585164, "start": 5854.64, "end": 5862.64, "text": " So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better.", "tokens": [50514, 407, 294, 1389, 512, 295, 291, 500, 380, 458, 437, 309, 307, 11, 309, 311, 516, 281, 312, 534, 1152, 281, 1223, 2293, 577, 321, 652, 264, 3209, 1101, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1222, "seek": 585164, "start": 5862.64, "end": 5866.64, "text": " So I'm going to go ahead and set up a little graph for that right now.", "tokens": [50914, 407, 286, 478, 516, 281, 352, 2286, 293, 992, 493, 257, 707, 4295, 337, 300, 558, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1223, "seek": 585164, "start": 5866.64, "end": 5869.64, "text": " So I'm going to be using a little tool called Desmos.", "tokens": [51114, 407, 286, 478, 516, 281, 312, 1228, 257, 707, 2290, 1219, 3885, 3415, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1224, "seek": 585164, "start": 5869.64, "end": 5871.64, "text": " Desmos is actually great.", "tokens": [51264, 3885, 3415, 307, 767, 869, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1225, "seek": 585164, "start": 5871.64, "end": 5873.64, "text": " It acts as a graphing calculator.", "tokens": [51364, 467, 10672, 382, 257, 1295, 79, 571, 24993, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1226, "seek": 585164, "start": 5873.64, "end": 5876.64, "text": " So you can plug in formulas and move things around.", "tokens": [51464, 407, 291, 393, 5452, 294, 30546, 293, 1286, 721, 926, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1227, "seek": 585164, "start": 5876.64, "end": 5879.64, "text": " You sort of visualize how math functions work.", "tokens": [51614, 509, 1333, 295, 23273, 577, 5221, 6828, 589, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06049341009568798, "compression_ratio": 1.6875, "no_speech_prob": 0.0012642984511330724}, {"id": 1228, "seek": 587964, "start": 5879.64, "end": 5886.64, "text": " So I've written some functions out here that will basically calculate the derivative of a sine wave.", "tokens": [50364, 407, 286, 600, 3720, 512, 6828, 484, 510, 300, 486, 1936, 8873, 264, 13760, 295, 257, 18609, 5772, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07186156681605747, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0008040200336836278}, {"id": 1229, "seek": 587964, "start": 5886.64, "end": 5890.64, "text": " So if I move A around, you'll see that changes.", "tokens": [50714, 407, 498, 286, 1286, 316, 926, 11, 291, 603, 536, 300, 2962, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07186156681605747, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0008040200336836278}, {"id": 1230, "seek": 587964, "start": 5890.64, "end": 5897.64, "text": " So before I get into what's really going on here, I need to first tell you what the loss actually is.", "tokens": [50914, 407, 949, 286, 483, 666, 437, 311, 534, 516, 322, 510, 11, 286, 643, 281, 700, 980, 291, 437, 264, 4470, 767, 307, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07186156681605747, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0008040200336836278}, {"id": 1231, "seek": 587964, "start": 5897.64, "end": 5903.64, "text": " If you're not familiar with the loss, let's say we have 80 characters in our vocabulary.", "tokens": [51264, 759, 291, 434, 406, 4963, 365, 264, 4470, 11, 718, 311, 584, 321, 362, 4688, 4342, 294, 527, 19864, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07186156681605747, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0008040200336836278}, {"id": 1232, "seek": 587964, "start": 5903.64, "end": 5908.64, "text": " And we have just started our model, no training at all, completely random weights.", "tokens": [51564, 400, 321, 362, 445, 1409, 527, 2316, 11, 572, 3097, 412, 439, 11, 2584, 4974, 17443, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07186156681605747, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0008040200336836278}, {"id": 1233, "seek": 590864, "start": 5908.64, "end": 5914.64, "text": " And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully.", "tokens": [50364, 400, 29400, 11, 456, 311, 516, 281, 312, 257, 472, 294, 4688, 2931, 300, 321, 767, 6069, 958, 14862, 10727, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1234, "seek": 590864, "start": 5914.64, "end": 5922.64, "text": " So how we can measure the loss of this is by taking the negative log likelihood.", "tokens": [50664, 407, 577, 321, 393, 3481, 264, 4470, 295, 341, 307, 538, 1940, 264, 3671, 3565, 22119, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1235, "seek": 590864, "start": 5922.64, "end": 5924.64, "text": " So the likelihood is one out of 80.", "tokens": [51064, 407, 264, 22119, 307, 472, 484, 295, 4688, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1236, "seek": 590864, "start": 5924.64, "end": 5927.64, "text": " We take the log of that and then negative.", "tokens": [51164, 492, 747, 264, 3565, 295, 300, 293, 550, 3671, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1237, "seek": 590864, "start": 5927.64, "end": 5931.64, "text": " So if we plug this in here, we'll get 4.38.", "tokens": [51314, 407, 498, 321, 5452, 341, 294, 510, 11, 321, 603, 483, 1017, 13, 12625, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1238, "seek": 590864, "start": 5931.64, "end": 5933.64, "text": " So that's a terrible loss.", "tokens": [51514, 407, 300, 311, 257, 6237, 4470, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1239, "seek": 590864, "start": 5933.64, "end": 5935.64, "text": " Obviously, that's one out of 80.", "tokens": [51614, 7580, 11, 300, 311, 472, 484, 295, 4688, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09916178996746357, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.03845017030835152}, {"id": 1240, "seek": 593564, "start": 5935.64, "end": 5939.64, "text": " So it's like, you know, not even 2% chance.", "tokens": [50364, 407, 309, 311, 411, 11, 291, 458, 11, 406, 754, 568, 4, 2931, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1241, "seek": 593564, "start": 5939.64, "end": 5941.64, "text": " So that's not great.", "tokens": [50564, 407, 300, 311, 406, 869, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1242, "seek": 593564, "start": 5941.64, "end": 5948.64, "text": " So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss.", "tokens": [50664, 407, 1238, 709, 264, 935, 307, 281, 17522, 264, 4470, 11, 3488, 264, 17630, 14170, 420, 17522, 264, 4470, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1243, "seek": 593564, "start": 5948.64, "end": 5950.64, "text": " And that's how we train our network.", "tokens": [51014, 400, 300, 311, 577, 321, 3847, 527, 3209, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1244, "seek": 593564, "start": 5950.64, "end": 5951.64, "text": " So how does this actually work?", "tokens": [51114, 407, 577, 775, 341, 767, 589, 30, 51164], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1245, "seek": 593564, "start": 5951.64, "end": 5953.64, "text": " How does this actually work out in code, you ask?", "tokens": [51164, 1012, 775, 341, 767, 589, 484, 294, 3089, 11, 291, 1029, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1246, "seek": 593564, "start": 5953.64, "end": 5957.64, "text": " So pretty much, let's say we have a loss here, okay?", "tokens": [51264, 407, 1238, 709, 11, 718, 311, 584, 321, 362, 257, 4470, 510, 11, 1392, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1247, "seek": 593564, "start": 5957.64, "end": 5960.64, "text": " Start off with a loss of 2, just arbitrary loss, whatever.", "tokens": [51464, 6481, 766, 365, 257, 4470, 295, 568, 11, 445, 23211, 4470, 11, 2035, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1248, "seek": 593564, "start": 5960.64, "end": 5964.64, "text": " And what we're trying to do is decrease it.", "tokens": [51614, 400, 437, 321, 434, 1382, 281, 360, 307, 11514, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08187894523143768, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.001987557392567396}, {"id": 1249, "seek": 596464, "start": 5964.64, "end": 5969.64, "text": " So over time, it's going to become smaller and smaller if we move in this direction.", "tokens": [50364, 407, 670, 565, 11, 309, 311, 516, 281, 1813, 4356, 293, 4356, 498, 321, 1286, 294, 341, 3513, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1250, "seek": 596464, "start": 5969.64, "end": 5972.64, "text": " So how do we know if we're moving in the right direction?", "tokens": [50614, 407, 577, 360, 321, 458, 498, 321, 434, 2684, 294, 264, 558, 3513, 30, 50764], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1251, "seek": 596464, "start": 5972.64, "end": 5977.64, "text": " Well, we take the derivative of what the current point is at right now,", "tokens": [50764, 1042, 11, 321, 747, 264, 13760, 295, 437, 264, 2190, 935, 307, 412, 558, 586, 11, 51014], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1252, "seek": 596464, "start": 5977.64, "end": 5979.64, "text": " and then we try moving it in a different direction.", "tokens": [51014, 293, 550, 321, 853, 2684, 309, 294, 257, 819, 3513, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1253, "seek": 596464, "start": 5979.64, "end": 5982.64, "text": " So if we move it this way, sure, it'll go down.", "tokens": [51114, 407, 498, 321, 1286, 309, 341, 636, 11, 988, 11, 309, 603, 352, 760, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1254, "seek": 596464, "start": 5982.64, "end": 5983.64, "text": " That's great.", "tokens": [51264, 663, 311, 869, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1255, "seek": 596464, "start": 5983.64, "end": 5987.64, "text": " We can hit the local bottom over there, or we can move to this side.", "tokens": [51314, 492, 393, 2045, 264, 2654, 2767, 670, 456, 11, 420, 321, 393, 1286, 281, 341, 1252, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1256, "seek": 596464, "start": 5987.64, "end": 5991.64, "text": " And then we can see that the slope is increasing in a negative direction.", "tokens": [51514, 400, 550, 321, 393, 536, 300, 264, 13525, 307, 5662, 294, 257, 3671, 3513, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06839542241059532, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.012050463818013668}, {"id": 1257, "seek": 599164, "start": 5991.64, "end": 5996.64, "text": " So we're going to keep adjusting the parameters in favor of this direction.", "tokens": [50364, 407, 321, 434, 516, 281, 1066, 23559, 264, 9834, 294, 2294, 295, 341, 3513, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1258, "seek": 599164, "start": 5996.64, "end": 5999.64, "text": " So that's pretty much what gradient descent is.", "tokens": [50614, 407, 300, 311, 1238, 709, 437, 16235, 23475, 307, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1259, "seek": 599164, "start": 5999.64, "end": 6003.64, "text": " We're descending with the gradient.", "tokens": [50764, 492, 434, 40182, 365, 264, 16235, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1260, "seek": 599164, "start": 6003.64, "end": 6005.64, "text": " So pretty self-explanatory.", "tokens": [50964, 407, 1238, 2698, 12, 3121, 16554, 4745, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1261, "seek": 599164, "start": 6005.64, "end": 6007.64, "text": " That's what the loss function does.", "tokens": [51064, 663, 311, 437, 264, 4470, 2445, 775, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1262, "seek": 599164, "start": 6007.64, "end": 6011.64, "text": " And gradient descent is an optimizer.", "tokens": [51164, 400, 16235, 23475, 307, 364, 5028, 6545, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1263, "seek": 599164, "start": 6011.64, "end": 6013.64, "text": " So it's an optimizer for the network.", "tokens": [51364, 407, 309, 311, 364, 5028, 6545, 337, 264, 3209, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1264, "seek": 599164, "start": 6013.64, "end": 6017.64, "text": " Optimizes our parameters, our weight, matrices, etc.", "tokens": [51464, 35013, 5660, 527, 9834, 11, 527, 3364, 11, 32284, 11, 5183, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1265, "seek": 599164, "start": 6017.64, "end": 6020.64, "text": " So these are some common optimizers that are used.", "tokens": [51664, 407, 613, 366, 512, 2689, 5028, 22525, 300, 366, 1143, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06969280417906035, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0021154084242880344}, {"id": 1266, "seek": 602064, "start": 6020.64, "end": 6025.64, "text": " And this is just by going to torch.optim, short for optimizer.", "tokens": [50364, 400, 341, 307, 445, 538, 516, 281, 27822, 13, 5747, 332, 11, 2099, 337, 5028, 6545, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1267, "seek": 602064, "start": 6025.64, "end": 6029.64, "text": " And these are just a list of a bunch of optimizers that PyTorch provides.", "tokens": [50614, 400, 613, 366, 445, 257, 1329, 295, 257, 3840, 295, 5028, 22525, 300, 9953, 51, 284, 339, 6417, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1268, "seek": 602064, "start": 6029.64, "end": 6034.64, "text": " So what we're going to be using is something called AdamW.", "tokens": [50814, 407, 437, 321, 434, 516, 281, 312, 1228, 307, 746, 1219, 7938, 54, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1269, "seek": 602064, "start": 6034.64, "end": 6040.64, "text": " And what AdamW is, is it pretty much...", "tokens": [51064, 400, 437, 7938, 54, 307, 11, 307, 309, 1238, 709, 485, 51364], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1270, "seek": 602064, "start": 6040.64, "end": 6042.64, "text": " I'm just going to read off my little script here,", "tokens": [51364, 286, 478, 445, 516, 281, 1401, 766, 452, 707, 5755, 510, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1271, "seek": 602064, "start": 6042.64, "end": 6046.64, "text": " because I can't memorize every optimizer that exists.", "tokens": [51464, 570, 286, 393, 380, 27478, 633, 5028, 6545, 300, 8198, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08675906122947226, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.0059093330055475235}, {"id": 1272, "seek": 604664, "start": 6046.64, "end": 6052.64, "text": " So Adam, without Adam, just Adam, not AdamW,", "tokens": [50364, 407, 7938, 11, 1553, 7938, 11, 445, 7938, 11, 406, 7938, 54, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1273, "seek": 604664, "start": 6052.64, "end": 6057.64, "text": " Adam is a popular optimization algorithm that combines ideas of momentum.", "tokens": [50664, 7938, 307, 257, 3743, 19618, 9284, 300, 29520, 3487, 295, 11244, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1274, "seek": 604664, "start": 6057.64, "end": 6063.64, "text": " And it uses a moving average of both the gradient and its squared value", "tokens": [50914, 400, 309, 4960, 257, 2684, 4274, 295, 1293, 264, 16235, 293, 1080, 8889, 2158, 51214], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1275, "seek": 604664, "start": 6063.64, "end": 6066.64, "text": " to adapt the learning rate of each parameter.", "tokens": [51214, 281, 6231, 264, 2539, 3314, 295, 1184, 13075, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1276, "seek": 604664, "start": 6066.64, "end": 6070.64, "text": " And the learning rate is something that we should also go over.", "tokens": [51364, 400, 264, 2539, 3314, 307, 746, 300, 321, 820, 611, 352, 670, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1277, "seek": 604664, "start": 6070.64, "end": 6075.64, "text": " So let's say I figure out I need to move in this direction.", "tokens": [51564, 407, 718, 311, 584, 286, 2573, 484, 286, 643, 281, 1286, 294, 341, 3513, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08433613777160645, "compression_ratio": 1.6, "no_speech_prob": 0.002396540716290474}, {"id": 1278, "seek": 607564, "start": 6075.64, "end": 6077.64, "text": " I move, I take a step like that.", "tokens": [50364, 286, 1286, 11, 286, 747, 257, 1823, 411, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1279, "seek": 607564, "start": 6077.64, "end": 6080.64, "text": " Okay, that's a very big step that I say,", "tokens": [50464, 1033, 11, 300, 311, 257, 588, 955, 1823, 300, 286, 584, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1280, "seek": 607564, "start": 6080.64, "end": 6082.64, "text": " okay, we need to keep moving in that direction.", "tokens": [50614, 1392, 11, 321, 643, 281, 1066, 2684, 294, 300, 3513, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1281, "seek": 607564, "start": 6082.64, "end": 6086.64, "text": " So what happens is I go like this, and then I end up there.", "tokens": [50714, 407, 437, 2314, 307, 286, 352, 411, 341, 11, 293, 550, 286, 917, 493, 456, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1282, "seek": 607564, "start": 6086.64, "end": 6089.64, "text": " And it's like, whoa, we're going up now, what happened?", "tokens": [50914, 400, 309, 311, 411, 11, 13310, 11, 321, 434, 516, 493, 586, 11, 437, 2011, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1283, "seek": 607564, "start": 6089.64, "end": 6091.64, "text": " So that's because you have a very high learning rate.", "tokens": [51064, 407, 300, 311, 570, 291, 362, 257, 588, 1090, 2539, 3314, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1284, "seek": 607564, "start": 6091.64, "end": 6095.64, "text": " If you have a lower learning rate, what will happen is you'll start here.", "tokens": [51164, 759, 291, 362, 257, 3126, 2539, 3314, 11, 437, 486, 1051, 307, 291, 603, 722, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1285, "seek": 607564, "start": 6095.64, "end": 6098.64, "text": " It'll take little one-pixel steps or very, very small steps.", "tokens": [51364, 467, 603, 747, 707, 472, 12, 79, 34599, 4439, 420, 588, 11, 588, 1359, 4439, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1286, "seek": 607564, "start": 6098.64, "end": 6101.64, "text": " Okay, that's good. That's better. It's even better.", "tokens": [51514, 1033, 11, 300, 311, 665, 13, 663, 311, 1101, 13, 467, 311, 754, 1101, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1287, "seek": 607564, "start": 6101.64, "end": 6104.64, "text": " Keep going in this direction. This is great.", "tokens": [51664, 5527, 516, 294, 341, 3513, 13, 639, 307, 869, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10714601407385176, "compression_ratio": 1.8612099644128113, "no_speech_prob": 0.009123023599386215}, {"id": 1288, "seek": 610464, "start": 6104.64, "end": 6107.64, "text": " And you keep going down. You're like, okay, this is good.", "tokens": [50364, 400, 291, 1066, 516, 760, 13, 509, 434, 411, 11, 1392, 11, 341, 307, 665, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1289, "seek": 610464, "start": 6107.64, "end": 6110.64, "text": " We're descending. And it's starting to flatten out.", "tokens": [50514, 492, 434, 40182, 13, 400, 309, 311, 2891, 281, 24183, 484, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1290, "seek": 610464, "start": 6110.64, "end": 6113.64, "text": " So we know that we're hitting a local bottom here.", "tokens": [50664, 407, 321, 458, 300, 321, 434, 8850, 257, 2654, 2767, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1291, "seek": 610464, "start": 6113.64, "end": 6116.64, "text": " And then we stop because it starts ascending again.", "tokens": [50814, 400, 550, 321, 1590, 570, 309, 3719, 15526, 2029, 797, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1292, "seek": 610464, "start": 6116.64, "end": 6123.64, "text": " So that means this is our best set of parameters because of what that loss is", "tokens": [50964, 407, 300, 1355, 341, 307, 527, 1151, 992, 295, 9834, 570, 295, 437, 300, 4470, 307, 51314], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1293, "seek": 610464, "start": 6123.64, "end": 6128.64, "text": " or what the derivative is of that particular point.", "tokens": [51314, 420, 437, 264, 13760, 307, 295, 300, 1729, 935, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1294, "seek": 610464, "start": 6128.64, "end": 6132.64, "text": " So pretty much this is what the learning rate is.", "tokens": [51564, 407, 1238, 709, 341, 307, 437, 264, 2539, 3314, 307, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09454399805802566, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0025505132507532835}, {"id": 1295, "seek": 613264, "start": 6132.64, "end": 6137.64, "text": " So you want to have a small learning rate so that you don't take too large steps", "tokens": [50364, 407, 291, 528, 281, 362, 257, 1359, 2539, 3314, 370, 300, 291, 500, 380, 747, 886, 2416, 4439, 50614], "temperature": 0.0, "avg_logprob": -0.0888878340573655, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.009123981930315495}, {"id": 1296, "seek": 613264, "start": 6137.64, "end": 6141.64, "text": " so that the parameters don't change dramatically and end up messing you up.", "tokens": [50614, 370, 300, 264, 9834, 500, 380, 1319, 17548, 293, 917, 493, 23258, 291, 493, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0888878340573655, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.009123981930315495}, {"id": 1297, "seek": 613264, "start": 6141.64, "end": 6144.64, "text": " So you want to make them small enough so that you can still have efficient training.", "tokens": [50814, 407, 291, 528, 281, 652, 552, 1359, 1547, 370, 300, 291, 393, 920, 362, 7148, 3097, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0888878340573655, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.009123981930315495}, {"id": 1298, "seek": 613264, "start": 6144.64, "end": 6153.64, "text": " You don't want to be moving in a millionth of one or something.", "tokens": [50964, 509, 500, 380, 528, 281, 312, 2684, 294, 257, 2459, 392, 295, 472, 420, 746, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0888878340573655, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.009123981930315495}, {"id": 1299, "seek": 613264, "start": 6153.64, "end": 6158.64, "text": " That would be ridiculous. You'd have to do so many iterations to even get this far.", "tokens": [51414, 663, 576, 312, 11083, 13, 509, 1116, 362, 281, 360, 370, 867, 36540, 281, 754, 483, 341, 1400, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0888878340573655, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.009123981930315495}, {"id": 1300, "seek": 615864, "start": 6158.64, "end": 6165.64, "text": " So maybe you'd make it decently high but not too high that it'll go like that, right?", "tokens": [50364, 407, 1310, 291, 1116, 652, 309, 979, 2276, 1090, 457, 406, 886, 1090, 300, 309, 603, 352, 411, 300, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1818017526106401, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.039619021117687225}, {"id": 1301, "seek": 615864, "start": 6165.64, "end": 6170.64, "text": " So that's what the learning rate is, just how fast it learns pretty much.", "tokens": [50714, 407, 300, 311, 437, 264, 2539, 3314, 307, 11, 445, 577, 2370, 309, 27152, 1238, 709, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1818017526106401, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.039619021117687225}, {"id": 1302, "seek": 615864, "start": 6170.64, "end": 6179.64, "text": " And yeah, so AtomW is a modification of the Atom Optimizer.", "tokens": [50964, 400, 1338, 11, 370, 1711, 298, 54, 307, 257, 26747, 295, 264, 1711, 298, 35013, 6545, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1818017526106401, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.039619021117687225}, {"id": 1303, "seek": 615864, "start": 6179.64, "end": 6185.64, "text": " And it adds weight to K. So pretty much there's just some features that you add on to gradient descent", "tokens": [51414, 400, 309, 10860, 3364, 281, 591, 13, 407, 1238, 709, 456, 311, 445, 512, 4122, 300, 291, 909, 322, 281, 16235, 23475, 51714], "temperature": 0.0, "avg_logprob": -0.1818017526106401, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.039619021117687225}, {"id": 1304, "seek": 618564, "start": 6185.64, "end": 6189.64, "text": " and then AtomW is the same thing except that has weight to K.", "tokens": [50364, 293, 550, 1711, 298, 54, 307, 264, 912, 551, 3993, 300, 575, 3364, 281, 591, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1305, "seek": 618564, "start": 6189.64, "end": 6193.64, "text": " And what this pretty much means is it generalizes the parameters more.", "tokens": [50564, 400, 437, 341, 1238, 709, 1355, 307, 309, 2674, 5660, 264, 9834, 544, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1306, "seek": 618564, "start": 6193.64, "end": 6197.64, "text": " So instead of having very high level of performance or very low level,", "tokens": [50764, 407, 2602, 295, 1419, 588, 1090, 1496, 295, 3389, 420, 588, 2295, 1496, 11, 50964], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1307, "seek": 618564, "start": 6197.64, "end": 6200.64, "text": " it takes a little generalized in between.", "tokens": [50964, 309, 2516, 257, 707, 44498, 294, 1296, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1308, "seek": 618564, "start": 6200.64, "end": 6207.64, "text": " So the weight significance will actually shrink as it flans out.", "tokens": [51114, 407, 264, 3364, 17687, 486, 767, 23060, 382, 309, 932, 599, 484, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1309, "seek": 618564, "start": 6207.64, "end": 6212.64, "text": " So this will pretty much make sure that certain parameters in your network,", "tokens": [51464, 407, 341, 486, 1238, 709, 652, 988, 300, 1629, 9834, 294, 428, 3209, 11, 51714], "temperature": 0.0, "avg_logprob": -0.0748928439232611, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.07154957950115204}, {"id": 1310, "seek": 621264, "start": 6212.64, "end": 6219.64, "text": " certain parameters in your weight matrices aren't affecting the output of this model drastically.", "tokens": [50364, 1629, 9834, 294, 428, 3364, 32284, 3212, 380, 17476, 264, 5598, 295, 341, 2316, 29673, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06050243160941384, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.011327597312629223}, {"id": 1311, "seek": 621264, "start": 6219.64, "end": 6222.64, "text": " That could be in a positive or negative direction.", "tokens": [50714, 663, 727, 312, 294, 257, 3353, 420, 3671, 3513, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06050243160941384, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.011327597312629223}, {"id": 1312, "seek": 621264, "start": 6222.64, "end": 6227.64, "text": " You can have insanely high performance from some lucky parameters in your weight matrices.", "tokens": [50864, 509, 393, 362, 40965, 1090, 3389, 490, 512, 6356, 9834, 294, 428, 3364, 32284, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06050243160941384, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.011327597312629223}, {"id": 1313, "seek": 621264, "start": 6227.64, "end": 6233.64, "text": " So pretty much the point is to minimize those, to decay those values.", "tokens": [51114, 407, 1238, 709, 264, 935, 307, 281, 17522, 729, 11, 281, 21039, 729, 4190, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06050243160941384, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.011327597312629223}, {"id": 1314, "seek": 621264, "start": 6233.64, "end": 6240.64, "text": " That's what weight to K is, to prevent it from having that insane or super low performance.", "tokens": [51414, 663, 311, 437, 3364, 281, 591, 307, 11, 281, 4871, 309, 490, 1419, 300, 10838, 420, 1687, 2295, 3389, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06050243160941384, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.011327597312629223}, {"id": 1315, "seek": 624064, "start": 6240.64, "end": 6242.64, "text": " That's what weight to K is.", "tokens": [50364, 663, 311, 437, 3364, 281, 591, 307, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1575020521115034, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.012622656300663948}, {"id": 1316, "seek": 624064, "start": 6242.64, "end": 6247.64, "text": " So that's a little background on gradient descent and optimizers.", "tokens": [50464, 407, 300, 311, 257, 707, 3678, 322, 16235, 23475, 293, 5028, 22525, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1575020521115034, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.012622656300663948}, {"id": 1317, "seek": 624064, "start": 6247.64, "end": 6250.64, "text": " Let's go ahead and finish typing this out.", "tokens": [50714, 961, 311, 352, 2286, 293, 2413, 18444, 341, 484, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1575020521115034, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.012622656300663948}, {"id": 1318, "seek": 624064, "start": 6250.64, "end": 6257.64, "text": " So next up, we actually, we need to initialize some things.", "tokens": [50864, 407, 958, 493, 11, 321, 767, 11, 321, 643, 281, 5883, 1125, 512, 721, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1575020521115034, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.012622656300663948}, {"id": 1319, "seek": 624064, "start": 6257.64, "end": 6267.64, "text": " So we have our init self, of course, since it's a class, vocab size.", "tokens": [51214, 407, 321, 362, 527, 3157, 2698, 11, 295, 1164, 11, 1670, 309, 311, 257, 1508, 11, 2329, 455, 2744, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1575020521115034, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.012622656300663948}, {"id": 1320, "seek": 626764, "start": 6267.64, "end": 6272.64, "text": " I want to make sure that's correct, vocabulary size.", "tokens": [50364, 286, 528, 281, 652, 988, 300, 311, 3006, 11, 19864, 2744, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19448873202006023, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.040219925343990326}, {"id": 1321, "seek": 626764, "start": 6272.64, "end": 6287.64, "text": " I might actually shrink this just a vocab size because it sounds way easier to type out.", "tokens": [50614, 286, 1062, 767, 23060, 341, 445, 257, 2329, 455, 2744, 570, 309, 3263, 636, 3571, 281, 2010, 484, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19448873202006023, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.040219925343990326}, {"id": 1322, "seek": 626764, "start": 6287.64, "end": 6289.64, "text": " And vocab size, good.", "tokens": [51364, 400, 2329, 455, 2744, 11, 665, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19448873202006023, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.040219925343990326}, {"id": 1323, "seek": 626764, "start": 6289.64, "end": 6293.64, "text": " So we're going to pump out some R code here.", "tokens": [51464, 407, 321, 434, 516, 281, 5889, 484, 512, 497, 3089, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19448873202006023, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.040219925343990326}, {"id": 1324, "seek": 629364, "start": 6293.64, "end": 6297.64, "text": " And this is just assuming that you have some sort of a background in Python.", "tokens": [50364, 400, 341, 307, 445, 11926, 300, 291, 362, 512, 1333, 295, 257, 3678, 294, 15329, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05870329615581466, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.006487212143838406}, {"id": 1325, "seek": 629364, "start": 6297.64, "end": 6301.64, "text": " If not, it's all good.", "tokens": [50564, 759, 406, 11, 309, 311, 439, 665, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05870329615581466, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.006487212143838406}, {"id": 1326, "seek": 629364, "start": 6301.64, "end": 6304.64, "text": " Just understanding the premise of what's going on here.", "tokens": [50764, 1449, 3701, 264, 22045, 295, 437, 311, 516, 322, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05870329615581466, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.006487212143838406}, {"id": 1327, "seek": 629364, "start": 6304.64, "end": 6309.64, "text": " So we're going to make something called an embedding table.", "tokens": [50914, 407, 321, 434, 516, 281, 652, 746, 1219, 364, 12240, 3584, 3199, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05870329615581466, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.006487212143838406}, {"id": 1328, "seek": 629364, "start": 6309.64, "end": 6319.64, "text": " And I'm going to explain this to you in a second here, why the embedding table is really important.", "tokens": [51164, 400, 286, 478, 516, 281, 2903, 341, 281, 291, 294, 257, 1150, 510, 11, 983, 264, 12240, 3584, 3199, 307, 534, 1021, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05870329615581466, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.006487212143838406}, {"id": 1329, "seek": 631964, "start": 6319.64, "end": 6323.64, "text": " Notice that we use the nn.", "tokens": [50364, 13428, 300, 321, 764, 264, 297, 77, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1330, "seek": 631964, "start": 6323.64, "end": 6326.64, "text": " We use the nn module in this.", "tokens": [50564, 492, 764, 264, 297, 77, 10088, 294, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1331, "seek": 631964, "start": 6326.64, "end": 6330.64, "text": " So that means this is going to be a learnable parameter, the init.embedding.", "tokens": [50714, 407, 300, 1355, 341, 307, 516, 281, 312, 257, 1466, 712, 13075, 11, 264, 3157, 13, 443, 2883, 3584, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1332, "seek": 631964, "start": 6330.64, "end": 6336.64, "text": " So we're going to make this vocab size by vocab size.", "tokens": [50914, 407, 321, 434, 516, 281, 652, 341, 2329, 455, 2744, 538, 2329, 455, 2744, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1333, "seek": 631964, "start": 6336.64, "end": 6341.64, "text": " So let's say you have all eight characters here and you have all eight characters here.", "tokens": [51214, 407, 718, 311, 584, 291, 362, 439, 3180, 4342, 510, 293, 291, 362, 439, 3180, 4342, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1334, "seek": 631964, "start": 6341.64, "end": 6346.64, "text": " I'm going to actually show you what this looks like in a second here and why this is really important.", "tokens": [51464, 286, 478, 516, 281, 767, 855, 291, 437, 341, 1542, 411, 294, 257, 1150, 510, 293, 983, 341, 307, 534, 1021, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10635733154584777, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.009266591630876064}, {"id": 1335, "seek": 634664, "start": 6346.64, "end": 6351.64, "text": " So first off, we're going to finish typing out this background language model.", "tokens": [50364, 407, 700, 766, 11, 321, 434, 516, 281, 2413, 18444, 484, 341, 3678, 2856, 2316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11352889802720811, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.1383747160434723}, {"id": 1336, "seek": 634664, "start": 6351.64, "end": 6355.64, "text": " So we're going to define our forward pass here.", "tokens": [50614, 407, 321, 434, 516, 281, 6964, 527, 2128, 1320, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11352889802720811, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.1383747160434723}, {"id": 1337, "seek": 634664, "start": 6355.64, "end": 6362.64, "text": " So the reason why we type this forward pass out, instead of just using what it offers by default,", "tokens": [50814, 407, 264, 1778, 983, 321, 2010, 341, 2128, 1320, 484, 11, 2602, 295, 445, 1228, 437, 309, 7736, 538, 7576, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11352889802720811, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.1383747160434723}, {"id": 1338, "seek": 634664, "start": 6362.64, "end": 6372.64, "text": " is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task.", "tokens": [51164, 307, 281, 718, 311, 584, 321, 362, 257, 2685, 764, 1389, 337, 257, 2316, 293, 321, 434, 406, 445, 1228, 512, 10688, 830, 293, 321, 434, 406, 884, 257, 2199, 5633, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11352889802720811, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.1383747160434723}, {"id": 1339, "seek": 637264, "start": 6372.64, "end": 6378.64, "text": " This is a really good practice because we want to actually know what's going on behind the scenes in our model.", "tokens": [50364, 639, 307, 257, 534, 665, 3124, 570, 321, 528, 281, 767, 458, 437, 311, 516, 322, 2261, 264, 8026, 294, 527, 2316, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12112575107150608, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.32732081413269043}, {"id": 1340, "seek": 637264, "start": 6378.64, "end": 6380.64, "text": " We want to know exactly what's going on.", "tokens": [50664, 492, 528, 281, 458, 2293, 437, 311, 516, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12112575107150608, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.32732081413269043}, {"id": 1341, "seek": 637264, "start": 6380.64, "end": 6389.64, "text": " We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug.", "tokens": [50764, 492, 528, 281, 458, 437, 34852, 321, 434, 884, 11, 577, 321, 434, 26085, 309, 11, 293, 445, 257, 688, 295, 264, 2261, 264, 8026, 1589, 300, 311, 516, 281, 854, 505, 24083, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12112575107150608, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.32732081413269043}, {"id": 1342, "seek": 637264, "start": 6389.64, "end": 6397.64, "text": " So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch?", "tokens": [51214, 407, 286, 767, 2351, 341, 11, 264, 5081, 38, 47, 51, 1619, 11, 983, 307, 309, 1021, 281, 2464, 257, 2128, 1320, 2445, 294, 9953, 51, 284, 339, 490, 8459, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12112575107150608, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.32732081413269043}, {"id": 1343, "seek": 639764, "start": 6397.64, "end": 6402.64, "text": " Well, like I said, understanding the process, what are all the transformations that are actually going on,", "tokens": [50364, 1042, 11, 411, 286, 848, 11, 3701, 264, 1399, 11, 437, 366, 439, 264, 34852, 300, 366, 767, 516, 322, 11, 50614], "temperature": 0.0, "avg_logprob": -0.119082700638544, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.3036835193634033}, {"id": 1344, "seek": 639764, "start": 6402.64, "end": 6410.64, "text": " all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output?", "tokens": [50614, 439, 264, 9482, 300, 311, 516, 322, 294, 527, 2128, 1320, 11, 1242, 364, 4846, 11, 2614, 309, 807, 257, 3209, 11, 293, 1242, 364, 5598, 30, 51014], "temperature": 0.0, "avg_logprob": -0.119082700638544, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.3036835193634033}, {"id": 1345, "seek": 639764, "start": 6410.64, "end": 6423.64, "text": " Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices", "tokens": [51014, 2621, 12635, 11, 45592, 11, 411, 286, 848, 11, 45592, 307, 516, 281, 7988, 291, 294, 264, 1256, 498, 291, 500, 380, 1333, 295, 1524, 613, 1151, 7525, 51664], "temperature": 0.0, "avg_logprob": -0.119082700638544, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.3036835193634033}, {"id": 1346, "seek": 642364, "start": 6423.64, "end": 6432.64, "text": " If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that.", "tokens": [50364, 759, 291, 434, 1228, 3657, 1412, 293, 264, 7576, 1943, 380, 534, 1143, 281, 6260, 365, 309, 11, 291, 434, 516, 281, 483, 15120, 490, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09076233914023951, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.09128352254629135}, {"id": 1347, "seek": 642364, "start": 6432.64, "end": 6441.64, "text": " So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up.", "tokens": [50814, 407, 291, 528, 281, 652, 988, 300, 562, 291, 434, 767, 516, 807, 428, 3209, 11, 291, 434, 13175, 300, 1412, 8944, 293, 1184, 9887, 11, 309, 767, 3876, 493, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09076233914023951, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.09128352254629135}, {"id": 1348, "seek": 642364, "start": 6441.64, "end": 6448.64, "text": " So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here.", "tokens": [51264, 407, 291, 393, 611, 4482, 484, 412, 1184, 1823, 437, 311, 516, 322, 370, 291, 393, 536, 411, 11, 1954, 11, 341, 307, 406, 1596, 1364, 484, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09076233914023951, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.09128352254629135}, {"id": 1349, "seek": 644864, "start": 6448.64, "end": 6454.64, "text": " Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right?", "tokens": [50364, 2704, 321, 643, 281, 11, 291, 458, 11, 764, 257, 819, 2445, 13, 2704, 341, 1943, 380, 264, 1151, 472, 337, 264, 5633, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.10841160554152268, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.21452268958091736}, {"id": 1350, "seek": 644864, "start": 6454.64, "end": 6461.64, "text": " So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right?", "tokens": [50664, 407, 854, 291, 484, 365, 300, 11, 2318, 13, 400, 295, 1164, 11, 39387, 11, 498, 291, 434, 2390, 2375, 5245, 11, 2375, 7914, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.10841160554152268, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.21452268958091736}, {"id": 1351, "seek": 644864, "start": 6461.64, "end": 6468.64, "text": " And optimization, of course. So that's pretty much why we write out the forward pass from scratch.", "tokens": [51014, 400, 19618, 11, 295, 1164, 13, 407, 300, 311, 1238, 709, 983, 321, 2464, 484, 264, 2128, 1320, 490, 8459, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10841160554152268, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.21452268958091736}, {"id": 1352, "seek": 644864, "start": 6468.64, "end": 6475.64, "text": " It's also just best practice. So it's never really a good idea to not write this.", "tokens": [51364, 467, 311, 611, 445, 1151, 3124, 13, 407, 309, 311, 1128, 534, 257, 665, 1558, 281, 406, 2464, 341, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10841160554152268, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.21452268958091736}, {"id": 1353, "seek": 647564, "start": 6475.64, "end": 6483.64, "text": " But let's continue. So self, and it will do index and targets.", "tokens": [50364, 583, 718, 311, 2354, 13, 407, 2698, 11, 293, 309, 486, 360, 8186, 293, 12911, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1382599639892578, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.030205778777599335}, {"id": 1354, "seek": 647564, "start": 6483.64, "end": 6492.64, "text": " So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here.", "tokens": [50764, 407, 321, 434, 516, 281, 3012, 666, 257, 777, 1433, 510, 1219, 3565, 1208, 13, 583, 949, 321, 360, 300, 11, 293, 286, 478, 733, 295, 439, 670, 264, 1081, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1382599639892578, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.030205778777599335}, {"id": 1355, "seek": 647564, "start": 6492.64, "end": 6497.64, "text": " Before we do logits, I'm going to explain to you this embedding table here.", "tokens": [51214, 4546, 321, 360, 3565, 1208, 11, 286, 478, 516, 281, 2903, 281, 291, 341, 12240, 3584, 3199, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1382599639892578, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.030205778777599335}, {"id": 1356, "seek": 649764, "start": 6498.64, "end": 6503.64, "text": " Paste that in.", "tokens": [50414, 43827, 300, 294, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2306379477183024, "compression_ratio": 1.319327731092437, "no_speech_prob": 0.06276910752058029}, {"id": 1357, "seek": 649764, "start": 6509.64, "end": 6515.64, "text": " Return logits. You're going to see why we return logits in a second here.", "tokens": [50964, 24350, 3565, 1208, 13, 509, 434, 516, 281, 536, 983, 321, 2736, 3565, 1208, 294, 257, 1150, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2306379477183024, "compression_ratio": 1.319327731092437, "no_speech_prob": 0.06276910752058029}, {"id": 1358, "seek": 649764, "start": 6515.64, "end": 6521.64, "text": " So this an end on embedding here is pretty much just a lookup table.", "tokens": [51264, 407, 341, 364, 917, 322, 12240, 3584, 510, 307, 1238, 709, 445, 257, 574, 1010, 3199, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2306379477183024, "compression_ratio": 1.319327731092437, "no_speech_prob": 0.06276910752058029}, {"id": 1359, "seek": 652164, "start": 6521.64, "end": 6525.64, "text": " So what we're going to have, I'm actually going to pull up my notebook here.", "tokens": [50364, 407, 437, 321, 434, 516, 281, 362, 11, 286, 478, 767, 516, 281, 2235, 493, 452, 21060, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13023658375163655, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004754770081490278}, {"id": 1360, "seek": 652164, "start": 6525.64, "end": 6533.64, "text": " So we have a giant sort of grid of what the predictions are going to look like.", "tokens": [50564, 407, 321, 362, 257, 7410, 1333, 295, 10748, 295, 437, 264, 21264, 366, 516, 281, 574, 411, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13023658375163655, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004754770081490278}, {"id": 1361, "seek": 652164, "start": 6533.64, "end": 6537.64, "text": " It's going to look, can I drag it in here? No.", "tokens": [50964, 467, 311, 516, 281, 574, 11, 393, 286, 5286, 309, 294, 510, 30, 883, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13023658375163655, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004754770081490278}, {"id": 1362, "seek": 652164, "start": 6537.64, "end": 6543.64, "text": " So go ahead and download this full screen. Boom.", "tokens": [51164, 407, 352, 2286, 293, 5484, 341, 1577, 2568, 13, 15523, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13023658375163655, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004754770081490278}, {"id": 1363, "seek": 652164, "start": 6543.64, "end": 6546.64, "text": " This is my notion here, but pretty much this is what it looks like.", "tokens": [51464, 639, 307, 452, 10710, 510, 11, 457, 1238, 709, 341, 307, 437, 309, 1542, 411, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13023658375163655, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004754770081490278}, {"id": 1364, "seek": 654664, "start": 6546.64, "end": 6549.64, "text": " And I took this picture from Andrei Karpathy's lecture.", "tokens": [50364, 400, 286, 1890, 341, 3036, 490, 400, 10271, 591, 6529, 9527, 311, 7991, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15381101199558803, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.03619798272848129}, {"id": 1365, "seek": 654664, "start": 6549.64, "end": 6554.64, "text": " But what this is, is it has start tokens and end tokens.", "tokens": [50514, 583, 437, 341, 307, 11, 307, 309, 575, 722, 22667, 293, 917, 22667, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15381101199558803, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.03619798272848129}, {"id": 1366, "seek": 654664, "start": 6554.64, "end": 6558.64, "text": " So start is at the start of the block, and end tokens are at the end of the block.", "tokens": [50764, 407, 722, 307, 412, 264, 722, 295, 264, 3461, 11, 293, 917, 22667, 366, 412, 264, 917, 295, 264, 3461, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15381101199558803, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.03619798272848129}, {"id": 1367, "seek": 654664, "start": 6558.64, "end": 6569.64, "text": " And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character.", "tokens": [50964, 400, 309, 311, 1238, 709, 445, 32884, 11, 309, 311, 4099, 1333, 295, 257, 8482, 7316, 295, 437, 2517, 1487, 958, 2212, 472, 2517, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15381101199558803, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.03619798272848129}, {"id": 1368, "seek": 656964, "start": 6569.64, "end": 6585.64, "text": " So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.", "tokens": [50364, 407, 498, 321, 362, 11, 584, 11, 286, 500, 380, 458, 11, 364, 316, 11, 1386, 11, 21, 5254, 1413, 484, 295, 341, 2302, 7316, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13753963988504292, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.22242911159992218}, {"id": 1369, "seek": 656964, "start": 6585.64, "end": 6591.64, "text": " So if we just add up all these, if we normalize them, and we get a little probability of this happening,", "tokens": [51164, 407, 498, 321, 445, 909, 493, 439, 613, 11, 498, 321, 2710, 1125, 552, 11, 293, 321, 483, 257, 707, 8482, 295, 341, 2737, 11, 51464], "temperature": 0.0, "avg_logprob": -0.13753963988504292, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.22242911159992218}, {"id": 1370, "seek": 656964, "start": 6591.64, "end": 6595.64, "text": " I don't know, if we add up all these together, I don't know what that is.", "tokens": [51464, 286, 500, 380, 458, 11, 498, 321, 909, 493, 439, 613, 1214, 11, 286, 500, 380, 458, 437, 300, 307, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13753963988504292, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.22242911159992218}, {"id": 1371, "seek": 659564, "start": 6595.64, "end": 6599.64, "text": " It's some crazy number, maybe 20,000 or something, something crazy.", "tokens": [50364, 467, 311, 512, 3219, 1230, 11, 1310, 945, 11, 1360, 420, 746, 11, 746, 3219, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15735767826889502, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.16865205764770508}, {"id": 1372, "seek": 659564, "start": 6599.64, "end": 6607.64, "text": " Pretty much that percentage is the percentage of the end token coming after the character A.", "tokens": [50564, 10693, 709, 300, 9668, 307, 264, 9668, 295, 264, 917, 14862, 1348, 934, 264, 2517, 316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15735767826889502, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.16865205764770508}, {"id": 1373, "seek": 659564, "start": 6607.64, "end": 6615.64, "text": " And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind.", "tokens": [50964, 400, 550, 912, 551, 510, 11, 411, 498, 321, 360, 497, 11, 300, 311, 364, 497, 43, 420, 364, 30474, 11, 286, 500, 380, 458, 11, 286, 478, 6865, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15735767826889502, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.16865205764770508}, {"id": 1374, "seek": 659564, "start": 6615.64, "end": 6624.64, "text": " That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that.", "tokens": [51364, 663, 311, 364, 30474, 13, 583, 1238, 709, 321, 2710, 1125, 613, 11, 597, 1355, 11, 2710, 3319, 1355, 291, 747, 577, 4776, 307, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15735767826889502, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.16865205764770508}, {"id": 1375, "seek": 662464, "start": 6624.64, "end": 6630.64, "text": " To that entire row. So this one's pretty significant in proportion to the others.", "tokens": [50364, 1407, 300, 2302, 5386, 13, 407, 341, 472, 311, 1238, 4776, 294, 16068, 281, 264, 2357, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1376, "seek": 662464, "start": 6630.64, "end": 6633.64, "text": " So this one's going to be a fairly high probability of coming next.", "tokens": [50664, 407, 341, 472, 311, 516, 281, 312, 257, 6457, 1090, 8482, 295, 1348, 958, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1377, "seek": 662464, "start": 6633.64, "end": 6637.64, "text": " A lot of the times you're going to have an I coming after an R.", "tokens": [50814, 316, 688, 295, 264, 1413, 291, 434, 516, 281, 362, 364, 286, 1348, 934, 364, 497, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1378, "seek": 662464, "start": 6637.64, "end": 6640.64, "text": " And that's pretty much what that is. That's the embedding table.", "tokens": [51014, 400, 300, 311, 1238, 709, 437, 300, 307, 13, 663, 311, 264, 12240, 3584, 3199, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1379, "seek": 662464, "start": 6640.64, "end": 6643.64, "text": " So that's why we make it vocab size by vocab size.", "tokens": [51164, 407, 300, 311, 983, 321, 652, 309, 2329, 455, 2744, 538, 2329, 455, 2744, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1380, "seek": 662464, "start": 6643.64, "end": 6648.64, "text": " So that's a little background on what we're doing here.", "tokens": [51314, 407, 300, 311, 257, 707, 3678, 322, 437, 321, 434, 884, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1381, "seek": 662464, "start": 6648.64, "end": 6653.64, "text": " So let's continue with the term logits.", "tokens": [51564, 407, 718, 311, 2354, 365, 264, 1433, 3565, 1208, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06927714628331802, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.028426647186279297}, {"id": 1382, "seek": 665364, "start": 6653.64, "end": 6657.64, "text": " So what exactly are the logits? You're probably asking that.", "tokens": [50364, 407, 437, 2293, 366, 264, 3565, 1208, 30, 509, 434, 1391, 3365, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10661778083214393, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0017271711258217692}, {"id": 1383, "seek": 665364, "start": 6657.64, "end": 6664.64, "text": " So let's actually go back to a little notebook I had over here.", "tokens": [50564, 407, 718, 311, 767, 352, 646, 281, 257, 707, 21060, 286, 632, 670, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10661778083214393, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0017271711258217692}, {"id": 1384, "seek": 665364, "start": 6664.64, "end": 6670.64, "text": " So remember our softmax function, right? Our softmax right here.", "tokens": [50914, 407, 1604, 527, 2787, 41167, 2445, 11, 558, 30, 2621, 2787, 41167, 558, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10661778083214393, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0017271711258217692}, {"id": 1385, "seek": 665364, "start": 6670.64, "end": 6675.64, "text": " So we exponentiated each of these values and then we normalized them.", "tokens": [51214, 407, 321, 37871, 72, 770, 1184, 295, 613, 4190, 293, 550, 321, 48704, 552, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10661778083214393, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0017271711258217692}, {"id": 1386, "seek": 665364, "start": 6675.64, "end": 6681.64, "text": " Normalized. We took its contribution to the sum of everything. That's what normalizing is.", "tokens": [51464, 21277, 1602, 13, 492, 1890, 1080, 13150, 281, 264, 2408, 295, 1203, 13, 663, 311, 437, 2710, 3319, 307, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10661778083214393, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0017271711258217692}, {"id": 1387, "seek": 668164, "start": 6681.64, "end": 6688.64, "text": " So you can think of logits as just a bunch of floating point numbers that are normalized, right?", "tokens": [50364, 407, 291, 393, 519, 295, 3565, 1208, 382, 445, 257, 3840, 295, 12607, 935, 3547, 300, 366, 48704, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.16684647857165727, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.0025507560931146145}, {"id": 1388, "seek": 668164, "start": 6688.64, "end": 6693.64, "text": " So you have a total, I'll write this out.", "tokens": [50714, 407, 291, 362, 257, 3217, 11, 286, 603, 2464, 341, 484, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16684647857165727, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.0025507560931146145}, {"id": 1389, "seek": 668164, "start": 6693.64, "end": 6703.64, "text": " So let's say we have, that's a terrible line. Let's draw a new one.", "tokens": [50964, 407, 718, 311, 584, 321, 362, 11, 300, 311, 257, 6237, 1622, 13, 961, 311, 2642, 257, 777, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16684647857165727, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.0025507560931146145}, {"id": 1390, "seek": 670364, "start": 6704.64, "end": 6723.64, "text": " So let's say we have 2, 4, and 6. And we want to normalize these.", "tokens": [50414, 407, 718, 311, 584, 321, 362, 568, 11, 1017, 11, 293, 1386, 13, 400, 321, 528, 281, 2710, 1125, 613, 13, 51364], "temperature": 0.0, "avg_logprob": -0.183542215599204, "compression_ratio": 1.2735042735042734, "no_speech_prob": 0.02675093710422516}, {"id": 1391, "seek": 670364, "start": 6723.64, "end": 6729.64, "text": " So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12.", "tokens": [51364, 407, 747, 568, 484, 295, 264, 1993, 1124, 13, 708, 311, 264, 3217, 30, 492, 362, 1386, 1804, 1017, 307, 1266, 1804, 568, 307, 2272, 13, 51664], "temperature": 0.0, "avg_logprob": -0.183542215599204, "compression_ratio": 1.2735042735042734, "no_speech_prob": 0.02675093710422516}, {"id": 1392, "seek": 672964, "start": 6729.64, "end": 6735.64, "text": " So 2 divided by 12. We take the percentage of that.", "tokens": [50364, 407, 568, 6666, 538, 2272, 13, 492, 747, 264, 9668, 295, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14721848213509336, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.2477625608444214}, {"id": 1393, "seek": 672964, "start": 6735.64, "end": 6741.64, "text": " 2 out of 12 is 0.16 something, okay?", "tokens": [50664, 568, 484, 295, 2272, 307, 1958, 13, 6866, 746, 11, 1392, 30, 50964], "temperature": 0.0, "avg_logprob": -0.14721848213509336, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.2477625608444214}, {"id": 1394, "seek": 672964, "start": 6741.64, "end": 6747.64, "text": " So 0.16, we'll just do 1.167.", "tokens": [50964, 407, 1958, 13, 6866, 11, 321, 603, 445, 360, 502, 13, 6866, 22, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14721848213509336, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.2477625608444214}, {"id": 1395, "seek": 672964, "start": 6747.64, "end": 6751.64, "text": " And then 4 out of 12 would be double that.", "tokens": [51264, 400, 550, 1017, 484, 295, 2272, 576, 312, 3834, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14721848213509336, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.2477625608444214}, {"id": 1396, "seek": 672964, "start": 6751.64, "end": 6758.64, "text": " So 4 out of 12 would be 33, 33%.", "tokens": [51464, 407, 1017, 484, 295, 2272, 576, 312, 11816, 11, 11816, 6856, 51814], "temperature": 0.0, "avg_logprob": -0.14721848213509336, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.2477625608444214}, {"id": 1397, "seek": 675864, "start": 6758.64, "end": 6762.64, "text": " And then 6 out of 12, that's 50. So 0.5.", "tokens": [50364, 400, 550, 1386, 484, 295, 2272, 11, 300, 311, 2625, 13, 407, 1958, 13, 20, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13695554058961193, "compression_ratio": 1.6403940886699508, "no_speech_prob": 0.006588600110262632}, {"id": 1398, "seek": 675864, "start": 6762.64, "end": 6770.64, "text": " So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution.", "tokens": [50564, 407, 300, 311, 437, 613, 1542, 411, 48704, 13, 400, 341, 307, 1238, 709, 437, 264, 3565, 1208, 366, 11, 3993, 309, 311, 544, 295, 257, 8482, 7316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13695554058961193, "compression_ratio": 1.6403940886699508, "no_speech_prob": 0.006588600110262632}, {"id": 1399, "seek": 675864, "start": 6770.64, "end": 6784.64, "text": " So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d.", "tokens": [50964, 407, 718, 311, 584, 321, 362, 11, 291, 458, 11, 257, 3840, 295, 11, 257, 3840, 295, 955, 2356, 82, 510, 11, 411, 11, 286, 500, 380, 458, 11, 257, 6263, 538, 272, 293, 550, 257, 6263, 538, 269, 293, 550, 257, 6263, 538, 274, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13695554058961193, "compression_ratio": 1.6403940886699508, "no_speech_prob": 0.006588600110262632}, {"id": 1400, "seek": 678464, "start": 6784.64, "end": 6788.64, "text": " We know that from this distribution, a followed by d is most likely to come next.", "tokens": [50364, 492, 458, 300, 490, 341, 7316, 11, 257, 6263, 538, 274, 307, 881, 3700, 281, 808, 958, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08027471436394586, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.0038242482114583254}, {"id": 1401, "seek": 678464, "start": 6788.64, "end": 6795.64, "text": " So this is what the logits are. They're pretty much a probability distribution of what we want to predict.", "tokens": [50564, 407, 341, 307, 437, 264, 3565, 1208, 366, 13, 814, 434, 1238, 709, 257, 8482, 7316, 295, 437, 321, 528, 281, 6069, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08027471436394586, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.0038242482114583254}, {"id": 1402, "seek": 678464, "start": 6795.64, "end": 6801.64, "text": " So given that, let's hop back into here. We're going to mess around with these a little bit.", "tokens": [50914, 407, 2212, 300, 11, 718, 311, 3818, 646, 666, 510, 13, 492, 434, 516, 281, 2082, 926, 365, 613, 257, 707, 857, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08027471436394586, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.0038242482114583254}, {"id": 1403, "seek": 678464, "start": 6801.64, "end": 6807.64, "text": " So we have this embedding table, and I already showed you what that looked like.", "tokens": [51214, 407, 321, 362, 341, 12240, 3584, 3199, 11, 293, 286, 1217, 4712, 291, 437, 300, 2956, 411, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08027471436394586, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.0038242482114583254}, {"id": 1404, "seek": 678464, "start": 6807.64, "end": 6810.64, "text": " It looked like this right here. This is our embedding table.", "tokens": [51514, 467, 2956, 411, 341, 558, 510, 13, 639, 307, 527, 12240, 3584, 3199, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08027471436394586, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.0038242482114583254}, {"id": 1405, "seek": 681064, "start": 6810.64, "end": 6818.64, "text": " So let's use something called, we're going to use a function called dot view.", "tokens": [50364, 407, 718, 311, 764, 746, 1219, 11, 321, 434, 516, 281, 764, 257, 2445, 1219, 5893, 1910, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06899893517587699, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.020959163084626198}, {"id": 1406, "seek": 681064, "start": 6818.64, "end": 6823.64, "text": " So this is going to help us sort of reshape what our logits look like.", "tokens": [50764, 407, 341, 307, 516, 281, 854, 505, 1333, 295, 725, 42406, 437, 527, 3565, 1208, 574, 411, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06899893517587699, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.020959163084626198}, {"id": 1407, "seek": 681064, "start": 6823.64, "end": 6826.64, "text": " And I'm going to go over an example of what this looks like in a second here.", "tokens": [51014, 400, 286, 478, 516, 281, 352, 670, 364, 1365, 295, 437, 341, 1542, 411, 294, 257, 1150, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06899893517587699, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.020959163084626198}, {"id": 1408, "seek": 681064, "start": 6826.64, "end": 6831.64, "text": " I'm just going to pump out some code. So we have our batch by our time.", "tokens": [51164, 286, 478, 445, 516, 281, 5889, 484, 512, 3089, 13, 407, 321, 362, 527, 15245, 538, 527, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06899893517587699, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.020959163084626198}, {"id": 1409, "seek": 681064, "start": 6831.64, "end": 6836.64, "text": " So the time is, you can think of time as that sequence of integers.", "tokens": [51414, 407, 264, 565, 307, 11, 291, 393, 519, 295, 565, 382, 300, 8310, 295, 41674, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06899893517587699, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.020959163084626198}, {"id": 1410, "seek": 683664, "start": 6836.64, "end": 6839.64, "text": " That's the time dimension, right? You start from here.", "tokens": [50364, 663, 311, 264, 565, 10139, 11, 558, 30, 509, 722, 490, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1411, "seek": 683664, "start": 6839.64, "end": 6843.64, "text": " Maybe through the generating process, we don't know what's here next.", "tokens": [50514, 2704, 807, 264, 17746, 1399, 11, 321, 500, 380, 458, 437, 311, 510, 958, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1412, "seek": 683664, "start": 6843.64, "end": 6846.64, "text": " We don't know what's on the, we don't know what the next token is.", "tokens": [50714, 492, 500, 380, 458, 437, 311, 322, 264, 11, 321, 500, 380, 458, 437, 264, 958, 14862, 307, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1413, "seek": 683664, "start": 6846.64, "end": 6850.64, "text": " So that's why we say it's time because there's some we don't know yet and there's some that we already do know.", "tokens": [50864, 407, 300, 311, 983, 321, 584, 309, 311, 565, 570, 456, 311, 512, 321, 500, 380, 458, 1939, 293, 456, 311, 512, 300, 321, 1217, 360, 458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1414, "seek": 683664, "start": 6850.64, "end": 6852.64, "text": " That's what we call the time dimension.", "tokens": [51064, 663, 311, 437, 321, 818, 264, 565, 10139, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1415, "seek": 683664, "start": 6852.64, "end": 6858.64, "text": " And then channels would just be, how many different channels are, what's the vocabulary size?", "tokens": [51164, 400, 550, 9235, 576, 445, 312, 11, 577, 867, 819, 9235, 366, 11, 437, 311, 264, 19864, 2744, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1416, "seek": 683664, "start": 6858.64, "end": 6860.64, "text": " Channels is the vocabulary size.", "tokens": [51464, 761, 969, 1625, 307, 264, 19864, 2744, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1417, "seek": 683664, "start": 6860.64, "end": 6864.64, "text": " So we can make this the logits dot shape.", "tokens": [51564, 407, 321, 393, 652, 341, 264, 3565, 1208, 5893, 3909, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1133235113961356, "compression_ratio": 1.9320754716981132, "no_speech_prob": 0.15195773541927338}, {"id": 1418, "seek": 686464, "start": 6864.64, "end": 6867.64, "text": " This is what logits going to return here is B by T by C.", "tokens": [50364, 639, 307, 437, 3565, 1208, 516, 281, 2736, 510, 307, 363, 538, 314, 538, 383, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1988525390625, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.030669037252664566}, {"id": 1419, "seek": 686464, "start": 6867.64, "end": 6869.64, "text": " That's the shape of it.", "tokens": [50514, 663, 311, 264, 3909, 295, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1988525390625, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.030669037252664566}, {"id": 1420, "seek": 686464, "start": 6869.64, "end": 6878.64, "text": " And then our targets do, actually, no, we won't do that yet.", "tokens": [50614, 400, 550, 527, 12911, 360, 11, 767, 11, 572, 11, 321, 1582, 380, 360, 300, 1939, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1988525390625, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.030669037252664566}, {"id": 1421, "seek": 686464, "start": 6878.64, "end": 6886.64, "text": " We'll do do logits equals logits dot view.", "tokens": [51064, 492, 603, 360, 360, 3565, 1208, 6915, 3565, 1208, 5893, 1910, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1988525390625, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.030669037252664566}, {"id": 1422, "seek": 686464, "start": 6886.64, "end": 6892.64, "text": " And then we'll, this is very important, B by T.", "tokens": [51464, 400, 550, 321, 603, 11, 341, 307, 588, 1021, 11, 363, 538, 314, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1988525390625, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.030669037252664566}, {"id": 1423, "seek": 689264, "start": 6892.64, "end": 6902.64, "text": " So because we're particularly paying attention to the channels, the vocabulary, the batch and time,", "tokens": [50364, 407, 570, 321, 434, 4098, 6229, 3202, 281, 264, 9235, 11, 264, 19864, 11, 264, 15245, 293, 565, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08999390144870706, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00498107448220253}, {"id": 1424, "seek": 689264, "start": 6902.64, "end": 6905.64, "text": " they, I mean, they're not as important here.", "tokens": [50864, 436, 11, 286, 914, 11, 436, 434, 406, 382, 1021, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08999390144870706, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00498107448220253}, {"id": 1425, "seek": 689264, "start": 6905.64, "end": 6907.64, "text": " So we can sort of blend these together.", "tokens": [51014, 407, 321, 393, 1333, 295, 10628, 613, 1214, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08999390144870706, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00498107448220253}, {"id": 1426, "seek": 689264, "start": 6907.64, "end": 6915.64, "text": " And as long as the logits and the targets have the same batch and time, we should be all right.", "tokens": [51114, 400, 382, 938, 382, 264, 3565, 1208, 293, 264, 12911, 362, 264, 912, 15245, 293, 565, 11, 321, 820, 312, 439, 558, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08999390144870706, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00498107448220253}, {"id": 1427, "seek": 691564, "start": 6915.64, "end": 6921.64, "text": " So we're going to do B, B times T by C.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 363, 11, 363, 1413, 314, 538, 383, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11439391358257973, "compression_ratio": 1.5524475524475525, "no_speech_prob": 0.04083506762981415}, {"id": 1428, "seek": 691564, "start": 6921.64, "end": 6926.64, "text": " And then we can go to initialize our targets.", "tokens": [50664, 400, 550, 321, 393, 352, 281, 5883, 1125, 527, 12911, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11439391358257973, "compression_ratio": 1.5524475524475525, "no_speech_prob": 0.04083506762981415}, {"id": 1429, "seek": 691564, "start": 6926.64, "end": 6933.64, "text": " It's going to be targets dot view.", "tokens": [50914, 467, 311, 516, 281, 312, 12911, 5893, 1910, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11439391358257973, "compression_ratio": 1.5524475524475525, "no_speech_prob": 0.04083506762981415}, {"id": 1430, "seek": 691564, "start": 6933.64, "end": 6939.64, "text": " And it's going to be just a B by T.", "tokens": [51264, 400, 309, 311, 516, 281, 312, 445, 257, 363, 538, 314, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11439391358257973, "compression_ratio": 1.5524475524475525, "no_speech_prob": 0.04083506762981415}, {"id": 1431, "seek": 691564, "start": 6939.64, "end": 6943.64, "text": " And then we can make our loss, remember the loss function, right?", "tokens": [51564, 400, 550, 321, 393, 652, 527, 4470, 11, 1604, 264, 4470, 2445, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.11439391358257973, "compression_ratio": 1.5524475524475525, "no_speech_prob": 0.04083506762981415}, {"id": 1432, "seek": 694364, "start": 6943.64, "end": 6949.64, "text": " We do the functional of cross entropy, just a way of measuring the loss.", "tokens": [50364, 492, 360, 264, 11745, 295, 3278, 30867, 11, 445, 257, 636, 295, 13389, 264, 4470, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1433, "seek": 694364, "start": 6949.64, "end": 6952.64, "text": " And we basically take where there's two parameters here.", "tokens": [50664, 400, 321, 1936, 747, 689, 456, 311, 732, 9834, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1434, "seek": 694364, "start": 6952.64, "end": 6958.64, "text": " So we have the logits and the targets.", "tokens": [50814, 407, 321, 362, 264, 3565, 1208, 293, 264, 12911, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1435, "seek": 694364, "start": 6958.64, "end": 6962.64, "text": " So I'm going to go over exactly what's going on here in a second.", "tokens": [51114, 407, 286, 478, 516, 281, 352, 670, 2293, 437, 311, 516, 322, 510, 294, 257, 1150, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1436, "seek": 694364, "start": 6962.64, "end": 6965.64, "text": " But first, you might be asking, what does this view mean?", "tokens": [51314, 583, 700, 11, 291, 1062, 312, 3365, 11, 437, 775, 341, 1910, 914, 30, 51464], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1437, "seek": 694364, "start": 6965.64, "end": 6966.64, "text": " What exactly does this do?", "tokens": [51464, 708, 2293, 775, 341, 360, 30, 51514], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1438, "seek": 694364, "start": 6966.64, "end": 6968.64, "text": " So I'm going to show you that right now.", "tokens": [51514, 407, 286, 478, 516, 281, 855, 291, 300, 558, 586, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07498361096523776, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.006692198105156422}, {"id": 1439, "seek": 696864, "start": 6968.64, "end": 6974.64, "text": " There's some code here that initializes a random tensor of shape 2 by 3 by 5.", "tokens": [50364, 821, 311, 512, 3089, 510, 300, 5883, 5660, 257, 4974, 40863, 295, 3909, 568, 538, 805, 538, 1025, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12219225642192795, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.4606238901615143}, {"id": 1440, "seek": 696864, "start": 6974.64, "end": 6982.64, "text": " And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape.", "tokens": [50664, 400, 370, 437, 286, 360, 307, 286, 1238, 709, 26699, 729, 11, 286, 26699, 729, 12819, 538, 1228, 257, 5893, 3909, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12219225642192795, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.4606238901615143}, {"id": 1441, "seek": 696864, "start": 6982.64, "end": 6987.64, "text": " So shape takes the, you know, it takes the 2 by 3 by 5.", "tokens": [51064, 407, 3909, 2516, 264, 11, 291, 458, 11, 309, 2516, 264, 568, 538, 805, 538, 1025, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12219225642192795, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.4606238901615143}, {"id": 1442, "seek": 696864, "start": 6987.64, "end": 6991.64, "text": " We get x equals 2, y equals 3, and z equals 5.", "tokens": [51314, 492, 483, 2031, 6915, 568, 11, 288, 6915, 805, 11, 293, 710, 6915, 1025, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12219225642192795, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.4606238901615143}, {"id": 1443, "seek": 699164, "start": 6991.64, "end": 7002.64, "text": " So then we can do dot view, and that'll pretty much make that tensor again with those dimensions.", "tokens": [50364, 407, 550, 321, 393, 360, 5893, 1910, 11, 293, 300, 603, 1238, 709, 652, 300, 40863, 797, 365, 729, 12819, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1460095674563677, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.07368062436580658}, {"id": 1444, "seek": 699164, "start": 7002.64, "end": 7005.64, "text": " So then we can just print that out afterwards.", "tokens": [50914, 407, 550, 321, 393, 445, 4482, 300, 484, 10543, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1460095674563677, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.07368062436580658}, {"id": 1445, "seek": 699164, "start": 7005.64, "end": 7013.64, "text": " We go, we could print out, I don't know, print x, y, z.", "tokens": [51064, 492, 352, 11, 321, 727, 4482, 484, 11, 286, 500, 380, 458, 11, 4482, 2031, 11, 288, 11, 710, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1460095674563677, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.07368062436580658}, {"id": 1446, "seek": 699164, "start": 7013.64, "end": 7016.64, "text": " We have 2, 3, 5.", "tokens": [51464, 492, 362, 568, 11, 805, 11, 1025, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1460095674563677, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.07368062436580658}, {"id": 1447, "seek": 699164, "start": 7016.64, "end": 7020.64, "text": " Print, print a dot shape.", "tokens": [51614, 34439, 11, 4482, 257, 5893, 3909, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1460095674563677, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.07368062436580658}, {"id": 1448, "seek": 702064, "start": 7020.64, "end": 7028.64, "text": " And actually, I'll print out a dot shape right here first so you can see that this actually does line up.", "tokens": [50364, 400, 767, 11, 286, 603, 4482, 484, 257, 5893, 3909, 558, 510, 700, 370, 291, 393, 536, 300, 341, 767, 775, 1622, 493, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1449, "seek": 702064, "start": 7028.64, "end": 7031.64, "text": " A dot shape.", "tokens": [50764, 316, 5893, 3909, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1450, "seek": 702064, "start": 7031.64, "end": 7033.64, "text": " And then down here as well.", "tokens": [50914, 400, 550, 760, 510, 382, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1451, "seek": 702064, "start": 7033.64, "end": 7035.64, "text": " Same exact thing.", "tokens": [51014, 10635, 1900, 551, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1452, "seek": 702064, "start": 7035.64, "end": 7040.64, "text": " This also view does, basically allows us to unpack with the dot shape,", "tokens": [51114, 639, 611, 1910, 775, 11, 1936, 4045, 505, 281, 26699, 365, 264, 5893, 3909, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1453, "seek": 702064, "start": 7040.64, "end": 7044.64, "text": " and then we can use view to put them back together into a tensor.", "tokens": [51364, 293, 550, 321, 393, 764, 1910, 281, 829, 552, 646, 1214, 666, 257, 40863, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14718615290630294, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0010161797981709242}, {"id": 1454, "seek": 704464, "start": 7044.64, "end": 7050.64, "text": " So you might be asking, why in this notebook did we, did we have to reshape these?", "tokens": [50364, 407, 291, 1062, 312, 3365, 11, 983, 294, 341, 21060, 630, 321, 11, 630, 321, 362, 281, 725, 42406, 613, 30, 50664], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1455, "seek": 704464, "start": 7050.64, "end": 7052.64, "text": " Why do we do that?", "tokens": [50664, 1545, 360, 321, 360, 300, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1456, "seek": 704464, "start": 7052.64, "end": 7058.64, "text": " Well, the answer sort of falls into what the shape needs to be here with cross entropy.", "tokens": [50764, 1042, 11, 264, 1867, 1333, 295, 8804, 666, 437, 264, 3909, 2203, 281, 312, 510, 365, 3278, 30867, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1457, "seek": 704464, "start": 7058.64, "end": 7060.64, "text": " What does it expect?", "tokens": [51064, 708, 775, 309, 2066, 30, 51164], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1458, "seek": 704464, "start": 7060.64, "end": 7063.64, "text": " What does PyTorch expect the actual shape to be?", "tokens": [51164, 708, 775, 9953, 51, 284, 339, 2066, 264, 3539, 3909, 281, 312, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1459, "seek": 704464, "start": 7063.64, "end": 7071.64, "text": " So I looked at the documentation here, and it pretty much says that we want either one dimension,", "tokens": [51314, 407, 286, 2956, 412, 264, 14333, 510, 11, 293, 309, 1238, 709, 1619, 300, 321, 528, 2139, 472, 10139, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07870769500732422, "compression_ratio": 1.59375, "no_speech_prob": 0.2537412643432617}, {"id": 1460, "seek": 707164, "start": 7071.64, "end": 7078.64, "text": " which is channels, or 2, which is n, which I believe n is also the batch.", "tokens": [50364, 597, 307, 9235, 11, 420, 568, 11, 597, 307, 297, 11, 597, 286, 1697, 297, 307, 611, 264, 15245, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11289693213797905, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.008576038293540478}, {"id": 1461, "seek": 707164, "start": 7078.64, "end": 7082.64, "text": " So you have n, n different blocks or batches.", "tokens": [50714, 407, 291, 362, 297, 11, 297, 819, 8474, 420, 15245, 279, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11289693213797905, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.008576038293540478}, {"id": 1462, "seek": 707164, "start": 7082.64, "end": 7086.64, "text": " And then you have some other dimensions here.", "tokens": [50914, 400, 550, 291, 362, 512, 661, 12819, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11289693213797905, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.008576038293540478}, {"id": 1463, "seek": 707164, "start": 7086.64, "end": 7095.64, "text": " So pretty much what it's expecting is a b by c by t instead of a b by t by c,", "tokens": [51114, 407, 1238, 709, 437, 309, 311, 9650, 307, 257, 272, 538, 269, 538, 256, 2602, 295, 257, 272, 538, 256, 538, 269, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11289693213797905, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.008576038293540478}, {"id": 1464, "seek": 709564, "start": 7095.64, "end": 7099.64, "text": " which is precisely what we get out of here.", "tokens": [50364, 597, 307, 13402, 437, 321, 483, 484, 295, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1465, "seek": 709564, "start": 7099.64, "end": 7102.64, "text": " It's the logits dot shape is b by t by c.", "tokens": [50564, 467, 311, 264, 3565, 1208, 5893, 3909, 307, 272, 538, 256, 538, 269, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1466, "seek": 709564, "start": 7102.64, "end": 7105.64, "text": " We want it in a b by c by t.", "tokens": [50714, 492, 528, 309, 294, 257, 272, 538, 269, 538, 256, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1467, "seek": 709564, "start": 7105.64, "end": 7110.64, "text": " So pretty much what we're doing is we're just putting this into,", "tokens": [50864, 407, 1238, 709, 437, 321, 434, 884, 307, 321, 434, 445, 3372, 341, 666, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1468, "seek": 709564, "start": 7110.64, "end": 7113.64, "text": " we're just making this one parameter by multiplying those.", "tokens": [51114, 321, 434, 445, 1455, 341, 472, 13075, 538, 30955, 729, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1469, "seek": 709564, "start": 7113.64, "end": 7114.64, "text": " That's what's going on here.", "tokens": [51264, 663, 311, 437, 311, 516, 322, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1470, "seek": 709564, "start": 7114.64, "end": 7117.64, "text": " And then that means the second one is going to be c.", "tokens": [51314, 400, 550, 300, 1355, 264, 1150, 472, 307, 516, 281, 312, 269, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1471, "seek": 709564, "start": 7117.64, "end": 7123.64, "text": " So you get like a b times t equals n, and then c, just the way that it expects it, right?", "tokens": [51464, 407, 291, 483, 411, 257, 272, 1413, 256, 6915, 297, 11, 293, 550, 269, 11, 445, 264, 636, 300, 309, 33280, 309, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1472, "seek": 709564, "start": 7123.64, "end": 7124.64, "text": " Just like that.", "tokens": [51764, 1449, 411, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09898914453637508, "compression_ratio": 1.704, "no_speech_prob": 0.23637844622135162}, {"id": 1473, "seek": 712464, "start": 7124.64, "end": 7127.64, "text": " So that's pretty much what we're doing there.", "tokens": [50364, 407, 300, 311, 1238, 709, 437, 321, 434, 884, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1474, "seek": 712464, "start": 7127.64, "end": 7137.64, "text": " And a lot of the times you might get errors from passing it into a functional function in PyTorch.", "tokens": [50514, 400, 257, 688, 295, 264, 1413, 291, 1062, 483, 13603, 490, 8437, 309, 666, 257, 11745, 2445, 294, 9953, 51, 284, 339, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1475, "seek": 712464, "start": 7137.64, "end": 7142.64, "text": " So it's important to pay attention to how PyTorch expects the shapes to be,", "tokens": [51014, 407, 309, 311, 1021, 281, 1689, 3202, 281, 577, 9953, 51, 284, 339, 33280, 264, 10854, 281, 312, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1476, "seek": 712464, "start": 7142.64, "end": 7144.64, "text": " because you're going to get errors from that.", "tokens": [51264, 570, 291, 434, 516, 281, 483, 13603, 490, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1477, "seek": 712464, "start": 7144.64, "end": 7147.64, "text": " And I mean, it's not very hard to reshape them.", "tokens": [51364, 400, 286, 914, 11, 309, 311, 406, 588, 1152, 281, 725, 42406, 552, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1478, "seek": 712464, "start": 7147.64, "end": 7152.64, "text": " You just use the dot view and dot shape and you unpack them, reshape them together.", "tokens": [51514, 509, 445, 764, 264, 5893, 1910, 293, 5893, 3909, 293, 291, 26699, 552, 11, 725, 42406, 552, 1214, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06177852370522239, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.004198137205094099}, {"id": 1479, "seek": 715264, "start": 7152.64, "end": 7159.64, "text": " It's overall pretty simple for a beginner to intermediate level projects.", "tokens": [50364, 467, 311, 4787, 1238, 2199, 337, 257, 22080, 281, 19376, 1496, 4455, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1480, "seek": 715264, "start": 7159.64, "end": 7162.64, "text": " So it shouldn't really be a trouble there, but just watch out for that,", "tokens": [50714, 407, 309, 4659, 380, 534, 312, 257, 5253, 456, 11, 457, 445, 1159, 484, 337, 300, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1481, "seek": 715264, "start": 7162.64, "end": 7167.64, "text": " because it will come back and get you if you're not aware at some point.", "tokens": [50864, 570, 309, 486, 808, 646, 293, 483, 291, 498, 291, 434, 406, 3650, 412, 512, 935, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1482, "seek": 715264, "start": 7167.64, "end": 7170.64, "text": " So I've added a new function here called generate,", "tokens": [51114, 407, 286, 600, 3869, 257, 777, 2445, 510, 1219, 8460, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1483, "seek": 715264, "start": 7170.64, "end": 7173.64, "text": " and this is pretty much going to generate tokens for us.", "tokens": [51264, 293, 341, 307, 1238, 709, 516, 281, 8460, 22667, 337, 505, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1484, "seek": 715264, "start": 7173.64, "end": 7179.64, "text": " So we pass an index, which is the current index or the context,", "tokens": [51414, 407, 321, 1320, 364, 8186, 11, 597, 307, 264, 2190, 8186, 420, 264, 4319, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11360197356252959, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.00433115940541029}, {"id": 1485, "seek": 717964, "start": 7179.64, "end": 7183.64, "text": " and then we have max new tokens, and this is passed in through here.", "tokens": [50364, 293, 550, 321, 362, 11469, 777, 22667, 11, 293, 341, 307, 4678, 294, 807, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1486, "seek": 717964, "start": 7183.64, "end": 7191.64, "text": " So we have our context, we make it a single zero, just the next line character.", "tokens": [50564, 407, 321, 362, 527, 4319, 11, 321, 652, 309, 257, 2167, 4018, 11, 445, 264, 958, 1622, 2517, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1487, "seek": 717964, "start": 7191.64, "end": 7196.64, "text": " And then we generate based on that, and then our max new tokens, second parameter,", "tokens": [50964, 400, 550, 321, 8460, 2361, 322, 300, 11, 293, 550, 527, 11469, 777, 22667, 11, 1150, 13075, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1488, "seek": 717964, "start": 7196.64, "end": 7199.64, "text": " we just make it 500 second parameter.", "tokens": [51214, 321, 445, 652, 309, 5923, 1150, 13075, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1489, "seek": 717964, "start": 7199.64, "end": 7200.64, "text": " So cool.", "tokens": [51364, 407, 1627, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1490, "seek": 717964, "start": 7200.64, "end": 7202.64, "text": " What do we do inside of here?", "tokens": [51414, 708, 360, 321, 360, 1854, 295, 510, 30, 51514], "temperature": 0.0, "avg_logprob": -0.12745424758556279, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.003376177279278636}, {"id": 1491, "seek": 720264, "start": 7202.64, "end": 7215.64, "text": " We have a little loop that pretty much it generates based on the range of the max new tokens.", "tokens": [50364, 492, 362, 257, 707, 6367, 300, 1238, 709, 309, 23815, 2361, 322, 264, 3613, 295, 264, 11469, 777, 22667, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09569733693049504, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.012818417511880398}, {"id": 1492, "seek": 720264, "start": 7215.64, "end": 7221.64, "text": " So we're going to generate max new tokens, tokens, if that makes sense.", "tokens": [51014, 407, 321, 434, 516, 281, 8460, 11469, 777, 22667, 11, 22667, 11, 498, 300, 1669, 2020, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09569733693049504, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.012818417511880398}, {"id": 1493, "seek": 720264, "start": 7221.64, "end": 7228.64, "text": " Pretty much what we do is we call forward pass based on the current state of the model parameters.", "tokens": [51314, 10693, 709, 437, 321, 360, 307, 321, 818, 2128, 1320, 2361, 322, 264, 2190, 1785, 295, 264, 2316, 9834, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09569733693049504, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.012818417511880398}, {"id": 1494, "seek": 722864, "start": 7228.64, "end": 7233.64, "text": " And I want to be explicit here and say self dot forward, rather than just self index,", "tokens": [50364, 400, 286, 528, 281, 312, 13691, 510, 293, 584, 2698, 5893, 2128, 11, 2831, 813, 445, 2698, 8186, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1495, "seek": 722864, "start": 7233.64, "end": 7235.64, "text": " it will call self dot forward when we do this.", "tokens": [50614, 309, 486, 818, 2698, 5893, 2128, 562, 321, 360, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1496, "seek": 722864, "start": 7235.64, "end": 7239.64, "text": " But let's just be explicit and say self dot forward here.", "tokens": [50714, 583, 718, 311, 445, 312, 13691, 293, 584, 2698, 5893, 2128, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1497, "seek": 722864, "start": 7239.64, "end": 7242.64, "text": " So we get the logic and the loss from this.", "tokens": [50914, 407, 321, 483, 264, 9952, 293, 264, 4470, 490, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1498, "seek": 722864, "start": 7242.64, "end": 7244.64, "text": " We focus on the last time step.", "tokens": [51064, 492, 1879, 322, 264, 1036, 565, 1823, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1499, "seek": 722864, "start": 7244.64, "end": 7247.64, "text": " That's the only one we care about diagram language model.", "tokens": [51164, 663, 311, 264, 787, 472, 321, 1127, 466, 10686, 2856, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1500, "seek": 722864, "start": 7247.64, "end": 7252.64, "text": " We only care about the single previous character, only one doesn't have context before.", "tokens": [51314, 492, 787, 1127, 466, 264, 2167, 3894, 2517, 11, 787, 472, 1177, 380, 362, 4319, 949, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1501, "seek": 722864, "start": 7252.64, "end": 7257.64, "text": " And then we apply the softmax to get probability distribution.", "tokens": [51564, 400, 550, 321, 3079, 264, 2787, 41167, 281, 483, 8482, 7316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12432613057538497, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.10814983397722244}, {"id": 1502, "seek": 725764, "start": 7257.64, "end": 7260.64, "text": " And we already went over the softmax function before.", "tokens": [50364, 400, 321, 1217, 1437, 670, 264, 2787, 41167, 2445, 949, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10170300110526707, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.01115449145436287}, {"id": 1503, "seek": 725764, "start": 7260.64, "end": 7267.64, "text": " The reason why we use negative one here is because we're focusing on the last dimension.", "tokens": [50514, 440, 1778, 983, 321, 764, 3671, 472, 510, 307, 570, 321, 434, 8416, 322, 264, 1036, 10139, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10170300110526707, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.01115449145436287}, {"id": 1504, "seek": 725764, "start": 7267.64, "end": 7273.64, "text": " And in case you aren't familiar with negative indexing, which is what this is here and same with here,", "tokens": [50864, 400, 294, 1389, 291, 3212, 380, 4963, 365, 3671, 8186, 278, 11, 597, 307, 437, 341, 307, 510, 293, 912, 365, 510, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10170300110526707, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.01115449145436287}, {"id": 1505, "seek": 725764, "start": 7273.64, "end": 7276.64, "text": " is imagine you have a little number line.", "tokens": [51164, 307, 3811, 291, 362, 257, 707, 1230, 1622, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10170300110526707, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.01115449145436287}, {"id": 1506, "seek": 725764, "start": 7276.64, "end": 7281.64, "text": " It starts at index zero, one, two, three, four, five, et cetera.", "tokens": [51314, 467, 3719, 412, 8186, 4018, 11, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 1030, 11458, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10170300110526707, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.01115449145436287}, {"id": 1507, "seek": 728164, "start": 7281.64, "end": 7289.64, "text": " So if you go before zero, it's just going to loop to the very end of that array.", "tokens": [50364, 407, 498, 291, 352, 949, 4018, 11, 309, 311, 445, 516, 281, 6367, 281, 264, 588, 917, 295, 300, 10225, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1508, "seek": 728164, "start": 7289.64, "end": 7294.64, "text": " So when we call negative one, it's going to do the last element, negative two,", "tokens": [50764, 407, 562, 321, 818, 3671, 472, 11, 309, 311, 516, 281, 360, 264, 1036, 4478, 11, 3671, 732, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1509, "seek": 728164, "start": 7294.64, "end": 7297.64, "text": " second last element, negative three, third last element, et cetera.", "tokens": [51014, 1150, 1036, 4478, 11, 3671, 1045, 11, 2636, 1036, 4478, 11, 1030, 11458, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1510, "seek": 728164, "start": 7297.64, "end": 7299.64, "text": " So that's pretty much all this is here.", "tokens": [51164, 407, 300, 311, 1238, 709, 439, 341, 307, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1511, "seek": 728164, "start": 7299.64, "end": 7301.64, "text": " And you can do this for anything in Python.", "tokens": [51264, 400, 291, 393, 360, 341, 337, 1340, 294, 15329, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1512, "seek": 728164, "start": 7301.64, "end": 7304.64, "text": " Negative indexing is quite common.", "tokens": [51364, 43230, 8186, 278, 307, 1596, 2689, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1513, "seek": 728164, "start": 7304.64, "end": 7307.64, "text": " So that's what we do here.", "tokens": [51514, 407, 300, 311, 437, 321, 360, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07588344755626861, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0035933854524046183}, {"id": 1514, "seek": 730764, "start": 7307.64, "end": 7311.64, "text": " We apply softmax to the last dimension.", "tokens": [50364, 492, 3079, 2787, 41167, 281, 264, 1036, 10139, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13186573636704596, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.16221444308757782}, {"id": 1515, "seek": 730764, "start": 7311.64, "end": 7314.64, "text": " And then we sample from the distribution.", "tokens": [50564, 400, 550, 321, 6889, 490, 264, 7316, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13186573636704596, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.16221444308757782}, {"id": 1516, "seek": 730764, "start": 7314.64, "end": 7319.64, "text": " So we already went over torch dot monomial, we get one sample.", "tokens": [50714, 407, 321, 1217, 1437, 670, 27822, 5893, 1108, 47429, 11, 321, 483, 472, 6889, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13186573636704596, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.16221444308757782}, {"id": 1517, "seek": 730764, "start": 7319.64, "end": 7329.64, "text": " And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate.", "tokens": [50964, 400, 341, 307, 1238, 709, 264, 958, 8186, 420, 264, 958, 2058, 12340, 2517, 300, 321, 550, 764, 27822, 5893, 3857, 2099, 337, 1588, 7186, 473, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13186573636704596, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.16221444308757782}, {"id": 1518, "seek": 732964, "start": 7329.64, "end": 7337.64, "text": " It concatenates the previous context or the previous tokens with the newly generated one.", "tokens": [50364, 467, 1588, 7186, 1024, 264, 3894, 4319, 420, 264, 3894, 22667, 365, 264, 15109, 10833, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1519, "seek": 732964, "start": 7337.64, "end": 7339.64, "text": " And then we just combine them together.", "tokens": [50764, 400, 550, 321, 445, 10432, 552, 1214, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1520, "seek": 732964, "start": 7339.64, "end": 7341.64, "text": " So they're one thing.", "tokens": [50864, 407, 436, 434, 472, 551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1521, "seek": 732964, "start": 7341.64, "end": 7346.64, "text": " And we do this on a B by T plus one.", "tokens": [50964, 400, 321, 360, 341, 322, 257, 363, 538, 314, 1804, 472, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1522, "seek": 732964, "start": 7346.64, "end": 7349.64, "text": " And if that doesn't make sense, let me help you out here.", "tokens": [51214, 400, 498, 300, 1177, 380, 652, 2020, 11, 718, 385, 854, 291, 484, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1523, "seek": 732964, "start": 7349.64, "end": 7354.64, "text": " So we have this time dimension, let's say we have, you know, maybe just one element here.", "tokens": [51364, 407, 321, 362, 341, 565, 10139, 11, 718, 311, 584, 321, 362, 11, 291, 458, 11, 1310, 445, 472, 4478, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1524, "seek": 732964, "start": 7354.64, "end": 7357.64, "text": " So we have something in the zero position.", "tokens": [51614, 407, 321, 362, 746, 294, 264, 4018, 2535, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09564303443545387, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.28758737444877625}, {"id": 1525, "seek": 735764, "start": 7357.64, "end": 7363.64, "text": " And then whenever we generate a token, we're going to take the information from the zero position.", "tokens": [50364, 400, 550, 5699, 321, 8460, 257, 14862, 11, 321, 434, 516, 281, 747, 264, 1589, 490, 264, 4018, 2535, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1526, "seek": 735764, "start": 7363.64, "end": 7365.64, "text": " And then we're going to add one to it.", "tokens": [50664, 400, 550, 321, 434, 516, 281, 909, 472, 281, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1527, "seek": 735764, "start": 7365.64, "end": 7367.64, "text": " So it becomes a B by T.", "tokens": [50764, 407, 309, 3643, 257, 363, 538, 314, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1528, "seek": 735764, "start": 7367.64, "end": 7371.64, "text": " Since there was only one element, the length of that was one, it is now two.", "tokens": [50864, 4162, 456, 390, 787, 472, 4478, 11, 264, 4641, 295, 300, 390, 472, 11, 309, 307, 586, 732, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1529, "seek": 735764, "start": 7371.64, "end": 7374.64, "text": " Then we have this two, we make it three.", "tokens": [51064, 1396, 321, 362, 341, 732, 11, 321, 652, 309, 1045, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1530, "seek": 735764, "start": 7374.64, "end": 7377.64, "text": " And then we have this three, we make it four.", "tokens": [51214, 400, 550, 321, 362, 341, 1045, 11, 321, 652, 309, 1451, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1531, "seek": 735764, "start": 7377.64, "end": 7384.64, "text": " So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it.", "tokens": [51364, 407, 300, 311, 1238, 709, 437, 341, 884, 307, 445, 1066, 11, 445, 1066, 1588, 7186, 990, 544, 22667, 3911, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08832835349716059, "compression_ratio": 1.8034188034188035, "no_speech_prob": 0.002889329567551613}, {"id": 1532, "seek": 738464, "start": 7384.64, "end": 7388.64, "text": " And then we, you know, after this loop, we just return the index.", "tokens": [50364, 400, 550, 321, 11, 291, 458, 11, 934, 341, 6367, 11, 321, 445, 2736, 264, 8186, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1533, "seek": 738464, "start": 7388.64, "end": 7392.64, "text": " So this is all the generated tokens for max new tokens.", "tokens": [50564, 407, 341, 307, 439, 264, 10833, 22667, 337, 11469, 777, 22667, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1534, "seek": 738464, "start": 7392.64, "end": 7396.64, "text": " And that's pretty much what that does.", "tokens": [50764, 400, 300, 311, 1238, 709, 437, 300, 775, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1535, "seek": 738464, "start": 7396.64, "end": 7404.64, "text": " Model up to device here, this is just going to push our parameters to the GPU for more efficient training.", "tokens": [50964, 17105, 493, 281, 4302, 510, 11, 341, 307, 445, 516, 281, 2944, 527, 9834, 281, 264, 18407, 337, 544, 7148, 3097, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1536, "seek": 738464, "start": 7404.64, "end": 7409.64, "text": " I'm not sure if this makes a huge difference right now because we're only doing background language modeling.", "tokens": [51364, 286, 478, 406, 988, 498, 341, 1669, 257, 2603, 2649, 558, 586, 570, 321, 434, 787, 884, 3678, 2856, 15983, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1537, "seek": 738464, "start": 7409.64, "end": 7413.64, "text": " But yeah, it's handy to have this here.", "tokens": [51614, 583, 1338, 11, 309, 311, 13239, 281, 362, 341, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13463444979685657, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0030751305166631937}, {"id": 1538, "seek": 741364, "start": 7413.64, "end": 7417.64, "text": " And then, I mean, this is, this is pretty self explanatory here.", "tokens": [50364, 400, 550, 11, 286, 914, 11, 341, 307, 11, 341, 307, 1238, 2698, 9045, 4745, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1539, "seek": 741364, "start": 7417.64, "end": 7420.64, "text": " We generate based on a context.", "tokens": [50564, 492, 8460, 2361, 322, 257, 4319, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1540, "seek": 741364, "start": 7420.64, "end": 7424.64, "text": " This is the context, which is a single zero or a next line character.", "tokens": [50714, 639, 307, 264, 4319, 11, 597, 307, 257, 2167, 4018, 420, 257, 958, 1622, 2517, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1541, "seek": 741364, "start": 7424.64, "end": 7427.64, "text": " We pass in our max new tokens.", "tokens": [50914, 492, 1320, 294, 527, 11469, 777, 22667, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1542, "seek": 741364, "start": 7427.64, "end": 7429.64, "text": " And then we pretty much just decode this.", "tokens": [51064, 400, 550, 321, 1238, 709, 445, 979, 1429, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1543, "seek": 741364, "start": 7429.64, "end": 7432.64, "text": " So that's how that works.", "tokens": [51164, 407, 300, 311, 577, 300, 1985, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1544, "seek": 741364, "start": 7432.64, "end": 7438.64, "text": " Let's move on to the optimizer and the training loop, the actual training process.", "tokens": [51314, 961, 311, 1286, 322, 281, 264, 5028, 6545, 293, 264, 3097, 6367, 11, 264, 3539, 3097, 1399, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1545, "seek": 741364, "start": 7438.64, "end": 7442.64, "text": " So I actually skipped something and probably left you a little bit confused.", "tokens": [51614, 407, 286, 767, 30193, 746, 293, 1391, 1411, 291, 257, 707, 857, 9019, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10550363789434018, "compression_ratio": 1.640926640926641, "no_speech_prob": 0.0015010316856205463}, {"id": 1546, "seek": 744264, "start": 7442.64, "end": 7452.64, "text": " But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here?", "tokens": [50364, 583, 291, 1062, 312, 3365, 11, 577, 264, 12872, 630, 321, 767, 2105, 264, 1150, 484, 295, 484, 295, 1045, 12819, 490, 341, 3565, 1208, 510, 30, 50864], "temperature": 0.0, "avg_logprob": -0.13659391095561366, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.01032645720988512}, {"id": 1547, "seek": 744264, "start": 7452.64, "end": 7458.64, "text": " Because the logits only returns two dimensions, right?", "tokens": [50864, 1436, 264, 3565, 1208, 787, 11247, 732, 12819, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.13659391095561366, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.01032645720988512}, {"id": 1548, "seek": 744264, "start": 7458.64, "end": 7463.64, "text": " You have a B by T, or you have a B times T by C.", "tokens": [51164, 509, 362, 257, 363, 538, 314, 11, 420, 291, 362, 257, 363, 1413, 314, 538, 383, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13659391095561366, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.01032645720988512}, {"id": 1549, "seek": 744264, "start": 7463.64, "end": 7466.64, "text": " So how exactly does this work?", "tokens": [51414, 407, 577, 2293, 775, 341, 589, 30, 51564], "temperature": 0.0, "avg_logprob": -0.13659391095561366, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.01032645720988512}, {"id": 1550, "seek": 744264, "start": 7466.64, "end": 7471.64, "text": " Well, when we call this forward pass, all we're passing in is the index here.", "tokens": [51564, 1042, 11, 562, 321, 818, 341, 2128, 1320, 11, 439, 321, 434, 8437, 294, 307, 264, 8186, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13659391095561366, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.01032645720988512}, {"id": 1551, "seek": 747164, "start": 7471.64, "end": 7475.64, "text": " So that means targets defaults to none.", "tokens": [50364, 407, 300, 1355, 12911, 7576, 82, 281, 6022, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07747113186380138, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.004754430148750544}, {"id": 1552, "seek": 747164, "start": 7475.64, "end": 7481.64, "text": " So because targets is none, the loss is none, and this code does not execute.", "tokens": [50564, 407, 570, 12911, 307, 6022, 11, 264, 4470, 307, 6022, 11, 293, 341, 3089, 775, 406, 14483, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07747113186380138, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.004754430148750544}, {"id": 1553, "seek": 747164, "start": 7481.64, "end": 7486.64, "text": " And it just uses this logits here, which is three dimensional.", "tokens": [50864, 400, 309, 445, 4960, 341, 3565, 1208, 510, 11, 597, 307, 1045, 18795, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07747113186380138, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.004754430148750544}, {"id": 1554, "seek": 747164, "start": 7486.64, "end": 7488.64, "text": " So that's how that works.", "tokens": [51114, 407, 300, 311, 577, 300, 1985, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07747113186380138, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.004754430148750544}, {"id": 1555, "seek": 747164, "start": 7488.64, "end": 7500.64, "text": " And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there.", "tokens": [51214, 400, 6095, 11, 498, 291, 11, 498, 291, 434, 12919, 294, 428, 15743, 293, 428, 12911, 281, 264, 2316, 11, 550, 291, 434, 2745, 516, 281, 362, 428, 12911, 294, 456, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07747113186380138, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.004754430148750544}, {"id": 1556, "seek": 750064, "start": 7500.64, "end": 7504.64, "text": " And that will make sure targets is not none.", "tokens": [50364, 400, 300, 486, 652, 988, 12911, 307, 406, 6022, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08071216871572096, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.00028684744029305875}, {"id": 1557, "seek": 750064, "start": 7504.64, "end": 7511.64, "text": " So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits.", "tokens": [50564, 407, 550, 291, 603, 767, 312, 32368, 341, 3089, 293, 291, 603, 362, 257, 732, 18795, 3565, 1208, 2831, 813, 257, 1045, 18795, 3565, 1208, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08071216871572096, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.00028684744029305875}, {"id": 1558, "seek": 750064, "start": 7511.64, "end": 7516.64, "text": " So that's just a little clarification there, if that was confusing to anybody.", "tokens": [50914, 407, 300, 311, 445, 257, 707, 34449, 456, 11, 498, 300, 390, 13181, 281, 4472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08071216871572096, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.00028684744029305875}, {"id": 1559, "seek": 750064, "start": 7516.64, "end": 7524.64, "text": " Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type.", "tokens": [51164, 3996, 1702, 551, 286, 528, 281, 2060, 949, 321, 3012, 666, 341, 3097, 6367, 307, 341, 707, 3930, 82, 5893, 938, 1412, 2010, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08071216871572096, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.00028684744029305875}, {"id": 1560, "seek": 752464, "start": 7524.64, "end": 7535.64, "text": " So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes.", "tokens": [50364, 407, 3930, 82, 5893, 938, 307, 264, 10344, 295, 560, 12145, 420, 24922, 12145, 11, 597, 8073, 530, 12145, 9239, 11, 420, 3180, 36088, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11618552063450668, "compression_ratio": 1.64, "no_speech_prob": 0.013018970377743244}, {"id": 1561, "seek": 752464, "start": 7535.64, "end": 7547.64, "text": " So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference", "tokens": [50914, 407, 291, 393, 362, 819, 1412, 3467, 11, 291, 393, 362, 257, 15706, 3165, 11, 291, 393, 362, 257, 15706, 8858, 15706, 12145, 11, 286, 1697, 291, 393, 362, 364, 560, 12145, 294, 8858, 2649, 51514], "temperature": 0.0, "avg_logprob": -0.11618552063450668, "compression_ratio": 1.64, "no_speech_prob": 0.013018970377743244}, {"id": 1562, "seek": 754764, "start": 7547.64, "end": 7551.64, "text": " between float and int is float has decimals, it's a floating point number.", "tokens": [50364, 1296, 15706, 293, 560, 307, 15706, 575, 979, 332, 1124, 11, 309, 311, 257, 12607, 935, 1230, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1563, "seek": 754764, "start": 7551.64, "end": 7556.64, "text": " And then integers just, just a single integer doesn't, it's not really anything more than that.", "tokens": [50564, 400, 550, 41674, 445, 11, 445, 257, 2167, 24922, 1177, 380, 11, 309, 311, 406, 534, 1340, 544, 813, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1564, "seek": 754764, "start": 7556.64, "end": 7561.64, "text": " It can just be bigger based on the amount of bits that occupies.", "tokens": [50814, 467, 393, 445, 312, 3801, 2361, 322, 264, 2372, 295, 9239, 300, 8073, 530, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1565, "seek": 754764, "start": 7561.64, "end": 7565.64, "text": " So that's just a overview on tors dot long.", "tokens": [51064, 407, 300, 311, 445, 257, 12492, 322, 3930, 82, 5893, 938, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1566, "seek": 754764, "start": 7565.64, "end": 7568.64, "text": " It's the exact same thing as in 64.", "tokens": [51264, 467, 311, 264, 1900, 912, 551, 382, 294, 12145, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1567, "seek": 754764, "start": 7568.64, "end": 7571.64, "text": " So that's that.", "tokens": [51414, 407, 300, 311, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1568, "seek": 754764, "start": 7571.64, "end": 7574.64, "text": " Now we have this, we have this training loop here.", "tokens": [51564, 823, 321, 362, 341, 11, 321, 362, 341, 3097, 6367, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1302763037725326, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.3344748616218567}, {"id": 1569, "seek": 757464, "start": 7574.64, "end": 7578.64, "text": " So we define our optimizer.", "tokens": [50364, 407, 321, 6964, 527, 5028, 6545, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1570, "seek": 757464, "start": 7578.64, "end": 7586.64, "text": " And I already meant over optimizers previously, Adam W, which is Adam weight decay.", "tokens": [50564, 400, 286, 1217, 4140, 670, 5028, 22525, 8046, 11, 7938, 343, 11, 597, 307, 7938, 3364, 21039, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1571, "seek": 757464, "start": 7586.64, "end": 7588.64, "text": " So we have weight decay in here.", "tokens": [50964, 407, 321, 362, 3364, 21039, 294, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1572, "seek": 757464, "start": 7588.64, "end": 7593.64, "text": " And then all of our model parameters, and then our learning rates.", "tokens": [51064, 400, 550, 439, 295, 527, 2316, 9834, 11, 293, 550, 527, 2539, 6846, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1573, "seek": 757464, "start": 7593.64, "end": 7596.64, "text": " So I actually wrote to learning rate up here.", "tokens": [51314, 407, 286, 767, 4114, 281, 2539, 3314, 493, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1574, "seek": 757464, "start": 7596.64, "end": 7601.64, "text": " So I would add this and then just rerun this part of the code here if you're typing along.", "tokens": [51464, 407, 286, 576, 909, 341, 293, 550, 445, 43819, 409, 341, 644, 295, 264, 3089, 510, 498, 291, 434, 18444, 2051, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12524423291606288, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.015186134725809097}, {"id": 1575, "seek": 760164, "start": 7601.64, "end": 7608.64, "text": " So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop.", "tokens": [50364, 407, 286, 362, 341, 2539, 6846, 11, 382, 731, 382, 11469, 309, 1559, 11, 597, 307, 577, 867, 36540, 321, 434, 516, 281, 362, 294, 341, 3097, 6367, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11490882794881604, "compression_ratio": 1.777292576419214, "no_speech_prob": 0.004467921797186136}, {"id": 1576, "seek": 760164, "start": 7608.64, "end": 7615.64, "text": " And the learning rate is special, because sometimes you're learning what will be too high.", "tokens": [50714, 400, 264, 2539, 3314, 307, 2121, 11, 570, 2171, 291, 434, 2539, 437, 486, 312, 886, 1090, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11490882794881604, "compression_ratio": 1.777292576419214, "no_speech_prob": 0.004467921797186136}, {"id": 1577, "seek": 760164, "start": 7615.64, "end": 7618.64, "text": " And some said, sometimes it'll be too low.", "tokens": [51064, 400, 512, 848, 11, 2171, 309, 603, 312, 886, 2295, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11490882794881604, "compression_ratio": 1.777292576419214, "no_speech_prob": 0.004467921797186136}, {"id": 1578, "seek": 760164, "start": 7618.64, "end": 7629.64, "text": " So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time.", "tokens": [51214, 407, 257, 688, 295, 264, 1413, 291, 603, 362, 281, 5120, 365, 428, 2539, 3314, 293, 536, 597, 472, 6417, 264, 1151, 1293, 3389, 293, 3125, 670, 565, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11490882794881604, "compression_ratio": 1.777292576419214, "no_speech_prob": 0.004467921797186136}, {"id": 1579, "seek": 762964, "start": 7629.64, "end": 7635.64, "text": " So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip.", "tokens": [50364, 407, 365, 512, 2539, 6846, 11, 291, 603, 483, 534, 1702, 7295, 1117, 293, 550, 309, 603, 411, 15488, 24467, 300, 707, 10460, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09121905316363324, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.026754379272460938}, {"id": 1580, "seek": 762964, "start": 7635.64, "end": 7642.64, "text": " So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly.", "tokens": [50664, 407, 291, 528, 281, 652, 988, 300, 1177, 380, 1051, 11, 457, 291, 611, 528, 281, 652, 988, 264, 3097, 1399, 1709, 2661, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09121905316363324, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.026754379272460938}, {"id": 1581, "seek": 762964, "start": 7642.64, "end": 7652.64, "text": " You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that.", "tokens": [51014, 509, 500, 380, 528, 281, 312, 3806, 411, 11, 291, 458, 11, 364, 2302, 1618, 337, 257, 3678, 2856, 2316, 281, 3847, 538, 1419, 11, 291, 458, 11, 538, 1419, 257, 1230, 411, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09121905316363324, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.026754379272460938}, {"id": 1582, "seek": 765264, "start": 7653.64, "end": 7662.64, "text": " So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs.", "tokens": [50414, 407, 300, 311, 257, 707, 12492, 322, 411, 11, 1936, 11, 321, 434, 445, 3372, 341, 341, 2539, 3314, 294, 510, 11, 300, 311, 689, 309, 12953, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0986426385600915, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.05919484794139862}, {"id": 1583, "seek": 765264, "start": 7662.64, "end": 7667.64, "text": " So now we have this training loop here, which is going to iterate over the max iterations.", "tokens": [50864, 407, 586, 321, 362, 341, 3097, 6367, 510, 11, 597, 307, 516, 281, 44497, 670, 264, 11469, 36540, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0986426385600915, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.05919484794139862}, {"id": 1584, "seek": 765264, "start": 7667.64, "end": 7670.64, "text": " We just give each iteration the term iter.", "tokens": [51114, 492, 445, 976, 1184, 24784, 264, 1433, 17138, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0986426385600915, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.05919484794139862}, {"id": 1585, "seek": 765264, "start": 7670.64, "end": 7677.64, "text": " And I don't think we use this yet, but we will later for just reporting on the loss over time.", "tokens": [51264, 400, 286, 500, 380, 519, 321, 764, 341, 1939, 11, 457, 321, 486, 1780, 337, 445, 10031, 322, 264, 4470, 670, 565, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0986426385600915, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.05919484794139862}, {"id": 1586, "seek": 767764, "start": 7678.64, "end": 7686.64, "text": " But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training.", "tokens": [50414, 583, 437, 321, 360, 307, 321, 483, 11, 321, 483, 257, 15245, 365, 264, 3847, 7472, 4682, 11, 321, 434, 445, 11, 797, 11, 321, 434, 445, 321, 434, 445, 3097, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12168520375301964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.008060944266617298}, {"id": 1587, "seek": 767764, "start": 7686.64, "end": 7688.64, "text": " This is the training loop, we don't care about validation.", "tokens": [50814, 639, 307, 264, 3097, 6367, 11, 321, 500, 380, 1127, 466, 24071, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12168520375301964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.008060944266617298}, {"id": 1588, "seek": 767764, "start": 7688.64, "end": 7696.64, "text": " So we're going to call train on this, we're going to get some x inputs and some y targets.", "tokens": [50914, 407, 321, 434, 516, 281, 818, 3847, 322, 341, 11, 321, 434, 516, 281, 483, 512, 2031, 15743, 293, 512, 288, 12911, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12168520375301964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.008060944266617298}, {"id": 1589, "seek": 769664, "start": 7696.64, "end": 7704.64, "text": " So we go in and do a model dot forward here, we got our logits and our loss.", "tokens": [50364, 407, 321, 352, 294, 293, 360, 257, 2316, 5893, 2128, 510, 11, 321, 658, 527, 3565, 1208, 293, 527, 4470, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15231379403008355, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.14998845756053925}, {"id": 1590, "seek": 769664, "start": 7704.64, "end": 7710.64, "text": " And then we're going to do our optimizer dot zero grad and I'll explain this in the second here.", "tokens": [50764, 400, 550, 321, 434, 516, 281, 360, 527, 5028, 6545, 5893, 4018, 2771, 293, 286, 603, 2903, 341, 294, 264, 1150, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15231379403008355, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.14998845756053925}, {"id": 1591, "seek": 769664, "start": 7710.64, "end": 7712.64, "text": " It's a little bit confusing.", "tokens": [51064, 467, 311, 257, 707, 857, 13181, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15231379403008355, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.14998845756053925}, {"id": 1592, "seek": 769664, "start": 7712.64, "end": 7722.64, "text": " But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops.", "tokens": [51164, 583, 797, 11, 321, 1562, 321, 362, 527, 4470, 5893, 23897, 293, 341, 294, 3331, 1177, 380, 1626, 4963, 294, 1389, 291, 366, 406, 4963, 365, 3097, 16121, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15231379403008355, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.14998845756053925}, {"id": 1593, "seek": 772264, "start": 7722.64, "end": 7724.64, "text": " I know I can go by this a little bit quickly.", "tokens": [50364, 286, 458, 286, 393, 352, 538, 341, 257, 707, 857, 2661, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1594, "seek": 772264, "start": 7724.64, "end": 7730.64, "text": " But this is the standard training loop architecture for basic models.", "tokens": [50464, 583, 341, 307, 264, 3832, 3097, 6367, 9482, 337, 3875, 5245, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1595, "seek": 772264, "start": 7730.64, "end": 7733.64, "text": " And this is what it'll usually look like.", "tokens": [50764, 400, 341, 307, 437, 309, 603, 2673, 574, 411, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1596, "seek": 772264, "start": 7733.64, "end": 7738.64, "text": " So you'll, you know, you'll get your data, get your inputs or outputs, whatever.", "tokens": [50914, 407, 291, 603, 11, 291, 458, 11, 291, 603, 483, 428, 1412, 11, 483, 428, 15743, 420, 23930, 11, 2035, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1597, "seek": 772264, "start": 7738.64, "end": 7740.64, "text": " You'll do a forward pass.", "tokens": [51164, 509, 603, 360, 257, 2128, 1320, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1598, "seek": 772264, "start": 7740.64, "end": 7743.64, "text": " You'll define some thing about the optimizer here.", "tokens": [51264, 509, 603, 6964, 512, 551, 466, 264, 5028, 6545, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1599, "seek": 772264, "start": 7743.64, "end": 7744.64, "text": " In our case, it's your grad.", "tokens": [51414, 682, 527, 1389, 11, 309, 311, 428, 2771, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1600, "seek": 772264, "start": 7744.64, "end": 7750.64, "text": " And then you'll have a loss dot backward, which is backward pass.", "tokens": [51464, 400, 550, 291, 603, 362, 257, 4470, 5893, 23897, 11, 597, 307, 23897, 1320, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12355851712434188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.033584777265787125}, {"id": 1601, "seek": 775064, "start": 7750.64, "end": 7755.64, "text": " And the optimizer dot step, which lets gradient descent work its magic.", "tokens": [50364, 400, 264, 5028, 6545, 5893, 1823, 11, 597, 6653, 16235, 23475, 589, 1080, 5585, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12150182616844606, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.036207813769578934}, {"id": 1602, "seek": 775064, "start": 7755.64, "end": 7759.64, "text": " So back to optimizer dot zero grad.", "tokens": [50614, 407, 646, 281, 5028, 6545, 5893, 4018, 2771, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12150182616844606, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.036207813769578934}, {"id": 1603, "seek": 775064, "start": 7759.64, "end": 7767.64, "text": " So by default, PyTorch will accumulate the gradients over time via adding them.", "tokens": [50814, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 33384, 264, 2771, 2448, 670, 565, 5766, 5127, 552, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12150182616844606, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.036207813769578934}, {"id": 1604, "seek": 775064, "start": 7767.64, "end": 7774.64, "text": " And what we do by by putting a zero grad is we make sure that they do not add over time.", "tokens": [51214, 400, 437, 321, 360, 538, 538, 3372, 257, 4018, 2771, 307, 321, 652, 988, 300, 436, 360, 406, 909, 670, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12150182616844606, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.036207813769578934}, {"id": 1605, "seek": 775064, "start": 7774.64, "end": 7778.64, "text": " So the previous gradients do not affect the current one.", "tokens": [51564, 407, 264, 3894, 2771, 2448, 360, 406, 3345, 264, 2190, 472, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12150182616844606, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.036207813769578934}, {"id": 1606, "seek": 777864, "start": 7778.64, "end": 7783.64, "text": " And the reason we don't want this is because previous gradients are from previous data.", "tokens": [50364, 400, 264, 1778, 321, 500, 380, 528, 341, 307, 570, 3894, 2771, 2448, 366, 490, 3894, 1412, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07666286138387826, "compression_ratio": 1.7920353982300885, "no_speech_prob": 0.009410157799720764}, {"id": 1607, "seek": 777864, "start": 7783.64, "end": 7788.64, "text": " And the data is, you know, kind of weird sometimes, sometimes it's biased.", "tokens": [50614, 400, 264, 1412, 307, 11, 291, 458, 11, 733, 295, 3657, 2171, 11, 2171, 309, 311, 28035, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07666286138387826, "compression_ratio": 1.7920353982300885, "no_speech_prob": 0.009410157799720764}, {"id": 1608, "seek": 777864, "start": 7788.64, "end": 7795.64, "text": " And we don't want that determining, you know, how much like what our error is, right?", "tokens": [50864, 400, 321, 500, 380, 528, 300, 23751, 11, 291, 458, 11, 577, 709, 411, 437, 527, 6713, 307, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07666286138387826, "compression_ratio": 1.7920353982300885, "no_speech_prob": 0.009410157799720764}, {"id": 1609, "seek": 777864, "start": 7795.64, "end": 7802.64, "text": " So we only want to decide, we only want to optimize based on the current gradient of our current data.", "tokens": [51214, 407, 321, 787, 528, 281, 4536, 11, 321, 787, 528, 281, 19719, 2361, 322, 264, 2190, 16235, 295, 527, 2190, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07666286138387826, "compression_ratio": 1.7920353982300885, "no_speech_prob": 0.009410157799720764}, {"id": 1610, "seek": 777864, "start": 7802.64, "end": 7805.64, "text": " And this little parameter in here, we go set to none.", "tokens": [51564, 400, 341, 707, 13075, 294, 510, 11, 321, 352, 992, 281, 6022, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07666286138387826, "compression_ratio": 1.7920353982300885, "no_speech_prob": 0.009410157799720764}, {"id": 1611, "seek": 780564, "start": 7805.64, "end": 7811.64, "text": " This pretty much means we're going to set, we're going to set the gradients instead of zero,", "tokens": [50364, 639, 1238, 709, 1355, 321, 434, 516, 281, 992, 11, 321, 434, 516, 281, 992, 264, 2771, 2448, 2602, 295, 4018, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1612, "seek": 780564, "start": 7811.64, "end": 7815.64, "text": " instead of zero gradient, we're going to set it to none.", "tokens": [50664, 2602, 295, 4018, 16235, 11, 321, 434, 516, 281, 992, 309, 281, 6022, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1613, "seek": 780564, "start": 7815.64, "end": 7820.64, "text": " And the reason why we set it to none is because none occupies a lot less space.", "tokens": [50864, 400, 264, 1778, 983, 321, 992, 309, 281, 6022, 307, 570, 6022, 8073, 530, 257, 688, 1570, 1901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1614, "seek": 780564, "start": 7820.64, "end": 7824.64, "text": " It just, yeah, just occupies a lot less space when you have a zero.", "tokens": [51114, 467, 445, 11, 1338, 11, 445, 8073, 530, 257, 688, 1570, 1901, 562, 291, 362, 257, 4018, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1615, "seek": 780564, "start": 7824.64, "end": 7828.64, "text": " That's, that's probably an int 64 or something that's going to take up space.", "tokens": [51314, 663, 311, 11, 300, 311, 1391, 364, 560, 12145, 420, 746, 300, 311, 516, 281, 747, 493, 1901, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1616, "seek": 780564, "start": 7828.64, "end": 7834.64, "text": " And because, you know, we might have a lot of these accumulating that takes up space over time.", "tokens": [51514, 400, 570, 11, 291, 458, 11, 321, 1062, 362, 257, 688, 295, 613, 12989, 12162, 300, 2516, 493, 1901, 670, 565, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08579343557357788, "compression_ratio": 1.9625, "no_speech_prob": 0.09265480190515518}, {"id": 1617, "seek": 783464, "start": 7834.64, "end": 7841.64, "text": " So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to.", "tokens": [50364, 407, 321, 528, 281, 652, 988, 300, 264, 992, 281, 6022, 307, 2074, 11, 412, 1935, 337, 341, 1389, 11, 2171, 291, 1062, 406, 528, 281, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09696629403651445, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.003272735746577382}, {"id": 1618, "seek": 783464, "start": 7841.64, "end": 7846.64, "text": " And that's pretty much what that does.", "tokens": [50714, 400, 300, 311, 1238, 709, 437, 300, 775, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09696629403651445, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.003272735746577382}, {"id": 1619, "seek": 783464, "start": 7846.64, "end": 7856.64, "text": " It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets,", "tokens": [50964, 467, 486, 11, 498, 291, 360, 362, 4018, 2771, 322, 11, 12719, 11, 264, 787, 1778, 291, 603, 643, 309, 307, 337, 3097, 2416, 18680, 1753, 18161, 36170, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09696629403651445, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.003272735746577382}, {"id": 1620, "seek": 783464, "start": 7856.64, "end": 7861.64, "text": " which need to understand previous context because they're recurrent.", "tokens": [51464, 597, 643, 281, 1223, 3894, 4319, 570, 436, 434, 18680, 1753, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09696629403651445, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.003272735746577382}, {"id": 1621, "seek": 786164, "start": 7861.64, "end": 7870.64, "text": " I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation.", "tokens": [50364, 286, 478, 406, 516, 281, 9192, 666, 45702, 45, 82, 558, 586, 11, 457, 729, 366, 257, 955, 764, 1389, 337, 406, 1419, 4018, 2771, 16235, 35647, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09384052319960161, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.09006685018539429}, {"id": 1622, "seek": 786164, "start": 7870.64, "end": 7877.64, "text": " We'll simply take an average of all the accumulation steps and just averages the gradients together.", "tokens": [50814, 492, 603, 2935, 747, 364, 4274, 295, 439, 264, 35647, 4439, 293, 445, 42257, 264, 2771, 2448, 1214, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09384052319960161, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.09006685018539429}, {"id": 1623, "seek": 786164, "start": 7877.64, "end": 7881.64, "text": " So you get a more effective, maybe block size, right?", "tokens": [51164, 407, 291, 483, 257, 544, 4942, 11, 1310, 3461, 2744, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09384052319960161, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.09006685018539429}, {"id": 1624, "seek": 786164, "start": 7881.64, "end": 7883.64, "text": " You get more context that way.", "tokens": [51364, 509, 483, 544, 4319, 300, 636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09384052319960161, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.09006685018539429}, {"id": 1625, "seek": 786164, "start": 7883.64, "end": 7885.64, "text": " And you can have the same batch size.", "tokens": [51464, 400, 291, 393, 362, 264, 912, 15245, 2744, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09384052319960161, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.09006685018539429}, {"id": 1626, "seek": 788564, "start": 7885.64, "end": 7887.64, "text": " So just little neat tricks like that.", "tokens": [50364, 407, 445, 707, 10654, 11733, 411, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1627, "seek": 788564, "start": 7887.64, "end": 7894.64, "text": " We'll talk about gradient accumulation more later in the course, but pretty much what's going on here.", "tokens": [50464, 492, 603, 751, 466, 16235, 35647, 544, 1780, 294, 264, 1164, 11, 457, 1238, 709, 437, 311, 516, 322, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1628, "seek": 788564, "start": 7894.64, "end": 7897.64, "text": " We define an optimizer, Adam W.", "tokens": [50814, 492, 6964, 364, 5028, 6545, 11, 7938, 343, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1629, "seek": 788564, "start": 7897.64, "end": 7899.64, "text": " We iterate over max editors.", "tokens": [50964, 492, 44497, 670, 11469, 31446, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1630, "seek": 788564, "start": 7899.64, "end": 7901.64, "text": " We get a batch training split.", "tokens": [51064, 492, 483, 257, 15245, 3097, 7472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1631, "seek": 788564, "start": 7901.64, "end": 7907.64, "text": " We do a forward pass, zero grad, backward pass, and then we get a step in the right direction.", "tokens": [51164, 492, 360, 257, 2128, 1320, 11, 4018, 2771, 11, 23897, 1320, 11, 293, 550, 321, 483, 257, 1823, 294, 264, 558, 3513, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1632, "seek": 788564, "start": 7907.64, "end": 7909.64, "text": " So we're gradient descent works as magic.", "tokens": [51464, 407, 321, 434, 16235, 23475, 1985, 382, 5585, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1633, "seek": 788564, "start": 7909.64, "end": 7912.64, "text": " And at the end, we could just print out the loss here.", "tokens": [51564, 400, 412, 264, 917, 11, 321, 727, 445, 4482, 484, 264, 4470, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1369826192441194, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.1560603678226471}, {"id": 1634, "seek": 791264, "start": 7912.64, "end": 7914.64, "text": " So I've run this a few times.", "tokens": [50364, 407, 286, 600, 1190, 341, 257, 1326, 1413, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1635, "seek": 791264, "start": 7914.64, "end": 7920.64, "text": " And over time, I've gotten the loss of 2.55, which is okay.", "tokens": [50464, 400, 670, 565, 11, 286, 600, 5768, 264, 4470, 295, 568, 13, 13622, 11, 597, 307, 1392, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1636, "seek": 791264, "start": 7920.64, "end": 7930.64, "text": " And if we generate based on that loss, we get still pretty garbage tokens.", "tokens": [50764, 400, 498, 321, 8460, 2361, 322, 300, 4470, 11, 321, 483, 920, 1238, 14150, 22667, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1637, "seek": 791264, "start": 7930.64, "end": 7934.64, "text": " But then again, this is a background language model.", "tokens": [51264, 583, 550, 797, 11, 341, 307, 257, 3678, 2856, 2316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1638, "seek": 791264, "start": 7934.64, "end": 7937.64, "text": " So actually, I might need to retrain this here.", "tokens": [51464, 407, 767, 11, 286, 1062, 643, 281, 1533, 7146, 341, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1639, "seek": 791264, "start": 7937.64, "end": 7939.64, "text": " It's not trained yet.", "tokens": [51614, 467, 311, 406, 8895, 1939, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12260423704635265, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.05338599532842636}, {"id": 1640, "seek": 793964, "start": 7939.64, "end": 7946.64, "text": " So what I'm actually going to do is run this, run this, run this, boom.", "tokens": [50364, 407, 437, 286, 478, 767, 516, 281, 360, 307, 1190, 341, 11, 1190, 341, 11, 1190, 341, 11, 9351, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1641, "seek": 793964, "start": 7946.64, "end": 7950.64, "text": " And then what I'll do, oh, it looks like we're printing out a lot of stuff here.", "tokens": [50714, 400, 550, 437, 286, 603, 360, 11, 1954, 11, 309, 1542, 411, 321, 434, 14699, 484, 257, 688, 295, 1507, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1642, "seek": 793964, "start": 7950.64, "end": 7953.64, "text": " So that's coming from our get batch.", "tokens": [50914, 407, 300, 311, 1348, 490, 527, 483, 15245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1643, "seek": 793964, "start": 7953.64, "end": 7955.64, "text": " So I'll just comment that.", "tokens": [51064, 407, 286, 603, 445, 2871, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1644, "seek": 793964, "start": 7955.64, "end": 7957.64, "text": " Or we can just delete it overall.", "tokens": [51164, 1610, 321, 393, 445, 12097, 309, 4787, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1645, "seek": 793964, "start": 7957.64, "end": 7959.64, "text": " Cool.", "tokens": [51264, 8561, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1646, "seek": 793964, "start": 7959.64, "end": 7968.64, "text": " And now if we run this again.", "tokens": [51364, 400, 586, 498, 321, 1190, 341, 797, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18000302107437796, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.010487592779099941}, {"id": 1647, "seek": 796864, "start": 7968.64, "end": 7973.64, "text": " Give it a second.", "tokens": [50364, 5303, 309, 257, 1150, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10298233843864278, "compression_ratio": 1.1517857142857142, "no_speech_prob": 0.014723040163516998}, {"id": 1648, "seek": 796864, "start": 7973.64, "end": 7976.64, "text": " Perfect.", "tokens": [50614, 10246, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10298233843864278, "compression_ratio": 1.1517857142857142, "no_speech_prob": 0.014723040163516998}, {"id": 1649, "seek": 796864, "start": 7976.64, "end": 7981.64, "text": " So I don't know why it's still doing that.", "tokens": [50764, 407, 286, 500, 380, 458, 983, 309, 311, 920, 884, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10298233843864278, "compression_ratio": 1.1517857142857142, "no_speech_prob": 0.014723040163516998}, {"id": 1650, "seek": 796864, "start": 7981.64, "end": 7986.64, "text": " If we run it again, let's see.", "tokens": [51014, 759, 321, 1190, 309, 797, 11, 718, 311, 536, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10298233843864278, "compression_ratio": 1.1517857142857142, "no_speech_prob": 0.014723040163516998}, {"id": 1651, "seek": 796864, "start": 7986.64, "end": 7995.64, "text": " Where are we printing stuff?", "tokens": [51264, 2305, 366, 321, 14699, 1507, 30, 51714], "temperature": 0.0, "avg_logprob": -0.10298233843864278, "compression_ratio": 1.1517857142857142, "no_speech_prob": 0.014723040163516998}, {"id": 1652, "seek": 799564, "start": 7996.64, "end": 7998.64, "text": " No.", "tokens": [50414, 883, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1653, "seek": 799564, "start": 7998.64, "end": 8000.64, "text": " Ah, yes.", "tokens": [50514, 2438, 11, 2086, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1654, "seek": 799564, "start": 8000.64, "end": 8002.64, "text": " We have to run this again after changing it.", "tokens": [50614, 492, 362, 281, 1190, 341, 797, 934, 4473, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1655, "seek": 799564, "start": 8002.64, "end": 8006.64, "text": " Silly me.", "tokens": [50714, 318, 6917, 385, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1656, "seek": 799564, "start": 8006.64, "end": 8010.64, "text": " And of course, 10,000 steps is a lot.", "tokens": [50914, 400, 295, 1164, 11, 1266, 11, 1360, 4439, 307, 257, 688, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1657, "seek": 799564, "start": 8010.64, "end": 8012.64, "text": " So it takes a little while.", "tokens": [51114, 407, 309, 2516, 257, 707, 1339, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1658, "seek": 799564, "start": 8012.64, "end": 8014.64, "text": " It takes a few seconds, which is actually quite quick.", "tokens": [51214, 467, 2516, 257, 1326, 3949, 11, 597, 307, 767, 1596, 1702, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1659, "seek": 799564, "start": 8014.64, "end": 8018.64, "text": " So after the first one, we get a loss of 3.15.", "tokens": [51314, 407, 934, 264, 700, 472, 11, 321, 483, 257, 4470, 295, 805, 13, 5211, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1660, "seek": 799564, "start": 8018.64, "end": 8020.64, "text": " We can generate from that.", "tokens": [51514, 492, 393, 8460, 490, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1661, "seek": 799564, "start": 8020.64, "end": 8022.64, "text": " And we get something that is less garbage.", "tokens": [51614, 400, 321, 483, 746, 300, 307, 1570, 14150, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1662, "seek": 799564, "start": 8022.64, "end": 8024.64, "text": " You know, it has some next line characters.", "tokens": [51714, 509, 458, 11, 309, 575, 512, 958, 1622, 4342, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15327934596849524, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.06184908747673035}, {"id": 1663, "seek": 802464, "start": 8024.64, "end": 8026.64, "text": " It understands a little bit more to, you know,", "tokens": [50364, 467, 15146, 257, 707, 857, 544, 281, 11, 291, 458, 11, 50464], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1664, "seek": 802464, "start": 8026.64, "end": 8028.64, "text": " space things out and whatnot.", "tokens": [50464, 1901, 721, 484, 293, 25882, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1665, "seek": 802464, "start": 8028.64, "end": 8032.64, "text": " So that's like slightly less garbage than before.", "tokens": [50564, 407, 300, 311, 411, 4748, 1570, 14150, 813, 949, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1666, "seek": 802464, "start": 8032.64, "end": 8036.64, "text": " But yeah, this, this is pretty good.", "tokens": [50764, 583, 1338, 11, 341, 11, 341, 307, 1238, 665, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1667, "seek": 802464, "start": 8036.64, "end": 8038.64, "text": " So I lied.", "tokens": [50964, 407, 286, 20101, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1668, "seek": 802464, "start": 8038.64, "end": 8042.64, "text": " There aren't actually any lectures previously where I talked about optimizers.", "tokens": [51064, 821, 3212, 380, 767, 604, 16564, 8046, 689, 286, 2825, 466, 5028, 22525, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1669, "seek": 802464, "start": 8042.64, "end": 8045.64, "text": " So might as well talk about it now.", "tokens": [51264, 407, 1062, 382, 731, 751, 466, 309, 586, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1670, "seek": 802464, "start": 8045.64, "end": 8047.64, "text": " So a bunch of common ones.", "tokens": [51414, 407, 257, 3840, 295, 2689, 2306, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1671, "seek": 802464, "start": 8047.64, "end": 8052.64, "text": " And honestly, you don't really need to know anything more than the common ones", "tokens": [51514, 400, 6095, 11, 291, 500, 380, 534, 643, 281, 458, 1340, 544, 813, 264, 2689, 2306, 51764], "temperature": 0.0, "avg_logprob": -0.15131568908691406, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.07684460282325745}, {"id": 1672, "seek": 805264, "start": 8052.64, "end": 8056.64, "text": " because most of them are just built off of these.", "tokens": [50364, 570, 881, 295, 552, 366, 445, 3094, 766, 295, 613, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1673, "seek": 805264, "start": 8056.64, "end": 8059.64, "text": " So you have your mean squared error,", "tokens": [50564, 407, 291, 362, 428, 914, 8889, 6713, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1674, "seek": 805264, "start": 8059.64, "end": 8062.64, "text": " common loss function using regression, regression problems,", "tokens": [50714, 2689, 4470, 2445, 1228, 24590, 11, 24590, 2740, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1675, "seek": 805264, "start": 8062.64, "end": 8065.64, "text": " where it's like, you know, you have a bunch of data points,", "tokens": [50864, 689, 309, 311, 411, 11, 291, 458, 11, 291, 362, 257, 3840, 295, 1412, 2793, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1676, "seek": 805264, "start": 8065.64, "end": 8067.64, "text": " find the best fit line, right?", "tokens": [51014, 915, 264, 1151, 3318, 1622, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1677, "seek": 805264, "start": 8067.64, "end": 8069.64, "text": " That's a common regression problem.", "tokens": [51114, 663, 311, 257, 2689, 24590, 1154, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1678, "seek": 805264, "start": 8069.64, "end": 8071.64, "text": " Goals to predict a continuous output", "tokens": [51214, 1037, 1124, 281, 6069, 257, 10957, 5598, 51314], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1679, "seek": 805264, "start": 8071.64, "end": 8073.64, "text": " and measures the average squared difference", "tokens": [51314, 293, 8000, 264, 4274, 8889, 2649, 51414], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1680, "seek": 805264, "start": 8073.64, "end": 8076.64, "text": " between the predicted and actual values,", "tokens": [51414, 1296, 264, 19147, 293, 3539, 4190, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1681, "seek": 805264, "start": 8076.64, "end": 8079.64, "text": " often used to train neural networks for regression tasks.", "tokens": [51564, 2049, 1143, 281, 3847, 18161, 9590, 337, 24590, 9608, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1682, "seek": 805264, "start": 8079.64, "end": 8081.64, "text": " So cool.", "tokens": [51714, 407, 1627, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11532906006122458, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0032727119978517294}, {"id": 1683, "seek": 808164, "start": 8081.64, "end": 8083.64, "text": " That's the most basic one.", "tokens": [50364, 663, 311, 264, 881, 3875, 472, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1684, "seek": 808164, "start": 8083.64, "end": 8085.64, "text": " You can look into that more if you'd like,", "tokens": [50464, 509, 393, 574, 666, 300, 544, 498, 291, 1116, 411, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1685, "seek": 808164, "start": 8085.64, "end": 8088.64, "text": " but that's our most basic optimizer.", "tokens": [50564, 457, 300, 311, 527, 881, 3875, 5028, 6545, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1686, "seek": 808164, "start": 8088.64, "end": 8090.64, "text": " Gradient descent is a step up from that.", "tokens": [50714, 16710, 1196, 23475, 307, 257, 1823, 493, 490, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1687, "seek": 808164, "start": 8090.64, "end": 8093.64, "text": " It's used to minimize the loss function in a model,", "tokens": [50814, 467, 311, 1143, 281, 17522, 264, 4470, 2445, 294, 257, 2316, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1688, "seek": 808164, "start": 8093.64, "end": 8095.64, "text": " measures how well the model,", "tokens": [50964, 8000, 577, 731, 264, 2316, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1689, "seek": 808164, "start": 8095.64, "end": 8099.64, "text": " the gradient measures how well the model is able to predict", "tokens": [51064, 264, 16235, 8000, 577, 731, 264, 2316, 307, 1075, 281, 6069, 51264], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1690, "seek": 808164, "start": 8099.64, "end": 8102.64, "text": " the target variable based on the input features.", "tokens": [51264, 264, 3779, 7006, 2361, 322, 264, 4846, 4122, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1691, "seek": 808164, "start": 8102.64, "end": 8104.64, "text": " So we have some input X,", "tokens": [51414, 407, 321, 362, 512, 4846, 1783, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1692, "seek": 808164, "start": 8104.64, "end": 8108.64, "text": " we have some weights and biases maybe, WX plus B.", "tokens": [51514, 321, 362, 512, 17443, 293, 32152, 1310, 11, 343, 55, 1804, 363, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09157120468270065, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.001987612806260586}, {"id": 1693, "seek": 810864, "start": 8108.64, "end": 8114.64, "text": " And all we're trying to do is make sure that the inputs", "tokens": [50364, 400, 439, 321, 434, 1382, 281, 360, 307, 652, 988, 300, 264, 15743, 50664], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1694, "seek": 810864, "start": 8114.64, "end": 8121.64, "text": " or make sure that we make the inputs become the desired outputs", "tokens": [50664, 420, 652, 988, 300, 321, 652, 264, 15743, 1813, 264, 14721, 23930, 51014], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1695, "seek": 810864, "start": 8121.64, "end": 8125.64, "text": " and based on how far it is away from the desired outputs,", "tokens": [51014, 293, 2361, 322, 577, 1400, 309, 307, 1314, 490, 264, 14721, 23930, 11, 51214], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1696, "seek": 810864, "start": 8125.64, "end": 8128.64, "text": " we can change the parameters of the model.", "tokens": [51214, 321, 393, 1319, 264, 9834, 295, 264, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1697, "seek": 810864, "start": 8128.64, "end": 8132.64, "text": " So we went over gradient descent recently or previously,", "tokens": [51364, 407, 321, 1437, 670, 16235, 23475, 3938, 420, 8046, 11, 51564], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1698, "seek": 810864, "start": 8132.64, "end": 8135.64, "text": " but that's pretty much what's going on here.", "tokens": [51564, 457, 300, 311, 1238, 709, 437, 311, 516, 322, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0750102822373553, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.003706760238856077}, {"id": 1699, "seek": 813564, "start": 8135.64, "end": 8140.64, "text": " And momentum is just a little extension of gradient descent", "tokens": [50364, 400, 11244, 307, 445, 257, 707, 10320, 295, 16235, 23475, 50614], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1700, "seek": 813564, "start": 8140.64, "end": 8143.64, "text": " that adds the momentum term.", "tokens": [50614, 300, 10860, 264, 11244, 1433, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1701, "seek": 813564, "start": 8143.64, "end": 8146.64, "text": " So it helps smooth out the training", "tokens": [50764, 407, 309, 3665, 5508, 484, 264, 3097, 50914], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1702, "seek": 813564, "start": 8146.64, "end": 8151.64, "text": " and allows it to continue moving in the right direction,", "tokens": [50914, 293, 4045, 309, 281, 2354, 2684, 294, 264, 558, 3513, 11, 51164], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1703, "seek": 813564, "start": 8151.64, "end": 8154.64, "text": " even if the gradient changes direction or varies in magnitude.", "tokens": [51164, 754, 498, 264, 16235, 2962, 3513, 420, 21716, 294, 15668, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1704, "seek": 813564, "start": 8154.64, "end": 8157.64, "text": " It's particularly useful for training deep neural nets.", "tokens": [51314, 467, 311, 4098, 4420, 337, 3097, 2452, 18161, 36170, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1705, "seek": 813564, "start": 8157.64, "end": 8160.64, "text": " So momentum is when you have, you know,", "tokens": [51464, 407, 11244, 307, 562, 291, 362, 11, 291, 458, 11, 51614], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1706, "seek": 813564, "start": 8160.64, "end": 8164.64, "text": " you consider some of the other gradients.", "tokens": [51614, 291, 1949, 512, 295, 264, 661, 2771, 2448, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06312573474386464, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.001926502212882042}, {"id": 1707, "seek": 816464, "start": 8164.64, "end": 8167.64, "text": " So you have something that's like maybe passed on from here", "tokens": [50364, 407, 291, 362, 746, 300, 311, 411, 1310, 4678, 322, 490, 510, 50514], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1708, "seek": 816464, "start": 8167.64, "end": 8170.64, "text": " and then it might include a little bit of the current one.", "tokens": [50514, 293, 550, 309, 1062, 4090, 257, 707, 857, 295, 264, 2190, 472, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1709, "seek": 816464, "start": 8170.64, "end": 8173.64, "text": " So like 90%, like a good momentum coefficient", "tokens": [50664, 407, 411, 4289, 8923, 411, 257, 665, 11244, 17619, 50814], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1710, "seek": 816464, "start": 8173.64, "end": 8176.64, "text": " would be like 90% previous gradients", "tokens": [50814, 576, 312, 411, 4289, 4, 3894, 2771, 2448, 50964], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1711, "seek": 816464, "start": 8176.64, "end": 8178.64, "text": " and then 10% of the current one.", "tokens": [50964, 293, 550, 1266, 4, 295, 264, 2190, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1712, "seek": 816464, "start": 8178.64, "end": 8183.64, "text": " So it kind of like lags behind and makes it converge sort of smoothly.", "tokens": [51064, 407, 309, 733, 295, 411, 8953, 82, 2261, 293, 1669, 309, 41881, 1333, 295, 19565, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1713, "seek": 816464, "start": 8183.64, "end": 8185.64, "text": " That makes sense.", "tokens": [51314, 663, 1669, 2020, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1714, "seek": 816464, "start": 8185.64, "end": 8187.64, "text": " Arm as prop, I've never used this,", "tokens": [51414, 11893, 382, 2365, 11, 286, 600, 1128, 1143, 341, 11, 51514], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1715, "seek": 816464, "start": 8187.64, "end": 8191.64, "text": " but it's an algorithm that use the moving average of the squared gradient", "tokens": [51514, 457, 309, 311, 364, 9284, 300, 764, 264, 2684, 4274, 295, 264, 8889, 16235, 51714], "temperature": 0.0, "avg_logprob": -0.13394008304761804, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.003376041306182742}, {"id": 1716, "seek": 819164, "start": 8191.64, "end": 8193.64, "text": " to adapt learning rates of each parameter,", "tokens": [50364, 281, 6231, 2539, 6846, 295, 1184, 13075, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1717, "seek": 819164, "start": 8193.64, "end": 8196.64, "text": " helps to avoid oscillations in the parameter updates", "tokens": [50464, 3665, 281, 5042, 18225, 763, 294, 264, 13075, 9205, 50614], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1718, "seek": 819164, "start": 8196.64, "end": 8199.64, "text": " and can move and can improve convergence in some cases.", "tokens": [50614, 293, 393, 1286, 293, 393, 3470, 32181, 294, 512, 3331, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1719, "seek": 819164, "start": 8199.64, "end": 8202.64, "text": " So you can look more into that if you'd like.", "tokens": [50764, 407, 291, 393, 574, 544, 666, 300, 498, 291, 1116, 411, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1720, "seek": 819164, "start": 8202.64, "end": 8204.64, "text": " Adam, very popular,", "tokens": [50914, 7938, 11, 588, 3743, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1721, "seek": 819164, "start": 8204.64, "end": 8208.64, "text": " combines the ideas of momentum and arm as prop.", "tokens": [51014, 29520, 264, 3487, 295, 11244, 293, 3726, 382, 2365, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1722, "seek": 819164, "start": 8208.64, "end": 8210.64, "text": " He uses a moving average,", "tokens": [51214, 634, 4960, 257, 2684, 4274, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1723, "seek": 819164, "start": 8210.64, "end": 8212.64, "text": " both the gradient and its squared value", "tokens": [51314, 1293, 264, 16235, 293, 1080, 8889, 2158, 51414], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1724, "seek": 819164, "start": 8212.64, "end": 8214.64, "text": " to adapt learning rate of each parameter.", "tokens": [51414, 281, 6231, 2539, 3314, 295, 1184, 13075, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1725, "seek": 819164, "start": 8214.64, "end": 8218.64, "text": " So often uses the default optimizer for deep learning models.", "tokens": [51514, 407, 2049, 4960, 264, 7576, 5028, 6545, 337, 2452, 2539, 5245, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10935812863436613, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.008314458653330803}, {"id": 1726, "seek": 821864, "start": 8218.64, "end": 8221.64, "text": " And in our case, when we continue to build this out,", "tokens": [50364, 400, 294, 527, 1389, 11, 562, 321, 2354, 281, 1322, 341, 484, 11, 50514], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1727, "seek": 821864, "start": 8221.64, "end": 8224.64, "text": " it's going to be quite a deep net.", "tokens": [50514, 309, 311, 516, 281, 312, 1596, 257, 2452, 2533, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1728, "seek": 821864, "start": 8224.64, "end": 8228.64, "text": " And Adam W is just a modification of the item optimizer", "tokens": [50664, 400, 7938, 343, 307, 445, 257, 26747, 295, 264, 3174, 5028, 6545, 50864], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1729, "seek": 821864, "start": 8228.64, "end": 8230.64, "text": " that adds weight decay to the parameter updates.", "tokens": [50864, 300, 10860, 3364, 21039, 281, 264, 13075, 9205, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1730, "seek": 821864, "start": 8230.64, "end": 8235.64, "text": " So helps to regularize and improve generalization performance.", "tokens": [50964, 407, 3665, 281, 3890, 1125, 293, 3470, 2674, 2144, 3389, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1731, "seek": 821864, "start": 8235.64, "end": 8239.64, "text": " Using this optimizer as it best suits the properties of the model", "tokens": [51214, 11142, 341, 5028, 6545, 382, 309, 1151, 15278, 264, 7221, 295, 264, 2316, 51414], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1732, "seek": 821864, "start": 8239.64, "end": 8241.64, "text": " we'll train in this video.", "tokens": [51414, 321, 603, 3847, 294, 341, 960, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1733, "seek": 821864, "start": 8241.64, "end": 8244.64, "text": " So, of course, I'm reading off the script here.", "tokens": [51514, 407, 11, 295, 1164, 11, 286, 478, 3760, 766, 264, 5755, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13321683991630123, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00394488824531436}, {"id": 1734, "seek": 824464, "start": 8244.64, "end": 8248.64, "text": " There's no really other better way to say how these optimizers work.", "tokens": [50364, 821, 311, 572, 534, 661, 1101, 636, 281, 584, 577, 613, 5028, 22525, 589, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1735, "seek": 824464, "start": 8248.64, "end": 8251.64, "text": " But, yeah, if you want to look more into, you know,", "tokens": [50564, 583, 11, 1338, 11, 498, 291, 528, 281, 574, 544, 666, 11, 291, 458, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1736, "seek": 824464, "start": 8251.64, "end": 8254.64, "text": " concepts like momentum or weight decay", "tokens": [50714, 10392, 411, 11244, 420, 3364, 21039, 50864], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1737, "seek": 824464, "start": 8254.64, "end": 8259.64, "text": " or, you know, oscillations and just some statistic stuff, you can.", "tokens": [50864, 420, 11, 291, 458, 11, 18225, 763, 293, 445, 512, 29588, 1507, 11, 291, 393, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1738, "seek": 824464, "start": 8259.64, "end": 8263.64, "text": " But honestly, the only thing that really matters", "tokens": [51114, 583, 6095, 11, 264, 787, 551, 300, 534, 7001, 51314], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1739, "seek": 824464, "start": 8263.64, "end": 8267.64, "text": " is just knowing which optimizers are used for certain things.", "tokens": [51314, 307, 445, 5276, 597, 5028, 22525, 366, 1143, 337, 1629, 721, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1740, "seek": 824464, "start": 8267.64, "end": 8270.64, "text": " So, like, what is the momentum used for?", "tokens": [51514, 407, 11, 411, 11, 437, 307, 264, 11244, 1143, 337, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1741, "seek": 824464, "start": 8270.64, "end": 8273.64, "text": " What is Adam W great for?", "tokens": [51664, 708, 307, 7938, 343, 869, 337, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10358188349172609, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.05919196829199791}, {"id": 1742, "seek": 827364, "start": 8273.64, "end": 8276.64, "text": " What is MSC good for, right?", "tokens": [50364, 708, 307, 7395, 34, 665, 337, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1743, "seek": 827364, "start": 8276.64, "end": 8280.64, "text": " Just knowing what the differences and similarities are,", "tokens": [50514, 1449, 5276, 437, 264, 7300, 293, 24197, 366, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1744, "seek": 827364, "start": 8280.64, "end": 8285.64, "text": " as well as when is the best case to use the optimizer.", "tokens": [50714, 382, 731, 382, 562, 307, 264, 1151, 1389, 281, 764, 264, 5028, 6545, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1745, "seek": 827364, "start": 8285.64, "end": 8290.64, "text": " So, yeah, you can find more information about that at torch.optim.", "tokens": [50964, 407, 11, 1338, 11, 291, 393, 915, 544, 1589, 466, 300, 412, 27822, 13, 5747, 332, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1746, "seek": 827364, "start": 8290.64, "end": 8292.64, "text": " So when we develop language models,", "tokens": [51214, 407, 562, 321, 1499, 2856, 5245, 11, 51314], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1747, "seek": 827364, "start": 8292.64, "end": 8295.64, "text": " something really important in language modeling,", "tokens": [51314, 746, 534, 1021, 294, 2856, 15983, 11, 51464], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1748, "seek": 827364, "start": 8295.64, "end": 8298.64, "text": " data science, machine learning, at all,", "tokens": [51464, 1412, 3497, 11, 3479, 2539, 11, 412, 439, 11, 51614], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1749, "seek": 827364, "start": 8298.64, "end": 8300.64, "text": " is just being able to report a loss", "tokens": [51614, 307, 445, 885, 1075, 281, 2275, 257, 4470, 51714], "temperature": 0.0, "avg_logprob": -0.0851079882407675, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.002182319527491927}, {"id": 1750, "seek": 830064, "start": 8300.64, "end": 8303.64, "text": " or get an idea of how well our model is performing", "tokens": [50364, 420, 483, 364, 1558, 295, 577, 731, 527, 2316, 307, 10205, 50514], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1751, "seek": 830064, "start": 8303.64, "end": 8305.64, "text": " over, you know, the first 1,000 iterations", "tokens": [50514, 670, 11, 291, 458, 11, 264, 700, 502, 11, 1360, 36540, 50614], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1752, "seek": 830064, "start": 8305.64, "end": 8307.64, "text": " and then the first 2,000 iterations", "tokens": [50614, 293, 550, 264, 700, 568, 11, 1360, 36540, 50714], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1753, "seek": 830064, "start": 8307.64, "end": 8309.64, "text": " and 4,000 iterations, right?", "tokens": [50714, 293, 1017, 11, 1360, 36540, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1754, "seek": 830064, "start": 8309.64, "end": 8311.64, "text": " So we want to get a general idea", "tokens": [50814, 407, 321, 528, 281, 483, 257, 2674, 1558, 50914], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1755, "seek": 830064, "start": 8311.64, "end": 8313.64, "text": " of how our model is converging over time.", "tokens": [50914, 295, 577, 527, 2316, 307, 9652, 3249, 670, 565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1756, "seek": 830064, "start": 8313.64, "end": 8316.64, "text": " But we don't want to just print every single step of this.", "tokens": [51014, 583, 321, 500, 380, 528, 281, 445, 4482, 633, 2167, 1823, 295, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1757, "seek": 830064, "start": 8316.64, "end": 8317.64, "text": " That wouldn't make sense.", "tokens": [51164, 663, 2759, 380, 652, 2020, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1758, "seek": 830064, "start": 8317.64, "end": 8321.64, "text": " So what we actually could do is print every, you know,", "tokens": [51214, 407, 437, 321, 767, 727, 360, 307, 4482, 633, 11, 291, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1759, "seek": 830064, "start": 8321.64, "end": 8323.64, "text": " 200 iterations, 500.", "tokens": [51414, 2331, 36540, 11, 5923, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1760, "seek": 830064, "start": 8323.64, "end": 8325.64, "text": " We could print every 10,000 iterations", "tokens": [51514, 492, 727, 4482, 633, 1266, 11, 1360, 36540, 51614], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1761, "seek": 830064, "start": 8325.64, "end": 8328.64, "text": " if you're running a crazy big language model if you wanted to.", "tokens": [51614, 498, 291, 434, 2614, 257, 3219, 955, 2856, 2316, 498, 291, 1415, 281, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06814039928812377, "compression_ratio": 1.8859315589353611, "no_speech_prob": 0.008059382438659668}, {"id": 1762, "seek": 832864, "start": 8329.64, "end": 8331.64, "text": " And that's exactly what we're going to implement right here.", "tokens": [50414, 400, 300, 311, 2293, 437, 321, 434, 516, 281, 4445, 558, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1763, "seek": 832864, "start": 8331.64, "end": 8337.64, "text": " So, actually, this doesn't require an insane amount of Python syntax.", "tokens": [50514, 407, 11, 767, 11, 341, 1177, 380, 3651, 364, 10838, 2372, 295, 15329, 28431, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1764, "seek": 832864, "start": 8337.64, "end": 8341.64, "text": " This is just, I'm actually just going to add it into our for loop here.", "tokens": [50814, 639, 307, 445, 11, 286, 478, 767, 445, 516, 281, 909, 309, 666, 527, 337, 6367, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1765, "seek": 832864, "start": 8341.64, "end": 8345.64, "text": " And what this is going to do is it's going to do what I just said,", "tokens": [51014, 400, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 360, 437, 286, 445, 848, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1766, "seek": 832864, "start": 8345.64, "end": 8349.64, "text": " is print every, you know, every certain number of iterations.", "tokens": [51214, 307, 4482, 633, 11, 291, 458, 11, 633, 1629, 1230, 295, 36540, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1767, "seek": 832864, "start": 8349.64, "end": 8356.64, "text": " So we can add a new hyper parameter up here called eval-itters.", "tokens": [51414, 407, 321, 393, 909, 257, 777, 9848, 13075, 493, 510, 1219, 1073, 304, 12, 38873, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13359669658625237, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.002714799717068672}, {"id": 1768, "seek": 835664, "start": 8356.64, "end": 8361.64, "text": " And I'm going to make this 250 just for,", "tokens": [50364, 400, 286, 478, 516, 281, 652, 341, 11650, 445, 337, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1769, "seek": 835664, "start": 8361.64, "end": 8364.64, "text": " just to make things sort of easy here.", "tokens": [50614, 445, 281, 652, 721, 1333, 295, 1858, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1770, "seek": 835664, "start": 8364.64, "end": 8368.64, "text": " And we're going to go ahead and add this in here.", "tokens": [50764, 400, 321, 434, 516, 281, 352, 2286, 293, 909, 341, 294, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1771, "seek": 835664, "start": 8368.64, "end": 8373.64, "text": " So I'm going to go if-iter.", "tokens": [50964, 407, 286, 478, 516, 281, 352, 498, 12, 1681, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1772, "seek": 835664, "start": 8373.64, "end": 8375.64, "text": " And we're going to do the module operator.", "tokens": [51214, 400, 321, 434, 516, 281, 360, 264, 10088, 12973, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1773, "seek": 835664, "start": 8375.64, "end": 8378.64, "text": " You can look more into this if you want later.", "tokens": [51314, 509, 393, 574, 544, 666, 341, 498, 291, 528, 1780, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1774, "seek": 835664, "start": 8378.64, "end": 8383.64, "text": " And we're going to do eval-itters equals equals zero.", "tokens": [51464, 400, 321, 434, 516, 281, 360, 1073, 304, 12, 38873, 6915, 6915, 4018, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10923478958454538, "compression_ratio": 1.760233918128655, "no_speech_prob": 0.0010161933023482561}, {"id": 1775, "seek": 838364, "start": 8383.64, "end": 8390.64, "text": " What this is going to do is it's going to check if the current iteration", "tokens": [50364, 708, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 1520, 498, 264, 2190, 24784, 50714], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1776, "seek": 838364, "start": 8390.64, "end": 8397.64, "text": " divided by, or sorry, if the remainder of the current iteration", "tokens": [50714, 6666, 538, 11, 420, 2597, 11, 498, 264, 29837, 295, 264, 2190, 24784, 51064], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1777, "seek": 838364, "start": 8397.64, "end": 8400.64, "text": " divided by our eval-itters parameter,", "tokens": [51064, 6666, 538, 527, 1073, 304, 12, 38873, 13075, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1778, "seek": 838364, "start": 8400.64, "end": 8405.64, "text": " if the remainder of that is zero, then we continue with it.", "tokens": [51214, 498, 264, 29837, 295, 300, 307, 4018, 11, 550, 321, 2354, 365, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1779, "seek": 838364, "start": 8405.64, "end": 8407.64, "text": " So hopefully that made sense.", "tokens": [51464, 407, 4696, 300, 1027, 2020, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1780, "seek": 838364, "start": 8407.64, "end": 8412.64, "text": " If you want to, you could just ask GPT4", "tokens": [51564, 759, 291, 528, 281, 11, 291, 727, 445, 1029, 26039, 51, 19, 51814], "temperature": 0.0, "avg_logprob": -0.13454428173246838, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.001782019855454564}, {"id": 1781, "seek": 841264, "start": 8412.64, "end": 8416.64, "text": " or GPT3.5, whatever you have, just this module operator,", "tokens": [50364, 420, 26039, 51, 18, 13, 20, 11, 2035, 291, 362, 11, 445, 341, 10088, 12973, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1782, "seek": 841264, "start": 8416.64, "end": 8419.64, "text": " and you should get a good general understanding of what it does.", "tokens": [50564, 293, 291, 820, 483, 257, 665, 2674, 3701, 295, 437, 309, 775, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1783, "seek": 841264, "start": 8419.64, "end": 8421.64, "text": " Cool.", "tokens": [50714, 8561, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1784, "seek": 841264, "start": 8421.64, "end": 8424.64, "text": " So all we can do now is we'll just say,", "tokens": [50814, 407, 439, 321, 393, 360, 586, 307, 321, 603, 445, 584, 11, 50964], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1785, "seek": 841264, "start": 8424.64, "end": 8426.64, "text": " we'll just have a filler statement here.", "tokens": [50964, 321, 603, 445, 362, 257, 34676, 5629, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1786, "seek": 841264, "start": 8426.64, "end": 8431.64, "text": " We'll just do print, we've been f-string,", "tokens": [51064, 492, 603, 445, 360, 4482, 11, 321, 600, 668, 283, 12, 37045, 11, 51314], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1787, "seek": 841264, "start": 8431.64, "end": 8440.64, "text": " and then we'll go losses, losses, maybe that.", "tokens": [51314, 293, 550, 321, 603, 352, 15352, 11, 15352, 11, 1310, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14386670307446553, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.005384213291108608}, {"id": 1788, "seek": 844064, "start": 8440.64, "end": 8442.64, "text": " Or actually, I'm going to change this here.", "tokens": [50364, 1610, 767, 11, 286, 478, 516, 281, 1319, 341, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.17014963873501482, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.010817360132932663}, {"id": 1789, "seek": 844064, "start": 8442.64, "end": 8449.64, "text": " We can go step-iter.", "tokens": [50464, 492, 393, 352, 1823, 12, 1681, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17014963873501482, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.010817360132932663}, {"id": 1790, "seek": 844064, "start": 8449.64, "end": 8453.64, "text": " Add a little colon in there.", "tokens": [50814, 5349, 257, 707, 8255, 294, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17014963873501482, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.010817360132932663}, {"id": 1791, "seek": 844064, "start": 8453.64, "end": 8461.64, "text": " And then I'll go split.", "tokens": [51014, 400, 550, 286, 603, 352, 7472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17014963873501482, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.010817360132932663}, {"id": 1792, "seek": 844064, "start": 8461.64, "end": 8467.64, "text": " Actually, I'll just go loss, and then losses like that.", "tokens": [51414, 5135, 11, 286, 603, 445, 352, 4470, 11, 293, 550, 15352, 411, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17014963873501482, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.010817360132932663}, {"id": 1793, "seek": 846764, "start": 8467.64, "end": 8471.64, "text": " And then we'll have some sort of put in here.", "tokens": [50364, 400, 550, 321, 603, 362, 512, 1333, 295, 829, 294, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1794, "seek": 846764, "start": 8471.64, "end": 8474.64, "text": " Something soon.", "tokens": [50564, 6595, 2321, 13, 50714], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1795, "seek": 846764, "start": 8474.64, "end": 8476.64, "text": " I don't know.", "tokens": [50714, 286, 500, 380, 458, 13, 50814], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1796, "seek": 846764, "start": 8476.64, "end": 8480.64, "text": " And all I've done is I've actually added a little function in here", "tokens": [50814, 400, 439, 286, 600, 1096, 307, 286, 600, 767, 3869, 257, 707, 2445, 294, 510, 51014], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1797, "seek": 846764, "start": 8480.64, "end": 8481.64, "text": " behind the scenes.", "tokens": [51014, 2261, 264, 8026, 13, 51064], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1798, "seek": 846764, "start": 8481.64, "end": 8483.64, "text": " You guys didn't see me do this yet.", "tokens": [51064, 509, 1074, 994, 380, 536, 385, 360, 341, 1939, 13, 51164], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1799, "seek": 846764, "start": 8483.64, "end": 8488.64, "text": " But pretty much, I'm not going to go through the actual function itself,", "tokens": [51164, 583, 1238, 709, 11, 286, 478, 406, 516, 281, 352, 807, 264, 3539, 2445, 2564, 11, 51414], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1800, "seek": 846764, "start": 8488.64, "end": 8492.64, "text": " but what is important is that you know this decorator right here.", "tokens": [51414, 457, 437, 307, 1021, 307, 300, 291, 458, 341, 7919, 1639, 558, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1801, "seek": 846764, "start": 8492.64, "end": 8494.64, "text": " This probably isn't very common to you.", "tokens": [51614, 639, 1391, 1943, 380, 588, 2689, 281, 291, 13, 51714], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1802, "seek": 846764, "start": 8494.64, "end": 8496.64, "text": " This is torch.nograt.", "tokens": [51714, 639, 307, 27822, 13, 77, 664, 4481, 13, 51814], "temperature": 0.0, "avg_logprob": -0.141527547674664, "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.01911791041493416}, {"id": 1803, "seek": 849664, "start": 8496.64, "end": 8499.64, "text": " And what this is going to do is it's going to make sure that", "tokens": [50364, 400, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 652, 988, 300, 50514], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1804, "seek": 849664, "start": 8499.64, "end": 8501.64, "text": " PyTorch doesn't use gradients at all in here.", "tokens": [50514, 9953, 51, 284, 339, 1177, 380, 764, 2771, 2448, 412, 439, 294, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1805, "seek": 849664, "start": 8501.64, "end": 8503.64, "text": " That'll reduce computation.", "tokens": [50614, 663, 603, 5407, 24903, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1806, "seek": 849664, "start": 8503.64, "end": 8505.64, "text": " It'll reduce memory usage.", "tokens": [50714, 467, 603, 5407, 4675, 14924, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1807, "seek": 849664, "start": 8505.64, "end": 8507.64, "text": " It's just overall better for performance.", "tokens": [50814, 467, 311, 445, 4787, 1101, 337, 3389, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1808, "seek": 849664, "start": 8507.64, "end": 8509.64, "text": " And because we're just reporting a loss,", "tokens": [50914, 400, 570, 321, 434, 445, 10031, 257, 4470, 11, 51014], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1809, "seek": 849664, "start": 8509.64, "end": 8513.64, "text": " we don't really need to do any optimizing or gradient computation here.", "tokens": [51014, 321, 500, 380, 534, 643, 281, 360, 604, 40425, 420, 16235, 24903, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1810, "seek": 849664, "start": 8513.64, "end": 8515.64, "text": " We're just getting losses.", "tokens": [51214, 492, 434, 445, 1242, 15352, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1811, "seek": 849664, "start": 8515.64, "end": 8517.64, "text": " We're feeding some stuff into the model.", "tokens": [51314, 492, 434, 12919, 512, 1507, 666, 264, 2316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1812, "seek": 849664, "start": 8517.64, "end": 8521.64, "text": " We're getting a loss out of it, and we're going from there.", "tokens": [51414, 492, 434, 1242, 257, 4470, 484, 295, 309, 11, 293, 321, 434, 516, 490, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06825747028473884, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.03356180340051651}, {"id": 1813, "seek": 852164, "start": 8521.64, "end": 8526.64, "text": " So that's pretty much what's happening with this torch.nograt.", "tokens": [50364, 407, 300, 311, 1238, 709, 437, 311, 2737, 365, 341, 27822, 13, 77, 664, 4481, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1814, "seek": 852164, "start": 8526.64, "end": 8531.64, "text": " And, you know, for things like, I don't know,", "tokens": [50614, 400, 11, 291, 458, 11, 337, 721, 411, 11, 286, 500, 380, 458, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1815, "seek": 852164, "start": 8531.64, "end": 8534.64, "text": " if you have other classes or other outside functions,", "tokens": [50864, 498, 291, 362, 661, 5359, 420, 661, 2380, 6828, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1816, "seek": 852164, "start": 8534.64, "end": 8537.64, "text": " like, I mean, get batched by default isn't using this", "tokens": [51014, 411, 11, 286, 914, 11, 483, 15245, 292, 538, 7576, 1943, 380, 1228, 341, 51164], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1817, "seek": 852164, "start": 8537.64, "end": 8540.64, "text": " because it doesn't have the model thing passed into it.", "tokens": [51164, 570, 309, 1177, 380, 362, 264, 2316, 551, 4678, 666, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1818, "seek": 852164, "start": 8540.64, "end": 8545.64, "text": " But estimate loss does have model pass into it right here.", "tokens": [51314, 583, 12539, 4470, 775, 362, 2316, 1320, 666, 309, 558, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1819, "seek": 852164, "start": 8545.64, "end": 8550.64, "text": " So we just kind of want to make sure that it's not using any gradients.", "tokens": [51564, 407, 321, 445, 733, 295, 528, 281, 652, 988, 300, 309, 311, 406, 1228, 604, 2771, 2448, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09015851361410958, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.0034293399658054113}, {"id": 1820, "seek": 855064, "start": 8550.64, "end": 8552.64, "text": " We're going to reduce computation that way.", "tokens": [50364, 492, 434, 516, 281, 5407, 24903, 300, 636, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1821, "seek": 855064, "start": 8552.64, "end": 8554.64, "text": " So anyways, if you want,", "tokens": [50464, 407, 13448, 11, 498, 291, 528, 11, 50564], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1822, "seek": 855064, "start": 8554.64, "end": 8557.64, "text": " you can just take a quick readover of this,", "tokens": [50564, 291, 393, 445, 747, 257, 1702, 1401, 3570, 295, 341, 11, 50714], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1823, "seek": 855064, "start": 8557.64, "end": 8560.64, "text": " and it should overall make sense.", "tokens": [50714, 293, 309, 820, 4787, 652, 2020, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1824, "seek": 855064, "start": 8560.64, "end": 8564.64, "text": " Terms like .item.me are pretty common.", "tokens": [50864, 19835, 82, 411, 2411, 270, 443, 13, 1398, 366, 1238, 2689, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1825, "seek": 855064, "start": 8564.64, "end": 8567.64, "text": " A lot of the other things here, like model, X and Y,", "tokens": [51064, 316, 688, 295, 264, 661, 721, 510, 11, 411, 2316, 11, 1783, 293, 398, 11, 51214], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1826, "seek": 855064, "start": 8567.64, "end": 8569.64, "text": " we get our logits and our loss.", "tokens": [51214, 321, 483, 527, 3565, 1208, 293, 527, 4470, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1827, "seek": 855064, "start": 8569.64, "end": 8571.64, "text": " This stuff should make sense.", "tokens": [51314, 639, 1507, 820, 652, 2020, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1828, "seek": 855064, "start": 8571.64, "end": 8573.64, "text": " It should be pretty straightforward.", "tokens": [51414, 467, 820, 312, 1238, 15325, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1829, "seek": 855064, "start": 8573.64, "end": 8576.64, "text": " And only two other things I want to touch on", "tokens": [51514, 400, 787, 732, 661, 721, 286, 528, 281, 2557, 322, 51664], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1830, "seek": 855064, "start": 8576.64, "end": 8578.64, "text": " is model.eval and model.train,", "tokens": [51664, 307, 2316, 13, 68, 3337, 293, 2316, 13, 83, 7146, 11, 51764], "temperature": 0.0, "avg_logprob": -0.18254943907730223, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.016652552410960197}, {"id": 1831, "seek": 857864, "start": 8578.64, "end": 8581.64, "text": " because you probably have not seen these yet.", "tokens": [50364, 570, 291, 1391, 362, 406, 1612, 613, 1939, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1832, "seek": 857864, "start": 8581.64, "end": 8588.64, "text": " So model.train essentially puts the model in the training mode.", "tokens": [50514, 407, 2316, 13, 83, 7146, 4476, 8137, 264, 2316, 294, 264, 3097, 4391, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1833, "seek": 857864, "start": 8588.64, "end": 8590.64, "text": " The model learns from the data,", "tokens": [50864, 440, 2316, 27152, 490, 264, 1412, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1834, "seek": 857864, "start": 8590.64, "end": 8592.64, "text": " meaning the weights and biases,", "tokens": [50964, 3620, 264, 17443, 293, 32152, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1835, "seek": 857864, "start": 8592.64, "end": 8594.64, "text": " if we have, well, sometimes you only have weights,", "tokens": [51064, 498, 321, 362, 11, 731, 11, 2171, 291, 787, 362, 17443, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1836, "seek": 857864, "start": 8594.64, "end": 8596.64, "text": " sometimes you, you know,", "tokens": [51164, 2171, 291, 11, 291, 458, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1837, "seek": 857864, "start": 8596.64, "end": 8599.64, "text": " sometimes you have weights and biases, whatever it is,", "tokens": [51264, 2171, 291, 362, 17443, 293, 32152, 11, 2035, 309, 307, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1838, "seek": 857864, "start": 8599.64, "end": 8601.64, "text": " those are updated during this phase.", "tokens": [51414, 729, 366, 10588, 1830, 341, 5574, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1839, "seek": 857864, "start": 8601.64, "end": 8603.64, "text": " And then some layers of the model,", "tokens": [51514, 400, 550, 512, 7914, 295, 264, 2316, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1840, "seek": 857864, "start": 8603.64, "end": 8605.64, "text": " like dropout and batch normalization,", "tokens": [51614, 411, 3270, 346, 293, 15245, 2710, 2144, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11512255235151811, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.007575919385999441}, {"id": 1841, "seek": 860564, "start": 8605.64, "end": 8608.64, "text": " which you may not be familiar with yet,", "tokens": [50364, 597, 291, 815, 406, 312, 4963, 365, 1939, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1842, "seek": 860564, "start": 8608.64, "end": 8610.64, "text": " operate differently in training mode.", "tokens": [50514, 9651, 7614, 294, 3097, 4391, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1843, "seek": 860564, "start": 8610.64, "end": 8612.64, "text": " For example, dropout is active,", "tokens": [50614, 1171, 1365, 11, 3270, 346, 307, 4967, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1844, "seek": 860564, "start": 8612.64, "end": 8615.64, "text": " and what dropout does is this little hyperparameter", "tokens": [50714, 293, 437, 3270, 346, 775, 307, 341, 707, 9848, 2181, 335, 2398, 50864], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1845, "seek": 860564, "start": 8615.64, "end": 8617.64, "text": " that we add up here.", "tokens": [50864, 300, 321, 909, 493, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1846, "seek": 860564, "start": 8617.64, "end": 8619.64, "text": " It'll look like this.", "tokens": [50964, 467, 603, 574, 411, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1847, "seek": 860564, "start": 8619.64, "end": 8621.64, "text": " Dropout would be like 0.2.", "tokens": [51064, 17675, 346, 576, 312, 411, 1958, 13, 17, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1848, "seek": 860564, "start": 8621.64, "end": 8623.64, "text": " So pretty much what dropout does", "tokens": [51164, 407, 1238, 709, 437, 3270, 346, 775, 51264], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1849, "seek": 860564, "start": 8623.64, "end": 8626.64, "text": " is it's going to drop out random neurons in the network", "tokens": [51264, 307, 309, 311, 516, 281, 3270, 484, 4974, 22027, 294, 264, 3209, 51414], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1850, "seek": 860564, "start": 8626.64, "end": 8628.64, "text": " so that we don't overfit.", "tokens": [51414, 370, 300, 321, 500, 380, 670, 6845, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1851, "seek": 860564, "start": 8628.64, "end": 8632.64, "text": " And this is actually disabled in validation mode,", "tokens": [51514, 400, 341, 307, 767, 15191, 294, 24071, 4391, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1852, "seek": 860564, "start": 8632.64, "end": 8634.64, "text": " or eval mode.", "tokens": [51714, 420, 1073, 304, 4391, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09248405549584365, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.005059658549726009}, {"id": 1853, "seek": 863464, "start": 8634.64, "end": 8638.64, "text": " So this will just help our model sort of learn better", "tokens": [50364, 407, 341, 486, 445, 854, 527, 2316, 1333, 295, 1466, 1101, 50564], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1854, "seek": 863464, "start": 8638.64, "end": 8640.64, "text": " when it has little, like, pieces of noise", "tokens": [50564, 562, 309, 575, 707, 11, 411, 11, 3755, 295, 5658, 50664], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1855, "seek": 863464, "start": 8640.64, "end": 8643.64, "text": " and when things aren't in quite the right place", "tokens": [50664, 293, 562, 721, 3212, 380, 294, 1596, 264, 558, 1081, 50814], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1856, "seek": 863464, "start": 8643.64, "end": 8646.64, "text": " so that you don't have, you know, certain neurons in the network", "tokens": [50814, 370, 300, 291, 500, 380, 362, 11, 291, 458, 11, 1629, 22027, 294, 264, 3209, 50964], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1857, "seek": 863464, "start": 8646.64, "end": 8650.64, "text": " taking priority and just making a lot of the heavy decisions.", "tokens": [50964, 1940, 9365, 293, 445, 1455, 257, 688, 295, 264, 4676, 5327, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1858, "seek": 863464, "start": 8650.64, "end": 8651.64, "text": " We don't want that.", "tokens": [51164, 492, 500, 380, 528, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1859, "seek": 863464, "start": 8651.64, "end": 8654.64, "text": " So dropout will just sort of help our model train better", "tokens": [51214, 407, 3270, 346, 486, 445, 1333, 295, 854, 527, 2316, 3847, 1101, 51364], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1860, "seek": 863464, "start": 8654.64, "end": 8658.64, "text": " by taking 20% of the neurons out, 0.2, at random.", "tokens": [51364, 538, 1940, 945, 4, 295, 264, 22027, 484, 11, 1958, 13, 17, 11, 412, 4974, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1861, "seek": 863464, "start": 8658.64, "end": 8660.64, "text": " And that's all dropout does.", "tokens": [51564, 400, 300, 311, 439, 3270, 346, 775, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1862, "seek": 863464, "start": 8660.64, "end": 8663.64, "text": " So I'm just going to delete that for now.", "tokens": [51664, 407, 286, 478, 445, 516, 281, 12097, 300, 337, 586, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07224973520838228, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0007321212906390429}, {"id": 1863, "seek": 866364, "start": 8664.64, "end": 8667.64, "text": " And then, yeah, model that train.", "tokens": [50414, 400, 550, 11, 1338, 11, 2316, 300, 3847, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1864, "seek": 866364, "start": 8667.64, "end": 8670.64, "text": " Well, dropout is active during this phase,", "tokens": [50564, 1042, 11, 3270, 346, 307, 4967, 1830, 341, 5574, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1865, "seek": 866364, "start": 8670.64, "end": 8673.64, "text": " during training, randomly turning off,", "tokens": [50714, 1830, 3097, 11, 16979, 6246, 766, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1866, "seek": 866364, "start": 8673.64, "end": 8675.64, "text": " random neurons in the network.", "tokens": [50864, 4974, 22027, 294, 264, 3209, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1867, "seek": 866364, "start": 8675.64, "end": 8677.64, "text": " And this is to prevent overfitting.", "tokens": [50964, 400, 341, 307, 281, 4871, 670, 69, 2414, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1868, "seek": 866364, "start": 8677.64, "end": 8679.64, "text": " We went over overfitting earlier, I believe.", "tokens": [51064, 492, 1437, 670, 670, 69, 2414, 3071, 11, 286, 1697, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1869, "seek": 866364, "start": 8679.64, "end": 8682.64, "text": " And as for evaluation mode,", "tokens": [51164, 400, 382, 337, 13344, 4391, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1870, "seek": 866364, "start": 8682.64, "end": 8686.64, "text": " evaluation mode is used when the model's being evaluated", "tokens": [51314, 13344, 4391, 307, 1143, 562, 264, 2316, 311, 885, 25509, 51514], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1871, "seek": 866364, "start": 8686.64, "end": 8688.64, "text": " or tested just like it sounds.", "tokens": [51514, 420, 8246, 445, 411, 309, 3263, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1872, "seek": 866364, "start": 8688.64, "end": 8689.64, "text": " It's being trained.", "tokens": [51614, 467, 311, 885, 8895, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1873, "seek": 866364, "start": 8689.64, "end": 8692.64, "text": " What the other mode is being validated or tested.", "tokens": [51664, 708, 264, 661, 4391, 307, 885, 40693, 420, 8246, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11864061522902104, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.00028683445998467505}, {"id": 1874, "seek": 869264, "start": 8692.64, "end": 8696.64, "text": " And layers like dropout and batch normalization", "tokens": [50364, 400, 7914, 411, 3270, 346, 293, 15245, 2710, 2144, 50564], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1875, "seek": 869264, "start": 8696.64, "end": 8698.64, "text": " behave differently in this mode.", "tokens": [50564, 15158, 7614, 294, 341, 4391, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1876, "seek": 869264, "start": 8698.64, "end": 8700.64, "text": " Like dropout is turned off in the evaluation, right?", "tokens": [50664, 1743, 3270, 346, 307, 3574, 766, 294, 264, 13344, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1877, "seek": 869264, "start": 8700.64, "end": 8702.64, "text": " Because what we're actually doing", "tokens": [50764, 1436, 437, 321, 434, 767, 884, 50864], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1878, "seek": 869264, "start": 8702.64, "end": 8704.64, "text": " is we're using the entire network.", "tokens": [50864, 307, 321, 434, 1228, 264, 2302, 3209, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1879, "seek": 869264, "start": 8704.64, "end": 8707.64, "text": " We want everything to be working sort of together.", "tokens": [50964, 492, 528, 1203, 281, 312, 1364, 1333, 295, 1214, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1880, "seek": 869264, "start": 8707.64, "end": 8710.64, "text": " And we want to actually see how well does it perform.", "tokens": [51114, 400, 321, 528, 281, 767, 536, 577, 731, 775, 309, 2042, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1881, "seek": 869264, "start": 8710.64, "end": 8712.64, "text": " Training mode is when we're just, you know,", "tokens": [51264, 20620, 4391, 307, 562, 321, 434, 445, 11, 291, 458, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1882, "seek": 869264, "start": 8712.64, "end": 8715.64, "text": " sampling, doing weird things to try to challenge the network", "tokens": [51364, 21179, 11, 884, 3657, 721, 281, 853, 281, 3430, 264, 3209, 51514], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1883, "seek": 869264, "start": 8715.64, "end": 8716.64, "text": " as we're training it.", "tokens": [51514, 382, 321, 434, 3097, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1884, "seek": 869264, "start": 8716.64, "end": 8719.64, "text": " And then evaluating or validation would be", "tokens": [51564, 400, 550, 27479, 420, 24071, 576, 312, 51714], "temperature": 0.0, "avg_logprob": -0.08091923447905994, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0014321490889415145}, {"id": 1885, "seek": 871964, "start": 8719.64, "end": 8722.64, "text": " when we just get the network in its optimal form", "tokens": [50364, 562, 321, 445, 483, 264, 3209, 294, 1080, 16252, 1254, 50514], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1886, "seek": 871964, "start": 8722.64, "end": 8725.64, "text": " and we're trying to see how good of results it produces.", "tokens": [50514, 293, 321, 434, 1382, 281, 536, 577, 665, 295, 3542, 309, 14725, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1887, "seek": 871964, "start": 8725.64, "end": 8727.64, "text": " So that's what a val is.", "tokens": [50664, 407, 300, 311, 437, 257, 1323, 307, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1888, "seek": 871964, "start": 8727.64, "end": 8729.64, "text": " And the reason we switched into a val here", "tokens": [50764, 400, 264, 1778, 321, 16858, 666, 257, 1323, 510, 50864], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1889, "seek": 871964, "start": 8729.64, "end": 8732.64, "text": " is just because, well, we are testing the model.", "tokens": [50864, 307, 445, 570, 11, 731, 11, 321, 366, 4997, 264, 2316, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1890, "seek": 871964, "start": 8732.64, "end": 8734.64, "text": " We want to see, you know, how well it does", "tokens": [51014, 492, 528, 281, 536, 11, 291, 458, 11, 577, 731, 309, 775, 51114], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1891, "seek": 871964, "start": 8734.64, "end": 8738.64, "text": " with any given set of data from a get batch.", "tokens": [51114, 365, 604, 2212, 992, 295, 1412, 490, 257, 483, 15245, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1892, "seek": 871964, "start": 8738.64, "end": 8740.64, "text": " And we don't actually need to train here.", "tokens": [51314, 400, 321, 500, 380, 767, 643, 281, 3847, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1893, "seek": 871964, "start": 8740.64, "end": 8743.64, "text": " If there was no training, this would not be here", "tokens": [51414, 759, 456, 390, 572, 3097, 11, 341, 576, 406, 312, 510, 51564], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1894, "seek": 871964, "start": 8743.64, "end": 8746.64, "text": " because we would not be using any gradients.", "tokens": [51564, 570, 321, 576, 406, 312, 1228, 604, 2771, 2448, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09299812465906143, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.001133482437580824}, {"id": 1895, "seek": 874664, "start": 8747.64, "end": 8750.64, "text": " So we would be using gradients if training was on.", "tokens": [50414, 407, 321, 576, 312, 1228, 2771, 2448, 498, 3097, 390, 322, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1896, "seek": 874664, "start": 8750.64, "end": 8753.64, "text": " Anyways, that's estimate loss for you.", "tokens": [50564, 15585, 11, 300, 311, 12539, 4470, 337, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1897, "seek": 874664, "start": 8753.64, "end": 8757.64, "text": " This function is, you know, just generally good", "tokens": [50714, 639, 2445, 307, 11, 291, 458, 11, 445, 5101, 665, 50914], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1898, "seek": 874664, "start": 8757.64, "end": 8759.64, "text": " to have a data science.", "tokens": [50914, 281, 362, 257, 1412, 3497, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1899, "seek": 874664, "start": 8759.64, "end": 8762.64, "text": " Your train and validation splits, whatnot.", "tokens": [51014, 2260, 3847, 293, 24071, 37741, 11, 25882, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1900, "seek": 874664, "start": 8762.64, "end": 8765.64, "text": " And yeah, good for reporting.", "tokens": [51164, 400, 1338, 11, 665, 337, 10031, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1901, "seek": 874664, "start": 8765.64, "end": 8767.64, "text": " You know how it is.", "tokens": [51314, 509, 458, 577, 309, 307, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1902, "seek": 874664, "start": 8767.64, "end": 8769.64, "text": " And we can go ahead and add this down here.", "tokens": [51414, 400, 321, 393, 352, 2286, 293, 909, 341, 760, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1903, "seek": 874664, "start": 8769.64, "end": 8771.64, "text": " So there's something soon.", "tokens": [51514, 407, 456, 311, 746, 2321, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12529164364463405, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.002631551818922162}, {"id": 1904, "seek": 877164, "start": 8771.64, "end": 8778.64, "text": " We'll go losses is equal to estimates loss.", "tokens": [50364, 492, 603, 352, 15352, 307, 2681, 281, 20561, 4470, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1905, "seek": 877164, "start": 8778.64, "end": 8784.64, "text": " And then we can go ahead and put a...", "tokens": [50714, 400, 550, 321, 393, 352, 2286, 293, 829, 257, 485, 51014], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1906, "seek": 877164, "start": 8784.64, "end": 8788.64, "text": " Yeah, we don't actually have to put anything in here.", "tokens": [51014, 865, 11, 321, 500, 380, 767, 362, 281, 829, 1340, 294, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1907, "seek": 877164, "start": 8788.64, "end": 8789.64, "text": " Cool.", "tokens": [51214, 8561, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1908, "seek": 877164, "start": 8789.64, "end": 8792.64, "text": " So now let's go ahead and run this.", "tokens": [51264, 407, 586, 718, 311, 352, 2286, 293, 1190, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1909, "seek": 877164, "start": 8792.64, "end": 8794.64, "text": " Let me run from the start here.", "tokens": [51414, 961, 385, 1190, 490, 264, 722, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1910, "seek": 877164, "start": 8794.64, "end": 8798.64, "text": " Boom, boom, boom, boom, boom, boom.", "tokens": [51514, 15523, 11, 9351, 11, 9351, 11, 9351, 11, 9351, 11, 9351, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14397977590560912, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0033242146018892527}, {"id": 1911, "seek": 880164, "start": 8802.64, "end": 8804.64, "text": " Perfect.", "tokens": [50414, 10246, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1912, "seek": 880164, "start": 8811.64, "end": 8813.64, "text": " I'm running for 10,000 iterations.", "tokens": [50864, 286, 478, 2614, 337, 1266, 11, 1360, 36540, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1913, "seek": 880164, "start": 8813.64, "end": 8815.64, "text": " That's interesting.", "tokens": [50964, 663, 311, 1880, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1914, "seek": 880164, "start": 8815.64, "end": 8817.64, "text": " Okay.", "tokens": [51064, 1033, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1915, "seek": 880164, "start": 8817.64, "end": 8821.64, "text": " So, yes.", "tokens": [51164, 407, 11, 2086, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1916, "seek": 880164, "start": 8821.64, "end": 8823.64, "text": " So what I'm going to do actually here", "tokens": [51364, 407, 437, 286, 478, 516, 281, 360, 767, 510, 51464], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1917, "seek": 880164, "start": 8823.64, "end": 8825.64, "text": " is you can see this loss part is weird.", "tokens": [51464, 307, 291, 393, 536, 341, 4470, 644, 307, 3657, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1918, "seek": 880164, "start": 8825.64, "end": 8828.64, "text": " So I'm actually going to change this up.", "tokens": [51564, 407, 286, 478, 767, 516, 281, 1319, 341, 493, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15117373185999253, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0010986723937094212}, {"id": 1919, "seek": 882864, "start": 8828.64, "end": 8832.64, "text": " And I'm just going to switch it to...", "tokens": [50364, 400, 286, 478, 445, 516, 281, 3679, 309, 281, 485, 50564], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1920, "seek": 882864, "start": 8832.64, "end": 8835.64, "text": " We're going to go train loss.", "tokens": [50564, 492, 434, 516, 281, 352, 3847, 4470, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1921, "seek": 882864, "start": 8835.64, "end": 8837.64, "text": " And we're going to go losses.", "tokens": [50714, 400, 321, 434, 516, 281, 352, 15352, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1922, "seek": 882864, "start": 8837.64, "end": 8840.64, "text": " And we're going to do the train split.", "tokens": [50814, 400, 321, 434, 516, 281, 360, 264, 3847, 7472, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1923, "seek": 882864, "start": 8840.64, "end": 8843.64, "text": " And then we're going to go over here", "tokens": [50964, 400, 550, 321, 434, 516, 281, 352, 670, 510, 51114], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1924, "seek": 882864, "start": 8843.64, "end": 8847.64, "text": " and just do the validation loss.", "tokens": [51114, 293, 445, 360, 264, 24071, 4470, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1925, "seek": 882864, "start": 8847.64, "end": 8850.64, "text": " We can do validation or just val for short.", "tokens": [51314, 492, 393, 360, 24071, 420, 445, 1323, 337, 2099, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1926, "seek": 882864, "start": 8850.64, "end": 8855.64, "text": " And I'm going to make it consistent here.", "tokens": [51464, 400, 286, 478, 516, 281, 652, 309, 8398, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12141569985283746, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0014778351178392768}, {"id": 1927, "seek": 885564, "start": 8855.64, "end": 8858.64, "text": " So we have a colon there, a colon here.", "tokens": [50364, 407, 321, 362, 257, 8255, 456, 11, 257, 8255, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1928, "seek": 885564, "start": 8858.64, "end": 8864.64, "text": " And then we just go losses and do val.", "tokens": [50514, 400, 550, 321, 445, 352, 15352, 293, 360, 1323, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1929, "seek": 885564, "start": 8864.64, "end": 8865.64, "text": " Cool.", "tokens": [50814, 8561, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1930, "seek": 885564, "start": 8865.64, "end": 8870.64, "text": " So I'm going to reduce these max editors up here to only 1,000.", "tokens": [50864, 407, 286, 478, 516, 281, 5407, 613, 11469, 31446, 493, 510, 281, 787, 502, 11, 1360, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1931, "seek": 885564, "start": 8870.64, "end": 8872.64, "text": " Run that.", "tokens": [51114, 8950, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1932, "seek": 885564, "start": 8872.64, "end": 8874.64, "text": " Run this.", "tokens": [51214, 8950, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1933, "seek": 885564, "start": 8874.64, "end": 8877.64, "text": " Oh, somebody did a match.", "tokens": [51314, 876, 11, 2618, 630, 257, 2995, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21509915239670696, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.0016742768930271268}, {"id": 1934, "seek": 888564, "start": 8886.64, "end": 8888.64, "text": " Okay.", "tokens": [50414, 1033, 13, 50514], "temperature": 0.0, "avg_logprob": -0.5721954504648844, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.8080384135246277}, {"id": 1935, "seek": 888864, "start": 8888.64, "end": 8890.64, "text": " Okay.", "tokens": [50364, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.7080005009969076, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.29701533913612366}, {"id": 1936, "seek": 889064, "start": 8890.64, "end": 8892.64, "text": " Okay.", "tokens": [50364, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.32414136992560494, "compression_ratio": 0.8490566037735849, "no_speech_prob": 0.06950412690639496}, {"id": 1937, "seek": 889064, "start": 8914.64, "end": 8916.64, "text": " Yes.", "tokens": [51564, 1079, 13, 51664], "temperature": 0.0, "avg_logprob": -0.32414136992560494, "compression_ratio": 0.8490566037735849, "no_speech_prob": 0.06950412690639496}, {"id": 1938, "seek": 889064, "start": 8916.64, "end": 8918.64, "text": " So what actually happened here was", "tokens": [51664, 407, 437, 767, 2011, 510, 390, 51764], "temperature": 0.0, "avg_logprob": -0.32414136992560494, "compression_ratio": 0.8490566037735849, "no_speech_prob": 0.06950412690639496}, {"id": 1939, "seek": 891864, "start": 8918.64, "end": 8921.64, "text": " when we were doing these little ticks,", "tokens": [50364, 562, 321, 645, 884, 613, 707, 42475, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1940, "seek": 891864, "start": 8921.64, "end": 8924.64, "text": " what was happening is these were matching up with these.", "tokens": [50514, 437, 390, 2737, 307, 613, 645, 14324, 493, 365, 613, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1941, "seek": 891864, "start": 8924.64, "end": 8927.64, "text": " And it was telling us, oh, you can't do that.", "tokens": [50664, 400, 309, 390, 3585, 505, 11, 1954, 11, 291, 393, 380, 360, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1942, "seek": 891864, "start": 8927.64, "end": 8929.64, "text": " You can't start here and then end there", "tokens": [50814, 509, 393, 380, 722, 510, 293, 550, 917, 456, 50914], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1943, "seek": 891864, "start": 8929.64, "end": 8931.64, "text": " and have all this weird stuff.", "tokens": [50914, 293, 362, 439, 341, 3657, 1507, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1944, "seek": 891864, "start": 8931.64, "end": 8933.64, "text": " Like, you can't do that.", "tokens": [51014, 1743, 11, 291, 393, 380, 360, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1945, "seek": 891864, "start": 8933.64, "end": 8936.64, "text": " So pretty much we just need to make sure that these are different.", "tokens": [51114, 407, 1238, 709, 321, 445, 643, 281, 652, 988, 300, 613, 366, 819, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1946, "seek": 891864, "start": 8936.64, "end": 8938.64, "text": " So I'm going to do a double quote instead of single", "tokens": [51264, 407, 286, 478, 516, 281, 360, 257, 3834, 6513, 2602, 295, 2167, 51364], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1947, "seek": 891864, "start": 8938.64, "end": 8940.64, "text": " and then double quote to finish it off.", "tokens": [51364, 293, 550, 3834, 6513, 281, 2413, 309, 766, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1948, "seek": 891864, "start": 8940.64, "end": 8943.64, "text": " And as you can see, this worked out here.", "tokens": [51464, 400, 382, 291, 393, 536, 11, 341, 2732, 484, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1949, "seek": 891864, "start": 8943.64, "end": 8945.64, "text": " So I'll just run that again", "tokens": [51614, 407, 286, 603, 445, 1190, 300, 797, 51714], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1950, "seek": 891864, "start": 8945.64, "end": 8947.64, "text": " so you guys can see what this looks like.", "tokens": [51714, 370, 291, 1074, 393, 536, 437, 341, 1542, 411, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1129660279783484, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.26247432827949524}, {"id": 1951, "seek": 894764, "start": 8947.64, "end": 8948.64, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1952, "seek": 894764, "start": 8948.64, "end": 8951.64, "text": " Because we have, you know, a lot of decimal places.", "tokens": [50414, 1436, 321, 362, 11, 291, 458, 11, 257, 688, 295, 26601, 3190, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1953, "seek": 894764, "start": 8951.64, "end": 8956.64, "text": " So what we can actually do here is we can add in a little format", "tokens": [50564, 407, 437, 321, 393, 767, 360, 510, 307, 321, 393, 909, 294, 257, 707, 7877, 50814], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1954, "seek": 894764, "start": 8956.64, "end": 8959.64, "text": " or a little decimal place reducer, if you call it,", "tokens": [50814, 420, 257, 707, 26601, 1081, 2783, 1776, 11, 498, 291, 818, 309, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1955, "seek": 894764, "start": 8959.64, "end": 8963.64, "text": " just for, you know, so you can read it.", "tokens": [50964, 445, 337, 11, 291, 458, 11, 370, 291, 393, 1401, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1956, "seek": 894764, "start": 8963.64, "end": 8965.64, "text": " So it's not like some weird decimal number", "tokens": [51164, 407, 309, 311, 406, 411, 512, 3657, 26601, 1230, 51264], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1957, "seek": 894764, "start": 8965.64, "end": 8967.64, "text": " and you're like, oh, does this eight matter?", "tokens": [51264, 293, 291, 434, 411, 11, 1954, 11, 775, 341, 3180, 1871, 30, 51364], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1958, "seek": 894764, "start": 8967.64, "end": 8968.64, "text": " Probably not.", "tokens": [51364, 9210, 406, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1959, "seek": 894764, "start": 8968.64, "end": 8970.64, "text": " Just like the first three digits, maybe.", "tokens": [51414, 1449, 411, 264, 700, 1045, 27011, 11, 1310, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1960, "seek": 894764, "start": 8970.64, "end": 8973.64, "text": " So all we can do here is just add in,", "tokens": [51514, 407, 439, 321, 393, 360, 510, 307, 445, 909, 294, 11, 51664], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1961, "seek": 894764, "start": 8973.64, "end": 8975.64, "text": " I believe this is how it goes.", "tokens": [51664, 286, 1697, 341, 307, 577, 309, 1709, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13074051783635066, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.13105575740337372}, {"id": 1962, "seek": 897764, "start": 8977.64, "end": 8980.64, "text": " I don't think it's the other way.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 264, 661, 636, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1963, "seek": 897764, "start": 8980.64, "end": 8982.64, "text": " We'll find out.", "tokens": [50514, 492, 603, 915, 484, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1964, "seek": 897764, "start": 8982.64, "end": 8986.64, "text": " Some stuff in Python is extremely confusing to me.", "tokens": [50614, 2188, 1507, 294, 15329, 307, 4664, 13181, 281, 385, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1965, "seek": 897764, "start": 8986.64, "end": 8989.64, "text": " But there we go.", "tokens": [50814, 583, 456, 321, 352, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1966, "seek": 897764, "start": 8989.64, "end": 8990.64, "text": " So I got it right.", "tokens": [50964, 407, 286, 658, 309, 558, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1967, "seek": 897764, "start": 8990.64, "end": 8992.64, "text": " Go on and then period.", "tokens": [51014, 1037, 322, 293, 550, 2896, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1968, "seek": 897764, "start": 8992.64, "end": 8994.64, "text": " And as you can see, we have those digits reduced.", "tokens": [51114, 400, 382, 291, 393, 536, 11, 321, 362, 729, 27011, 9212, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1969, "seek": 897764, "start": 8994.64, "end": 8997.64, "text": " So I can actually put this down to 3F.", "tokens": [51214, 407, 286, 393, 767, 829, 341, 760, 281, 805, 37, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1970, "seek": 897764, "start": 9003.64, "end": 9004.64, "text": " Wonderful.", "tokens": [51664, 22768, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13667121974901222, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.0019876693841069937}, {"id": 1971, "seek": 900464, "start": 9004.64, "end": 9009.64, "text": " So we have our train loss and our validation loss.", "tokens": [50364, 407, 321, 362, 527, 3847, 4470, 293, 527, 24071, 4470, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1972, "seek": 900464, "start": 9009.64, "end": 9011.64, "text": " Great job you made it this far.", "tokens": [50614, 3769, 1691, 291, 1027, 309, 341, 1400, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1973, "seek": 900464, "start": 9011.64, "end": 9013.64, "text": " This is absolutely amazing.", "tokens": [50714, 639, 307, 3122, 2243, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1974, "seek": 900464, "start": 9013.64, "end": 9014.64, "text": " This is insane.", "tokens": [50814, 639, 307, 10838, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1975, "seek": 900464, "start": 9014.64, "end": 9016.64, "text": " You've gotten this far in the video.", "tokens": [50864, 509, 600, 5768, 341, 1400, 294, 264, 960, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1976, "seek": 900464, "start": 9016.64, "end": 9019.64, "text": " We've covered all the basics, everything you need to know", "tokens": [50964, 492, 600, 5343, 439, 264, 14688, 11, 1203, 291, 643, 281, 458, 51114], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1977, "seek": 900464, "start": 9019.64, "end": 9022.64, "text": " about background language models, optimizers,", "tokens": [51114, 466, 3678, 2856, 5245, 11, 5028, 22525, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1978, "seek": 900464, "start": 9022.64, "end": 9025.64, "text": " training loops, reporting losses.", "tokens": [51264, 3097, 16121, 11, 10031, 15352, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1979, "seek": 900464, "start": 9025.64, "end": 9028.64, "text": " I can't even name everything we've done because it's so much.", "tokens": [51414, 286, 393, 380, 754, 1315, 1203, 321, 600, 1096, 570, 309, 311, 370, 709, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1980, "seek": 900464, "start": 9028.64, "end": 9031.64, "text": " So congratulations that you made it this far.", "tokens": [51564, 407, 13568, 300, 291, 1027, 309, 341, 1400, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1981, "seek": 900464, "start": 9031.64, "end": 9033.64, "text": " You should go take a quick break.", "tokens": [51714, 509, 820, 352, 747, 257, 1702, 1821, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08005479604256253, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.025554988533258438}, {"id": 1982, "seek": 903364, "start": 9033.64, "end": 9037.64, "text": " Give yourself a pat on the back and get ready for the next part", "tokens": [50364, 5303, 1803, 257, 1947, 322, 264, 646, 293, 483, 1919, 337, 264, 958, 644, 50564], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1983, "seek": 903364, "start": 9037.64, "end": 9039.64, "text": " here because it's going to be absolutely insane.", "tokens": [50564, 510, 570, 309, 311, 516, 281, 312, 3122, 10838, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1984, "seek": 903364, "start": 9039.64, "end": 9043.64, "text": " We're going to dig into literally state of the art language", "tokens": [50664, 492, 434, 516, 281, 2528, 666, 3736, 1785, 295, 264, 1523, 2856, 50864], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1985, "seek": 903364, "start": 9043.64, "end": 9047.64, "text": " models and how we can build them from scratch,", "tokens": [50864, 5245, 293, 577, 321, 393, 1322, 552, 490, 8459, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1986, "seek": 903364, "start": 9047.64, "end": 9049.64, "text": " or at least how we can pre-train them.", "tokens": [51064, 420, 412, 1935, 577, 321, 393, 659, 12, 83, 7146, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1987, "seek": 903364, "start": 9049.64, "end": 9052.64, "text": " And some of these terms are going to seem a little bit out", "tokens": [51164, 400, 512, 295, 613, 2115, 366, 516, 281, 1643, 257, 707, 857, 484, 51314], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1988, "seek": 903364, "start": 9052.64, "end": 9056.64, "text": " there, but I can ensure you by the end of this next section", "tokens": [51314, 456, 11, 457, 286, 393, 5586, 291, 538, 264, 917, 295, 341, 958, 3541, 51514], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1989, "seek": 903364, "start": 9056.64, "end": 9060.64, "text": " here, you're going to have a pretty good understanding", "tokens": [51514, 510, 11, 291, 434, 516, 281, 362, 257, 1238, 665, 3701, 51714], "temperature": 0.0, "avg_logprob": -0.08817517966554876, "compression_ratio": 1.6875, "no_speech_prob": 0.031077394261956215}, {"id": 1990, "seek": 906064, "start": 9060.64, "end": 9063.64, "text": " about the state of language models right now.", "tokens": [50364, 466, 264, 1785, 295, 2856, 5245, 558, 586, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1991, "seek": 906064, "start": 9063.64, "end": 9067.64, "text": " So go take a quick break and I'll see you back in a little bit.", "tokens": [50514, 407, 352, 747, 257, 1702, 1821, 293, 286, 603, 536, 291, 646, 294, 257, 707, 857, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1992, "seek": 906064, "start": 9067.64, "end": 9071.64, "text": " So there's something I'd like to clear up and actually sort of", "tokens": [50714, 407, 456, 311, 746, 286, 1116, 411, 281, 1850, 493, 293, 767, 1333, 295, 50914], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1993, "seek": 906064, "start": 9071.64, "end": 9075.64, "text": " lied to you a little bit, a little while back in this course", "tokens": [50914, 20101, 281, 291, 257, 707, 857, 11, 257, 707, 1339, 646, 294, 341, 1164, 51114], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1994, "seek": 906064, "start": 9075.64, "end": 9077.64, "text": " about what normalizing is.", "tokens": [51114, 466, 437, 2710, 3319, 307, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1995, "seek": 906064, "start": 9077.64, "end": 9081.64, "text": " So I recall we were talking about the softmax function", "tokens": [51214, 407, 286, 9901, 321, 645, 1417, 466, 264, 2787, 41167, 2445, 51414], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1996, "seek": 906064, "start": 9081.64, "end": 9084.64, "text": " and normalizing vectors.", "tokens": [51414, 293, 2710, 3319, 18875, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1997, "seek": 906064, "start": 9084.64, "end": 9089.64, "text": " So the softmax is definitely a form of normalization,", "tokens": [51564, 407, 264, 2787, 41167, 307, 2138, 257, 1254, 295, 2710, 2144, 11, 51814], "temperature": 0.0, "avg_logprob": -0.08945580248562794, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.017705457285046577}, {"id": 1998, "seek": 908964, "start": 9089.64, "end": 9091.64, "text": " but there are many forms.", "tokens": [50364, 457, 456, 366, 867, 6422, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 1999, "seek": 908964, "start": 9091.64, "end": 9095.64, "text": " There are not just a few or like there's not just one or two normalizations.", "tokens": [50464, 821, 366, 406, 445, 257, 1326, 420, 411, 456, 311, 406, 445, 472, 420, 732, 2710, 14455, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 2000, "seek": 908964, "start": 9095.64, "end": 9100.64, "text": " There are actually many of them and I have them on my second monitor here,", "tokens": [50664, 821, 366, 767, 867, 295, 552, 293, 286, 362, 552, 322, 452, 1150, 6002, 510, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 2001, "seek": 908964, "start": 9100.64, "end": 9104.64, "text": " but I don't want to just dump that library of information on your head", "tokens": [50914, 457, 286, 500, 380, 528, 281, 445, 11430, 300, 6405, 295, 1589, 322, 428, 1378, 51114], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 2002, "seek": 908964, "start": 9104.64, "end": 9106.64, "text": " because that's not how you learn.", "tokens": [51114, 570, 300, 311, 406, 577, 291, 1466, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 2003, "seek": 908964, "start": 9106.64, "end": 9109.64, "text": " So what we're going to do is we're going to plug this into GPT-4.", "tokens": [51214, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 5452, 341, 666, 26039, 51, 12, 19, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09194106662396304, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.005909849423915148}, {"id": 2004, "seek": 910964, "start": 9110.64, "end": 9126.64, "text": " I'm going to say, can you list all the forms of normalizing in machine learning?", "tokens": [50414, 286, 478, 516, 281, 584, 11, 393, 291, 1329, 439, 264, 6422, 295, 2710, 3319, 294, 3479, 2539, 30, 51214], "temperature": 0.0, "avg_logprob": -0.13586488299899632, "compression_ratio": 1.1935483870967742, "no_speech_prob": 0.6183902621269226}, {"id": 2005, "seek": 910964, "start": 9126.64, "end": 9136.64, "text": " And how are they different from one another?", "tokens": [51214, 400, 577, 366, 436, 819, 490, 472, 1071, 30, 51714], "temperature": 0.0, "avg_logprob": -0.13586488299899632, "compression_ratio": 1.1935483870967742, "no_speech_prob": 0.6183902621269226}, {"id": 2006, "seek": 910964, "start": 9136.64, "end": 9138.64, "text": " GPT-4 is a great tool.", "tokens": [51714, 26039, 51, 12, 19, 307, 257, 869, 2290, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13586488299899632, "compression_ratio": 1.1935483870967742, "no_speech_prob": 0.6183902621269226}, {"id": 2007, "seek": 913864, "start": 9138.64, "end": 9142.64, "text": " If you don't already use it, I highly suggest you use it,", "tokens": [50364, 759, 291, 500, 380, 1217, 764, 309, 11, 286, 5405, 3402, 291, 764, 309, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2008, "seek": 913864, "start": 9142.64, "end": 9146.64, "text": " or even GPT-3.5, which is the free version.", "tokens": [50564, 420, 754, 26039, 51, 12, 18, 13, 20, 11, 597, 307, 264, 1737, 3037, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2009, "seek": 913864, "start": 9146.64, "end": 9150.64, "text": " But yeah, it's a great tool for just quickly learning anything", "tokens": [50764, 583, 1338, 11, 309, 311, 257, 869, 2290, 337, 445, 2661, 2539, 1340, 50964], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2010, "seek": 913864, "start": 9150.64, "end": 9155.64, "text": " and then have it give you example practice questions with answers", "tokens": [50964, 293, 550, 362, 309, 976, 291, 1365, 3124, 1651, 365, 6338, 51214], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2011, "seek": 913864, "start": 9155.64, "end": 9158.64, "text": " so you can learn topics in literally minutes", "tokens": [51214, 370, 291, 393, 1466, 8378, 294, 3736, 2077, 51364], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2012, "seek": 913864, "start": 9158.64, "end": 9163.64, "text": " that would take you several lectures to learn in a university course.", "tokens": [51364, 300, 576, 747, 291, 2940, 16564, 281, 1466, 294, 257, 5454, 1164, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2013, "seek": 913864, "start": 9163.64, "end": 9167.64, "text": " But anyways, there's a few here.", "tokens": [51614, 583, 13448, 11, 456, 311, 257, 1326, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09555879592895508, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.007457670755684376}, {"id": 2014, "seek": 916764, "start": 9167.64, "end": 9171.64, "text": " So min-max normalization, yep.", "tokens": [50364, 407, 923, 12, 41167, 2710, 2144, 11, 18633, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2015, "seek": 916764, "start": 9171.64, "end": 9174.64, "text": " z-score, decimal scaling, mean normalization,", "tokens": [50564, 710, 12, 4417, 418, 11, 26601, 21589, 11, 914, 2710, 2144, 11, 50714], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2016, "seek": 916764, "start": 9174.64, "end": 9179.64, "text": " unit vector, or layer 2, robust scaling, power transformations.", "tokens": [50714, 4985, 8062, 11, 420, 4583, 568, 11, 13956, 21589, 11, 1347, 34852, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2017, "seek": 916764, "start": 9179.64, "end": 9180.64, "text": " Okay.", "tokens": [50964, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2018, "seek": 916764, "start": 9180.64, "end": 9183.64, "text": " So yeah, and then softmax would be another one.", "tokens": [51014, 407, 1338, 11, 293, 550, 2787, 41167, 576, 312, 1071, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2019, "seek": 916764, "start": 9183.64, "end": 9189.64, "text": " What about softmax?", "tokens": [51164, 708, 466, 2787, 41167, 30, 51464], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2020, "seek": 916764, "start": 9189.64, "end": 9191.64, "text": " It is in data type normalization,", "tokens": [51464, 467, 307, 294, 1412, 2010, 2710, 2144, 11, 51564], "temperature": 0.0, "avg_logprob": -0.21359453572855366, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.011684357188642025}, {"id": 2021, "seek": 919164, "start": 9191.64, "end": 9199.64, "text": " but it's not typically using from normalizing input data.", "tokens": [50364, 457, 309, 311, 406, 5850, 1228, 490, 2710, 3319, 4846, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2022, "seek": 919164, "start": 9199.64, "end": 9201.64, "text": " It's commonly used in the output layer.", "tokens": [50764, 467, 311, 12719, 1143, 294, 264, 5598, 4583, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2023, "seek": 919164, "start": 9201.64, "end": 9204.64, "text": " So softmax is a type of normalization,", "tokens": [50864, 407, 2787, 41167, 307, 257, 2010, 295, 2710, 2144, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2024, "seek": 919164, "start": 9204.64, "end": 9209.64, "text": " but it's not used for normalizing input data.", "tokens": [51014, 457, 309, 311, 406, 1143, 337, 2710, 3319, 4846, 1412, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2025, "seek": 919164, "start": 9209.64, "end": 9216.64, "text": " And honestly, we've proved that here by actually producing some probabilities.", "tokens": [51264, 400, 6095, 11, 321, 600, 14617, 300, 510, 538, 767, 10501, 512, 33783, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2026, "seek": 919164, "start": 9216.64, "end": 9219.64, "text": " So this isn't something we used in our forward pass.", "tokens": [51614, 407, 341, 1943, 380, 746, 321, 1143, 294, 527, 2128, 1320, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09068038405441656, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0030749046709388494}, {"id": 2027, "seek": 921964, "start": 9219.64, "end": 9221.64, "text": " This is something we used in our generate function", "tokens": [50364, 639, 307, 746, 321, 1143, 294, 527, 8460, 2445, 50464], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2028, "seek": 921964, "start": 9221.64, "end": 9224.64, "text": " to get a bunch of probabilities from our logits.", "tokens": [50464, 281, 483, 257, 3840, 295, 33783, 490, 527, 3565, 1208, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2029, "seek": 921964, "start": 9224.64, "end": 9227.64, "text": " So this is, yeah, interesting.", "tokens": [50614, 407, 341, 307, 11, 1338, 11, 1880, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2030, "seek": 921964, "start": 9227.64, "end": 9230.64, "text": " It's good to just figure little things like these out for, you know,", "tokens": [50764, 467, 311, 665, 281, 445, 2573, 707, 721, 411, 613, 484, 337, 11, 291, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2031, "seek": 921964, "start": 9230.64, "end": 9234.64, "text": " just to be, put you on the edge a little bit more", "tokens": [50914, 445, 281, 312, 11, 829, 291, 322, 264, 4691, 257, 707, 857, 544, 51114], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2032, "seek": 921964, "start": 9234.64, "end": 9237.64, "text": " for the future when it comes to engineering these kind of things.", "tokens": [51114, 337, 264, 2027, 562, 309, 1487, 281, 7043, 613, 733, 295, 721, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2033, "seek": 921964, "start": 9237.64, "end": 9239.64, "text": " All right, great.", "tokens": [51264, 1057, 558, 11, 869, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2034, "seek": 921964, "start": 9239.64, "end": 9242.64, "text": " So the next thing I want to touch on is activation functions.", "tokens": [51364, 407, 264, 958, 551, 286, 528, 281, 2557, 322, 307, 24433, 6828, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2035, "seek": 921964, "start": 9242.64, "end": 9246.64, "text": " And activation functions are extremely important", "tokens": [51514, 400, 24433, 6828, 366, 4664, 1021, 51714], "temperature": 0.0, "avg_logprob": -0.10054914574874074, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.00180997874122113}, {"id": 2036, "seek": 924664, "start": 9246.64, "end": 9252.64, "text": " in offering new ways of changing our inputs that are not linear.", "tokens": [50364, 294, 8745, 777, 2098, 295, 4473, 527, 15743, 300, 366, 406, 8213, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2037, "seek": 924664, "start": 9252.64, "end": 9256.64, "text": " So, for example, if we were to have a bunch of linear layers,", "tokens": [50664, 407, 11, 337, 1365, 11, 498, 321, 645, 281, 362, 257, 3840, 295, 8213, 7914, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2038, "seek": 924664, "start": 9256.64, "end": 9259.64, "text": " a bunch of, let me erase this,", "tokens": [50864, 257, 3840, 295, 11, 718, 385, 23525, 341, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2039, "seek": 924664, "start": 9259.64, "end": 9263.64, "text": " if we were to have a bunch of, you know,", "tokens": [51014, 498, 321, 645, 281, 362, 257, 3840, 295, 11, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2040, "seek": 924664, "start": 9263.64, "end": 9266.64, "text": " nn.linears in a row,", "tokens": [51214, 297, 77, 13, 1889, 685, 294, 257, 5386, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2041, "seek": 924664, "start": 9266.64, "end": 9270.64, "text": " what would actually happen is they would all just, you know,", "tokens": [51364, 437, 576, 767, 1051, 307, 436, 576, 439, 445, 11, 291, 458, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2042, "seek": 924664, "start": 9270.64, "end": 9272.64, "text": " they would all squeeze together", "tokens": [51564, 436, 576, 439, 13578, 1214, 51664], "temperature": 0.0, "avg_logprob": -0.12297511357133106, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.05662371218204498}, {"id": 2043, "seek": 927264, "start": 9272.64, "end": 9277.64, "text": " and essentially apply one transformation that sums up all of them kind of.", "tokens": [50364, 293, 4476, 3079, 472, 9887, 300, 34499, 493, 439, 295, 552, 733, 295, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2044, "seek": 927264, "start": 9277.64, "end": 9281.64, "text": " They all sort of multiply together and it gives us one transformation", "tokens": [50614, 814, 439, 1333, 295, 12972, 1214, 293, 309, 2709, 505, 472, 9887, 50814], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2045, "seek": 927264, "start": 9281.64, "end": 9285.64, "text": " that is kind of just a waste of computation", "tokens": [50814, 300, 307, 733, 295, 445, 257, 5964, 295, 24903, 51014], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2046, "seek": 927264, "start": 9285.64, "end": 9289.64, "text": " because let's say you have 100 of these nn.linear layers", "tokens": [51014, 570, 718, 311, 584, 291, 362, 2319, 295, 613, 297, 77, 13, 28263, 7914, 51214], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2047, "seek": 927264, "start": 9289.64, "end": 9292.64, "text": " and nothing else.", "tokens": [51214, 293, 1825, 1646, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2048, "seek": 927264, "start": 9292.64, "end": 9294.64, "text": " You're essentially going from inputs to outputs,", "tokens": [51364, 509, 434, 4476, 516, 490, 15743, 281, 23930, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2049, "seek": 927264, "start": 9294.64, "end": 9299.64, "text": " but you're doing 100 times the computation for just one multiplication.", "tokens": [51464, 457, 291, 434, 884, 2319, 1413, 264, 24903, 337, 445, 472, 27290, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2050, "seek": 927264, "start": 9299.64, "end": 9301.64, "text": " That doesn't really make sense.", "tokens": [51714, 663, 1177, 380, 534, 652, 2020, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09756954193115235, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.05662108585238457}, {"id": 2051, "seek": 930164, "start": 9301.64, "end": 9306.64, "text": " So what can we do to actually make these deep neural networks important", "tokens": [50364, 407, 437, 393, 321, 360, 281, 767, 652, 613, 2452, 18161, 9590, 1021, 50614], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2052, "seek": 930164, "start": 9306.64, "end": 9310.64, "text": " and what can we offer that's more than just linear transformations?", "tokens": [50614, 293, 437, 393, 321, 2626, 300, 311, 544, 813, 445, 8213, 34852, 30, 50814], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2053, "seek": 930164, "start": 9310.64, "end": 9313.64, "text": " Well, that's where activation functions come in", "tokens": [50814, 1042, 11, 300, 311, 689, 24433, 6828, 808, 294, 50964], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2054, "seek": 930164, "start": 9313.64, "end": 9316.64, "text": " and I'm going to go over these in a quick second here.", "tokens": [50964, 293, 286, 478, 516, 281, 352, 670, 613, 294, 257, 1702, 1150, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2055, "seek": 930164, "start": 9316.64, "end": 9319.64, "text": " So let's go navigate over to the PyTorch docs.", "tokens": [51114, 407, 718, 311, 352, 12350, 670, 281, 264, 9953, 51, 284, 339, 45623, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2056, "seek": 930164, "start": 9319.64, "end": 9323.64, "text": " So the three activation functions I'm going to cover", "tokens": [51264, 407, 264, 1045, 24433, 6828, 286, 478, 516, 281, 2060, 51464], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2057, "seek": 930164, "start": 9323.64, "end": 9327.64, "text": " in this little part of the video are the relu, the sigmoid,", "tokens": [51464, 294, 341, 707, 644, 295, 264, 960, 366, 264, 1039, 84, 11, 264, 4556, 3280, 327, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2058, "seek": 930164, "start": 9327.64, "end": 9329.64, "text": " and the tanh activation functions.", "tokens": [51664, 293, 264, 7603, 71, 24433, 6828, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07436847686767578, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.0010648437310010195}, {"id": 2059, "seek": 932964, "start": 9329.64, "end": 9335.64, "text": " So let's start off with the relu or rectified linear unit.", "tokens": [50364, 407, 718, 311, 722, 766, 365, 264, 1039, 84, 420, 11048, 2587, 8213, 4985, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2060, "seek": 932964, "start": 9335.64, "end": 9337.64, "text": " So we're going to use functional relu", "tokens": [50664, 407, 321, 434, 516, 281, 764, 11745, 1039, 84, 50764], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2061, "seek": 932964, "start": 9337.64, "end": 9340.64, "text": " and the reason why we're not just going to use torch.n", "tokens": [50764, 293, 264, 1778, 983, 321, 434, 406, 445, 516, 281, 764, 27822, 13, 77, 50914], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2062, "seek": 932964, "start": 9340.64, "end": 9343.64, "text": " is because we're not doing any forward passes here.", "tokens": [50914, 307, 570, 321, 434, 406, 884, 604, 2128, 11335, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2063, "seek": 932964, "start": 9343.64, "end": 9347.64, "text": " I'm just going to add these into our,", "tokens": [51064, 286, 478, 445, 516, 281, 909, 613, 666, 527, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2064, "seek": 932964, "start": 9347.64, "end": 9352.64, "text": " I'm going to add these, let me clear this, clear this output.", "tokens": [51264, 286, 478, 516, 281, 909, 613, 11, 718, 385, 1850, 341, 11, 1850, 341, 5598, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2065, "seek": 932964, "start": 9352.64, "end": 9353.64, "text": " That's fine.", "tokens": [51514, 663, 311, 2489, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2066, "seek": 932964, "start": 9353.64, "end": 9356.64, "text": " I'm actually going to add these into here and there's no forward pass.", "tokens": [51564, 286, 478, 767, 516, 281, 909, 613, 666, 510, 293, 456, 311, 572, 2128, 1320, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10042651353684147, "compression_ratio": 1.8878048780487804, "no_speech_prob": 0.0018101183231920004}, {"id": 2067, "seek": 935664, "start": 9356.64, "end": 9358.64, "text": " We're just going to simply run them through a function", "tokens": [50364, 492, 434, 445, 516, 281, 2935, 1190, 552, 807, 257, 2445, 50464], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2068, "seek": 935664, "start": 9358.64, "end": 9361.64, "text": " and get an output just so we can see what it looks like.", "tokens": [50464, 293, 483, 364, 5598, 445, 370, 321, 393, 536, 437, 309, 1542, 411, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2069, "seek": 935664, "start": 9361.64, "end": 9366.64, "text": " So I've actually added this up here from torch.n", "tokens": [50614, 407, 286, 600, 767, 3869, 341, 493, 510, 490, 27822, 13, 77, 50864], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2070, "seek": 935664, "start": 9366.64, "end": 9368.64, "text": " and import functional as capital F.", "tokens": [50864, 293, 974, 11745, 382, 4238, 479, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2071, "seek": 935664, "start": 9368.64, "end": 9372.64, "text": " It's just kind of a common PyTorch practice, capital F.", "tokens": [50964, 467, 311, 445, 733, 295, 257, 2689, 9953, 51, 284, 339, 3124, 11, 4238, 479, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2072, "seek": 935664, "start": 9372.64, "end": 9377.64, "text": " And let's go ahead and start off with the relu here.", "tokens": [51164, 400, 718, 311, 352, 2286, 293, 722, 766, 365, 264, 1039, 84, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08115056428042325, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.004399004392325878}, {"id": 2073, "seek": 937764, "start": 9377.64, "end": 9385.64, "text": " So we can go, I don't know, x equals torch.tensor", "tokens": [50364, 407, 321, 393, 352, 11, 286, 500, 380, 458, 11, 2031, 6915, 27822, 13, 83, 23153, 50764], "temperature": 0.0, "avg_logprob": -0.21809970798777112, "compression_ratio": 1.3969465648854962, "no_speech_prob": 0.313932329416275}, {"id": 2074, "seek": 937764, "start": 9385.64, "end": 9391.64, "text": " and then we'll make it a negative 0.05, for example.", "tokens": [50764, 293, 550, 321, 603, 652, 309, 257, 3671, 1958, 13, 13328, 11, 337, 1365, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21809970798777112, "compression_ratio": 1.3969465648854962, "no_speech_prob": 0.313932329416275}, {"id": 2075, "seek": 937764, "start": 9391.64, "end": 9398.64, "text": " And then we'll go dtype equals torch.flurp32", "tokens": [51064, 400, 550, 321, 603, 352, 274, 20467, 6915, 27822, 13, 3423, 20130, 11440, 51414], "temperature": 0.0, "avg_logprob": -0.21809970798777112, "compression_ratio": 1.3969465648854962, "no_speech_prob": 0.313932329416275}, {"id": 2076, "seek": 937764, "start": 9398.64, "end": 9405.64, "text": " and we can go y equals f.relu of x.", "tokens": [51414, 293, 321, 393, 352, 288, 6915, 283, 13, 265, 2781, 295, 2031, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21809970798777112, "compression_ratio": 1.3969465648854962, "no_speech_prob": 0.313932329416275}, {"id": 2077, "seek": 940564, "start": 9405.64, "end": 9412.64, "text": " And then we'll go ahead and print y.", "tokens": [50364, 400, 550, 321, 603, 352, 2286, 293, 4482, 288, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2078, "seek": 940564, "start": 9412.64, "end": 9414.64, "text": " It has no attribute relu.", "tokens": [50714, 467, 575, 572, 19667, 1039, 84, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2079, "seek": 940564, "start": 9414.64, "end": 9416.64, "text": " Okay, let's try nn then.", "tokens": [50814, 1033, 11, 718, 311, 853, 297, 77, 550, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2080, "seek": 940564, "start": 9416.64, "end": 9421.64, "text": " Let's try nn and see if that works.", "tokens": [50914, 961, 311, 853, 297, 77, 293, 536, 498, 300, 1985, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2081, "seek": 940564, "start": 9421.64, "end": 9424.64, "text": " Okay, well that didn't work and that's fine", "tokens": [51164, 1033, 11, 731, 300, 994, 380, 589, 293, 300, 311, 2489, 51314], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2082, "seek": 940564, "start": 9424.64, "end": 9427.64, "text": " because we can simply take a look at this", "tokens": [51314, 570, 321, 393, 2935, 747, 257, 574, 412, 341, 51464], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2083, "seek": 940564, "start": 9427.64, "end": 9430.64, "text": " and it'll help us understand.", "tokens": [51464, 293, 309, 603, 854, 505, 1223, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2084, "seek": 940564, "start": 9430.64, "end": 9432.64, "text": " We don't actually need to,", "tokens": [51614, 492, 500, 380, 767, 643, 281, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2085, "seek": 940564, "start": 9432.64, "end": 9433.64, "text": " we don't need to write this out in code", "tokens": [51714, 321, 500, 380, 643, 281, 2464, 341, 484, 294, 3089, 51764], "temperature": 0.0, "avg_logprob": -0.1392652681558439, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0014778564218431711}, {"id": 2086, "seek": 943364, "start": 9433.64, "end": 9435.64, "text": " as long as it sort of makes sense.", "tokens": [50364, 382, 938, 382, 309, 1333, 295, 1669, 2020, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2087, "seek": 943364, "start": 9435.64, "end": 9437.64, "text": " We don't need to write this in the forward pass, really.", "tokens": [50464, 492, 500, 380, 643, 281, 2464, 341, 294, 264, 2128, 1320, 11, 534, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2088, "seek": 943364, "start": 9437.64, "end": 9439.64, "text": " You're not going to use it anywhere else.", "tokens": [50564, 509, 434, 406, 516, 281, 764, 309, 4992, 1646, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2089, "seek": 943364, "start": 9439.64, "end": 9442.64, "text": " So yeah, I'm not going to be too discouraged", "tokens": [50664, 407, 1338, 11, 286, 478, 406, 516, 281, 312, 886, 35010, 50814], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2090, "seek": 943364, "start": 9442.64, "end": 9446.64, "text": " that that does not work in the functional library.", "tokens": [50814, 300, 300, 775, 406, 589, 294, 264, 11745, 6405, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2091, "seek": 943364, "start": 9446.64, "end": 9449.64, "text": " But yeah, so pretty much what this does", "tokens": [51014, 583, 1338, 11, 370, 1238, 709, 437, 341, 775, 51164], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2092, "seek": 943364, "start": 9449.64, "end": 9451.64, "text": " is if a number is below,", "tokens": [51164, 307, 498, 257, 1230, 307, 2507, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2093, "seek": 943364, "start": 9451.64, "end": 9454.64, "text": " if a number is 0 or below 0,", "tokens": [51264, 498, 257, 1230, 307, 1958, 420, 2507, 1958, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2094, "seek": 943364, "start": 9454.64, "end": 9456.64, "text": " it will turn that number into 0.", "tokens": [51414, 309, 486, 1261, 300, 1230, 666, 1958, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2095, "seek": 943364, "start": 9456.64, "end": 9460.64, "text": " And then if it's above 0, it'll stay the same.", "tokens": [51514, 400, 550, 498, 309, 311, 3673, 1958, 11, 309, 603, 1754, 264, 912, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10571641844462573, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.0038239548448473215}, {"id": 2096, "seek": 946064, "start": 9460.64, "end": 9463.64, "text": " So this graph sort of helps you visualize that.", "tokens": [50364, 407, 341, 4295, 1333, 295, 3665, 291, 23273, 300, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2097, "seek": 946064, "start": 9463.64, "end": 9465.64, "text": " There's a little function here.", "tokens": [50514, 821, 311, 257, 707, 2445, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2098, "seek": 946064, "start": 9465.64, "end": 9467.64, "text": " That might make sense to some people.", "tokens": [50614, 663, 1062, 652, 2020, 281, 512, 561, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2099, "seek": 946064, "start": 9467.64, "end": 9469.64, "text": " I don't really care about the functions too much", "tokens": [50714, 286, 500, 380, 534, 1127, 466, 264, 6828, 886, 709, 50814], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2100, "seek": 946064, "start": 9469.64, "end": 9472.64, "text": " as long as I can sort of visualize what the function means,", "tokens": [50814, 382, 938, 382, 286, 393, 1333, 295, 23273, 437, 264, 2445, 1355, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2101, "seek": 946064, "start": 9472.64, "end": 9475.64, "text": " what it does, what are some applications it can be used.", "tokens": [50964, 437, 309, 775, 11, 437, 366, 512, 5821, 309, 393, 312, 1143, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2102, "seek": 946064, "start": 9475.64, "end": 9479.64, "text": " That usually covers enough for like any function at all.", "tokens": [51114, 663, 2673, 10538, 1547, 337, 411, 604, 2445, 412, 439, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2103, "seek": 946064, "start": 9479.64, "end": 9482.64, "text": " So that's the Relu function.", "tokens": [51314, 407, 300, 311, 264, 8738, 84, 2445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2104, "seek": 946064, "start": 9482.64, "end": 9483.64, "text": " Pretty cool.", "tokens": [51464, 10693, 1627, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2105, "seek": 946064, "start": 9483.64, "end": 9487.64, "text": " It simply offers a non-linearity to our linear networks.", "tokens": [51514, 467, 2935, 7736, 257, 2107, 12, 1889, 17409, 281, 527, 8213, 9590, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08581179279392048, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0017819488421082497}, {"id": 2106, "seek": 948764, "start": 9487.64, "end": 9489.64, "text": " So if you have 100 layers deep", "tokens": [50364, 407, 498, 291, 362, 2319, 7914, 2452, 50464], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2107, "seek": 948764, "start": 9489.64, "end": 9491.64, "text": " and every, I don't know,", "tokens": [50464, 293, 633, 11, 286, 500, 380, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2108, "seek": 948764, "start": 9491.64, "end": 9494.64, "text": " every second step you put a Relu,", "tokens": [50564, 633, 1150, 1823, 291, 829, 257, 8738, 84, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2109, "seek": 948764, "start": 9494.64, "end": 9496.64, "text": " that network is going to learn a lot more things.", "tokens": [50714, 300, 3209, 307, 516, 281, 1466, 257, 688, 544, 721, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2110, "seek": 948764, "start": 9496.64, "end": 9499.64, "text": " It's going to learn a lot more linearity, non-linearity.", "tokens": [50814, 467, 311, 516, 281, 1466, 257, 688, 544, 8213, 507, 11, 2107, 12, 1889, 17409, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2111, "seek": 948764, "start": 9499.64, "end": 9502.64, "text": " Then if you were to just have 100 layers", "tokens": [50964, 1396, 498, 291, 645, 281, 445, 362, 2319, 7914, 51114], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2112, "seek": 948764, "start": 9502.64, "end": 9505.64, "text": " multiplying all into one transformation.", "tokens": [51114, 30955, 439, 666, 472, 9887, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2113, "seek": 948764, "start": 9505.64, "end": 9507.64, "text": " So that's what that is.", "tokens": [51264, 407, 300, 311, 437, 300, 307, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2114, "seek": 948764, "start": 9507.64, "end": 9508.64, "text": " That's the Relu.", "tokens": [51364, 663, 311, 264, 8738, 84, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2115, "seek": 948764, "start": 9508.64, "end": 9510.64, "text": " Now let's go over to Sigmoid.", "tokens": [51414, 823, 718, 311, 352, 670, 281, 37763, 3280, 327, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2116, "seek": 948764, "start": 9510.64, "end": 9514.64, "text": " So here we can actually use the functional library.", "tokens": [51514, 407, 510, 321, 393, 767, 764, 264, 11745, 6405, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09198930786877144, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0017544068396091461}, {"id": 2117, "seek": 951464, "start": 9514.64, "end": 9518.64, "text": " And all Sigmoid does is we go 1 over 1 plus", "tokens": [50364, 400, 439, 37763, 3280, 327, 775, 307, 321, 352, 502, 670, 502, 1804, 50564], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2118, "seek": 951464, "start": 9518.64, "end": 9521.64, "text": " exponentiated of negative x.", "tokens": [50564, 37871, 72, 770, 295, 3671, 2031, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2119, "seek": 951464, "start": 9521.64, "end": 9524.64, "text": " So I'm going to add that here.", "tokens": [50714, 407, 286, 478, 516, 281, 909, 300, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2120, "seek": 951464, "start": 9524.64, "end": 9527.64, "text": " We could, yeah, why not do that?", "tokens": [50864, 492, 727, 11, 1338, 11, 983, 406, 360, 300, 30, 51014], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2121, "seek": 951464, "start": 9527.64, "end": 9531.64, "text": " Negative 0.05 float 32.", "tokens": [51014, 43230, 1958, 13, 13328, 15706, 8858, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2122, "seek": 951464, "start": 9531.64, "end": 9532.64, "text": " Sure.", "tokens": [51214, 4894, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2123, "seek": 951464, "start": 9532.64, "end": 9536.64, "text": " We'll go f dot Sigmoid.", "tokens": [51264, 492, 603, 352, 283, 5893, 37763, 3280, 327, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2124, "seek": 951464, "start": 9536.64, "end": 9539.64, "text": " And then we'll just go x and then we'll print y.", "tokens": [51464, 400, 550, 321, 603, 445, 352, 2031, 293, 550, 321, 603, 4482, 288, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2125, "seek": 951464, "start": 9539.64, "end": 9540.64, "text": " Cool.", "tokens": [51614, 8561, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17659757470571866, "compression_ratio": 1.416184971098266, "no_speech_prob": 0.0022517479956150055}, {"id": 2126, "seek": 954064, "start": 9540.64, "end": 9544.64, "text": " So we get a tensor 0.4875.", "tokens": [50364, 407, 321, 483, 257, 40863, 1958, 13, 13318, 11901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2127, "seek": 954064, "start": 9544.64, "end": 9545.64, "text": " Interesting.", "tokens": [50564, 14711, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2128, "seek": 954064, "start": 9545.64, "end": 9549.64, "text": " So this little negative 0.05 here", "tokens": [50614, 407, 341, 707, 3671, 1958, 13, 13328, 510, 50814], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2129, "seek": 954064, "start": 9549.64, "end": 9553.64, "text": " is essentially being plugged into this negative x.", "tokens": [50814, 307, 4476, 885, 25679, 666, 341, 3671, 2031, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2130, "seek": 954064, "start": 9553.64, "end": 9561.64, "text": " So 1 over 1 plus 2.71 to the power of negative 0.05.", "tokens": [51014, 407, 502, 670, 502, 1804, 568, 13, 29985, 281, 264, 1347, 295, 3671, 1958, 13, 13328, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2131, "seek": 954064, "start": 9561.64, "end": 9563.64, "text": " So it's essentially,", "tokens": [51414, 407, 309, 311, 4476, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12505670694204477, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.006002357229590416}, {"id": 2132, "seek": 956364, "start": 9563.64, "end": 9573.64, "text": " if we do 2.71, 2.71 to the power of negative 0.05,", "tokens": [50364, 498, 321, 360, 568, 13, 29985, 11, 568, 13, 29985, 281, 264, 1347, 295, 3671, 1958, 13, 13328, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2133, "seek": 956364, "start": 9573.64, "end": 9575.64, "text": " we're just going to get positive.", "tokens": [50864, 321, 434, 445, 516, 281, 483, 3353, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2134, "seek": 956364, "start": 9575.64, "end": 9580.64, "text": " So 1.05 and then 1 plus that.", "tokens": [50964, 407, 502, 13, 13328, 293, 550, 502, 1804, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2135, "seek": 956364, "start": 9580.64, "end": 9584.64, "text": " So that's 2.05.", "tokens": [51214, 407, 300, 311, 568, 13, 13328, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2136, "seek": 956364, "start": 9584.64, "end": 9586.64, "text": " We just do 1 over that.", "tokens": [51414, 492, 445, 360, 502, 670, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2137, "seek": 956364, "start": 9586.64, "end": 9587.64, "text": " 2.05.", "tokens": [51514, 568, 13, 13328, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2138, "seek": 956364, "start": 9587.64, "end": 9590.64, "text": " So we get about 0.487.", "tokens": [51564, 407, 321, 483, 466, 1958, 13, 13318, 22, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14588468074798583, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.005910616368055344}, {"id": 2139, "seek": 959064, "start": 9590.64, "end": 9593.64, "text": " And what do we get here?", "tokens": [50364, 400, 437, 360, 321, 483, 510, 30, 50514], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2140, "seek": 959064, "start": 9593.64, "end": 9594.64, "text": " 0.4 at 7.", "tokens": [50514, 1958, 13, 19, 412, 1614, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2141, "seek": 959064, "start": 9594.64, "end": 9596.64, "text": " Cool.", "tokens": [50564, 8561, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2142, "seek": 959064, "start": 9596.64, "end": 9597.64, "text": " So that's interesting.", "tokens": [50664, 407, 300, 311, 1880, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2143, "seek": 959064, "start": 9597.64, "end": 9600.64, "text": " And let's actually look, is there a graph here?", "tokens": [50714, 400, 718, 311, 767, 574, 11, 307, 456, 257, 4295, 510, 30, 50864], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2144, "seek": 959064, "start": 9600.64, "end": 9605.64, "text": " Let's look at the Sigmoid activation function.", "tokens": [50864, 961, 311, 574, 412, 264, 37763, 3280, 327, 24433, 2445, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2145, "seek": 959064, "start": 9605.64, "end": 9606.64, "text": " Wikipedia.", "tokens": [51114, 28999, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2146, "seek": 959064, "start": 9606.64, "end": 9608.64, "text": " Don't get too scared by this math here.", "tokens": [51164, 1468, 380, 483, 886, 5338, 538, 341, 5221, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2147, "seek": 959064, "start": 9608.64, "end": 9609.64, "text": " I don't like it either,", "tokens": [51264, 286, 500, 380, 411, 309, 2139, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2148, "seek": 959064, "start": 9609.64, "end": 9613.64, "text": " but I like the graphs they're cool to look at.", "tokens": [51314, 457, 286, 411, 264, 24877, 436, 434, 1627, 281, 574, 412, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2149, "seek": 959064, "start": 9613.64, "end": 9615.64, "text": " So this is pretty much what it's doing here.", "tokens": [51514, 407, 341, 307, 1238, 709, 437, 309, 311, 884, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09704819950488729, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0029808299150317907}, {"id": 2150, "seek": 961564, "start": 9615.64, "end": 9619.64, "text": " So yeah, it's just a little curve.", "tokens": [50364, 407, 1338, 11, 309, 311, 445, 257, 707, 7605, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2151, "seek": 961564, "start": 9619.64, "end": 9623.64, "text": " Kind of looks like a, it's kind of just like a wave,", "tokens": [50564, 9242, 295, 1542, 411, 257, 11, 309, 311, 733, 295, 445, 411, 257, 5772, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2152, "seek": 961564, "start": 9623.64, "end": 9626.64, "text": " but it's cool looking.", "tokens": [50764, 457, 309, 311, 1627, 1237, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2153, "seek": 961564, "start": 9626.64, "end": 9628.64, "text": " That's what the Sigmoid function does.", "tokens": [50914, 663, 311, 437, 264, 37763, 3280, 327, 2445, 775, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2154, "seek": 961564, "start": 9628.64, "end": 9631.64, "text": " It's used to just generalize over this line.", "tokens": [51014, 467, 311, 1143, 281, 445, 2674, 1125, 670, 341, 1622, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2155, "seek": 961564, "start": 9631.64, "end": 9636.64, "text": " And yeah, Sigmoid function is pretty cool.", "tokens": [51164, 400, 1338, 11, 37763, 3280, 327, 2445, 307, 1238, 1627, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2156, "seek": 961564, "start": 9636.64, "end": 9638.64, "text": " So now let's move on to the tanh.", "tokens": [51414, 407, 586, 718, 311, 1286, 322, 281, 264, 7603, 71, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2157, "seek": 961564, "start": 9638.64, "end": 9640.64, "text": " The tanh function.", "tokens": [51514, 440, 7603, 71, 2445, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2158, "seek": 961564, "start": 9640.64, "end": 9643.64, "text": " Google Bing is, or Microsoft Bing is giving me", "tokens": [51614, 3329, 30755, 307, 11, 420, 8116, 30755, 307, 2902, 385, 51764], "temperature": 0.0, "avg_logprob": -0.1493736057106508, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0018100638408213854}, {"id": 2159, "seek": 964364, "start": 9643.64, "end": 9644.64, "text": " a nice description of that.", "tokens": [50364, 257, 1481, 3855, 295, 300, 13, 50414], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2160, "seek": 964364, "start": 9644.64, "end": 9646.64, "text": " Cool.", "tokens": [50414, 8561, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2161, "seek": 964364, "start": 9646.64, "end": 9647.64, "text": " Perfect.", "tokens": [50514, 10246, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2162, "seek": 964364, "start": 9647.64, "end": 9648.64, "text": " E to the negative x.", "tokens": [50564, 462, 281, 264, 3671, 2031, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2163, "seek": 964364, "start": 9648.64, "end": 9651.64, "text": " I like that.", "tokens": [50614, 286, 411, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2164, "seek": 964364, "start": 9651.64, "end": 9653.64, "text": " So tanh is a little bit different.", "tokens": [50764, 407, 7603, 71, 307, 257, 707, 857, 819, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2165, "seek": 964364, "start": 9653.64, "end": 9656.64, "text": " There's a lot more exponentiating going on here.", "tokens": [50864, 821, 311, 257, 688, 544, 37871, 72, 990, 516, 322, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2166, "seek": 964364, "start": 9656.64, "end": 9661.64, "text": " So you have, I'll just say expo or exp of x", "tokens": [51014, 407, 291, 362, 11, 286, 603, 445, 584, 1278, 78, 420, 1278, 295, 2031, 51264], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2167, "seek": 964364, "start": 9661.64, "end": 9663.64, "text": " minus exp of negative x", "tokens": [51264, 3175, 1278, 295, 3671, 2031, 51364], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2168, "seek": 964364, "start": 9663.64, "end": 9666.64, "text": " divided by exp of x plus exp of negative x.", "tokens": [51364, 6666, 538, 1278, 295, 2031, 1804, 1278, 295, 3671, 2031, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2169, "seek": 964364, "start": 9666.64, "end": 9669.64, "text": " There's a lot of positives and negatives in here.", "tokens": [51514, 821, 311, 257, 688, 295, 35127, 293, 40019, 294, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11570824317212375, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.006289114244282246}, {"id": 2170, "seek": 966964, "start": 9669.64, "end": 9673.64, "text": " Positive, positive, negative, negative, negative, positive.", "tokens": [50364, 46326, 11, 3353, 11, 3671, 11, 3671, 11, 3671, 11, 3353, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2171, "seek": 966964, "start": 9673.64, "end": 9676.64, "text": " So that's interesting.", "tokens": [50564, 407, 300, 311, 1880, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2172, "seek": 966964, "start": 9676.64, "end": 9679.64, "text": " Let's go ahead and put this into code here.", "tokens": [50714, 961, 311, 352, 2286, 293, 829, 341, 666, 3089, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2173, "seek": 966964, "start": 9679.64, "end": 9683.64, "text": " So I'll go torch dot examples, or torch examples.", "tokens": [50864, 407, 286, 603, 352, 27822, 5893, 5110, 11, 420, 27822, 5110, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2174, "seek": 966964, "start": 9683.64, "end": 9685.64, "text": " This is our file here.", "tokens": [51064, 639, 307, 527, 3991, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2175, "seek": 966964, "start": 9685.64, "end": 9690.64, "text": " And I'll just go tanh.", "tokens": [51164, 400, 286, 603, 445, 352, 7603, 71, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2176, "seek": 966964, "start": 9690.64, "end": 9692.64, "text": " Cool.", "tokens": [51414, 8561, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2177, "seek": 966964, "start": 9692.64, "end": 9695.64, "text": " So negative 0.05.", "tokens": [51514, 407, 3671, 1958, 13, 13328, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2178, "seek": 966964, "start": 9695.64, "end": 9696.64, "text": " Cool.", "tokens": [51664, 8561, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2179, "seek": 966964, "start": 9696.64, "end": 9697.64, "text": " What if we do a one?", "tokens": [51714, 708, 498, 321, 360, 257, 472, 30, 51764], "temperature": 0.0, "avg_logprob": -0.12064609732679142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.007010988891124725}, {"id": 2180, "seek": 969764, "start": 9697.64, "end": 9701.64, "text": " What will that produce?", "tokens": [50364, 708, 486, 300, 5258, 30, 50564], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2181, "seek": 969764, "start": 9701.64, "end": 9705.64, "text": " Oh, 0.76.", "tokens": [50564, 876, 11, 1958, 13, 25026, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2182, "seek": 969764, "start": 9705.64, "end": 9710.64, "text": " What if we do a 10?", "tokens": [50764, 708, 498, 321, 360, 257, 1266, 30, 51014], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2183, "seek": 969764, "start": 9710.64, "end": 9711.64, "text": " 1.0.", "tokens": [51014, 502, 13, 15, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2184, "seek": 969764, "start": 9711.64, "end": 9713.64, "text": " Interesting.", "tokens": [51064, 14711, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2185, "seek": 969764, "start": 9713.64, "end": 9716.64, "text": " So this is sort of similar to the sigmoid,", "tokens": [51164, 407, 341, 307, 1333, 295, 2531, 281, 264, 4556, 3280, 327, 11, 51314], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2186, "seek": 969764, "start": 9716.64, "end": 9718.64, "text": " except it's, you know,", "tokens": [51314, 3993, 309, 311, 11, 291, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2187, "seek": 969764, "start": 9718.64, "end": 9725.64, "text": " it's actually asked to attach a BT what the difference is.", "tokens": [51414, 309, 311, 767, 2351, 281, 5085, 257, 31144, 437, 264, 2649, 307, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2035958315875079, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.0023964126594364643}, {"id": 2188, "seek": 972564, "start": 9726.64, "end": 9733.64, "text": " When would you use tanh over sigmoid?", "tokens": [50414, 1133, 576, 291, 764, 7603, 71, 670, 4556, 3280, 327, 30, 50764], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2189, "seek": 972564, "start": 9733.64, "end": 9736.64, "text": " Let's see here.", "tokens": [50764, 961, 311, 536, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2190, "seek": 972564, "start": 9736.64, "end": 9740.64, "text": " Sigmoid function and hyperbolic tangent or tanh function", "tokens": [50914, 37763, 3280, 327, 2445, 293, 9848, 65, 7940, 27747, 420, 7603, 71, 2445, 51114], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2191, "seek": 972564, "start": 9740.64, "end": 9743.64, "text": " are activations functions used in neural networks.", "tokens": [51114, 366, 2430, 763, 6828, 1143, 294, 18161, 9590, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2192, "seek": 972564, "start": 9743.64, "end": 9745.64, "text": " They have a similar s-shaped curve,", "tokens": [51264, 814, 362, 257, 2531, 262, 12, 23103, 7605, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2193, "seek": 972564, "start": 9745.64, "end": 9747.64, "text": " but have different ranges.", "tokens": [51364, 457, 362, 819, 22526, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2194, "seek": 972564, "start": 9747.64, "end": 9750.64, "text": " So sigmoid output values between a 0 and a 1", "tokens": [51464, 407, 4556, 3280, 327, 5598, 4190, 1296, 257, 1958, 293, 257, 502, 51614], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2195, "seek": 972564, "start": 9750.64, "end": 9753.64, "text": " while tanh is between a negative 1 and a 1.", "tokens": [51614, 1339, 7603, 71, 307, 1296, 257, 3671, 502, 293, 257, 502, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11070196679297914, "compression_ratio": 1.565, "no_speech_prob": 0.0010986827546730638}, {"id": 2196, "seek": 975364, "start": 9753.64, "end": 9755.64, "text": " So if you're, you know,", "tokens": [50364, 407, 498, 291, 434, 11, 291, 458, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2197, "seek": 975364, "start": 9755.64, "end": 9758.64, "text": " if you're rating maybe the,", "tokens": [50464, 498, 291, 434, 10990, 1310, 264, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2198, "seek": 975364, "start": 9758.64, "end": 9762.64, "text": " maybe if you're getting a probability distribution,", "tokens": [50614, 1310, 498, 291, 434, 1242, 257, 8482, 7316, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2199, "seek": 975364, "start": 9762.64, "end": 9766.64, "text": " for example, you want it to be between 0 and 1,", "tokens": [50814, 337, 1365, 11, 291, 528, 309, 281, 312, 1296, 1958, 293, 502, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2200, "seek": 975364, "start": 9766.64, "end": 9768.64, "text": " meaning percentages or decimal places.", "tokens": [51014, 3620, 42270, 420, 26601, 3190, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2201, "seek": 975364, "start": 9768.64, "end": 9773.64, "text": " So like a 0.5 would be 50%, 0.87 would be 87%.", "tokens": [51114, 407, 411, 257, 1958, 13, 20, 576, 312, 2625, 8923, 1958, 13, 23853, 576, 312, 27990, 6856, 51364], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2202, "seek": 975364, "start": 9773.64, "end": 9776.64, "text": " And that's what the sigmoid function does.", "tokens": [51364, 400, 300, 311, 437, 264, 4556, 3280, 327, 2445, 775, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2203, "seek": 975364, "start": 9776.64, "end": 9779.64, "text": " It's quite close to the softmax function, actually.", "tokens": [51514, 467, 311, 1596, 1998, 281, 264, 2787, 41167, 2445, 11, 767, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2204, "seek": 975364, "start": 9779.64, "end": 9782.64, "text": " Except the softmax just, you know,", "tokens": [51664, 16192, 264, 2787, 41167, 445, 11, 291, 458, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09792180572237287, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.001548574073240161}, {"id": 2205, "seek": 978264, "start": 9782.64, "end": 9785.64, "text": " it prioritizes the bigger values", "tokens": [50364, 309, 14846, 5660, 264, 3801, 4190, 50514], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2206, "seek": 978264, "start": 9785.64, "end": 9788.64, "text": " and puts the smaller values to our priority.", "tokens": [50514, 293, 8137, 264, 4356, 4190, 281, 527, 9365, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2207, "seek": 978264, "start": 9788.64, "end": 9790.64, "text": " That's all the softmax says.", "tokens": [50664, 663, 311, 439, 264, 2787, 41167, 1619, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2208, "seek": 978264, "start": 9790.64, "end": 9792.64, "text": " It's kind of a sigmoid on steroids.", "tokens": [50764, 467, 311, 733, 295, 257, 4556, 3280, 327, 322, 45717, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2209, "seek": 978264, "start": 9792.64, "end": 9796.64, "text": " And the tanh outputs between negative 1 and 1.", "tokens": [50864, 400, 264, 7603, 71, 23930, 1296, 3671, 502, 293, 502, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2210, "seek": 978264, "start": 9796.64, "end": 9800.64, "text": " So, yeah, you could maybe even start theorycrafting", "tokens": [51064, 407, 11, 1338, 11, 291, 727, 1310, 754, 722, 5261, 66, 10437, 783, 51264], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2211, "seek": 978264, "start": 9800.64, "end": 9803.64, "text": " and thinking of some ways you could use", "tokens": [51264, 293, 1953, 295, 512, 2098, 291, 727, 764, 51414], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2212, "seek": 978264, "start": 9803.64, "end": 9806.64, "text": " even the tanh function and sigmoid in different use cases.", "tokens": [51414, 754, 264, 7603, 71, 2445, 293, 4556, 3280, 327, 294, 819, 764, 3331, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2213, "seek": 978264, "start": 9806.64, "end": 9809.64, "text": " So that's kind of a general overview on those.", "tokens": [51564, 407, 300, 311, 733, 295, 257, 2674, 12492, 322, 729, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2214, "seek": 978264, "start": 9809.64, "end": 9811.64, "text": " So biogram language models are finished.", "tokens": [51714, 407, 3228, 12820, 2856, 5245, 366, 4335, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10865775021639737, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.0009849054040387273}, {"id": 2215, "seek": 981164, "start": 9811.64, "end": 9813.64, "text": " All of this we finished here is now done.", "tokens": [50364, 1057, 295, 341, 321, 4335, 510, 307, 586, 1096, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2216, "seek": 981164, "start": 9813.64, "end": 9815.64, "text": " You're back from your break.", "tokens": [50464, 509, 434, 646, 490, 428, 1821, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2217, "seek": 981164, "start": 9815.64, "end": 9818.64, "text": " If you took one, if you didn't, that's fine too.", "tokens": [50564, 759, 291, 1890, 472, 11, 498, 291, 994, 380, 11, 300, 311, 2489, 886, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2218, "seek": 981164, "start": 9818.64, "end": 9821.64, "text": " But pretty much we're going to dig into", "tokens": [50714, 583, 1238, 709, 321, 434, 516, 281, 2528, 666, 50864], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2219, "seek": 981164, "start": 9821.64, "end": 9824.64, "text": " the transformer architecture now.", "tokens": [50864, 264, 31782, 9482, 586, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2220, "seek": 981164, "start": 9824.64, "end": 9827.64, "text": " And we're actually going to build it from scratch.", "tokens": [51014, 400, 321, 434, 767, 516, 281, 1322, 309, 490, 8459, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2221, "seek": 981164, "start": 9827.64, "end": 9830.64, "text": " So there was recently a paper proposed", "tokens": [51164, 407, 456, 390, 3938, 257, 3035, 10348, 51314], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2222, "seek": 981164, "start": 9830.64, "end": 9833.64, "text": " called the transformer model.", "tokens": [51314, 1219, 264, 31782, 2316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2223, "seek": 981164, "start": 9833.64, "end": 9837.64, "text": " And this uses a mechanism called self-attention.", "tokens": [51464, 400, 341, 4960, 257, 7513, 1219, 2698, 12, 1591, 1251, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2224, "seek": 981164, "start": 9837.64, "end": 9840.64, "text": " Self-attention is used in these multi-head attention,", "tokens": [51664, 16348, 12, 1591, 1251, 307, 1143, 294, 613, 4825, 12, 1934, 3202, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09638132841690727, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0011333872098475695}, {"id": 2225, "seek": 984064, "start": 9840.64, "end": 9842.64, "text": " little bricks here.", "tokens": [50364, 707, 25497, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2226, "seek": 984064, "start": 9842.64, "end": 9845.64, "text": " And there's a lot that happens.", "tokens": [50464, 400, 456, 311, 257, 688, 300, 2314, 13, 50614], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2227, "seek": 984064, "start": 9845.64, "end": 9847.64, "text": " So there's something I want to clarify", "tokens": [50614, 407, 456, 311, 746, 286, 528, 281, 17594, 50714], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2228, "seek": 984064, "start": 9847.64, "end": 9849.64, "text": " before we jump right into this architecture", "tokens": [50714, 949, 321, 3012, 558, 666, 341, 9482, 50814], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2229, "seek": 984064, "start": 9849.64, "end": 9852.64, "text": " and just dump a bunch of information", "tokens": [50814, 293, 445, 11430, 257, 3840, 295, 1589, 50964], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2230, "seek": 984064, "start": 9852.64, "end": 9855.64, "text": " on your poor little brain right now.", "tokens": [50964, 322, 428, 4716, 707, 3567, 558, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2231, "seek": 984064, "start": 9855.64, "end": 9858.64, "text": " But a lot of these networks, at first,", "tokens": [51114, 583, 257, 688, 295, 613, 9590, 11, 412, 700, 11, 51264], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2232, "seek": 984064, "start": 9858.64, "end": 9861.64, "text": " can be extremely confusing to beginners.", "tokens": [51264, 393, 312, 4664, 13181, 281, 26992, 13, 51414], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2233, "seek": 984064, "start": 9861.64, "end": 9863.64, "text": " So I want to make it clear.", "tokens": [51414, 407, 286, 528, 281, 652, 309, 1850, 13, 51514], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2234, "seek": 984064, "start": 9863.64, "end": 9866.64, "text": " It's perfectly okay if you don't understand this at first.", "tokens": [51514, 467, 311, 6239, 1392, 498, 291, 500, 380, 1223, 341, 412, 700, 13, 51664], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2235, "seek": 984064, "start": 9866.64, "end": 9869.64, "text": " I'm going to try to explain this in the best way possible.", "tokens": [51664, 286, 478, 516, 281, 853, 281, 2903, 341, 294, 264, 1151, 636, 1944, 13, 51814], "temperature": 0.0, "avg_logprob": -0.04575195554959572, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.013013435527682304}, {"id": 2236, "seek": 986964, "start": 9869.64, "end": 9871.64, "text": " Believe me, I've seen tons of videos", "tokens": [50364, 21486, 385, 11, 286, 600, 1612, 9131, 295, 2145, 50464], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2237, "seek": 986964, "start": 9871.64, "end": 9874.64, "text": " on people explaining the transformer architecture.", "tokens": [50464, 322, 561, 13468, 264, 31782, 9482, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2238, "seek": 986964, "start": 9874.64, "end": 9877.64, "text": " And all of them have been, to some degree,", "tokens": [50614, 400, 439, 295, 552, 362, 668, 11, 281, 512, 4314, 11, 50764], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2239, "seek": 986964, "start": 9877.64, "end": 9879.64, "text": " a bit confusing to me as well.", "tokens": [50764, 257, 857, 13181, 281, 385, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2240, "seek": 986964, "start": 9879.64, "end": 9881.64, "text": " So I'm going to try to clarify", "tokens": [50864, 407, 286, 478, 516, 281, 853, 281, 17594, 50964], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2241, "seek": 986964, "start": 9881.64, "end": 9886.64, "text": " all those little pieces of confusion.", "tokens": [50964, 439, 729, 707, 3755, 295, 15075, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2242, "seek": 986964, "start": 9886.64, "end": 9888.64, "text": " Like what does that mean?", "tokens": [51214, 1743, 437, 775, 300, 914, 30, 51314], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2243, "seek": 986964, "start": 9888.64, "end": 9889.64, "text": " You didn't cover that piece.", "tokens": [51314, 509, 994, 380, 2060, 300, 2522, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2244, "seek": 986964, "start": 9889.64, "end": 9891.64, "text": " I don't know what's going on here.", "tokens": [51364, 286, 500, 380, 458, 437, 311, 516, 322, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2245, "seek": 986964, "start": 9891.64, "end": 9893.64, "text": " I'm going to cover all those little bits", "tokens": [51464, 286, 478, 516, 281, 2060, 439, 729, 707, 9239, 51564], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2246, "seek": 986964, "start": 9893.64, "end": 9896.64, "text": " and make sure that nothing is left behind.", "tokens": [51564, 293, 652, 988, 300, 1825, 307, 1411, 2261, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06484417293382727, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.007344177924096584}, {"id": 2247, "seek": 989664, "start": 9896.64, "end": 9898.64, "text": " So you're going to want to sit tight", "tokens": [50364, 407, 291, 434, 516, 281, 528, 281, 1394, 4524, 50464], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2248, "seek": 989664, "start": 9898.64, "end": 9901.64, "text": " and pay attention for this next part here.", "tokens": [50464, 293, 1689, 3202, 337, 341, 958, 644, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2249, "seek": 989664, "start": 9901.64, "end": 9904.64, "text": " So yeah, let's go ahead and dive into", "tokens": [50614, 407, 1338, 11, 718, 311, 352, 2286, 293, 9192, 666, 50764], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2250, "seek": 989664, "start": 9904.64, "end": 9907.64, "text": " just the general transformer architecture", "tokens": [50764, 445, 264, 2674, 31782, 9482, 50914], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2251, "seek": 989664, "start": 9907.64, "end": 9909.64, "text": " and why it's important.", "tokens": [50914, 293, 983, 309, 311, 1021, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2252, "seek": 989664, "start": 9909.64, "end": 9911.64, "text": " So in the transformer network,", "tokens": [51014, 407, 294, 264, 31782, 3209, 11, 51114], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2253, "seek": 989664, "start": 9911.64, "end": 9914.64, "text": " you have a lot of computation going on.", "tokens": [51114, 291, 362, 257, 688, 295, 24903, 516, 322, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2254, "seek": 989664, "start": 9914.64, "end": 9917.64, "text": " You have some adding and normalizing.", "tokens": [51264, 509, 362, 512, 5127, 293, 2710, 3319, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2255, "seek": 989664, "start": 9917.64, "end": 9919.64, "text": " You have some multi-hat attention.", "tokens": [51414, 509, 362, 512, 4825, 12, 15178, 3202, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2256, "seek": 989664, "start": 9919.64, "end": 9921.64, "text": " You have some feed forward networks.", "tokens": [51514, 509, 362, 512, 3154, 2128, 9590, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2257, "seek": 989664, "start": 9921.64, "end": 9923.64, "text": " There's a lot going on here.", "tokens": [51614, 821, 311, 257, 688, 516, 322, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2258, "seek": 989664, "start": 9923.64, "end": 9925.64, "text": " There's a lot of computation, a lot of multiplying,", "tokens": [51714, 821, 311, 257, 688, 295, 24903, 11, 257, 688, 295, 30955, 11, 51814], "temperature": 0.0, "avg_logprob": -0.0766738565956674, "compression_ratio": 1.8697478991596639, "no_speech_prob": 0.004467189311981201}, {"id": 2259, "seek": 992564, "start": 9925.64, "end": 9927.64, "text": " there's a lot going on.", "tokens": [50364, 456, 311, 257, 688, 516, 322, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2260, "seek": 992564, "start": 9927.64, "end": 9929.64, "text": " So the question I actually had at first was,", "tokens": [50464, 407, 264, 1168, 286, 767, 632, 412, 700, 390, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2261, "seek": 992564, "start": 9929.64, "end": 9932.64, "text": " well, if you're just multiplying these inputs", "tokens": [50564, 731, 11, 498, 291, 434, 445, 30955, 613, 15743, 50714], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2262, "seek": 992564, "start": 9932.64, "end": 9934.64, "text": " by a bunch of different things along,", "tokens": [50714, 538, 257, 3840, 295, 819, 721, 2051, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2263, "seek": 992564, "start": 9934.64, "end": 9937.64, "text": " you should just end up with some random value at the end", "tokens": [50814, 291, 820, 445, 917, 493, 365, 512, 4974, 2158, 412, 264, 917, 50964], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2264, "seek": 992564, "start": 9937.64, "end": 9940.64, "text": " that maybe doesn't really mean that much", "tokens": [50964, 300, 1310, 1177, 380, 534, 914, 300, 709, 51114], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2265, "seek": 992564, "start": 9940.64, "end": 9942.64, "text": " of the initial input.", "tokens": [51114, 295, 264, 5883, 4846, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2266, "seek": 992564, "start": 9942.64, "end": 9944.64, "text": " And that's actually correct.", "tokens": [51214, 400, 300, 311, 767, 3006, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2267, "seek": 992564, "start": 9944.64, "end": 9947.64, "text": " For the first few iterations,", "tokens": [51314, 1171, 264, 700, 1326, 36540, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2268, "seek": 992564, "start": 9947.64, "end": 9949.64, "text": " the model has absolutely no context", "tokens": [51464, 264, 2316, 575, 3122, 572, 4319, 51564], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2269, "seek": 992564, "start": 9949.64, "end": 9950.64, "text": " as to what's going on.", "tokens": [51564, 382, 281, 437, 311, 516, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2270, "seek": 992564, "start": 9950.64, "end": 9951.64, "text": " It is clueless.", "tokens": [51614, 467, 307, 596, 3483, 442, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2271, "seek": 992564, "start": 9951.64, "end": 9953.64, "text": " It is going in random directions", "tokens": [51664, 467, 307, 516, 294, 4974, 11095, 51764], "temperature": 0.0, "avg_logprob": -0.07375526428222656, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.004608252085745335}, {"id": 2272, "seek": 995364, "start": 9953.64, "end": 9957.64, "text": " and it's just trying to find the best way to converge.", "tokens": [50364, 293, 309, 311, 445, 1382, 281, 915, 264, 1151, 636, 281, 41881, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2273, "seek": 995364, "start": 9957.64, "end": 9959.64, "text": " So this is what machine learning and deep learning", "tokens": [50564, 407, 341, 307, 437, 3479, 2539, 293, 2452, 2539, 50664], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2274, "seek": 995364, "start": 9959.64, "end": 9961.64, "text": " is actually all about,", "tokens": [50664, 307, 767, 439, 466, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2275, "seek": 995364, "start": 9961.64, "end": 9964.64, "text": " is having all these little parameters in,", "tokens": [50764, 307, 1419, 439, 613, 707, 9834, 294, 11, 50914], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2276, "seek": 995364, "start": 9964.64, "end": 9966.64, "text": " you know, the adding and normalizing,", "tokens": [50914, 291, 458, 11, 264, 5127, 293, 2710, 3319, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2277, "seek": 995364, "start": 9966.64, "end": 9969.64, "text": " the feed forward networks, even multi-hat attention.", "tokens": [51014, 264, 3154, 2128, 9590, 11, 754, 4825, 12, 15178, 3202, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2278, "seek": 995364, "start": 9969.64, "end": 9972.64, "text": " We're trying to optimize the parameters", "tokens": [51164, 492, 434, 1382, 281, 19719, 264, 9834, 51314], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2279, "seek": 995364, "start": 9972.64, "end": 9975.64, "text": " for producing an output that is meaningful", "tokens": [51314, 337, 10501, 364, 5598, 300, 307, 10995, 51464], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2280, "seek": 995364, "start": 9975.64, "end": 9977.64, "text": " that will actually help us produce", "tokens": [51464, 300, 486, 767, 854, 505, 5258, 51564], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2281, "seek": 995364, "start": 9977.64, "end": 9981.64, "text": " almost perfectly like English text.", "tokens": [51564, 1920, 6239, 411, 3669, 2487, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07229886471646503, "compression_ratio": 1.6468253968253967, "no_speech_prob": 0.0038235217798501253}, {"id": 2282, "seek": 998164, "start": 9981.64, "end": 9983.64, "text": " And so this is the entire process of pre-training.", "tokens": [50364, 400, 370, 341, 307, 264, 2302, 1399, 295, 659, 12, 17227, 1760, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2283, "seek": 998164, "start": 9983.64, "end": 9986.64, "text": " You send a bunch of inputs into a transformer", "tokens": [50464, 509, 2845, 257, 3840, 295, 15743, 666, 257, 31782, 50614], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2284, "seek": 998164, "start": 9986.64, "end": 9989.64, "text": " and you get some output probabilities", "tokens": [50614, 293, 291, 483, 512, 5598, 33783, 50764], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2285, "seek": 998164, "start": 9989.64, "end": 9991.64, "text": " that you used to generate from.", "tokens": [50764, 300, 291, 1143, 281, 8460, 490, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2286, "seek": 998164, "start": 9991.64, "end": 9994.64, "text": " And what attention does", "tokens": [50864, 400, 437, 3202, 775, 51014], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2287, "seek": 998164, "start": 9994.64, "end": 9997.64, "text": " is it sets little different scores", "tokens": [51014, 307, 309, 6352, 707, 819, 13444, 51164], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2288, "seek": 998164, "start": 9997.64, "end": 10001.64, "text": " to, you know, each little token in a sentence.", "tokens": [51164, 281, 11, 291, 458, 11, 1184, 707, 14862, 294, 257, 8174, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2289, "seek": 998164, "start": 10001.64, "end": 10004.64, "text": " For tokens you have character, subword,", "tokens": [51364, 1171, 22667, 291, 362, 2517, 11, 1422, 7462, 11, 51514], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2290, "seek": 998164, "start": 10004.64, "end": 10006.64, "text": " and word-level tokens.", "tokens": [51514, 293, 1349, 12, 12418, 22667, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2291, "seek": 998164, "start": 10006.64, "end": 10009.64, "text": " So you're pretty much just mapping", "tokens": [51614, 407, 291, 434, 1238, 709, 445, 18350, 51764], "temperature": 0.0, "avg_logprob": -0.0847324805684609, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.003649125574156642}, {"id": 2292, "seek": 1000964, "start": 10009.64, "end": 10011.64, "text": " bits of attention to each of these,", "tokens": [50364, 9239, 295, 3202, 281, 1184, 295, 613, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2293, "seek": 1000964, "start": 10011.64, "end": 10013.64, "text": " as well as, you know,", "tokens": [50464, 382, 731, 382, 11, 291, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2294, "seek": 1000964, "start": 10013.64, "end": 10016.64, "text": " what is the position also mean as well.", "tokens": [50564, 437, 307, 264, 2535, 611, 914, 382, 731, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2295, "seek": 1000964, "start": 10016.64, "end": 10020.64, "text": " So you could have two words that are right next to each other,", "tokens": [50714, 407, 291, 727, 362, 732, 2283, 300, 366, 558, 958, 281, 1184, 661, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2296, "seek": 1000964, "start": 10020.64, "end": 10022.64, "text": " but then if you don't actually, you know,", "tokens": [50914, 457, 550, 498, 291, 500, 380, 767, 11, 291, 458, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2297, "seek": 1000964, "start": 10022.64, "end": 10024.64, "text": " positionally encode them,", "tokens": [51014, 2535, 379, 2058, 1429, 552, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2298, "seek": 1000964, "start": 10024.64, "end": 10026.64, "text": " it doesn't really mean much,", "tokens": [51114, 309, 1177, 380, 534, 914, 709, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2299, "seek": 1000964, "start": 10026.64, "end": 10029.64, "text": " because it's like, oh, these could be like 4,000 characters apart.", "tokens": [51214, 570, 309, 311, 411, 11, 1954, 11, 613, 727, 312, 411, 1017, 11, 1360, 4342, 4936, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2300, "seek": 1000964, "start": 10029.64, "end": 10031.64, "text": " So that's why you need both", "tokens": [51364, 407, 300, 311, 983, 291, 643, 1293, 51464], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2301, "seek": 1000964, "start": 10031.64, "end": 10034.64, "text": " to put attention scores on these tokens", "tokens": [51464, 281, 829, 3202, 13444, 322, 613, 22667, 51614], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2302, "seek": 1000964, "start": 10034.64, "end": 10037.64, "text": " and to positionally encode them.", "tokens": [51614, 293, 281, 2535, 379, 2058, 1429, 552, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09226020812988281, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0066917589865624905}, {"id": 2303, "seek": 1003764, "start": 10037.64, "end": 10039.64, "text": " And that's what's happening here.", "tokens": [50364, 400, 300, 311, 437, 311, 2737, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2304, "seek": 1003764, "start": 10039.64, "end": 10044.64, "text": " So what we do is we get to our inputs.", "tokens": [50464, 407, 437, 321, 360, 307, 321, 483, 281, 527, 15743, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2305, "seek": 1003764, "start": 10044.64, "end": 10046.64, "text": " We got our inputs.", "tokens": [50714, 492, 658, 527, 15743, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2306, "seek": 1003764, "start": 10046.64, "end": 10048.64, "text": " So, I mean, we went over this with", "tokens": [50814, 407, 11, 286, 914, 11, 321, 1437, 670, 341, 365, 50914], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2307, "seek": 1003764, "start": 10048.64, "end": 10050.64, "text": " diagram language models.", "tokens": [50914, 10686, 2856, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2308, "seek": 1003764, "start": 10050.64, "end": 10053.64, "text": " We feed our X and Y,", "tokens": [51014, 492, 3154, 527, 1783, 293, 398, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2309, "seek": 1003764, "start": 10053.64, "end": 10055.64, "text": " so X would be our inputs,", "tokens": [51164, 370, 1783, 576, 312, 527, 15743, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2310, "seek": 1003764, "start": 10055.64, "end": 10058.64, "text": " Y would be our targets or outputs.", "tokens": [51264, 398, 576, 312, 527, 12911, 420, 23930, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2311, "seek": 1003764, "start": 10058.64, "end": 10061.64, "text": " And what we're going to do", "tokens": [51414, 400, 437, 321, 434, 516, 281, 360, 51564], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2312, "seek": 1003764, "start": 10061.64, "end": 10064.64, "text": " is give these little embeddings.", "tokens": [51564, 307, 976, 613, 707, 12240, 29432, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12043909231821696, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.005300384946167469}, {"id": 2313, "seek": 1006464, "start": 10064.64, "end": 10067.64, "text": " So I believe we went over embeddings a little while ago,", "tokens": [50364, 407, 286, 1697, 321, 1437, 670, 12240, 29432, 257, 707, 1339, 2057, 11, 50514], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2314, "seek": 1006464, "start": 10067.64, "end": 10069.64, "text": " and pretty much what those mean", "tokens": [50514, 293, 1238, 709, 437, 729, 914, 50614], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2315, "seek": 1006464, "start": 10069.64, "end": 10071.64, "text": " is it's going to have a little row", "tokens": [50614, 307, 309, 311, 516, 281, 362, 257, 707, 5386, 50714], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2316, "seek": 1006464, "start": 10071.64, "end": 10073.64, "text": " for each token on that table,", "tokens": [50714, 337, 1184, 14862, 322, 300, 3199, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2317, "seek": 1006464, "start": 10073.64, "end": 10075.64, "text": " and that's going to store, you know,", "tokens": [50814, 293, 300, 311, 516, 281, 3531, 11, 291, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2318, "seek": 1006464, "start": 10075.64, "end": 10078.64, "text": " some vector as to what that token means.", "tokens": [50914, 512, 8062, 382, 281, 437, 300, 14862, 1355, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2319, "seek": 1006464, "start": 10078.64, "end": 10081.64, "text": " So let's say you had, like, you know,", "tokens": [51064, 407, 718, 311, 584, 291, 632, 11, 411, 11, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2320, "seek": 1006464, "start": 10081.64, "end": 10084.64, "text": " the character E, for example,", "tokens": [51214, 264, 2517, 462, 11, 337, 1365, 11, 51364], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2321, "seek": 1006464, "start": 10084.64, "end": 10089.64, "text": " the sentiment or the vector of the character E", "tokens": [51364, 264, 16149, 420, 264, 8062, 295, 264, 2517, 462, 51614], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2322, "seek": 1006464, "start": 10089.64, "end": 10091.64, "text": " is probably going to be vastly different", "tokens": [51614, 307, 1391, 516, 281, 312, 41426, 819, 51714], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2323, "seek": 1006464, "start": 10091.64, "end": 10093.64, "text": " than the sentiment of Z, right?", "tokens": [51714, 813, 264, 16149, 295, 1176, 11, 558, 30, 51814], "temperature": 0.0, "avg_logprob": -0.07735869510114686, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.0004728318308480084}, {"id": 2324, "seek": 1009364, "start": 10093.64, "end": 10095.64, "text": " Because E is a very common vowel,", "tokens": [50364, 1436, 462, 307, 257, 588, 2689, 29410, 11, 50464], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2325, "seek": 1009364, "start": 10095.64, "end": 10097.64, "text": " and Z is one of the most uncommon,", "tokens": [50464, 293, 1176, 307, 472, 295, 264, 881, 29289, 11, 50564], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2326, "seek": 1009364, "start": 10097.64, "end": 10101.64, "text": " if not the most uncommon letter in the English language.", "tokens": [50564, 498, 406, 264, 881, 29289, 5063, 294, 264, 3669, 2856, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2327, "seek": 1009364, "start": 10101.64, "end": 10104.64, "text": " So these embeddings are learned.", "tokens": [50764, 407, 613, 12240, 29432, 366, 3264, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2328, "seek": 1009364, "start": 10104.64, "end": 10107.64, "text": " We have these both for our inputs and our outputs.", "tokens": [50914, 492, 362, 613, 1293, 337, 527, 15743, 293, 527, 23930, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2329, "seek": 1009364, "start": 10107.64, "end": 10109.64, "text": " We give them positional encodings", "tokens": [51064, 492, 976, 552, 2535, 304, 2058, 378, 1109, 51164], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2330, "seek": 1009364, "start": 10109.64, "end": 10111.64, "text": " like I was talking about,", "tokens": [51164, 411, 286, 390, 1417, 466, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2331, "seek": 1009364, "start": 10111.64, "end": 10113.64, "text": " and there's ways we can do that.", "tokens": [51264, 293, 456, 311, 2098, 321, 393, 360, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2332, "seek": 1009364, "start": 10113.64, "end": 10116.64, "text": " We can actually use learnable parameters", "tokens": [51364, 492, 393, 767, 764, 1466, 712, 9834, 51514], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2333, "seek": 1009364, "start": 10116.64, "end": 10118.64, "text": " to assign these encodings.", "tokens": [51514, 281, 6269, 613, 2058, 378, 1109, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2334, "seek": 1009364, "start": 10118.64, "end": 10121.64, "text": " A lot of these are learnable parameters, by the way,", "tokens": [51614, 316, 688, 295, 613, 366, 1466, 712, 9834, 11, 538, 264, 636, 11, 51764], "temperature": 0.0, "avg_logprob": -0.0594825267791748, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.0007552556344307959}, {"id": 2335, "seek": 1012164, "start": 10121.64, "end": 10123.64, "text": " and you'll see that as you, you know,", "tokens": [50364, 293, 291, 603, 536, 300, 382, 291, 11, 291, 458, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2336, "seek": 1012164, "start": 10123.64, "end": 10125.64, "text": " delve more and more into transformers.", "tokens": [50464, 43098, 544, 293, 544, 666, 4088, 433, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2337, "seek": 1012164, "start": 10125.64, "end": 10130.64, "text": " But, yeah, so after we've given these inputs,", "tokens": [50564, 583, 11, 1338, 11, 370, 934, 321, 600, 2212, 613, 15743, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2338, "seek": 1012164, "start": 10130.64, "end": 10132.64, "text": " embeddings, and positional encodings,", "tokens": [50814, 12240, 29432, 11, 293, 2535, 304, 2058, 378, 1109, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2339, "seek": 1012164, "start": 10132.64, "end": 10134.64, "text": " and same thing with the outputs,", "tokens": [50914, 293, 912, 551, 365, 264, 23930, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2340, "seek": 1012164, "start": 10134.64, "end": 10136.64, "text": " which are essentially just shifted right,", "tokens": [51014, 597, 366, 4476, 445, 18892, 558, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2341, "seek": 1012164, "start": 10136.64, "end": 10139.64, "text": " you have, you know, I up to block size for inputs,", "tokens": [51114, 291, 362, 11, 291, 458, 11, 286, 493, 281, 3461, 2744, 337, 15743, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2342, "seek": 1012164, "start": 10139.64, "end": 10144.64, "text": " and then I plus one up to block size plus one, right?", "tokens": [51264, 293, 550, 286, 1804, 472, 493, 281, 3461, 2744, 1804, 472, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2343, "seek": 1012164, "start": 10144.64, "end": 10148.64, "text": " Or whatever little thing we employed here", "tokens": [51514, 1610, 2035, 707, 551, 321, 20115, 510, 51714], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2344, "seek": 1012164, "start": 10148.64, "end": 10150.64, "text": " in our background language models.", "tokens": [51714, 294, 527, 3678, 2856, 5245, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09938639301364705, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.0007915101596154273}, {"id": 2345, "seek": 1015064, "start": 10150.64, "end": 10152.64, "text": " Quite what it was.", "tokens": [50364, 20464, 437, 309, 390, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2346, "seek": 1015064, "start": 10152.64, "end": 10154.64, "text": " Or even if we did that at all.", "tokens": [50464, 1610, 754, 498, 321, 630, 300, 412, 439, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2347, "seek": 1015064, "start": 10158.64, "end": 10160.64, "text": " No.", "tokens": [50764, 883, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2348, "seek": 1015064, "start": 10160.64, "end": 10162.64, "text": " I'm just speaking gibberish right now,", "tokens": [50864, 286, 478, 445, 4124, 4553, 43189, 558, 586, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2349, "seek": 1015064, "start": 10162.64, "end": 10164.64, "text": " but that's fine because it's going to make sense", "tokens": [50964, 457, 300, 311, 2489, 570, 309, 311, 516, 281, 652, 2020, 51064], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2350, "seek": 1015064, "start": 10164.64, "end": 10166.64, "text": " in a little bit here.", "tokens": [51064, 294, 257, 707, 857, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2351, "seek": 1015064, "start": 10168.64, "end": 10171.64, "text": " So what I'm going to actually do", "tokens": [51264, 407, 437, 286, 478, 516, 281, 767, 360, 51414], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2352, "seek": 1015064, "start": 10171.64, "end": 10173.64, "text": " is I'm not going to read off of this right here", "tokens": [51414, 307, 286, 478, 406, 516, 281, 1401, 766, 295, 341, 558, 510, 51514], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2353, "seek": 1015064, "start": 10173.64, "end": 10175.64, "text": " because this is really confusing.", "tokens": [51514, 570, 341, 307, 534, 13181, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2354, "seek": 1015064, "start": 10175.64, "end": 10178.64, "text": " So I'm going to switch over to a little,", "tokens": [51614, 407, 286, 478, 516, 281, 3679, 670, 281, 257, 707, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09872675414132599, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.004981274250894785}, {"id": 2355, "seek": 1017864, "start": 10179.64, "end": 10182.64, "text": " like a little sketch that I drew out.", "tokens": [50414, 411, 257, 707, 12325, 300, 286, 12804, 484, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2356, "seek": 1017864, "start": 10182.64, "end": 10184.64, "text": " And this is pretty much the entire transformer", "tokens": [50564, 400, 341, 307, 1238, 709, 264, 2302, 31782, 50664], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2357, "seek": 1017864, "start": 10184.64, "end": 10186.64, "text": " with a lot of other things considered", "tokens": [50664, 365, 257, 688, 295, 661, 721, 4888, 50764], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2358, "seek": 1017864, "start": 10186.64, "end": 10190.64, "text": " that this initial image does not really put into perspective.", "tokens": [50764, 300, 341, 5883, 3256, 775, 406, 534, 829, 666, 4585, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2359, "seek": 1017864, "start": 10190.64, "end": 10193.64, "text": " So let's go ahead and jump into", "tokens": [50964, 407, 718, 311, 352, 2286, 293, 3012, 666, 51114], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2360, "seek": 1017864, "start": 10193.64, "end": 10197.64, "text": " sort of what's going on in here from the ground up.", "tokens": [51114, 1333, 295, 437, 311, 516, 322, 294, 510, 490, 264, 2727, 493, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2361, "seek": 1017864, "start": 10197.64, "end": 10199.64, "text": " So like I was talking about before,", "tokens": [51314, 407, 411, 286, 390, 1417, 466, 949, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2362, "seek": 1017864, "start": 10199.64, "end": 10201.64, "text": " we have some inputs and we have some outputs", "tokens": [51414, 321, 362, 512, 15743, 293, 321, 362, 512, 23930, 51514], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2363, "seek": 1017864, "start": 10201.64, "end": 10203.64, "text": " which are shifted right,", "tokens": [51514, 597, 366, 18892, 558, 11, 51614], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2364, "seek": 1017864, "start": 10203.64, "end": 10207.64, "text": " and we give each of them some embedding vectors", "tokens": [51614, 293, 321, 976, 1184, 295, 552, 512, 12240, 3584, 18875, 51814], "temperature": 0.0, "avg_logprob": -0.08220986452969638, "compression_ratio": 1.6614173228346456, "no_speech_prob": 0.0007792711840011179}, {"id": 2365, "seek": 1020764, "start": 10207.64, "end": 10209.64, "text": " and positional encodings.", "tokens": [50364, 293, 2535, 304, 2058, 378, 1109, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2366, "seek": 1020764, "start": 10209.64, "end": 10212.64, "text": " So from here, let's say we have n layers.", "tokens": [50464, 407, 490, 510, 11, 718, 311, 584, 321, 362, 297, 7914, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2367, "seek": 1020764, "start": 10212.64, "end": 10214.64, "text": " This is going to make sense in a second.", "tokens": [50614, 639, 307, 516, 281, 652, 2020, 294, 257, 1150, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2368, "seek": 1020764, "start": 10214.64, "end": 10216.64, "text": " n layers is set to four.", "tokens": [50714, 297, 7914, 307, 992, 281, 1451, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2369, "seek": 1020764, "start": 10216.64, "end": 10218.64, "text": " So the amount of layers we have is set to four.", "tokens": [50814, 407, 264, 2372, 295, 7914, 321, 362, 307, 992, 281, 1451, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2370, "seek": 1020764, "start": 10218.64, "end": 10221.64, "text": " So you can see we have an encoder, encoder.", "tokens": [50914, 407, 291, 393, 536, 321, 362, 364, 2058, 19866, 11, 2058, 19866, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2371, "seek": 1020764, "start": 10221.64, "end": 10224.64, "text": " Like we have four of these and we have four decoders.", "tokens": [51064, 1743, 321, 362, 1451, 295, 613, 293, 321, 362, 1451, 979, 378, 433, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2372, "seek": 1020764, "start": 10224.64, "end": 10227.64, "text": " So four is actually the amount of encoders", "tokens": [51214, 407, 1451, 307, 767, 264, 2372, 295, 2058, 378, 433, 51364], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2373, "seek": 1020764, "start": 10227.64, "end": 10229.64, "text": " and decoders we have.", "tokens": [51364, 293, 979, 378, 433, 321, 362, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2374, "seek": 1020764, "start": 10229.64, "end": 10231.64, "text": " We always have the same amount of each.", "tokens": [51464, 492, 1009, 362, 264, 912, 2372, 295, 1184, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2375, "seek": 1020764, "start": 10231.64, "end": 10234.64, "text": " So if we have, you know, ten layers,", "tokens": [51564, 407, 498, 321, 362, 11, 291, 458, 11, 2064, 7914, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07661028469310087, "compression_ratio": 1.995260663507109, "no_speech_prob": 0.0030271103605628014}, {"id": 2376, "seek": 1023464, "start": 10234.64, "end": 10237.64, "text": " that means we have ten encoders and ten decoders.", "tokens": [50364, 300, 1355, 321, 362, 2064, 2058, 378, 433, 293, 2064, 979, 378, 433, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2377, "seek": 1023464, "start": 10237.64, "end": 10239.64, "text": " And pretty much what would happen", "tokens": [50514, 400, 1238, 709, 437, 576, 1051, 50614], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2378, "seek": 1023464, "start": 10239.64, "end": 10242.64, "text": " is after this input,", "tokens": [50614, 307, 934, 341, 4846, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2379, "seek": 1023464, "start": 10242.64, "end": 10244.64, "text": " embedding and positional embedding,", "tokens": [50764, 12240, 3584, 293, 2535, 304, 12240, 3584, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2380, "seek": 1023464, "start": 10244.64, "end": 10247.64, "text": " we feed that into the first encoder layer", "tokens": [50864, 321, 3154, 300, 666, 264, 700, 2058, 19866, 4583, 51014], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2381, "seek": 1023464, "start": 10247.64, "end": 10249.64, "text": " and then the next, and then next,", "tokens": [51014, 293, 550, 264, 958, 11, 293, 550, 958, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2382, "seek": 1023464, "start": 10249.64, "end": 10251.64, "text": " and then right as soon as we hit the last one,", "tokens": [51114, 293, 550, 558, 382, 2321, 382, 321, 2045, 264, 1036, 472, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2383, "seek": 1023464, "start": 10251.64, "end": 10256.64, "text": " we feed these into each of these decoders here,", "tokens": [51214, 321, 3154, 613, 666, 1184, 295, 613, 979, 378, 433, 510, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2384, "seek": 1023464, "start": 10256.64, "end": 10258.64, "text": " each of these decoder layers.", "tokens": [51464, 1184, 295, 613, 979, 19866, 7914, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2385, "seek": 1023464, "start": 10258.64, "end": 10263.64, "text": " So only the last encoder will feed into these decoders.", "tokens": [51564, 407, 787, 264, 1036, 2058, 19866, 486, 3154, 666, 613, 979, 378, 433, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08290655330076056, "compression_ratio": 1.946078431372549, "no_speech_prob": 0.004006762523204088}, {"id": 2386, "seek": 1026364, "start": 10263.64, "end": 10268.64, "text": " And pretty much these decoders will all run.", "tokens": [50364, 400, 1238, 709, 613, 979, 378, 433, 486, 439, 1190, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2387, "seek": 1026364, "start": 10268.64, "end": 10270.64, "text": " They'll all learn different things.", "tokens": [50614, 814, 603, 439, 1466, 819, 721, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2388, "seek": 1026364, "start": 10270.64, "end": 10272.64, "text": " And then they'll turn what they learned.", "tokens": [50714, 400, 550, 436, 603, 1261, 437, 436, 3264, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2389, "seek": 1026364, "start": 10272.64, "end": 10275.64, "text": " They'll do, they'll apply a linear transformation", "tokens": [50814, 814, 603, 360, 11, 436, 603, 3079, 257, 8213, 9887, 50964], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2390, "seek": 1026364, "start": 10275.64, "end": 10276.64, "text": " at the end of it.", "tokens": [50964, 412, 264, 917, 295, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2391, "seek": 1026364, "start": 10276.64, "end": 10278.64, "text": " This is not in the decoder function.", "tokens": [51014, 639, 307, 406, 294, 264, 979, 19866, 2445, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2392, "seek": 1026364, "start": 10278.64, "end": 10280.64, "text": " This is actually after the last decoder.", "tokens": [51114, 639, 307, 767, 934, 264, 1036, 979, 19866, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2393, "seek": 1026364, "start": 10280.64, "end": 10282.64, "text": " It'll apply a linear transformation", "tokens": [51214, 467, 603, 3079, 257, 8213, 9887, 51314], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2394, "seek": 1026364, "start": 10282.64, "end": 10286.64, "text": " to pretty much sort of simplify", "tokens": [51314, 281, 1238, 709, 1333, 295, 20460, 51514], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2395, "seek": 1026364, "start": 10286.64, "end": 10288.64, "text": " or give a summary of what it learned.", "tokens": [51514, 420, 976, 257, 12691, 295, 437, 309, 3264, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07888407890613262, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.001324881217442453}, {"id": 2396, "seek": 1028864, "start": 10288.64, "end": 10293.64, "text": " And then we apply a softmax on that new, you know, tensor", "tokens": [50364, 400, 550, 321, 3079, 257, 2787, 41167, 322, 300, 777, 11, 291, 458, 11, 40863, 50614], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2397, "seek": 1028864, "start": 10293.64, "end": 10296.64, "text": " to get some probabilities to sample from,", "tokens": [50614, 281, 483, 512, 33783, 281, 6889, 490, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2398, "seek": 1028864, "start": 10296.64, "end": 10298.64, "text": " like we talked about in the generate function", "tokens": [50764, 411, 321, 2825, 466, 294, 264, 8460, 2445, 50864], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2399, "seek": 1028864, "start": 10298.64, "end": 10299.64, "text": " in our biogram.", "tokens": [50864, 294, 527, 3228, 12820, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2400, "seek": 1028864, "start": 10299.64, "end": 10302.64, "text": " And then once we get these probabilities,", "tokens": [50914, 400, 550, 1564, 321, 483, 613, 33783, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2401, "seek": 1028864, "start": 10302.64, "end": 10307.64, "text": " we can then sample from them and generate tokens.", "tokens": [51064, 321, 393, 550, 6889, 490, 552, 293, 8460, 22667, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2402, "seek": 1028864, "start": 10307.64, "end": 10311.64, "text": " And that's kind of like the first little step here.", "tokens": [51314, 400, 300, 311, 733, 295, 411, 264, 700, 707, 1823, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2403, "seek": 1028864, "start": 10311.64, "end": 10312.64, "text": " That's what's going on.", "tokens": [51514, 663, 311, 437, 311, 516, 322, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2404, "seek": 1028864, "start": 10312.64, "end": 10313.64, "text": " We have some encoders.", "tokens": [51564, 492, 362, 512, 2058, 378, 433, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2405, "seek": 1028864, "start": 10313.64, "end": 10315.64, "text": " We have some decoders.", "tokens": [51614, 492, 362, 512, 979, 378, 433, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2406, "seek": 1028864, "start": 10315.64, "end": 10317.64, "text": " We do a transformation to summarize.", "tokens": [51714, 492, 360, 257, 9887, 281, 20858, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08457060183508922, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.004904899746179581}, {"id": 2407, "seek": 1031764, "start": 10317.64, "end": 10319.64, "text": " We have a softmax to get probabilities.", "tokens": [50364, 492, 362, 257, 2787, 41167, 281, 483, 33783, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2408, "seek": 1031764, "start": 10319.64, "end": 10321.64, "text": " And then we generate based on those probabilities.", "tokens": [50464, 400, 550, 321, 8460, 2361, 322, 729, 33783, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2409, "seek": 1031764, "start": 10321.64, "end": 10324.64, "text": " Cool.", "tokens": [50564, 8561, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2410, "seek": 1031764, "start": 10324.64, "end": 10326.64, "text": " Next up, in the encoder,", "tokens": [50714, 3087, 493, 11, 294, 264, 2058, 19866, 11, 50814], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2411, "seek": 1031764, "start": 10326.64, "end": 10329.64, "text": " in each of these encoders, this is what it's going to look like.", "tokens": [50814, 294, 1184, 295, 613, 2058, 378, 433, 11, 341, 307, 437, 309, 311, 516, 281, 574, 411, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2412, "seek": 1031764, "start": 10329.64, "end": 10331.64, "text": " So we have multi-hat attention,", "tokens": [50964, 407, 321, 362, 4825, 12, 15178, 3202, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2413, "seek": 1031764, "start": 10331.64, "end": 10334.64, "text": " which I'm going to dub into a second here.", "tokens": [51064, 597, 286, 478, 516, 281, 18540, 666, 257, 1150, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2414, "seek": 1031764, "start": 10334.64, "end": 10337.64, "text": " So after this multi-hat attention,", "tokens": [51214, 407, 934, 341, 4825, 12, 15178, 3202, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2415, "seek": 1031764, "start": 10337.64, "end": 10339.64, "text": " we have a residual connection.", "tokens": [51364, 321, 362, 257, 27980, 4984, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2416, "seek": 1031764, "start": 10339.64, "end": 10342.64, "text": " So in case you aren't familiar with residual connections,", "tokens": [51464, 407, 294, 1389, 291, 3212, 380, 4963, 365, 27980, 9271, 11, 51614], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2417, "seek": 1031764, "start": 10342.64, "end": 10344.64, "text": " I might have went over this before.", "tokens": [51614, 286, 1062, 362, 1437, 670, 341, 949, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2418, "seek": 1031764, "start": 10344.64, "end": 10346.64, "text": " But pretty much what they do is", "tokens": [51714, 583, 1238, 709, 437, 436, 360, 307, 51814], "temperature": 0.0, "avg_logprob": -0.08569884118233018, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.0005792431766167283}, {"id": 2419, "seek": 1034664, "start": 10346.64, "end": 10348.64, "text": " it's a little connector.", "tokens": [50364, 309, 311, 257, 707, 19127, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2420, "seek": 1034664, "start": 10348.64, "end": 10350.64, "text": " So I don't know.", "tokens": [50464, 407, 286, 500, 380, 458, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2421, "seek": 1034664, "start": 10350.64, "end": 10352.64, "text": " Let's say you get some inputs X,", "tokens": [50564, 961, 311, 584, 291, 483, 512, 15743, 1783, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2422, "seek": 1034664, "start": 10352.64, "end": 10354.64, "text": " you have some inputs X down here,", "tokens": [50664, 291, 362, 512, 15743, 1783, 760, 510, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2423, "seek": 1034664, "start": 10354.64, "end": 10358.64, "text": " and you put them into some sort of function here,", "tokens": [50764, 293, 291, 829, 552, 666, 512, 1333, 295, 2445, 510, 11, 50964], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2424, "seek": 1034664, "start": 10358.64, "end": 10360.64, "text": " some sort of like feedforward network, whatever it is.", "tokens": [50964, 512, 1333, 295, 411, 3154, 13305, 3209, 11, 2035, 309, 307, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2425, "seek": 1034664, "start": 10360.64, "end": 10363.64, "text": " A feedforward network is essentially just a linear,", "tokens": [51064, 316, 3154, 13305, 3209, 307, 4476, 445, 257, 8213, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2426, "seek": 1034664, "start": 10363.64, "end": 10365.64, "text": " a RELU, and then a linear.", "tokens": [51214, 257, 497, 3158, 52, 11, 293, 550, 257, 8213, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2427, "seek": 1034664, "start": 10365.64, "end": 10367.64, "text": " That's all feedforward network is right here.", "tokens": [51314, 663, 311, 439, 3154, 13305, 3209, 307, 558, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2428, "seek": 1034664, "start": 10367.64, "end": 10369.64, "text": " Linear, really, really linear.", "tokens": [51414, 14670, 289, 11, 534, 11, 534, 8213, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2429, "seek": 1034664, "start": 10369.64, "end": 10375.64, "text": " And all you do is you wrap those inputs", "tokens": [51514, 400, 439, 291, 360, 307, 291, 7019, 729, 15743, 51814], "temperature": 0.0, "avg_logprob": -0.12623509352769308, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.00912343617528677}, {"id": 2430, "seek": 1037564, "start": 10375.64, "end": 10378.64, "text": " around so you don't actually put them", "tokens": [50364, 926, 370, 291, 500, 380, 767, 829, 552, 50514], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2431, "seek": 1037564, "start": 10378.64, "end": 10380.64, "text": " into that feedforward network.", "tokens": [50514, 666, 300, 3154, 13305, 3209, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2432, "seek": 1037564, "start": 10380.64, "end": 10382.64, "text": " You actually wrap them around,", "tokens": [50614, 509, 767, 7019, 552, 926, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2433, "seek": 1037564, "start": 10382.64, "end": 10385.64, "text": " and then you can add them to the output.", "tokens": [50714, 293, 550, 291, 393, 909, 552, 281, 264, 5598, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2434, "seek": 1037564, "start": 10385.64, "end": 10387.64, "text": " So you had some X values here,", "tokens": [50864, 407, 291, 632, 512, 1783, 4190, 510, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2435, "seek": 1037564, "start": 10387.64, "end": 10390.64, "text": " go through the RELU, and then you had some wrap around.", "tokens": [50964, 352, 807, 264, 497, 3158, 52, 11, 293, 550, 291, 632, 512, 7019, 926, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2436, "seek": 1037564, "start": 10390.64, "end": 10394.64, "text": " And then right here, you simply add them together", "tokens": [51114, 400, 550, 558, 510, 11, 291, 2935, 909, 552, 1214, 51314], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2437, "seek": 1037564, "start": 10394.64, "end": 10397.64, "text": " and you normalize them using some encod layer norm,", "tokens": [51314, 293, 291, 2710, 1125, 552, 1228, 512, 2058, 378, 4583, 2026, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2438, "seek": 1037564, "start": 10397.64, "end": 10399.64, "text": " which we're going to cover in a little bit.", "tokens": [51464, 597, 321, 434, 516, 281, 2060, 294, 257, 707, 857, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2439, "seek": 1037564, "start": 10399.64, "end": 10403.64, "text": " And the reason why residual connections", "tokens": [51564, 400, 264, 1778, 983, 27980, 9271, 51764], "temperature": 0.0, "avg_logprob": -0.09929827639931127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.005383573472499847}, {"id": 2440, "seek": 1040364, "start": 10403.64, "end": 10406.64, "text": " are so useful in transformers", "tokens": [50364, 366, 370, 4420, 294, 4088, 433, 50514], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2441, "seek": 1040364, "start": 10406.64, "end": 10409.64, "text": " is because when you have a really deep neural network,", "tokens": [50514, 307, 570, 562, 291, 362, 257, 534, 2452, 18161, 3209, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2442, "seek": 1040364, "start": 10409.64, "end": 10412.64, "text": " a lot of the information is actually forgotten", "tokens": [50664, 257, 688, 295, 264, 1589, 307, 767, 11832, 50814], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2443, "seek": 1040364, "start": 10412.64, "end": 10414.64, "text": " in the first steps.", "tokens": [50814, 294, 264, 700, 4439, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2444, "seek": 1040364, "start": 10414.64, "end": 10417.64, "text": " So if you have your first view encoder layers", "tokens": [50914, 407, 498, 291, 362, 428, 700, 1910, 2058, 19866, 7914, 51064], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2445, "seek": 1040364, "start": 10417.64, "end": 10419.64, "text": " and your first view decoder layers,", "tokens": [51064, 293, 428, 700, 1910, 979, 19866, 7914, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2446, "seek": 1040364, "start": 10419.64, "end": 10422.64, "text": " a lot of the information here is going to be forgotten", "tokens": [51164, 257, 688, 295, 264, 1589, 510, 307, 516, 281, 312, 11832, 51314], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2447, "seek": 1040364, "start": 10422.64, "end": 10424.64, "text": " because it's not being carried through.", "tokens": [51314, 570, 309, 311, 406, 885, 9094, 807, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2448, "seek": 1040364, "start": 10424.64, "end": 10428.64, "text": " The first steps of it aren't explicitly being carried through", "tokens": [51414, 440, 700, 4439, 295, 309, 3212, 380, 20803, 885, 9094, 807, 51614], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2449, "seek": 1040364, "start": 10428.64, "end": 10432.64, "text": " and sort of skipped through the functions.", "tokens": [51614, 293, 1333, 295, 30193, 807, 264, 6828, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0478248949404116, "compression_ratio": 1.9417040358744395, "no_speech_prob": 0.021936967968940735}, {"id": 2450, "seek": 1043264, "start": 10432.64, "end": 10436.64, "text": " And yeah, you can sort of see how they would just be forgotten.", "tokens": [50364, 400, 1338, 11, 291, 393, 1333, 295, 536, 577, 436, 576, 445, 312, 11832, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2451, "seek": 1043264, "start": 10436.64, "end": 10439.64, "text": " So residual connections are sort of just a cheat", "tokens": [50564, 407, 27980, 9271, 366, 1333, 295, 445, 257, 17470, 50714], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2452, "seek": 1043264, "start": 10439.64, "end": 10441.64, "text": " for getting around that,", "tokens": [50714, 337, 1242, 926, 300, 11, 50814], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2453, "seek": 1043264, "start": 10441.64, "end": 10443.64, "text": " for not having deep neural networks forget things", "tokens": [50814, 337, 406, 1419, 2452, 18161, 9590, 2870, 721, 50914], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2454, "seek": 1043264, "start": 10443.64, "end": 10445.64, "text": " from the beginning,", "tokens": [50914, 490, 264, 2863, 11, 51014], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2455, "seek": 1043264, "start": 10445.64, "end": 10447.64, "text": " and having them all sort of work together to the same degree.", "tokens": [51014, 293, 1419, 552, 439, 1333, 295, 589, 1214, 281, 264, 912, 4314, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2456, "seek": 1043264, "start": 10447.64, "end": 10450.64, "text": " So residual connections are great that way.", "tokens": [51114, 407, 27980, 9271, 366, 869, 300, 636, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2457, "seek": 1043264, "start": 10450.64, "end": 10453.64, "text": " And then, you know, at the end there,", "tokens": [51264, 400, 550, 11, 291, 458, 11, 412, 264, 917, 456, 11, 51414], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2458, "seek": 1043264, "start": 10453.64, "end": 10456.64, "text": " you would add them together and then normalize.", "tokens": [51414, 291, 576, 909, 552, 1214, 293, 550, 2710, 1125, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2459, "seek": 1043264, "start": 10456.64, "end": 10459.64, "text": " And there's two different ways that you can do this add a norm.", "tokens": [51564, 400, 456, 311, 732, 819, 2098, 300, 291, 393, 360, 341, 909, 257, 2026, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0788951317469279, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00035694948746822774}, {"id": 2460, "seek": 1045964, "start": 10459.64, "end": 10462.64, "text": " There's add a norm and then norm and add.", "tokens": [50364, 821, 311, 909, 257, 2026, 293, 550, 2026, 293, 909, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2461, "seek": 1045964, "start": 10462.64, "end": 10467.64, "text": " So these are two different separate architectures", "tokens": [50514, 407, 613, 366, 732, 819, 4994, 6331, 1303, 50764], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2462, "seek": 1045964, "start": 10467.64, "end": 10470.64, "text": " that you can do in transformers.", "tokens": [50764, 300, 291, 393, 360, 294, 4088, 433, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2463, "seek": 1045964, "start": 10470.64, "end": 10474.64, "text": " And both of these are sort of like meta architectures.", "tokens": [50914, 400, 1293, 295, 613, 366, 1333, 295, 411, 19616, 6331, 1303, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2464, "seek": 1045964, "start": 10474.64, "end": 10480.64, "text": " But pretty much pre-norm is the normalize then add,", "tokens": [51114, 583, 1238, 709, 659, 12, 13403, 307, 264, 2710, 1125, 550, 909, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2465, "seek": 1045964, "start": 10480.64, "end": 10482.64, "text": " and then post-norm is add then normalize.", "tokens": [51414, 293, 550, 2183, 12, 13403, 307, 909, 550, 2710, 1125, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2466, "seek": 1045964, "start": 10482.64, "end": 10486.64, "text": " So in this attention is all you need paper", "tokens": [51514, 407, 294, 341, 3202, 307, 439, 291, 643, 3035, 51714], "temperature": 0.0, "avg_logprob": -0.1078193187713623, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004467809572815895}, {"id": 2467, "seek": 1048664, "start": 10486.64, "end": 10491.64, "text": " proposed by a bunch of research scientists was", "tokens": [50364, 10348, 538, 257, 3840, 295, 2132, 7708, 390, 50614], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2468, "seek": 1048664, "start": 10491.64, "end": 10495.64, "text": " initially you want to add these,", "tokens": [50614, 9105, 291, 528, 281, 909, 613, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2469, "seek": 1048664, "start": 10495.64, "end": 10499.64, "text": " you want to add these together and then normalize them.", "tokens": [50814, 291, 528, 281, 909, 613, 1214, 293, 550, 2710, 1125, 552, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2470, "seek": 1048664, "start": 10499.64, "end": 10505.64, "text": " So that is what we call the post-norm architecture.", "tokens": [51014, 407, 300, 307, 437, 321, 818, 264, 2183, 12, 13403, 9482, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2471, "seek": 1048664, "start": 10505.64, "end": 10508.64, "text": " And then pre-norm is just flip them around.", "tokens": [51314, 400, 550, 659, 12, 13403, 307, 445, 7929, 552, 926, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2472, "seek": 1048664, "start": 10508.64, "end": 10513.64, "text": " So I've actually done some testing with pre-norm and post-norm", "tokens": [51464, 407, 286, 600, 767, 1096, 512, 4997, 365, 659, 12, 13403, 293, 2183, 12, 13403, 51714], "temperature": 0.0, "avg_logprob": -0.09103281286698353, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.005301052704453468}, {"id": 2473, "seek": 1051364, "start": 10513.64, "end": 10517.64, "text": " and the original transformer paper", "tokens": [50364, 293, 264, 3380, 31782, 3035, 50564], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2474, "seek": 1051364, "start": 10517.64, "end": 10521.64, "text": " turned out to be quite actually a lot better,", "tokens": [50564, 3574, 484, 281, 312, 1596, 767, 257, 688, 1101, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2475, "seek": 1051364, "start": 10521.64, "end": 10524.64, "text": " at least for training very small language models.", "tokens": [50764, 412, 1935, 337, 3097, 588, 1359, 2856, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2476, "seek": 1051364, "start": 10524.64, "end": 10526.64, "text": " If you're training bigger ones, it might be different,", "tokens": [50914, 759, 291, 434, 3097, 3801, 2306, 11, 309, 1062, 312, 819, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2477, "seek": 1051364, "start": 10526.64, "end": 10531.64, "text": " but essentially we're just going to go by the rules that we use in here.", "tokens": [51014, 457, 4476, 321, 434, 445, 516, 281, 352, 538, 264, 4474, 300, 321, 764, 294, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2478, "seek": 1051364, "start": 10531.64, "end": 10532.64, "text": " So add a norm.", "tokens": [51264, 407, 909, 257, 2026, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2479, "seek": 1051364, "start": 10532.64, "end": 10533.64, "text": " We're not going to do norm and add.", "tokens": [51314, 492, 434, 406, 516, 281, 360, 2026, 293, 909, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2480, "seek": 1051364, "start": 10533.64, "end": 10537.64, "text": " Add a norm in this video specifically because it works better", "tokens": [51364, 5349, 257, 2026, 294, 341, 960, 4682, 570, 309, 1985, 1101, 51564], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2481, "seek": 1051364, "start": 10537.64, "end": 10541.64, "text": " and we just don't want to break any of the rules and go outside of it", "tokens": [51564, 293, 321, 445, 500, 380, 528, 281, 1821, 604, 295, 264, 4474, 293, 352, 2380, 295, 309, 51764], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2482, "seek": 1051364, "start": 10541.64, "end": 10542.64, "text": " because then that starts to get confusing.", "tokens": [51764, 570, 550, 300, 3719, 281, 483, 13181, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09105504505218022, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.012428720481693745}, {"id": 2483, "seek": 1054264, "start": 10542.64, "end": 10545.64, "text": " And actually if you watch the Andre Carpathi lecture", "tokens": [50364, 400, 767, 498, 291, 1159, 264, 20667, 2741, 31852, 72, 7991, 50514], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2484, "seek": 1054264, "start": 10545.64, "end": 10547.64, "text": " on building GPTs from scratch,", "tokens": [50514, 322, 2390, 26039, 33424, 490, 8459, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2485, "seek": 1054264, "start": 10547.64, "end": 10553.64, "text": " he actually implemented it in the pre-norm way.", "tokens": [50614, 415, 767, 12270, 309, 294, 264, 659, 12, 13403, 636, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2486, "seek": 1054264, "start": 10553.64, "end": 10555.64, "text": " So normalize then add.", "tokens": [50914, 407, 2710, 1125, 550, 909, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2487, "seek": 1054264, "start": 10555.64, "end": 10558.64, "text": " So yeah, based on my experience,", "tokens": [51014, 407, 1338, 11, 2361, 322, 452, 1752, 11, 51164], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2488, "seek": 1054264, "start": 10558.64, "end": 10565.64, "text": " what I've done on my computer here is the post-norm architecture works quite better.", "tokens": [51164, 437, 286, 600, 1096, 322, 452, 3820, 510, 307, 264, 2183, 12, 13403, 9482, 1985, 1596, 1101, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2489, "seek": 1054264, "start": 10565.64, "end": 10567.64, "text": " So that's why we're going to use it.", "tokens": [51514, 407, 300, 311, 983, 321, 434, 516, 281, 764, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2490, "seek": 1054264, "start": 10567.64, "end": 10570.64, "text": " We're going to do add then normalize.", "tokens": [51614, 492, 434, 516, 281, 360, 909, 550, 2710, 1125, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13218326568603517, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0022868711967021227}, {"id": 2491, "seek": 1057064, "start": 10570.64, "end": 10575.64, "text": " So then we essentially feed this into a feedforward network", "tokens": [50364, 407, 550, 321, 4476, 3154, 341, 666, 257, 3154, 13305, 3209, 50614], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2492, "seek": 1057064, "start": 10575.64, "end": 10576.64, "text": " which we covered earlier.", "tokens": [50614, 597, 321, 5343, 3071, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2493, "seek": 1057064, "start": 10576.64, "end": 10580.64, "text": " And then how did it go?", "tokens": [50664, 400, 550, 577, 630, 309, 352, 30, 50864], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2494, "seek": 1057064, "start": 10580.64, "end": 10583.64, "text": " So we're encoder.", "tokens": [50864, 407, 321, 434, 2058, 19866, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2495, "seek": 1057064, "start": 10583.64, "end": 10588.64, "text": " We do a residual connection from here to here", "tokens": [51014, 492, 360, 257, 27980, 4984, 490, 510, 281, 510, 51264], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2496, "seek": 1057064, "start": 10588.64, "end": 10595.64, "text": " and then another residual connection from outside of our feedforward network.", "tokens": [51264, 293, 550, 1071, 27980, 4984, 490, 2380, 295, 527, 3154, 13305, 3209, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14200857602632963, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.005467670504003763}, {"id": 2497, "seek": 1059564, "start": 10595.64, "end": 10599.64, "text": " So each time we're doing some other things like some, you know,", "tokens": [50364, 407, 1184, 565, 321, 434, 884, 512, 661, 721, 411, 512, 11, 291, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2498, "seek": 1059564, "start": 10599.64, "end": 10601.64, "text": " some computation blocks in here,", "tokens": [50564, 512, 24903, 8474, 294, 510, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2499, "seek": 1059564, "start": 10601.64, "end": 10603.64, "text": " we're going to have a rest connection.", "tokens": [50664, 321, 434, 516, 281, 362, 257, 1472, 4984, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2500, "seek": 1059564, "start": 10603.64, "end": 10606.64, "text": " Same with our feedforward rest connection.", "tokens": [50764, 10635, 365, 527, 3154, 13305, 1472, 4984, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2501, "seek": 1059564, "start": 10606.64, "end": 10609.64, "text": " And then of course the output from here,", "tokens": [50914, 400, 550, 295, 1164, 264, 5598, 490, 510, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2502, "seek": 1059564, "start": 10609.64, "end": 10611.64, "text": " just when it exits,", "tokens": [51064, 445, 562, 309, 44183, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2503, "seek": 1059564, "start": 10611.64, "end": 10613.64, "text": " it's going to feed into the next encoder block", "tokens": [51164, 309, 311, 516, 281, 3154, 666, 264, 958, 2058, 19866, 3461, 51264], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2504, "seek": 1059564, "start": 10613.64, "end": 10615.64, "text": " if it's not the last encoder.", "tokens": [51264, 498, 309, 311, 406, 264, 1036, 2058, 19866, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2505, "seek": 1059564, "start": 10615.64, "end": 10617.64, "text": " So this one is going to do all this.", "tokens": [51364, 407, 341, 472, 307, 516, 281, 360, 439, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2506, "seek": 1059564, "start": 10617.64, "end": 10618.64, "text": " It's going to feed into that one.", "tokens": [51464, 467, 311, 516, 281, 3154, 666, 300, 472, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2507, "seek": 1059564, "start": 10618.64, "end": 10619.64, "text": " It's going to do the same thing.", "tokens": [51514, 467, 311, 516, 281, 360, 264, 912, 551, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2508, "seek": 1059564, "start": 10619.64, "end": 10620.64, "text": " Feed into this one.", "tokens": [51564, 33720, 666, 341, 472, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2509, "seek": 1059564, "start": 10620.64, "end": 10622.64, "text": " Going to feed into that one.", "tokens": [51614, 10963, 281, 3154, 666, 300, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09821078436715262, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.025952598080039024}, {"id": 2510, "seek": 1062264, "start": 10622.64, "end": 10626.64, "text": " And then the output of this is going to feed into each of these decoders,", "tokens": [50364, 400, 550, 264, 5598, 295, 341, 307, 516, 281, 3154, 666, 1184, 295, 613, 979, 378, 433, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2511, "seek": 1062264, "start": 10626.64, "end": 10629.64, "text": " all the same information.", "tokens": [50564, 439, 264, 912, 1589, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2512, "seek": 1062264, "start": 10629.64, "end": 10635.64, "text": " And yeah, so that's a little bit scoped in as to what these encoders look like.", "tokens": [50714, 400, 1338, 11, 370, 300, 311, 257, 707, 857, 795, 27277, 294, 382, 281, 437, 613, 2058, 378, 433, 574, 411, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2513, "seek": 1062264, "start": 10635.64, "end": 10638.64, "text": " So now that you know what the encoder looks like,", "tokens": [51014, 407, 586, 300, 291, 458, 437, 264, 2058, 19866, 1542, 411, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2514, "seek": 1062264, "start": 10638.64, "end": 10639.64, "text": " what the feedforward looks like,", "tokens": [51164, 437, 264, 3154, 13305, 1542, 411, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2515, "seek": 1062264, "start": 10639.64, "end": 10641.64, "text": " we're going to go into multi-head attention,", "tokens": [51214, 321, 434, 516, 281, 352, 666, 4825, 12, 1934, 3202, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2516, "seek": 1062264, "start": 10641.64, "end": 10643.64, "text": " sort of the premise,", "tokens": [51314, 1333, 295, 264, 22045, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2517, "seek": 1062264, "start": 10643.64, "end": 10646.64, "text": " sort of the highlight of the transformer architecture", "tokens": [51414, 1333, 295, 264, 5078, 295, 264, 31782, 9482, 51564], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2518, "seek": 1062264, "start": 10646.64, "end": 10648.64, "text": " and why it's so important.", "tokens": [51564, 293, 983, 309, 311, 370, 1021, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07722591731859291, "compression_ratio": 1.7860262008733625, "no_speech_prob": 0.002115188166499138}, {"id": 2519, "seek": 1064864, "start": 10648.64, "end": 10651.64, "text": " So multi-head attention,", "tokens": [50364, 407, 4825, 12, 1934, 3202, 11, 50514], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2520, "seek": 1064864, "start": 10651.64, "end": 10653.64, "text": " we call it multi-head attention", "tokens": [50514, 321, 818, 309, 4825, 12, 1934, 3202, 50614], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2521, "seek": 1064864, "start": 10653.64, "end": 10655.64, "text": " because there are a bunch of these different heads", "tokens": [50614, 570, 456, 366, 257, 3840, 295, 613, 819, 8050, 50714], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2522, "seek": 1064864, "start": 10655.64, "end": 10657.64, "text": " learning different semantic info", "tokens": [50714, 2539, 819, 47982, 13614, 50814], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2523, "seek": 1064864, "start": 10657.64, "end": 10659.64, "text": " from a unique perspective.", "tokens": [50814, 490, 257, 3845, 4585, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2524, "seek": 1064864, "start": 10659.64, "end": 10662.64, "text": " So let's say you have 10 different people", "tokens": [50914, 407, 718, 311, 584, 291, 362, 1266, 819, 561, 51064], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2525, "seek": 1064864, "start": 10662.64, "end": 10665.64, "text": " looking at the same book.", "tokens": [51064, 1237, 412, 264, 912, 1446, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2526, "seek": 1064864, "start": 10665.64, "end": 10667.64, "text": " If you have 10 different people,", "tokens": [51214, 759, 291, 362, 1266, 819, 561, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2527, "seek": 1064864, "start": 10667.64, "end": 10672.64, "text": " let's say they're all reading the same Harry Potter book.", "tokens": [51314, 718, 311, 584, 436, 434, 439, 3760, 264, 912, 9378, 18115, 1446, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2528, "seek": 1064864, "start": 10672.64, "end": 10674.64, "text": " These different people,", "tokens": [51564, 1981, 819, 561, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2529, "seek": 1064864, "start": 10674.64, "end": 10677.64, "text": " they might have different cognitive abilities.", "tokens": [51664, 436, 1062, 362, 819, 15605, 11582, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08820047472963238, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0022515091113746166}, {"id": 2530, "seek": 1067764, "start": 10677.64, "end": 10679.64, "text": " They might have different IQs.", "tokens": [50364, 814, 1062, 362, 819, 28921, 82, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2531, "seek": 1067764, "start": 10679.64, "end": 10681.64, "text": " They might have been raised in different ways.", "tokens": [50464, 814, 1062, 362, 668, 6005, 294, 819, 2098, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2532, "seek": 1067764, "start": 10681.64, "end": 10683.64, "text": " So they might interpret things differently.", "tokens": [50564, 407, 436, 1062, 7302, 721, 7614, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2533, "seek": 1067764, "start": 10683.64, "end": 10686.64, "text": " They might look at little things in that book", "tokens": [50664, 814, 1062, 574, 412, 707, 721, 294, 300, 1446, 50814], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2534, "seek": 1067764, "start": 10686.64, "end": 10688.64, "text": " and their mind will,", "tokens": [50814, 293, 641, 1575, 486, 11, 50914], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2535, "seek": 1067764, "start": 10688.64, "end": 10690.64, "text": " they'll imagine different scenarios,", "tokens": [50914, 436, 603, 3811, 819, 15077, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2536, "seek": 1067764, "start": 10690.64, "end": 10692.64, "text": " different environments from the book.", "tokens": [51014, 819, 12388, 490, 264, 1446, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2537, "seek": 1067764, "start": 10692.64, "end": 10696.64, "text": " And essentially why this is so valuable", "tokens": [51114, 400, 4476, 983, 341, 307, 370, 8263, 51314], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2538, "seek": 1067764, "start": 10696.64, "end": 10699.64, "text": " is because we don't just want to have one person,", "tokens": [51314, 307, 570, 321, 500, 380, 445, 528, 281, 362, 472, 954, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2539, "seek": 1067764, "start": 10699.64, "end": 10701.64, "text": " just one perspective on this.", "tokens": [51464, 445, 472, 4585, 322, 341, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2540, "seek": 1067764, "start": 10701.64, "end": 10704.64, "text": " We want to have a bunch of different heads in parallel", "tokens": [51564, 492, 528, 281, 362, 257, 3840, 295, 819, 8050, 294, 8952, 51714], "temperature": 0.0, "avg_logprob": -0.07662280546415837, "compression_ratio": 1.8481012658227849, "no_speech_prob": 0.005383502226322889}, {"id": 2541, "seek": 1070464, "start": 10704.64, "end": 10710.64, "text": " looking at this same piece of data", "tokens": [50364, 1237, 412, 341, 912, 2522, 295, 1412, 50664], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2542, "seek": 1070464, "start": 10710.64, "end": 10713.64, "text": " because they're all going to capture different things about it.", "tokens": [50664, 570, 436, 434, 439, 516, 281, 7983, 819, 721, 466, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2543, "seek": 1070464, "start": 10713.64, "end": 10716.64, "text": " And keep in mind each of these heads,", "tokens": [50814, 400, 1066, 294, 1575, 1184, 295, 613, 8050, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2544, "seek": 1070464, "start": 10716.64, "end": 10718.64, "text": " each of these heads in parallel,", "tokens": [50964, 1184, 295, 613, 8050, 294, 8952, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2545, "seek": 1070464, "start": 10718.64, "end": 10720.64, "text": " these different perspectives,", "tokens": [51064, 613, 819, 16766, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2546, "seek": 1070464, "start": 10720.64, "end": 10722.64, "text": " they have different learnable parameters.", "tokens": [51164, 436, 362, 819, 1466, 712, 9834, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2547, "seek": 1070464, "start": 10722.64, "end": 10724.64, "text": " So they're not all the same one", "tokens": [51264, 407, 436, 434, 406, 439, 264, 912, 472, 51364], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2548, "seek": 1070464, "start": 10724.64, "end": 10726.64, "text": " looking at this piece of data.", "tokens": [51364, 1237, 412, 341, 2522, 295, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2549, "seek": 1070464, "start": 10726.64, "end": 10729.64, "text": " They're actually,", "tokens": [51464, 814, 434, 767, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2550, "seek": 1070464, "start": 10729.64, "end": 10731.64, "text": " they all have different learnable parameters.", "tokens": [51614, 436, 439, 362, 819, 1466, 712, 9834, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09684419130024156, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.008707297034561634}, {"id": 2551, "seek": 1073164, "start": 10731.64, "end": 10734.64, "text": " So you have a bunch of these", "tokens": [50364, 407, 291, 362, 257, 3840, 295, 613, 50514], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2552, "seek": 1073164, "start": 10734.64, "end": 10736.64, "text": " at the same time learning different things", "tokens": [50514, 412, 264, 912, 565, 2539, 819, 721, 50614], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2553, "seek": 1073164, "start": 10736.64, "end": 10738.64, "text": " and that's why it's so powerful.", "tokens": [50614, 293, 300, 311, 983, 309, 311, 370, 4005, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2554, "seek": 1073164, "start": 10738.64, "end": 10744.64, "text": " So this scale.product attention runs in parallel,", "tokens": [50714, 407, 341, 4373, 13, 33244, 3202, 6676, 294, 8952, 11, 51014], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2555, "seek": 1073164, "start": 10744.64, "end": 10746.64, "text": " which means we can scale that to the GPU,", "tokens": [51014, 597, 1355, 321, 393, 4373, 300, 281, 264, 18407, 11, 51114], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2556, "seek": 1073164, "start": 10746.64, "end": 10748.64, "text": " which is very useful.", "tokens": [51114, 597, 307, 588, 4420, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2557, "seek": 1073164, "start": 10748.64, "end": 10750.64, "text": " It's good to touch on that.", "tokens": [51214, 467, 311, 665, 281, 2557, 322, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2558, "seek": 1073164, "start": 10750.64, "end": 10752.64, "text": " Anything with the GPU that you can accelerate", "tokens": [51314, 11998, 365, 264, 18407, 300, 291, 393, 21341, 51414], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2559, "seek": 1073164, "start": 10752.64, "end": 10754.64, "text": " is just an automatic win", "tokens": [51414, 307, 445, 364, 12509, 1942, 51514], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2560, "seek": 1073164, "start": 10754.64, "end": 10759.64, "text": " because parallelism is great in machine learning.", "tokens": [51514, 570, 8952, 1434, 307, 869, 294, 3479, 2539, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0998858642578125, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.009120595641434193}, {"id": 2561, "seek": 1075964, "start": 10759.64, "end": 10761.64, "text": " Why not have parallelism, right?", "tokens": [50364, 1545, 406, 362, 8952, 1434, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2562, "seek": 1075964, "start": 10761.64, "end": 10763.64, "text": " If it's just going to be running the CPU, what's the point?", "tokens": [50464, 759, 309, 311, 445, 516, 281, 312, 2614, 264, 13199, 11, 437, 311, 264, 935, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2563, "seek": 1075964, "start": 10763.64, "end": 10765.64, "text": " That's why we love GPUs.", "tokens": [50564, 663, 311, 983, 321, 959, 18407, 82, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2564, "seek": 1075964, "start": 10765.64, "end": 10767.64, "text": " Anyways, yeah.", "tokens": [50664, 15585, 11, 1338, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2565, "seek": 1075964, "start": 10767.64, "end": 10769.64, "text": " So you're going to have these different,", "tokens": [50764, 407, 291, 434, 516, 281, 362, 613, 819, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2566, "seek": 1075964, "start": 10769.64, "end": 10771.64, "text": " you're going to have these things that are called keys,", "tokens": [50864, 291, 434, 516, 281, 362, 613, 721, 300, 366, 1219, 9317, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2567, "seek": 1075964, "start": 10771.64, "end": 10773.64, "text": " queries and values.", "tokens": [50964, 24109, 293, 4190, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2568, "seek": 1075964, "start": 10773.64, "end": 10775.64, "text": " I'll touch on those in a second here", "tokens": [51064, 286, 603, 2557, 322, 729, 294, 257, 1150, 510, 51164], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2569, "seek": 1075964, "start": 10775.64, "end": 10777.64, "text": " because keys, queries and values", "tokens": [51164, 570, 9317, 11, 24109, 293, 4190, 51264], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2570, "seek": 1075964, "start": 10777.64, "end": 10779.64, "text": " sort of point to self-attention,", "tokens": [51264, 1333, 295, 935, 281, 2698, 12, 1591, 1251, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2571, "seek": 1075964, "start": 10779.64, "end": 10781.64, "text": " which is literally the entire point of the transformer.", "tokens": [51364, 597, 307, 3736, 264, 2302, 935, 295, 264, 31782, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2572, "seek": 1075964, "start": 10781.64, "end": 10783.64, "text": " Transformer wouldn't really mean anything", "tokens": [51464, 27938, 260, 2759, 380, 534, 914, 1340, 51564], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2573, "seek": 1075964, "start": 10783.64, "end": 10785.64, "text": " without self-attention.", "tokens": [51564, 1553, 2698, 12, 1591, 1251, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2574, "seek": 1075964, "start": 10785.64, "end": 10787.64, "text": " So I'll touch on those in a second here", "tokens": [51664, 407, 286, 603, 2557, 322, 729, 294, 257, 1150, 510, 51764], "temperature": 0.0, "avg_logprob": -0.08457880406766324, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.005466711241751909}, {"id": 2575, "seek": 1078764, "start": 10787.64, "end": 10789.64, "text": " and we'll actually delve deeper", "tokens": [50364, 293, 321, 603, 767, 43098, 7731, 50464], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2576, "seek": 1078764, "start": 10789.64, "end": 10791.64, "text": " as we hit this sort of block.", "tokens": [50464, 382, 321, 2045, 341, 1333, 295, 3461, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2577, "seek": 1078764, "start": 10791.64, "end": 10793.64, "text": " But yeah, you have these keys, queries and values.", "tokens": [50564, 583, 1338, 11, 291, 362, 613, 9317, 11, 24109, 293, 4190, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2578, "seek": 1078764, "start": 10793.64, "end": 10795.64, "text": " They go into scale.product attention.", "tokens": [50664, 814, 352, 666, 4373, 13, 33244, 3202, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2579, "seek": 1078764, "start": 10795.64, "end": 10797.64, "text": " So a bunch of these running in parallel", "tokens": [50764, 407, 257, 3840, 295, 613, 2614, 294, 8952, 50864], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2580, "seek": 1078764, "start": 10797.64, "end": 10799.64, "text": " and then you concatenate the results", "tokens": [50864, 293, 550, 291, 1588, 7186, 473, 264, 3542, 50964], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2581, "seek": 1078764, "start": 10799.64, "end": 10801.64, "text": " from all these different heads running in parallel.", "tokens": [50964, 490, 439, 613, 819, 8050, 2614, 294, 8952, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2582, "seek": 1078764, "start": 10801.64, "end": 10803.64, "text": " You have all these different people.", "tokens": [51064, 509, 362, 439, 613, 819, 561, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2583, "seek": 1078764, "start": 10803.64, "end": 10805.64, "text": " You concatenate all of them,", "tokens": [51164, 509, 1588, 7186, 473, 439, 295, 552, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2584, "seek": 1078764, "start": 10805.64, "end": 10807.64, "text": " you generalize it,", "tokens": [51264, 291, 2674, 1125, 309, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2585, "seek": 1078764, "start": 10807.64, "end": 10809.64, "text": " and then you apply a transformation", "tokens": [51364, 293, 550, 291, 3079, 257, 9887, 51464], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2586, "seek": 1078764, "start": 10809.64, "end": 10811.64, "text": " to a linear transformation", "tokens": [51464, 281, 257, 8213, 9887, 51564], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2587, "seek": 1078764, "start": 10811.64, "end": 10813.64, "text": " to pretty much summarize that", "tokens": [51564, 281, 1238, 709, 20858, 300, 51664], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2588, "seek": 1078764, "start": 10813.64, "end": 10816.64, "text": " and then do your add a norm,", "tokens": [51664, 293, 550, 360, 428, 909, 257, 2026, 11, 51814], "temperature": 0.0, "avg_logprob": -0.08940069603197502, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.007693585008382797}, {"id": 2589, "seek": 1081664, "start": 10816.64, "end": 10818.64, "text": " then pay for a network.", "tokens": [50364, 550, 1689, 337, 257, 3209, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2590, "seek": 1081664, "start": 10818.64, "end": 10820.64, "text": " So that's what's going on in multi-head attention.", "tokens": [50464, 407, 300, 311, 437, 311, 516, 322, 294, 4825, 12, 1934, 3202, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2591, "seek": 1081664, "start": 10820.64, "end": 10822.64, "text": " You're just doing a bunch of self-attentions", "tokens": [50564, 509, 434, 445, 884, 257, 3840, 295, 2698, 12, 1591, 317, 626, 50664], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2592, "seek": 1081664, "start": 10822.64, "end": 10824.64, "text": " in parallel, concatenating,", "tokens": [50664, 294, 8952, 11, 1588, 7186, 990, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2593, "seek": 1081664, "start": 10824.64, "end": 10826.64, "text": " and then continuing on with this part.", "tokens": [50764, 293, 550, 9289, 322, 365, 341, 644, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2594, "seek": 1081664, "start": 10826.64, "end": 10828.64, "text": " So scale.product attention.", "tokens": [50864, 407, 4373, 13, 33244, 3202, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2595, "seek": 1081664, "start": 10828.64, "end": 10830.64, "text": " What is that?", "tokens": [50964, 708, 307, 300, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2596, "seek": 1081664, "start": 10830.64, "end": 10832.64, "text": " So let's just start from the ground up here.", "tokens": [51064, 407, 718, 311, 445, 722, 490, 264, 2727, 493, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2597, "seek": 1081664, "start": 10832.64, "end": 10834.64, "text": " We'll just go from left to right.", "tokens": [51164, 492, 603, 445, 352, 490, 1411, 281, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2598, "seek": 1081664, "start": 10834.64, "end": 10836.64, "text": " So you have your keys, queries and values.", "tokens": [51264, 407, 291, 362, 428, 9317, 11, 24109, 293, 4190, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2599, "seek": 1081664, "start": 10836.64, "end": 10838.64, "text": " What do your keys do?", "tokens": [51364, 708, 360, 428, 9317, 360, 30, 51464], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2600, "seek": 1081664, "start": 10838.64, "end": 10840.64, "text": " Well, a key is", "tokens": [51464, 1042, 11, 257, 2141, 307, 51564], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2601, "seek": 1081664, "start": 10840.64, "end": 10842.64, "text": " let's just say you have a token and a sentence.", "tokens": [51564, 718, 311, 445, 584, 291, 362, 257, 14862, 293, 257, 8174, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2602, "seek": 1081664, "start": 10842.64, "end": 10844.64, "text": " Okay?", "tokens": [51664, 1033, 30, 51764], "temperature": 0.0, "avg_logprob": -0.10939127333620761, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.00046547423698939383}, {"id": 2603, "seek": 1084464, "start": 10844.64, "end": 10846.64, "text": " So if you have", "tokens": [50364, 407, 498, 291, 362, 50464], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2604, "seek": 1084464, "start": 10846.64, "end": 10848.64, "text": " let me just", "tokens": [50464, 718, 385, 445, 50564], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2605, "seek": 1084464, "start": 10848.64, "end": 10850.64, "text": " roll down here to a good example.", "tokens": [50564, 3373, 760, 510, 281, 257, 665, 1365, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2606, "seek": 1084464, "start": 10850.64, "end": 10852.64, "text": " So", "tokens": [50664, 407, 50764], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2607, "seek": 1084464, "start": 10852.64, "end": 10854.64, "text": " self-attention", "tokens": [50764, 2698, 12, 1591, 1251, 50864], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2608, "seek": 1084464, "start": 10854.64, "end": 10856.64, "text": " uses", "tokens": [50864, 4960, 50964], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2609, "seek": 1084464, "start": 10856.64, "end": 10858.64, "text": " keys, queries and values.", "tokens": [50964, 9317, 11, 24109, 293, 4190, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2610, "seek": 1084464, "start": 10858.64, "end": 10860.64, "text": " Self-attention helps", "tokens": [51064, 16348, 12, 1591, 1251, 3665, 51164], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2611, "seek": 1084464, "start": 10860.64, "end": 10862.64, "text": " identify", "tokens": [51164, 5876, 51264], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2612, "seek": 1084464, "start": 10862.64, "end": 10864.64, "text": " which of these tokens in a sentence", "tokens": [51264, 597, 295, 613, 22667, 294, 257, 8174, 51364], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2613, "seek": 1084464, "start": 10864.64, "end": 10866.64, "text": " in any given sentence are more important", "tokens": [51364, 294, 604, 2212, 8174, 366, 544, 1021, 51464], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2614, "seek": 1084464, "start": 10866.64, "end": 10868.64, "text": " and how much attention", "tokens": [51464, 293, 577, 709, 3202, 51564], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2615, "seek": 1084464, "start": 10868.64, "end": 10870.64, "text": " you should pay", "tokens": [51564, 291, 820, 1689, 51664], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2616, "seek": 1084464, "start": 10870.64, "end": 10872.64, "text": " to each of those characters or words, whatever you're using.", "tokens": [51664, 281, 1184, 295, 729, 4342, 420, 2283, 11, 2035, 291, 434, 1228, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07351305066924735, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.0010004069190472364}, {"id": 2617, "seek": 1087264, "start": 10872.64, "end": 10874.64, "text": " We'll just use words", "tokens": [50364, 492, 603, 445, 764, 2283, 50464], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2618, "seek": 1087264, "start": 10874.64, "end": 10876.64, "text": " to", "tokens": [50464, 281, 50564], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2619, "seek": 1087264, "start": 10876.64, "end": 10878.64, "text": " make it easier to understand for the purpose of this video.", "tokens": [50564, 652, 309, 3571, 281, 1223, 337, 264, 4334, 295, 341, 960, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2620, "seek": 1087264, "start": 10878.64, "end": 10880.64, "text": " But", "tokens": [50664, 583, 50764], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2621, "seek": 1087264, "start": 10880.64, "end": 10882.64, "text": " essentially imagine you have", "tokens": [50764, 4476, 3811, 291, 362, 50864], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2622, "seek": 1087264, "start": 10884.64, "end": 10886.64, "text": " these two sentences here.", "tokens": [50964, 613, 732, 16579, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2623, "seek": 1087264, "start": 10886.64, "end": 10888.64, "text": " So you have", "tokens": [51064, 407, 291, 362, 51164], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2624, "seek": 1087264, "start": 10888.64, "end": 10890.64, "text": " let me bring out my little piece of text.", "tokens": [51164, 718, 385, 1565, 484, 452, 707, 2522, 295, 2487, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2625, "seek": 1087264, "start": 10890.64, "end": 10892.64, "text": " So you have", "tokens": [51264, 407, 291, 362, 51364], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2626, "seek": 1087264, "start": 10894.64, "end": 10896.64, "text": " that didn't work.", "tokens": [51464, 300, 994, 380, 589, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2627, "seek": 1087264, "start": 10898.64, "end": 10900.64, "text": " So imagine you have", "tokens": [51664, 407, 3811, 291, 362, 51764], "temperature": 0.0, "avg_logprob": -0.08146398945858604, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.006690739654004574}, {"id": 2628, "seek": 1090264, "start": 10902.64, "end": 10904.64, "text": " server, can I have the check?", "tokens": [50364, 7154, 11, 393, 286, 362, 264, 1520, 30, 50464], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2629, "seek": 1090264, "start": 10904.64, "end": 10906.64, "text": " And then you have", "tokens": [50464, 400, 550, 291, 362, 50564], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2630, "seek": 1090264, "start": 10908.64, "end": 10910.64, "text": " and you have", "tokens": [50664, 293, 291, 362, 50764], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2631, "seek": 1090264, "start": 10910.64, "end": 10912.64, "text": " looks like I crashed the server.", "tokens": [50764, 1542, 411, 286, 24190, 264, 7154, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2632, "seek": 1090264, "start": 10912.64, "end": 10914.64, "text": " So", "tokens": [50864, 407, 50964], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2633, "seek": 1090264, "start": 10914.64, "end": 10916.64, "text": " I mean, both of these have", "tokens": [50964, 286, 914, 11, 1293, 295, 613, 362, 51064], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2634, "seek": 1090264, "start": 10916.64, "end": 10918.64, "text": " the word server in them, but they mean different things.", "tokens": [51064, 264, 1349, 7154, 294, 552, 11, 457, 436, 914, 819, 721, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2635, "seek": 1090264, "start": 10918.64, "end": 10920.64, "text": " Server meaning like the waiter", "tokens": [51164, 25684, 3620, 411, 264, 45389, 51264], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2636, "seek": 1090264, "start": 10920.64, "end": 10922.64, "text": " or the waitress or whoever", "tokens": [51264, 420, 264, 1699, 735, 420, 11387, 51364], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2637, "seek": 1090264, "start": 10922.64, "end": 10924.64, "text": " is billing", "tokens": [51364, 307, 35618, 51464], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2638, "seek": 1090264, "start": 10924.64, "end": 10926.64, "text": " you at the end of your restaurant visit.", "tokens": [51464, 291, 412, 264, 917, 295, 428, 6383, 3441, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2639, "seek": 1090264, "start": 10926.64, "end": 10928.64, "text": " And then looks like I crashed the server", "tokens": [51564, 400, 550, 1542, 411, 286, 24190, 264, 7154, 51664], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2640, "seek": 1090264, "start": 10928.64, "end": 10930.64, "text": " is like, oh, there's actually a server running", "tokens": [51664, 307, 411, 11, 1954, 11, 456, 311, 767, 257, 7154, 2614, 51764], "temperature": 0.0, "avg_logprob": -0.09608094327084653, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0025101236533373594}, {"id": 2641, "seek": 1093064, "start": 10930.64, "end": 10932.64, "text": " in the cloud, not like a person", "tokens": [50364, 294, 264, 4588, 11, 406, 411, 257, 954, 50464], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2642, "seek": 1093064, "start": 10932.64, "end": 10934.64, "text": " that's billing me, but an actual server.", "tokens": [50464, 300, 311, 35618, 385, 11, 457, 364, 3539, 7154, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2643, "seek": 1093064, "start": 10934.64, "end": 10936.64, "text": " That's maybe running a video game.", "tokens": [50564, 663, 311, 1310, 2614, 257, 960, 1216, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2644, "seek": 1093064, "start": 10936.64, "end": 10938.64, "text": " And", "tokens": [50664, 400, 50764], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2645, "seek": 1093064, "start": 10938.64, "end": 10940.64, "text": " these are two different things. So what attention can do", "tokens": [50764, 613, 366, 732, 819, 721, 13, 407, 437, 3202, 393, 360, 50864], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2646, "seek": 1093064, "start": 10940.64, "end": 10942.64, "text": " is it can actually identify", "tokens": [50864, 307, 309, 393, 767, 5876, 50964], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2647, "seek": 1093064, "start": 10942.64, "end": 10944.64, "text": " which words would get attention here.", "tokens": [50964, 597, 2283, 576, 483, 3202, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2648, "seek": 1093064, "start": 10944.64, "end": 10946.64, "text": " So it can say", "tokens": [51064, 407, 309, 393, 584, 51164], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2649, "seek": 1093064, "start": 10946.64, "end": 10948.64, "text": " server, can I have the check?", "tokens": [51164, 7154, 11, 393, 286, 362, 264, 1520, 30, 51264], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2650, "seek": 1093064, "start": 10948.64, "end": 10950.64, "text": " Can I have?", "tokens": [51264, 1664, 286, 362, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2651, "seek": 1093064, "start": 10950.64, "end": 10952.64, "text": " So it's maybe you're looking", "tokens": [51364, 407, 309, 311, 1310, 291, 434, 1237, 51464], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2652, "seek": 1093064, "start": 10952.64, "end": 10954.64, "text": " for something you're looking for the check", "tokens": [51464, 337, 746, 291, 434, 1237, 337, 264, 1520, 51564], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2653, "seek": 1093064, "start": 10954.64, "end": 10956.64, "text": " and then server", "tokens": [51564, 293, 550, 7154, 51664], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2654, "seek": 1093064, "start": 10956.64, "end": 10958.64, "text": " is like, oh, well in this", "tokens": [51664, 307, 411, 11, 1954, 11, 731, 294, 341, 51764], "temperature": 0.0, "avg_logprob": -0.11473422754006307, "compression_ratio": 1.7339055793991416, "no_speech_prob": 0.007457345258444548}, {"id": 2655, "seek": 1095864, "start": 10958.64, "end": 10960.64, "text": " in this particular sequence or in this", "tokens": [50364, 294, 341, 1729, 8310, 420, 294, 341, 50464], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2656, "seek": 1095864, "start": 10960.64, "end": 10962.64, "text": " in the sentiment of this sentence here", "tokens": [50464, 294, 264, 16149, 295, 341, 8174, 510, 50564], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2657, "seek": 1095864, "start": 10962.64, "end": 10964.64, "text": " server", "tokens": [50564, 7154, 50664], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2658, "seek": 1095864, "start": 10964.64, "end": 10966.64, "text": " is specifically tied to", "tokens": [50664, 307, 4682, 9601, 281, 50764], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2659, "seek": 1095864, "start": 10966.64, "end": 10968.64, "text": " this one meaning, maybe a human", "tokens": [50764, 341, 472, 3620, 11, 1310, 257, 1952, 50864], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2660, "seek": 1095864, "start": 10968.64, "end": 10970.64, "text": " someone at a restaurant", "tokens": [50864, 1580, 412, 257, 6383, 50964], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2661, "seek": 1095864, "start": 10970.64, "end": 10972.64, "text": " and then crash", "tokens": [50964, 293, 550, 8252, 51064], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2662, "seek": 1095864, "start": 10972.64, "end": 10974.64, "text": " the server", "tokens": [51064, 264, 7154, 51164], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2663, "seek": 1095864, "start": 10974.64, "end": 10976.64, "text": " crash is going to get a very high attention", "tokens": [51164, 8252, 307, 516, 281, 483, 257, 588, 1090, 3202, 51264], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2664, "seek": 1095864, "start": 10976.64, "end": 10978.64, "text": " score because", "tokens": [51264, 6175, 570, 51364], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2665, "seek": 1095864, "start": 10978.64, "end": 10980.64, "text": " you don't normally", "tokens": [51364, 291, 500, 380, 5646, 51464], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2666, "seek": 1095864, "start": 10980.64, "end": 10982.64, "text": " crash a server at a restaurant", "tokens": [51464, 8252, 257, 7154, 412, 257, 6383, 51564], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2667, "seek": 1095864, "start": 10982.64, "end": 10984.64, "text": " that doesn't particularly make sense.", "tokens": [51564, 300, 1177, 380, 4098, 652, 2020, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2668, "seek": 1095864, "start": 10984.64, "end": 10986.64, "text": " So", "tokens": [51664, 407, 51764], "temperature": 0.0, "avg_logprob": -0.09886162331763734, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0025905047077685595}, {"id": 2669, "seek": 1098664, "start": 10986.64, "end": 10988.64, "text": " if you have different words like this", "tokens": [50364, 498, 291, 362, 819, 2283, 411, 341, 50464], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2670, "seek": 1098664, "start": 10988.64, "end": 10990.64, "text": " what self-attention will do", "tokens": [50464, 437, 2698, 12, 1591, 1251, 486, 360, 50564], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2671, "seek": 1098664, "start": 10990.64, "end": 10992.64, "text": " is it will learn", "tokens": [50564, 307, 309, 486, 1466, 50664], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2672, "seek": 1098664, "start": 10992.64, "end": 10994.64, "text": " which words in the sentence", "tokens": [50664, 597, 2283, 294, 264, 8174, 50764], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2673, "seek": 1098664, "start": 10994.64, "end": 10996.64, "text": " are actually more important", "tokens": [50764, 366, 767, 544, 1021, 50864], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2674, "seek": 1098664, "start": 10996.64, "end": 10998.64, "text": " and which words should", "tokens": [50864, 293, 597, 2283, 820, 50964], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2675, "seek": 1098664, "start": 10998.64, "end": 11000.64, "text": " pay more attention to.", "tokens": [50964, 1689, 544, 3202, 281, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2676, "seek": 1098664, "start": 11000.64, "end": 11002.64, "text": " So that's really all that's going on here", "tokens": [51064, 407, 300, 311, 534, 439, 300, 311, 516, 322, 510, 51164], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2677, "seek": 1098664, "start": 11002.64, "end": 11004.64, "text": " and", "tokens": [51164, 293, 51264], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2678, "seek": 1098664, "start": 11004.64, "end": 11006.64, "text": " the key", "tokens": [51264, 264, 2141, 51364], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2679, "seek": 1098664, "start": 11006.64, "end": 11008.64, "text": " is essentially going to emit", "tokens": [51364, 307, 4476, 516, 281, 32084, 51464], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2680, "seek": 1098664, "start": 11008.64, "end": 11010.64, "text": " a different", "tokens": [51464, 257, 819, 51564], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2681, "seek": 1098664, "start": 11010.64, "end": 11012.64, "text": " it's going to emit", "tokens": [51564, 309, 311, 516, 281, 32084, 51664], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2682, "seek": 1098664, "start": 11012.64, "end": 11014.64, "text": " a little tensor", "tokens": [51664, 257, 707, 40863, 51764], "temperature": 0.0, "avg_logprob": -0.12263595296981487, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.01743493415415287}, {"id": 2683, "seek": 1101464, "start": 11014.64, "end": 11016.64, "text": " here saying", "tokens": [50364, 510, 1566, 50464], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2684, "seek": 1101464, "start": 11016.64, "end": 11018.64, "text": " what do I contain", "tokens": [50464, 437, 360, 286, 5304, 50564], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2685, "seek": 1101464, "start": 11018.64, "end": 11020.64, "text": " and then query", "tokens": [50564, 293, 550, 14581, 50664], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2686, "seek": 1101464, "start": 11020.64, "end": 11022.64, "text": " is going to say", "tokens": [50664, 307, 516, 281, 584, 50764], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2687, "seek": 1101464, "start": 11022.64, "end": 11024.64, "text": " what am I looking for?", "tokens": [50764, 437, 669, 286, 1237, 337, 30, 50864], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2688, "seek": 1101464, "start": 11024.64, "end": 11026.64, "text": " So what's going to happen", "tokens": [50864, 407, 437, 311, 516, 281, 1051, 50964], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2689, "seek": 1101464, "start": 11026.64, "end": 11028.64, "text": " is if these, let's say", "tokens": [50964, 307, 498, 613, 11, 718, 311, 584, 51064], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2690, "seek": 1101464, "start": 11028.64, "end": 11030.64, "text": " server, it's going to look for things like", "tokens": [51064, 7154, 11, 309, 311, 516, 281, 574, 337, 721, 411, 51164], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2691, "seek": 1101464, "start": 11030.64, "end": 11032.64, "text": " check or crashed", "tokens": [51164, 1520, 420, 24190, 51264], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2692, "seek": 1101464, "start": 11032.64, "end": 11034.64, "text": " so if it sees crashed", "tokens": [51264, 370, 498, 309, 8194, 24190, 51364], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2693, "seek": 1101464, "start": 11034.64, "end": 11036.64, "text": " then that means the key and the query", "tokens": [51364, 550, 300, 1355, 264, 2141, 293, 264, 14581, 51464], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2694, "seek": 1101464, "start": 11036.64, "end": 11038.64, "text": " are going to multiply", "tokens": [51464, 366, 516, 281, 12972, 51564], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2695, "seek": 1101464, "start": 11038.64, "end": 11040.64, "text": " and it's going to get a very high attention score", "tokens": [51564, 293, 309, 311, 516, 281, 483, 257, 588, 1090, 3202, 6175, 51664], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2696, "seek": 1101464, "start": 11040.64, "end": 11042.64, "text": " but if you have something", "tokens": [51664, 457, 498, 291, 362, 746, 51764], "temperature": 0.0, "avg_logprob": -0.12652459851017706, "compression_ratio": 1.75, "no_speech_prob": 0.014053719118237495}, {"id": 2697, "seek": 1104264, "start": 11042.64, "end": 11044.64, "text": " like", "tokens": [50364, 411, 50464], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2698, "seek": 1104264, "start": 11044.64, "end": 11046.64, "text": " it's like", "tokens": [50464, 309, 311, 411, 50564], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2699, "seek": 1104264, "start": 11046.64, "end": 11048.64, "text": " there's literally almost any sentence", "tokens": [50564, 456, 311, 3736, 1920, 604, 8174, 50664], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2700, "seek": 1104264, "start": 11048.64, "end": 11050.64, "text": " so that doesn't mean much.", "tokens": [50664, 370, 300, 1177, 380, 914, 709, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2701, "seek": 1104264, "start": 11050.64, "end": 11052.64, "text": " We're not going to pay attention to those words", "tokens": [50764, 492, 434, 406, 516, 281, 1689, 3202, 281, 729, 2283, 50864], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2702, "seek": 1104264, "start": 11052.64, "end": 11054.64, "text": " so that's going to get a very low attention score", "tokens": [50864, 370, 300, 311, 516, 281, 483, 257, 588, 2295, 3202, 6175, 50964], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2703, "seek": 1104264, "start": 11054.64, "end": 11056.64, "text": " and all attention", "tokens": [50964, 293, 439, 3202, 51064], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2704, "seek": 1104264, "start": 11056.64, "end": 11058.64, "text": " is you're just dot-producting", "tokens": [51064, 307, 291, 434, 445, 5893, 12, 33244, 278, 51164], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2705, "seek": 1104264, "start": 11058.64, "end": 11060.64, "text": " these vectors together.", "tokens": [51164, 613, 18875, 1214, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2706, "seek": 1104264, "start": 11060.64, "end": 11062.64, "text": " So you get a key", "tokens": [51264, 407, 291, 483, 257, 2141, 51364], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2707, "seek": 1104264, "start": 11062.64, "end": 11064.64, "text": " and a query, you dot-product them", "tokens": [51364, 293, 257, 14581, 11, 291, 5893, 12, 33244, 552, 51464], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2708, "seek": 1104264, "start": 11064.64, "end": 11066.64, "text": " we already went over dot-products", "tokens": [51464, 321, 1217, 1437, 670, 5893, 12, 33244, 82, 51564], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2709, "seek": 1104264, "start": 11066.64, "end": 11068.64, "text": " in this course before", "tokens": [51564, 294, 341, 1164, 949, 51664], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2710, "seek": 1104264, "start": 11068.64, "end": 11070.64, "text": " and then", "tokens": [51664, 293, 550, 51764], "temperature": 0.0, "avg_logprob": -0.0951938972816811, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.011680476367473602}, {"id": 2711, "seek": 1107064, "start": 11070.64, "end": 11072.64, "text": " this is a little bit of a confusing part", "tokens": [50364, 341, 307, 257, 707, 857, 295, 257, 13181, 644, 50464], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2712, "seek": 1107064, "start": 11072.64, "end": 11074.64, "text": " is you just scale", "tokens": [50464, 307, 291, 445, 4373, 50564], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2713, "seek": 1107064, "start": 11074.64, "end": 11076.64, "text": " by one over the", "tokens": [50564, 538, 472, 670, 264, 50664], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2714, "seek": 1107064, "start": 11076.64, "end": 11078.64, "text": " square root", "tokens": [50664, 3732, 5593, 50764], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2715, "seek": 1107064, "start": 11078.64, "end": 11080.64, "text": " of the length of a row", "tokens": [50764, 295, 264, 4641, 295, 257, 5386, 50864], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2716, "seek": 1107064, "start": 11080.64, "end": 11082.64, "text": " in the keys or queries matrix", "tokens": [50864, 294, 264, 9317, 420, 24109, 8141, 50964], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2717, "seek": 1107064, "start": 11082.64, "end": 11084.64, "text": " otherwise known as", "tokens": [50964, 5911, 2570, 382, 51064], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2718, "seek": 1107064, "start": 11084.64, "end": 11086.64, "text": " DK.", "tokens": [51064, 31934, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2719, "seek": 1107064, "start": 11086.64, "end": 11088.64, "text": " So let's say we have", "tokens": [51164, 407, 718, 311, 584, 321, 362, 51264], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2720, "seek": 1107064, "start": 11088.64, "end": 11090.64, "text": " our key and our query", "tokens": [51264, 527, 2141, 293, 527, 14581, 51364], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2721, "seek": 1107064, "start": 11090.64, "end": 11092.64, "text": " these are all going to be the same length by the way.", "tokens": [51364, 613, 366, 439, 516, 281, 312, 264, 912, 4641, 538, 264, 636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2722, "seek": 1107064, "start": 11092.64, "end": 11094.64, "text": " Let's say our keys", "tokens": [51464, 961, 311, 584, 527, 9317, 51564], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2723, "seek": 1107064, "start": 11094.64, "end": 11096.64, "text": " is", "tokens": [51564, 307, 51664], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2724, "seek": 1107064, "start": 11096.64, "end": 11098.64, "text": " maybe our keys is going to be like", "tokens": [51664, 1310, 527, 9317, 307, 516, 281, 312, 411, 51764], "temperature": 0.0, "avg_logprob": -0.11506711519681491, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00228694686666131}, {"id": 2725, "seek": 1109864, "start": 11098.64, "end": 11100.64, "text": " 10 characters long", "tokens": [50364, 1266, 4342, 938, 50464], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2726, "seek": 1109864, "start": 11100.64, "end": 11102.64, "text": " our keys are going to be 10 characters long as well", "tokens": [50464, 527, 9317, 366, 516, 281, 312, 1266, 4342, 938, 382, 731, 50564], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2727, "seek": 1109864, "start": 11102.64, "end": 11104.64, "text": " so it's going to do", "tokens": [50564, 370, 309, 311, 516, 281, 360, 50664], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2728, "seek": 1109864, "start": 11104.64, "end": 11106.64, "text": " one over the square root of 10", "tokens": [50664, 472, 670, 264, 3732, 5593, 295, 1266, 50764], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2729, "seek": 1109864, "start": 11106.64, "end": 11108.64, "text": " if that makes sense", "tokens": [50764, 498, 300, 1669, 2020, 50864], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2730, "seek": 1109864, "start": 11108.64, "end": 11110.64, "text": " and so", "tokens": [50864, 293, 370, 50964], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2731, "seek": 1109864, "start": 11110.64, "end": 11112.64, "text": " that's just", "tokens": [50964, 300, 311, 445, 51064], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2732, "seek": 1109864, "start": 11112.64, "end": 11114.64, "text": " essentially a way of preventing", "tokens": [51064, 4476, 257, 636, 295, 19965, 51164], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2733, "seek": 1109864, "start": 11114.64, "end": 11116.64, "text": " these dot-products", "tokens": [51164, 613, 5893, 12, 33244, 82, 51264], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2734, "seek": 1109864, "start": 11116.64, "end": 11118.64, "text": " from exploding", "tokens": [51264, 490, 35175, 51364], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2735, "seek": 1109864, "start": 11118.64, "end": 11120.64, "text": " we want to scale them because", "tokens": [51364, 321, 528, 281, 4373, 552, 570, 51464], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2736, "seek": 1109864, "start": 11120.64, "end": 11122.64, "text": " as we have", "tokens": [51464, 382, 321, 362, 51564], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2737, "seek": 1109864, "start": 11122.64, "end": 11124.64, "text": " as the length of it increases", "tokens": [51564, 382, 264, 4641, 295, 309, 8637, 51664], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2738, "seek": 1109864, "start": 11124.64, "end": 11126.64, "text": " so will the", "tokens": [51664, 370, 486, 264, 51764], "temperature": 0.0, "avg_logprob": -0.0743820567925771, "compression_ratio": 1.6436170212765957, "no_speech_prob": 0.0071196043863892555}, {"id": 2739, "seek": 1112664, "start": 11126.64, "end": 11128.64, "text": " ending dot-product", "tokens": [50364, 8121, 5893, 12, 33244, 50464], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2740, "seek": 1112664, "start": 11128.64, "end": 11130.64, "text": " because there's more of these to multiply", "tokens": [50464, 570, 456, 311, 544, 295, 613, 281, 12972, 50564], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2741, "seek": 1112664, "start": 11130.64, "end": 11132.64, "text": " so we pretty much just want to", "tokens": [50564, 370, 321, 1238, 709, 445, 528, 281, 50664], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2742, "seek": 1112664, "start": 11132.64, "end": 11134.64, "text": " scale it by using", "tokens": [50664, 4373, 309, 538, 1228, 50764], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2743, "seek": 1112664, "start": 11134.64, "end": 11136.64, "text": " an inverse square root", "tokens": [50764, 364, 17340, 3732, 5593, 50864], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2744, "seek": 1112664, "start": 11136.64, "end": 11138.64, "text": " and that will just help us with", "tokens": [50864, 293, 300, 486, 445, 854, 505, 365, 50964], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2745, "seek": 1112664, "start": 11138.64, "end": 11140.64, "text": " scaling make sure nothing explodes", "tokens": [50964, 21589, 652, 988, 1825, 42610, 51064], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2746, "seek": 1112664, "start": 11140.64, "end": 11142.64, "text": " in unnecessary ways", "tokens": [51064, 294, 19350, 2098, 51164], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2747, "seek": 1112664, "start": 11142.64, "end": 11144.64, "text": " and then", "tokens": [51164, 293, 550, 51264], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2748, "seek": 1112664, "start": 11144.64, "end": 11146.64, "text": " the next little important part", "tokens": [51264, 264, 958, 707, 1021, 644, 51364], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2749, "seek": 1112664, "start": 11146.64, "end": 11148.64, "text": " is using tort.trill", "tokens": [51364, 307, 1228, 10806, 13, 6903, 373, 51464], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2750, "seek": 1112664, "start": 11148.64, "end": 11150.64, "text": " which I imagine we went over in our examples here", "tokens": [51464, 597, 286, 3811, 321, 1437, 670, 294, 527, 5110, 510, 51564], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2751, "seek": 1112664, "start": 11150.64, "end": 11152.64, "text": " trill", "tokens": [51564, 504, 373, 51664], "temperature": 0.0, "avg_logprob": -0.10220374057167454, "compression_ratio": 1.6028708133971292, "no_speech_prob": 0.0037064687348902225}, {"id": 2752, "seek": 1115264, "start": 11152.64, "end": 11154.64, "text": " yeah", "tokens": [50364, 1338, 50464], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2753, "seek": 1115264, "start": 11154.64, "end": 11156.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2754, "seek": 1115264, "start": 11156.64, "end": 11158.64, "text": " you can see that", "tokens": [50564, 291, 393, 536, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2755, "seek": 1115264, "start": 11158.64, "end": 11160.64, "text": " it's a diagonal", "tokens": [50664, 309, 311, 257, 21539, 50764], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2756, "seek": 1115264, "start": 11160.64, "end": 11162.64, "text": " it's a left triangular", "tokens": [50764, 309, 311, 257, 1411, 38190, 50864], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2757, "seek": 1115264, "start": 11162.64, "end": 11164.64, "text": " matrix of ones", "tokens": [50864, 8141, 295, 2306, 50964], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2758, "seek": 1115264, "start": 11164.64, "end": 11166.64, "text": " and these aren't going to be ones", "tokens": [50964, 293, 613, 3212, 380, 516, 281, 312, 2306, 51064], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2759, "seek": 1115264, "start": 11166.64, "end": 11168.64, "text": " in our self-attention here", "tokens": [51064, 294, 527, 2698, 12, 1591, 1251, 510, 51164], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2760, "seek": 1115264, "start": 11168.64, "end": 11170.64, "text": " in our tort.trill or masking", "tokens": [51164, 294, 527, 10806, 13, 6903, 373, 420, 31226, 51264], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2761, "seek": 1115264, "start": 11170.64, "end": 11172.64, "text": " what this is going to be", "tokens": [51264, 437, 341, 307, 516, 281, 312, 51364], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2762, "seek": 1115264, "start": 11172.64, "end": 11174.64, "text": " is", "tokens": [51364, 307, 51464], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2763, "seek": 1115264, "start": 11174.64, "end": 11176.64, "text": " the scores at each time step", "tokens": [51464, 264, 13444, 412, 1184, 565, 1823, 51564], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2764, "seek": 1115264, "start": 11176.64, "end": 11178.64, "text": " the combination of scores", "tokens": [51564, 264, 6562, 295, 13444, 51664], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2765, "seek": 1115264, "start": 11178.64, "end": 11180.64, "text": " at each time step", "tokens": [51664, 412, 1184, 565, 1823, 51764], "temperature": 0.0, "avg_logprob": -0.08351384556811789, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.013010328635573387}, {"id": 2766, "seek": 1118064, "start": 11180.64, "end": 11182.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2767, "seek": 1118064, "start": 11182.64, "end": 11184.64, "text": " if we've only gone", "tokens": [50464, 498, 321, 600, 787, 2780, 50564], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2768, "seek": 1118064, "start": 11184.64, "end": 11186.64, "text": " if we're only looking at the first", "tokens": [50564, 498, 321, 434, 787, 1237, 412, 264, 700, 50664], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2769, "seek": 1118064, "start": 11186.64, "end": 11188.64, "text": " time step", "tokens": [50664, 565, 1823, 50764], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2770, "seek": 1118064, "start": 11188.64, "end": 11190.64, "text": " we should not have access to the rest of things", "tokens": [50764, 321, 820, 406, 362, 2105, 281, 264, 1472, 295, 721, 50864], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2771, "seek": 1118064, "start": 11190.64, "end": 11192.64, "text": " or else that would be cheating", "tokens": [50864, 420, 1646, 300, 576, 312, 18309, 50964], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2772, "seek": 1118064, "start": 11192.64, "end": 11194.64, "text": " we shouldn't be allowed to look ahead", "tokens": [50964, 321, 4659, 380, 312, 4350, 281, 574, 2286, 51064], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2773, "seek": 1118064, "start": 11194.64, "end": 11196.64, "text": " because we haven't actually produced these yet", "tokens": [51064, 570, 321, 2378, 380, 767, 7126, 613, 1939, 51164], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2774, "seek": 1118064, "start": 11196.64, "end": 11198.64, "text": " we need to produce these before we can", "tokens": [51164, 321, 643, 281, 5258, 613, 949, 321, 393, 51264], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2775, "seek": 1118064, "start": 11198.64, "end": 11200.64, "text": " put them into perspective", "tokens": [51264, 829, 552, 666, 4585, 51364], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2776, "seek": 1118064, "start": 11200.64, "end": 11202.64, "text": " and put a weight on them", "tokens": [51364, 293, 829, 257, 3364, 322, 552, 51464], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2777, "seek": 1118064, "start": 11202.64, "end": 11204.64, "text": " so we're going to set all these to zero", "tokens": [51464, 370, 321, 434, 516, 281, 992, 439, 613, 281, 4018, 51564], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2778, "seek": 1118064, "start": 11204.64, "end": 11206.64, "text": " and then we go to the next time step", "tokens": [51564, 293, 550, 321, 352, 281, 264, 958, 565, 1823, 51664], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2779, "seek": 1118064, "start": 11206.64, "end": 11208.64, "text": " so now we've just generated this", "tokens": [51664, 370, 586, 321, 600, 445, 10833, 341, 51764], "temperature": 0.0, "avg_logprob": -0.06075488934751417, "compression_ratio": 1.8454935622317596, "no_speech_prob": 0.01000755000859499}, {"id": 2780, "seek": 1120864, "start": 11208.64, "end": 11210.64, "text": " one we haven't generated these yet", "tokens": [50364, 472, 321, 2378, 380, 10833, 613, 1939, 50464], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2781, "seek": 1120864, "start": 11210.64, "end": 11212.64, "text": " so we can't look at them", "tokens": [50464, 370, 321, 393, 380, 574, 412, 552, 50564], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2782, "seek": 1120864, "start": 11212.64, "end": 11214.64, "text": " and then as we go more and more", "tokens": [50564, 293, 550, 382, 321, 352, 544, 293, 544, 50664], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2783, "seek": 1120864, "start": 11214.64, "end": 11216.64, "text": " as the time step increases", "tokens": [50664, 382, 264, 565, 1823, 8637, 50764], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2784, "seek": 1120864, "start": 11216.64, "end": 11218.64, "text": " we know more and more context", "tokens": [50764, 321, 458, 544, 293, 544, 4319, 50864], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2785, "seek": 1120864, "start": 11218.64, "end": 11220.64, "text": " about all of these tokens", "tokens": [50864, 466, 439, 295, 613, 22667, 50964], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2786, "seek": 1120864, "start": 11220.64, "end": 11222.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2787, "seek": 1120864, "start": 11222.64, "end": 11224.64, "text": " that's all that's doing", "tokens": [51064, 300, 311, 439, 300, 311, 884, 51164], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2788, "seek": 1120864, "start": 11224.64, "end": 11226.64, "text": " mask attention is pretty much just saying", "tokens": [51164, 6094, 3202, 307, 1238, 709, 445, 1566, 51264], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2789, "seek": 1120864, "start": 11226.64, "end": 11228.64, "text": " we don't want to look into the future", "tokens": [51264, 321, 500, 380, 528, 281, 574, 666, 264, 2027, 51364], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2790, "seek": 1120864, "start": 11228.64, "end": 11230.64, "text": " we want to only guess with what we currently know", "tokens": [51364, 321, 528, 281, 787, 2041, 365, 437, 321, 4362, 458, 51464], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2791, "seek": 1120864, "start": 11230.64, "end": 11232.64, "text": " in our current time step", "tokens": [51464, 294, 527, 2190, 565, 1823, 51564], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2792, "seek": 1120864, "start": 11232.64, "end": 11234.64, "text": " and everything before it", "tokens": [51564, 293, 1203, 949, 309, 51664], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2793, "seek": 1120864, "start": 11234.64, "end": 11236.64, "text": " you can't jump into the future", "tokens": [51664, 291, 393, 380, 3012, 666, 264, 2027, 51764], "temperature": 0.0, "avg_logprob": -0.05161493660038353, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.007229739800095558}, {"id": 2794, "seek": 1123664, "start": 11236.64, "end": 11238.64, "text": " look at what happened in the past", "tokens": [50364, 574, 412, 437, 2011, 294, 264, 1791, 50464], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2795, "seek": 1123664, "start": 11238.64, "end": 11240.64, "text": " and do stuff based on that", "tokens": [50464, 293, 360, 1507, 2361, 322, 300, 50564], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2796, "seek": 1123664, "start": 11240.64, "end": 11242.64, "text": " same thing applies to life", "tokens": [50564, 912, 551, 13165, 281, 993, 50664], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2797, "seek": 1123664, "start": 11242.64, "end": 11244.64, "text": " you can't really skip to the future and say", "tokens": [50664, 291, 393, 380, 534, 10023, 281, 264, 2027, 293, 584, 50764], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2798, "seek": 1123664, "start": 11244.64, "end": 11246.64, "text": " hey if you do this you're going to be a billionaire", "tokens": [50764, 4177, 498, 291, 360, 341, 291, 434, 516, 281, 312, 257, 42358, 50864], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2799, "seek": 1123664, "start": 11246.64, "end": 11248.64, "text": " no that would be cheating", "tokens": [50864, 572, 300, 576, 312, 18309, 50964], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2800, "seek": 1123664, "start": 11248.64, "end": 11250.64, "text": " you're not allowed to do that", "tokens": [50964, 291, 434, 406, 4350, 281, 360, 300, 51064], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2801, "seek": 1123664, "start": 11250.64, "end": 11252.64, "text": " you can only look at the mistakes you made", "tokens": [51064, 291, 393, 787, 574, 412, 264, 8038, 291, 1027, 51164], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2802, "seek": 1123664, "start": 11252.64, "end": 11254.64, "text": " and say how can I become a billionaire", "tokens": [51164, 293, 584, 577, 393, 286, 1813, 257, 42358, 51264], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2803, "seek": 1123664, "start": 11254.64, "end": 11256.64, "text": " based on all these other mistakes that I made", "tokens": [51264, 2361, 322, 439, 613, 661, 8038, 300, 286, 1027, 51364], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2804, "seek": 1123664, "start": 11256.64, "end": 11258.64, "text": " how can I become as close to perfect as possible", "tokens": [51364, 577, 393, 286, 1813, 382, 1998, 281, 2176, 382, 1944, 51464], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2805, "seek": 1123664, "start": 11258.64, "end": 11260.64, "text": " which no one I can ever be perfect", "tokens": [51464, 597, 572, 472, 286, 393, 1562, 312, 2176, 51564], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2806, "seek": 1123664, "start": 11260.64, "end": 11262.64, "text": " but that's my little analogy for the day", "tokens": [51564, 457, 300, 311, 452, 707, 21663, 337, 264, 786, 51664], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2807, "seek": 1123664, "start": 11262.64, "end": 11264.64, "text": " so that's mask attention", "tokens": [51664, 370, 300, 311, 6094, 3202, 51764], "temperature": 0.0, "avg_logprob": -0.0660390853881836, "compression_ratio": 1.9291044776119404, "no_speech_prob": 0.016639824956655502}, {"id": 2808, "seek": 1126464, "start": 11264.64, "end": 11266.64, "text": " pretty much just not letting us skip time steps", "tokens": [50364, 1238, 709, 445, 406, 8295, 505, 10023, 565, 4439, 50464], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2809, "seek": 1126464, "start": 11266.64, "end": 11268.64, "text": " so that's fun", "tokens": [50464, 370, 300, 311, 1019, 50564], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2810, "seek": 1126464, "start": 11268.64, "end": 11270.64, "text": " let's continue", "tokens": [50564, 718, 311, 2354, 50664], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2811, "seek": 1126464, "start": 11270.64, "end": 11272.64, "text": " two more little things I want to touch on before I jump forward here", "tokens": [50664, 732, 544, 707, 721, 286, 528, 281, 2557, 322, 949, 286, 3012, 2128, 510, 50764], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2812, "seek": 1126464, "start": 11272.64, "end": 11274.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2813, "seek": 1126464, "start": 11274.64, "end": 11276.64, "text": " these keys, queries and values", "tokens": [50864, 613, 9317, 11, 24109, 293, 4190, 50964], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2814, "seek": 1126464, "start": 11276.64, "end": 11278.64, "text": " each of these are learned through a linear transformation", "tokens": [50964, 1184, 295, 613, 366, 3264, 807, 257, 8213, 9887, 51064], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2815, "seek": 1126464, "start": 11278.64, "end": 11280.64, "text": " just an end dot linear", "tokens": [51064, 445, 364, 917, 5893, 8213, 51164], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2816, "seek": 1126464, "start": 11280.64, "end": 11282.64, "text": " is applied", "tokens": [51164, 307, 6456, 51264], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2817, "seek": 1126464, "start": 11282.64, "end": 11284.64, "text": " and that's how we get our keys, queries and values", "tokens": [51264, 293, 300, 311, 577, 321, 483, 527, 9317, 11, 24109, 293, 4190, 51364], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2818, "seek": 1126464, "start": 11284.64, "end": 11286.64, "text": " so that's just a little", "tokens": [51364, 370, 300, 311, 445, 257, 707, 51464], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2819, "seek": 1126464, "start": 11286.64, "end": 11288.64, "text": " touching there if you're wondering how do we get those", "tokens": [51464, 11175, 456, 498, 291, 434, 6359, 577, 360, 321, 483, 729, 51564], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2820, "seek": 1126464, "start": 11288.64, "end": 11290.64, "text": " it's just an end dot linear transformation", "tokens": [51564, 309, 311, 445, 364, 917, 5893, 8213, 9887, 51664], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2821, "seek": 1126464, "start": 11290.64, "end": 11292.64, "text": " and then as for our", "tokens": [51664, 293, 550, 382, 337, 527, 51764], "temperature": 0.0, "avg_logprob": -0.08347104644775391, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.022955944761633873}, {"id": 2822, "seek": 1129264, "start": 11292.64, "end": 11294.64, "text": " masking we don't actually apply this all the time", "tokens": [50364, 31226, 321, 500, 380, 767, 3079, 341, 439, 264, 565, 50464], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2823, "seek": 1129264, "start": 11294.64, "end": 11296.64, "text": " you might have seen right here", "tokens": [50464, 291, 1062, 362, 1612, 558, 510, 50564], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2824, "seek": 1129264, "start": 11296.64, "end": 11298.64, "text": " we have", "tokens": [50564, 321, 362, 50664], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2825, "seek": 1129264, "start": 11298.64, "end": 11300.64, "text": " multi-head attention", "tokens": [50664, 4825, 12, 1934, 3202, 50764], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2826, "seek": 1129264, "start": 11300.64, "end": 11302.64, "text": " multi-head attention and then mask", "tokens": [50764, 4825, 12, 1934, 3202, 293, 550, 6094, 50864], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2827, "seek": 1129264, "start": 11302.64, "end": 11304.64, "text": " multi-head attention", "tokens": [50864, 4825, 12, 1934, 3202, 50964], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2828, "seek": 1129264, "start": 11304.64, "end": 11306.64, "text": " so this masked attention isn't used all the time", "tokens": [50964, 370, 341, 45249, 3202, 1943, 380, 1143, 439, 264, 565, 51064], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2829, "seek": 1129264, "start": 11306.64, "end": 11308.64, "text": " it's only used", "tokens": [51064, 309, 311, 787, 1143, 51164], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2830, "seek": 1129264, "start": 11308.64, "end": 11310.64, "text": " actually one out of the three attentions", "tokens": [51164, 767, 472, 484, 295, 264, 1045, 30980, 626, 51264], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2831, "seek": 1129264, "start": 11310.64, "end": 11312.64, "text": " we have per layer", "tokens": [51264, 321, 362, 680, 4583, 51364], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2832, "seek": 1129264, "start": 11312.64, "end": 11314.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2833, "seek": 1129264, "start": 11314.64, "end": 11316.64, "text": " I'll give you a little bit more information", "tokens": [51464, 286, 603, 976, 291, 257, 707, 857, 544, 1589, 51564], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2834, "seek": 1129264, "start": 11316.64, "end": 11318.64, "text": " about that as we", "tokens": [51564, 466, 300, 382, 321, 51664], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2835, "seek": 1129264, "start": 11318.64, "end": 11320.64, "text": " progress more and more into the architecture", "tokens": [51664, 4205, 544, 293, 544, 666, 264, 9482, 51764], "temperature": 0.0, "avg_logprob": -0.08172499916770241, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.006900775711983442}, {"id": 2836, "seek": 1132064, "start": 11320.64, "end": 11322.64, "text": " as we learn more about it", "tokens": [50364, 382, 321, 1466, 544, 466, 309, 50464], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2837, "seek": 1132064, "start": 11322.64, "end": 11324.64, "text": " I'm not going to dive into that", "tokens": [50464, 286, 478, 406, 516, 281, 9192, 666, 300, 50564], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2838, "seek": 1132064, "start": 11324.64, "end": 11326.64, "text": " quite yet though", "tokens": [50564, 1596, 1939, 1673, 50664], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2839, "seek": 1132064, "start": 11326.64, "end": 11328.64, "text": " so let's just continue on with what's going on", "tokens": [50664, 370, 718, 311, 445, 2354, 322, 365, 437, 311, 516, 322, 50764], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2840, "seek": 1132064, "start": 11328.64, "end": 11330.64, "text": " so we have a softmax", "tokens": [50764, 370, 321, 362, 257, 2787, 41167, 50864], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2841, "seek": 1132064, "start": 11330.64, "end": 11332.64, "text": " and why softmax important", "tokens": [50864, 293, 983, 2787, 41167, 1021, 50964], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2842, "seek": 1132064, "start": 11332.64, "end": 11334.64, "text": " well", "tokens": [50964, 731, 51064], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2843, "seek": 1132064, "start": 11334.64, "end": 11336.64, "text": " I actually mentioned earlier", "tokens": [51064, 286, 767, 2835, 3071, 51164], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2844, "seek": 1132064, "start": 11336.64, "end": 11338.64, "text": " softmax is not commonly used", "tokens": [51164, 2787, 41167, 307, 406, 12719, 1143, 51264], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2845, "seek": 1132064, "start": 11338.64, "end": 11340.64, "text": " as a normalization method", "tokens": [51264, 382, 257, 2710, 2144, 3170, 51364], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2846, "seek": 1132064, "start": 11340.64, "end": 11342.64, "text": " but here we're actually using", "tokens": [51364, 457, 510, 321, 434, 767, 1228, 51464], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2847, "seek": 1132064, "start": 11342.64, "end": 11344.64, "text": " softmax to normalize", "tokens": [51464, 2787, 41167, 281, 2710, 1125, 51564], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2848, "seek": 1132064, "start": 11344.64, "end": 11346.64, "text": " so when you have all of these", "tokens": [51564, 370, 562, 291, 362, 439, 295, 613, 51664], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2849, "seek": 1132064, "start": 11346.64, "end": 11348.64, "text": " when you have all of these", "tokens": [51664, 562, 291, 362, 439, 295, 613, 51764], "temperature": 0.0, "avg_logprob": -0.08543812463042932, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.020311424508690834}, {"id": 2850, "seek": 1134864, "start": 11348.64, "end": 11350.64, "text": " attention scores", "tokens": [50364, 3202, 13444, 50464], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2851, "seek": 1134864, "start": 11350.64, "end": 11352.64, "text": " essentially what the softmax is doing", "tokens": [50464, 4476, 437, 264, 2787, 41167, 307, 884, 50564], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2852, "seek": 1134864, "start": 11352.64, "end": 11354.64, "text": " is it's going to", "tokens": [50564, 307, 309, 311, 516, 281, 50664], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2853, "seek": 1134864, "start": 11354.64, "end": 11356.64, "text": " exponentiate and normalize all of these", "tokens": [50664, 37871, 13024, 293, 2710, 1125, 439, 295, 613, 50764], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2854, "seek": 1134864, "start": 11356.64, "end": 11358.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2855, "seek": 1134864, "start": 11358.64, "end": 11360.64, "text": " all of the attention scores that have scored", "tokens": [50864, 439, 295, 264, 3202, 13444, 300, 362, 18139, 50964], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2856, "seek": 1134864, "start": 11360.64, "end": 11362.64, "text": " high like maybe 50 to", "tokens": [50964, 1090, 411, 1310, 2625, 281, 51064], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2857, "seek": 1134864, "start": 11362.64, "end": 11364.64, "text": " 90% or whatever it is", "tokens": [51064, 4289, 4, 420, 2035, 309, 307, 51164], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2858, "seek": 1134864, "start": 11364.64, "end": 11366.64, "text": " those are going to take a massive effect", "tokens": [51164, 729, 366, 516, 281, 747, 257, 5994, 1802, 51264], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2859, "seek": 1134864, "start": 11366.64, "end": 11368.64, "text": " in that entire", "tokens": [51264, 294, 300, 2302, 51364], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2860, "seek": 1134864, "start": 11368.64, "end": 11370.64, "text": " attention", "tokens": [51364, 3202, 51464], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2861, "seek": 1134864, "start": 11370.64, "end": 11372.64, "text": " I guess tensor if you want to call it that", "tokens": [51464, 286, 2041, 40863, 498, 291, 528, 281, 818, 309, 300, 51564], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2862, "seek": 1134864, "start": 11374.64, "end": 11376.64, "text": " and that's important", "tokens": [51664, 293, 300, 311, 1021, 51764], "temperature": 0.0, "avg_logprob": -0.08894284566243489, "compression_ratio": 1.6903553299492386, "no_speech_prob": 0.005728502292186022}, {"id": 2863, "seek": 1137664, "start": 11376.64, "end": 11378.64, "text": " it might not seem important", "tokens": [50364, 309, 1062, 406, 1643, 1021, 50464], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2864, "seek": 1137664, "start": 11378.64, "end": 11380.64, "text": " but it's essentially just giving the model", "tokens": [50464, 457, 309, 311, 4476, 445, 2902, 264, 2316, 50564], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2865, "seek": 1137664, "start": 11380.64, "end": 11382.64, "text": " more confidence", "tokens": [50564, 544, 6687, 50664], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2866, "seek": 1137664, "start": 11382.64, "end": 11384.64, "text": " as to which tokens matter more", "tokens": [50664, 382, 281, 597, 22667, 1871, 544, 50764], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2867, "seek": 1137664, "start": 11384.64, "end": 11386.64, "text": " so for example", "tokens": [50764, 370, 337, 1365, 50864], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2868, "seek": 1137664, "start": 11386.64, "end": 11388.64, "text": " if we just", "tokens": [50864, 498, 321, 445, 50964], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2869, "seek": 1137664, "start": 11388.64, "end": 11390.64, "text": " did a normalization", "tokens": [50964, 630, 257, 2710, 2144, 51064], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2870, "seek": 1137664, "start": 11390.64, "end": 11392.64, "text": " we would", "tokens": [51064, 321, 576, 51164], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2871, "seek": 1137664, "start": 11392.64, "end": 11394.64, "text": " have words like server and crash", "tokens": [51164, 362, 2283, 411, 7154, 293, 8252, 51264], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2872, "seek": 1137664, "start": 11394.64, "end": 11396.64, "text": " and then server and check", "tokens": [51264, 293, 550, 7154, 293, 1520, 51364], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2873, "seek": 1137664, "start": 11396.64, "end": 11398.64, "text": " and then", "tokens": [51364, 293, 550, 51464], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2874, "seek": 1137664, "start": 11398.64, "end": 11400.64, "text": " you would just know", "tokens": [51464, 291, 576, 445, 458, 51564], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2875, "seek": 1137664, "start": 11400.64, "end": 11402.64, "text": " a decent amount about those", "tokens": [51564, 257, 8681, 2372, 466, 729, 51664], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2876, "seek": 1137664, "start": 11402.64, "end": 11404.64, "text": " those would pay attention to a decent amount", "tokens": [51664, 729, 576, 1689, 3202, 281, 257, 8681, 2372, 51764], "temperature": 0.0, "avg_logprob": -0.07487416523759083, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.0042629786767065525}, {"id": 2877, "seek": 1140464, "start": 11404.64, "end": 11406.64, "text": " because they multiply together quite well", "tokens": [50364, 570, 436, 12972, 1214, 1596, 731, 50464], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2878, "seek": 1140464, "start": 11406.64, "end": 11408.64, "text": " but if you softmax those", "tokens": [50464, 457, 498, 291, 2787, 41167, 729, 50564], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2879, "seek": 1140464, "start": 11408.64, "end": 11410.64, "text": " then it's like", "tokens": [50564, 550, 309, 311, 411, 50664], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2880, "seek": 1140464, "start": 11410.64, "end": 11412.64, "text": " those are almost the only characters that matter", "tokens": [50664, 729, 366, 1920, 264, 787, 4342, 300, 1871, 50764], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2881, "seek": 1140464, "start": 11412.64, "end": 11414.64, "text": " so it's looking at the context", "tokens": [50764, 370, 309, 311, 1237, 412, 264, 4319, 50864], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2882, "seek": 1140464, "start": 11414.64, "end": 11416.64, "text": " of those two", "tokens": [50864, 295, 729, 732, 50964], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2883, "seek": 1140464, "start": 11416.64, "end": 11418.64, "text": " and then we're sort of filling in", "tokens": [50964, 293, 550, 321, 434, 1333, 295, 10623, 294, 51064], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2884, "seek": 1140464, "start": 11418.64, "end": 11420.64, "text": " like we're learning about the rest of the sentence", "tokens": [51064, 411, 321, 434, 2539, 466, 264, 1472, 295, 264, 8174, 51164], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2885, "seek": 1140464, "start": 11420.64, "end": 11422.64, "text": " based on just the", "tokens": [51164, 2361, 322, 445, 264, 51264], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2886, "seek": 1140464, "start": 11422.64, "end": 11424.64, "text": " sentiment of those attention scores", "tokens": [51264, 16149, 295, 729, 3202, 13444, 51364], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2887, "seek": 1140464, "start": 11424.64, "end": 11426.64, "text": " because they're so high priority", "tokens": [51364, 570, 436, 434, 370, 1090, 9365, 51464], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2888, "seek": 1140464, "start": 11426.64, "end": 11428.64, "text": " because they multiply together", "tokens": [51464, 570, 436, 12972, 1214, 51564], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2889, "seek": 1140464, "start": 11428.64, "end": 11430.64, "text": " to such a high degree", "tokens": [51564, 281, 1270, 257, 1090, 4314, 51664], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2890, "seek": 1140464, "start": 11430.64, "end": 11432.64, "text": " we want to emphasize them", "tokens": [51664, 321, 528, 281, 16078, 552, 51764], "temperature": 0.0, "avg_logprob": -0.06812337067750124, "compression_ratio": 1.8805309734513274, "no_speech_prob": 0.032563466578722}, {"id": 2891, "seek": 1143264, "start": 11432.64, "end": 11434.64, "text": " basically let the model learn more", "tokens": [50364, 1936, 718, 264, 2316, 1466, 544, 50464], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2892, "seek": 1143264, "start": 11434.64, "end": 11436.64, "text": " about which words matter more together", "tokens": [50464, 466, 597, 2283, 1871, 544, 1214, 50564], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2893, "seek": 1143264, "start": 11436.64, "end": 11438.64, "text": " so", "tokens": [50564, 370, 50664], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2894, "seek": 1143264, "start": 11438.64, "end": 11440.64, "text": " that's pretty much just what the softmax does", "tokens": [50664, 300, 311, 1238, 709, 445, 437, 264, 2787, 41167, 775, 50764], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2895, "seek": 1143264, "start": 11440.64, "end": 11442.64, "text": " it increases our confidence in", "tokens": [50764, 309, 8637, 527, 6687, 294, 50864], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2896, "seek": 1143264, "start": 11442.64, "end": 11444.64, "text": " attention", "tokens": [50864, 3202, 50964], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2897, "seek": 1143264, "start": 11444.64, "end": 11446.64, "text": " and then a matrix multiply", "tokens": [50964, 293, 550, 257, 8141, 12972, 51064], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2898, "seek": 1143264, "start": 11446.64, "end": 11448.64, "text": " we go back to our V here", "tokens": [51064, 321, 352, 646, 281, 527, 691, 510, 51164], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2899, "seek": 1143264, "start": 11448.64, "end": 11450.64, "text": " and this is a value", "tokens": [51164, 293, 341, 307, 257, 2158, 51264], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2900, "seek": 1143264, "start": 11450.64, "end": 11452.64, "text": " so essentially what this is", "tokens": [51264, 370, 4476, 437, 341, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2901, "seek": 1143264, "start": 11452.64, "end": 11454.64, "text": " is just a linear transformation", "tokens": [51364, 307, 445, 257, 8213, 9887, 51464], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2902, "seek": 1143264, "start": 11454.64, "end": 11456.64, "text": " and we apply this on our", "tokens": [51464, 293, 321, 3079, 341, 322, 527, 51564], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2903, "seek": 1143264, "start": 11456.64, "end": 11458.64, "text": " we apply this on our inputs", "tokens": [51564, 321, 3079, 341, 322, 527, 15743, 51664], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2904, "seek": 1143264, "start": 11458.64, "end": 11460.64, "text": " and", "tokens": [51664, 293, 51764], "temperature": 0.0, "avg_logprob": -0.08332382548939098, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.010164435021579266}, {"id": 2905, "seek": 1146064, "start": 11460.64, "end": 11462.64, "text": " we have some value about", "tokens": [50364, 321, 362, 512, 2158, 466, 50464], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2906, "seek": 1146064, "start": 11462.64, "end": 11464.64, "text": " you know", "tokens": [50464, 291, 458, 50564], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2907, "seek": 1146064, "start": 11464.64, "end": 11466.64, "text": " what exactly those tokens are", "tokens": [50564, 437, 2293, 729, 22667, 366, 50664], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2908, "seek": 1146064, "start": 11466.64, "end": 11468.64, "text": " and after we've gotten all of our attention", "tokens": [50664, 293, 934, 321, 600, 5768, 439, 295, 527, 3202, 50764], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2909, "seek": 1146064, "start": 11468.64, "end": 11470.64, "text": " our softmax everything done", "tokens": [50764, 527, 2787, 41167, 1203, 1096, 50864], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2910, "seek": 1146064, "start": 11470.64, "end": 11472.64, "text": " it's just going to multiply", "tokens": [50864, 309, 311, 445, 516, 281, 12972, 50964], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2911, "seek": 1146064, "start": 11472.64, "end": 11474.64, "text": " the original values", "tokens": [50964, 264, 3380, 4190, 51064], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2912, "seek": 1146064, "start": 11474.64, "end": 11476.64, "text": " by everything we've gotten so far", "tokens": [51064, 538, 1203, 321, 600, 5768, 370, 1400, 51164], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2913, "seek": 1146064, "start": 11476.64, "end": 11478.64, "text": " just so that you don't have any information", "tokens": [51164, 445, 370, 300, 291, 500, 380, 362, 604, 1589, 51264], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2914, "seek": 1146064, "start": 11478.64, "end": 11480.64, "text": " that's really lost or we don't have anything scrambled", "tokens": [51264, 300, 311, 534, 2731, 420, 321, 500, 380, 362, 1340, 49127, 51364], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2915, "seek": 1146064, "start": 11480.64, "end": 11482.64, "text": " just that we have like a general idea", "tokens": [51364, 445, 300, 321, 362, 411, 257, 2674, 1558, 51464], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2916, "seek": 1146064, "start": 11482.64, "end": 11484.64, "text": " of okay these are actually", "tokens": [51464, 295, 1392, 613, 366, 767, 51564], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2917, "seek": 1146064, "start": 11484.64, "end": 11486.64, "text": " all the tokens we have", "tokens": [51564, 439, 264, 22667, 321, 362, 51664], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2918, "seek": 1146064, "start": 11486.64, "end": 11488.64, "text": " and then these are", "tokens": [51664, 293, 550, 613, 366, 51764], "temperature": 0.0, "avg_logprob": -0.09269707663017407, "compression_ratio": 1.8471615720524017, "no_speech_prob": 0.010646190494298935}, {"id": 2919, "seek": 1148864, "start": 11488.64, "end": 11490.64, "text": " we found interesting the attention scores", "tokens": [50364, 321, 1352, 1880, 264, 3202, 13444, 50464], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2920, "seek": 1148864, "start": 11492.64, "end": 11494.64, "text": " so", "tokens": [50564, 370, 50664], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2921, "seek": 1148864, "start": 11494.64, "end": 11496.64, "text": " we have an output which is a blend of input", "tokens": [50664, 321, 362, 364, 5598, 597, 307, 257, 10628, 295, 4846, 50764], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2922, "seek": 1148864, "start": 11496.64, "end": 11498.64, "text": " vector values and attention placed on each token", "tokens": [50764, 8062, 4190, 293, 3202, 7074, 322, 1184, 14862, 50864], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2923, "seek": 1148864, "start": 11498.64, "end": 11500.64, "text": " and", "tokens": [50864, 293, 50964], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2924, "seek": 1148864, "start": 11500.64, "end": 11502.64, "text": " that's pretty much what's happening in scaled dot", "tokens": [50964, 300, 311, 1238, 709, 437, 311, 2737, 294, 36039, 5893, 51064], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2925, "seek": 1148864, "start": 11502.64, "end": 11504.64, "text": " product attention in parallel", "tokens": [51064, 1674, 3202, 294, 8952, 51164], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2926, "seek": 1148864, "start": 11504.64, "end": 11506.64, "text": " so we have a bunch of these that are just happening", "tokens": [51164, 370, 321, 362, 257, 3840, 295, 613, 300, 366, 445, 2737, 51264], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2927, "seek": 1148864, "start": 11506.64, "end": 11508.64, "text": " at the same time", "tokens": [51264, 412, 264, 912, 565, 51364], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2928, "seek": 1148864, "start": 11508.64, "end": 11510.64, "text": " many of these happening at the same time", "tokens": [51364, 867, 295, 613, 2737, 412, 264, 912, 565, 51464], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2929, "seek": 1148864, "start": 11510.64, "end": 11512.64, "text": " and yeah so", "tokens": [51464, 293, 1338, 370, 51564], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2930, "seek": 1148864, "start": 11512.64, "end": 11514.64, "text": " that's what attention is", "tokens": [51564, 300, 311, 437, 3202, 307, 51664], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2931, "seek": 1148864, "start": 11514.64, "end": 11516.64, "text": " that's what feedforward networks are", "tokens": [51664, 300, 311, 437, 3154, 13305, 9590, 366, 51764], "temperature": 0.0, "avg_logprob": -0.10244618721728055, "compression_ratio": 1.937799043062201, "no_speech_prob": 0.0097056208178401}, {"id": 2932, "seek": 1151664, "start": 11516.64, "end": 11518.64, "text": " residual connections are", "tokens": [50364, 27980, 9271, 366, 50464], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2933, "seek": 1151664, "start": 11520.64, "end": 11522.64, "text": " and yeah", "tokens": [50564, 293, 1338, 50664], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2934, "seek": 1151664, "start": 11522.64, "end": 11524.64, "text": " and then so after this after we've", "tokens": [50664, 293, 550, 370, 934, 341, 934, 321, 600, 50764], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2935, "seek": 1151664, "start": 11524.64, "end": 11526.64, "text": " fed these into our decoders", "tokens": [50764, 4636, 613, 666, 527, 979, 378, 433, 50864], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2936, "seek": 1151664, "start": 11526.64, "end": 11528.64, "text": " we get an output", "tokens": [50864, 321, 483, 364, 5598, 50964], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2937, "seek": 1151664, "start": 11528.64, "end": 11530.64, "text": " we apply linear transformation to summarize", "tokens": [50964, 321, 3079, 8213, 9887, 281, 20858, 51064], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2938, "seek": 1151664, "start": 11530.64, "end": 11532.64, "text": " softmax probabilities", "tokens": [51064, 2787, 41167, 33783, 51164], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2939, "seek": 1151664, "start": 11532.64, "end": 11534.64, "text": " and then we generate based on that", "tokens": [51164, 293, 550, 321, 8460, 2361, 322, 300, 51264], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2940, "seek": 1151664, "start": 11534.64, "end": 11536.64, "text": " based on everything that we learned", "tokens": [51264, 2361, 322, 1203, 300, 321, 3264, 51364], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2941, "seek": 1151664, "start": 11536.64, "end": 11538.64, "text": " and", "tokens": [51364, 293, 51464], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2942, "seek": 1151664, "start": 11538.64, "end": 11540.64, "text": " actually what I didn't quite write a lot about", "tokens": [51464, 767, 437, 286, 994, 380, 1596, 2464, 257, 688, 466, 51564], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2943, "seek": 1151664, "start": 11540.64, "end": 11542.64, "text": " was the decoder", "tokens": [51564, 390, 264, 979, 19866, 51664], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2944, "seek": 1151664, "start": 11542.64, "end": 11544.64, "text": " so what I'm actually going to talk about next", "tokens": [51664, 370, 437, 286, 478, 767, 516, 281, 751, 466, 958, 51764], "temperature": 0.0, "avg_logprob": -0.08017021236997662, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.004467506427317858}, {"id": 2945, "seek": 1154464, "start": 11544.64, "end": 11546.64, "text": " is something I didn't fill in yet", "tokens": [50364, 307, 746, 286, 994, 380, 2836, 294, 1939, 50464], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2946, "seek": 1154464, "start": 11546.64, "end": 11548.64, "text": " which is why", "tokens": [50464, 597, 307, 983, 50564], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2947, "seek": 1154464, "start": 11548.64, "end": 11550.64, "text": " why the heck do we", "tokens": [50564, 983, 264, 12872, 360, 321, 50664], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2948, "seek": 1154464, "start": 11550.64, "end": 11552.64, "text": " use mass attention here", "tokens": [50664, 764, 2758, 3202, 510, 50764], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2949, "seek": 1154464, "start": 11552.64, "end": 11554.64, "text": " but not in these places so why the heck", "tokens": [50764, 457, 406, 294, 613, 3190, 370, 983, 264, 12872, 50864], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2950, "seek": 1154464, "start": 11554.64, "end": 11556.64, "text": " do we have a multi attention here", "tokens": [50864, 360, 321, 362, 257, 4825, 3202, 510, 50964], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2951, "seek": 1154464, "start": 11556.64, "end": 11558.64, "text": " all that attention here but mass attention here", "tokens": [50964, 439, 300, 3202, 510, 457, 2758, 3202, 510, 51064], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2952, "seek": 1154464, "start": 11558.64, "end": 11560.64, "text": " so why is this", "tokens": [51064, 370, 983, 307, 341, 51164], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2953, "seek": 1154464, "start": 11560.64, "end": 11562.64, "text": " well the purpose of the encoder", "tokens": [51164, 731, 264, 4334, 295, 264, 2058, 19866, 51264], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2954, "seek": 1154464, "start": 11562.64, "end": 11564.64, "text": " is to pretty much learn", "tokens": [51264, 307, 281, 1238, 709, 1466, 51364], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2955, "seek": 1154464, "start": 11564.64, "end": 11566.64, "text": " the present", "tokens": [51364, 264, 1974, 51464], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2956, "seek": 1154464, "start": 11566.64, "end": 11568.64, "text": " past and future", "tokens": [51464, 1791, 293, 2027, 51564], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2957, "seek": 1154464, "start": 11568.64, "end": 11570.64, "text": " and put that into a vector representation", "tokens": [51564, 293, 829, 300, 666, 257, 8062, 10290, 51664], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2958, "seek": 1154464, "start": 11570.64, "end": 11572.64, "text": " for the decoder", "tokens": [51664, 337, 264, 979, 19866, 51764], "temperature": 0.0, "avg_logprob": -0.07377167467801075, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.01970953308045864}, {"id": 2959, "seek": 1157264, "start": 11572.64, "end": 11574.64, "text": " that's what the encoder does", "tokens": [50364, 300, 311, 437, 264, 2058, 19866, 775, 50464], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2960, "seek": 1157264, "start": 11574.64, "end": 11576.64, "text": " so it's okay if we look into the future", "tokens": [50464, 370, 309, 311, 1392, 498, 321, 574, 666, 264, 2027, 50564], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2961, "seek": 1157264, "start": 11576.64, "end": 11578.64, "text": " and understand tokens that way", "tokens": [50564, 293, 1223, 22667, 300, 636, 50664], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2962, "seek": 1157264, "start": 11578.64, "end": 11580.64, "text": " because we're technically not cheating", "tokens": [50664, 570, 321, 434, 12120, 406, 18309, 50764], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2963, "seek": 1157264, "start": 11580.64, "end": 11582.64, "text": " we're just learning the different attention scores", "tokens": [50764, 321, 434, 445, 2539, 264, 819, 3202, 13444, 50864], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2964, "seek": 1157264, "start": 11582.64, "end": 11584.64, "text": " and yeah we're just using that", "tokens": [50864, 293, 1338, 321, 434, 445, 1228, 300, 50964], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2965, "seek": 1157264, "start": 11584.64, "end": 11586.64, "text": " to help us predict based on", "tokens": [50964, 281, 854, 505, 6069, 2361, 322, 51064], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2966, "seek": 1157264, "start": 11586.64, "end": 11588.64, "text": " what the sentence looks like", "tokens": [51064, 437, 264, 8174, 1542, 411, 51164], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2967, "seek": 1157264, "start": 11588.64, "end": 11590.64, "text": " but not explicitly giving it away", "tokens": [51164, 457, 406, 20803, 2902, 309, 1314, 51264], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2968, "seek": 1157264, "start": 11590.64, "end": 11592.64, "text": " just giving it an idea of", "tokens": [51264, 445, 2902, 309, 364, 1558, 295, 51364], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2969, "seek": 1157264, "start": 11592.64, "end": 11594.64, "text": " what to look for type of thing", "tokens": [51364, 437, 281, 574, 337, 2010, 295, 551, 51464], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2970, "seek": 1157264, "start": 11594.64, "end": 11596.64, "text": " and then", "tokens": [51464, 293, 550, 51564], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2971, "seek": 1157264, "start": 11596.64, "end": 11598.64, "text": " we use mass attention here because", "tokens": [51564, 321, 764, 2758, 3202, 510, 570, 51664], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2972, "seek": 1157264, "start": 11598.64, "end": 11600.64, "text": " well we don't want to look ahead", "tokens": [51664, 731, 321, 500, 380, 528, 281, 574, 2286, 51764], "temperature": 0.0, "avg_logprob": -0.05872917175292969, "compression_ratio": 1.8464730290456433, "no_speech_prob": 0.005639436189085245}, {"id": 2973, "seek": 1160064, "start": 11600.64, "end": 11602.64, "text": " we want to look at the present and the past", "tokens": [50364, 321, 528, 281, 574, 412, 264, 1974, 293, 264, 1791, 50464], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2974, "seek": 1160064, "start": 11602.64, "end": 11604.64, "text": " and", "tokens": [50464, 293, 50564], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2975, "seek": 1160064, "start": 11604.64, "end": 11606.64, "text": " later on", "tokens": [50564, 1780, 322, 50664], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2976, "seek": 1160064, "start": 11606.64, "end": 11608.64, "text": " we're not giving anything explicit", "tokens": [50664, 321, 434, 406, 2902, 1340, 13691, 50764], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2977, "seek": 1160064, "start": 11608.64, "end": 11610.64, "text": " here we're not giving anything yet", "tokens": [50764, 510, 321, 434, 406, 2902, 1340, 1939, 50864], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2978, "seek": 1160064, "start": 11610.64, "end": 11612.64, "text": " so we want to make some raw guesses", "tokens": [50864, 370, 321, 528, 281, 652, 512, 8936, 42703, 50964], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2979, "seek": 1160064, "start": 11612.64, "end": 11614.64, "text": " they're not going to be very good guesses at first", "tokens": [50964, 436, 434, 406, 516, 281, 312, 588, 665, 42703, 412, 700, 51064], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2980, "seek": 1160064, "start": 11614.64, "end": 11616.64, "text": " we want to make some raw guesses", "tokens": [51064, 321, 528, 281, 652, 512, 8936, 42703, 51164], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2981, "seek": 1160064, "start": 11616.64, "end": 11618.64, "text": " and then later on", "tokens": [51164, 293, 550, 1780, 322, 51264], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2982, "seek": 1160064, "start": 11618.64, "end": 11620.64, "text": " we can feed these", "tokens": [51264, 321, 393, 3154, 613, 51364], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2983, "seek": 1160064, "start": 11620.64, "end": 11622.64, "text": " the added and normalized guesses", "tokens": [51364, 264, 3869, 293, 48704, 42703, 51464], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2984, "seek": 1160064, "start": 11622.64, "end": 11624.64, "text": " into", "tokens": [51464, 666, 51564], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2985, "seek": 1160064, "start": 11624.64, "end": 11626.64, "text": " this next multi attention", "tokens": [51564, 341, 958, 4825, 3202, 51664], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2986, "seek": 1160064, "start": 11626.64, "end": 11628.64, "text": " which isn't masked", "tokens": [51664, 597, 1943, 380, 45249, 51764], "temperature": 0.0, "avg_logprob": -0.09066259860992432, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.012616269290447235}, {"id": 2987, "seek": 1162864, "start": 11628.64, "end": 11630.64, "text": " and then we can use", "tokens": [50364, 293, 550, 321, 393, 764, 50464], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2988, "seek": 1162864, "start": 11630.64, "end": 11632.64, "text": " this max multi head attention", "tokens": [50464, 341, 11469, 4825, 1378, 3202, 50564], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2989, "seek": 1162864, "start": 11632.64, "end": 11634.64, "text": " with the vector representation", "tokens": [50564, 365, 264, 8062, 10290, 50664], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2990, "seek": 1162864, "start": 11634.64, "end": 11636.64, "text": " given by the encoder", "tokens": [50664, 2212, 538, 264, 2058, 19866, 50764], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2991, "seek": 1162864, "start": 11636.64, "end": 11638.64, "text": " and then we can sort of do", "tokens": [50764, 293, 550, 321, 393, 1333, 295, 360, 50864], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2992, "seek": 1162864, "start": 11638.64, "end": 11640.64, "text": " more useful things with that", "tokens": [50864, 544, 4420, 721, 365, 300, 50964], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2993, "seek": 1162864, "start": 11640.64, "end": 11642.64, "text": " rather than just being forced to guess", "tokens": [50964, 2831, 813, 445, 885, 7579, 281, 2041, 51064], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2994, "seek": 1162864, "start": 11642.64, "end": 11644.64, "text": " raw attention scores", "tokens": [51064, 8936, 3202, 13444, 51164], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2995, "seek": 1162864, "start": 11644.64, "end": 11646.64, "text": " and then being judged for that", "tokens": [51164, 293, 550, 885, 27485, 337, 300, 51264], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2996, "seek": 1162864, "start": 11646.64, "end": 11648.64, "text": " we can sort of introduce more", "tokens": [51264, 321, 393, 1333, 295, 5366, 544, 51364], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2997, "seek": 1162864, "start": 11648.64, "end": 11650.64, "text": " more and more elements", "tokens": [51364, 544, 293, 544, 4959, 51464], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2998, "seek": 1162864, "start": 11650.64, "end": 11652.64, "text": " in this decoder block to help us learn more meaningful things", "tokens": [51464, 294, 341, 979, 19866, 3461, 281, 854, 505, 1466, 544, 10995, 721, 51564], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 2999, "seek": 1162864, "start": 11652.64, "end": 11654.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 3000, "seek": 1162864, "start": 11654.64, "end": 11656.64, "text": " we start off with", "tokens": [51664, 321, 722, 766, 365, 51764], "temperature": 0.0, "avg_logprob": -0.058590985261476956, "compression_ratio": 1.8916256157635467, "no_speech_prob": 0.010162543505430222}, {"id": 3001, "seek": 1165664, "start": 11656.64, "end": 11658.64, "text": " making this", "tokens": [50364, 1455, 341, 50464], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3002, "seek": 1165664, "start": 11658.64, "end": 11660.64, "text": " mass multi head attention", "tokens": [50464, 2758, 4825, 1378, 3202, 50564], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3003, "seek": 1165664, "start": 11660.64, "end": 11662.64, "text": " and then combining that", "tokens": [50564, 293, 550, 21928, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3004, "seek": 1165664, "start": 11662.64, "end": 11664.64, "text": " with", "tokens": [50664, 365, 50764], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3005, "seek": 1165664, "start": 11664.64, "end": 11666.64, "text": " our", "tokens": [50764, 527, 50864], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3006, "seek": 1165664, "start": 11666.64, "end": 11668.64, "text": " then afterwards we do a multi head attention", "tokens": [50864, 550, 10543, 321, 360, 257, 4825, 1378, 3202, 50964], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3007, "seek": 1165664, "start": 11668.64, "end": 11670.64, "text": " with the", "tokens": [50964, 365, 264, 51064], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3008, "seek": 1165664, "start": 11670.64, "end": 11672.64, "text": " vector representation from the encoder", "tokens": [51064, 8062, 10290, 490, 264, 2058, 19866, 51164], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3009, "seek": 1165664, "start": 11672.64, "end": 11674.64, "text": " and then we can make decisions on that", "tokens": [51164, 293, 550, 321, 393, 652, 5327, 322, 300, 51264], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3010, "seek": 1165664, "start": 11674.64, "end": 11676.64, "text": " so that's kind of why that works", "tokens": [51264, 370, 300, 311, 733, 295, 983, 300, 1985, 51364], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3011, "seek": 1165664, "start": 11676.64, "end": 11678.64, "text": " this way", "tokens": [51364, 341, 636, 51464], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3012, "seek": 1165664, "start": 11678.64, "end": 11680.64, "text": " if you don't think I explain it like amazingly", "tokens": [51464, 498, 291, 500, 380, 519, 286, 2903, 309, 411, 31762, 51564], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3013, "seek": 1165664, "start": 11680.64, "end": 11682.64, "text": " well you can totally just", "tokens": [51564, 731, 291, 393, 3879, 445, 51664], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3014, "seek": 1165664, "start": 11682.64, "end": 11684.64, "text": " ask GPT4", "tokens": [51664, 1029, 26039, 51, 19, 51764], "temperature": 0.0, "avg_logprob": -0.08877181002968236, "compression_ratio": 1.63, "no_speech_prob": 0.04667824134230614}, {"id": 3015, "seek": 1168464, "start": 11684.64, "end": 11686.64, "text": " or GPT3.5", "tokens": [50364, 420, 26039, 51, 18, 13, 20, 50464], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3016, "seek": 1168464, "start": 11686.64, "end": 11688.64, "text": " and get a pretty decent answer", "tokens": [50464, 293, 483, 257, 1238, 8681, 1867, 50564], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3017, "seek": 1168464, "start": 11688.64, "end": 11690.64, "text": " but that's how that works", "tokens": [50564, 457, 300, 311, 577, 300, 1985, 50664], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3018, "seek": 1168464, "start": 11690.64, "end": 11692.64, "text": " and", "tokens": [50664, 293, 50764], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3019, "seek": 1168464, "start": 11692.64, "end": 11694.64, "text": " another thing I kind of wanted to point out here", "tokens": [50764, 1071, 551, 286, 733, 295, 1415, 281, 935, 484, 510, 50864], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3020, "seek": 1168464, "start": 11694.64, "end": 11696.64, "text": " is these linear transformations", "tokens": [50864, 307, 613, 8213, 34852, 50964], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3021, "seek": 1168464, "start": 11696.64, "end": 11698.64, "text": " that you see", "tokens": [50964, 300, 291, 536, 51064], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3022, "seek": 1168464, "start": 11698.64, "end": 11700.64, "text": " I mean there's a lot of them", "tokens": [51064, 286, 914, 456, 311, 257, 688, 295, 552, 51164], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3023, "seek": 1168464, "start": 11700.64, "end": 11702.64, "text": " in the", "tokens": [51164, 294, 264, 51264], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3024, "seek": 1168464, "start": 11702.64, "end": 11704.64, "text": " scaled dot project attention", "tokens": [51264, 36039, 5893, 1716, 3202, 51364], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3025, "seek": 1168464, "start": 11704.64, "end": 11706.64, "text": " so you have your linears", "tokens": [51364, 370, 291, 362, 428, 1622, 685, 51464], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3026, "seek": 1168464, "start": 11706.64, "end": 11708.64, "text": " for your value or key value", "tokens": [51464, 337, 428, 2158, 420, 2141, 2158, 51564], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3027, "seek": 1168464, "start": 11708.64, "end": 11710.64, "text": " and key query and values", "tokens": [51564, 293, 2141, 14581, 293, 4190, 51664], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3028, "seek": 1168464, "start": 11710.64, "end": 11712.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.12121772766113281, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.008058382198214531}, {"id": 3029, "seek": 1171264, "start": 11712.64, "end": 11714.64, "text": " as well as the one up here", "tokens": [50364, 382, 731, 382, 264, 472, 493, 510, 50464], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3030, "seek": 1171264, "start": 11714.64, "end": 11716.64, "text": " linears are great", "tokens": [50464, 1622, 685, 366, 869, 50564], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3031, "seek": 1171264, "start": 11716.64, "end": 11718.64, "text": " for just expanding or shrinking", "tokens": [50564, 337, 445, 14702, 420, 41684, 50664], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3032, "seek": 1171264, "start": 11718.64, "end": 11720.64, "text": " a bunch of important info", "tokens": [50664, 257, 3840, 295, 1021, 13614, 50764], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3033, "seek": 1171264, "start": 11720.64, "end": 11722.64, "text": " into something easier to work with", "tokens": [50764, 666, 746, 3571, 281, 589, 365, 50864], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3034, "seek": 1171264, "start": 11722.64, "end": 11724.64, "text": " so if you have a bunch of", "tokens": [50864, 370, 498, 291, 362, 257, 3840, 295, 50964], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3035, "seek": 1171264, "start": 11724.64, "end": 11726.64, "text": " if you have a large vector containing a bunch", "tokens": [50964, 498, 291, 362, 257, 2416, 8062, 19273, 257, 3840, 51064], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3036, "seek": 1171264, "start": 11726.64, "end": 11728.64, "text": " of info learned from this", "tokens": [51064, 295, 13614, 3264, 490, 341, 51164], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3037, "seek": 1171264, "start": 11728.64, "end": 11730.64, "text": " scaled dot project attention", "tokens": [51164, 36039, 5893, 1716, 3202, 51264], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3038, "seek": 1171264, "start": 11730.64, "end": 11732.64, "text": " you can", "tokens": [51264, 291, 393, 51364], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3039, "seek": 1171264, "start": 11732.64, "end": 11734.64, "text": " you can sort of just compress", "tokens": [51364, 291, 393, 1333, 295, 445, 14778, 51464], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3040, "seek": 1171264, "start": 11734.64, "end": 11736.64, "text": " that into something more manageable", "tokens": [51464, 300, 666, 746, 544, 38798, 51564], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3041, "seek": 1171264, "start": 11736.64, "end": 11738.64, "text": " through a linear transformation", "tokens": [51564, 807, 257, 8213, 9887, 51664], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3042, "seek": 1171264, "start": 11738.64, "end": 11740.64, "text": " and it's essentially what's just happening here", "tokens": [51664, 293, 309, 311, 4476, 437, 311, 445, 2737, 510, 51764], "temperature": 0.0, "avg_logprob": -0.11156273771215368, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.005467778071761131}, {"id": 3043, "seek": 1174064, "start": 11740.64, "end": 11742.64, "text": " with Softmax as well as", "tokens": [50364, 365, 16985, 41167, 382, 731, 382, 50464], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3044, "seek": 1174064, "start": 11742.64, "end": 11744.64, "text": " in our", "tokens": [50464, 294, 527, 50564], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3045, "seek": 1174064, "start": 11744.64, "end": 11746.64, "text": " scaled dot project attention here", "tokens": [50564, 36039, 5893, 1716, 3202, 510, 50664], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3046, "seek": 1174064, "start": 11746.64, "end": 11748.64, "text": " for these linear transformations", "tokens": [50664, 337, 613, 8213, 34852, 50764], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3047, "seek": 1174064, "start": 11748.64, "end": 11750.64, "text": " from our inputs", "tokens": [50764, 490, 527, 15743, 50864], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3048, "seek": 1174064, "start": 11750.64, "end": 11752.64, "text": " to", "tokens": [50864, 281, 50964], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3049, "seek": 1174064, "start": 11752.64, "end": 11754.64, "text": " quick keys, queries and values", "tokens": [50964, 1702, 9317, 11, 24109, 293, 4190, 51064], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3050, "seek": 1174064, "start": 11754.64, "end": 11756.64, "text": " that's all that's happening", "tokens": [51064, 300, 311, 439, 300, 311, 2737, 51164], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3051, "seek": 1174064, "start": 11756.64, "end": 11758.64, "text": " if you want to read more about", "tokens": [51164, 498, 291, 528, 281, 1401, 544, 466, 51264], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3052, "seek": 1174064, "start": 11758.64, "end": 11760.64, "text": " linear transformations the importance of them", "tokens": [51264, 8213, 34852, 264, 7379, 295, 552, 51364], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3053, "seek": 1174064, "start": 11760.64, "end": 11762.64, "text": " you can totally go out of your way to do that", "tokens": [51364, 291, 393, 3879, 352, 484, 295, 428, 636, 281, 360, 300, 51464], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3054, "seek": 1174064, "start": 11762.64, "end": 11764.64, "text": " but that's just sort of a brief summary", "tokens": [51464, 457, 300, 311, 445, 1333, 295, 257, 5353, 12691, 51564], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3055, "seek": 1174064, "start": 11764.64, "end": 11766.64, "text": " as to why they're important", "tokens": [51564, 382, 281, 983, 436, 434, 1021, 51664], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3056, "seek": 1174064, "start": 11766.64, "end": 11768.64, "text": " just shrinking or expanding", "tokens": [51664, 445, 41684, 420, 14702, 51764], "temperature": 0.0, "avg_logprob": -0.1062194806224895, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.03566332161426544}, {"id": 3057, "seek": 1176864, "start": 11768.64, "end": 11770.64, "text": " so that's sort of a brief overview on how", "tokens": [50364, 370, 300, 311, 1333, 295, 257, 5353, 12492, 322, 577, 50464], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3058, "seek": 1176864, "start": 11770.64, "end": 11772.64, "text": " transformers work", "tokens": [50464, 4088, 433, 589, 50564], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3059, "seek": 1176864, "start": 11772.64, "end": 11774.64, "text": " however in this", "tokens": [50564, 4461, 294, 341, 50664], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3060, "seek": 1176864, "start": 11774.64, "end": 11776.64, "text": " course we will not be building the transformer", "tokens": [50664, 1164, 321, 486, 406, 312, 2390, 264, 31782, 50764], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3061, "seek": 1176864, "start": 11776.64, "end": 11778.64, "text": " architecture we'll be building", "tokens": [50764, 9482, 321, 603, 312, 2390, 50864], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3062, "seek": 1176864, "start": 11778.64, "end": 11780.64, "text": " something called a GPT which you're probably familiar", "tokens": [50864, 746, 1219, 257, 26039, 51, 597, 291, 434, 1391, 4963, 50964], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3063, "seek": 1176864, "start": 11780.64, "end": 11782.64, "text": " with and GPT stands for", "tokens": [50964, 365, 293, 26039, 51, 7382, 337, 51064], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3064, "seek": 1176864, "start": 11782.64, "end": 11784.64, "text": " Generatively Pre-Trained Transformer", "tokens": [51064, 15409, 19020, 6001, 12, 51, 31774, 27938, 260, 51164], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3065, "seek": 1176864, "start": 11784.64, "end": 11786.64, "text": " or Generative Pre-Trained Transformer", "tokens": [51164, 420, 15409, 1166, 6001, 12, 51, 31774, 27938, 260, 51264], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3066, "seek": 1176864, "start": 11786.64, "end": 11788.64, "text": " one of the two", "tokens": [51264, 472, 295, 264, 732, 51364], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3067, "seek": 1176864, "start": 11788.64, "end": 11790.64, "text": " and pretty much what this is", "tokens": [51364, 293, 1238, 709, 437, 341, 307, 51464], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3068, "seek": 1176864, "start": 11790.64, "end": 11792.64, "text": " it's pretty close to the transformer", "tokens": [51464, 309, 311, 1238, 1998, 281, 264, 31782, 51564], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3069, "seek": 1176864, "start": 11792.64, "end": 11794.64, "text": " this architecture here except", "tokens": [51564, 341, 9482, 510, 3993, 51664], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3070, "seek": 1176864, "start": 11794.64, "end": 11796.64, "text": " it only adopts", "tokens": [51664, 309, 787, 22486, 1373, 51764], "temperature": 0.0, "avg_logprob": -0.09100181220943092, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.002216360764577985}, {"id": 3071, "seek": 1179664, "start": 11796.64, "end": 11798.64, "text": " the decoder blocks and it takes away", "tokens": [50364, 264, 979, 19866, 8474, 293, 309, 2516, 1314, 50464], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3072, "seek": 1179664, "start": 11798.64, "end": 11800.64, "text": " this multi-head attention here", "tokens": [50464, 341, 4825, 12, 1934, 3202, 510, 50564], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3073, "seek": 1179664, "start": 11800.64, "end": 11802.64, "text": " so all we're doing is we're removing", "tokens": [50564, 370, 439, 321, 434, 884, 307, 321, 434, 12720, 50664], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3074, "seek": 1179664, "start": 11802.64, "end": 11804.64, "text": " the encoder", "tokens": [50664, 264, 2058, 19866, 50764], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3075, "seek": 1179664, "start": 11804.64, "end": 11806.64, "text": " as well as what the encoder plugs into", "tokens": [50764, 382, 731, 382, 437, 264, 2058, 19866, 33899, 666, 50864], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3076, "seek": 1179664, "start": 11806.64, "end": 11808.64, "text": " so all we have left", "tokens": [50864, 370, 439, 321, 362, 1411, 50964], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3077, "seek": 1179664, "start": 11808.64, "end": 11810.64, "text": " is just some inputs", "tokens": [50964, 307, 445, 512, 15743, 51064], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3078, "seek": 1179664, "start": 11810.64, "end": 11812.64, "text": " our max multi-head", "tokens": [51064, 527, 11469, 4825, 12, 1934, 51164], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3079, "seek": 1179664, "start": 11812.64, "end": 11814.64, "text": " attention", "tokens": [51164, 3202, 51264], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3080, "seek": 1179664, "start": 11814.64, "end": 11816.64, "text": " our post-norm architecture", "tokens": [51264, 527, 2183, 12, 13403, 9482, 51364], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3081, "seek": 1179664, "start": 11816.64, "end": 11818.64, "text": " and then", "tokens": [51364, 293, 550, 51464], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3082, "seek": 1179664, "start": 11818.64, "end": 11820.64, "text": " right after this we're not going to", "tokens": [51464, 558, 934, 341, 321, 434, 406, 516, 281, 51564], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3083, "seek": 1179664, "start": 11820.64, "end": 11822.64, "text": " a non-mass multi-head attention", "tokens": [51564, 257, 2107, 12, 47386, 4825, 12, 1934, 3202, 51664], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3084, "seek": 1179664, "start": 11822.64, "end": 11824.64, "text": " but rather to a feed forward network", "tokens": [51664, 457, 2831, 281, 257, 3154, 2128, 3209, 51764], "temperature": 0.0, "avg_logprob": -0.07195283282886852, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.01882162131369114}, {"id": 3085, "seek": 1182464, "start": 11824.64, "end": 11826.64, "text": " and then a post-norm", "tokens": [50364, 293, 550, 257, 2183, 12, 13403, 50464], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3086, "seek": 1182464, "start": 11826.64, "end": 11828.64, "text": " so that's all it is, it's just 1, 2, 3, 4", "tokens": [50464, 370, 300, 311, 439, 309, 307, 11, 309, 311, 445, 502, 11, 568, 11, 805, 11, 1017, 50564], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3087, "seek": 1182464, "start": 11828.64, "end": 11830.64, "text": " that's all it's going to look like", "tokens": [50564, 300, 311, 439, 309, 311, 516, 281, 574, 411, 50664], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3088, "seek": 1182464, "start": 11830.64, "end": 11832.64, "text": " that's all the blocks are going to be", "tokens": [50664, 300, 311, 439, 264, 8474, 366, 516, 281, 312, 50764], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3089, "seek": 1182464, "start": 11832.64, "end": 11834.64, "text": " it is still important", "tokens": [50764, 309, 307, 920, 1021, 50864], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3090, "seek": 1182464, "start": 11834.64, "end": 11836.64, "text": " to understand the transformer architecture itself", "tokens": [50864, 281, 1223, 264, 31782, 9482, 2564, 50964], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3091, "seek": 1182464, "start": 11836.64, "end": 11838.64, "text": " because you might need that in the future", "tokens": [50964, 570, 291, 1062, 643, 300, 294, 264, 2027, 51064], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3092, "seek": 1182464, "start": 11838.64, "end": 11840.64, "text": " and it is sort of a good practice in language", "tokens": [51064, 293, 309, 307, 1333, 295, 257, 665, 3124, 294, 2856, 51164], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3093, "seek": 1182464, "start": 11840.64, "end": 11842.64, "text": " modeling to", "tokens": [51164, 15983, 281, 51264], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3094, "seek": 1182464, "start": 11842.64, "end": 11844.64, "text": " have a grasp on and to understand", "tokens": [51264, 362, 257, 21743, 322, 293, 281, 1223, 51364], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3095, "seek": 1182464, "start": 11844.64, "end": 11846.64, "text": " you know why we use mass multi-head", "tokens": [51364, 291, 458, 983, 321, 764, 2758, 4825, 12, 1934, 51464], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3096, "seek": 1182464, "start": 11846.64, "end": 11848.64, "text": " attention in the decoder and why we don't", "tokens": [51464, 3202, 294, 264, 979, 19866, 293, 983, 321, 500, 380, 51564], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3097, "seek": 1182464, "start": 11848.64, "end": 11850.64, "text": " use it in the encoder and stuff like that", "tokens": [51564, 764, 309, 294, 264, 2058, 19866, 293, 1507, 411, 300, 51664], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3098, "seek": 1182464, "start": 11850.64, "end": 11852.64, "text": " so anyways", "tokens": [51664, 370, 13448, 51764], "temperature": 0.0, "avg_logprob": -0.06200805499399309, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0061897640116512775}, {"id": 3099, "seek": 1185264, "start": 11852.64, "end": 11854.64, "text": " we're going to go ahead and build this", "tokens": [50364, 321, 434, 516, 281, 352, 2286, 293, 1322, 341, 50464], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3100, "seek": 1185264, "start": 11854.64, "end": 11856.64, "text": " if you need to", "tokens": [50464, 498, 291, 643, 281, 50564], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3101, "seek": 1185264, "start": 11856.64, "end": 11858.64, "text": " look back if something wasn't quite clear", "tokens": [50564, 574, 646, 498, 746, 2067, 380, 1596, 1850, 50664], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3102, "seek": 1185264, "start": 11858.64, "end": 11860.64, "text": " definitely skip back a few seconds", "tokens": [50664, 2138, 10023, 646, 257, 1326, 3949, 50764], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3103, "seek": 1185264, "start": 11860.64, "end": 11862.64, "text": " or a few minutes through the video and just", "tokens": [50764, 420, 257, 1326, 2077, 807, 264, 960, 293, 445, 50864], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3104, "seek": 1185264, "start": 11862.64, "end": 11864.64, "text": " make sure you clarify everything up to this point", "tokens": [50864, 652, 988, 291, 17594, 1203, 493, 281, 341, 935, 50964], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3105, "seek": 1185264, "start": 11864.64, "end": 11866.64, "text": " but yeah", "tokens": [50964, 457, 1338, 51064], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3106, "seek": 1185264, "start": 11866.64, "end": 11868.64, "text": " I'm going to go over some more", "tokens": [51064, 286, 478, 516, 281, 352, 670, 512, 544, 51164], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3107, "seek": 1185264, "start": 11868.64, "end": 11870.64, "text": " math on the side here and just some other", "tokens": [51164, 5221, 322, 264, 1252, 510, 293, 445, 512, 661, 51264], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3108, "seek": 1185264, "start": 11870.64, "end": 11872.64, "text": " little", "tokens": [51264, 707, 51364], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3109, "seek": 1185264, "start": 11872.64, "end": 11874.64, "text": " little widgets we're going to need", "tokens": [51364, 707, 43355, 321, 434, 516, 281, 643, 51464], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3110, "seek": 1185264, "start": 11874.64, "end": 11876.64, "text": " for building the decoder", "tokens": [51464, 337, 2390, 264, 979, 19866, 51564], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3111, "seek": 1185264, "start": 11876.64, "end": 11878.64, "text": " GPT architecture", "tokens": [51564, 26039, 51, 9482, 51664], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3112, "seek": 1185264, "start": 11878.64, "end": 11880.64, "text": " so let's go ahead and do that", "tokens": [51664, 370, 718, 311, 352, 2286, 293, 360, 300, 51764], "temperature": 0.0, "avg_logprob": -0.06440393803483349, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010980200953781605}, {"id": 3113, "seek": 1188064, "start": 11880.64, "end": 11882.64, "text": " we're going to jump into", "tokens": [50364, 321, 434, 516, 281, 3012, 666, 50464], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3114, "seek": 1188064, "start": 11882.64, "end": 11884.64, "text": " building the transformer rather than", "tokens": [50464, 2390, 264, 31782, 2831, 813, 50564], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3115, "seek": 1188064, "start": 11884.64, "end": 11886.64, "text": " building the GPT from scratch", "tokens": [50564, 2390, 264, 26039, 51, 490, 8459, 50664], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3116, "seek": 1188064, "start": 11886.64, "end": 11888.64, "text": " what I want to do is linger on", "tokens": [50664, 437, 286, 528, 281, 360, 307, 45657, 322, 50764], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3117, "seek": 1188064, "start": 11888.64, "end": 11890.64, "text": " self-attention for a little bit", "tokens": [50764, 2698, 12, 1591, 1251, 337, 257, 707, 857, 50864], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3118, "seek": 1188064, "start": 11890.64, "end": 11892.64, "text": " or rather just the attention mechanism", "tokens": [50864, 420, 2831, 445, 264, 3202, 7513, 50964], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3119, "seek": 1188064, "start": 11892.64, "end": 11894.64, "text": " and the matrix multiplication behind it", "tokens": [50964, 293, 264, 8141, 27290, 2261, 309, 51064], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3120, "seek": 1188064, "start": 11894.64, "end": 11896.64, "text": " and why it works", "tokens": [51064, 293, 983, 309, 1985, 51164], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3121, "seek": 1188064, "start": 11896.64, "end": 11898.64, "text": " so I'm going to use", "tokens": [51164, 370, 286, 478, 516, 281, 764, 51264], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3122, "seek": 1188064, "start": 11898.64, "end": 11900.64, "text": " whiteboard to illustrate this", "tokens": [51264, 2418, 3787, 281, 23221, 341, 51364], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3123, "seek": 1188064, "start": 11900.64, "end": 11902.64, "text": " so we're going to go ahead and draw out", "tokens": [51364, 370, 321, 434, 516, 281, 352, 2286, 293, 2642, 484, 51464], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3124, "seek": 1188064, "start": 11902.64, "end": 11904.64, "text": " a", "tokens": [51464, 257, 51564], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3125, "seek": 1188064, "start": 11904.64, "end": 11906.64, "text": " we'll just use maybe a four token", "tokens": [51564, 321, 603, 445, 764, 1310, 257, 1451, 14862, 51664], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3126, "seek": 1188064, "start": 11906.64, "end": 11908.64, "text": " sequence here of words", "tokens": [51664, 8310, 510, 295, 2283, 51764], "temperature": 0.0, "avg_logprob": -0.11397669800614889, "compression_ratio": 1.6978723404255318, "no_speech_prob": 0.04267805814743042}, {"id": 3127, "seek": 1190864, "start": 11908.64, "end": 11910.64, "text": " okay", "tokens": [50364, 1392, 50464], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3128, "seek": 1190864, "start": 11910.64, "end": 11912.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3129, "seek": 1190864, "start": 11912.64, "end": 11914.64, "text": " we're going to highlight which words", "tokens": [50564, 321, 434, 516, 281, 5078, 597, 2283, 50664], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3130, "seek": 1190864, "start": 11914.64, "end": 11916.64, "text": " are probably going to end up", "tokens": [50664, 366, 1391, 516, 281, 917, 493, 50764], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3131, "seek": 1190864, "start": 11916.64, "end": 11918.64, "text": " correlating together", "tokens": [50764, 13983, 990, 1214, 50864], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3132, "seek": 1190864, "start": 11918.64, "end": 11920.64, "text": " or the attention mechanism", "tokens": [50864, 420, 264, 3202, 7513, 50964], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3133, "seek": 1190864, "start": 11920.64, "end": 11922.64, "text": " is going to multiply them together", "tokens": [50964, 307, 516, 281, 12972, 552, 1214, 51064], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3134, "seek": 1190864, "start": 11922.64, "end": 11924.64, "text": " to a high amount based on what it learns", "tokens": [51064, 281, 257, 1090, 2372, 2361, 322, 437, 309, 27152, 51164], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3135, "seek": 1190864, "start": 11924.64, "end": 11926.64, "text": " about those tokens this is what this is", "tokens": [51164, 466, 729, 22667, 341, 307, 437, 341, 307, 51264], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3136, "seek": 1190864, "start": 11926.64, "end": 11928.64, "text": " so I'm going to help us illustrate that", "tokens": [51264, 370, 286, 478, 516, 281, 854, 505, 23221, 300, 51364], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3137, "seek": 1190864, "start": 11928.64, "end": 11930.64, "text": " and what the", "tokens": [51364, 293, 437, 264, 51464], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3138, "seek": 1190864, "start": 11930.64, "end": 11932.64, "text": " GPT is going to see", "tokens": [51464, 26039, 51, 307, 516, 281, 536, 51564], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3139, "seek": 1190864, "start": 11932.64, "end": 11934.64, "text": " sort of from the inside what it looks like from the inside", "tokens": [51564, 1333, 295, 490, 264, 1854, 437, 309, 1542, 411, 490, 264, 1854, 51664], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3140, "seek": 1190864, "start": 11934.64, "end": 11936.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.049832501501407264, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007342733442783356}, {"id": 3141, "seek": 1193664, "start": 11936.64, "end": 11938.64, "text": " I'm going to go ahead and draw this out here", "tokens": [50364, 286, 478, 516, 281, 352, 2286, 293, 2642, 341, 484, 510, 50464], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3142, "seek": 1193664, "start": 11942.64, "end": 11944.64, "text": " just make a table here", "tokens": [50664, 445, 652, 257, 3199, 510, 50764], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3143, "seek": 1193664, "start": 11944.64, "end": 11946.64, "text": " we'll give it", "tokens": [50764, 321, 603, 976, 309, 50864], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3144, "seek": 1193664, "start": 11950.64, "end": 11952.64, "text": " four of these", "tokens": [51064, 1451, 295, 613, 51164], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3145, "seek": 1193664, "start": 11954.64, "end": 11956.64, "text": " and draw a little line through the middle", "tokens": [51264, 293, 2642, 257, 707, 1622, 807, 264, 2808, 51364], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3146, "seek": 1193664, "start": 11956.64, "end": 11958.64, "text": " my drawing might not be", "tokens": [51364, 452, 6316, 1062, 406, 312, 51464], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3147, "seek": 1193664, "start": 11958.64, "end": 11960.64, "text": " perfect but it's definitely better", "tokens": [51464, 2176, 457, 309, 311, 2138, 1101, 51564], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3148, "seek": 1193664, "start": 11960.64, "end": 11962.64, "text": " than on paper", "tokens": [51564, 813, 322, 3035, 51664], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3149, "seek": 1193664, "start": 11962.64, "end": 11964.64, "text": " so cool we have this", "tokens": [51664, 370, 1627, 321, 362, 341, 51764], "temperature": 0.0, "avg_logprob": -0.09194836616516114, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.005552325397729874}, {"id": 3150, "seek": 1196464, "start": 11964.64, "end": 11966.64, "text": " we have", "tokens": [50364, 321, 362, 50464], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3151, "seek": 1196464, "start": 11966.64, "end": 11968.64, "text": " my", "tokens": [50464, 452, 50564], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3152, "seek": 1196464, "start": 11970.64, "end": 11972.64, "text": " I'm going to go here", "tokens": [50664, 286, 478, 516, 281, 352, 510, 50764], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3153, "seek": 1196464, "start": 11972.64, "end": 11974.64, "text": " dog", "tokens": [50764, 3000, 50864], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3154, "seek": 1196464, "start": 11978.64, "end": 11980.64, "text": " has", "tokens": [51064, 575, 51164], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3155, "seek": 1196464, "start": 11980.64, "end": 11982.64, "text": " please", "tokens": [51164, 1767, 51264], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3156, "seek": 1196464, "start": 11982.64, "end": 11984.64, "text": " and then my", "tokens": [51264, 293, 550, 452, 51364], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3157, "seek": 1196464, "start": 11986.64, "end": 11988.64, "text": " my dog", "tokens": [51464, 452, 3000, 51564], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3158, "seek": 1196464, "start": 11990.64, "end": 11992.64, "text": " so I delete that", "tokens": [51664, 370, 286, 12097, 300, 51764], "temperature": 0.0, "avg_logprob": -0.2462294578552246, "compression_ratio": 1.1081081081081081, "no_speech_prob": 0.025529326871037483}, {"id": 3159, "seek": 1199464, "start": 11994.64, "end": 11996.64, "text": " my dog has", "tokens": [50364, 452, 3000, 575, 50464], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3160, "seek": 1199464, "start": 11996.64, "end": 11998.64, "text": " please", "tokens": [50464, 1767, 50564], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3161, "seek": 1199464, "start": 11998.64, "end": 12000.64, "text": " cool", "tokens": [50564, 1627, 50664], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3162, "seek": 1199464, "start": 12000.64, "end": 12002.64, "text": " so to what degree", "tokens": [50664, 370, 281, 437, 4314, 50764], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3163, "seek": 1199464, "start": 12002.64, "end": 12004.64, "text": " are these going to interact well my and my", "tokens": [50764, 366, 613, 516, 281, 4648, 731, 452, 293, 452, 50864], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3164, "seek": 1199464, "start": 12004.64, "end": 12006.64, "text": " I mean it doesn't really", "tokens": [50864, 286, 914, 309, 1177, 380, 534, 50964], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3165, "seek": 1199464, "start": 12006.64, "end": 12008.64, "text": " give away that much it's only just the start", "tokens": [50964, 976, 1314, 300, 709, 309, 311, 787, 445, 264, 722, 51064], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3166, "seek": 1199464, "start": 12008.64, "end": 12010.64, "text": " so maybe this will interact to", "tokens": [51064, 370, 1310, 341, 486, 4648, 281, 51164], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3167, "seek": 1199464, "start": 12010.64, "end": 12012.64, "text": " a low amount", "tokens": [51164, 257, 2295, 2372, 51264], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3168, "seek": 1199464, "start": 12012.64, "end": 12014.64, "text": " and then you have my and dog", "tokens": [51264, 293, 550, 291, 362, 452, 293, 3000, 51364], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3169, "seek": 1199464, "start": 12014.64, "end": 12016.64, "text": " these might interact to a medium", "tokens": [51364, 613, 1062, 4648, 281, 257, 6399, 51464], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3170, "seek": 1199464, "start": 12016.64, "end": 12018.64, "text": " amount because it's like your dog", "tokens": [51464, 2372, 570, 309, 311, 411, 428, 3000, 51564], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3171, "seek": 1199464, "start": 12018.64, "end": 12020.64, "text": " so we might go", "tokens": [51564, 370, 321, 1062, 352, 51664], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3172, "seek": 1199464, "start": 12020.64, "end": 12022.64, "text": " we might go medium", "tokens": [51664, 321, 1062, 352, 6399, 51764], "temperature": 0.0, "avg_logprob": -0.10281252152848952, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.003481746418401599}, {"id": 3173, "seek": 1202264, "start": 12022.64, "end": 12024.64, "text": " like that", "tokens": [50364, 411, 300, 50464], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3174, "seek": 1202264, "start": 12024.64, "end": 12026.64, "text": " and then my and has well that doesn't give away too much", "tokens": [50464, 293, 550, 452, 293, 575, 731, 300, 1177, 380, 976, 1314, 886, 709, 50564], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3175, "seek": 1202264, "start": 12026.64, "end": 12028.64, "text": " so maybe that'll be low", "tokens": [50564, 370, 1310, 300, 603, 312, 2295, 50664], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3176, "seek": 1202264, "start": 12028.64, "end": 12030.64, "text": " and then my and please it's like oh", "tokens": [50664, 293, 550, 452, 293, 1767, 309, 311, 411, 1954, 50764], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3177, "seek": 1202264, "start": 12030.64, "end": 12032.64, "text": " that doesn't really mean much my please that doesn't", "tokens": [50764, 300, 1177, 380, 534, 914, 709, 452, 1767, 300, 1177, 380, 50864], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3178, "seek": 1202264, "start": 12032.64, "end": 12034.64, "text": " really make sense maybe we'll", "tokens": [50864, 534, 652, 2020, 1310, 321, 603, 50964], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3179, "seek": 1202264, "start": 12034.64, "end": 12036.64, "text": " have it interact to a low amount", "tokens": [50964, 362, 309, 4648, 281, 257, 2295, 2372, 51064], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3180, "seek": 1202264, "start": 12036.64, "end": 12038.64, "text": " and then", "tokens": [51064, 293, 550, 51164], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3181, "seek": 1202264, "start": 12038.64, "end": 12040.64, "text": " these would be the same", "tokens": [51164, 613, 576, 312, 264, 912, 51264], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3182, "seek": 1202264, "start": 12040.64, "end": 12042.64, "text": " thing so", "tokens": [51264, 551, 370, 51364], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3183, "seek": 1202264, "start": 12042.64, "end": 12044.64, "text": " my and dog so be medium", "tokens": [51364, 452, 293, 3000, 370, 312, 6399, 51464], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3184, "seek": 1202264, "start": 12044.64, "end": 12046.64, "text": " and then has and has", "tokens": [51464, 293, 550, 575, 293, 575, 51564], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3185, "seek": 1202264, "start": 12046.64, "end": 12048.64, "text": " would be low", "tokens": [51564, 576, 312, 2295, 51664], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3186, "seek": 1202264, "start": 12048.64, "end": 12050.64, "text": " and then my and please would also be low", "tokens": [51664, 293, 550, 452, 293, 1767, 576, 611, 312, 2295, 51764], "temperature": 0.0, "avg_logprob": -0.11124942220490554, "compression_ratio": 2.1396648044692737, "no_speech_prob": 0.015651186928153038}, {"id": 3187, "seek": 1205064, "start": 12050.64, "end": 12052.64, "text": " and then you have dog and dog", "tokens": [50364, 293, 550, 291, 362, 3000, 293, 3000, 50464], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3188, "seek": 1205064, "start": 12052.64, "end": 12054.64, "text": " so these might interact to a low amount they're the same word", "tokens": [50464, 370, 613, 1062, 4648, 281, 257, 2295, 2372, 436, 434, 264, 912, 1349, 50564], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3189, "seek": 1205064, "start": 12054.64, "end": 12056.64, "text": " so we'll just", "tokens": [50564, 370, 321, 603, 445, 50664], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3190, "seek": 1205064, "start": 12056.64, "end": 12058.64, "text": " forget about that and then we have", "tokens": [50664, 2870, 466, 300, 293, 550, 321, 362, 50764], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3191, "seek": 1205064, "start": 12058.64, "end": 12060.64, "text": " a dog has", "tokens": [50764, 257, 3000, 575, 50864], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3192, "seek": 1205064, "start": 12060.64, "end": 12062.64, "text": " so these might interact to a medium amount", "tokens": [50864, 370, 613, 1062, 4648, 281, 257, 6399, 2372, 50964], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3193, "seek": 1205064, "start": 12062.64, "end": 12064.64, "text": " dog has the dog has", "tokens": [50964, 3000, 575, 264, 3000, 575, 51064], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3194, "seek": 1205064, "start": 12064.64, "end": 12066.64, "text": " something", "tokens": [51064, 746, 51164], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3195, "seek": 1205064, "start": 12066.64, "end": 12068.64, "text": " and then dog and please", "tokens": [51164, 293, 550, 3000, 293, 1767, 51264], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3196, "seek": 1205064, "start": 12068.64, "end": 12070.64, "text": " these might interact to a high amount", "tokens": [51264, 613, 1062, 4648, 281, 257, 1090, 2372, 51364], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3197, "seek": 1205064, "start": 12070.64, "end": 12072.64, "text": " because they're associating the dog", "tokens": [51364, 570, 436, 434, 4180, 990, 264, 3000, 51464], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3198, "seek": 1205064, "start": 12072.64, "end": 12074.64, "text": " with something else meaning please", "tokens": [51464, 365, 746, 1646, 3620, 1767, 51564], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3199, "seek": 1205064, "start": 12074.64, "end": 12076.64, "text": " we have has", "tokens": [51564, 321, 362, 575, 51664], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3200, "seek": 1205064, "start": 12076.64, "end": 12078.64, "text": " and dog these would interact to the same amount so", "tokens": [51664, 293, 3000, 613, 576, 4648, 281, 264, 912, 2372, 370, 51764], "temperature": 0.0, "avg_logprob": -0.10652474113132643, "compression_ratio": 2.252688172043011, "no_speech_prob": 0.003323795273900032}, {"id": 3201, "seek": 1207864, "start": 12078.64, "end": 12080.64, "text": " medium and then has and has", "tokens": [50364, 6399, 293, 550, 575, 293, 575, 50464], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3202, "seek": 1207864, "start": 12080.64, "end": 12082.64, "text": " be", "tokens": [50464, 312, 50564], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3203, "seek": 1207864, "start": 12082.64, "end": 12084.64, "text": " probably", "tokens": [50564, 1391, 50664], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3204, "seek": 1207864, "start": 12084.64, "end": 12086.64, "text": " to a low amount", "tokens": [50664, 281, 257, 2295, 2372, 50764], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3205, "seek": 1207864, "start": 12086.64, "end": 12088.64, "text": " and then", "tokens": [50764, 293, 550, 50864], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3206, "seek": 1207864, "start": 12088.64, "end": 12090.64, "text": " we could do low for", "tokens": [50864, 321, 727, 360, 2295, 337, 50964], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3207, "seek": 1207864, "start": 12090.64, "end": 12092.64, "text": " we could do what was it high", "tokens": [50964, 321, 727, 360, 437, 390, 309, 1090, 51064], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3208, "seek": 1207864, "start": 12092.64, "end": 12094.64, "text": " for this one as well please and dog", "tokens": [51064, 337, 341, 472, 382, 731, 1767, 293, 3000, 51164], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3209, "seek": 1207864, "start": 12094.64, "end": 12096.64, "text": " so these will interact", "tokens": [51164, 370, 613, 486, 4648, 51264], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3210, "seek": 1207864, "start": 12096.64, "end": 12098.64, "text": " to a high amount", "tokens": [51264, 281, 257, 1090, 2372, 51364], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3211, "seek": 1207864, "start": 12098.64, "end": 12100.64, "text": " and then we have has and please", "tokens": [51364, 293, 550, 321, 362, 575, 293, 1767, 51464], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3212, "seek": 1207864, "start": 12100.64, "end": 12102.64, "text": " so", "tokens": [51464, 370, 51564], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3213, "seek": 1207864, "start": 12102.64, "end": 12104.64, "text": " these could interact maybe a medium", "tokens": [51564, 613, 727, 4648, 1310, 257, 6399, 51664], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3214, "seek": 1207864, "start": 12104.64, "end": 12106.64, "text": " amount", "tokens": [51664, 2372, 51764], "temperature": 0.0, "avg_logprob": -0.1415936261758037, "compression_ratio": 1.8936170212765957, "no_speech_prob": 0.016143852844834328}, {"id": 3215, "seek": 1210664, "start": 12106.64, "end": 12108.64, "text": " medium and then please and please which would be low", "tokens": [50364, 6399, 293, 550, 1767, 293, 1767, 597, 576, 312, 2295, 50464], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3216, "seek": 1210664, "start": 12108.64, "end": 12110.64, "text": " so what you get", "tokens": [50464, 370, 437, 291, 483, 50564], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3217, "seek": 1210664, "start": 12110.64, "end": 12112.64, "text": " I'll just highlight this in", "tokens": [50564, 286, 603, 445, 5078, 341, 294, 50664], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3218, "seek": 1210664, "start": 12112.64, "end": 12114.64, "text": " I'll just highlight this in green here", "tokens": [50664, 286, 603, 445, 5078, 341, 294, 3092, 510, 50764], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3219, "seek": 1210664, "start": 12114.64, "end": 12116.64, "text": " so you get", "tokens": [50764, 370, 291, 483, 50864], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3220, "seek": 1210664, "start": 12116.64, "end": 12118.64, "text": " all the medium", "tokens": [50864, 439, 264, 6399, 50964], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3221, "seek": 1210664, "start": 12118.64, "end": 12120.64, "text": " and high attention scores", "tokens": [50964, 293, 1090, 3202, 13444, 51064], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3222, "seek": 1210664, "start": 12120.64, "end": 12122.64, "text": " you'd have your medium here", "tokens": [51064, 291, 1116, 362, 428, 6399, 510, 51164], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3223, "seek": 1210664, "start": 12122.64, "end": 12124.64, "text": " medium here", "tokens": [51164, 6399, 510, 51264], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3224, "seek": 1210664, "start": 12124.64, "end": 12126.64, "text": " high medium", "tokens": [51264, 1090, 6399, 51364], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3225, "seek": 1210664, "start": 12126.64, "end": 12128.64, "text": " medium high", "tokens": [51364, 6399, 1090, 51464], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3226, "seek": 1210664, "start": 12128.64, "end": 12130.64, "text": " medium and medium", "tokens": [51464, 6399, 293, 6399, 51564], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3227, "seek": 1210664, "start": 12130.64, "end": 12132.64, "text": " so you can see that these are sort of symmetrical", "tokens": [51564, 370, 291, 393, 536, 300, 613, 366, 1333, 295, 40360, 51664], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3228, "seek": 1210664, "start": 12132.64, "end": 12134.64, "text": " and this is what the attention map", "tokens": [51664, 293, 341, 307, 437, 264, 3202, 4471, 51764], "temperature": 0.0, "avg_logprob": -0.14031518936157228, "compression_ratio": 2.0, "no_speech_prob": 0.008054180070757866}, {"id": 3229, "seek": 1213464, "start": 12134.64, "end": 12136.64, "text": " will look like of course there's going to be some", "tokens": [50364, 486, 574, 411, 295, 1164, 456, 311, 516, 281, 312, 512, 50464], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3230, "seek": 1213464, "start": 12136.64, "end": 12138.64, "text": " scaling going on here based on the amount", "tokens": [50464, 21589, 516, 322, 510, 2361, 322, 264, 2372, 50564], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3231, "seek": 1213464, "start": 12138.64, "end": 12140.64, "text": " of actual attention's", "tokens": [50564, 295, 3539, 3202, 311, 50664], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3232, "seek": 1213464, "start": 12140.64, "end": 12142.64, "text": " heads we have running in parallel", "tokens": [50664, 8050, 321, 362, 2614, 294, 8952, 50764], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3233, "seek": 1213464, "start": 12142.64, "end": 12144.64, "text": " but that's besides the point", "tokens": [50764, 457, 300, 311, 11868, 264, 935, 50864], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3234, "seek": 1213464, "start": 12144.64, "end": 12146.64, "text": " really what's going on here", "tokens": [50864, 534, 437, 311, 516, 322, 510, 50964], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3235, "seek": 1213464, "start": 12146.64, "end": 12148.64, "text": " is the network", "tokens": [50964, 307, 264, 3209, 51064], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3236, "seek": 1213464, "start": 12148.64, "end": 12150.64, "text": " is going to learn how to place", "tokens": [51064, 307, 516, 281, 1466, 577, 281, 1081, 51164], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3237, "seek": 1213464, "start": 12150.64, "end": 12152.64, "text": " the right", "tokens": [51164, 264, 558, 51264], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3238, "seek": 1213464, "start": 12152.64, "end": 12154.64, "text": " attention scores because attention is simply", "tokens": [51264, 3202, 13444, 570, 3202, 307, 2935, 51364], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3239, "seek": 1213464, "start": 12154.64, "end": 12156.64, "text": " being used to generate tokens", "tokens": [51364, 885, 1143, 281, 8460, 22667, 51464], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3240, "seek": 1213464, "start": 12156.64, "end": 12158.64, "text": " that's that's how the", "tokens": [51464, 300, 311, 300, 311, 577, 264, 51564], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3241, "seek": 1213464, "start": 12158.64, "end": 12160.64, "text": " that's how the GPT works it's using attention", "tokens": [51564, 300, 311, 577, 264, 26039, 51, 1985, 309, 311, 1228, 3202, 51664], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3242, "seek": 1213464, "start": 12160.64, "end": 12162.64, "text": " to generate tokens", "tokens": [51664, 281, 8460, 22667, 51764], "temperature": 0.0, "avg_logprob": -0.08933980841385691, "compression_ratio": 1.8923766816143497, "no_speech_prob": 0.03304355964064598}, {"id": 3243, "seek": 1216264, "start": 12162.64, "end": 12164.64, "text": " so we can make", "tokens": [50364, 370, 321, 393, 652, 50464], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3244, "seek": 1216264, "start": 12164.64, "end": 12166.64, "text": " those sort of attention", "tokens": [50464, 729, 1333, 295, 3202, 50564], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3245, "seek": 1216264, "start": 12166.64, "end": 12168.64, "text": " scores how they're placed", "tokens": [50564, 13444, 577, 436, 434, 7074, 50664], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3246, "seek": 1216264, "start": 12168.64, "end": 12170.64, "text": " we can make those learnable", "tokens": [50664, 321, 393, 652, 729, 1466, 712, 50764], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3247, "seek": 1216264, "start": 12170.64, "end": 12172.64, "text": " through all of the like embeddings", "tokens": [50764, 807, 439, 295, 264, 411, 12240, 29432, 50864], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3248, "seek": 1216264, "start": 12172.64, "end": 12174.64, "text": " like everything we have in the entire", "tokens": [50864, 411, 1203, 321, 362, 294, 264, 2302, 50964], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3249, "seek": 1216264, "start": 12174.64, "end": 12176.64, "text": " network can make sure", "tokens": [50964, 3209, 393, 652, 988, 51064], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3250, "seek": 1216264, "start": 12176.64, "end": 12178.64, "text": " that we place effective attention scores", "tokens": [51064, 300, 321, 1081, 4942, 3202, 13444, 51164], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3251, "seek": 1216264, "start": 12178.64, "end": 12180.64, "text": " and to make sure that they're measured properly", "tokens": [51164, 293, 281, 652, 988, 300, 436, 434, 12690, 6108, 51264], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3252, "seek": 1216264, "start": 12180.64, "end": 12182.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3253, "seek": 1216264, "start": 12182.64, "end": 12184.64, "text": " obviously I didn't quantify these very well", "tokens": [51364, 2745, 286, 994, 380, 40421, 613, 588, 731, 51464], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3254, "seek": 1216264, "start": 12184.64, "end": 12186.64, "text": " like not with floating point numbers", "tokens": [51464, 411, 406, 365, 12607, 935, 3547, 51564], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3255, "seek": 1216264, "start": 12186.64, "end": 12188.64, "text": " but this is sort of the premise", "tokens": [51564, 457, 341, 307, 1333, 295, 264, 22045, 51664], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3256, "seek": 1216264, "start": 12188.64, "end": 12190.64, "text": " of how it works and how we want", "tokens": [51664, 295, 577, 309, 1985, 293, 577, 321, 528, 51764], "temperature": 0.0, "avg_logprob": -0.03669948237282889, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.005382866598665714}, {"id": 3257, "seek": 1219064, "start": 12190.64, "end": 12192.64, "text": " the model to look at different tokens", "tokens": [50364, 264, 2316, 281, 574, 412, 819, 22667, 50464], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3258, "seek": 1219064, "start": 12192.64, "end": 12194.64, "text": " and how they relate to one another", "tokens": [50464, 293, 577, 436, 10961, 281, 472, 1071, 50564], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3259, "seek": 1219064, "start": 12194.64, "end": 12196.64, "text": " so that's what the", "tokens": [50564, 370, 300, 311, 437, 264, 50664], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3260, "seek": 1219064, "start": 12196.64, "end": 12198.64, "text": " attention mechanism looks like under the hood", "tokens": [50664, 3202, 7513, 1542, 411, 833, 264, 13376, 50764], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3261, "seek": 1219064, "start": 12198.64, "end": 12200.64, "text": " so this is what the actual", "tokens": [50764, 370, 341, 307, 437, 264, 3539, 50864], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3262, "seek": 1219064, "start": 12200.64, "end": 12202.64, "text": " GPT or decoder only", "tokens": [50864, 26039, 51, 420, 979, 19866, 787, 50964], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3263, "seek": 1219064, "start": 12202.64, "end": 12204.64, "text": " transformer architecture looks like", "tokens": [50964, 31782, 9482, 1542, 411, 51064], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3264, "seek": 1219064, "start": 12204.64, "end": 12206.64, "text": " and", "tokens": [51064, 293, 51164], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3265, "seek": 1219064, "start": 12206.64, "end": 12208.64, "text": " so I'm just going to go through this step by step here", "tokens": [51164, 370, 286, 478, 445, 516, 281, 352, 807, 341, 1823, 538, 1823, 510, 51264], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3266, "seek": 1219064, "start": 12208.64, "end": 12210.64, "text": " and then we can hopefully jump into some of the math", "tokens": [51264, 293, 550, 321, 393, 4696, 3012, 666, 512, 295, 264, 5221, 51364], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3267, "seek": 1219064, "start": 12210.64, "end": 12212.64, "text": " and code behind how this works", "tokens": [51364, 293, 3089, 2261, 577, 341, 1985, 51464], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3268, "seek": 1219064, "start": 12212.64, "end": 12214.64, "text": " so we have", "tokens": [51464, 370, 321, 362, 51564], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3269, "seek": 1219064, "start": 12214.64, "end": 12216.64, "text": " our inputs embeddings and positional", "tokens": [51564, 527, 15743, 12240, 29432, 293, 2535, 304, 51664], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3270, "seek": 1219064, "start": 12216.64, "end": 12218.64, "text": " encodings we have only decoder", "tokens": [51664, 2058, 378, 1109, 321, 362, 787, 979, 19866, 51764], "temperature": 0.0, "avg_logprob": -0.08735689841026117, "compression_ratio": 1.7265625, "no_speech_prob": 0.010008283890783787}, {"id": 3271, "seek": 1221864, "start": 12218.64, "end": 12220.64, "text": " blocks and then some", "tokens": [50364, 8474, 293, 550, 512, 50464], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3272, "seek": 1221864, "start": 12220.64, "end": 12222.64, "text": " linear transformation", "tokens": [50464, 8213, 9887, 50564], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3273, "seek": 1221864, "start": 12222.64, "end": 12224.64, "text": " and then pretty much just", "tokens": [50564, 293, 550, 1238, 709, 445, 50664], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3274, "seek": 1221864, "start": 12224.64, "end": 12226.64, "text": " we do some softmax", "tokens": [50664, 321, 360, 512, 2787, 41167, 50764], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3275, "seek": 1221864, "start": 12226.64, "end": 12228.64, "text": " probability distribution", "tokens": [50764, 8482, 7316, 50864], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3276, "seek": 1221864, "start": 12228.64, "end": 12230.64, "text": " we sample from those and then we", "tokens": [50864, 321, 6889, 490, 729, 293, 550, 321, 50964], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3277, "seek": 1221864, "start": 12230.64, "end": 12232.64, "text": " start just generating some output", "tokens": [50964, 722, 445, 17746, 512, 5598, 51064], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3278, "seek": 1221864, "start": 12232.64, "end": 12234.64, "text": " and then we compare those to our inputs", "tokens": [51064, 293, 550, 321, 6794, 729, 281, 527, 15743, 51164], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3279, "seek": 1221864, "start": 12234.64, "end": 12236.64, "text": " and see how off they were, optimized from that", "tokens": [51164, 293, 536, 577, 766, 436, 645, 11, 26941, 490, 300, 51264], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3280, "seek": 1221864, "start": 12236.64, "end": 12238.64, "text": " in each of these", "tokens": [51264, 294, 1184, 295, 613, 51364], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3281, "seek": 1221864, "start": 12238.64, "end": 12240.64, "text": " decoder blocks we have our all data", "tokens": [51364, 979, 19866, 8474, 321, 362, 527, 439, 1412, 51464], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3282, "seek": 1221864, "start": 12240.64, "end": 12242.64, "text": " attention, res connections", "tokens": [51464, 3202, 11, 725, 9271, 51564], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3283, "seek": 1221864, "start": 12242.64, "end": 12244.64, "text": " feedforward network consists", "tokens": [51564, 3154, 13305, 3209, 14689, 51664], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3284, "seek": 1221864, "start": 12244.64, "end": 12246.64, "text": " of a linear, real linear", "tokens": [51664, 295, 257, 8213, 11, 957, 8213, 51764], "temperature": 0.0, "avg_logprob": -0.14195038722111628, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0033755204640328884}, {"id": 3285, "seek": 1224664, "start": 12246.64, "end": 12248.64, "text": " border and then", "tokens": [50364, 7838, 293, 550, 50464], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3286, "seek": 1224664, "start": 12248.64, "end": 12250.64, "text": " another res connection", "tokens": [50464, 1071, 725, 4984, 50564], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3287, "seek": 1224664, "start": 12250.64, "end": 12252.64, "text": " in each of these multi-attentions", "tokens": [50564, 294, 1184, 295, 613, 4825, 12, 1591, 317, 626, 50664], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3288, "seek": 1224664, "start": 12252.64, "end": 12254.64, "text": " we have", "tokens": [50664, 321, 362, 50764], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3289, "seek": 1224664, "start": 12254.64, "end": 12256.64, "text": " multiple heads running in parallel", "tokens": [50764, 3866, 8050, 2614, 294, 8952, 50864], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3290, "seek": 1224664, "start": 12256.64, "end": 12258.64, "text": " and each of these heads is going to take a", "tokens": [50864, 293, 1184, 295, 613, 8050, 307, 516, 281, 747, 257, 50964], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3291, "seek": 1224664, "start": 12258.64, "end": 12260.64, "text": " key, query and value", "tokens": [50964, 2141, 11, 14581, 293, 2158, 51064], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3292, "seek": 1224664, "start": 12260.64, "end": 12262.64, "text": " these are all learnable", "tokens": [51064, 613, 366, 439, 1466, 712, 51164], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3293, "seek": 1224664, "start": 12262.64, "end": 12264.64, "text": " linear transformations", "tokens": [51164, 8213, 34852, 51264], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3294, "seek": 1224664, "start": 12264.64, "end": 12266.64, "text": " and", "tokens": [51264, 293, 51364], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3295, "seek": 1224664, "start": 12266.64, "end": 12268.64, "text": " we're going to basically dot product the key and query together", "tokens": [51364, 321, 434, 516, 281, 1936, 5893, 1674, 264, 2141, 293, 14581, 1214, 51464], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3296, "seek": 1224664, "start": 12268.64, "end": 12270.64, "text": " concatenate these results", "tokens": [51464, 1588, 7186, 473, 613, 3542, 51564], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3297, "seek": 1224664, "start": 12270.64, "end": 12272.64, "text": " and", "tokens": [51564, 293, 51664], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3298, "seek": 1224664, "start": 12272.64, "end": 12274.64, "text": " do a little transformation to sort of", "tokens": [51664, 360, 257, 707, 9887, 281, 1333, 295, 51764], "temperature": 0.0, "avg_logprob": -0.11509975433349609, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.017166586592793465}, {"id": 3299, "seek": 1227464, "start": 12274.64, "end": 12276.64, "text": " summarize it afterwards", "tokens": [50364, 20858, 309, 10543, 50464], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3300, "seek": 1227464, "start": 12276.64, "end": 12278.64, "text": " and then what actually goes on in the", "tokens": [50464, 293, 550, 437, 767, 1709, 322, 294, 264, 50564], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3301, "seek": 1227464, "start": 12278.64, "end": 12280.64, "text": " dot product attention is just the dot", "tokens": [50564, 5893, 1674, 3202, 307, 445, 264, 5893, 50664], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3302, "seek": 1227464, "start": 12280.64, "end": 12282.64, "text": " product meaning of the key and query", "tokens": [50664, 1674, 3620, 295, 264, 2141, 293, 14581, 50764], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3303, "seek": 1227464, "start": 12282.64, "end": 12284.64, "text": " the scaling to prevent", "tokens": [50764, 264, 21589, 281, 4871, 50864], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3304, "seek": 1227464, "start": 12284.64, "end": 12286.64, "text": " these values from exploding", "tokens": [50864, 613, 4190, 490, 35175, 50964], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3305, "seek": 1227464, "start": 12286.64, "end": 12288.64, "text": " to prevent the vanishing gradient problem", "tokens": [50964, 281, 4871, 264, 3161, 3807, 16235, 1154, 51064], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3306, "seek": 1227464, "start": 12288.64, "end": 12290.64, "text": " and then we have our", "tokens": [51064, 293, 550, 321, 362, 527, 51164], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3307, "seek": 1227464, "start": 12290.64, "end": 12292.64, "text": " masking to make sure that", "tokens": [51164, 31226, 281, 652, 988, 300, 51264], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3308, "seek": 1227464, "start": 12292.64, "end": 12294.64, "text": " these, to make sure the model", "tokens": [51264, 613, 11, 281, 652, 988, 264, 2316, 51364], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3309, "seek": 1227464, "start": 12294.64, "end": 12296.64, "text": " isn't looking ahead and cheating", "tokens": [51364, 1943, 380, 1237, 2286, 293, 18309, 51464], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3310, "seek": 1227464, "start": 12296.64, "end": 12298.64, "text": " and then softmax matrix multiply", "tokens": [51464, 293, 550, 2787, 41167, 8141, 12972, 51564], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3311, "seek": 1227464, "start": 12298.64, "end": 12300.64, "text": " we output that and then", "tokens": [51564, 321, 5598, 300, 293, 550, 51664], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3312, "seek": 1227464, "start": 12300.64, "end": 12302.64, "text": " kind of fill in the blank there, so cool", "tokens": [51664, 733, 295, 2836, 294, 264, 8247, 456, 11, 370, 1627, 51764], "temperature": 0.0, "avg_logprob": -0.10981744632386324, "compression_ratio": 1.8595744680851063, "no_speech_prob": 0.017433172091841698}, {"id": 3313, "seek": 1230264, "start": 12302.64, "end": 12304.64, "text": " this is a little bit", "tokens": [50364, 341, 307, 257, 707, 857, 50464], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3314, "seek": 1230264, "start": 12304.64, "end": 12306.64, "text": " pretty much the", "tokens": [50464, 1238, 709, 264, 50564], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3315, "seek": 1230264, "start": 12306.64, "end": 12308.64, "text": " transform architecture a little bit dumb", "tokens": [50564, 4088, 9482, 257, 707, 857, 10316, 50664], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3316, "seek": 1230264, "start": 12308.64, "end": 12310.64, "text": " down a little smaller", "tokens": [50664, 760, 257, 707, 4356, 50764], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3317, "seek": 1230264, "start": 12310.64, "end": 12312.64, "text": " in complexity to actually understand but", "tokens": [50764, 294, 14024, 281, 767, 1223, 457, 50864], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3318, "seek": 1230264, "start": 12312.64, "end": 12314.64, "text": " that's kind of the premise of what's going on here", "tokens": [50864, 300, 311, 733, 295, 264, 22045, 295, 437, 311, 516, 322, 510, 50964], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3319, "seek": 1230264, "start": 12314.64, "end": 12316.64, "text": " so still", "tokens": [50964, 370, 920, 51064], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3320, "seek": 1230264, "start": 12316.64, "end": 12318.64, "text": " implements a self-attention mechanism", "tokens": [51064, 704, 17988, 257, 2698, 12, 1591, 1251, 7513, 51164], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3321, "seek": 1230264, "start": 12320.64, "end": 12322.64, "text": " so as you can see now", "tokens": [51264, 370, 382, 291, 393, 536, 586, 51364], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3322, "seek": 1230264, "start": 12322.64, "end": 12324.64, "text": " I am currently", "tokens": [51364, 286, 669, 4362, 51464], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3323, "seek": 1230264, "start": 12324.64, "end": 12326.64, "text": " on my macbook", "tokens": [51464, 322, 452, 7912, 2939, 51564], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3324, "seek": 1230264, "start": 12326.64, "end": 12328.64, "text": " M2 chip, I'm not going to", "tokens": [51564, 376, 17, 11409, 11, 286, 478, 406, 516, 281, 51664], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3325, "seek": 1230264, "start": 12328.64, "end": 12330.64, "text": " go into the specs of why it's important", "tokens": [51664, 352, 666, 264, 27911, 295, 983, 309, 311, 1021, 51764], "temperature": 0.0, "avg_logprob": -0.13574757348923455, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0016480758786201477}, {"id": 3326, "seek": 1233064, "start": 12330.64, "end": 12332.64, "text": " but really quick, I'm just going to show you", "tokens": [50364, 457, 534, 1702, 11, 286, 478, 445, 516, 281, 855, 291, 50464], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3327, "seek": 1233064, "start": 12332.64, "end": 12334.64, "text": " how I SSH onto my other PC", "tokens": [50464, 577, 286, 12238, 39, 3911, 452, 661, 6465, 50564], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3328, "seek": 1233064, "start": 12334.64, "end": 12336.64, "text": " so I go", "tokens": [50564, 370, 286, 352, 50664], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3329, "seek": 1233064, "start": 12336.64, "end": 12338.64, "text": " SSH", "tokens": [50664, 12238, 39, 50764], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3330, "seek": 1233064, "start": 12338.64, "end": 12340.64, "text": " just like that and then I type in my", "tokens": [50764, 445, 411, 300, 293, 550, 286, 2010, 294, 452, 50864], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3331, "seek": 1233064, "start": 12340.64, "end": 12342.64, "text": " ipv4 address", "tokens": [50864, 28501, 85, 19, 2985, 50964], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3332, "seek": 1233064, "start": 12342.64, "end": 12344.64, "text": " and then", "tokens": [50964, 293, 550, 51064], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3333, "seek": 1233064, "start": 12344.64, "end": 12346.64, "text": " I just", "tokens": [51064, 286, 445, 51164], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3334, "seek": 1233064, "start": 12346.64, "end": 12348.64, "text": " get a simple password", "tokens": [51164, 483, 257, 2199, 11524, 51264], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3335, "seek": 1233064, "start": 12348.64, "end": 12350.64, "text": " here, password that I've never had", "tokens": [51264, 510, 11, 11524, 300, 286, 600, 1128, 632, 51364], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3336, "seek": 1233064, "start": 12350.64, "end": 12352.64, "text": " is cool", "tokens": [51364, 307, 1627, 51464], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3337, "seek": 1233064, "start": 12352.64, "end": 12354.64, "text": " so now I'm on my desktop computer", "tokens": [51464, 370, 586, 286, 478, 322, 452, 14502, 3820, 51564], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3338, "seek": 1233064, "start": 12354.64, "end": 12356.64, "text": " and this is the command prompt that I use for it", "tokens": [51564, 293, 341, 307, 264, 5622, 12391, 300, 286, 764, 337, 309, 51664], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3339, "seek": 1233064, "start": 12356.64, "end": 12358.64, "text": " so awesome", "tokens": [51664, 370, 3476, 51764], "temperature": 0.0, "avg_logprob": -0.1281319114397157, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01743525266647339}, {"id": 3340, "seek": 1235864, "start": 12358.64, "end": 12360.64, "text": " I'm going to go ahead and go into the", "tokens": [50364, 286, 478, 516, 281, 352, 2286, 293, 352, 666, 264, 50464], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3341, "seek": 1235864, "start": 12360.64, "end": 12362.64, "text": " free code camp", "tokens": [50464, 1737, 3089, 2255, 50564], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3342, "seek": 1235864, "start": 12362.64, "end": 12364.64, "text": " little directory I have", "tokens": [50564, 707, 21120, 286, 362, 50664], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3343, "seek": 1235864, "start": 12364.64, "end": 12366.64, "text": " so cd desktop", "tokens": [50664, 370, 269, 67, 14502, 50764], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3344, "seek": 1235864, "start": 12366.64, "end": 12368.64, "text": " cd python testing", "tokens": [50764, 269, 67, 38797, 4997, 50864], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3345, "seek": 1235864, "start": 12368.64, "end": 12370.64, "text": " and then here I'm actually going to activate", "tokens": [50864, 293, 550, 510, 286, 478, 767, 516, 281, 13615, 50964], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3346, "seek": 1235864, "start": 12370.64, "end": 12372.64, "text": " my CUDA virtual", "tokens": [50964, 452, 29777, 7509, 6374, 51064], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3347, "seek": 1235864, "start": 12372.64, "end": 12374.64, "text": " environment", "tokens": [51064, 2823, 51164], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3348, "seek": 1235864, "start": 12374.64, "end": 12376.64, "text": " oops, not accelerate", "tokens": [51164, 34166, 11, 406, 21341, 51264], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3349, "seek": 1235864, "start": 12376.64, "end": 12378.64, "text": " I'm going to go CUDA", "tokens": [51264, 286, 478, 516, 281, 352, 29777, 7509, 51364], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3350, "seek": 1235864, "start": 12378.64, "end": 12380.64, "text": " activate", "tokens": [51364, 13615, 51464], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3351, "seek": 1235864, "start": 12380.64, "end": 12382.64, "text": " cool and then I'm going to go", "tokens": [51464, 1627, 293, 550, 286, 478, 516, 281, 352, 51564], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3352, "seek": 1235864, "start": 12382.64, "end": 12384.64, "text": " cd into free code camp", "tokens": [51564, 269, 67, 666, 1737, 3089, 2255, 51664], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3353, "seek": 1235864, "start": 12384.64, "end": 12386.64, "text": " gbt course, awesome", "tokens": [51664, 290, 4517, 1164, 11, 3476, 51764], "temperature": 0.0, "avg_logprob": -0.14226896286010743, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.03728151321411133}, {"id": 3354, "seek": 1238664, "start": 12386.64, "end": 12388.64, "text": " so now, if I actually do", "tokens": [50364, 370, 586, 11, 498, 286, 767, 360, 50464], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3355, "seek": 1238664, "start": 12388.64, "end": 12390.64, "text": " code on here like this to open up my", "tokens": [50464, 3089, 322, 510, 411, 341, 281, 1269, 493, 452, 50564], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3356, "seek": 1238664, "start": 12390.64, "end": 12392.64, "text": " VS code, it doesn't do that", "tokens": [50564, 25091, 3089, 11, 309, 1177, 380, 360, 300, 50664], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3357, "seek": 1238664, "start": 12392.64, "end": 12394.64, "text": " so there's another little way I have to do this", "tokens": [50664, 370, 456, 311, 1071, 707, 636, 286, 362, 281, 360, 341, 50764], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3358, "seek": 1238664, "start": 12394.64, "end": 12396.64, "text": " and you have to go into", "tokens": [50764, 293, 291, 362, 281, 352, 666, 50864], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3359, "seek": 1238664, "start": 12396.64, "end": 12398.64, "text": " VS code", "tokens": [50864, 25091, 3089, 50964], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3360, "seek": 1238664, "start": 12398.64, "end": 12400.64, "text": " go into a little remote explorer here", "tokens": [50964, 352, 666, 257, 707, 8607, 39680, 510, 51064], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3361, "seek": 1238664, "start": 12400.64, "end": 12402.64, "text": " and then you can simply connect", "tokens": [51064, 293, 550, 291, 393, 2935, 1745, 51164], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3362, "seek": 1238664, "start": 12402.64, "end": 12404.64, "text": " so I'm just going to connect", "tokens": [51164, 370, 286, 478, 445, 516, 281, 1745, 51264], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3363, "seek": 1238664, "start": 12404.64, "end": 12406.64, "text": " to the current window", "tokens": [51264, 281, 264, 2190, 4910, 51364], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3364, "seek": 1238664, "start": 12406.64, "end": 12408.64, "text": " itself", "tokens": [51364, 2564, 51464], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3365, "seek": 1238664, "start": 12408.64, "end": 12410.64, "text": " there's an extension you need for this", "tokens": [51464, 456, 311, 364, 10320, 291, 643, 337, 341, 51564], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3366, "seek": 1238664, "start": 12410.64, "end": 12412.64, "text": " called open SSH server, I think it's what it's called", "tokens": [51564, 1219, 1269, 12238, 39, 7154, 11, 286, 519, 309, 311, 437, 309, 311, 1219, 51664], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3367, "seek": 1238664, "start": 12412.64, "end": 12414.64, "text": " and", "tokens": [51664, 293, 51764], "temperature": 0.0, "avg_logprob": -0.10194065157046989, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.008705873973667622}, {"id": 3368, "seek": 1241464, "start": 12414.64, "end": 12416.64, "text": " it's simply the same password I used in the command prompt", "tokens": [50364, 309, 311, 2935, 264, 912, 11524, 286, 1143, 294, 264, 5622, 12391, 50464], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3369, "seek": 1241464, "start": 12416.64, "end": 12418.64, "text": " I can type it correctly", "tokens": [50464, 286, 393, 2010, 309, 8944, 50564], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3370, "seek": 1241464, "start": 12424.64, "end": 12426.64, "text": " awesome", "tokens": [50864, 3476, 50964], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3371, "seek": 1241464, "start": 12426.64, "end": 12428.64, "text": " so now it's SSH into my computer", "tokens": [50964, 370, 586, 309, 311, 12238, 39, 666, 452, 3820, 51064], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3372, "seek": 1241464, "start": 12428.64, "end": 12430.64, "text": " upstairs", "tokens": [51064, 16462, 51164], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3373, "seek": 1241464, "start": 12430.64, "end": 12432.64, "text": " and I'm just going to open the little editor in here", "tokens": [51164, 293, 286, 478, 445, 516, 281, 1269, 264, 707, 9839, 294, 510, 51264], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3374, "seek": 1241464, "start": 12434.64, "end": 12436.64, "text": " nice, so you can see", "tokens": [51364, 1481, 11, 370, 291, 393, 536, 51464], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3375, "seek": 1241464, "start": 12436.64, "end": 12438.64, "text": " that it looks just like that, that's wonderful", "tokens": [51464, 300, 309, 1542, 445, 411, 300, 11, 300, 311, 3715, 51564], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3376, "seek": 1241464, "start": 12438.64, "end": 12440.64, "text": " so now", "tokens": [51564, 370, 586, 51664], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3377, "seek": 1241464, "start": 12440.64, "end": 12442.64, "text": " I'm going to open this in a Jupyter notebook", "tokens": [51664, 286, 478, 516, 281, 1269, 341, 294, 257, 22125, 88, 391, 21060, 51764], "temperature": 0.0, "avg_logprob": -0.12088387877076537, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.008440391160547733}, {"id": 3378, "seek": 1244464, "start": 12444.64, "end": 12446.64, "text": " actually", "tokens": [50364, 767, 50464], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3379, "seek": 1244464, "start": 12446.64, "end": 12448.64, "text": " cd into desktop here", "tokens": [50464, 269, 67, 666, 14502, 510, 50564], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3380, "seek": 1244464, "start": 12448.64, "end": 12450.64, "text": " cd python", "tokens": [50564, 269, 67, 38797, 50664], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3381, "seek": 1244464, "start": 12450.64, "end": 12452.64, "text": " cd python testing", "tokens": [50664, 269, 67, 38797, 4997, 50764], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3382, "seek": 1244464, "start": 12452.64, "end": 12454.64, "text": " CUDA scripts", "tokens": [50764, 29777, 7509, 23294, 50864], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3383, "seek": 1244464, "start": 12454.64, "end": 12456.64, "text": " activate", "tokens": [50864, 13615, 50964], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3384, "seek": 1244464, "start": 12456.64, "end": 12458.64, "text": " cd free code camp", "tokens": [50964, 269, 67, 1737, 3089, 2255, 51064], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3385, "seek": 1244464, "start": 12458.64, "end": 12460.64, "text": " gbt course and then code", "tokens": [51064, 290, 4517, 1164, 293, 550, 3089, 51164], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3386, "seek": 1244464, "start": 12460.64, "end": 12462.64, "text": " like that and it will open", "tokens": [51164, 411, 300, 293, 309, 486, 1269, 51264], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3387, "seek": 1244464, "start": 12462.64, "end": 12464.64, "text": " perfect", "tokens": [51264, 2176, 51364], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3388, "seek": 1244464, "start": 12464.64, "end": 12466.64, "text": " how wonderful is that and I've already done", "tokens": [51364, 577, 3715, 307, 300, 293, 286, 600, 1217, 1096, 51464], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3389, "seek": 1244464, "start": 12466.64, "end": 12468.64, "text": " a little bit of this here but", "tokens": [51464, 257, 707, 857, 295, 341, 510, 457, 51564], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3390, "seek": 1244464, "start": 12468.64, "end": 12470.64, "text": " we're going to", "tokens": [51564, 321, 434, 516, 281, 51664], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3391, "seek": 1244464, "start": 12470.64, "end": 12472.64, "text": " jump into exactly", "tokens": [51664, 3012, 666, 2293, 51764], "temperature": 0.0, "avg_logprob": -0.1230089556087147, "compression_ratio": 1.5, "no_speech_prob": 0.020631197839975357}, {"id": 3392, "seek": 1247264, "start": 12472.64, "end": 12474.64, "text": " how we can build up this transformer", "tokens": [50364, 577, 321, 393, 1322, 493, 341, 31782, 50464], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3393, "seek": 1247264, "start": 12474.64, "end": 12476.64, "text": " or gbt architecture", "tokens": [50464, 420, 290, 4517, 9482, 50564], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3394, "seek": 1247264, "start": 12476.64, "end": 12478.64, "text": " in the code itself", "tokens": [50564, 294, 264, 3089, 2564, 50664], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3395, "seek": 1247264, "start": 12478.64, "end": 12480.64, "text": " so I'm going to", "tokens": [50664, 370, 286, 478, 516, 281, 50764], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3396, "seek": 1247264, "start": 12480.64, "end": 12482.64, "text": " pop over to my Jupyter notebook in here", "tokens": [50764, 1665, 670, 281, 452, 22125, 88, 391, 21060, 294, 510, 50864], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3397, "seek": 1247264, "start": 12486.64, "end": 12488.64, "text": " cool and now this little address", "tokens": [51064, 1627, 293, 586, 341, 707, 2985, 51164], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3398, "seek": 1247264, "start": 12488.64, "end": 12490.64, "text": " I'm going to paste that", "tokens": [51164, 286, 478, 516, 281, 9163, 300, 51264], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3399, "seek": 1247264, "start": 12490.64, "end": 12492.64, "text": " into my", "tokens": [51264, 666, 452, 51364], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3400, "seek": 1247264, "start": 12492.64, "end": 12494.64, "text": " browser", "tokens": [51364, 11185, 51464], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3401, "seek": 1247264, "start": 12494.64, "end": 12496.64, "text": " awesome", "tokens": [51464, 3476, 51564], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3402, "seek": 1247264, "start": 12496.64, "end": 12498.64, "text": " so we have this gbt v1", "tokens": [51564, 370, 321, 362, 341, 290, 4517, 371, 16, 51664], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3403, "seek": 1247264, "start": 12498.64, "end": 12500.64, "text": " Jupyter notebook", "tokens": [51664, 22125, 88, 391, 21060, 51764], "temperature": 0.0, "avg_logprob": -0.10431879112519414, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.0041320184245705605}, {"id": 3404, "seek": 1250264, "start": 12502.64, "end": 12504.64, "text": " so what I've actually done is", "tokens": [50364, 370, 437, 286, 600, 767, 1096, 307, 50464], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3405, "seek": 1250264, "start": 12504.64, "end": 12506.64, "text": " I've done some importations here", "tokens": [50464, 286, 600, 1096, 512, 974, 763, 510, 50564], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3406, "seek": 1250264, "start": 12506.64, "end": 12508.64, "text": " so I've", "tokens": [50564, 370, 286, 600, 50664], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3407, "seek": 1250264, "start": 12508.64, "end": 12510.64, "text": " imported all of these", "tokens": [50664, 25524, 439, 295, 613, 50764], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3408, "seek": 1250264, "start": 12510.64, "end": 12512.64, "text": " python importations", "tokens": [50764, 38797, 974, 763, 50864], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3409, "seek": 1250264, "start": 12512.64, "end": 12514.64, "text": " all the hyper parameters that we used from before", "tokens": [50864, 439, 264, 9848, 9834, 300, 321, 1143, 490, 949, 50964], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3410, "seek": 1250264, "start": 12516.64, "end": 12518.64, "text": " I've imported the data loader", "tokens": [51064, 286, 600, 25524, 264, 1412, 3677, 260, 51164], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3411, "seek": 1250264, "start": 12518.64, "end": 12520.64, "text": " I've imported the tokenizer", "tokens": [51164, 286, 600, 25524, 264, 14862, 6545, 51264], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3412, "seek": 1250264, "start": 12520.64, "end": 12522.64, "text": " the train and bell splits", "tokens": [51264, 264, 3847, 293, 4549, 37741, 51364], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3413, "seek": 1250264, "start": 12522.64, "end": 12524.64, "text": " they get batch function", "tokens": [51364, 436, 483, 15245, 2445, 51464], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3414, "seek": 1250264, "start": 12524.64, "end": 12526.64, "text": " estimate loss, just everything", "tokens": [51464, 12539, 4470, 11, 445, 1203, 51564], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3415, "seek": 1250264, "start": 12526.64, "end": 12528.64, "text": " that we're going to need and it's all in", "tokens": [51564, 300, 321, 434, 516, 281, 643, 293, 309, 311, 439, 294, 51664], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3416, "seek": 1250264, "start": 12528.64, "end": 12530.64, "text": " neatly organized little code blocks", "tokens": [51664, 36634, 9983, 707, 3089, 8474, 51764], "temperature": 0.0, "avg_logprob": -0.11021050123068002, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.006484587676823139}, {"id": 3417, "seek": 1253064, "start": 12530.64, "end": 12532.64, "text": " so awesome", "tokens": [50364, 370, 3476, 50464], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3418, "seek": 1253064, "start": 12532.64, "end": 12534.64, "text": " now what?", "tokens": [50464, 586, 437, 30, 50564], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3419, "seek": 1253064, "start": 12534.64, "end": 12536.64, "text": " well let's go ahead and continue here", "tokens": [50564, 731, 718, 311, 352, 2286, 293, 2354, 510, 50664], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3420, "seek": 1253064, "start": 12536.64, "end": 12538.64, "text": " with the actual", "tokens": [50664, 365, 264, 3539, 50764], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3421, "seek": 1253064, "start": 12538.64, "end": 12540.64, "text": " upgrading", "tokens": [50764, 36249, 50864], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3422, "seek": 1253064, "start": 12540.64, "end": 12542.64, "text": " from the very", "tokens": [50864, 490, 264, 588, 50964], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3423, "seek": 1253064, "start": 12542.64, "end": 12544.64, "text": " top level so I remember", "tokens": [50964, 1192, 1496, 370, 286, 1604, 51064], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3424, "seek": 1253064, "start": 12544.64, "end": 12546.64, "text": " I actually showed", "tokens": [51064, 286, 767, 4712, 51164], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3425, "seek": 1253064, "start": 12546.64, "end": 12548.64, "text": " and you can skip back to this", "tokens": [51164, 293, 291, 393, 10023, 646, 281, 341, 51264], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3426, "seek": 1253064, "start": 12548.64, "end": 12550.64, "text": " I actually showed", "tokens": [51264, 286, 767, 4712, 51364], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3427, "seek": 1253064, "start": 12550.64, "end": 12552.64, "text": " the architecture of the gbt", "tokens": [51364, 264, 9482, 295, 264, 290, 4517, 51464], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3428, "seek": 1253064, "start": 12552.64, "end": 12554.64, "text": " sort of", "tokens": [51464, 1333, 295, 51564], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3429, "seek": 1253064, "start": 12554.64, "end": 12556.64, "text": " lined out in I guess a little sketch", "tokens": [51564, 17189, 484, 294, 286, 2041, 257, 707, 12325, 51664], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3430, "seek": 1253064, "start": 12556.64, "end": 12558.64, "text": " a little sketch that I did", "tokens": [51664, 257, 707, 12325, 300, 286, 630, 51764], "temperature": 0.0, "avg_logprob": -0.09282111061943901, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.005638403352349997}, {"id": 3431, "seek": 1255864, "start": 12558.64, "end": 12560.64, "text": " and all we're going to do", "tokens": [50364, 293, 439, 321, 434, 516, 281, 360, 50464], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3432, "seek": 1255864, "start": 12560.64, "end": 12562.64, "text": " is pretty much build up from the high level", "tokens": [50464, 307, 1238, 709, 1322, 493, 490, 264, 1090, 1496, 50564], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3433, "seek": 1255864, "start": 12562.64, "end": 12564.64, "text": " the high high level general", "tokens": [50564, 264, 1090, 1090, 1496, 2674, 50664], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3434, "seek": 1255864, "start": 12564.64, "end": 12566.64, "text": " architecture down to the technical stuff", "tokens": [50664, 9482, 760, 281, 264, 6191, 1507, 50764], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3435, "seek": 1255864, "start": 12566.64, "end": 12568.64, "text": " down to the very root", "tokens": [50764, 760, 281, 264, 588, 5593, 50864], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3436, "seek": 1255864, "start": 12568.64, "end": 12570.64, "text": " dot product attention", "tokens": [50864, 5893, 1674, 3202, 50964], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3437, "seek": 1255864, "start": 12570.64, "end": 12572.64, "text": " that we're going to be doing here", "tokens": [50964, 300, 321, 434, 516, 281, 312, 884, 510, 51064], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3438, "seek": 1255864, "start": 12572.64, "end": 12574.64, "text": " so I'm going to go ahead and start off", "tokens": [51064, 370, 286, 478, 516, 281, 352, 2286, 293, 722, 766, 51164], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3439, "seek": 1255864, "start": 12574.64, "end": 12576.64, "text": " with this", "tokens": [51164, 365, 341, 51264], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3440, "seek": 1255864, "start": 12576.64, "end": 12578.64, "text": " gbt language model which I just", "tokens": [51264, 290, 4517, 2856, 2316, 597, 286, 445, 51364], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3441, "seek": 1255864, "start": 12578.64, "end": 12580.64, "text": " renamed I replaced", "tokens": [51364, 40949, 286, 10772, 51464], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3442, "seek": 1255864, "start": 12580.64, "end": 12582.64, "text": " bygram", "tokens": [51464, 538, 1342, 51564], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3443, "seek": 1255864, "start": 12582.64, "end": 12584.64, "text": " with gbt here", "tokens": [51564, 365, 290, 4517, 510, 51664], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3444, "seek": 1255864, "start": 12584.64, "end": 12586.64, "text": " so that's all we're doing and", "tokens": [51664, 370, 300, 311, 439, 321, 434, 884, 293, 51764], "temperature": 0.0, "avg_logprob": -0.058797254474884875, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.012422065250575542}, {"id": 3445, "seek": 1258664, "start": 12586.64, "end": 12588.64, "text": " we're going to add some", "tokens": [50364, 321, 434, 516, 281, 909, 512, 50464], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3446, "seek": 1258664, "start": 12588.64, "end": 12590.64, "text": " little code bits and", "tokens": [50464, 707, 3089, 9239, 293, 50564], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3447, "seek": 1258664, "start": 12590.64, "end": 12592.64, "text": " just walk through step by step", "tokens": [50564, 445, 1792, 807, 1823, 538, 1823, 50664], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3448, "seek": 1258664, "start": 12592.64, "end": 12594.64, "text": " what we're doing so", "tokens": [50664, 437, 321, 434, 884, 370, 50764], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3449, "seek": 1258664, "start": 12594.64, "end": 12596.64, "text": " let's do that so great", "tokens": [50764, 718, 311, 360, 300, 370, 869, 50864], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3450, "seek": 1258664, "start": 12596.64, "end": 12598.64, "text": " we're going to next we're going to talk about", "tokens": [50864, 321, 434, 516, 281, 958, 321, 434, 516, 281, 751, 466, 50964], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3451, "seek": 1258664, "start": 12598.64, "end": 12600.64, "text": " these positional encodings", "tokens": [50964, 613, 2535, 304, 2058, 378, 1109, 51064], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3452, "seek": 1258664, "start": 12600.64, "end": 12602.64, "text": " so I go back to the paper here", "tokens": [51064, 370, 286, 352, 646, 281, 264, 3035, 510, 51164], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3453, "seek": 1258664, "start": 12602.64, "end": 12604.64, "text": " rather this architecture", "tokens": [51164, 2831, 341, 9482, 51264], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3454, "seek": 1258664, "start": 12604.64, "end": 12606.64, "text": " we initially have our tokenize inputs", "tokens": [51264, 321, 9105, 362, 527, 14862, 1125, 15743, 51364], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3455, "seek": 1258664, "start": 12606.64, "end": 12608.64, "text": " and then we give", "tokens": [51364, 293, 550, 321, 976, 51464], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3456, "seek": 1258664, "start": 12608.64, "end": 12610.64, "text": " we give them embedding", "tokens": [51464, 321, 976, 552, 12240, 3584, 51564], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3457, "seek": 1258664, "start": 12610.64, "end": 12612.64, "text": " so token embeddings and then a positional", "tokens": [51564, 370, 14862, 12240, 29432, 293, 550, 257, 2535, 304, 51664], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3458, "seek": 1258664, "start": 12612.64, "end": 12614.64, "text": " encoding so this positional", "tokens": [51664, 43430, 370, 341, 2535, 304, 51764], "temperature": 0.0, "avg_logprob": -0.09906416353972061, "compression_ratio": 1.8899521531100478, "no_speech_prob": 0.009408443234860897}, {"id": 3459, "seek": 1261464, "start": 12614.64, "end": 12616.64, "text": " encoding going back to the attention paper is right here", "tokens": [50364, 43430, 516, 646, 281, 264, 3202, 3035, 307, 558, 510, 50464], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3460, "seek": 1261464, "start": 12616.64, "end": 12618.64, "text": " so all it does", "tokens": [50464, 370, 439, 309, 775, 50564], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3461, "seek": 1261464, "start": 12618.64, "end": 12620.64, "text": " is every", "tokens": [50564, 307, 633, 50664], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3462, "seek": 1261464, "start": 12620.64, "end": 12622.64, "text": " even token index", "tokens": [50664, 754, 14862, 8186, 50764], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3463, "seek": 1261464, "start": 12622.64, "end": 12624.64, "text": " we apply this function", "tokens": [50764, 321, 3079, 341, 2445, 50864], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3464, "seek": 1261464, "start": 12624.64, "end": 12626.64, "text": " and then every odd token index", "tokens": [50864, 293, 550, 633, 7401, 14862, 8186, 50964], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3465, "seek": 1261464, "start": 12626.64, "end": 12628.64, "text": " we apply this function you don't really need to know", "tokens": [50964, 321, 3079, 341, 2445, 291, 500, 380, 534, 643, 281, 458, 51064], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3466, "seek": 1261464, "start": 12628.64, "end": 12630.64, "text": " what it's doing other than", "tokens": [51064, 437, 309, 311, 884, 661, 813, 51164], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3467, "seek": 1261464, "start": 12630.64, "end": 12632.64, "text": " the fact that these are the different sine", "tokens": [51164, 264, 1186, 300, 613, 366, 264, 819, 18609, 51264], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3468, "seek": 1261464, "start": 12632.64, "end": 12634.64, "text": " and cosine functions that it uses", "tokens": [51264, 293, 23565, 6828, 300, 309, 4960, 51364], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3469, "seek": 1261464, "start": 12634.64, "end": 12636.64, "text": " to apply positional encodings", "tokens": [51364, 281, 3079, 2535, 304, 2058, 378, 1109, 51464], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3470, "seek": 1261464, "start": 12636.64, "end": 12638.64, "text": " to the tokenized inputs", "tokens": [51464, 281, 264, 14862, 1602, 15743, 51564], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3471, "seek": 1261464, "start": 12638.64, "end": 12640.64, "text": " so every", "tokens": [51564, 370, 633, 51664], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3472, "seek": 1261464, "start": 12640.64, "end": 12642.64, "text": " so on our first", "tokens": [51664, 370, 322, 527, 700, 51764], "temperature": 0.0, "avg_logprob": -0.05644229606345848, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.006588295102119446}, {"id": 3473, "seek": 1264264, "start": 12642.64, "end": 12644.64, "text": " index or whatever let's say we have hello world", "tokens": [50364, 8186, 420, 2035, 718, 311, 584, 321, 362, 7751, 1002, 50464], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3474, "seek": 1264264, "start": 12644.64, "end": 12646.64, "text": " okay there's five characters here", "tokens": [50464, 1392, 456, 311, 1732, 4342, 510, 50564], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3475, "seek": 1264264, "start": 12646.64, "end": 12648.64, "text": " h will be index zero", "tokens": [50564, 276, 486, 312, 8186, 4018, 50664], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3476, "seek": 1264264, "start": 12648.64, "end": 12650.64, "text": " so it'll get an even", "tokens": [50664, 370, 309, 603, 483, 364, 754, 50764], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3477, "seek": 1264264, "start": 12650.64, "end": 12652.64, "text": " encoding function", "tokens": [50764, 43430, 2445, 50864], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3478, "seek": 1264264, "start": 12652.64, "end": 12654.64, "text": " and then e will be odd", "tokens": [50864, 293, 550, 308, 486, 312, 7401, 50964], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3479, "seek": 1264264, "start": 12654.64, "end": 12656.64, "text": " since it's index one so it'll get this one", "tokens": [50964, 1670, 309, 311, 8186, 472, 370, 309, 603, 483, 341, 472, 51064], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3480, "seek": 1264264, "start": 12656.64, "end": 12658.64, "text": " and then l will get this the next l will get", "tokens": [51064, 293, 550, 287, 486, 483, 341, 264, 958, 287, 486, 483, 51164], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3481, "seek": 1264264, "start": 12658.64, "end": 12660.64, "text": " this and then", "tokens": [51164, 341, 293, 550, 51264], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3482, "seek": 1264264, "start": 12660.64, "end": 12662.64, "text": " or I don't know if I messed up that", "tokens": [51264, 420, 286, 500, 380, 458, 498, 286, 16507, 493, 300, 51364], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3483, "seek": 1264264, "start": 12662.64, "end": 12664.64, "text": " order but essentially it just iterates", "tokens": [51364, 1668, 457, 4476, 309, 445, 17138, 1024, 51464], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3484, "seek": 1264264, "start": 12664.64, "end": 12666.64, "text": " and it goes back and forth between", "tokens": [51464, 293, 309, 1709, 646, 293, 5220, 1296, 51564], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3485, "seek": 1264264, "start": 12666.64, "end": 12668.64, "text": " those applying these fixed functions", "tokens": [51564, 729, 9275, 613, 6806, 6828, 51664], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3486, "seek": 1264264, "start": 12668.64, "end": 12670.64, "text": " and the thing is with fixed functions", "tokens": [51664, 293, 264, 551, 307, 365, 6806, 6828, 51764], "temperature": 0.0, "avg_logprob": -0.12193000884283156, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.026748059317469597}, {"id": 3487, "seek": 1267064, "start": 12670.64, "end": 12672.64, "text": " is that they don't actually", "tokens": [50364, 307, 300, 436, 500, 380, 767, 50464], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3488, "seek": 1267064, "start": 12672.64, "end": 12674.64, "text": " learn about the data at all", "tokens": [50464, 1466, 466, 264, 1412, 412, 439, 50564], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3489, "seek": 1267064, "start": 12674.64, "end": 12676.64, "text": " because they're fixed so another way we could", "tokens": [50564, 570, 436, 434, 6806, 370, 1071, 636, 321, 727, 50664], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3490, "seek": 1267064, "start": 12676.64, "end": 12678.64, "text": " do this would be using", "tokens": [50664, 360, 341, 576, 312, 1228, 50764], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3491, "seek": 1267064, "start": 12678.64, "end": 12680.64, "text": " nn.embedding which is what we use", "tokens": [50764, 297, 77, 13, 443, 2883, 3584, 597, 307, 437, 321, 764, 50864], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3492, "seek": 1267064, "start": 12680.64, "end": 12682.64, "text": " for the token", "tokens": [50864, 337, 264, 14862, 50964], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3493, "seek": 1267064, "start": 12682.64, "end": 12684.64, "text": " embedding so I'm going to go ahead", "tokens": [50964, 12240, 3584, 370, 286, 478, 516, 281, 352, 2286, 51064], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3494, "seek": 1267064, "start": 12684.64, "end": 12686.64, "text": " and implement this here in our", "tokens": [51064, 293, 4445, 341, 510, 294, 527, 51164], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3495, "seek": 1267064, "start": 12686.64, "end": 12688.64, "text": " gbtv one script so I'm going to go", "tokens": [51164, 290, 4517, 85, 472, 5755, 370, 286, 478, 516, 281, 352, 51264], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3496, "seek": 1267064, "start": 12688.64, "end": 12690.64, "text": " ahead and add on this line", "tokens": [51264, 2286, 293, 909, 322, 341, 1622, 51364], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3497, "seek": 1267064, "start": 12690.64, "end": 12692.64, "text": " self dot positional", "tokens": [51364, 2698, 5893, 2535, 304, 51464], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3498, "seek": 1267064, "start": 12692.64, "end": 12694.64, "text": " self dot position embedding table", "tokens": [51464, 2698, 5893, 2535, 12240, 3584, 3199, 51564], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3499, "seek": 1267064, "start": 12694.64, "end": 12696.64, "text": " nn.embedding block size", "tokens": [51564, 297, 77, 13, 443, 2883, 3584, 3461, 2744, 51664], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3500, "seek": 1267064, "start": 12696.64, "end": 12698.64, "text": " so the block size is the length", "tokens": [51664, 370, 264, 3461, 2744, 307, 264, 4641, 51764], "temperature": 0.0, "avg_logprob": -0.11233591845655065, "compression_ratio": 1.8807339449541285, "no_speech_prob": 0.003375783795490861}, {"id": 3501, "seek": 1269864, "start": 12698.64, "end": 12700.64, "text": " or the sequence length", "tokens": [50364, 420, 264, 8310, 4641, 50464], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3502, "seek": 1269864, "start": 12700.64, "end": 12702.64, "text": " which in our case", "tokens": [50464, 597, 294, 527, 1389, 50564], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3503, "seek": 1269864, "start": 12702.64, "end": 12704.64, "text": " it's going to be 8 so there's going to be 8 tokens", "tokens": [50564, 309, 311, 516, 281, 312, 1649, 370, 456, 311, 516, 281, 312, 1649, 22667, 50664], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3504, "seek": 1269864, "start": 12704.64, "end": 12706.64, "text": " and", "tokens": [50664, 293, 50764], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3505, "seek": 1269864, "start": 12706.64, "end": 12708.64, "text": " this means", "tokens": [50764, 341, 1355, 50864], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3506, "seek": 1269864, "start": 12708.64, "end": 12710.64, "text": " we're going to have 8 different indices", "tokens": [50864, 321, 434, 516, 281, 362, 1649, 819, 43840, 50964], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3507, "seek": 1269864, "start": 12710.64, "end": 12712.64, "text": " and each one is going to be", "tokens": [50964, 293, 1184, 472, 307, 516, 281, 312, 51064], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3508, "seek": 1269864, "start": 12712.64, "end": 12714.64, "text": " of size nn.embed", "tokens": [51064, 295, 2744, 297, 77, 13, 443, 2883, 51164], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3509, "seek": 1269864, "start": 12714.64, "end": 12716.64, "text": " and this is a new parameter I actually want to add here", "tokens": [51164, 293, 341, 307, 257, 777, 13075, 286, 767, 528, 281, 909, 510, 51264], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3510, "seek": 1269864, "start": 12716.64, "end": 12718.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3511, "seek": 1269864, "start": 12718.64, "end": 12720.64, "text": " nn.embed will not only be used", "tokens": [51364, 297, 77, 13, 443, 2883, 486, 406, 787, 312, 1143, 51464], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3512, "seek": 1269864, "start": 12720.64, "end": 12722.64, "text": " in positional embedding", "tokens": [51464, 294, 2535, 304, 12240, 3584, 51564], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3513, "seek": 1269864, "start": 12722.64, "end": 12724.64, "text": " but it will also be used in our", "tokens": [51564, 457, 309, 486, 611, 312, 1143, 294, 527, 51664], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3514, "seek": 1269864, "start": 12724.64, "end": 12726.64, "text": " token embedding because when we actually", "tokens": [51664, 14862, 12240, 3584, 570, 562, 321, 767, 51764], "temperature": 0.0, "avg_logprob": -0.06937437057495117, "compression_ratio": 1.8, "no_speech_prob": 0.000855836202390492}, {"id": 3515, "seek": 1272664, "start": 12726.64, "end": 12728.64, "text": " store", "tokens": [50364, 3531, 50464], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3516, "seek": 1272664, "start": 12728.64, "end": 12730.64, "text": " information about the tokens", "tokens": [50464, 1589, 466, 264, 22667, 50564], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3517, "seek": 1272664, "start": 12730.64, "end": 12732.64, "text": " we want that to be in a very large", "tokens": [50564, 321, 528, 300, 281, 312, 294, 257, 588, 2416, 50664], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3518, "seek": 1272664, "start": 12732.64, "end": 12734.64, "text": " vector so not necessarily", "tokens": [50664, 8062, 370, 406, 4725, 50764], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3519, "seek": 1272664, "start": 12734.64, "end": 12736.64, "text": " a probability distribution", "tokens": [50764, 257, 8482, 7316, 50864], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3520, "seek": 1272664, "start": 12736.64, "end": 12738.64, "text": " or what we were using before in the", "tokens": [50864, 420, 437, 321, 645, 1228, 949, 294, 264, 50964], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3521, "seek": 1272664, "start": 12738.64, "end": 12740.64, "text": " bi-gram language model but rather", "tokens": [50964, 3228, 12, 1342, 2856, 2316, 457, 2831, 51064], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3522, "seek": 1272664, "start": 12740.64, "end": 12742.64, "text": " a really large vector", "tokens": [51064, 257, 534, 2416, 8062, 51164], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3523, "seek": 1272664, "start": 12742.64, "end": 12744.64, "text": " or a list you could think about it", "tokens": [51164, 420, 257, 1329, 291, 727, 519, 466, 309, 51264], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3524, "seek": 1272664, "start": 12744.64, "end": 12746.64, "text": " as a bunch of different", "tokens": [51264, 382, 257, 3840, 295, 819, 51364], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3525, "seek": 1272664, "start": 12746.64, "end": 12748.64, "text": " attributes that", "tokens": [51364, 17212, 300, 51464], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3526, "seek": 1272664, "start": 12748.64, "end": 12750.64, "text": " are about a character so maybe", "tokens": [51464, 366, 466, 257, 2517, 370, 1310, 51564], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3527, "seek": 1272664, "start": 12750.64, "end": 12752.64, "text": " you know", "tokens": [51564, 291, 458, 51664], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3528, "seek": 1272664, "start": 12752.64, "end": 12754.64, "text": " A and E would be pretty close", "tokens": [51664, 316, 293, 462, 576, 312, 1238, 1998, 51764], "temperature": 0.0, "avg_logprob": -0.082409086227417, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.01168259046971798}, {"id": 3529, "seek": 1275464, "start": 12754.64, "end": 12756.64, "text": " but both vowels versus like", "tokens": [50364, 457, 1293, 44972, 5717, 411, 50464], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3530, "seek": 1275464, "start": 12756.64, "end": 12758.64, "text": " E and Z", "tokens": [50464, 462, 293, 1176, 50564], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3531, "seek": 1275464, "start": 12758.64, "end": 12760.64, "text": " would be very different because Z is not", "tokens": [50564, 576, 312, 588, 819, 570, 1176, 307, 406, 50664], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3532, "seek": 1275464, "start": 12760.64, "end": 12762.64, "text": " a very common letter and E is the most common letter", "tokens": [50664, 257, 588, 2689, 5063, 293, 462, 307, 264, 881, 2689, 5063, 50764], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3533, "seek": 1275464, "start": 12762.64, "end": 12764.64, "text": " in the alphabet so", "tokens": [50764, 294, 264, 23339, 370, 50864], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3534, "seek": 1275464, "start": 12764.64, "end": 12766.64, "text": " we pretty much just want to have", "tokens": [50864, 321, 1238, 709, 445, 528, 281, 362, 50964], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3535, "seek": 1275464, "start": 12766.64, "end": 12768.64, "text": " vectors to differentiate", "tokens": [50964, 18875, 281, 23203, 51064], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3536, "seek": 1275464, "start": 12768.64, "end": 12770.64, "text": " these tokens to place some", "tokens": [51064, 613, 22667, 281, 1081, 512, 51164], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3537, "seek": 1275464, "start": 12770.64, "end": 12772.64, "text": " semantic meaning on them", "tokens": [51164, 47982, 3620, 322, 552, 51264], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3538, "seek": 1275464, "start": 12772.64, "end": 12774.64, "text": " and anyways", "tokens": [51264, 293, 13448, 51364], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3539, "seek": 1275464, "start": 12774.64, "end": 12776.64, "text": " that's a little talk about what token embedding table", "tokens": [51364, 300, 311, 257, 707, 751, 466, 437, 14862, 12240, 3584, 3199, 51464], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3540, "seek": 1275464, "start": 12776.64, "end": 12778.64, "text": " is going to do when we add n.embed", "tokens": [51464, 307, 516, 281, 360, 562, 321, 909, 297, 13, 443, 2883, 51564], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3541, "seek": 1275464, "start": 12778.64, "end": 12780.64, "text": " and then positional embedding table", "tokens": [51564, 293, 550, 2535, 304, 12240, 3584, 3199, 51664], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3542, "seek": 1275464, "start": 12780.64, "end": 12782.64, "text": " is just the same thing", "tokens": [51664, 307, 445, 264, 912, 551, 51764], "temperature": 0.0, "avg_logprob": -0.07569575309753418, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.0012254100292921066}, {"id": 3543, "seek": 1278264, "start": 12782.64, "end": 12784.64, "text": " but instead of each character", "tokens": [50364, 457, 2602, 295, 1184, 2517, 50464], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3544, "seek": 1278264, "start": 12784.64, "end": 12786.64, "text": " having its own thing", "tokens": [50464, 1419, 1080, 1065, 551, 50564], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3545, "seek": 1278264, "start": 12786.64, "end": 12788.64, "text": " each letter", "tokens": [50564, 1184, 5063, 50664], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3546, "seek": 1278264, "start": 12788.64, "end": 12790.64, "text": " index in the input is going to have its own embedding", "tokens": [50664, 8186, 294, 264, 4846, 307, 516, 281, 362, 1080, 1065, 12240, 3584, 50764], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3547, "seek": 1278264, "start": 12790.64, "end": 12792.64, "text": " so I can go and add this", "tokens": [50764, 370, 286, 393, 352, 293, 909, 341, 50864], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3548, "seek": 1278264, "start": 12792.64, "end": 12794.64, "text": " up here", "tokens": [50864, 493, 510, 50964], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3549, "seek": 1278264, "start": 12794.64, "end": 12796.64, "text": " the n.embed", "tokens": [50964, 264, 297, 13, 443, 2883, 51064], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3550, "seek": 1278264, "start": 12796.64, "end": 12798.64, "text": " and we can just make this", "tokens": [51064, 293, 321, 393, 445, 652, 341, 51164], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3551, "seek": 1278264, "start": 12798.64, "end": 12800.64, "text": " maybe 384", "tokens": [51164, 1310, 12843, 19, 51264], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3552, "seek": 1278264, "start": 12800.64, "end": 12802.64, "text": " so 384 is quite huge", "tokens": [51264, 370, 12843, 19, 307, 1596, 2603, 51364], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3553, "seek": 1278264, "start": 12802.64, "end": 12804.64, "text": " and it's maybe a little too big", "tokens": [51364, 293, 309, 311, 1310, 257, 707, 886, 955, 51464], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3554, "seek": 1278264, "start": 12804.64, "end": 12806.64, "text": " for your PC but we'll see in a second", "tokens": [51464, 337, 428, 6465, 457, 321, 603, 536, 294, 257, 1150, 51564], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3555, "seek": 1278264, "start": 12806.64, "end": 12808.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3556, "seek": 1278264, "start": 12808.64, "end": 12810.64, "text": " what this is going to do is it's going to have a giant vector", "tokens": [51664, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 362, 257, 7410, 8062, 51764], "temperature": 0.0, "avg_logprob": -0.07857778976703512, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.003027344588190317}, {"id": 3557, "seek": 1281064, "start": 12810.64, "end": 12812.64, "text": " it's going to be like", "tokens": [50364, 309, 311, 516, 281, 312, 411, 50464], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3558, "seek": 1281064, "start": 12812.64, "end": 12814.64, "text": " we could say like", "tokens": [50464, 321, 727, 584, 411, 50564], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3559, "seek": 1281064, "start": 12814.64, "end": 12816.64, "text": " embedding", "tokens": [50564, 12240, 3584, 50664], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3560, "seek": 1281064, "start": 12816.64, "end": 12818.64, "text": " embedding vector", "tokens": [50664, 12240, 3584, 8062, 50764], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3561, "seek": 1281064, "start": 12818.64, "end": 12820.64, "text": " and then it would be like this", "tokens": [50764, 293, 550, 309, 576, 312, 411, 341, 50864], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3562, "seek": 1281064, "start": 12820.64, "end": 12822.64, "text": " and you would have", "tokens": [50864, 293, 291, 576, 362, 50964], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3563, "seek": 1281064, "start": 12822.64, "end": 12824.64, "text": " a bunch of different attributes so like 0.1", "tokens": [50964, 257, 3840, 295, 819, 17212, 370, 411, 1958, 13, 16, 51064], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3564, "seek": 1281064, "start": 12824.64, "end": 12826.64, "text": " 0.2", "tokens": [51064, 1958, 13, 17, 51164], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3565, "seek": 1281064, "start": 12826.64, "end": 12828.64, "text": " 0.8", "tokens": [51164, 1958, 13, 23, 51264], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3566, "seek": 1281064, "start": 12828.64, "end": 12830.64, "text": " 1.1", "tokens": [51264, 502, 13, 16, 51364], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3567, "seek": 1281064, "start": 12830.64, "end": 12832.64, "text": " right? except", "tokens": [51364, 558, 30, 3993, 51464], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3568, "seek": 1281064, "start": 12832.64, "end": 12834.64, "text": " instead of 4 this is", "tokens": [51464, 2602, 295, 1017, 341, 307, 51564], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3569, "seek": 1281064, "start": 12834.64, "end": 12836.64, "text": " 384 elements long", "tokens": [51564, 12843, 19, 4959, 938, 51664], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3570, "seek": 1281064, "start": 12836.64, "end": 12838.64, "text": " and each of these", "tokens": [51664, 293, 1184, 295, 613, 51764], "temperature": 0.0, "avg_logprob": -0.10402694115271935, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.016651300713419914}, {"id": 3571, "seek": 1283864, "start": 12838.64, "end": 12840.64, "text": " is just going to store a tiny little attribute", "tokens": [50364, 307, 445, 516, 281, 3531, 257, 5870, 707, 19667, 50464], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3572, "seek": 1283864, "start": 12840.64, "end": 12842.64, "text": " about that token", "tokens": [50464, 466, 300, 14862, 50564], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3573, "seek": 1283864, "start": 12842.64, "end": 12844.64, "text": " so", "tokens": [50564, 370, 50664], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3574, "seek": 1283864, "start": 12844.64, "end": 12846.64, "text": " let's say we maybe had like a", "tokens": [50664, 718, 311, 584, 321, 1310, 632, 411, 257, 50764], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3575, "seek": 1283864, "start": 12846.64, "end": 12848.64, "text": " two dimensional and we were using a word", "tokens": [50764, 732, 18795, 293, 321, 645, 1228, 257, 1349, 50864], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3576, "seek": 1283864, "start": 12848.64, "end": 12850.64, "text": " so if we had", "tokens": [50864, 370, 498, 321, 632, 50964], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3577, "seek": 1283864, "start": 12850.64, "end": 12852.64, "text": " sad versus", "tokens": [50964, 4227, 5717, 51064], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3578, "seek": 1283864, "start": 12852.64, "end": 12854.64, "text": " happy", "tokens": [51064, 2055, 51164], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3579, "seek": 1283864, "start": 12854.64, "end": 12856.64, "text": " sad might be", "tokens": [51164, 4227, 1062, 312, 51264], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3580, "seek": 1283864, "start": 12856.64, "end": 12858.64, "text": " sad might be", "tokens": [51264, 4227, 1062, 312, 51364], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3581, "seek": 1283864, "start": 12858.64, "end": 12860.64, "text": " 0.1", "tokens": [51364, 1958, 13, 16, 51464], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3582, "seek": 1283864, "start": 12860.64, "end": 12862.64, "text": " and then", "tokens": [51464, 293, 550, 51564], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3583, "seek": 1283864, "start": 12862.64, "end": 12864.64, "text": " 0.8", "tokens": [51564, 1958, 13, 23, 51664], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3584, "seek": 1283864, "start": 12864.64, "end": 12866.64, "text": " or 0.8", "tokens": [51664, 420, 1958, 13, 23, 51764], "temperature": 0.0, "avg_logprob": -0.0766494217373076, "compression_ratio": 1.4863013698630136, "no_speech_prob": 0.006287414580583572}, {"id": 3585, "seek": 1286664, "start": 12866.64, "end": 12868.64, "text": " whereas happy", "tokens": [50364, 9735, 2055, 50464], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3586, "seek": 1286664, "start": 12868.64, "end": 12870.64, "text": " sad would be", "tokens": [50464, 4227, 576, 312, 50564], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3587, "seek": 1286664, "start": 12870.64, "end": 12872.64, "text": " maybe the positivity", "tokens": [50564, 1310, 264, 35198, 50664], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3588, "seek": 1286664, "start": 12872.64, "end": 12874.64, "text": " of what it's saying and then 0.8 would be", "tokens": [50664, 295, 437, 309, 311, 1566, 293, 550, 1958, 13, 23, 576, 312, 50764], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3589, "seek": 1286664, "start": 12874.64, "end": 12876.64, "text": " is it showing some sort of emotion", "tokens": [50764, 307, 309, 4099, 512, 1333, 295, 8913, 50864], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3590, "seek": 1286664, "start": 12876.64, "end": 12878.64, "text": " which is a lot right?", "tokens": [50864, 597, 307, 257, 688, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3591, "seek": 1286664, "start": 12878.64, "end": 12880.64, "text": " it's 80% emotion", "tokens": [50964, 309, 311, 4688, 4, 8913, 51064], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3592, "seek": 1286664, "start": 12880.64, "end": 12882.64, "text": " and 0.1", "tokens": [51064, 293, 1958, 13, 16, 51164], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3593, "seek": 1286664, "start": 12882.64, "end": 12884.64, "text": " of maybe positive sentiment", "tokens": [51164, 295, 1310, 3353, 16149, 51264], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3594, "seek": 1286664, "start": 12884.64, "end": 12886.64, "text": " and then if we had", "tokens": [51264, 293, 550, 498, 321, 632, 51364], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3595, "seek": 1286664, "start": 12886.64, "end": 12888.64, "text": " 0.9", "tokens": [51364, 1958, 13, 24, 51464], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3596, "seek": 1286664, "start": 12888.64, "end": 12890.64, "text": " would be happy because it's happy", "tokens": [51464, 576, 312, 2055, 570, 309, 311, 2055, 51564], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3597, "seek": 1286664, "start": 12890.64, "end": 12892.64, "text": " it's very good and then 0.8", "tokens": [51564, 309, 311, 588, 665, 293, 550, 1958, 13, 23, 51664], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3598, "seek": 1286664, "start": 12892.64, "end": 12894.64, "text": " is emotional because they're sort of the same", "tokens": [51664, 307, 6863, 570, 436, 434, 1333, 295, 264, 912, 51764], "temperature": 0.0, "avg_logprob": -0.08182668248447802, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0011510227341204882}, {"id": 3599, "seek": 1289464, "start": 12894.64, "end": 12896.64, "text": " emotional level", "tokens": [50364, 6863, 1496, 50464], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3600, "seek": 1289464, "start": 12896.64, "end": 12898.64, "text": " but yeah so this is what our embedding vectors", "tokens": [50464, 457, 1338, 370, 341, 307, 437, 527, 12240, 3584, 18875, 50564], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3601, "seek": 1289464, "start": 12898.64, "end": 12900.64, "text": " are pretty much describing and", "tokens": [50564, 366, 1238, 709, 16141, 293, 50664], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3602, "seek": 1289464, "start": 12900.64, "end": 12902.64, "text": " all this hyperparameter", "tokens": [50664, 439, 341, 9848, 2181, 335, 2398, 50764], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3603, "seek": 1289464, "start": 12902.64, "end": 12904.64, "text": " is concerned with is how long", "tokens": [50764, 307, 5922, 365, 307, 577, 938, 50864], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3604, "seek": 1289464, "start": 12904.64, "end": 12906.64, "text": " that vector actually is", "tokens": [50864, 300, 8062, 767, 307, 50964], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3605, "seek": 1289464, "start": 12906.64, "end": 12908.64, "text": " so anyways", "tokens": [50964, 370, 13448, 51064], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3606, "seek": 1289464, "start": 12908.64, "end": 12910.64, "text": " let's continue with the GPT", "tokens": [51064, 718, 311, 2354, 365, 264, 26039, 51, 51164], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3607, "seek": 1289464, "start": 12910.64, "end": 12912.64, "text": " language model class so the next bit I like", "tokens": [51164, 2856, 2316, 1508, 370, 264, 958, 857, 286, 411, 51264], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3608, "seek": 1289464, "start": 12912.64, "end": 12914.64, "text": " to talk about is how many decoder", "tokens": [51264, 281, 751, 466, 307, 577, 867, 979, 19866, 51364], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3609, "seek": 1289464, "start": 12914.64, "end": 12916.64, "text": " layers we have", "tokens": [51364, 7914, 321, 362, 51464], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3610, "seek": 1289464, "start": 12916.64, "end": 12918.64, "text": " so in here let's just say we have", "tokens": [51464, 370, 294, 510, 718, 311, 445, 584, 321, 362, 51564], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3611, "seek": 1289464, "start": 12918.64, "end": 12920.64, "text": " four decoder layers", "tokens": [51564, 1451, 979, 19866, 7914, 51664], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3612, "seek": 1289464, "start": 12920.64, "end": 12922.64, "text": " so we have four of these it's going to go through this one", "tokens": [51664, 370, 321, 362, 1451, 295, 613, 309, 311, 516, 281, 352, 807, 341, 472, 51764], "temperature": 0.0, "avg_logprob": -0.09283118688759684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.0038235357496887445}, {"id": 3613, "seek": 1292264, "start": 12922.64, "end": 12924.64, "text": " and then this one and then this one", "tokens": [50364, 293, 550, 341, 472, 293, 550, 341, 472, 50464], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3614, "seek": 1292264, "start": 12924.64, "end": 12926.64, "text": " then this one this is all happening", "tokens": [50464, 550, 341, 472, 341, 307, 439, 2737, 50564], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3615, "seek": 1292264, "start": 12926.64, "end": 12928.64, "text": " sequentially so we could", "tokens": [50564, 5123, 3137, 370, 321, 727, 50664], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3616, "seek": 1292264, "start": 12928.64, "end": 12930.64, "text": " actually make a little", "tokens": [50664, 767, 652, 257, 707, 50764], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3617, "seek": 1292264, "start": 12930.64, "end": 12932.64, "text": " sequential neural network with", "tokens": [50764, 42881, 18161, 3209, 365, 50864], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3618, "seek": 1292264, "start": 12932.64, "end": 12934.64, "text": " four decoder layers", "tokens": [50864, 1451, 979, 19866, 7914, 50964], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3619, "seek": 1292264, "start": 12934.64, "end": 12936.64, "text": " so I'm actually going to add this in", "tokens": [50964, 370, 286, 478, 767, 516, 281, 909, 341, 294, 51064], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3620, "seek": 1292264, "start": 12936.64, "end": 12938.64, "text": " and then a little bit of extra code which I'll explain", "tokens": [51064, 293, 550, 257, 707, 857, 295, 2857, 3089, 597, 286, 603, 2903, 51164], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3621, "seek": 1292264, "start": 12938.64, "end": 12940.64, "text": " in a second here so this", "tokens": [51164, 294, 257, 1150, 510, 370, 341, 51264], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3622, "seek": 1292264, "start": 12940.64, "end": 12942.64, "text": " self", "tokens": [51264, 2698, 51364], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3623, "seek": 1292264, "start": 12942.64, "end": 12944.64, "text": " dot blocks is how many", "tokens": [51364, 5893, 8474, 307, 577, 867, 51464], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3624, "seek": 1292264, "start": 12944.64, "end": 12946.64, "text": " decoder blocks we have running", "tokens": [51464, 979, 19866, 8474, 321, 362, 2614, 51564], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3625, "seek": 1292264, "start": 12946.64, "end": 12948.64, "text": " sequentially or layers", "tokens": [51564, 5123, 3137, 420, 7914, 51664], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3626, "seek": 1292264, "start": 12948.64, "end": 12950.64, "text": " blocks and layers can be used interchangeably in this", "tokens": [51664, 8474, 293, 7914, 393, 312, 1143, 30358, 1188, 294, 341, 51764], "temperature": 0.0, "avg_logprob": -0.0766877381697945, "compression_ratio": 1.9403669724770642, "no_speech_prob": 0.0034289476461708546}, {"id": 3627, "seek": 1295064, "start": 12950.64, "end": 12952.64, "text": " context", "tokens": [50364, 4319, 50464], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3628, "seek": 1295064, "start": 12952.64, "end": 12954.64, "text": " but yeah we have an end dot sequential", "tokens": [50464, 457, 1338, 321, 362, 364, 917, 5893, 42881, 50564], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3629, "seek": 1295064, "start": 12954.64, "end": 12956.64, "text": " and this asterisk is pretty much saying", "tokens": [50564, 293, 341, 257, 3120, 7797, 307, 1238, 709, 1566, 50664], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3630, "seek": 1295064, "start": 12956.64, "end": 12958.64, "text": " we're going to repeat", "tokens": [50664, 321, 434, 516, 281, 7149, 50764], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3631, "seek": 1295064, "start": 12958.64, "end": 12960.64, "text": " this right here", "tokens": [50764, 341, 558, 510, 50864], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3632, "seek": 1295064, "start": 12960.64, "end": 12962.64, "text": " for how many", "tokens": [50864, 337, 577, 867, 50964], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3633, "seek": 1295064, "start": 12962.64, "end": 12964.64, "text": " end layer is and end layer is another hyperparameter", "tokens": [50964, 917, 4583, 307, 293, 917, 4583, 307, 1071, 9848, 2181, 335, 2398, 51064], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3634, "seek": 1295064, "start": 12964.64, "end": 12966.64, "text": " we're going to add", "tokens": [51064, 321, 434, 516, 281, 909, 51164], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3635, "seek": 1295064, "start": 12966.64, "end": 12968.64, "text": " we go end underscore layer", "tokens": [51164, 321, 352, 917, 37556, 4583, 51264], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3636, "seek": 1295064, "start": 12968.64, "end": 12970.64, "text": " we go equals four", "tokens": [51264, 321, 352, 6915, 1451, 51364], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3637, "seek": 1295064, "start": 12972.64, "end": 12974.64, "text": " so end underscore layer equals four", "tokens": [51464, 370, 917, 37556, 4583, 6915, 1451, 51564], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3638, "seek": 1295064, "start": 12974.64, "end": 12976.64, "text": " that means it's going to make four of these", "tokens": [51564, 300, 1355, 309, 311, 516, 281, 652, 1451, 295, 613, 51664], "temperature": 0.0, "avg_logprob": -0.10191992140307869, "compression_ratio": 1.8453038674033149, "no_speech_prob": 0.0032220620196312666}, {"id": 3639, "seek": 1297664, "start": 12976.64, "end": 12978.64, "text": " I guess blocks", "tokens": [50364, 286, 2041, 8474, 50464], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3640, "seek": 1297664, "start": 12978.64, "end": 12980.64, "text": " or layers sequentially", "tokens": [50464, 420, 7914, 5123, 3137, 50564], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3641, "seek": 1297664, "start": 12980.64, "end": 12982.64, "text": " it's going to make four of them", "tokens": [50564, 309, 311, 516, 281, 652, 1451, 295, 552, 50664], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3642, "seek": 1297664, "start": 12982.64, "end": 12984.64, "text": " and this little block thing", "tokens": [50664, 293, 341, 707, 3461, 551, 50764], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3643, "seek": 1297664, "start": 12984.64, "end": 12986.64, "text": " we're going to build on top of this in a second here", "tokens": [50764, 321, 434, 516, 281, 1322, 322, 1192, 295, 341, 294, 257, 1150, 510, 50864], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3644, "seek": 1297664, "start": 12986.64, "end": 12988.64, "text": " we're going to make an actual block", "tokens": [50864, 321, 434, 516, 281, 652, 364, 3539, 3461, 50964], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3645, "seek": 1297664, "start": 12988.64, "end": 12990.64, "text": " class and I'm going to explain what that does", "tokens": [50964, 1508, 293, 286, 478, 516, 281, 2903, 437, 300, 775, 51064], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3646, "seek": 1297664, "start": 12990.64, "end": 12992.64, "text": " but for now", "tokens": [51064, 457, 337, 586, 51164], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3647, "seek": 1297664, "start": 12992.64, "end": 12994.64, "text": " this is going to be some temporary code", "tokens": [51164, 341, 307, 516, 281, 312, 512, 13413, 3089, 51264], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3648, "seek": 1297664, "start": 12994.64, "end": 12996.64, "text": " as long as you understand that this is what", "tokens": [51264, 382, 938, 382, 291, 1223, 300, 341, 307, 437, 51364], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3649, "seek": 1297664, "start": 12996.64, "end": 12998.64, "text": " this is how we create our four layers", "tokens": [51364, 341, 307, 577, 321, 1884, 527, 1451, 7914, 51464], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3650, "seek": 1297664, "start": 12998.64, "end": 13000.64, "text": " our four decoder layers", "tokens": [51464, 527, 1451, 979, 19866, 7914, 51564], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3651, "seek": 1297664, "start": 13000.64, "end": 13002.64, "text": " that's all you need to know for now", "tokens": [51564, 300, 311, 439, 291, 643, 281, 458, 337, 586, 51664], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3652, "seek": 1297664, "start": 13002.64, "end": 13004.64, "text": " I'm going to move more into this block later", "tokens": [51664, 286, 478, 516, 281, 1286, 544, 666, 341, 3461, 1780, 51764], "temperature": 0.0, "avg_logprob": -0.04390974331619148, "compression_ratio": 1.9462809917355373, "no_speech_prob": 0.0066903624683618546}, {"id": 3653, "seek": 1300464, "start": 13004.64, "end": 13006.64, "text": " as for this", "tokens": [50364, 382, 337, 341, 50464], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3654, "seek": 1300464, "start": 13006.64, "end": 13008.64, "text": " self dot layer norm final", "tokens": [50464, 2698, 5893, 4583, 2026, 2572, 50564], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3655, "seek": 1300464, "start": 13008.64, "end": 13010.64, "text": " this is the final layer norm", "tokens": [50564, 341, 307, 264, 2572, 4583, 2026, 50664], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3656, "seek": 1300464, "start": 13010.64, "end": 13012.64, "text": " all this is going to do", "tokens": [50664, 439, 341, 307, 516, 281, 360, 50764], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3657, "seek": 1300464, "start": 13012.64, "end": 13014.64, "text": " is we're just simply going to add this", "tokens": [50764, 307, 321, 434, 445, 2935, 516, 281, 909, 341, 50864], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3658, "seek": 1300464, "start": 13014.64, "end": 13016.64, "text": " to the end of our network here", "tokens": [50864, 281, 264, 917, 295, 527, 3209, 510, 50964], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3659, "seek": 1300464, "start": 13020.64, "end": 13022.64, "text": " just simply at the end here", "tokens": [51164, 445, 2935, 412, 264, 917, 510, 51264], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3660, "seek": 1300464, "start": 13022.64, "end": 13024.64, "text": " and all this is going to do", "tokens": [51264, 293, 439, 341, 307, 516, 281, 360, 51364], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3661, "seek": 1300464, "start": 13024.64, "end": 13026.64, "text": " is just going to help the model converge better", "tokens": [51364, 307, 445, 516, 281, 854, 264, 2316, 41881, 1101, 51464], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3662, "seek": 1300464, "start": 13026.64, "end": 13028.64, "text": " layer norms are super useful", "tokens": [51464, 4583, 24357, 366, 1687, 4420, 51564], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3663, "seek": 1300464, "start": 13028.64, "end": 13030.64, "text": " and yeah", "tokens": [51564, 293, 1338, 51664], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3664, "seek": 1300464, "start": 13030.64, "end": 13032.64, "text": " so you'll see more how that works", "tokens": [51664, 370, 291, 603, 536, 544, 577, 300, 1985, 51764], "temperature": 0.0, "avg_logprob": -0.10006193681196733, "compression_ratio": 1.9421965317919074, "no_speech_prob": 0.0029803807847201824}, {"id": 3665, "seek": 1303264, "start": 13032.64, "end": 13034.64, "text": " I'll actually remove it later on", "tokens": [50364, 286, 603, 767, 4159, 309, 1780, 322, 50464], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3666, "seek": 1303264, "start": 13034.64, "end": 13036.64, "text": " and we'll actually", "tokens": [50464, 293, 321, 603, 767, 50564], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3667, "seek": 1303264, "start": 13036.64, "end": 13038.64, "text": " compare and see", "tokens": [50564, 6794, 293, 536, 50664], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3668, "seek": 1303264, "start": 13038.64, "end": 13040.64, "text": " how good it actually does", "tokens": [50664, 577, 665, 309, 767, 775, 50764], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3669, "seek": 1303264, "start": 13040.64, "end": 13042.64, "text": " and you can totally go out of your way", "tokens": [50764, 293, 291, 393, 3879, 352, 484, 295, 428, 636, 50864], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3670, "seek": 1303264, "start": 13042.64, "end": 13044.64, "text": " to experiment", "tokens": [50864, 281, 5120, 50964], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3671, "seek": 1303264, "start": 13044.64, "end": 13046.64, "text": " with different normalizations", "tokens": [50964, 365, 819, 2710, 14455, 51064], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3672, "seek": 1303264, "start": 13046.64, "end": 13048.64, "text": " and see how well the layer norm", "tokens": [51064, 293, 536, 577, 731, 264, 4583, 2026, 51164], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3673, "seek": 1303264, "start": 13048.64, "end": 13050.64, "text": " helps the model perform", "tokens": [51164, 3665, 264, 2316, 2042, 51264], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3674, "seek": 1303264, "start": 13050.64, "end": 13052.64, "text": " or how well the loss", "tokens": [51264, 420, 577, 731, 264, 4470, 51364], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3675, "seek": 1303264, "start": 13052.64, "end": 13054.64, "text": " sort of converges over time", "tokens": [51364, 1333, 295, 9652, 2880, 670, 565, 51464], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3676, "seek": 1303264, "start": 13054.64, "end": 13056.64, "text": " when you put the layer norm in different places", "tokens": [51464, 562, 291, 829, 264, 4583, 2026, 294, 819, 3190, 51564], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3677, "seek": 1303264, "start": 13056.64, "end": 13058.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3678, "seek": 1303264, "start": 13058.64, "end": 13060.64, "text": " let's go back here", "tokens": [51664, 718, 311, 352, 646, 510, 51764], "temperature": 0.0, "avg_logprob": -0.04437343672950669, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.005218230653554201}, {"id": 3679, "seek": 1306064, "start": 13060.64, "end": 13062.64, "text": " and now we have this", "tokens": [50364, 293, 586, 321, 362, 341, 50464], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3680, "seek": 1306064, "start": 13062.64, "end": 13064.64, "text": " end here", "tokens": [50464, 917, 510, 50564], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3681, "seek": 1306064, "start": 13064.64, "end": 13066.64, "text": " which is the language", "tokens": [50564, 597, 307, 264, 2856, 50664], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3682, "seek": 1306064, "start": 13066.64, "end": 13068.64, "text": " I believe this is the language modeling", "tokens": [50664, 286, 1697, 341, 307, 264, 2856, 15983, 50764], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3683, "seek": 1306064, "start": 13068.64, "end": 13070.64, "text": " head or something", "tokens": [50764, 1378, 420, 746, 50864], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3684, "seek": 1306064, "start": 13070.64, "end": 13072.64, "text": " again this is what Andrey Karpathy used", "tokens": [50864, 797, 341, 307, 437, 400, 7950, 591, 6529, 9527, 1143, 50964], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3685, "seek": 1306064, "start": 13072.64, "end": 13074.64, "text": " I'm assuming that means language modeling head", "tokens": [50964, 286, 478, 11926, 300, 1355, 2856, 15983, 1378, 51064], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3686, "seek": 1306064, "start": 13074.64, "end": 13076.64, "text": " but pretty much", "tokens": [51064, 457, 1238, 709, 51164], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3687, "seek": 1306064, "start": 13076.64, "end": 13078.64, "text": " all we're doing is we're just", "tokens": [51164, 439, 321, 434, 884, 307, 321, 434, 445, 51264], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3688, "seek": 1306064, "start": 13078.64, "end": 13080.64, "text": " projecting", "tokens": [51264, 43001, 51364], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3689, "seek": 1306064, "start": 13080.64, "end": 13082.64, "text": " we're doing this final", "tokens": [51364, 321, 434, 884, 341, 2572, 51464], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3690, "seek": 1306064, "start": 13082.64, "end": 13084.64, "text": " transformation here", "tokens": [51464, 9887, 510, 51564], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3691, "seek": 1306064, "start": 13084.64, "end": 13086.64, "text": " this final little linear layer here", "tokens": [51564, 341, 2572, 707, 8213, 4583, 510, 51664], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3692, "seek": 1306064, "start": 13086.64, "end": 13088.64, "text": " from all of these sequential", "tokens": [51664, 490, 439, 295, 613, 42881, 51764], "temperature": 0.0, "avg_logprob": -0.1041649134472163, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.0024719408247619867}, {"id": 3693, "seek": 1308864, "start": 13088.64, "end": 13090.64, "text": " decoder outputs", "tokens": [50364, 979, 19866, 23930, 50464], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3694, "seek": 1308864, "start": 13090.64, "end": 13092.64, "text": " and we're just going to transform that", "tokens": [50464, 293, 321, 434, 445, 516, 281, 4088, 300, 50564], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3695, "seek": 1308864, "start": 13092.64, "end": 13094.64, "text": " to", "tokens": [50564, 281, 50664], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3696, "seek": 1308864, "start": 13094.64, "end": 13096.64, "text": " something that the softmax can work with", "tokens": [50664, 746, 300, 264, 2787, 41167, 393, 589, 365, 50764], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3697, "seek": 1308864, "start": 13096.64, "end": 13098.64, "text": " so we have our layer norm afterwards", "tokens": [50764, 370, 321, 362, 527, 4583, 2026, 10543, 50864], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3698, "seek": 1308864, "start": 13098.64, "end": 13100.64, "text": " to sort of normalize help the model converge", "tokens": [50864, 281, 1333, 295, 2710, 1125, 854, 264, 2316, 41881, 50964], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3699, "seek": 1308864, "start": 13100.64, "end": 13102.64, "text": " after all these", "tokens": [50964, 934, 439, 613, 51064], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3700, "seek": 1308864, "start": 13102.64, "end": 13104.64, "text": " after all this computation", "tokens": [51064, 934, 439, 341, 24903, 51164], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3701, "seek": 1308864, "start": 13104.64, "end": 13106.64, "text": " we're going to feed that into a linear layer", "tokens": [51164, 321, 434, 516, 281, 3154, 300, 666, 257, 8213, 4583, 51264], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3702, "seek": 1308864, "start": 13106.64, "end": 13108.64, "text": " to make it I guess", "tokens": [51264, 281, 652, 309, 286, 2041, 51364], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3703, "seek": 1308864, "start": 13108.64, "end": 13110.64, "text": " softmax", "tokens": [51364, 2787, 41167, 51464], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3704, "seek": 1308864, "start": 13110.64, "end": 13112.64, "text": " workable so the softmax can work with it", "tokens": [51464, 589, 712, 370, 264, 2787, 41167, 393, 589, 365, 309, 51564], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3705, "seek": 1308864, "start": 13112.64, "end": 13114.64, "text": " and", "tokens": [51564, 293, 51664], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3706, "seek": 1308864, "start": 13114.64, "end": 13116.64, "text": " yeah so we're just", "tokens": [51664, 1338, 370, 321, 434, 445, 51764], "temperature": 0.0, "avg_logprob": -0.08280968216230285, "compression_ratio": 1.8316326530612246, "no_speech_prob": 0.0038231106009334326}, {"id": 3707, "seek": 1311664, "start": 13116.64, "end": 13118.64, "text": " simply projecting it from", "tokens": [50364, 2935, 43001, 309, 490, 50464], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3708, "seek": 1311664, "start": 13118.64, "end": 13120.64, "text": " an embed which is the vector length that we get", "tokens": [50464, 364, 12240, 597, 307, 264, 8062, 4641, 300, 321, 483, 50564], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3709, "seek": 1311664, "start": 13120.64, "end": 13122.64, "text": " from our decoder", "tokens": [50564, 490, 527, 979, 19866, 50664], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3710, "seek": 1311664, "start": 13122.64, "end": 13124.64, "text": " and", "tokens": [50664, 293, 50764], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3711, "seek": 1311664, "start": 13126.64, "end": 13128.64, "text": " and this vocab size", "tokens": [50864, 293, 341, 2329, 455, 2744, 50964], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3712, "seek": 1311664, "start": 13128.64, "end": 13130.64, "text": " so the vocab size is going to", "tokens": [50964, 370, 264, 2329, 455, 2744, 307, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3713, "seek": 1311664, "start": 13130.64, "end": 13132.64, "text": " essentially give up a little", "tokens": [51064, 4476, 976, 493, 257, 707, 51164], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3714, "seek": 1311664, "start": 13132.64, "end": 13134.64, "text": " probability distribution on each token that we have", "tokens": [51164, 8482, 7316, 322, 1184, 14862, 300, 321, 362, 51264], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3715, "seek": 1311664, "start": 13134.64, "end": 13136.64, "text": " or the vocabulary", "tokens": [51264, 420, 264, 19864, 51364], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3716, "seek": 1311664, "start": 13136.64, "end": 13138.64, "text": " so anyways", "tokens": [51364, 370, 13448, 51464], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3717, "seek": 1311664, "start": 13138.64, "end": 13140.64, "text": " I'm going to make this back to normal", "tokens": [51464, 286, 478, 516, 281, 652, 341, 646, 281, 2710, 51564], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3718, "seek": 1311664, "start": 13140.64, "end": 13142.64, "text": " here and we're going to just", "tokens": [51564, 510, 293, 321, 434, 516, 281, 445, 51664], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3719, "seek": 1311664, "start": 13142.64, "end": 13144.64, "text": " apply this", "tokens": [51664, 3079, 341, 51764], "temperature": 0.0, "avg_logprob": -0.07518844803174336, "compression_ratio": 1.66, "no_speech_prob": 0.00400589918717742}, {"id": 3720, "seek": 1314464, "start": 13144.64, "end": 13146.64, "text": " to the forward pass", "tokens": [50364, 281, 264, 2128, 1320, 50464], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3721, "seek": 1314464, "start": 13146.64, "end": 13148.64, "text": " so a little thing I wanted to add on", "tokens": [50464, 370, 257, 707, 551, 286, 1415, 281, 909, 322, 50564], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3722, "seek": 1314464, "start": 13148.64, "end": 13150.64, "text": " to", "tokens": [50564, 281, 50664], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3723, "seek": 1314464, "start": 13150.64, "end": 13152.64, "text": " this positional embedding", "tokens": [50664, 341, 2535, 304, 12240, 3584, 50764], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3724, "seek": 1314464, "start": 13152.64, "end": 13154.64, "text": " or rather just the idea of", "tokens": [50764, 420, 2831, 445, 264, 1558, 295, 50864], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3725, "seek": 1314464, "start": 13154.64, "end": 13156.64, "text": " embeddings versus", "tokens": [50864, 12240, 29432, 5717, 50964], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3726, "seek": 1314464, "start": 13156.64, "end": 13158.64, "text": " the fixed definite function", "tokens": [50964, 264, 6806, 25131, 2445, 51064], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3727, "seek": 1314464, "start": 13158.64, "end": 13160.64, "text": " of the", "tokens": [51064, 295, 264, 51164], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3728, "seek": 1314464, "start": 13160.64, "end": 13162.64, "text": " sinusoidal functions", "tokens": [51164, 41503, 17079, 304, 6828, 51264], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3729, "seek": 1314464, "start": 13162.64, "end": 13164.64, "text": " and the cosine functions that we used here", "tokens": [51264, 293, 264, 23565, 6828, 300, 321, 1143, 510, 51364], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3730, "seek": 1314464, "start": 13164.64, "end": 13166.64, "text": " these are both actually", "tokens": [51364, 613, 366, 1293, 767, 51464], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3731, "seek": 1314464, "start": 13166.64, "end": 13168.64, "text": " used in practice", "tokens": [51464, 1143, 294, 3124, 51564], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3732, "seek": 1314464, "start": 13168.64, "end": 13170.64, "text": " the reason I said we're going to use embeddings", "tokens": [51564, 264, 1778, 286, 848, 321, 434, 516, 281, 764, 12240, 29432, 51664], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3733, "seek": 1314464, "start": 13170.64, "end": 13172.64, "text": " is because we just want it to be more oriented", "tokens": [51664, 307, 570, 321, 445, 528, 309, 281, 312, 544, 21841, 51764], "temperature": 0.0, "avg_logprob": -0.07114011507767898, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.009264465421438217}, {"id": 3734, "seek": 1317264, "start": 13172.64, "end": 13174.64, "text": " around our data", "tokens": [50364, 926, 527, 1412, 50464], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3735, "seek": 1317264, "start": 13174.64, "end": 13176.64, "text": " however in practice", "tokens": [50464, 4461, 294, 3124, 50564], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3736, "seek": 1317264, "start": 13176.64, "end": 13178.64, "text": " sinusoidal encodings are used", "tokens": [50564, 41503, 17079, 304, 2058, 378, 1109, 366, 1143, 50664], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3737, "seek": 1317264, "start": 13178.64, "end": 13180.64, "text": " in base transformer models", "tokens": [50664, 294, 3096, 31782, 5245, 50764], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3738, "seek": 1317264, "start": 13180.64, "end": 13182.64, "text": " whereas learned embeddings what we're using", "tokens": [50764, 9735, 3264, 12240, 29432, 437, 321, 434, 1228, 50864], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3739, "seek": 1317264, "start": 13182.64, "end": 13184.64, "text": " are used in variants like", "tokens": [50864, 366, 1143, 294, 21669, 411, 50964], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3740, "seek": 1317264, "start": 13184.64, "end": 13186.64, "text": " GBT and we are building a", "tokens": [50964, 26809, 51, 293, 321, 366, 2390, 257, 51064], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3741, "seek": 1317264, "start": 13186.64, "end": 13188.64, "text": " GBT so we're probably", "tokens": [51064, 26809, 51, 370, 321, 434, 1391, 51164], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3742, "seek": 1317264, "start": 13188.64, "end": 13190.64, "text": " going to find out a performance from learning about embeddings", "tokens": [51164, 516, 281, 915, 484, 257, 3389, 490, 2539, 466, 12240, 29432, 51264], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3743, "seek": 1317264, "start": 13190.64, "end": 13192.64, "text": " and this is just", "tokens": [51264, 293, 341, 307, 445, 51364], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3744, "seek": 1317264, "start": 13192.64, "end": 13194.64, "text": " summing up the experts do", "tokens": [51364, 2408, 2810, 493, 264, 8572, 360, 51464], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3745, "seek": 1317264, "start": 13194.64, "end": 13196.64, "text": " it's a little practice that experts do", "tokens": [51464, 309, 311, 257, 707, 3124, 300, 8572, 360, 51564], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3746, "seek": 1317264, "start": 13196.64, "end": 13198.64, "text": " when they're building transformer models", "tokens": [51564, 562, 436, 434, 2390, 31782, 5245, 51664], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3747, "seek": 1317264, "start": 13198.64, "end": 13200.64, "text": " versus variants like GBTs", "tokens": [51664, 5717, 21669, 411, 26809, 33424, 51764], "temperature": 0.0, "avg_logprob": -0.1125245679888809, "compression_ratio": 1.8508771929824561, "no_speech_prob": 0.004680254962295294}, {"id": 3748, "seek": 1320064, "start": 13200.64, "end": 13202.64, "text": " so that's just a little background on", "tokens": [50364, 370, 300, 311, 445, 257, 707, 3678, 322, 50464], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3749, "seek": 1320064, "start": 13202.64, "end": 13204.64, "text": " why we're using", "tokens": [50464, 983, 321, 434, 1228, 50564], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3750, "seek": 1320064, "start": 13204.64, "end": 13206.64, "text": " learnable embeddings", "tokens": [50564, 1466, 712, 12240, 29432, 50664], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3751, "seek": 1320064, "start": 13206.64, "end": 13208.64, "text": " so now let's continue", "tokens": [50664, 370, 586, 718, 311, 2354, 50764], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3752, "seek": 1320064, "start": 13208.64, "end": 13210.64, "text": " with the forward pass here", "tokens": [50764, 365, 264, 2128, 1320, 510, 50864], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3753, "seek": 1320064, "start": 13210.64, "end": 13212.64, "text": " so I'm going to paste in some more code", "tokens": [50864, 370, 286, 478, 516, 281, 9163, 294, 512, 544, 3089, 50964], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3754, "seek": 1320064, "start": 13212.64, "end": 13214.64, "text": " and", "tokens": [50964, 293, 51064], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3755, "seek": 1320064, "start": 13214.64, "end": 13216.64, "text": " let me just make sure this is", "tokens": [51064, 718, 385, 445, 652, 988, 341, 307, 51164], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3756, "seek": 1320064, "start": 13216.64, "end": 13218.64, "text": " formatted properly cool", "tokens": [51164, 1254, 32509, 6108, 1627, 51264], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3757, "seek": 1320064, "start": 13220.64, "end": 13222.64, "text": " so we have this", "tokens": [51364, 370, 321, 362, 341, 51464], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3758, "seek": 1320064, "start": 13222.64, "end": 13224.64, "text": " token embedding which is our token embedding", "tokens": [51464, 14862, 12240, 3584, 597, 307, 527, 14862, 12240, 3584, 51564], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3759, "seek": 1320064, "start": 13224.64, "end": 13226.64, "text": " table", "tokens": [51564, 3199, 51664], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3760, "seek": 1320064, "start": 13226.64, "end": 13228.64, "text": " we take an IDX", "tokens": [51664, 321, 747, 364, 7348, 55, 51764], "temperature": 0.0, "avg_logprob": -0.09435765115838302, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0003199780185241252}, {"id": 3761, "seek": 1322864, "start": 13228.64, "end": 13230.64, "text": " token embedding here", "tokens": [50364, 14862, 12240, 3584, 510, 50464], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3762, "seek": 1322864, "start": 13230.64, "end": 13232.64, "text": " then what we do with this positional embedding table", "tokens": [50464, 550, 437, 321, 360, 365, 341, 2535, 304, 12240, 3584, 3199, 50564], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3763, "seek": 1322864, "start": 13232.64, "end": 13234.64, "text": " so we have this torch.arrange", "tokens": [50564, 370, 321, 362, 341, 27822, 13, 2284, 933, 50664], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3764, "seek": 1322864, "start": 13234.64, "end": 13236.64, "text": " we make sure this is on the CUDA device", "tokens": [50664, 321, 652, 988, 341, 307, 322, 264, 29777, 7509, 4302, 50764], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3765, "seek": 1322864, "start": 13236.64, "end": 13238.64, "text": " the GPU device", "tokens": [50764, 264, 18407, 4302, 50864], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3766, "seek": 1322864, "start": 13238.64, "end": 13240.64, "text": " so it's in parallel", "tokens": [50864, 370, 309, 311, 294, 8952, 50964], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3767, "seek": 1322864, "start": 13240.64, "end": 13242.64, "text": " and all this is going to do", "tokens": [50964, 293, 439, 341, 307, 516, 281, 360, 51064], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3768, "seek": 1322864, "start": 13242.64, "end": 13244.64, "text": " is it's going to look at how long is T", "tokens": [51064, 307, 309, 311, 516, 281, 574, 412, 577, 938, 307, 314, 51164], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3769, "seek": 1322864, "start": 13244.64, "end": 13246.64, "text": " and", "tokens": [51164, 293, 51264], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3770, "seek": 1322864, "start": 13246.64, "end": 13248.64, "text": " let's say T is our block size", "tokens": [51264, 718, 311, 584, 314, 307, 527, 3461, 2744, 51364], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3771, "seek": 1322864, "start": 13248.64, "end": 13250.64, "text": " so T is going to be 8", "tokens": [51364, 370, 314, 307, 516, 281, 312, 1649, 51464], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3772, "seek": 1322864, "start": 13250.64, "end": 13252.64, "text": " so all it's going to do is give us 8 indices", "tokens": [51464, 370, 439, 309, 311, 516, 281, 360, 307, 976, 505, 1649, 43840, 51564], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3773, "seek": 1322864, "start": 13252.64, "end": 13254.64, "text": " it's going to be like 0, 1, 2, 3,", "tokens": [51564, 309, 311, 516, 281, 312, 411, 1958, 11, 502, 11, 568, 11, 805, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3774, "seek": 1322864, "start": 13254.64, "end": 13256.64, "text": " 4, 5, 6, 7", "tokens": [51664, 1017, 11, 1025, 11, 1386, 11, 1614, 51764], "temperature": 0.0, "avg_logprob": -0.10990624496902245, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.003883825382217765}, {"id": 3775, "seek": 1325664, "start": 13256.64, "end": 13258.64, "text": " 8 of those", "tokens": [50364, 1649, 295, 729, 50464], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3776, "seek": 1325664, "start": 13258.64, "end": 13260.64, "text": " and we're essentially just going to give each of those", "tokens": [50464, 293, 321, 434, 4476, 445, 516, 281, 976, 1184, 295, 729, 50564], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3777, "seek": 1325664, "start": 13260.64, "end": 13262.64, "text": " each of those indices", "tokens": [50564, 1184, 295, 729, 43840, 50664], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3778, "seek": 1325664, "start": 13262.64, "end": 13264.64, "text": " a different", "tokens": [50664, 257, 819, 50764], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3779, "seek": 1325664, "start": 13266.64, "end": 13268.64, "text": " a different", "tokens": [50864, 257, 819, 50964], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3780, "seek": 1325664, "start": 13268.64, "end": 13270.64, "text": " end embedding vector", "tokens": [50964, 917, 12240, 3584, 8062, 51064], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3781, "seek": 1325664, "start": 13270.64, "end": 13272.64, "text": " for each of those indices", "tokens": [51064, 337, 1184, 295, 729, 43840, 51164], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3782, "seek": 1325664, "start": 13272.64, "end": 13274.64, "text": " just a little lookup table", "tokens": [51164, 445, 257, 707, 574, 1010, 3199, 51264], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3783, "seek": 1325664, "start": 13274.64, "end": 13276.64, "text": " and that's what that is", "tokens": [51264, 293, 300, 311, 437, 300, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3784, "seek": 1325664, "start": 13276.64, "end": 13278.64, "text": " so all we do now", "tokens": [51364, 370, 439, 321, 360, 586, 51464], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3785, "seek": 1325664, "start": 13278.64, "end": 13280.64, "text": " is it's actually quite simple", "tokens": [51464, 307, 309, 311, 767, 1596, 2199, 51564], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3786, "seek": 1325664, "start": 13280.64, "end": 13282.64, "text": " and this is a very efficient way to do it", "tokens": [51564, 293, 341, 307, 257, 588, 7148, 636, 281, 360, 309, 51664], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3787, "seek": 1325664, "start": 13282.64, "end": 13284.64, "text": " is you just add these two together", "tokens": [51664, 307, 291, 445, 909, 613, 732, 1214, 51764], "temperature": 0.0, "avg_logprob": -0.08900377485487196, "compression_ratio": 1.7807486631016043, "no_speech_prob": 0.009264551103115082}, {"id": 3788, "seek": 1328464, "start": 13284.64, "end": 13286.64, "text": " broadcasting rules", "tokens": [50364, 30024, 4474, 50464], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3789, "seek": 1328464, "start": 13286.64, "end": 13288.64, "text": " which you might want to look into", "tokens": [50464, 597, 291, 1062, 528, 281, 574, 666, 50564], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3790, "seek": 1328464, "start": 13288.64, "end": 13290.64, "text": " I'll actually search that up right now", "tokens": [50564, 286, 603, 767, 3164, 300, 493, 558, 586, 50664], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3791, "seek": 1328464, "start": 13290.64, "end": 13292.64, "text": " torch", "tokens": [50664, 27822, 50764], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3792, "seek": 1328464, "start": 13292.64, "end": 13294.64, "text": " broadcasting semantics", "tokens": [50764, 30024, 4361, 45298, 50864], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3793, "seek": 1328464, "start": 13296.64, "end": 13298.64, "text": " pie torch", "tokens": [50964, 1730, 27822, 51064], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3794, "seek": 1328464, "start": 13298.64, "end": 13300.64, "text": " broadcasting", "tokens": [51064, 30024, 51164], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3795, "seek": 1328464, "start": 13300.64, "end": 13302.64, "text": " I cannot spell", "tokens": [51164, 286, 2644, 9827, 51264], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3796, "seek": 1328464, "start": 13302.64, "end": 13304.64, "text": " broadcasting semantics", "tokens": [51264, 30024, 4361, 45298, 51364], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3797, "seek": 1328464, "start": 13304.64, "end": 13306.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3798, "seek": 1328464, "start": 13306.64, "end": 13308.64, "text": " these are a little bit funky", "tokens": [51464, 613, 366, 257, 707, 857, 33499, 51564], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3799, "seek": 1328464, "start": 13308.64, "end": 13310.64, "text": " when you look at them the first time", "tokens": [51564, 562, 291, 574, 412, 552, 264, 700, 565, 51664], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3800, "seek": 1328464, "start": 13310.64, "end": 13312.64, "text": " but pretty much these are just rules", "tokens": [51664, 457, 1238, 709, 613, 366, 445, 4474, 51764], "temperature": 0.0, "avg_logprob": -0.11494977474212646, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.030194180086255074}, {"id": 3801, "seek": 1331264, "start": 13312.64, "end": 13314.64, "text": " about how you can do", "tokens": [50364, 466, 577, 291, 393, 360, 50464], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3802, "seek": 1331264, "start": 13314.64, "end": 13316.64, "text": " arithmetic operations", "tokens": [50464, 42973, 7705, 50564], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3803, "seek": 1331264, "start": 13316.64, "end": 13318.64, "text": " and just operations in general", "tokens": [50564, 293, 445, 7705, 294, 2674, 50664], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3804, "seek": 1331264, "start": 13318.64, "end": 13320.64, "text": " to tensors", "tokens": [50664, 281, 10688, 830, 50764], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3805, "seek": 1331264, "start": 13320.64, "end": 13322.64, "text": " so tensors are like you think of matrices", "tokens": [50764, 370, 10688, 830, 366, 411, 291, 519, 295, 32284, 50864], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3806, "seek": 1331264, "start": 13322.64, "end": 13324.64, "text": " where it's like a 2x2", "tokens": [50864, 689, 309, 311, 411, 257, 568, 87, 17, 50964], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3807, "seek": 1331264, "start": 13324.64, "end": 13326.64, "text": " tensors can be the same thing", "tokens": [50964, 10688, 830, 393, 312, 264, 912, 551, 51064], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3808, "seek": 1331264, "start": 13326.64, "end": 13328.64, "text": " but they could be like a 2x2x2", "tokens": [51064, 457, 436, 727, 312, 411, 257, 568, 87, 17, 87, 17, 51164], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3809, "seek": 1331264, "start": 13328.64, "end": 13330.64, "text": " or a 2x2x2x2x2", "tokens": [51164, 420, 257, 568, 87, 17, 87, 17, 87, 17, 87, 17, 51264], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3810, "seek": 1331264, "start": 13330.64, "end": 13332.64, "text": " whatever dimension you want to have", "tokens": [51264, 2035, 10139, 291, 528, 281, 362, 51364], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3811, "seek": 1331264, "start": 13332.64, "end": 13334.64, "text": " there", "tokens": [51364, 456, 51464], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3812, "seek": 1331264, "start": 13334.64, "end": 13336.64, "text": " and pretty much it's just rules about how you can", "tokens": [51464, 293, 1238, 709, 309, 311, 445, 4474, 466, 577, 291, 393, 51564], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3813, "seek": 1331264, "start": 13338.64, "end": 13340.64, "text": " have two of those", "tokens": [51664, 362, 732, 295, 729, 51764], "temperature": 0.0, "avg_logprob": -0.07611340015858144, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.008572445251047611}, {"id": 3814, "seek": 1334064, "start": 13340.64, "end": 13342.64, "text": " weirdly", "tokens": [50364, 48931, 50464], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3815, "seek": 1334064, "start": 13342.64, "end": 13344.64, "text": " shaped tensors and do things", "tokens": [50464, 13475, 10688, 830, 293, 360, 721, 50564], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3816, "seek": 1334064, "start": 13344.64, "end": 13346.64, "text": " to them", "tokens": [50564, 281, 552, 50664], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3817, "seek": 1334064, "start": 13346.64, "end": 13348.64, "text": " so just some rules here", "tokens": [50664, 370, 445, 512, 4474, 510, 50764], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3818, "seek": 1334064, "start": 13348.64, "end": 13350.64, "text": " I would advise you familiarize yourself with these", "tokens": [50764, 286, 576, 18312, 291, 4963, 1125, 1803, 365, 613, 50864], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3819, "seek": 1334064, "start": 13350.64, "end": 13352.64, "text": " even play around with it if you want", "tokens": [50864, 754, 862, 926, 365, 309, 498, 291, 528, 50964], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3820, "seek": 1334064, "start": 13352.64, "end": 13354.64, "text": " just for a few minutes", "tokens": [50964, 445, 337, 257, 1326, 2077, 51064], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3821, "seek": 1334064, "start": 13354.64, "end": 13356.64, "text": " and just get an idea for", "tokens": [51064, 293, 445, 483, 364, 1558, 337, 51164], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3822, "seek": 1334064, "start": 13356.64, "end": 13358.64, "text": " which, like just try to multiply", "tokens": [51164, 597, 11, 411, 445, 853, 281, 12972, 51264], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3823, "seek": 1334064, "start": 13358.64, "end": 13360.64, "text": " tensors together", "tokens": [51264, 10688, 830, 1214, 51364], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3824, "seek": 1334064, "start": 13360.64, "end": 13362.64, "text": " and see which ones throw errors and which ones don't", "tokens": [51364, 293, 536, 597, 2306, 3507, 13603, 293, 597, 2306, 500, 380, 51464], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3825, "seek": 1334064, "start": 13362.64, "end": 13364.64, "text": " so it's a good idea to understand how broadcasting", "tokens": [51464, 370, 309, 311, 257, 665, 1558, 281, 1223, 577, 30024, 51564], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3826, "seek": 1334064, "start": 13364.64, "end": 13366.64, "text": " rules work", "tokens": [51564, 4474, 589, 51664], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3827, "seek": 1334064, "start": 13366.64, "end": 13368.64, "text": " obviously this term", "tokens": [51664, 2745, 341, 1433, 51764], "temperature": 0.0, "avg_logprob": -0.071457893760116, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.008058443665504456}, {"id": 3828, "seek": 1336864, "start": 13368.64, "end": 13370.64, "text": " is a little fancy and it's like", "tokens": [50364, 307, 257, 707, 10247, 293, 309, 311, 411, 50464], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3829, "seek": 1336864, "start": 13370.64, "end": 13372.64, "text": " that's like a crazy advanced term", "tokens": [50464, 300, 311, 411, 257, 3219, 7339, 1433, 50564], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3830, "seek": 1336864, "start": 13372.64, "end": 13374.64, "text": " not really", "tokens": [50564, 406, 534, 50664], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3831, "seek": 1336864, "start": 13374.64, "end": 13376.64, "text": " it's pretty much just", "tokens": [50664, 309, 311, 1238, 709, 445, 50764], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3832, "seek": 1336864, "start": 13376.64, "end": 13378.64, "text": " some rules about how you're", "tokens": [50764, 512, 4474, 466, 577, 291, 434, 50864], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3833, "seek": 1336864, "start": 13378.64, "end": 13380.64, "text": " multiplying these really weirdly shaped tensors", "tokens": [50864, 30955, 613, 534, 48931, 13475, 10688, 830, 50964], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3834, "seek": 1336864, "start": 13380.64, "end": 13382.64, "text": " so yeah", "tokens": [50964, 370, 1338, 51064], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3835, "seek": 1336864, "start": 13382.64, "end": 13384.64, "text": " anyways", "tokens": [51064, 13448, 51164], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3836, "seek": 1336864, "start": 13384.64, "end": 13386.64, "text": " if we go back to here", "tokens": [51164, 498, 321, 352, 646, 281, 510, 51264], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3837, "seek": 1336864, "start": 13388.64, "end": 13390.64, "text": " we are allowed to broadcast these", "tokens": [51364, 321, 366, 4350, 281, 9975, 613, 51464], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3838, "seek": 1336864, "start": 13390.64, "end": 13392.64, "text": " we're allowed to actually add them together", "tokens": [51464, 321, 434, 4350, 281, 767, 909, 552, 1214, 51564], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3839, "seek": 1336864, "start": 13392.64, "end": 13394.64, "text": " so the positional embedding and the token embedding", "tokens": [51564, 370, 264, 2535, 304, 12240, 3584, 293, 264, 14862, 12240, 3584, 51664], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3840, "seek": 1336864, "start": 13394.64, "end": 13396.64, "text": " we get X from this", "tokens": [51664, 321, 483, 1783, 490, 341, 51764], "temperature": 0.0, "avg_logprob": -0.10219671211990655, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.0012447508051991463}, {"id": 3841, "seek": 1339664, "start": 13396.64, "end": 13398.64, "text": " B by T by C shape", "tokens": [50364, 363, 538, 314, 538, 383, 3909, 50464], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3842, "seek": 1339664, "start": 13398.64, "end": 13400.64, "text": " so now", "tokens": [50464, 370, 586, 50564], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3843, "seek": 1339664, "start": 13400.64, "end": 13402.64, "text": " what we can do", "tokens": [50564, 437, 321, 393, 360, 50664], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3844, "seek": 1339664, "start": 13402.64, "end": 13404.64, "text": " with these is we can actually feed it", "tokens": [50664, 365, 613, 307, 321, 393, 767, 3154, 309, 50764], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3845, "seek": 1339664, "start": 13404.64, "end": 13406.64, "text": " into the", "tokens": [50764, 666, 264, 50864], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3846, "seek": 1339664, "start": 13406.64, "end": 13408.64, "text": " GPT or I guess", "tokens": [50864, 26039, 51, 420, 286, 2041, 50964], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3847, "seek": 1339664, "start": 13408.64, "end": 13410.64, "text": " sort of a transformer network if you want to say that", "tokens": [50964, 1333, 295, 257, 31782, 3209, 498, 291, 528, 281, 584, 300, 51064], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3848, "seek": 1339664, "start": 13410.64, "end": 13412.64, "text": " so we have these embeddings", "tokens": [51064, 370, 321, 362, 613, 12240, 29432, 51164], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3849, "seek": 1339664, "start": 13412.64, "end": 13414.64, "text": " and positional encodings", "tokens": [51164, 293, 2535, 304, 2058, 378, 1109, 51264], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3850, "seek": 1339664, "start": 13414.64, "end": 13416.64, "text": " we add these together and then we feed them", "tokens": [51264, 321, 909, 613, 1214, 293, 550, 321, 3154, 552, 51364], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3851, "seek": 1339664, "start": 13416.64, "end": 13418.64, "text": " into our sequential network", "tokens": [51364, 666, 527, 42881, 3209, 51464], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3852, "seek": 1339664, "start": 13418.64, "end": 13420.64, "text": " so how are we doing this", "tokens": [51464, 370, 577, 366, 321, 884, 341, 51564], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3853, "seek": 1339664, "start": 13420.64, "end": 13422.64, "text": " well we go self dot blocks which is up here", "tokens": [51564, 731, 321, 352, 2698, 5893, 8474, 597, 307, 493, 510, 51664], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3854, "seek": 1339664, "start": 13422.64, "end": 13424.64, "text": " and we essentially just feed", "tokens": [51664, 293, 321, 4476, 445, 3154, 51764], "temperature": 0.0, "avg_logprob": -0.0854468680264657, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0034827578347176313}, {"id": 3855, "seek": 1342464, "start": 13424.64, "end": 13426.64, "text": " an X which is literally", "tokens": [50364, 364, 1783, 597, 307, 3736, 50464], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3856, "seek": 1342464, "start": 13426.64, "end": 13428.64, "text": " exactly what happens here", "tokens": [50464, 2293, 437, 2314, 510, 50564], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3857, "seek": 1342464, "start": 13428.64, "end": 13430.64, "text": " we have our tokenized inputs", "tokens": [50564, 321, 362, 527, 14862, 1602, 15743, 50664], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3858, "seek": 1342464, "start": 13430.64, "end": 13432.64, "text": " we got our embeddings and our positional encodings", "tokens": [50664, 321, 658, 527, 12240, 29432, 293, 527, 2535, 304, 2058, 378, 1109, 50764], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3859, "seek": 1342464, "start": 13432.64, "end": 13434.64, "text": " through learnable embeddings we add them together", "tokens": [50764, 807, 1466, 712, 12240, 29432, 321, 909, 552, 1214, 50864], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3860, "seek": 1342464, "start": 13434.64, "end": 13436.64, "text": " and then we feed them into the network directly", "tokens": [50864, 293, 550, 321, 3154, 552, 666, 264, 3209, 3838, 50964], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3861, "seek": 1342464, "start": 13436.64, "end": 13438.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3862, "seek": 1342464, "start": 13438.64, "end": 13440.64, "text": " that's all that's happening here", "tokens": [51064, 300, 311, 439, 300, 311, 2737, 510, 51164], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3863, "seek": 1342464, "start": 13440.64, "end": 13442.64, "text": " and that's how we're feeding an X", "tokens": [51164, 293, 300, 311, 577, 321, 434, 12919, 364, 1783, 51264], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3864, "seek": 1342464, "start": 13442.64, "end": 13444.64, "text": " which is the output of these", "tokens": [51264, 597, 307, 264, 5598, 295, 613, 51364], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3865, "seek": 1342464, "start": 13444.64, "end": 13446.64, "text": " then after", "tokens": [51364, 550, 934, 51464], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3866, "seek": 1342464, "start": 13446.64, "end": 13448.64, "text": " this is like way after", "tokens": [51464, 341, 307, 411, 636, 934, 51564], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3867, "seek": 1342464, "start": 13448.64, "end": 13450.64, "text": " we've gotten through all of these", "tokens": [51564, 321, 600, 5768, 807, 439, 295, 613, 51664], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3868, "seek": 1342464, "start": 13450.64, "end": 13452.64, "text": " GPT layers or blocks", "tokens": [51664, 26039, 51, 7914, 420, 8474, 51764], "temperature": 0.0, "avg_logprob": -0.07097420325646034, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.0028891602996736765}, {"id": 3869, "seek": 1345264, "start": 13452.64, "end": 13454.64, "text": " we do this final layer norm", "tokens": [50364, 321, 360, 341, 2572, 4583, 2026, 50464], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3870, "seek": 1345264, "start": 13454.64, "end": 13456.64, "text": " and then this linear transformation", "tokens": [50464, 293, 550, 341, 8213, 9887, 50564], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3871, "seek": 1345264, "start": 13456.64, "end": 13458.64, "text": " to get it to a", "tokens": [50564, 281, 483, 309, 281, 257, 50664], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3872, "seek": 1345264, "start": 13458.64, "end": 13460.64, "text": " softmax", "tokens": [50664, 2787, 41167, 50764], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3873, "seek": 1345264, "start": 13460.64, "end": 13462.64, "text": " to get it to essentially probabilities", "tokens": [50764, 281, 483, 309, 281, 4476, 33783, 50864], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3874, "seek": 1345264, "start": 13462.64, "end": 13464.64, "text": " that we can feed into our softmax function", "tokens": [50864, 300, 321, 393, 3154, 666, 527, 2787, 41167, 2445, 50964], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3875, "seek": 1345264, "start": 13464.64, "end": 13466.64, "text": " and then other than that", "tokens": [50964, 293, 550, 661, 813, 300, 51064], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3876, "seek": 1345264, "start": 13466.64, "end": 13468.64, "text": " this forward pass is exactly the same", "tokens": [51064, 341, 2128, 1320, 307, 2293, 264, 912, 51164], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3877, "seek": 1345264, "start": 13468.64, "end": 13470.64, "text": " other than this little block of code here", "tokens": [51164, 661, 813, 341, 707, 3461, 295, 3089, 510, 51264], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3878, "seek": 1345264, "start": 13470.64, "end": 13472.64, "text": " so if this makes sense so far", "tokens": [51264, 370, 498, 341, 1669, 2020, 370, 1400, 51364], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3879, "seek": 1345264, "start": 13472.64, "end": 13474.64, "text": " that is absolutely amazing", "tokens": [51364, 300, 307, 3122, 2243, 51464], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3880, "seek": 1345264, "start": 13474.64, "end": 13476.64, "text": " let's continue I'm actually going to add", "tokens": [51464, 718, 311, 2354, 286, 478, 767, 516, 281, 909, 51564], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3881, "seek": 1345264, "start": 13476.64, "end": 13478.64, "text": " a little bit of", "tokens": [51564, 257, 707, 857, 295, 51664], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3882, "seek": 1345264, "start": 13478.64, "end": 13480.64, "text": " in practice", "tokens": [51664, 294, 3124, 51764], "temperature": 0.0, "avg_logprob": -0.05725762603479788, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0018673950107768178}, {"id": 3883, "seek": 1348064, "start": 13480.64, "end": 13482.64, "text": " some little", "tokens": [50364, 512, 707, 50464], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3884, "seek": 1348064, "start": 13482.64, "end": 13484.64, "text": " weight initializations", "tokens": [50464, 3364, 5883, 14455, 50564], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3885, "seek": 1348064, "start": 13484.64, "end": 13486.64, "text": " that we should be using", "tokens": [50564, 300, 321, 820, 312, 1228, 50664], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3886, "seek": 1348064, "start": 13486.64, "end": 13488.64, "text": " in our language model", "tokens": [50664, 294, 527, 2856, 2316, 50764], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3887, "seek": 1348064, "start": 13488.64, "end": 13490.64, "text": " and in module subclass", "tokens": [50764, 293, 294, 10088, 1422, 11665, 50864], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3888, "seek": 1348064, "start": 13490.64, "end": 13492.64, "text": " so", "tokens": [50864, 370, 50964], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3889, "seek": 1348064, "start": 13492.64, "end": 13494.64, "text": " I'm going to go over a little bit of math here", "tokens": [50964, 286, 478, 516, 281, 352, 670, 257, 707, 857, 295, 5221, 510, 51064], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3890, "seek": 1348064, "start": 13494.64, "end": 13496.64, "text": " but this is just really important for practice", "tokens": [51064, 457, 341, 307, 445, 534, 1021, 337, 3124, 51164], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3891, "seek": 1348064, "start": 13496.64, "end": 13498.64, "text": " and to make sure that your model", "tokens": [51164, 293, 281, 652, 988, 300, 428, 2316, 51264], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3892, "seek": 1348064, "start": 13498.64, "end": 13500.64, "text": " does not fail in the training process", "tokens": [51264, 775, 406, 3061, 294, 264, 3097, 1399, 51364], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3893, "seek": 1348064, "start": 13500.64, "end": 13502.64, "text": " this is very important", "tokens": [51364, 341, 307, 588, 1021, 51464], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3894, "seek": 1348064, "start": 13502.64, "end": 13504.64, "text": " it's going to be a little funky", "tokens": [51464, 309, 311, 516, 281, 312, 257, 707, 33499, 51564], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3895, "seek": 1348064, "start": 13504.64, "end": 13506.64, "text": " on the conceptualizing", "tokens": [51564, 322, 264, 24106, 3319, 51664], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3896, "seek": 1348064, "start": 13506.64, "end": 13508.64, "text": " but bring out some pen and paper", "tokens": [51664, 457, 1565, 484, 512, 3435, 293, 3035, 51764], "temperature": 0.0, "avg_logprob": -0.08837970840596707, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004536028951406479}, {"id": 3897, "seek": 1350864, "start": 13508.64, "end": 13510.64, "text": " and do some math with me", "tokens": [50364, 293, 360, 512, 5221, 365, 385, 50464], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3898, "seek": 1350864, "start": 13510.64, "end": 13512.64, "text": " we've built up some of these", "tokens": [50464, 321, 600, 3094, 493, 512, 295, 613, 50564], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3899, "seek": 1350864, "start": 13512.64, "end": 13514.64, "text": " initial GPT language model architecture", "tokens": [50564, 5883, 26039, 51, 2856, 2316, 9482, 50664], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3900, "seek": 1350864, "start": 13514.64, "end": 13516.64, "text": " and before we continue building", "tokens": [50664, 293, 949, 321, 2354, 2390, 50764], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3901, "seek": 1350864, "start": 13516.64, "end": 13518.64, "text": " more of it and the other functions", "tokens": [50764, 544, 295, 309, 293, 264, 661, 6828, 50864], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3902, "seek": 1350864, "start": 13518.64, "end": 13520.64, "text": " some of the math stuff that's going on", "tokens": [50864, 512, 295, 264, 5221, 1507, 300, 311, 516, 322, 50964], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3903, "seek": 1350864, "start": 13520.64, "end": 13522.64, "text": " the parallelization that's going on in the script", "tokens": [50964, 264, 8952, 2144, 300, 311, 516, 322, 294, 264, 5755, 51064], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3904, "seek": 1350864, "start": 13522.64, "end": 13524.64, "text": " I want to show you some of the math", "tokens": [51064, 286, 528, 281, 855, 291, 512, 295, 264, 5221, 51164], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3905, "seek": 1350864, "start": 13524.64, "end": 13526.64, "text": " that we're going to use to initialize the weights", "tokens": [51164, 300, 321, 434, 516, 281, 764, 281, 5883, 1125, 264, 17443, 51264], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3906, "seek": 1350864, "start": 13526.64, "end": 13528.64, "text": " of the model to help it train", "tokens": [51264, 295, 264, 2316, 281, 854, 309, 3847, 51364], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3907, "seek": 1350864, "start": 13528.64, "end": 13530.64, "text": " and converge better", "tokens": [51364, 293, 41881, 1101, 51464], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3908, "seek": 1350864, "start": 13530.64, "end": 13532.64, "text": " so there's this new thing", "tokens": [51464, 370, 456, 311, 341, 777, 551, 51564], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3909, "seek": 1350864, "start": 13532.64, "end": 13534.64, "text": " that I want to introduce called standard deviation", "tokens": [51564, 300, 286, 528, 281, 5366, 1219, 3832, 25163, 51664], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3910, "seek": 1350864, "start": 13534.64, "end": 13536.64, "text": " and this is used in intermediate level mathematics", "tokens": [51664, 293, 341, 307, 1143, 294, 19376, 1496, 18666, 51764], "temperature": 0.0, "avg_logprob": -0.049237421064665825, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0008968066540546715}, {"id": 3911, "seek": 1353664, "start": 13536.64, "end": 13538.64, "text": " the symbol essentially looks like this", "tokens": [50364, 264, 5986, 4476, 1542, 411, 341, 50464], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3912, "seek": 1353664, "start": 13538.64, "end": 13540.64, "text": " population standard deviation", "tokens": [50464, 4415, 3832, 25163, 50564], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3913, "seek": 1353664, "start": 13540.64, "end": 13542.64, "text": " so", "tokens": [50564, 370, 50664], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3914, "seek": 1353664, "start": 13542.64, "end": 13544.64, "text": " n", "tokens": [50664, 297, 50764], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3915, "seek": 1353664, "start": 13544.64, "end": 13546.64, "text": " the size", "tokens": [50764, 264, 2744, 50864], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3916, "seek": 1353664, "start": 13546.64, "end": 13548.64, "text": " so it's just going to be an array", "tokens": [50864, 370, 309, 311, 445, 516, 281, 312, 364, 10225, 50964], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3917, "seek": 1353664, "start": 13548.64, "end": 13550.64, "text": " the length of the array", "tokens": [50964, 264, 4641, 295, 264, 10225, 51064], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3918, "seek": 1353664, "start": 13550.64, "end": 13552.64, "text": " and then xi", "tokens": [51064, 293, 550, 36800, 51164], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3919, "seek": 1353664, "start": 13552.64, "end": 13554.64, "text": " we iterate over each value", "tokens": [51164, 321, 44497, 670, 1184, 2158, 51264], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3920, "seek": 1353664, "start": 13554.64, "end": 13556.64, "text": " so xf position 0", "tokens": [51264, 370, 2031, 69, 2535, 1958, 51364], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3921, "seek": 1353664, "start": 13556.64, "end": 13558.64, "text": " xf position 1", "tokens": [51364, 2031, 69, 2535, 502, 51464], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3922, "seek": 1353664, "start": 13558.64, "end": 13560.64, "text": " xf position 2", "tokens": [51464, 2031, 69, 2535, 568, 51564], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3923, "seek": 1353664, "start": 13560.64, "end": 13562.64, "text": " and then this u here is the mean", "tokens": [51564, 293, 550, 341, 344, 510, 307, 264, 914, 51664], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3924, "seek": 1353664, "start": 13562.64, "end": 13564.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.10754134737212083, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.013214414939284325}, {"id": 3925, "seek": 1356464, "start": 13564.64, "end": 13566.64, "text": " we iterate over each element", "tokens": [50364, 321, 44497, 670, 1184, 4478, 50464], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3926, "seek": 1356464, "start": 13566.64, "end": 13568.64, "text": " we're going to", "tokens": [50464, 321, 434, 516, 281, 50564], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3927, "seek": 1356464, "start": 13568.64, "end": 13570.64, "text": " subtract it by the mean", "tokens": [50564, 16390, 309, 538, 264, 914, 50664], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3928, "seek": 1356464, "start": 13570.64, "end": 13572.64, "text": " we're going to square that and then keep adding", "tokens": [50664, 321, 434, 516, 281, 3732, 300, 293, 550, 1066, 5127, 50764], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3929, "seek": 1356464, "start": 13572.64, "end": 13574.64, "text": " all these squared results together", "tokens": [50764, 439, 613, 8889, 3542, 1214, 50864], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3930, "seek": 1356464, "start": 13574.64, "end": 13576.64, "text": " and then once we get the sum of that", "tokens": [50864, 293, 550, 1564, 321, 483, 264, 2408, 295, 300, 50964], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3931, "seek": 1356464, "start": 13576.64, "end": 13578.64, "text": " we're going to", "tokens": [50964, 321, 434, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3932, "seek": 1356464, "start": 13578.64, "end": 13580.64, "text": " subtract or we're going to divide", "tokens": [51064, 16390, 420, 321, 434, 516, 281, 9845, 51164], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3933, "seek": 1356464, "start": 13580.64, "end": 13582.64, "text": " this by the number of elements there are", "tokens": [51164, 341, 538, 264, 1230, 295, 4959, 456, 366, 51264], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3934, "seek": 1356464, "start": 13582.64, "end": 13584.64, "text": " and then once we get this result", "tokens": [51264, 293, 550, 1564, 321, 483, 341, 1874, 51364], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3935, "seek": 1356464, "start": 13584.64, "end": 13586.64, "text": " we're going to square root that", "tokens": [51364, 321, 434, 516, 281, 3732, 5593, 300, 51464], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3936, "seek": 1356464, "start": 13586.64, "end": 13588.64, "text": " so this symbol here", "tokens": [51464, 370, 341, 5986, 510, 51564], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3937, "seek": 1356464, "start": 13588.64, "end": 13590.64, "text": " might also look a little bit unfamiliar", "tokens": [51564, 1062, 611, 574, 257, 707, 857, 29415, 51664], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3938, "seek": 1356464, "start": 13590.64, "end": 13592.64, "text": " and", "tokens": [51664, 293, 51764], "temperature": 0.0, "avg_logprob": -0.056181456135437555, "compression_ratio": 2.0927835051546393, "no_speech_prob": 0.0017002527602016926}, {"id": 3939, "seek": 1359264, "start": 13592.64, "end": 13594.64, "text": " I'll illustrate this out for you", "tokens": [50364, 286, 603, 23221, 341, 484, 337, 291, 50464], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3940, "seek": 1359264, "start": 13594.64, "end": 13596.64, "text": " so we go to our whiteboard", "tokens": [50464, 370, 321, 352, 281, 527, 2418, 3787, 50564], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3941, "seek": 1359264, "start": 13596.64, "end": 13598.64, "text": " and this e", "tokens": [50564, 293, 341, 308, 50664], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3942, "seek": 1359264, "start": 13598.64, "end": 13600.64, "text": " looks like", "tokens": [50664, 1542, 411, 50764], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3943, "seek": 1359264, "start": 13600.64, "end": 13602.64, "text": " looks like that", "tokens": [50764, 1542, 411, 300, 50864], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3944, "seek": 1359264, "start": 13602.64, "end": 13604.64, "text": " let's just say we were to put in", "tokens": [50864, 718, 311, 445, 584, 321, 645, 281, 829, 294, 50964], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3945, "seek": 1359264, "start": 13604.64, "end": 13606.64, "text": " x", "tokens": [50964, 2031, 51064], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3946, "seek": 1359264, "start": 13606.64, "end": 13608.64, "text": " i like that", "tokens": [51064, 741, 411, 300, 51164], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3947, "seek": 1359264, "start": 13608.64, "end": 13610.64, "text": " and our array", "tokens": [51164, 293, 527, 10225, 51264], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3948, "seek": 1359264, "start": 13610.64, "end": 13612.64, "text": " let's just say for instance", "tokens": [51264, 718, 311, 445, 584, 337, 5197, 51364], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3949, "seek": 1359264, "start": 13612.64, "end": 13614.64, "text": " our array", "tokens": [51364, 527, 10225, 51464], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3950, "seek": 1359264, "start": 13614.64, "end": 13616.64, "text": " is 0.1", "tokens": [51464, 307, 1958, 13, 16, 51564], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3951, "seek": 1359264, "start": 13616.64, "end": 13618.64, "text": " 0.2, 0.3", "tokens": [51564, 1958, 13, 17, 11, 1958, 13, 18, 51664], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3952, "seek": 1359264, "start": 13618.64, "end": 13620.64, "text": " so what would the result of this be", "tokens": [51664, 370, 437, 576, 264, 1874, 295, 341, 312, 51764], "temperature": 0.0, "avg_logprob": -0.07809993342349404, "compression_ratio": 1.6209150326797386, "no_speech_prob": 0.005909929517656565}, {"id": 3953, "seek": 1362064, "start": 13620.64, "end": 13622.64, "text": " well if we look at each element", "tokens": [50364, 731, 498, 321, 574, 412, 1184, 4478, 50464], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3954, "seek": 1362064, "start": 13622.64, "end": 13624.64, "text": " iteratively add them together", "tokens": [50464, 17138, 19020, 909, 552, 1214, 50564], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3955, "seek": 1362064, "start": 13624.64, "end": 13626.64, "text": " so 0.1", "tokens": [50564, 370, 1958, 13, 16, 50664], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3956, "seek": 1362064, "start": 13626.64, "end": 13628.64, "text": " plus 0.2 plus 0.3", "tokens": [50664, 1804, 1958, 13, 17, 1804, 1958, 13, 18, 50764], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3957, "seek": 1362064, "start": 13628.64, "end": 13630.64, "text": " well we get 0.6 from that", "tokens": [50764, 731, 321, 483, 1958, 13, 21, 490, 300, 50864], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3958, "seek": 1362064, "start": 13630.64, "end": 13632.64, "text": " so this would essentially", "tokens": [50864, 370, 341, 576, 4476, 50964], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3959, "seek": 1362064, "start": 13632.64, "end": 13634.64, "text": " be equal to", "tokens": [50964, 312, 2681, 281, 51064], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3960, "seek": 1362064, "start": 13634.64, "end": 13636.64, "text": " 0.6", "tokens": [51064, 1958, 13, 21, 51164], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3961, "seek": 1362064, "start": 13636.64, "end": 13638.64, "text": " that's what that equals", "tokens": [51164, 300, 311, 437, 300, 6915, 51264], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3962, "seek": 1362064, "start": 13638.64, "end": 13640.64, "text": " we just add each of these up together", "tokens": [51264, 321, 445, 909, 1184, 295, 613, 493, 1214, 51364], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3963, "seek": 1362064, "start": 13640.64, "end": 13642.64, "text": " or we do whatever this is iteratively", "tokens": [51364, 420, 321, 360, 2035, 341, 307, 17138, 19020, 51464], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3964, "seek": 1362064, "start": 13642.64, "end": 13644.64, "text": " whatever this element is", "tokens": [51464, 2035, 341, 4478, 307, 51564], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3965, "seek": 1362064, "start": 13644.64, "end": 13646.64, "text": " we iterate over", "tokens": [51564, 321, 44497, 670, 51664], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3966, "seek": 1362064, "start": 13646.64, "end": 13648.64, "text": " the number of elements we have in", "tokens": [51664, 264, 1230, 295, 4959, 321, 362, 294, 51764], "temperature": 0.0, "avg_logprob": -0.05659428712363555, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0012064722832292318}, {"id": 3967, "seek": 1364864, "start": 13648.64, "end": 13650.64, "text": " the arbitrary array", "tokens": [50364, 264, 23211, 10225, 50464], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3968, "seek": 1364864, "start": 13650.64, "end": 13652.64, "text": " or", "tokens": [50464, 420, 50564], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3969, "seek": 1364864, "start": 13652.64, "end": 13654.64, "text": " vector or list or whatever you want to call it", "tokens": [50564, 8062, 420, 1329, 420, 2035, 291, 528, 281, 818, 309, 50664], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3970, "seek": 1364864, "start": 13654.64, "end": 13656.64, "text": " and then we just", "tokens": [50664, 293, 550, 321, 445, 50764], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3971, "seek": 1364864, "start": 13656.64, "end": 13658.64, "text": " sort of look at what's going on here", "tokens": [50764, 1333, 295, 574, 412, 437, 311, 516, 322, 510, 50864], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3972, "seek": 1364864, "start": 13658.64, "end": 13660.64, "text": " and we can do some basic arithmetic stuff", "tokens": [50864, 293, 321, 393, 360, 512, 3875, 42973, 1507, 50964], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3973, "seek": 1364864, "start": 13660.64, "end": 13662.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3974, "seek": 1364864, "start": 13662.64, "end": 13664.64, "text": " let's walk through a few examples", "tokens": [51064, 718, 311, 1792, 807, 257, 1326, 5110, 51164], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3975, "seek": 1364864, "start": 13664.64, "end": 13666.64, "text": " just to illustrate to you", "tokens": [51164, 445, 281, 23221, 281, 291, 51264], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3976, "seek": 1364864, "start": 13666.64, "end": 13668.64, "text": " what the results look like", "tokens": [51264, 437, 264, 3542, 574, 411, 51364], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3977, "seek": 1364864, "start": 13668.64, "end": 13670.64, "text": " based on the inputs here", "tokens": [51364, 2361, 322, 264, 15743, 510, 51464], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3978, "seek": 1364864, "start": 13670.64, "end": 13672.64, "text": " so I'm going to go back to my whiteboard", "tokens": [51464, 370, 286, 478, 516, 281, 352, 646, 281, 452, 2418, 3787, 51564], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3979, "seek": 1364864, "start": 13672.64, "end": 13674.64, "text": " we're going to draw a little line here", "tokens": [51564, 321, 434, 516, 281, 2642, 257, 707, 1622, 510, 51664], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3980, "seek": 1364864, "start": 13674.64, "end": 13676.64, "text": " just to separate this", "tokens": [51664, 445, 281, 4994, 341, 51764], "temperature": 0.0, "avg_logprob": -0.06441367524010795, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.005553677678108215}, {"id": 3981, "seek": 1367664, "start": 13676.64, "end": 13678.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3982, "seek": 1367664, "start": 13678.64, "end": 13680.64, "text": " I want to calculate the standard deviation", "tokens": [50464, 286, 528, 281, 8873, 264, 3832, 25163, 50564], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3983, "seek": 1367664, "start": 13680.64, "end": 13682.64, "text": " do standard deviation", "tokens": [50564, 360, 3832, 25163, 50664], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3984, "seek": 1367664, "start": 13682.64, "end": 13684.64, "text": " of", "tokens": [50664, 295, 50764], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3985, "seek": 1367664, "start": 13686.64, "end": 13688.64, "text": " and then we'll just make some random array", "tokens": [50864, 293, 550, 321, 603, 445, 652, 512, 4974, 10225, 50964], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3986, "seek": 1367664, "start": 13688.64, "end": 13690.64, "text": " negative", "tokens": [50964, 3671, 51064], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3987, "seek": 1367664, "start": 13690.64, "end": 13692.64, "text": " 0.38", "tokens": [51064, 1958, 13, 12625, 51164], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3988, "seek": 1367664, "start": 13692.64, "end": 13694.64, "text": " negative 0.38", "tokens": [51164, 3671, 1958, 13, 12625, 51264], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3989, "seek": 1367664, "start": 13694.64, "end": 13696.64, "text": " 0.52", "tokens": [51264, 1958, 13, 17602, 51364], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3990, "seek": 1367664, "start": 13698.64, "end": 13700.64, "text": " and then 2.48", "tokens": [51464, 293, 550, 568, 13, 13318, 51564], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3991, "seek": 1367664, "start": 13700.64, "end": 13702.64, "text": " cool", "tokens": [51564, 1627, 51664], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3992, "seek": 1367664, "start": 13702.64, "end": 13704.64, "text": " so we have this array this is three elements", "tokens": [51664, 370, 321, 362, 341, 10225, 341, 307, 1045, 4959, 51764], "temperature": 0.0, "avg_logprob": -0.095751651345867, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.00433091726154089}, {"id": 3993, "seek": 1370464, "start": 13704.64, "end": 13706.64, "text": " so that means n", "tokens": [50364, 370, 300, 1355, 297, 50464], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3994, "seek": 1370464, "start": 13706.64, "end": 13708.64, "text": " is going to be equal to three", "tokens": [50464, 307, 516, 281, 312, 2681, 281, 1045, 50564], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3995, "seek": 1370464, "start": 13708.64, "end": 13710.64, "text": " let me drag this over here", "tokens": [50564, 718, 385, 5286, 341, 670, 510, 50664], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3996, "seek": 1370464, "start": 13710.64, "end": 13712.64, "text": " so n is the number of elements", "tokens": [50664, 370, 297, 307, 264, 1230, 295, 4959, 50764], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3997, "seek": 1370464, "start": 13712.64, "end": 13714.64, "text": " so n is going to be equal to three", "tokens": [50764, 370, 297, 307, 516, 281, 312, 2681, 281, 1045, 50864], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3998, "seek": 1370464, "start": 13714.64, "end": 13716.64, "text": " our mean", "tokens": [50864, 527, 914, 50964], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 3999, "seek": 1370464, "start": 13716.64, "end": 13718.64, "text": " well", "tokens": [50964, 731, 51064], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4000, "seek": 1370464, "start": 13718.64, "end": 13720.64, "text": " our mean is just", "tokens": [51064, 527, 914, 307, 445, 51164], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4001, "seek": 1370464, "start": 13720.64, "end": 13722.64, "text": " we add all these up together and then we average them", "tokens": [51164, 321, 909, 439, 613, 493, 1214, 293, 550, 321, 4274, 552, 51264], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4002, "seek": 1370464, "start": 13722.64, "end": 13724.64, "text": " so our mean", "tokens": [51264, 370, 527, 914, 51364], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4003, "seek": 1370464, "start": 13724.64, "end": 13726.64, "text": " is going to be equal to", "tokens": [51364, 307, 516, 281, 312, 2681, 281, 51464], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4004, "seek": 1370464, "start": 13726.64, "end": 13728.64, "text": " let's just say", "tokens": [51464, 718, 311, 445, 584, 51564], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4005, "seek": 1370464, "start": 13728.64, "end": 13730.64, "text": " 0.38", "tokens": [51564, 1958, 13, 12625, 51664], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4006, "seek": 1370464, "start": 13730.64, "end": 13732.64, "text": " plus 0.52", "tokens": [51664, 1804, 1958, 13, 17602, 51764], "temperature": 0.0, "avg_logprob": -0.074709155771992, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.005219174083322287}, {"id": 4007, "seek": 1373264, "start": 13732.64, "end": 13734.64, "text": " plus", "tokens": [50364, 1804, 50464], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4008, "seek": 1373264, "start": 13734.64, "end": 13736.64, "text": " 2.48", "tokens": [50464, 568, 13, 13318, 50564], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4009, "seek": 1373264, "start": 13736.64, "end": 13738.64, "text": " and then divided by three", "tokens": [50564, 293, 550, 6666, 538, 1045, 50664], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4010, "seek": 1373264, "start": 13738.64, "end": 13740.64, "text": " and the answer to this", "tokens": [50664, 293, 264, 1867, 281, 341, 50764], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4011, "seek": 1373264, "start": 13740.64, "end": 13742.64, "text": " I did the math ahead of time", "tokens": [50764, 286, 630, 264, 5221, 2286, 295, 565, 50864], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4012, "seek": 1373264, "start": 13742.64, "end": 13744.64, "text": " is literally 0.873", "tokens": [50864, 307, 3736, 1958, 13, 23853, 18, 50964], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4013, "seek": 1373264, "start": 13744.64, "end": 13746.64, "text": " repeated but we're just going to put 0.87", "tokens": [50964, 10477, 457, 321, 434, 445, 516, 281, 829, 1958, 13, 23853, 51064], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4014, "seek": 1373264, "start": 13746.64, "end": 13748.64, "text": " for simplicity's sake", "tokens": [51064, 337, 25632, 311, 9717, 51164], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4015, "seek": 1373264, "start": 13748.64, "end": 13750.64, "text": " cool so the mean of this", "tokens": [51164, 1627, 370, 264, 914, 295, 341, 51264], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4016, "seek": 1373264, "start": 13750.64, "end": 13752.64, "text": " is 0.87 and n is equal to three", "tokens": [51264, 307, 1958, 13, 23853, 293, 297, 307, 2681, 281, 1045, 51364], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4017, "seek": 1373264, "start": 13752.64, "end": 13754.64, "text": " now we can start doing", "tokens": [51364, 586, 321, 393, 722, 884, 51464], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4018, "seek": 1373264, "start": 13754.64, "end": 13756.64, "text": " some of the other math", "tokens": [51464, 512, 295, 264, 661, 5221, 51564], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4019, "seek": 1373264, "start": 13756.64, "end": 13758.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4020, "seek": 1373264, "start": 13758.64, "end": 13760.64, "text": " we have this", "tokens": [51664, 321, 362, 341, 51764], "temperature": 0.0, "avg_logprob": -0.06945481019861557, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.0015485547482967377}, {"id": 4021, "seek": 1376064, "start": 13760.64, "end": 13762.64, "text": " O has a cool line", "tokens": [50364, 422, 575, 257, 1627, 1622, 50464], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4022, "seek": 1376064, "start": 13764.64, "end": 13766.64, "text": " and we do", "tokens": [50564, 293, 321, 360, 50664], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4023, "seek": 1376064, "start": 13766.64, "end": 13768.64, "text": " square root", "tokens": [50664, 3732, 5593, 50764], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4024, "seek": 1376064, "start": 13768.64, "end": 13770.64, "text": " one over", "tokens": [50764, 472, 670, 50864], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4025, "seek": 1376064, "start": 13770.64, "end": 13772.64, "text": " n which is equal to three", "tokens": [50864, 297, 597, 307, 2681, 281, 1045, 50964], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4026, "seek": 1376064, "start": 13772.64, "end": 13774.64, "text": " and then we", "tokens": [50964, 293, 550, 321, 51064], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4027, "seek": 1376064, "start": 13774.64, "end": 13776.64, "text": " multiply this", "tokens": [51064, 12972, 341, 51164], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4028, "seek": 1376064, "start": 13776.64, "end": 13778.64, "text": " by sigma", "tokens": [51164, 538, 12771, 51264], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4029, "seek": 1376064, "start": 13778.64, "end": 13780.64, "text": " that's what this symbol is", "tokens": [51264, 300, 311, 437, 341, 5986, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4030, "seek": 1376064, "start": 13780.64, "end": 13782.64, "text": " that's sigma that's the name for it", "tokens": [51364, 300, 311, 12771, 300, 311, 264, 1315, 337, 309, 51464], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4031, "seek": 1376064, "start": 13782.64, "end": 13784.64, "text": " and then we go", "tokens": [51464, 293, 550, 321, 352, 51564], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4032, "seek": 1376064, "start": 13784.64, "end": 13786.64, "text": " X", "tokens": [51564, 1783, 51664], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4033, "seek": 1376064, "start": 13786.64, "end": 13788.64, "text": " I", "tokens": [51664, 286, 51764], "temperature": 0.0, "avg_logprob": -0.08889912270210884, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.007010568864643574}, {"id": 4034, "seek": 1378864, "start": 13788.64, "end": 13790.64, "text": " minus", "tokens": [50364, 3175, 50464], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4035, "seek": 1378864, "start": 13790.64, "end": 13792.64, "text": " and then our mean of", "tokens": [50464, 293, 550, 527, 914, 295, 50564], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4036, "seek": 1378864, "start": 13792.64, "end": 13794.64, "text": " 0.87", "tokens": [50564, 1958, 13, 23853, 50664], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4037, "seek": 1378864, "start": 13798.64, "end": 13800.64, "text": " apologies for the sloppy writing", "tokens": [50864, 34929, 337, 264, 43684, 3579, 50964], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4038, "seek": 1378864, "start": 13802.64, "end": 13804.64, "text": " and then we square that", "tokens": [51064, 293, 550, 321, 3732, 300, 51164], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4039, "seek": 1378864, "start": 13804.64, "end": 13806.64, "text": " so let me drag this out", "tokens": [51164, 370, 718, 385, 5286, 341, 484, 51264], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4040, "seek": 1378864, "start": 13806.64, "end": 13808.64, "text": " awesome", "tokens": [51264, 3476, 51364], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4041, "seek": 1378864, "start": 13808.64, "end": 13810.64, "text": " so let's just do this", "tokens": [51364, 370, 718, 311, 445, 360, 341, 51464], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4042, "seek": 1378864, "start": 13810.64, "end": 13812.64, "text": " step by step here", "tokens": [51464, 1823, 538, 1823, 510, 51564], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4043, "seek": 1378864, "start": 13812.64, "end": 13814.64, "text": " so the first one is going to be", "tokens": [51564, 370, 264, 700, 472, 307, 516, 281, 312, 51664], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4044, "seek": 1378864, "start": 13814.64, "end": 13816.64, "text": " 0.38", "tokens": [51664, 1958, 13, 12625, 51764], "temperature": 0.0, "avg_logprob": -0.08998992409504636, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.0013882225612178445}, {"id": 4045, "seek": 1381664, "start": 13816.64, "end": 13818.64, "text": " 0.", "tokens": [50364, 1958, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4046, "seek": 1381664, "start": 13818.64, "end": 13820.64, "text": " negative", "tokens": [50464, 3671, 50564], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4047, "seek": 1381664, "start": 13820.64, "end": 13822.64, "text": " 0.38", "tokens": [50564, 1958, 13, 12625, 50664], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4048, "seek": 1381664, "start": 13822.64, "end": 13824.64, "text": " and we're going to do minus the mean here", "tokens": [50664, 293, 321, 434, 516, 281, 360, 3175, 264, 914, 510, 50764], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4049, "seek": 1381664, "start": 13824.64, "end": 13826.64, "text": " so minus 0.87", "tokens": [50764, 370, 3175, 1958, 13, 23853, 50864], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4050, "seek": 1381664, "start": 13826.64, "end": 13828.64, "text": " and I'm just going to wrap all this", "tokens": [50864, 293, 286, 478, 445, 516, 281, 7019, 439, 341, 50964], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4051, "seek": 1381664, "start": 13828.64, "end": 13830.64, "text": " in brackets so that we don't miss anything", "tokens": [50964, 294, 26179, 370, 300, 321, 500, 380, 1713, 1340, 51064], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4052, "seek": 1381664, "start": 13830.64, "end": 13832.64, "text": " wrap it in brackets", "tokens": [51064, 7019, 309, 294, 26179, 51164], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4053, "seek": 1381664, "start": 13832.64, "end": 13834.64, "text": " and then just square it and see what we get after", "tokens": [51164, 293, 550, 445, 3732, 309, 293, 536, 437, 321, 483, 934, 51264], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4054, "seek": 1381664, "start": 13834.64, "end": 13836.64, "text": " so I'm just going to write all these out", "tokens": [51264, 370, 286, 478, 445, 516, 281, 2464, 439, 613, 484, 51364], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4055, "seek": 1381664, "start": 13836.64, "end": 13838.64, "text": " then we can do the calculations", "tokens": [51364, 550, 321, 393, 360, 264, 20448, 51464], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4056, "seek": 1381664, "start": 13838.64, "end": 13840.64, "text": " so next up we have 0.52", "tokens": [51464, 370, 958, 493, 321, 362, 1958, 13, 17602, 51564], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4057, "seek": 1381664, "start": 13840.64, "end": 13842.64, "text": " minus 0.87", "tokens": [51564, 3175, 1958, 13, 23853, 51664], "temperature": 0.0, "avg_logprob": -0.07800554795698686, "compression_ratio": 1.6958762886597938, "no_speech_prob": 0.005553479306399822}, {"id": 4058, "seek": 1384264, "start": 13842.64, "end": 13844.64, "text": " we'll square that", "tokens": [50364, 321, 603, 3732, 300, 50464], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4059, "seek": 1384264, "start": 13846.64, "end": 13848.64, "text": " and then next up we have", "tokens": [50564, 293, 550, 958, 493, 321, 362, 50664], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4060, "seek": 1384264, "start": 13848.64, "end": 13850.64, "text": " 2.48", "tokens": [50664, 568, 13, 13318, 50764], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4061, "seek": 1384264, "start": 13850.64, "end": 13852.64, "text": " minus 0.87", "tokens": [50764, 3175, 1958, 13, 23853, 50864], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4062, "seek": 1384264, "start": 13852.64, "end": 13854.64, "text": " and then we square that as well", "tokens": [50864, 293, 550, 321, 3732, 300, 382, 731, 50964], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4063, "seek": 1384264, "start": 13854.64, "end": 13856.64, "text": " so awesome", "tokens": [50964, 370, 3476, 51064], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4064, "seek": 1384264, "start": 13856.64, "end": 13858.64, "text": " what is the result of this", "tokens": [51064, 437, 307, 264, 1874, 295, 341, 51164], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4065, "seek": 1384264, "start": 13858.64, "end": 13860.64, "text": " the result of", "tokens": [51164, 264, 1874, 295, 51264], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4066, "seek": 1384264, "start": 13860.64, "end": 13862.64, "text": " negative 0.38 minus", "tokens": [51264, 3671, 1958, 13, 12625, 3175, 51364], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4067, "seek": 1384264, "start": 13862.64, "end": 13864.64, "text": " 0.87", "tokens": [51364, 1958, 13, 23853, 51464], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4068, "seek": 1384264, "start": 13864.64, "end": 13866.64, "text": " squared is", "tokens": [51464, 8889, 307, 51564], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4069, "seek": 1384264, "start": 13866.64, "end": 13868.64, "text": " 1.57", "tokens": [51564, 502, 13, 19004, 51664], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4070, "seek": 1384264, "start": 13868.64, "end": 13870.64, "text": " the result of", "tokens": [51664, 264, 1874, 295, 51764], "temperature": 0.0, "avg_logprob": -0.06136906297901009, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.004467550199478865}, {"id": 4071, "seek": 1387064, "start": 13870.64, "end": 13872.64, "text": " this line", "tokens": [50364, 341, 1622, 50464], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4072, "seek": 1387064, "start": 13872.64, "end": 13874.64, "text": " is 0.12", "tokens": [50464, 307, 1958, 13, 4762, 50564], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4073, "seek": 1387064, "start": 13874.64, "end": 13876.64, "text": " again these are all approximations", "tokens": [50564, 797, 613, 366, 439, 8542, 763, 50664], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4074, "seek": 1387064, "start": 13876.64, "end": 13878.64, "text": " they're not super spot on", "tokens": [50664, 436, 434, 406, 1687, 4008, 322, 50764], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4075, "seek": 1387064, "start": 13878.64, "end": 13880.64, "text": " we're just doing this to understand", "tokens": [50764, 321, 434, 445, 884, 341, 281, 1223, 50864], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4076, "seek": 1387064, "start": 13880.64, "end": 13882.64, "text": " what's going on here", "tokens": [50864, 437, 311, 516, 322, 510, 50964], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4077, "seek": 1387064, "start": 13882.64, "end": 13884.64, "text": " just to overview the function not for precision", "tokens": [50964, 445, 281, 12492, 264, 2445, 406, 337, 18356, 51064], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4078, "seek": 1387064, "start": 13884.64, "end": 13886.64, "text": " then the next one is going to be", "tokens": [51064, 550, 264, 958, 472, 307, 516, 281, 312, 51164], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4079, "seek": 1387064, "start": 13886.64, "end": 13888.64, "text": " 2.59", "tokens": [51164, 568, 13, 19600, 51264], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4080, "seek": 1387064, "start": 13888.64, "end": 13890.64, "text": " and you can double check all these", "tokens": [51264, 293, 291, 393, 3834, 1520, 439, 613, 51364], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4081, "seek": 1387064, "start": 13890.64, "end": 13892.64, "text": " calculations if you'd like", "tokens": [51364, 20448, 498, 291, 1116, 411, 51464], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4082, "seek": 1387064, "start": 13892.64, "end": 13894.64, "text": " I have done these preemptively so", "tokens": [51464, 286, 362, 1096, 613, 659, 4543, 3413, 370, 51564], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4083, "seek": 1387064, "start": 13894.64, "end": 13896.64, "text": " that is that", "tokens": [51564, 300, 307, 300, 51664], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4084, "seek": 1387064, "start": 13896.64, "end": 13898.64, "text": " and now from here", "tokens": [51664, 293, 586, 490, 510, 51764], "temperature": 0.0, "avg_logprob": -0.06039506084514114, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.002631215378642082}, {"id": 4085, "seek": 1389864, "start": 13898.64, "end": 13900.64, "text": " what we have to do is add each of these together", "tokens": [50364, 437, 321, 362, 281, 360, 307, 909, 1184, 295, 613, 1214, 50464], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4086, "seek": 1389864, "start": 13900.64, "end": 13902.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4087, "seek": 1389864, "start": 13902.64, "end": 13904.64, "text": " 1.57", "tokens": [50564, 502, 13, 19004, 50664], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4088, "seek": 1389864, "start": 13904.64, "end": 13906.64, "text": " plus 0.12", "tokens": [50664, 1804, 1958, 13, 4762, 50764], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4089, "seek": 1389864, "start": 13906.64, "end": 13908.64, "text": " plus 2.59", "tokens": [50764, 1804, 568, 13, 19600, 50864], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4090, "seek": 1389864, "start": 13908.64, "end": 13910.64, "text": " divided by 3", "tokens": [50864, 6666, 538, 805, 50964], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4091, "seek": 1389864, "start": 13910.64, "end": 13912.64, "text": " is", "tokens": [50964, 307, 51064], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4092, "seek": 1389864, "start": 13916.64, "end": 13918.64, "text": " 1.57", "tokens": [51264, 502, 13, 19004, 51364], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4093, "seek": 1389864, "start": 13918.64, "end": 13920.64, "text": " plus 0.12", "tokens": [51364, 1804, 1958, 13, 4762, 51464], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4094, "seek": 1389864, "start": 13920.64, "end": 13922.64, "text": " plus 2.59", "tokens": [51464, 1804, 568, 13, 19600, 51564], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4095, "seek": 1389864, "start": 13922.64, "end": 13924.64, "text": " all that divided by 3", "tokens": [51564, 439, 300, 6666, 538, 805, 51664], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4096, "seek": 1389864, "start": 13924.64, "end": 13926.64, "text": " is going to be equal to 1.42", "tokens": [51664, 307, 516, 281, 312, 2681, 281, 502, 13, 15628, 51764], "temperature": 0.0, "avg_logprob": -0.06693687805762658, "compression_ratio": 1.5, "no_speech_prob": 0.06458712369203568}, {"id": 4097, "seek": 1392664, "start": 13926.64, "end": 13928.64, "text": " keep in mind we also have to", "tokens": [50364, 1066, 294, 1575, 321, 611, 362, 281, 50464], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4098, "seek": 1392664, "start": 13928.64, "end": 13930.64, "text": " square root this", "tokens": [50464, 3732, 5593, 341, 50564], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4099, "seek": 1392664, "start": 13930.64, "end": 13932.64, "text": " so the square root of that", "tokens": [50564, 370, 264, 3732, 5593, 295, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4100, "seek": 1392664, "start": 13932.64, "end": 13934.64, "text": " is going to be", "tokens": [50664, 307, 516, 281, 312, 50764], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4101, "seek": 1392664, "start": 13934.64, "end": 13936.64, "text": " 1.19", "tokens": [50764, 502, 13, 3405, 50864], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4102, "seek": 1392664, "start": 13936.64, "end": 13938.64, "text": " approximately", "tokens": [50864, 10447, 50964], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4103, "seek": 1392664, "start": 13938.64, "end": 13940.64, "text": " we'll just add", "tokens": [50964, 321, 603, 445, 909, 51064], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4104, "seek": 1392664, "start": 13940.64, "end": 13942.64, "text": " this guy ahead of it", "tokens": [51064, 341, 2146, 2286, 295, 309, 51164], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4105, "seek": 1392664, "start": 13942.64, "end": 13944.64, "text": " so that's what the", "tokens": [51164, 370, 300, 311, 437, 264, 51264], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4106, "seek": 1392664, "start": 13944.64, "end": 13946.64, "text": " standard deviation of", "tokens": [51264, 3832, 25163, 295, 51364], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4107, "seek": 1392664, "start": 13946.64, "end": 13948.64, "text": " this array is", "tokens": [51364, 341, 10225, 307, 51464], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4108, "seek": 1392664, "start": 13948.64, "end": 13950.64, "text": " negative 0.38", "tokens": [51464, 3671, 1958, 13, 12625, 51564], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4109, "seek": 1392664, "start": 13950.64, "end": 13952.64, "text": " 0.52, 2.48", "tokens": [51564, 1958, 13, 17602, 11, 568, 13, 13318, 51664], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4110, "seek": 1392664, "start": 13952.64, "end": 13954.64, "text": " standard deviation is", "tokens": [51664, 3832, 25163, 307, 51764], "temperature": 0.0, "avg_logprob": -0.08127798275514082, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.012617029249668121}, {"id": 4111, "seek": 1395464, "start": 13954.64, "end": 13956.64, "text": " 1.19", "tokens": [50364, 502, 13, 3405, 50464], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4112, "seek": 1395464, "start": 13956.64, "end": 13958.64, "text": " let's do another example", "tokens": [50464, 718, 311, 360, 1071, 1365, 50564], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4113, "seek": 1395464, "start": 13962.64, "end": 13964.64, "text": " so let's say", "tokens": [50764, 370, 718, 311, 584, 50864], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4114, "seek": 1395464, "start": 13964.64, "end": 13966.64, "text": " we want to do the standard deviation", "tokens": [50864, 321, 528, 281, 360, 264, 3832, 25163, 50964], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4115, "seek": 1395464, "start": 13966.64, "end": 13968.64, "text": " of", "tokens": [50964, 295, 51064], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4116, "seek": 1395464, "start": 13968.64, "end": 13970.64, "text": " 0.48", "tokens": [51064, 1958, 13, 13318, 51164], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4117, "seek": 1395464, "start": 13972.64, "end": 13974.64, "text": " 0.5", "tokens": [51264, 1958, 13, 20, 51364], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4118, "seek": 1395464, "start": 13976.64, "end": 13978.64, "text": " 0.50", "tokens": [51464, 1958, 13, 2803, 51564], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4119, "seek": 1395464, "start": 13978.64, "end": 13980.64, "text": " I guess 0.52", "tokens": [51564, 286, 2041, 1958, 13, 17602, 51664], "temperature": 0.0, "avg_logprob": -0.14700661965136258, "compression_ratio": 1.1473684210526316, "no_speech_prob": 0.033040713518857956}, {"id": 4120, "seek": 1398464, "start": 13984.64, "end": 13986.64, "text": " so there's a little pattern here", "tokens": [50364, 370, 456, 311, 257, 707, 5102, 510, 50464], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4121, "seek": 1398464, "start": 13986.64, "end": 13988.64, "text": " just goes up by 0.02 each time", "tokens": [50464, 445, 1709, 493, 538, 1958, 13, 12756, 1184, 565, 50564], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4122, "seek": 1398464, "start": 13988.64, "end": 13990.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4123, "seek": 1398464, "start": 13990.64, "end": 13992.64, "text": " you're going to see why this is", "tokens": [50664, 291, 434, 516, 281, 536, 983, 341, 307, 50764], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4124, "seek": 1398464, "start": 13992.64, "end": 13994.64, "text": " vastly different than the other example", "tokens": [50764, 41426, 819, 813, 264, 661, 1365, 50864], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4125, "seek": 1398464, "start": 13994.64, "end": 13996.64, "text": " so let's walk through this", "tokens": [50864, 370, 718, 311, 1792, 807, 341, 50964], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4126, "seek": 1398464, "start": 13996.64, "end": 13998.64, "text": " so first of all we have N", "tokens": [50964, 370, 700, 295, 439, 321, 362, 426, 51064], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4127, "seek": 1398464, "start": 14000.64, "end": 14002.64, "text": " N is equal to 3", "tokens": [51164, 426, 307, 2681, 281, 805, 51264], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4128, "seek": 1398464, "start": 14002.64, "end": 14004.64, "text": " cool", "tokens": [51264, 1627, 51364], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4129, "seek": 1398464, "start": 14004.64, "end": 14006.64, "text": " what does our mean", "tokens": [51364, 437, 775, 527, 914, 51464], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4130, "seek": 1398464, "start": 14006.64, "end": 14008.64, "text": " our mean", "tokens": [51464, 527, 914, 51564], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4131, "seek": 1398464, "start": 14008.64, "end": 14010.64, "text": " well if you do our mean our mean is 0.5", "tokens": [51564, 731, 498, 291, 360, 527, 914, 527, 914, 307, 1958, 13, 20, 51664], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4132, "seek": 1398464, "start": 14010.64, "end": 14012.64, "text": " 0.48 plus this", "tokens": [51664, 1958, 13, 13318, 1804, 341, 51764], "temperature": 0.0, "avg_logprob": -0.10074457791772219, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.0025503456126898527}, {"id": 4133, "seek": 1401264, "start": 14012.64, "end": 14014.64, "text": " plus that", "tokens": [50364, 1804, 300, 50464], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4134, "seek": 1401264, "start": 14014.64, "end": 14016.64, "text": " that's going to be 0.5", "tokens": [50464, 300, 311, 516, 281, 312, 1958, 13, 20, 50564], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4135, "seek": 1401264, "start": 14016.64, "end": 14018.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4136, "seek": 1401264, "start": 14018.64, "end": 14020.64, "text": " if you're good with numbers", "tokens": [50664, 498, 291, 434, 665, 365, 3547, 50764], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4137, "seek": 1401264, "start": 14020.64, "end": 14022.64, "text": " you'll probably already be able to do this in your head", "tokens": [50764, 291, 603, 1391, 1217, 312, 1075, 281, 360, 341, 294, 428, 1378, 50864], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4138, "seek": 1401264, "start": 14022.64, "end": 14024.64, "text": " but that's okay if not", "tokens": [50864, 457, 300, 311, 1392, 498, 406, 50964], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4139, "seek": 1401264, "start": 14024.64, "end": 14026.64, "text": " next up", "tokens": [50964, 958, 493, 51064], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4140, "seek": 1401264, "start": 14026.64, "end": 14028.64, "text": " we're going to do this in the formula", "tokens": [51064, 321, 434, 516, 281, 360, 341, 294, 264, 8513, 51164], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4141, "seek": 1401264, "start": 14028.64, "end": 14030.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4142, "seek": 1401264, "start": 14030.64, "end": 14032.64, "text": " what do these iterations look like", "tokens": [51264, 437, 360, 613, 36540, 574, 411, 51364], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4143, "seek": 1401264, "start": 14032.64, "end": 14034.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4144, "seek": 1401264, "start": 14034.64, "end": 14036.64, "text": " 0.", "tokens": [51464, 1958, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4145, "seek": 1401264, "start": 14036.64, "end": 14038.64, "text": " let's just do these in brackets", "tokens": [51564, 718, 311, 445, 360, 613, 294, 26179, 51664], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4146, "seek": 1401264, "start": 14038.64, "end": 14040.64, "text": " the old way", "tokens": [51664, 264, 1331, 636, 51764], "temperature": 0.0, "avg_logprob": -0.06810287634531657, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.002472336171194911}, {"id": 4147, "seek": 1404064, "start": 14040.64, "end": 14042.64, "text": " 0.5", "tokens": [50364, 1958, 13, 20, 50464], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4148, "seek": 1404064, "start": 14042.64, "end": 14044.64, "text": " squared", "tokens": [50464, 8889, 50564], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4149, "seek": 1404064, "start": 14044.64, "end": 14046.64, "text": " the next one is", "tokens": [50564, 264, 958, 472, 307, 50664], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4150, "seek": 1404064, "start": 14046.64, "end": 14048.64, "text": " 0.5", "tokens": [50664, 1958, 13, 20, 50764], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4151, "seek": 1404064, "start": 14048.64, "end": 14050.64, "text": " minus 0.5", "tokens": [50764, 3175, 1958, 13, 20, 50864], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4152, "seek": 1404064, "start": 14050.64, "end": 14052.64, "text": " squared which we already know is 0", "tokens": [50864, 8889, 597, 321, 1217, 458, 307, 1958, 50964], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4153, "seek": 1404064, "start": 14054.64, "end": 14056.64, "text": " and this one is 0.52", "tokens": [51064, 293, 341, 472, 307, 1958, 13, 17602, 51164], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4154, "seek": 1404064, "start": 14056.64, "end": 14058.64, "text": " minus", "tokens": [51164, 3175, 51264], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4155, "seek": 1404064, "start": 14058.64, "end": 14060.64, "text": " 0.5", "tokens": [51264, 1958, 13, 20, 51364], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4156, "seek": 1404064, "start": 14060.64, "end": 14062.64, "text": " squared so the result of 0.48", "tokens": [51364, 8889, 370, 264, 1874, 295, 1958, 13, 13318, 51464], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4157, "seek": 1404064, "start": 14062.64, "end": 14064.64, "text": " minus 0.5 squared", "tokens": [51464, 3175, 1958, 13, 20, 8889, 51564], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4158, "seek": 1404064, "start": 14064.64, "end": 14066.64, "text": " and what's right equals here", "tokens": [51564, 293, 437, 311, 558, 6915, 510, 51664], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4159, "seek": 1404064, "start": 14066.64, "end": 14068.64, "text": " is going to be", "tokens": [51664, 307, 516, 281, 312, 51764], "temperature": 0.0, "avg_logprob": -0.09901220457894462, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.014059482142329216}, {"id": 4160, "seek": 1406864, "start": 14068.64, "end": 14070.64, "text": " approximately 0.02", "tokens": [50364, 10447, 1958, 13, 12756, 50464], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4161, "seek": 1406864, "start": 14070.64, "end": 14072.64, "text": " squared", "tokens": [50464, 8889, 50564], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4162, "seek": 1406864, "start": 14072.64, "end": 14074.64, "text": " so that would be 0.004", "tokens": [50564, 370, 300, 576, 312, 1958, 13, 628, 19, 50664], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4163, "seek": 1406864, "start": 14074.64, "end": 14076.64, "text": " like that", "tokens": [50664, 411, 300, 50764], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4164, "seek": 1406864, "start": 14076.64, "end": 14078.64, "text": " so I'll make this not actually overlap", "tokens": [50764, 370, 286, 603, 652, 341, 406, 767, 19959, 50864], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4165, "seek": 1406864, "start": 14078.64, "end": 14080.64, "text": " 0.004", "tokens": [50864, 1958, 13, 628, 19, 50964], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4166, "seek": 1406864, "start": 14080.64, "end": 14082.64, "text": " and then this one", "tokens": [50964, 293, 550, 341, 472, 51064], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4167, "seek": 1406864, "start": 14082.64, "end": 14084.64, "text": " we obviously know would be 0", "tokens": [51064, 321, 2745, 458, 576, 312, 1958, 51164], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4168, "seek": 1406864, "start": 14084.64, "end": 14086.64, "text": " because 0.5 minus 0.5", "tokens": [51164, 570, 1958, 13, 20, 3175, 1958, 13, 20, 51264], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4169, "seek": 1406864, "start": 14086.64, "end": 14088.64, "text": " that's 0 then you square 0", "tokens": [51264, 300, 311, 1958, 550, 291, 3732, 1958, 51364], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4170, "seek": 1406864, "start": 14088.64, "end": 14090.64, "text": " still the same thing", "tokens": [51364, 920, 264, 912, 551, 51464], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4171, "seek": 1406864, "start": 14090.64, "end": 14092.64, "text": " and then this one is", "tokens": [51464, 293, 550, 341, 472, 307, 51564], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4172, "seek": 1406864, "start": 14092.64, "end": 14094.64, "text": " 0.0004 as well", "tokens": [51564, 1958, 13, 1360, 19, 382, 731, 51664], "temperature": 0.0, "avg_logprob": -0.1093286815442537, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.0014102786080911756}, {"id": 4173, "seek": 1409464, "start": 14094.64, "end": 14096.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4174, "seek": 1409464, "start": 14098.64, "end": 14100.64, "text": " when we add these two together", "tokens": [50564, 562, 321, 909, 613, 732, 1214, 50664], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4175, "seek": 1409464, "start": 14100.64, "end": 14102.64, "text": " we're going to get", "tokens": [50664, 321, 434, 516, 281, 483, 50764], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4176, "seek": 1409464, "start": 14102.64, "end": 14104.64, "text": " 0.0008", "tokens": [50764, 1958, 13, 1360, 23, 50864], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4177, "seek": 1409464, "start": 14104.64, "end": 14106.64, "text": " just like that", "tokens": [50864, 445, 411, 300, 50964], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4178, "seek": 1409464, "start": 14106.64, "end": 14108.64, "text": " and then if we divide them by 3 or whatever", "tokens": [50964, 293, 550, 498, 321, 9845, 552, 538, 805, 420, 2035, 51064], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4179, "seek": 1409464, "start": 14108.64, "end": 14110.64, "text": " n is", "tokens": [51064, 297, 307, 51164], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4180, "seek": 1409464, "start": 14110.64, "end": 14112.64, "text": " then we end up getting", "tokens": [51164, 550, 321, 917, 493, 1242, 51264], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4181, "seek": 1409464, "start": 14112.64, "end": 14114.64, "text": " 0.00026", "tokens": [51264, 1958, 13, 1360, 10880, 51364], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4182, "seek": 1409464, "start": 14114.64, "end": 14116.64, "text": " repeating so I'll just write", "tokens": [51364, 18617, 370, 286, 603, 445, 2464, 51464], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4183, "seek": 1409464, "start": 14116.64, "end": 14118.64, "text": " 266 like that", "tokens": [51464, 7551, 21, 411, 300, 51564], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4184, "seek": 1409464, "start": 14118.64, "end": 14120.64, "text": " and so", "tokens": [51564, 293, 370, 51664], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4185, "seek": 1409464, "start": 14120.64, "end": 14122.64, "text": " all we have to do at this point", "tokens": [51664, 439, 321, 362, 281, 360, 412, 341, 935, 51764], "temperature": 0.0, "avg_logprob": -0.0847610289400274, "compression_ratio": 1.5225806451612902, "no_speech_prob": 0.0010321600129827857}, {"id": 4186, "seek": 1412264, "start": 14122.64, "end": 14124.64, "text": " is do the", "tokens": [50364, 307, 360, 264, 50464], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4187, "seek": 1412264, "start": 14124.64, "end": 14126.64, "text": " square root of this", "tokens": [50464, 3732, 5593, 295, 341, 50564], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4188, "seek": 1412264, "start": 14126.64, "end": 14128.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4189, "seek": 1412264, "start": 14128.64, "end": 14130.64, "text": " we'll do", "tokens": [50664, 321, 603, 360, 50764], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4190, "seek": 1412264, "start": 14130.64, "end": 14132.64, "text": " square root of 0.00026", "tokens": [50764, 3732, 5593, 295, 1958, 13, 1360, 10880, 50864], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4191, "seek": 1412264, "start": 14132.64, "end": 14134.64, "text": " approximately", "tokens": [50864, 10447, 50964], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4192, "seek": 1412264, "start": 14134.64, "end": 14136.64, "text": " and", "tokens": [50964, 293, 51064], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4193, "seek": 1412264, "start": 14136.64, "end": 14138.64, "text": " that's going to be equal to about", "tokens": [51064, 300, 311, 516, 281, 312, 2681, 281, 466, 51164], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4194, "seek": 1412264, "start": 14138.64, "end": 14140.64, "text": " 0.0163", "tokens": [51164, 1958, 13, 15, 6866, 18, 51264], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4195, "seek": 1412264, "start": 14140.64, "end": 14142.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4196, "seek": 1412264, "start": 14142.64, "end": 14144.64, "text": " that is our", "tokens": [51364, 300, 307, 527, 51464], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4197, "seek": 1412264, "start": 14144.64, "end": 14146.64, "text": " standard deviation", "tokens": [51464, 3832, 25163, 51564], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4198, "seek": 1412264, "start": 14146.64, "end": 14148.64, "text": " of both of these arrays here", "tokens": [51564, 295, 1293, 295, 613, 41011, 510, 51664], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4199, "seek": 1412264, "start": 14148.64, "end": 14150.64, "text": " so 0.048", "tokens": [51664, 370, 1958, 13, 15, 13318, 51764], "temperature": 0.0, "avg_logprob": -0.10709937810897827, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.013844980858266354}, {"id": 4200, "seek": 1415064, "start": 14150.64, "end": 14152.64, "text": " and then 0.52", "tokens": [50364, 293, 550, 1958, 13, 17602, 50464], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4201, "seek": 1415064, "start": 14152.64, "end": 14154.64, "text": " our standard deviation is", "tokens": [50464, 527, 3832, 25163, 307, 50564], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4202, "seek": 1415064, "start": 14154.64, "end": 14156.64, "text": " 0.0163", "tokens": [50564, 1958, 13, 15, 6866, 18, 50664], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4203, "seek": 1415064, "start": 14156.64, "end": 14158.64, "text": " so very small", "tokens": [50664, 370, 588, 1359, 50764], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4204, "seek": 1415064, "start": 14158.64, "end": 14160.64, "text": " and then we have", "tokens": [50764, 293, 550, 321, 362, 50864], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4205, "seek": 1415064, "start": 14160.64, "end": 14162.64, "text": " negative 0.38, 0.52", "tokens": [50864, 3671, 1958, 13, 12625, 11, 1958, 13, 17602, 50964], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4206, "seek": 1415064, "start": 14162.64, "end": 14164.64, "text": " and 2.48", "tokens": [50964, 293, 568, 13, 13318, 51064], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4207, "seek": 1415064, "start": 14164.64, "end": 14166.64, "text": " we get a standard deviation of 1.19", "tokens": [51064, 321, 483, 257, 3832, 25163, 295, 502, 13, 3405, 51164], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4208, "seek": 1415064, "start": 14166.64, "end": 14168.64, "text": " so you can see that these numbers are vastly different", "tokens": [51164, 370, 291, 393, 536, 300, 613, 3547, 366, 41426, 819, 51264], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4209, "seek": 1415064, "start": 14168.64, "end": 14170.64, "text": " one is like", "tokens": [51264, 472, 307, 411, 51364], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4210, "seek": 1415064, "start": 14170.64, "end": 14172.64, "text": " one is literally", "tokens": [51364, 472, 307, 3736, 51464], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4211, "seek": 1415064, "start": 14172.64, "end": 14174.64, "text": " 100 times greater than the other", "tokens": [51464, 2319, 1413, 5044, 813, 264, 661, 51564], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4212, "seek": 1415064, "start": 14174.64, "end": 14176.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4213, "seek": 1415064, "start": 14176.64, "end": 14178.64, "text": " the reason for this is because these", "tokens": [51664, 264, 1778, 337, 341, 307, 570, 613, 51764], "temperature": 0.0, "avg_logprob": -0.07854014751957912, "compression_ratio": 1.582010582010582, "no_speech_prob": 0.0038236044347286224}, {"id": 4214, "seek": 1417864, "start": 14178.64, "end": 14180.64, "text": " numbers are super", "tokens": [50364, 3547, 366, 1687, 50464], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4215, "seek": 1417864, "start": 14180.64, "end": 14182.64, "text": " diverse", "tokens": [50464, 9521, 50564], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4216, "seek": 1417864, "start": 14182.64, "end": 14184.64, "text": " I guess another way", "tokens": [50564, 286, 2041, 1071, 636, 50664], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4217, "seek": 1417864, "start": 14184.64, "end": 14186.64, "text": " you could think of them is that", "tokens": [50664, 291, 727, 519, 295, 552, 307, 300, 50764], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4218, "seek": 1417864, "start": 14186.64, "end": 14188.64, "text": " they stretch out very far from the", "tokens": [50764, 436, 5985, 484, 588, 1400, 490, 264, 50864], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4219, "seek": 1417864, "start": 14188.64, "end": 14190.64, "text": " mean", "tokens": [50864, 914, 50964], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4220, "seek": 1417864, "start": 14190.64, "end": 14192.64, "text": " this essentially means when you're initializing", "tokens": [50964, 341, 4476, 1355, 562, 291, 434, 5883, 3319, 51064], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4221, "seek": 1417864, "start": 14192.64, "end": 14194.64, "text": " your parameters", "tokens": [51064, 428, 9834, 51164], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4222, "seek": 1417864, "start": 14194.64, "end": 14196.64, "text": " that if you have some outliers", "tokens": [51164, 300, 498, 291, 362, 512, 484, 23646, 51264], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4223, "seek": 1417864, "start": 14196.64, "end": 14198.64, "text": " then your network", "tokens": [51264, 550, 428, 3209, 51364], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4224, "seek": 1417864, "start": 14198.64, "end": 14200.64, "text": " is going to be funky", "tokens": [51364, 307, 516, 281, 312, 33499, 51464], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4225, "seek": 1417864, "start": 14200.64, "end": 14202.64, "text": " because it's", "tokens": [51464, 570, 309, 311, 51564], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4226, "seek": 1417864, "start": 14202.64, "end": 14204.64, "text": " the learning process just messed up because you have outliers", "tokens": [51564, 264, 2539, 1399, 445, 16507, 493, 570, 291, 362, 484, 23646, 51664], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4227, "seek": 1417864, "start": 14204.64, "end": 14206.64, "text": " and it's not just learning the right way", "tokens": [51664, 293, 309, 311, 406, 445, 2539, 264, 558, 636, 51764], "temperature": 0.0, "avg_logprob": -0.0771164327564806, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.004396896343678236}, {"id": 4228, "seek": 1420664, "start": 14206.64, "end": 14208.64, "text": " it's supposed to", "tokens": [50364, 309, 311, 3442, 281, 50464], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4229, "seek": 1420664, "start": 14208.64, "end": 14210.64, "text": " whereas if you had", "tokens": [50464, 9735, 498, 291, 632, 50564], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4230, "seek": 1420664, "start": 14210.64, "end": 14212.64, "text": " way too small of a standard deviation", "tokens": [50564, 636, 886, 1359, 295, 257, 3832, 25163, 50664], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4231, "seek": 1420664, "start": 14212.64, "end": 14214.64, "text": " from your initial parameters", "tokens": [50664, 490, 428, 5883, 9834, 50764], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4232, "seek": 1420664, "start": 14214.64, "end": 14216.64, "text": " like in here but maybe even smaller", "tokens": [50764, 411, 294, 510, 457, 1310, 754, 4356, 50864], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4233, "seek": 1420664, "start": 14216.64, "end": 14218.64, "text": " so let's say they were all", "tokens": [50864, 370, 718, 311, 584, 436, 645, 439, 50964], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4234, "seek": 1420664, "start": 14218.64, "end": 14220.64, "text": " 0.5", "tokens": [50964, 1958, 13, 20, 51064], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4235, "seek": 1420664, "start": 14220.64, "end": 14222.64, "text": " then all of your neurons", "tokens": [51064, 550, 439, 295, 428, 22027, 51164], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4236, "seek": 1420664, "start": 14222.64, "end": 14224.64, "text": " would effectively be the same", "tokens": [51164, 576, 8659, 312, 264, 912, 51264], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4237, "seek": 1420664, "start": 14224.64, "end": 14226.64, "text": " and they would all learn the same pattern", "tokens": [51264, 293, 436, 576, 439, 1466, 264, 912, 5102, 51364], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4238, "seek": 1420664, "start": 14226.64, "end": 14228.64, "text": " so then you would have no learning done", "tokens": [51364, 370, 550, 291, 576, 362, 572, 2539, 1096, 51464], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4239, "seek": 1420664, "start": 14228.64, "end": 14230.64, "text": " so one would either be", "tokens": [51464, 370, 472, 576, 2139, 312, 51564], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4240, "seek": 1420664, "start": 14230.64, "end": 14232.64, "text": " you're learning a super super unstable", "tokens": [51564, 291, 434, 2539, 257, 1687, 1687, 23742, 51664], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4241, "seek": 1420664, "start": 14232.64, "end": 14234.64, "text": " and you have outliers that are", "tokens": [51664, 293, 291, 362, 484, 23646, 300, 366, 51764], "temperature": 0.0, "avg_logprob": -0.06631836590466199, "compression_ratio": 1.7654867256637168, "no_speech_prob": 0.01541581004858017}, {"id": 4242, "seek": 1423464, "start": 14234.64, "end": 14236.64, "text": " just learning", "tokens": [50364, 445, 2539, 50464], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4243, "seek": 1423464, "start": 14236.64, "end": 14238.64, "text": " very distinct things and not really", "tokens": [50464, 588, 10644, 721, 293, 406, 534, 50564], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4244, "seek": 1423464, "start": 14238.64, "end": 14240.64, "text": " not really", "tokens": [50564, 406, 534, 50664], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4245, "seek": 1423464, "start": 14240.64, "end": 14242.64, "text": " not really letting other neurons", "tokens": [50664, 406, 534, 8295, 661, 22027, 50764], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4246, "seek": 1423464, "start": 14242.64, "end": 14244.64, "text": " get opportunities to learn", "tokens": [50764, 483, 4786, 281, 1466, 50864], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4247, "seek": 1423464, "start": 14244.64, "end": 14246.64, "text": " or rather other parameters to learn", "tokens": [50864, 420, 2831, 661, 9834, 281, 1466, 50964], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4248, "seek": 1423464, "start": 14248.64, "end": 14250.64, "text": " if you have a lot of diversity", "tokens": [51064, 498, 291, 362, 257, 688, 295, 8811, 51164], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4249, "seek": 1423464, "start": 14250.64, "end": 14252.64, "text": " you just have outliers and then if you have", "tokens": [51164, 291, 445, 362, 484, 23646, 293, 550, 498, 291, 362, 51264], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4250, "seek": 1423464, "start": 14252.64, "end": 14254.64, "text": " no", "tokens": [51264, 572, 51364], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4251, "seek": 1423464, "start": 14254.64, "end": 14256.64, "text": " diversity at all then", "tokens": [51364, 8811, 412, 439, 550, 51464], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4252, "seek": 1423464, "start": 14256.64, "end": 14258.64, "text": " essentially nothing is learned and your network", "tokens": [51464, 4476, 1825, 307, 3264, 293, 428, 3209, 51564], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4253, "seek": 1423464, "start": 14258.64, "end": 14260.64, "text": " is useless so all we want to do", "tokens": [51564, 307, 14115, 370, 439, 321, 528, 281, 360, 51664], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4254, "seek": 1423464, "start": 14260.64, "end": 14262.64, "text": " is make sure that our standard deviation", "tokens": [51664, 307, 652, 988, 300, 527, 3832, 25163, 51764], "temperature": 0.0, "avg_logprob": -0.07576007449749819, "compression_ratio": 1.904040404040404, "no_speech_prob": 0.01690274104475975}, {"id": 4255, "seek": 1426264, "start": 14262.64, "end": 14264.64, "text": " is balanced and stable", "tokens": [50364, 307, 13902, 293, 8351, 50464], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4256, "seek": 1426264, "start": 14264.64, "end": 14266.64, "text": " so that the training process", "tokens": [50464, 370, 300, 264, 3097, 1399, 50564], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4257, "seek": 1426264, "start": 14266.64, "end": 14268.64, "text": " can learn effective things", "tokens": [50564, 393, 1466, 4942, 721, 50664], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4258, "seek": 1426264, "start": 14268.64, "end": 14270.64, "text": " so each neuron can learn a little bit", "tokens": [50664, 370, 1184, 34090, 393, 1466, 257, 707, 857, 50764], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4259, "seek": 1426264, "start": 14270.64, "end": 14272.64, "text": " so you can see here", "tokens": [50764, 370, 291, 393, 536, 510, 50864], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4260, "seek": 1426264, "start": 14272.64, "end": 14274.64, "text": " this would probably be an okay standard deviation", "tokens": [50864, 341, 576, 1391, 312, 364, 1392, 3832, 25163, 50964], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4261, "seek": 1426264, "start": 14274.64, "end": 14276.64, "text": " if these were some parameters because", "tokens": [50964, 498, 613, 645, 512, 9834, 570, 51064], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4262, "seek": 1426264, "start": 14276.64, "end": 14278.64, "text": " they're a little bit different than each other", "tokens": [51064, 436, 434, 257, 707, 857, 819, 813, 1184, 661, 51164], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4263, "seek": 1426264, "start": 14278.64, "end": 14280.64, "text": " they're not all like super super", "tokens": [51164, 436, 434, 406, 439, 411, 1687, 1687, 51264], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4264, "seek": 1426264, "start": 14280.64, "end": 14282.64, "text": " close to the same", "tokens": [51264, 1998, 281, 264, 912, 51364], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4265, "seek": 1426264, "start": 14282.64, "end": 14284.64, "text": " and yeah", "tokens": [51364, 293, 1338, 51464], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4266, "seek": 1426264, "start": 14284.64, "end": 14286.64, "text": " so essentially what", "tokens": [51464, 370, 4476, 437, 51564], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4267, "seek": 1426264, "start": 14286.64, "end": 14288.64, "text": " this looks like in", "tokens": [51564, 341, 1542, 411, 294, 51664], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4268, "seek": 1426264, "start": 14288.64, "end": 14290.64, "text": " code here is the following", "tokens": [51664, 3089, 510, 307, 264, 3480, 51764], "temperature": 0.0, "avg_logprob": -0.07217309108147255, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.013840461149811745}, {"id": 4269, "seek": 1429064, "start": 14290.64, "end": 14292.64, "text": " so you don't actually need to", "tokens": [50364, 370, 291, 500, 380, 767, 643, 281, 50464], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4270, "seek": 1429064, "start": 14292.64, "end": 14294.64, "text": " memorize what this does as it's", "tokens": [50464, 27478, 437, 341, 775, 382, 309, 311, 50564], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4271, "seek": 1429064, "start": 14294.64, "end": 14296.64, "text": " just used in practice", "tokens": [50564, 445, 1143, 294, 3124, 50664], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4272, "seek": 1429064, "start": 14296.64, "end": 14298.64, "text": " by professionals", "tokens": [50664, 538, 11954, 50764], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4273, "seek": 1429064, "start": 14298.64, "end": 14300.64, "text": " but essentially what this does", "tokens": [50764, 457, 4476, 437, 341, 775, 50864], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4274, "seek": 1429064, "start": 14300.64, "end": 14302.64, "text": " is it initializes our weights", "tokens": [50864, 307, 309, 5883, 5660, 527, 17443, 50964], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4275, "seek": 1429064, "start": 14302.64, "end": 14304.64, "text": " around certain standard deviations", "tokens": [50964, 926, 1629, 3832, 31219, 763, 51064], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4276, "seek": 1429064, "start": 14304.64, "end": 14306.64, "text": " so here we set it to 0.02", "tokens": [51064, 370, 510, 321, 992, 309, 281, 1958, 13, 12756, 51164], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4277, "seek": 1429064, "start": 14306.64, "end": 14308.64, "text": " which is pretty much the same", "tokens": [51164, 597, 307, 1238, 709, 264, 912, 51264], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4278, "seek": 1429064, "start": 14308.64, "end": 14310.64, "text": " as what we had in here", "tokens": [51264, 382, 437, 321, 632, 294, 510, 51364], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4279, "seek": 1429064, "start": 14310.64, "end": 14312.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4280, "seek": 1429064, "start": 14312.64, "end": 14314.64, "text": " point", "tokens": [51464, 935, 51564], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4281, "seek": 1429064, "start": 14314.64, "end": 14316.64, "text": " point", "tokens": [51564, 935, 51664], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4282, "seek": 1429064, "start": 14316.64, "end": 14318.64, "text": " this one's a little bit off in the standard deviation", "tokens": [51664, 341, 472, 311, 257, 707, 857, 766, 294, 264, 3832, 25163, 51764], "temperature": 0.0, "avg_logprob": -0.05860261161728661, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.0015484662726521492}, {"id": 4283, "seek": 1431864, "start": 14318.64, "end": 14320.64, "text": " set here", "tokens": [50364, 992, 510, 50464], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4284, "seek": 1431864, "start": 14320.64, "end": 14322.64, "text": " but essentially", "tokens": [50464, 457, 4476, 50564], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4285, "seek": 1431864, "start": 14322.64, "end": 14324.64, "text": " we're just making sure that our weights", "tokens": [50564, 321, 434, 445, 1455, 988, 300, 527, 17443, 50664], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4286, "seek": 1431864, "start": 14324.64, "end": 14326.64, "text": " are initialized properly", "tokens": [50664, 366, 5883, 1602, 6108, 50764], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4287, "seek": 1431864, "start": 14326.64, "end": 14328.64, "text": " and you don't have to memorize this at all", "tokens": [50764, 293, 291, 500, 380, 362, 281, 27478, 341, 412, 439, 50864], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4288, "seek": 1431864, "start": 14328.64, "end": 14330.64, "text": " it's just used in practice and it's going to help our training", "tokens": [50864, 309, 311, 445, 1143, 294, 3124, 293, 309, 311, 516, 281, 854, 527, 3097, 50964], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4289, "seek": 1431864, "start": 14330.64, "end": 14332.64, "text": " converge better", "tokens": [50964, 41881, 1101, 51064], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4290, "seek": 1431864, "start": 14332.64, "end": 14334.64, "text": " so as long as you understand", "tokens": [51064, 370, 382, 938, 382, 291, 1223, 51164], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4291, "seek": 1431864, "start": 14334.64, "end": 14336.64, "text": " that we can apply some initializations", "tokens": [51164, 300, 321, 393, 3079, 512, 5883, 14455, 51264], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4292, "seek": 1431864, "start": 14336.64, "end": 14338.64, "text": " on our weights", "tokens": [51264, 322, 527, 17443, 51364], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4293, "seek": 1431864, "start": 14338.64, "end": 14340.64, "text": " that's all that really matters, so cool", "tokens": [51364, 300, 311, 439, 300, 534, 7001, 11, 370, 1627, 51464], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4294, "seek": 1431864, "start": 14340.64, "end": 14342.64, "text": " let's move on to the next part", "tokens": [51464, 718, 311, 1286, 322, 281, 264, 958, 644, 51564], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4295, "seek": 1431864, "start": 14342.64, "end": 14344.64, "text": " of our GBT architecture", "tokens": [51564, 295, 527, 26809, 51, 9482, 51664], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4296, "seek": 1431864, "start": 14344.64, "end": 14346.64, "text": " so awesome, we finished this GBT language", "tokens": [51664, 370, 3476, 11, 321, 4335, 341, 26809, 51, 2856, 51764], "temperature": 0.0, "avg_logprob": -0.08141501611020385, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.004197385627776384}, {"id": 4297, "seek": 1434664, "start": 14346.64, "end": 14348.64, "text": " class, everything's pretty much done here", "tokens": [50364, 1508, 11, 1203, 311, 1238, 709, 1096, 510, 50464], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4298, "seek": 1434664, "start": 14348.64, "end": 14350.64, "text": " we did our knit", "tokens": [50464, 321, 630, 527, 15594, 50564], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4299, "seek": 1434664, "start": 14350.64, "end": 14352.64, "text": " we did some weight initializations", "tokens": [50564, 321, 630, 512, 3364, 5883, 14455, 50664], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4300, "seek": 1434664, "start": 14352.64, "end": 14354.64, "text": " and we did our forward pass, so awesome", "tokens": [50664, 293, 321, 630, 527, 2128, 1320, 11, 370, 3476, 50764], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4301, "seek": 1434664, "start": 14354.64, "end": 14356.64, "text": " that's all done, now let's move on to the next", "tokens": [50764, 300, 311, 439, 1096, 11, 586, 718, 311, 1286, 322, 281, 264, 958, 50864], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4302, "seek": 1434664, "start": 14356.64, "end": 14358.64, "text": " which is the", "tokens": [50864, 597, 307, 264, 50964], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4303, "seek": 1434664, "start": 14358.64, "end": 14360.64, "text": " block class", "tokens": [50964, 3461, 1508, 51064], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4304, "seek": 1434664, "start": 14360.64, "end": 14362.64, "text": " so what is block?", "tokens": [51064, 370, 437, 307, 3461, 30, 51164], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4305, "seek": 1434664, "start": 14362.64, "end": 14364.64, "text": " well, if we go back to this diagram", "tokens": [51164, 731, 11, 498, 321, 352, 646, 281, 341, 10686, 51264], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4306, "seek": 1434664, "start": 14364.64, "end": 14366.64, "text": " each of these decoder blocks is a block", "tokens": [51264, 1184, 295, 613, 979, 19866, 8474, 307, 257, 3461, 51364], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4307, "seek": 1434664, "start": 14366.64, "end": 14368.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4308, "seek": 1434664, "start": 14368.64, "end": 14370.64, "text": " we're pretty much just going to fill in this gap here", "tokens": [51464, 321, 434, 1238, 709, 445, 516, 281, 2836, 294, 341, 7417, 510, 51564], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4309, "seek": 1434664, "start": 14370.64, "end": 14372.64, "text": " our GBT language model has these two", "tokens": [51564, 527, 26809, 51, 2856, 2316, 575, 613, 732, 51664], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4310, "seek": 1434664, "start": 14372.64, "end": 14374.64, "text": " where we get our tokenized inputs", "tokens": [51664, 689, 321, 483, 527, 14862, 1602, 15743, 51764], "temperature": 0.0, "avg_logprob": -0.07605710105290489, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.010164624080061913}, {"id": 4311, "seek": 1437464, "start": 14374.64, "end": 14376.64, "text": " and then we do some transformations", "tokens": [50364, 293, 550, 321, 360, 512, 34852, 50464], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4312, "seek": 1437464, "start": 14376.64, "end": 14378.64, "text": " and the softmax after", "tokens": [50464, 293, 264, 2787, 41167, 934, 50564], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4313, "seek": 1437464, "start": 14378.64, "end": 14380.64, "text": " and essentially we're just filling", "tokens": [50564, 293, 4476, 321, 434, 445, 10623, 50664], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4314, "seek": 1437464, "start": 14380.64, "end": 14382.64, "text": " in this gap here and then we're going to build out", "tokens": [50664, 294, 341, 7417, 510, 293, 550, 321, 434, 516, 281, 1322, 484, 50764], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4315, "seek": 1437464, "start": 14382.64, "end": 14384.64, "text": " and just sort of branch out until it's", "tokens": [50764, 293, 445, 1333, 295, 9819, 484, 1826, 309, 311, 50864], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4316, "seek": 1437464, "start": 14384.64, "end": 14386.64, "text": " completely built", "tokens": [50864, 2584, 3094, 50964], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4317, "seek": 1437464, "start": 14386.64, "end": 14388.64, "text": " so let's go ahead and build these blocks here", "tokens": [50964, 370, 718, 311, 352, 2286, 293, 1322, 613, 8474, 510, 51064], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4318, "seek": 1437464, "start": 14388.64, "end": 14390.64, "text": " what does this look like?", "tokens": [51064, 437, 775, 341, 574, 411, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4319, "seek": 1437464, "start": 14390.64, "end": 14392.64, "text": " that's what this does", "tokens": [51164, 300, 311, 437, 341, 775, 51264], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4320, "seek": 1437464, "start": 14392.64, "end": 14394.64, "text": " so we have our knit, we have a forward pass", "tokens": [51264, 370, 321, 362, 527, 15594, 11, 321, 362, 257, 2128, 1320, 51364], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4321, "seek": 1437464, "start": 14394.64, "end": 14396.64, "text": " as per usual", "tokens": [51364, 382, 680, 7713, 51464], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4322, "seek": 1437464, "start": 14396.64, "end": 14398.64, "text": " and knit", "tokens": [51464, 293, 15594, 51564], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4323, "seek": 1437464, "start": 14398.64, "end": 14400.64, "text": " and a forward pass as seen", "tokens": [51564, 293, 257, 2128, 1320, 382, 1612, 51664], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4324, "seek": 1437464, "start": 14400.64, "end": 14402.64, "text": " in the GBT language model class", "tokens": [51664, 294, 264, 26809, 51, 2856, 2316, 1508, 51764], "temperature": 0.0, "avg_logprob": -0.06308183272679647, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0010985955595970154}, {"id": 4325, "seek": 1440264, "start": 14402.64, "end": 14404.64, "text": " which is going to look like this", "tokens": [50364, 597, 307, 516, 281, 574, 411, 341, 50464], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4326, "seek": 1440264, "start": 14404.64, "end": 14406.64, "text": " forward and an init", "tokens": [50464, 2128, 293, 364, 3157, 50564], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4327, "seek": 1440264, "start": 14406.64, "end": 14408.64, "text": " so the init", "tokens": [50564, 370, 264, 3157, 50664], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4328, "seek": 1440264, "start": 14408.64, "end": 14410.64, "text": " is going to just initialize some things", "tokens": [50664, 307, 516, 281, 445, 5883, 1125, 512, 721, 50764], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4329, "seek": 1440264, "start": 14410.64, "end": 14412.64, "text": " it's going to initialize some transformations", "tokens": [50764, 309, 311, 516, 281, 5883, 1125, 512, 34852, 50864], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4330, "seek": 1440264, "start": 14412.64, "end": 14414.64, "text": " and some things that we're going to do in the forward pass", "tokens": [50864, 293, 512, 721, 300, 321, 434, 516, 281, 360, 294, 264, 2128, 1320, 50964], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4331, "seek": 1440264, "start": 14414.64, "end": 14416.64, "text": " that's all it's doing", "tokens": [50964, 300, 311, 439, 309, 311, 884, 51064], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4332, "seek": 1440264, "start": 14416.64, "end": 14418.64, "text": " so what do we do first?", "tokens": [51064, 370, 437, 360, 321, 360, 700, 30, 51164], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4333, "seek": 1440264, "start": 14418.64, "end": 14420.64, "text": " well we have this new head size", "tokens": [51164, 731, 321, 362, 341, 777, 1378, 2744, 51264], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4334, "seek": 1440264, "start": 14420.64, "end": 14422.64, "text": " parameter introduced", "tokens": [51264, 13075, 7268, 51364], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4335, "seek": 1440264, "start": 14422.64, "end": 14424.64, "text": " so head size is the number of features", "tokens": [51364, 370, 1378, 2744, 307, 264, 1230, 295, 4122, 51464], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4336, "seek": 1440264, "start": 14424.64, "end": 14426.64, "text": " that each head will be capturing", "tokens": [51464, 300, 1184, 1378, 486, 312, 23384, 51564], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4337, "seek": 1440264, "start": 14426.64, "end": 14428.64, "text": " in our multi-head attention", "tokens": [51564, 294, 527, 4825, 12, 1934, 3202, 51664], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4338, "seek": 1440264, "start": 14428.64, "end": 14430.64, "text": " so all the heads in parallel", "tokens": [51664, 370, 439, 264, 8050, 294, 8952, 51764], "temperature": 0.0, "avg_logprob": -0.08818744627897404, "compression_ratio": 1.908296943231441, "no_speech_prob": 0.0028441997710615396}, {"id": 4339, "seek": 1443064, "start": 14430.64, "end": 14432.64, "text": " features are each of them capturing", "tokens": [50364, 4122, 366, 1184, 295, 552, 23384, 50464], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4340, "seek": 1443064, "start": 14432.64, "end": 14434.64, "text": " so we do that by dividing", "tokens": [50464, 370, 321, 360, 300, 538, 26764, 50564], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4341, "seek": 1443064, "start": 14434.64, "end": 14436.64, "text": " n embed by n head", "tokens": [50564, 297, 12240, 538, 297, 1378, 50664], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4342, "seek": 1443064, "start": 14436.64, "end": 14438.64, "text": " so n head is the number", "tokens": [50664, 370, 297, 1378, 307, 264, 1230, 50764], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4343, "seek": 1443064, "start": 14438.64, "end": 14440.64, "text": " of heads we have", "tokens": [50764, 295, 8050, 321, 362, 50864], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4344, "seek": 1443064, "start": 14440.64, "end": 14442.64, "text": " and n embed is the number of features we have", "tokens": [50864, 293, 297, 12240, 307, 264, 1230, 295, 4122, 321, 362, 50964], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4345, "seek": 1443064, "start": 14442.64, "end": 14444.64, "text": " where we're capturing", "tokens": [50964, 689, 321, 434, 23384, 51064], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4346, "seek": 1443064, "start": 14444.64, "end": 14446.64, "text": " so 384 features divided by 4 heads", "tokens": [51064, 370, 12843, 19, 4122, 6666, 538, 1017, 8050, 51164], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4347, "seek": 1443064, "start": 14446.64, "end": 14448.64, "text": " so each head is going to be capturing", "tokens": [51164, 370, 1184, 1378, 307, 516, 281, 312, 23384, 51264], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4348, "seek": 1443064, "start": 14448.64, "end": 14450.64, "text": " 96 features", "tokens": [51264, 24124, 4122, 51364], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4349, "seek": 1443064, "start": 14450.64, "end": 14452.64, "text": " hence head size", "tokens": [51364, 16678, 1378, 2744, 51464], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4350, "seek": 1443064, "start": 14452.64, "end": 14454.64, "text": " so", "tokens": [51464, 370, 51564], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4351, "seek": 1443064, "start": 14454.64, "end": 14456.64, "text": " next up we have self.sa", "tokens": [51564, 958, 493, 321, 362, 2698, 13, 5790, 51664], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4352, "seek": 1443064, "start": 14456.64, "end": 14458.64, "text": " which is just short for self-attention", "tokens": [51664, 597, 307, 445, 2099, 337, 2698, 12, 1591, 1251, 51764], "temperature": 0.0, "avg_logprob": -0.11178416505866094, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.0066909827291965485}, {"id": 4353, "seek": 1445864, "start": 14458.64, "end": 14460.64, "text": " we do a multi-head attention", "tokens": [50364, 321, 360, 257, 4825, 12, 1934, 3202, 50464], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4354, "seek": 1445864, "start": 14460.64, "end": 14462.64, "text": " we pass in our n head", "tokens": [50464, 321, 1320, 294, 527, 297, 1378, 50564], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4355, "seek": 1445864, "start": 14462.64, "end": 14464.64, "text": " and our head size and you'll see how these", "tokens": [50564, 293, 527, 1378, 2744, 293, 291, 603, 536, 577, 613, 50664], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4356, "seek": 1445864, "start": 14464.64, "end": 14466.64, "text": " parameters fit in later", "tokens": [50664, 9834, 3318, 294, 1780, 50764], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4357, "seek": 1445864, "start": 14466.64, "end": 14468.64, "text": " once we build up this multi-head attention", "tokens": [50764, 1564, 321, 1322, 493, 341, 4825, 12, 1934, 3202, 50864], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4358, "seek": 1445864, "start": 14468.64, "end": 14470.64, "text": " class so cool", "tokens": [50864, 1508, 370, 1627, 50964], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4359, "seek": 1445864, "start": 14470.64, "end": 14472.64, "text": " now we have a feed forward", "tokens": [50964, 586, 321, 362, 257, 3154, 2128, 51064], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4360, "seek": 1445864, "start": 14472.64, "end": 14474.64, "text": " which is as explained", "tokens": [51064, 597, 307, 382, 8825, 51164], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4361, "seek": 1445864, "start": 14474.64, "end": 14476.64, "text": " just in the diagram here", "tokens": [51164, 445, 294, 264, 10686, 510, 51264], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4362, "seek": 1445864, "start": 14476.64, "end": 14478.64, "text": " our feed forward is just this", "tokens": [51264, 527, 3154, 2128, 307, 445, 341, 51364], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4363, "seek": 1445864, "start": 14478.64, "end": 14480.64, "text": " which we're actually going to build out next", "tokens": [51364, 597, 321, 434, 767, 516, 281, 1322, 484, 958, 51464], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4364, "seek": 1445864, "start": 14482.64, "end": 14484.64, "text": " and we have two layer norms", "tokens": [51564, 293, 321, 362, 732, 4583, 24357, 51664], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4365, "seek": 1445864, "start": 14484.64, "end": 14486.64, "text": " and these are just for the", "tokens": [51664, 293, 613, 366, 445, 337, 264, 51764], "temperature": 0.0, "avg_logprob": -0.0844395663760124, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.022277168929576874}, {"id": 4366, "seek": 1448664, "start": 14486.64, "end": 14488.64, "text": " post norm", "tokens": [50364, 2183, 2026, 50464], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4367, "seek": 1448664, "start": 14488.64, "end": 14490.64, "text": " pre norm architecture that we could implement here", "tokens": [50464, 659, 2026, 9482, 300, 321, 727, 4445, 510, 50564], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4368, "seek": 1448664, "start": 14490.64, "end": 14492.64, "text": " in this case it's going to be", "tokens": [50564, 294, 341, 1389, 309, 311, 516, 281, 312, 50664], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4369, "seek": 1448664, "start": 14492.64, "end": 14494.64, "text": " post norm just because", "tokens": [50664, 2183, 2026, 445, 570, 50764], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4370, "seek": 1448664, "start": 14494.64, "end": 14496.64, "text": " I found that it converges better for this", "tokens": [50764, 286, 1352, 300, 309, 9652, 2880, 1101, 337, 341, 50864], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4371, "seek": 1448664, "start": 14496.64, "end": 14498.64, "text": " for this course and the data that we're using", "tokens": [50864, 337, 341, 1164, 293, 264, 1412, 300, 321, 434, 1228, 50964], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4372, "seek": 1448664, "start": 14498.64, "end": 14500.64, "text": " and just the model parameters", "tokens": [50964, 293, 445, 264, 2316, 9834, 51064], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4373, "seek": 1448664, "start": 14500.64, "end": 14502.64, "text": " and what not it just works better", "tokens": [51064, 293, 437, 406, 309, 445, 1985, 1101, 51164], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4374, "seek": 1448664, "start": 14502.64, "end": 14504.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4375, "seek": 1448664, "start": 14504.64, "end": 14506.64, "text": " also that is the original", "tokens": [51264, 611, 300, 307, 264, 3380, 51364], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4376, "seek": 1448664, "start": 14506.64, "end": 14508.64, "text": " architecture that we use in the", "tokens": [51364, 9482, 300, 321, 764, 294, 264, 51464], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4377, "seek": 1448664, "start": 14508.64, "end": 14510.64, "text": " attention paper", "tokens": [51464, 3202, 3035, 51564], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4378, "seek": 1448664, "start": 14510.64, "end": 14512.64, "text": " so you might have seen that they do an add a norm", "tokens": [51564, 370, 291, 1062, 362, 1612, 300, 436, 360, 364, 909, 257, 2026, 51664], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4379, "seek": 1448664, "start": 14512.64, "end": 14514.64, "text": " rather than a norm and add", "tokens": [51664, 2831, 813, 257, 2026, 293, 909, 51764], "temperature": 0.0, "avg_logprob": -0.10657098189644192, "compression_ratio": 1.8622222222222222, "no_speech_prob": 0.008184040896594524}, {"id": 4380, "seek": 1451464, "start": 14514.64, "end": 14516.64, "text": " anyways", "tokens": [50364, 13448, 50464], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4381, "seek": 1451464, "start": 14516.64, "end": 14518.64, "text": " we've initialized all of these", "tokens": [50464, 321, 600, 5883, 1602, 439, 295, 613, 50564], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4382, "seek": 1451464, "start": 14518.64, "end": 14520.64, "text": " so we have head size, self attention", "tokens": [50564, 370, 321, 362, 1378, 2744, 11, 2698, 3202, 50664], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4383, "seek": 1451464, "start": 14520.64, "end": 14522.64, "text": " feed forward and then two layer norms", "tokens": [50664, 3154, 2128, 293, 550, 732, 4583, 24357, 50764], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4384, "seek": 1451464, "start": 14522.64, "end": 14524.64, "text": " so in our forward pass", "tokens": [50764, 370, 294, 527, 2128, 1320, 50864], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4385, "seek": 1451464, "start": 14524.64, "end": 14526.64, "text": " we do our self attention first", "tokens": [50864, 321, 360, 527, 2698, 3202, 700, 50964], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4386, "seek": 1451464, "start": 14526.64, "end": 14528.64, "text": " let's actually go back to here", "tokens": [50964, 718, 311, 767, 352, 646, 281, 510, 51064], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4387, "seek": 1451464, "start": 14528.64, "end": 14530.64, "text": " so we do our self attention", "tokens": [51064, 370, 321, 360, 527, 2698, 3202, 51164], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4388, "seek": 1451464, "start": 14530.64, "end": 14532.64, "text": " then add a norm", "tokens": [51164, 550, 909, 257, 2026, 51264], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4389, "seek": 1451464, "start": 14532.64, "end": 14534.64, "text": " then a feed forward and then add a norm again", "tokens": [51264, 550, 257, 3154, 2128, 293, 550, 909, 257, 2026, 797, 51364], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4390, "seek": 1451464, "start": 14536.64, "end": 14538.64, "text": " so what does this look like", "tokens": [51464, 370, 437, 775, 341, 574, 411, 51564], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4391, "seek": 1451464, "start": 14538.64, "end": 14540.64, "text": " self attention, add a norm", "tokens": [51564, 2698, 3202, 11, 909, 257, 2026, 51664], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4392, "seek": 1451464, "start": 14540.64, "end": 14542.64, "text": " feed forward, add a norm", "tokens": [51664, 3154, 2128, 11, 909, 257, 2026, 51764], "temperature": 0.0, "avg_logprob": -0.09231354365839023, "compression_ratio": 2.090909090909091, "no_speech_prob": 0.0013043716317042708}, {"id": 4393, "seek": 1454264, "start": 14542.64, "end": 14544.64, "text": " cool", "tokens": [50364, 1627, 50464], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4394, "seek": 1454264, "start": 14544.64, "end": 14546.64, "text": " so we're doing an add so we're going", "tokens": [50464, 370, 321, 434, 884, 364, 909, 370, 321, 434, 516, 50564], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4395, "seek": 1454264, "start": 14546.64, "end": 14548.64, "text": " x plus the previous", "tokens": [50564, 2031, 1804, 264, 3894, 50664], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4396, "seek": 1454264, "start": 14548.64, "end": 14550.64, "text": " answer which is adding them together", "tokens": [50664, 1867, 597, 307, 5127, 552, 1214, 50764], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4397, "seek": 1454264, "start": 14550.64, "end": 14552.64, "text": " and then we're just applying a layer norm to this", "tokens": [50764, 293, 550, 321, 434, 445, 9275, 257, 4583, 2026, 281, 341, 50864], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4398, "seek": 1454264, "start": 14552.64, "end": 14554.64, "text": " so cool", "tokens": [50864, 370, 1627, 50964], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4399, "seek": 1454264, "start": 14554.64, "end": 14556.64, "text": " if you want to look up more into what layer norm does", "tokens": [50964, 498, 291, 528, 281, 574, 493, 544, 666, 437, 4583, 2026, 775, 51064], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4400, "seek": 1454264, "start": 14556.64, "end": 14558.64, "text": " and everything and why it's so useful", "tokens": [51064, 293, 1203, 293, 983, 309, 311, 370, 4420, 51164], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4401, "seek": 1454264, "start": 14558.64, "end": 14560.64, "text": " you can totally go out of your way to do that", "tokens": [51164, 291, 393, 3879, 352, 484, 295, 428, 636, 281, 360, 300, 51264], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4402, "seek": 1454264, "start": 14560.64, "end": 14562.64, "text": " but", "tokens": [51264, 457, 51364], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4403, "seek": 1454264, "start": 14562.64, "end": 14564.64, "text": " layer norm is essentially just going to", "tokens": [51364, 4583, 2026, 307, 4476, 445, 516, 281, 51464], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4404, "seek": 1454264, "start": 14564.64, "end": 14566.64, "text": " help smoothen out our features", "tokens": [51464, 854, 5508, 268, 484, 527, 4122, 51564], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4405, "seek": 1454264, "start": 14566.64, "end": 14568.64, "text": " here", "tokens": [51564, 510, 51664], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4406, "seek": 1454264, "start": 14568.64, "end": 14570.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.07780536445411476, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.009264327585697174}, {"id": 4407, "seek": 1457064, "start": 14570.64, "end": 14572.64, "text": " and honestly there's not much else to that", "tokens": [50364, 293, 6095, 456, 311, 406, 709, 1646, 281, 300, 50464], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4408, "seek": 1457064, "start": 14572.64, "end": 14574.64, "text": " we just return this final value here", "tokens": [50464, 321, 445, 2736, 341, 2572, 2158, 510, 50564], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4409, "seek": 1457064, "start": 14574.64, "end": 14576.64, "text": " and that's pretty much the output of our blocks", "tokens": [50564, 293, 300, 311, 1238, 709, 264, 5598, 295, 527, 8474, 50664], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4410, "seek": 1457064, "start": 14576.64, "end": 14578.64, "text": " so", "tokens": [50664, 370, 50764], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4411, "seek": 1457064, "start": 14578.64, "end": 14580.64, "text": " next up I'm going to add", "tokens": [50764, 958, 493, 286, 478, 516, 281, 909, 50864], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4412, "seek": 1457064, "start": 14580.64, "end": 14582.64, "text": " a new little code block here", "tokens": [50864, 257, 777, 707, 3089, 3461, 510, 50964], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4413, "seek": 1457064, "start": 14582.64, "end": 14584.64, "text": " which is going to be", "tokens": [50964, 597, 307, 516, 281, 312, 51064], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4414, "seek": 1457064, "start": 14584.64, "end": 14586.64, "text": " our feed forward", "tokens": [51064, 527, 3154, 2128, 51164], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4415, "seek": 1457064, "start": 14586.64, "end": 14588.64, "text": " so let's go ahead and do that", "tokens": [51164, 370, 718, 311, 352, 2286, 293, 360, 300, 51264], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4416, "seek": 1457064, "start": 14588.64, "end": 14590.64, "text": " so feed forward, it's just going to look exactly like this", "tokens": [51264, 370, 3154, 2128, 11, 309, 311, 445, 516, 281, 574, 2293, 411, 341, 51364], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4417, "seek": 1457064, "start": 14590.64, "end": 14592.64, "text": " it's actually quite simple", "tokens": [51364, 309, 311, 767, 1596, 2199, 51464], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4418, "seek": 1457064, "start": 14592.64, "end": 14594.64, "text": " so all we do is we make an nn dot sequential", "tokens": [51464, 370, 439, 321, 360, 307, 321, 652, 364, 297, 77, 5893, 42881, 51564], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4419, "seek": 1457064, "start": 14594.64, "end": 14596.64, "text": " torch dot nn", "tokens": [51564, 27822, 5893, 297, 77, 51664], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4420, "seek": 1457064, "start": 14596.64, "end": 14598.64, "text": " we make this a sequential network of linear", "tokens": [51664, 321, 652, 341, 257, 42881, 3209, 295, 8213, 51764], "temperature": 0.0, "avg_logprob": -0.07367147505283356, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0006877542473375797}, {"id": 4421, "seek": 1459864, "start": 14598.64, "end": 14600.64, "text": " linear, relu, and then linear", "tokens": [50364, 8213, 11, 1039, 84, 11, 293, 550, 8213, 50464], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4422, "seek": 1459864, "start": 14600.64, "end": 14602.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4423, "seek": 1459864, "start": 14602.64, "end": 14604.64, "text": " in our linear", "tokens": [50564, 294, 527, 8213, 50664], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4424, "seek": 1459864, "start": 14604.64, "end": 14606.64, "text": " we have to pay attention to the shapes here", "tokens": [50664, 321, 362, 281, 1689, 3202, 281, 264, 10854, 510, 50764], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4425, "seek": 1459864, "start": 14606.64, "end": 14608.64, "text": " so we have n embed", "tokens": [50764, 370, 321, 362, 297, 12240, 50864], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4426, "seek": 1459864, "start": 14608.64, "end": 14610.64, "text": " and then n embed times 4", "tokens": [50864, 293, 550, 297, 12240, 1413, 1017, 50964], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4427, "seek": 1459864, "start": 14610.64, "end": 14612.64, "text": " and then the relu will just", "tokens": [50964, 293, 550, 264, 1039, 84, 486, 445, 51064], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4428, "seek": 1459864, "start": 14612.64, "end": 14614.64, "text": " essentially", "tokens": [51064, 4476, 51164], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4429, "seek": 1459864, "start": 14614.64, "end": 14616.64, "text": " what the relu will do is it looks like this", "tokens": [51164, 437, 264, 1039, 84, 486, 360, 307, 309, 1542, 411, 341, 51264], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4430, "seek": 1459864, "start": 14616.64, "end": 14618.64, "text": " let me illustrate this for you guys", "tokens": [51264, 718, 385, 23221, 341, 337, 291, 1074, 51364], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4431, "seek": 1459864, "start": 14618.64, "end": 14620.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4432, "seek": 1459864, "start": 14620.64, "end": 14622.64, "text": " essentially you have this graph here", "tokens": [51464, 4476, 291, 362, 341, 4295, 510, 51564], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4433, "seek": 1459864, "start": 14624.64, "end": 14626.64, "text": " and", "tokens": [51664, 293, 51764], "temperature": 0.0, "avg_logprob": -0.11290347322504571, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.013214950449764729}, {"id": 4434, "seek": 1462664, "start": 14626.64, "end": 14628.64, "text": " let's just make this a whole plane actually", "tokens": [50364, 718, 311, 445, 652, 341, 257, 1379, 5720, 767, 50464], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4435, "seek": 1462664, "start": 14630.64, "end": 14632.64, "text": " so", "tokens": [50564, 370, 50664], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4436, "seek": 1462664, "start": 14632.64, "end": 14634.64, "text": " all of these values", "tokens": [50664, 439, 295, 613, 4190, 50764], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4437, "seek": 1462664, "start": 14634.64, "end": 14636.64, "text": " that are below 0", "tokens": [50764, 300, 366, 2507, 1958, 50864], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4438, "seek": 1462664, "start": 14636.64, "end": 14638.64, "text": " all these values that are below 0 on the x axis", "tokens": [50864, 439, 613, 4190, 300, 366, 2507, 1958, 322, 264, 2031, 10298, 50964], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4439, "seek": 1462664, "start": 14638.64, "end": 14640.64, "text": " and", "tokens": [50964, 293, 51064], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4440, "seek": 1462664, "start": 14640.64, "end": 14642.64, "text": " equal to 0 will be changed", "tokens": [51064, 2681, 281, 1958, 486, 312, 3105, 51164], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4441, "seek": 1462664, "start": 14642.64, "end": 14644.64, "text": " just to 0 like that so you have all these values", "tokens": [51164, 445, 281, 1958, 411, 300, 370, 291, 362, 439, 613, 4190, 51264], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4442, "seek": 1462664, "start": 14644.64, "end": 14646.64, "text": " that look like this", "tokens": [51264, 300, 574, 411, 341, 51364], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4443, "seek": 1462664, "start": 14646.64, "end": 14648.64, "text": " and then everything that is above 0 just stays the same", "tokens": [51364, 293, 550, 1203, 300, 307, 3673, 1958, 445, 10834, 264, 912, 51464], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4444, "seek": 1462664, "start": 14648.64, "end": 14650.64, "text": " so you essentially just have this", "tokens": [51464, 370, 291, 4476, 445, 362, 341, 51564], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4445, "seek": 1462664, "start": 14650.64, "end": 14652.64, "text": " funny looking shape it's like straight", "tokens": [51564, 4074, 1237, 3909, 309, 311, 411, 2997, 51664], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4446, "seek": 1462664, "start": 14652.64, "end": 14654.64, "text": " and then diagonal that's what the relu function does", "tokens": [51664, 293, 550, 21539, 300, 311, 437, 264, 1039, 84, 2445, 775, 51764], "temperature": 0.0, "avg_logprob": -0.08543198568779126, "compression_ratio": 1.9209302325581394, "no_speech_prob": 0.0008039995445869863}, {"id": 4447, "seek": 1465464, "start": 14654.64, "end": 14656.64, "text": " it looks at a number", "tokens": [50364, 309, 1542, 412, 257, 1230, 50464], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4448, "seek": 1465464, "start": 14656.64, "end": 14658.64, "text": " sees if it's equal to or less than 0", "tokens": [50464, 8194, 498, 309, 311, 2681, 281, 420, 1570, 813, 1958, 50564], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4449, "seek": 1465464, "start": 14658.64, "end": 14660.64, "text": " if that's true we give that number 0", "tokens": [50564, 498, 300, 311, 2074, 321, 976, 300, 1230, 1958, 50664], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4450, "seek": 1465464, "start": 14660.64, "end": 14662.64, "text": " and if it's not", "tokens": [50664, 293, 498, 309, 311, 406, 50764], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4451, "seek": 1465464, "start": 14662.64, "end": 14664.64, "text": " then we just leave the number alone", "tokens": [50764, 550, 321, 445, 1856, 264, 1230, 3312, 50864], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4452, "seek": 1465464, "start": 14664.64, "end": 14666.64, "text": " so cool very cool", "tokens": [50864, 370, 1627, 588, 1627, 50964], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4453, "seek": 1465464, "start": 14666.64, "end": 14668.64, "text": " non-linearity function", "tokens": [50964, 2107, 12, 1889, 17409, 2445, 51064], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4454, "seek": 1465464, "start": 14668.64, "end": 14670.64, "text": " you can read papers on that if you like", "tokens": [51064, 291, 393, 1401, 10577, 322, 300, 498, 291, 411, 51164], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4455, "seek": 1465464, "start": 14670.64, "end": 14672.64, "text": " but", "tokens": [51164, 457, 51264], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4456, "seek": 1465464, "start": 14672.64, "end": 14674.64, "text": " essentially the shape of this", "tokens": [51264, 4476, 264, 3909, 295, 341, 51364], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4457, "seek": 1465464, "start": 14674.64, "end": 14676.64, "text": " just doesn't matter all we're doing is", "tokens": [51364, 445, 1177, 380, 1871, 439, 321, 434, 884, 307, 51464], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4458, "seek": 1465464, "start": 14676.64, "end": 14678.64, "text": " we're just making sure that we're just converting", "tokens": [51464, 321, 434, 445, 1455, 988, 300, 321, 434, 445, 29942, 51564], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4459, "seek": 1465464, "start": 14678.64, "end": 14680.64, "text": " some values if they're equal to", "tokens": [51564, 512, 4190, 498, 436, 434, 2681, 281, 51664], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4460, "seek": 1465464, "start": 14680.64, "end": 14682.64, "text": " or below 0 that's all this is doing", "tokens": [51664, 420, 2507, 1958, 300, 311, 439, 341, 307, 884, 51764], "temperature": 0.0, "avg_logprob": -0.07693630981445312, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004828495439141989}, {"id": 4461, "seek": 1468264, "start": 14682.64, "end": 14684.64, "text": " and then", "tokens": [50364, 293, 550, 50464], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4462, "seek": 1468264, "start": 14684.64, "end": 14686.64, "text": " we essentially are multiplying", "tokens": [50464, 321, 4476, 366, 30955, 50564], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4463, "seek": 1468264, "start": 14686.64, "end": 14688.64, "text": " this we're doing this", "tokens": [50564, 341, 321, 434, 884, 341, 50664], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4464, "seek": 1468264, "start": 14688.64, "end": 14690.64, "text": " linear transformation times this one", "tokens": [50664, 8213, 9887, 1413, 341, 472, 50764], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4465, "seek": 1468264, "start": 14690.64, "end": 14692.64, "text": " so we have to make sure that these inner", "tokens": [50764, 370, 321, 362, 281, 652, 988, 300, 613, 7284, 50864], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4466, "seek": 1468264, "start": 14692.64, "end": 14694.64, "text": " we have to make sure that these", "tokens": [50864, 321, 362, 281, 652, 988, 300, 613, 50964], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4467, "seek": 1468264, "start": 14694.64, "end": 14696.64, "text": " inner dimensions line up so 4 times", "tokens": [50964, 7284, 12819, 1622, 493, 370, 1017, 1413, 51064], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4468, "seek": 1468264, "start": 14696.64, "end": 14698.64, "text": " N embed and 4 times N embed", "tokens": [51064, 426, 12240, 293, 1017, 1413, 426, 12240, 51164], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4469, "seek": 1468264, "start": 14698.64, "end": 14700.64, "text": " those are equal to each other so our output shape", "tokens": [51164, 729, 366, 2681, 281, 1184, 661, 370, 527, 5598, 3909, 51264], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4470, "seek": 1468264, "start": 14700.64, "end": 14702.64, "text": " should be N embed", "tokens": [51264, 820, 312, 426, 12240, 51364], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4471, "seek": 1468264, "start": 14702.64, "end": 14704.64, "text": " by N embed cool", "tokens": [51364, 538, 426, 12240, 1627, 51464], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4472, "seek": 1468264, "start": 14704.64, "end": 14706.64, "text": " so now we have our dropout", "tokens": [51464, 370, 586, 321, 362, 527, 3270, 346, 51564], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4473, "seek": 1468264, "start": 14706.64, "end": 14708.64, "text": " and in case you don't know what dropout is", "tokens": [51564, 293, 294, 1389, 291, 500, 380, 458, 437, 3270, 346, 307, 51664], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4474, "seek": 1468264, "start": 14708.64, "end": 14710.64, "text": " it pretty much just", "tokens": [51664, 309, 1238, 709, 445, 51764], "temperature": 0.0, "avg_logprob": -0.08268976211547852, "compression_ratio": 1.8675799086757991, "no_speech_prob": 0.0023593304213136435}, {"id": 4475, "seek": 1471064, "start": 14710.64, "end": 14712.64, "text": " makes a certain percentage", "tokens": [50364, 1669, 257, 1629, 9668, 50464], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4476, "seek": 1471064, "start": 14712.64, "end": 14714.64, "text": " of our neurons just", "tokens": [50464, 295, 527, 22027, 445, 50564], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4477, "seek": 1471064, "start": 14714.64, "end": 14716.64, "text": " dropout and become 0", "tokens": [50564, 3270, 346, 293, 1813, 1958, 50664], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4478, "seek": 1471064, "start": 14716.64, "end": 14718.64, "text": " this is used to prevent overfitting", "tokens": [50664, 341, 307, 1143, 281, 4871, 670, 69, 2414, 50764], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4479, "seek": 1471064, "start": 14718.64, "end": 14720.64, "text": " and some other little details", "tokens": [50764, 293, 512, 661, 707, 4365, 50864], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4480, "seek": 1471064, "start": 14720.64, "end": 14722.64, "text": " that I'm sure you could", "tokens": [50864, 300, 286, 478, 988, 291, 727, 50964], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4481, "seek": 1471064, "start": 14722.64, "end": 14724.64, "text": " you could figure out through experimenting", "tokens": [50964, 291, 727, 2573, 484, 807, 29070, 51064], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4482, "seek": 1471064, "start": 14724.64, "end": 14726.64, "text": " so", "tokens": [51064, 370, 51164], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4483, "seek": 1471064, "start": 14726.64, "end": 14728.64, "text": " all this actually looks like in a parameter form", "tokens": [51164, 439, 341, 767, 1542, 411, 294, 257, 13075, 1254, 51264], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4484, "seek": 1471064, "start": 14728.64, "end": 14730.64, "text": " is just", "tokens": [51264, 307, 445, 51364], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4485, "seek": 1471064, "start": 14730.64, "end": 14732.64, "text": " dropout", "tokens": [51364, 3270, 346, 51464], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4486, "seek": 1471064, "start": 14734.64, "end": 14736.64, "text": " dropout equals", "tokens": [51564, 3270, 346, 6915, 51664], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4487, "seek": 1471064, "start": 14736.64, "end": 14738.64, "text": " we'll just say 0.2 for the same", "tokens": [51664, 321, 603, 445, 584, 1958, 13, 17, 337, 264, 912, 51764], "temperature": 0.0, "avg_logprob": -0.0722918459164199, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.013630380854010582}, {"id": 4488, "seek": 1473864, "start": 14738.64, "end": 14740.64, "text": " so 0.2 means", "tokens": [50364, 370, 1958, 13, 17, 1355, 50464], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4489, "seek": 1473864, "start": 14740.64, "end": 14742.64, "text": " 20%", "tokens": [50464, 945, 4, 50564], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4490, "seek": 1473864, "start": 14742.64, "end": 14744.64, "text": " or 0.2 is going to", "tokens": [50564, 420, 1958, 13, 17, 307, 516, 281, 50664], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4491, "seek": 1473864, "start": 14744.64, "end": 14746.64, "text": " yeah so 0.2", "tokens": [50664, 1338, 370, 1958, 13, 17, 50764], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4492, "seek": 1473864, "start": 14746.64, "end": 14748.64, "text": " in percentage form is just going to dropout", "tokens": [50764, 294, 9668, 1254, 307, 445, 516, 281, 3270, 346, 50864], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4493, "seek": 1473864, "start": 14748.64, "end": 14750.64, "text": " 20% of our", "tokens": [50864, 945, 4, 295, 527, 50964], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4494, "seek": 1473864, "start": 14750.64, "end": 14752.64, "text": " neurons turn them to 0 to prevent overfitting", "tokens": [50964, 22027, 1261, 552, 281, 1958, 281, 4871, 670, 69, 2414, 51064], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4495, "seek": 1473864, "start": 14752.64, "end": 14754.64, "text": " that's what that's doing", "tokens": [51064, 300, 311, 437, 300, 311, 884, 51164], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4496, "seek": 1473864, "start": 14754.64, "end": 14756.64, "text": " so cool", "tokens": [51164, 370, 1627, 51264], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4497, "seek": 1473864, "start": 14756.64, "end": 14758.64, "text": " we have our feedforward network we dropout after", "tokens": [51264, 321, 362, 527, 3154, 13305, 3209, 321, 3270, 346, 934, 51364], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4498, "seek": 1473864, "start": 14758.64, "end": 14760.64, "text": " to prevent overfitting and then we just", "tokens": [51364, 281, 4871, 670, 69, 2414, 293, 550, 321, 445, 51464], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4499, "seek": 1473864, "start": 14760.64, "end": 14762.64, "text": " call it forward on this sequential network", "tokens": [51464, 818, 309, 2128, 322, 341, 42881, 3209, 51564], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4500, "seek": 1473864, "start": 14762.64, "end": 14764.64, "text": " so cool", "tokens": [51564, 370, 1627, 51664], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4501, "seek": 1473864, "start": 14764.64, "end": 14766.64, "text": " feedforward pretty self-explanatory", "tokens": [51664, 3154, 13305, 1238, 2698, 12, 3121, 16554, 4745, 51764], "temperature": 0.0, "avg_logprob": -0.0961768544953445, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.00045118696289137006}, {"id": 4502, "seek": 1476664, "start": 14766.64, "end": 14768.64, "text": " we're going to add the", "tokens": [50364, 321, 434, 516, 281, 909, 264, 50464], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4503, "seek": 1476664, "start": 14768.64, "end": 14770.64, "text": " multi-head attention class", "tokens": [50464, 4825, 12, 1934, 3202, 1508, 50564], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4504, "seek": 1476664, "start": 14770.64, "end": 14772.64, "text": " so we've built all these decoder blocks", "tokens": [50564, 370, 321, 600, 3094, 439, 613, 979, 19866, 8474, 50664], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4505, "seek": 1476664, "start": 14772.64, "end": 14774.64, "text": " we've built", "tokens": [50664, 321, 600, 3094, 50764], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4506, "seek": 1476664, "start": 14774.64, "end": 14776.64, "text": " inside of the decoder blocks we've built the feedforward", "tokens": [50764, 1854, 295, 264, 979, 19866, 8474, 321, 600, 3094, 264, 3154, 13305, 50864], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4507, "seek": 1476664, "start": 14776.64, "end": 14778.64, "text": " and our res connections", "tokens": [50864, 293, 527, 725, 9271, 50964], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4508, "seek": 1476664, "start": 14778.64, "end": 14780.64, "text": " and now", "tokens": [50964, 293, 586, 51064], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4509, "seek": 1476664, "start": 14780.64, "end": 14782.64, "text": " all we have to do left in this block", "tokens": [51064, 439, 321, 362, 281, 360, 1411, 294, 341, 3461, 51164], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4510, "seek": 1476664, "start": 14782.64, "end": 14784.64, "text": " is the multi-head attention", "tokens": [51164, 307, 264, 4825, 12, 1934, 3202, 51264], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4511, "seek": 1476664, "start": 14784.64, "end": 14786.64, "text": " so it's going to look exactly like this here", "tokens": [51264, 370, 309, 311, 516, 281, 574, 2293, 411, 341, 510, 51364], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4512, "seek": 1476664, "start": 14786.64, "end": 14788.64, "text": " we're going to ignore the keys and queers for now", "tokens": [51364, 321, 434, 516, 281, 11200, 264, 9317, 293, 631, 433, 337, 586, 51464], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4513, "seek": 1476664, "start": 14788.64, "end": 14790.64, "text": " and save this for dot product attention", "tokens": [51464, 293, 3155, 341, 337, 5893, 1674, 3202, 51564], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4514, "seek": 1476664, "start": 14790.64, "end": 14792.64, "text": " so we're going to", "tokens": [51564, 370, 321, 434, 516, 281, 51664], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4515, "seek": 1476664, "start": 14792.64, "end": 14794.64, "text": " essentially just make a bunch of these", "tokens": [51664, 4476, 445, 652, 257, 3840, 295, 613, 51764], "temperature": 0.0, "avg_logprob": -0.10552008320966105, "compression_ratio": 1.9955357142857142, "no_speech_prob": 0.007120063528418541}, {"id": 4516, "seek": 1479464, "start": 14794.64, "end": 14796.64, "text": " multiple", "tokens": [50364, 3866, 50464], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4517, "seek": 1479464, "start": 14796.64, "end": 14798.64, "text": " heads", "tokens": [50464, 8050, 50564], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4518, "seek": 1479464, "start": 14798.64, "end": 14800.64, "text": " and we're going to concatenate results and do a linear", "tokens": [50564, 293, 321, 434, 516, 281, 1588, 7186, 473, 3542, 293, 360, 257, 8213, 50664], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4519, "seek": 1479464, "start": 14800.64, "end": 14802.64, "text": " transformation so what does this look like in code", "tokens": [50664, 9887, 370, 437, 775, 341, 574, 411, 294, 3089, 50764], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4520, "seek": 1479464, "start": 14802.64, "end": 14804.64, "text": " well let's go ahead", "tokens": [50764, 731, 718, 311, 352, 2286, 50864], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4521, "seek": 1479464, "start": 14804.64, "end": 14806.64, "text": " and add this here", "tokens": [50864, 293, 909, 341, 510, 50964], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4522, "seek": 1479464, "start": 14806.64, "end": 14808.64, "text": " all that attention cool", "tokens": [50964, 439, 300, 3202, 1627, 51064], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4523, "seek": 1479464, "start": 14808.64, "end": 14810.64, "text": " so multiple heads of attention in parallel", "tokens": [51064, 370, 3866, 8050, 295, 3202, 294, 8952, 51164], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4524, "seek": 1479464, "start": 14810.64, "end": 14812.64, "text": " I explained this earlier so I'm not going to jump into", "tokens": [51164, 286, 8825, 341, 3071, 370, 286, 478, 406, 516, 281, 3012, 666, 51264], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4525, "seek": 1479464, "start": 14812.64, "end": 14814.64, "text": " too much detail on that", "tokens": [51264, 886, 709, 2607, 322, 300, 51364], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4526, "seek": 1479464, "start": 14814.64, "end": 14816.64, "text": " but we have our knit", "tokens": [51364, 457, 321, 362, 527, 15594, 51464], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4527, "seek": 1479464, "start": 14816.64, "end": 14818.64, "text": " we have our forward", "tokens": [51464, 321, 362, 527, 2128, 51564], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4528, "seek": 1479464, "start": 14818.64, "end": 14820.64, "text": " and what are we doing in here", "tokens": [51564, 293, 437, 366, 321, 884, 294, 510, 51664], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4529, "seek": 1479464, "start": 14820.64, "end": 14822.64, "text": " so our self dot heads is just a module list", "tokens": [51664, 370, 527, 2698, 5893, 8050, 307, 445, 257, 10088, 1329, 51764], "temperature": 0.0, "avg_logprob": -0.12596376533182257, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0016483267536386847}, {"id": 4530, "seek": 1482264, "start": 14822.64, "end": 14824.64, "text": " and", "tokens": [50364, 293, 50464], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4531, "seek": 1482264, "start": 14824.64, "end": 14826.64, "text": " module list is kind of funky I'll dive into it", "tokens": [50464, 10088, 1329, 307, 733, 295, 33499, 286, 603, 9192, 666, 309, 50564], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4532, "seek": 1482264, "start": 14826.64, "end": 14828.64, "text": " a little bit later", "tokens": [50564, 257, 707, 857, 1780, 50664], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4533, "seek": 1482264, "start": 14828.64, "end": 14830.64, "text": " but essentially what we're doing is we're having", "tokens": [50664, 457, 4476, 437, 321, 434, 884, 307, 321, 434, 1419, 50764], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4534, "seek": 1482264, "start": 14830.64, "end": 14832.64, "text": " a bunch of these heads", "tokens": [50764, 257, 3840, 295, 613, 8050, 50864], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4535, "seek": 1482264, "start": 14832.64, "end": 14834.64, "text": " essentially in parallel", "tokens": [50864, 4476, 294, 8952, 50964], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4536, "seek": 1482264, "start": 14834.64, "end": 14836.64, "text": " for each head", "tokens": [50964, 337, 1184, 1378, 51064], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4537, "seek": 1482264, "start": 14836.64, "end": 14838.64, "text": " so num heads let's say our num heads is", "tokens": [51064, 370, 1031, 8050, 718, 311, 584, 527, 1031, 8050, 307, 51164], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4538, "seek": 1482264, "start": 14838.64, "end": 14840.64, "text": " set to", "tokens": [51164, 992, 281, 51264], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4539, "seek": 1482264, "start": 14840.64, "end": 14842.64, "text": " our num heads", "tokens": [51264, 527, 1031, 8050, 51364], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4540, "seek": 1482264, "start": 14842.64, "end": 14844.64, "text": " is set to", "tokens": [51364, 307, 992, 281, 51464], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4541, "seek": 1482264, "start": 14844.64, "end": 14846.64, "text": " maybe four in this", "tokens": [51464, 1310, 1451, 294, 341, 51564], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4542, "seek": 1482264, "start": 14846.64, "end": 14848.64, "text": " block we do multi-head attention", "tokens": [51564, 3461, 321, 360, 4825, 12, 1934, 3202, 51664], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4543, "seek": 1482264, "start": 14848.64, "end": 14850.64, "text": " we do n heads and then head size", "tokens": [51664, 321, 360, 297, 8050, 293, 550, 1378, 2744, 51764], "temperature": 0.0, "avg_logprob": -0.10020918112534744, "compression_ratio": 1.7539267015706805, "no_speech_prob": 0.006190226413309574}, {"id": 4544, "seek": 1485064, "start": 14850.64, "end": 14852.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4545, "seek": 1485064, "start": 14852.64, "end": 14854.64, "text": " and heads and then head size so num heads", "tokens": [50464, 293, 8050, 293, 550, 1378, 2744, 370, 1031, 8050, 50564], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4546, "seek": 1485064, "start": 14854.64, "end": 14856.64, "text": " essentially what it is so for the number of", "tokens": [50564, 4476, 437, 309, 307, 370, 337, 264, 1230, 295, 50664], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4547, "seek": 1485064, "start": 14856.64, "end": 14858.64, "text": " heads that we have which is four", "tokens": [50664, 8050, 300, 321, 362, 597, 307, 1451, 50764], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4548, "seek": 1485064, "start": 14858.64, "end": 14860.64, "text": " we're going to pretty much make one head", "tokens": [50764, 321, 434, 516, 281, 1238, 709, 652, 472, 1378, 50864], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4549, "seek": 1485064, "start": 14860.64, "end": 14862.64, "text": " running in parallel", "tokens": [50864, 2614, 294, 8952, 50964], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4550, "seek": 1485064, "start": 14862.64, "end": 14864.64, "text": " so four heads running in parallel is what this", "tokens": [50964, 370, 1451, 8050, 2614, 294, 8952, 307, 437, 341, 51064], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4551, "seek": 1485064, "start": 14864.64, "end": 14866.64, "text": " does here", "tokens": [51064, 775, 510, 51164], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4552, "seek": 1485064, "start": 14866.64, "end": 14868.64, "text": " then we have this projection", "tokens": [51164, 550, 321, 362, 341, 22743, 51264], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4553, "seek": 1485064, "start": 14868.64, "end": 14870.64, "text": " which is essentially just going to", "tokens": [51264, 597, 307, 4476, 445, 516, 281, 51364], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4554, "seek": 1485064, "start": 14870.64, "end": 14872.64, "text": " project the", "tokens": [51364, 1716, 264, 51464], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4555, "seek": 1485064, "start": 14872.64, "end": 14874.64, "text": " head size", "tokens": [51464, 1378, 2744, 51564], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4556, "seek": 1485064, "start": 14874.64, "end": 14876.64, "text": " times the number of", "tokens": [51564, 1413, 264, 1230, 295, 51664], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4557, "seek": 1485064, "start": 14876.64, "end": 14878.64, "text": " heads to an embed", "tokens": [51664, 8050, 281, 364, 12240, 51764], "temperature": 0.0, "avg_logprob": -0.0788872101727654, "compression_ratio": 1.9945054945054945, "no_speech_prob": 0.003375477157533169}, {"id": 4558, "seek": 1487864, "start": 14878.64, "end": 14880.64, "text": " and you might ask well that's", "tokens": [50364, 293, 291, 1062, 1029, 731, 300, 311, 50464], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4559, "seek": 1487864, "start": 14880.64, "end": 14882.64, "text": " weird because", "tokens": [50464, 3657, 570, 50564], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4560, "seek": 1487864, "start": 14882.64, "end": 14884.64, "text": " num heads times this is", "tokens": [50564, 1031, 8050, 1413, 341, 307, 50664], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4561, "seek": 1487864, "start": 14884.64, "end": 14886.64, "text": " literally equal to an embedding", "tokens": [50664, 3736, 2681, 281, 364, 12240, 3584, 50764], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4562, "seek": 1487864, "start": 14886.64, "end": 14888.64, "text": " if you go back to the math", "tokens": [50764, 498, 291, 352, 646, 281, 264, 5221, 50864], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4563, "seek": 1487864, "start": 14888.64, "end": 14890.64, "text": " we did here", "tokens": [50864, 321, 630, 510, 50964], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4564, "seek": 1487864, "start": 14890.64, "end": 14892.64, "text": " and the purpose of this is just to be", "tokens": [50964, 293, 264, 4334, 295, 341, 307, 445, 281, 312, 51064], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4565, "seek": 1487864, "start": 14892.64, "end": 14894.64, "text": " super hackable so that if you actually do want to", "tokens": [51064, 1687, 10339, 712, 370, 300, 498, 291, 767, 360, 528, 281, 51164], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4566, "seek": 1487864, "start": 14894.64, "end": 14896.64, "text": " change these around it won't be throwing you dimensionality", "tokens": [51164, 1319, 613, 926, 309, 1582, 380, 312, 10238, 291, 10139, 1860, 51264], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4567, "seek": 1487864, "start": 14896.64, "end": 14898.64, "text": " errors so that's what we're doing", "tokens": [51264, 13603, 370, 300, 311, 437, 321, 434, 884, 51364], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4568, "seek": 1487864, "start": 14898.64, "end": 14900.64, "text": " just a little projection", "tokens": [51364, 445, 257, 707, 22743, 51464], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4569, "seek": 1487864, "start": 14900.64, "end": 14902.64, "text": " from our", "tokens": [51464, 490, 527, 51564], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4570, "seek": 1487864, "start": 14902.64, "end": 14904.64, "text": " whatever these values are", "tokens": [51564, 2035, 613, 4190, 366, 51664], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4571, "seek": 1487864, "start": 14904.64, "end": 14906.64, "text": " up to this", "tokens": [51664, 493, 281, 341, 51764], "temperature": 0.0, "avg_logprob": -0.04274262275014605, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0005702824564650655}, {"id": 4572, "seek": 1490664, "start": 14906.64, "end": 14908.64, "text": " constant feature", "tokens": [50364, 5754, 4111, 50464], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4573, "seek": 1490664, "start": 14908.64, "end": 14910.64, "text": " length of an embed", "tokens": [50464, 4641, 295, 364, 12240, 50564], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4574, "seek": 1490664, "start": 14910.64, "end": 14912.64, "text": " so then we just follow that with a drop out", "tokens": [50564, 370, 550, 321, 445, 1524, 300, 365, 257, 3270, 484, 50664], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4575, "seek": 1490664, "start": 14912.64, "end": 14914.64, "text": " dropping out 20% of the", "tokens": [50664, 13601, 484, 945, 4, 295, 264, 50764], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4576, "seek": 1490664, "start": 14914.64, "end": 14916.64, "text": " networks neurons", "tokens": [50764, 9590, 22027, 50864], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4577, "seek": 1490664, "start": 14916.64, "end": 14918.64, "text": " now let's go into this forward here", "tokens": [50864, 586, 718, 311, 352, 666, 341, 2128, 510, 50964], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4578, "seek": 1490664, "start": 14918.64, "end": 14920.64, "text": " so forward", "tokens": [50964, 370, 2128, 51064], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4579, "seek": 1490664, "start": 14920.64, "end": 14922.64, "text": " torch dot concatenate or torch dot cat", "tokens": [51064, 27822, 5893, 1588, 7186, 473, 420, 27822, 5893, 3857, 51164], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4580, "seek": 1490664, "start": 14922.64, "end": 14924.64, "text": " we do four h and self dot heads", "tokens": [51164, 321, 360, 1451, 276, 293, 2698, 5893, 8050, 51264], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4581, "seek": 1490664, "start": 14924.64, "end": 14926.64, "text": " so we're going to concatenate", "tokens": [51264, 370, 321, 434, 516, 281, 1588, 7186, 473, 51364], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4582, "seek": 1490664, "start": 14926.64, "end": 14928.64, "text": " each head together", "tokens": [51364, 1184, 1378, 1214, 51464], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4583, "seek": 1490664, "start": 14928.64, "end": 14930.64, "text": " along the last dimension", "tokens": [51464, 2051, 264, 1036, 10139, 51564], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4584, "seek": 1490664, "start": 14930.64, "end": 14932.64, "text": " and the last dimension in this case", "tokens": [51564, 293, 264, 1036, 10139, 294, 341, 1389, 51664], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4585, "seek": 1490664, "start": 14932.64, "end": 14934.64, "text": " is the", "tokens": [51664, 307, 264, 51764], "temperature": 0.0, "avg_logprob": -0.09627277737572079, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.002322887070477009}, {"id": 4586, "seek": 1493464, "start": 14934.64, "end": 14936.64, "text": " b batch", "tokens": [50364, 272, 15245, 50464], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4587, "seek": 1493464, "start": 14936.64, "end": 14938.64, "text": " by time", "tokens": [50464, 538, 565, 50564], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4588, "seek": 1493464, "start": 14938.64, "end": 14940.64, "text": " by we just say feature dimension or channel dimension", "tokens": [50564, 538, 321, 445, 584, 4111, 10139, 420, 2269, 10139, 50664], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4589, "seek": 1493464, "start": 14942.64, "end": 14944.64, "text": " the channel dimension here is the", "tokens": [50764, 264, 2269, 10139, 510, 307, 264, 50864], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4590, "seek": 1493464, "start": 14944.64, "end": 14946.64, "text": " last one so we're going to", "tokens": [50864, 1036, 472, 370, 321, 434, 516, 281, 50964], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4591, "seek": 1493464, "start": 14946.64, "end": 14948.64, "text": " concatenate along this feature dimension", "tokens": [50964, 1588, 7186, 473, 2051, 341, 4111, 10139, 51064], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4592, "seek": 1493464, "start": 14950.64, "end": 14952.64, "text": " and let me just help you illustrate", "tokens": [51164, 293, 718, 385, 445, 854, 291, 23221, 51264], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4593, "seek": 1493464, "start": 14952.64, "end": 14954.64, "text": " what exactly this looks like", "tokens": [51264, 437, 2293, 341, 1542, 411, 51364], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4594, "seek": 1493464, "start": 14954.64, "end": 14956.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4595, "seek": 1493464, "start": 14956.64, "end": 14958.64, "text": " when we concatenate along these", "tokens": [51464, 562, 321, 1588, 7186, 473, 2051, 613, 51564], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4596, "seek": 1493464, "start": 14958.64, "end": 14960.64, "text": " we have this b by t", "tokens": [51564, 321, 362, 341, 272, 538, 256, 51664], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4597, "seek": 1493464, "start": 14960.64, "end": 14962.64, "text": " and then we'll just say", "tokens": [51664, 293, 550, 321, 603, 445, 584, 51764], "temperature": 0.0, "avg_logprob": -0.11493646833631728, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.003429009346291423}, {"id": 4598, "seek": 1496464, "start": 14964.64, "end": 14966.64, "text": " our features are going to be", "tokens": [50364, 527, 4122, 366, 516, 281, 312, 50464], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4599, "seek": 1496464, "start": 14966.64, "end": 14968.64, "text": " h1 like", "tokens": [50464, 276, 16, 411, 50564], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4600, "seek": 1496464, "start": 14968.64, "end": 14970.64, "text": " each of our heads here", "tokens": [50564, 1184, 295, 527, 8050, 510, 50664], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4601, "seek": 1496464, "start": 14970.64, "end": 14972.64, "text": " another h1", "tokens": [50664, 1071, 276, 16, 50764], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4602, "seek": 1496464, "start": 14972.64, "end": 14974.64, "text": " h1 h1 and these are all just features", "tokens": [50764, 276, 16, 276, 16, 293, 613, 366, 439, 445, 4122, 50864], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4603, "seek": 1496464, "start": 14974.64, "end": 14976.64, "text": " of head one and then our next", "tokens": [50864, 295, 1378, 472, 293, 550, 527, 958, 50964], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4604, "seek": 1496464, "start": 14976.64, "end": 14978.64, "text": " would be h2", "tokens": [50964, 576, 312, 276, 17, 51064], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4605, "seek": 1496464, "start": 14978.64, "end": 14980.64, "text": " h2 h2 h2", "tokens": [51064, 276, 17, 276, 17, 276, 17, 51164], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4606, "seek": 1496464, "start": 14980.64, "end": 14982.64, "text": " and then let's just say we have", "tokens": [51164, 293, 550, 718, 311, 445, 584, 321, 362, 51264], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4607, "seek": 1496464, "start": 14982.64, "end": 14984.64, "text": " a third head go h3", "tokens": [51264, 257, 2636, 1378, 352, 276, 18, 51364], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4608, "seek": 1496464, "start": 14984.64, "end": 14986.64, "text": " h3", "tokens": [51364, 276, 18, 51464], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4609, "seek": 1496464, "start": 14986.64, "end": 14988.64, "text": " h3 h3", "tokens": [51464, 276, 18, 276, 18, 51564], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4610, "seek": 1496464, "start": 14988.64, "end": 14990.64, "text": " h3 like that", "tokens": [51564, 276, 18, 411, 300, 51664], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4611, "seek": 1496464, "start": 14990.64, "end": 14992.64, "text": " so we have", "tokens": [51664, 370, 321, 362, 51764], "temperature": 0.0, "avg_logprob": -0.10161182667949412, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.010816317051649094}, {"id": 4612, "seek": 1499264, "start": 14992.64, "end": 14994.64, "text": " maybe four features per head", "tokens": [50364, 1310, 1451, 4122, 680, 1378, 50464], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4613, "seek": 1499264, "start": 14994.64, "end": 14996.64, "text": " and there's three heads", "tokens": [50464, 293, 456, 311, 1045, 8050, 50564], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4614, "seek": 1499264, "start": 14996.64, "end": 14998.64, "text": " so essentially all we're doing", "tokens": [50564, 370, 4476, 439, 321, 434, 884, 50664], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4615, "seek": 1499264, "start": 14998.64, "end": 15000.64, "text": " when we do this concatenate", "tokens": [50664, 562, 321, 360, 341, 1588, 7186, 473, 50764], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4616, "seek": 1499264, "start": 15000.64, "end": 15002.64, "text": " is we're just concatenating these along the last", "tokens": [50764, 307, 321, 434, 445, 1588, 7186, 990, 613, 2051, 264, 1036, 50864], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4617, "seek": 1499264, "start": 15002.64, "end": 15004.64, "text": " dimension so to convert", "tokens": [50864, 10139, 370, 281, 7620, 50964], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4618, "seek": 1499264, "start": 15004.64, "end": 15006.64, "text": " this like ugly list format", "tokens": [50964, 341, 411, 12246, 1329, 7877, 51064], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4619, "seek": 1499264, "start": 15006.64, "end": 15008.64, "text": " of just each head", "tokens": [51064, 295, 445, 1184, 1378, 51164], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4620, "seek": 1499264, "start": 15008.64, "end": 15010.64, "text": " features sequentially in order", "tokens": [51164, 4122, 5123, 3137, 294, 1668, 51264], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4621, "seek": 1499264, "start": 15010.64, "end": 15012.64, "text": " which is like really hard", "tokens": [51264, 597, 307, 411, 534, 1152, 51364], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4622, "seek": 1499264, "start": 15012.64, "end": 15014.64, "text": " to process we're just concatenating these", "tokens": [51364, 281, 1399, 321, 434, 445, 1588, 7186, 990, 613, 51464], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4623, "seek": 1499264, "start": 15014.64, "end": 15016.64, "text": " so they're easier to process", "tokens": [51464, 370, 436, 434, 3571, 281, 1399, 51564], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4624, "seek": 1499264, "start": 15016.64, "end": 15018.64, "text": " so that's what that does", "tokens": [51564, 370, 300, 311, 437, 300, 775, 51664], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4625, "seek": 1499264, "start": 15018.64, "end": 15020.64, "text": " and then we just follow this with a dropout", "tokens": [51664, 293, 550, 321, 445, 1524, 341, 365, 257, 3270, 346, 51764], "temperature": 0.0, "avg_logprob": -0.06156087325791181, "compression_ratio": 1.9276018099547512, "no_speech_prob": 0.0033758599311113358}, {"id": 4626, "seek": 1502064, "start": 15020.64, "end": 15022.64, "text": " self dot projection", "tokens": [50364, 2698, 5893, 22743, 50464], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4627, "seek": 1502064, "start": 15022.64, "end": 15024.64, "text": " and then just follow that with a", "tokens": [50464, 293, 550, 445, 1524, 300, 365, 257, 50564], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4628, "seek": 1502064, "start": 15024.64, "end": 15026.64, "text": " dropout so cool", "tokens": [50564, 3270, 346, 370, 1627, 50664], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4629, "seek": 1502064, "start": 15026.64, "end": 15028.64, "text": " if that didn't totally make", "tokens": [50664, 498, 300, 994, 380, 3879, 652, 50764], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4630, "seek": 1502064, "start": 15028.64, "end": 15030.64, "text": " sense you can totally just plug this code into chat", "tokens": [50764, 2020, 291, 393, 3879, 445, 5452, 341, 3089, 666, 5081, 50864], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4631, "seek": 1502064, "start": 15030.64, "end": 15032.64, "text": " gbt and", "tokens": [50864, 290, 4517, 293, 50964], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4632, "seek": 1502064, "start": 15032.64, "end": 15034.64, "text": " get a detailed explanation on how it works", "tokens": [50964, 483, 257, 9942, 10835, 322, 577, 309, 1985, 51064], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4633, "seek": 1502064, "start": 15034.64, "end": 15036.64, "text": " if something wasn't particularly clear", "tokens": [51064, 498, 746, 2067, 380, 4098, 1850, 51164], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4634, "seek": 1502064, "start": 15036.64, "end": 15038.64, "text": " but essentially that's the premise", "tokens": [51164, 457, 4476, 300, 311, 264, 22045, 51264], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4635, "seek": 1502064, "start": 15038.64, "end": 15040.64, "text": " you have your batch by time", "tokens": [51264, 291, 362, 428, 15245, 538, 565, 51364], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4636, "seek": 1502064, "start": 15040.64, "end": 15042.64, "text": " batch by", "tokens": [51364, 15245, 538, 51464], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4637, "seek": 1502064, "start": 15042.64, "end": 15044.64, "text": " sequence length", "tokens": [51464, 8310, 4641, 51564], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4638, "seek": 1502064, "start": 15044.64, "end": 15046.64, "text": " or time use interchangeably", "tokens": [51564, 420, 565, 764, 30358, 1188, 51664], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4639, "seek": 1502064, "start": 15046.64, "end": 15048.64, "text": " and then you have your features which are all", "tokens": [51664, 293, 550, 291, 362, 428, 4122, 597, 366, 439, 51764], "temperature": 0.0, "avg_logprob": -0.11694810992089387, "compression_ratio": 1.7167381974248928, "no_speech_prob": 0.002396524418145418}, {"id": 4640, "seek": 1504864, "start": 15048.64, "end": 15050.64, "text": " just in this weird list format", "tokens": [50364, 445, 294, 341, 3657, 1329, 7877, 50464], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4641, "seek": 1504864, "start": 15050.64, "end": 15052.64, "text": " of each feature just listed", "tokens": [50464, 295, 1184, 4111, 445, 10052, 50564], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4642, "seek": 1504864, "start": 15052.64, "end": 15054.64, "text": " after another", "tokens": [50564, 934, 1071, 50664], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4643, "seek": 1504864, "start": 15054.64, "end": 15056.64, "text": " so cool", "tokens": [50664, 370, 1627, 50764], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4644, "seek": 1504864, "start": 15056.64, "end": 15058.64, "text": " that's what multi head attention looks like", "tokens": [50764, 300, 311, 437, 4825, 1378, 3202, 1542, 411, 50864], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4645, "seek": 1504864, "start": 15058.64, "end": 15060.64, "text": " let's go ahead and implement dot product", "tokens": [50864, 718, 311, 352, 2286, 293, 4445, 5893, 1674, 50964], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4646, "seek": 1504864, "start": 15060.64, "end": 15062.64, "text": " attention or scale dot product attention", "tokens": [50964, 3202, 420, 4373, 5893, 1674, 3202, 51064], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4647, "seek": 1504864, "start": 15062.64, "end": 15064.64, "text": " so a little something I'd like to cover before", "tokens": [51064, 370, 257, 707, 746, 286, 1116, 411, 281, 2060, 949, 51164], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4648, "seek": 1504864, "start": 15064.64, "end": 15066.64, "text": " we go into our next scaled", "tokens": [51164, 321, 352, 666, 527, 958, 36039, 51264], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4649, "seek": 1504864, "start": 15066.64, "end": 15068.64, "text": " dot product attention was just this linear", "tokens": [51264, 5893, 1674, 3202, 390, 445, 341, 8213, 51364], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4650, "seek": 1504864, "start": 15068.64, "end": 15070.64, "text": " transformation here", "tokens": [51364, 9887, 510, 51464], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4651, "seek": 1504864, "start": 15070.64, "end": 15072.64, "text": " and you might think well what's the point if we're just", "tokens": [51464, 293, 291, 1062, 519, 731, 437, 311, 264, 935, 498, 321, 434, 445, 51564], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4652, "seek": 1504864, "start": 15072.64, "end": 15074.64, "text": " transforming", "tokens": [51564, 27210, 51664], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4653, "seek": 1504864, "start": 15074.64, "end": 15076.64, "text": " an embed to an embed right", "tokens": [51664, 364, 12240, 281, 364, 12240, 558, 51764], "temperature": 0.0, "avg_logprob": -0.07937419414520264, "compression_ratio": 1.876068376068376, "no_speech_prob": 0.0010648940224200487}, {"id": 4654, "seek": 1507664, "start": 15076.64, "end": 15078.64, "text": " we're to have the match like that", "tokens": [50364, 321, 434, 281, 362, 264, 2995, 411, 300, 50464], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4655, "seek": 1507664, "start": 15078.64, "end": 15080.64, "text": " and", "tokens": [50464, 293, 50564], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4656, "seek": 1507664, "start": 15080.64, "end": 15082.64, "text": " essentially what this does is it just adds in another", "tokens": [50564, 4476, 437, 341, 775, 307, 309, 445, 10860, 294, 1071, 50664], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4657, "seek": 1507664, "start": 15082.64, "end": 15084.64, "text": " learnable parameter", "tokens": [50664, 1466, 712, 13075, 50764], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4658, "seek": 1507664, "start": 15084.64, "end": 15086.64, "text": " for us so it has a weight", "tokens": [50764, 337, 505, 370, 309, 575, 257, 3364, 50864], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4659, "seek": 1507664, "start": 15086.64, "end": 15088.64, "text": " and a bias if we set bias", "tokens": [50864, 293, 257, 12577, 498, 321, 992, 12577, 50964], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4660, "seek": 1507664, "start": 15088.64, "end": 15090.64, "text": " to false", "tokens": [50964, 281, 7908, 51064], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4661, "seek": 1507664, "start": 15090.64, "end": 15092.64, "text": " like that then it wouldn't have a bias", "tokens": [51064, 411, 300, 550, 309, 2759, 380, 362, 257, 12577, 51164], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4662, "seek": 1507664, "start": 15092.64, "end": 15094.64, "text": " but it does have", "tokens": [51164, 457, 309, 775, 362, 51264], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4663, "seek": 1507664, "start": 15094.64, "end": 15096.64, "text": " a bias so another just wx", "tokens": [51264, 257, 12577, 370, 1071, 445, 261, 87, 51364], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4664, "seek": 1507664, "start": 15096.64, "end": 15098.64, "text": " plus b if you will a weight times x", "tokens": [51364, 1804, 272, 498, 291, 486, 257, 3364, 1413, 2031, 51464], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4665, "seek": 1507664, "start": 15098.64, "end": 15100.64, "text": " plus a bias so it just adds", "tokens": [51464, 1804, 257, 12577, 370, 309, 445, 10860, 51564], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4666, "seek": 1507664, "start": 15100.64, "end": 15102.64, "text": " more learnable parameters to help our", "tokens": [51564, 544, 1466, 712, 9834, 281, 854, 527, 51664], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4667, "seek": 1507664, "start": 15102.64, "end": 15104.64, "text": " network", "tokens": [51664, 3209, 51764], "temperature": 0.0, "avg_logprob": -0.12386698382241386, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.022966699674725533}, {"id": 4668, "seek": 1510464, "start": 15104.64, "end": 15106.64, "text": " learn more about this text", "tokens": [50364, 1466, 544, 466, 341, 2487, 50464], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4669, "seek": 1510464, "start": 15106.64, "end": 15108.64, "text": " so cool I'm going to go ahead and add", "tokens": [50464, 370, 1627, 286, 478, 516, 281, 352, 2286, 293, 909, 50564], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4670, "seek": 1510464, "start": 15108.64, "end": 15110.64, "text": " in this last but not least", "tokens": [50564, 294, 341, 1036, 457, 406, 1935, 50664], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4671, "seek": 1510464, "start": 15112.64, "end": 15114.64, "text": " scale dot product attention", "tokens": [50764, 4373, 5893, 1674, 3202, 50864], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4672, "seek": 1510464, "start": 15114.64, "end": 15116.64, "text": " or head class so there's going to be", "tokens": [50864, 420, 1378, 1508, 370, 456, 311, 516, 281, 312, 50964], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4673, "seek": 1510464, "start": 15116.64, "end": 15118.64, "text": " a bunch of these", "tokens": [50964, 257, 3840, 295, 613, 51064], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4674, "seek": 1510464, "start": 15118.64, "end": 15120.64, "text": " heads hence", "tokens": [51064, 8050, 16678, 51164], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4675, "seek": 1510464, "start": 15120.64, "end": 15122.64, "text": " class head running in parallel", "tokens": [51164, 1508, 1378, 2614, 294, 8952, 51264], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4676, "seek": 1510464, "start": 15122.64, "end": 15124.64, "text": " and inside of here we're going to do some", "tokens": [51264, 293, 1854, 295, 510, 321, 434, 516, 281, 360, 512, 51364], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4677, "seek": 1510464, "start": 15124.64, "end": 15126.64, "text": " scale dot product attention", "tokens": [51364, 4373, 5893, 1674, 3202, 51464], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4678, "seek": 1510464, "start": 15126.64, "end": 15128.64, "text": " so there's a lot of code in here don't get", "tokens": [51464, 370, 456, 311, 257, 688, 295, 3089, 294, 510, 500, 380, 483, 51564], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4679, "seek": 1510464, "start": 15128.64, "end": 15130.64, "text": " too overwhelmed by this but I'm going to walk", "tokens": [51564, 886, 19042, 538, 341, 457, 286, 478, 516, 281, 1792, 51664], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4680, "seek": 1510464, "start": 15130.64, "end": 15132.64, "text": " through this step by step so we have our", "tokens": [51664, 807, 341, 1823, 538, 1823, 370, 321, 362, 527, 51764], "temperature": 0.0, "avg_logprob": -0.09397954455876754, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0027145864441990852}, {"id": 4681, "seek": 1513264, "start": 15132.64, "end": 15134.64, "text": " in it we have our forward", "tokens": [50364, 294, 309, 321, 362, 527, 2128, 50464], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4682, "seek": 1513264, "start": 15134.64, "end": 15136.64, "text": " awesome", "tokens": [50464, 3476, 50564], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4683, "seek": 1513264, "start": 15136.64, "end": 15138.64, "text": " so what do we do in our", "tokens": [50564, 370, 437, 360, 321, 360, 294, 527, 50664], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4684, "seek": 1513264, "start": 15138.64, "end": 15140.64, "text": " architecture here", "tokens": [50664, 9482, 510, 50764], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4685, "seek": 1513264, "start": 15140.64, "end": 15142.64, "text": " so we have a key", "tokens": [50764, 370, 321, 362, 257, 2141, 50864], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4686, "seek": 1513264, "start": 15142.64, "end": 15144.64, "text": " a query and a value", "tokens": [50864, 257, 14581, 293, 257, 2158, 50964], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4687, "seek": 1513264, "start": 15144.64, "end": 15146.64, "text": " the keys and the queries dot", "tokens": [50964, 264, 9317, 293, 264, 24109, 5893, 51064], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4688, "seek": 1513264, "start": 15146.64, "end": 15148.64, "text": " product together they get scaled", "tokens": [51064, 1674, 1214, 436, 483, 36039, 51164], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4689, "seek": 1513264, "start": 15148.64, "end": 15150.64, "text": " by", "tokens": [51164, 538, 51264], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4690, "seek": 1513264, "start": 15150.64, "end": 15152.64, "text": " one over the square root of", "tokens": [51264, 472, 670, 264, 3732, 5593, 295, 51364], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4691, "seek": 1513264, "start": 15152.64, "end": 15154.64, "text": " length of a row in the keys or queries", "tokens": [51364, 4641, 295, 257, 5386, 294, 264, 9317, 420, 24109, 51464], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4692, "seek": 1513264, "start": 15154.64, "end": 15156.64, "text": " matrix so we'll just say maybe keys", "tokens": [51464, 8141, 370, 321, 603, 445, 584, 1310, 9317, 51564], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4693, "seek": 1513264, "start": 15156.64, "end": 15158.64, "text": " for example", "tokens": [51564, 337, 1365, 51664], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4694, "seek": 1513264, "start": 15158.64, "end": 15160.64, "text": " the row of keys", "tokens": [51664, 264, 5386, 295, 9317, 51764], "temperature": 0.0, "avg_logprob": -0.0966567108311604, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005383147858083248}, {"id": 4695, "seek": 1516064, "start": 15160.64, "end": 15162.64, "text": " the length of a row in keys", "tokens": [50364, 264, 4641, 295, 257, 5386, 294, 9317, 50464], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4696, "seek": 1516064, "start": 15162.64, "end": 15164.64, "text": " and then we just do our", "tokens": [50464, 293, 550, 321, 445, 360, 527, 50564], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4697, "seek": 1516064, "start": 15164.64, "end": 15166.64, "text": " masking to make sure the network", "tokens": [50564, 31226, 281, 652, 988, 264, 3209, 50664], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4698, "seek": 1516064, "start": 15166.64, "end": 15168.64, "text": " does not look ahead and cheat", "tokens": [50664, 775, 406, 574, 2286, 293, 17470, 50764], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4699, "seek": 1516064, "start": 15168.64, "end": 15170.64, "text": " and then we do a softmax", "tokens": [50764, 293, 550, 321, 360, 257, 2787, 41167, 50864], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4700, "seek": 1516064, "start": 15170.64, "end": 15172.64, "text": " and a matrix", "tokens": [50864, 293, 257, 8141, 50964], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4701, "seek": 1516064, "start": 15172.64, "end": 15174.64, "text": " multiply to", "tokens": [50964, 12972, 281, 51064], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4702, "seek": 1516064, "start": 15174.64, "end": 15176.64, "text": " essentially add this", "tokens": [51064, 4476, 909, 341, 51164], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4703, "seek": 1516064, "start": 15176.64, "end": 15178.64, "text": " value weight on top of it", "tokens": [51164, 2158, 3364, 322, 1192, 295, 309, 51264], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4704, "seek": 1516064, "start": 15178.64, "end": 15180.64, "text": " so cool", "tokens": [51264, 370, 1627, 51364], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4705, "seek": 1516064, "start": 15180.64, "end": 15182.64, "text": " we do this", "tokens": [51364, 321, 360, 341, 51464], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4706, "seek": 1516064, "start": 15182.64, "end": 15184.64, "text": " keep in mind this initialization", "tokens": [51464, 1066, 294, 1575, 341, 5883, 2144, 51564], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4707, "seek": 1516064, "start": 15184.64, "end": 15186.64, "text": " is not actually doing any calculations", "tokens": [51564, 307, 406, 767, 884, 604, 20448, 51664], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4708, "seek": 1516064, "start": 15186.64, "end": 15188.64, "text": " but just rather initializing", "tokens": [51664, 457, 445, 2831, 5883, 3319, 51764], "temperature": 0.0, "avg_logprob": -0.05621156887132294, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.0028441613540053368}, {"id": 4709, "seek": 1518864, "start": 15188.64, "end": 15190.64, "text": " linear transformations that we will do", "tokens": [50364, 8213, 34852, 300, 321, 486, 360, 50464], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4710, "seek": 1518864, "start": 15190.64, "end": 15192.64, "text": " in the forward pass", "tokens": [50464, 294, 264, 2128, 1320, 50564], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4711, "seek": 1518864, "start": 15192.64, "end": 15194.64, "text": " so this self dot key", "tokens": [50564, 370, 341, 2698, 5893, 2141, 50664], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4712, "seek": 1518864, "start": 15194.64, "end": 15196.64, "text": " is just going to", "tokens": [50664, 307, 445, 516, 281, 50764], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4713, "seek": 1518864, "start": 15196.64, "end": 15198.64, "text": " transform and embed to head size", "tokens": [50764, 4088, 293, 12240, 281, 1378, 2744, 50864], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4714, "seek": 1518864, "start": 15198.64, "end": 15200.64, "text": " bias false and then", "tokens": [50864, 12577, 7908, 293, 550, 50964], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4715, "seek": 1518864, "start": 15200.64, "end": 15202.64, "text": " I mean the rest of these are just the same", "tokens": [50964, 286, 914, 264, 1472, 295, 613, 366, 445, 264, 912, 51064], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4716, "seek": 1518864, "start": 15202.64, "end": 15204.64, "text": " and embed to head size because each head", "tokens": [51064, 293, 12240, 281, 1378, 2744, 570, 1184, 1378, 51164], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4717, "seek": 1518864, "start": 15204.64, "end": 15206.64, "text": " will have 96 features", "tokens": [51164, 486, 362, 24124, 4122, 51264], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4718, "seek": 1518864, "start": 15206.64, "end": 15208.64, "text": " rather than 384", "tokens": [51264, 2831, 813, 12843, 19, 51364], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4719, "seek": 1518864, "start": 15208.64, "end": 15210.64, "text": " so we kind of already went over that", "tokens": [51364, 370, 321, 733, 295, 1217, 1437, 670, 300, 51464], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4720, "seek": 1518864, "start": 15210.64, "end": 15212.64, "text": " but that's just what that's doing", "tokens": [51464, 457, 300, 311, 445, 437, 300, 311, 884, 51564], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4721, "seek": 1518864, "start": 15212.64, "end": 15214.64, "text": " cool that's just a linear transformation", "tokens": [51564, 1627, 300, 311, 445, 257, 8213, 9887, 51664], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4722, "seek": 1518864, "start": 15214.64, "end": 15216.64, "text": " that's happening to convert", "tokens": [51664, 300, 311, 2737, 281, 7620, 51764], "temperature": 0.0, "avg_logprob": -0.06636871067823562, "compression_ratio": 1.84304932735426, "no_speech_prob": 0.0027145743370056152}, {"id": 4723, "seek": 1521664, "start": 15216.64, "end": 15218.64, "text": " from 384 to 96 features", "tokens": [50364, 490, 12843, 19, 281, 24124, 4122, 50464], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4724, "seek": 1521664, "start": 15218.64, "end": 15220.64, "text": " then we have this", "tokens": [50464, 550, 321, 362, 341, 50564], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4725, "seek": 1521664, "start": 15220.64, "end": 15222.64, "text": " self dot register buffer", "tokens": [50564, 2698, 5893, 7280, 21762, 50664], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4726, "seek": 1521664, "start": 15222.64, "end": 15224.64, "text": " well what does this do you might ask", "tokens": [50664, 731, 437, 775, 341, 360, 291, 1062, 1029, 50764], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4727, "seek": 1521664, "start": 15224.64, "end": 15226.64, "text": " register buffer is essentially just going", "tokens": [50764, 7280, 21762, 307, 4476, 445, 516, 50864], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4728, "seek": 1521664, "start": 15226.64, "end": 15228.64, "text": " to register", "tokens": [50864, 281, 7280, 50964], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4729, "seek": 1521664, "start": 15228.64, "end": 15230.64, "text": " this no look ahead", "tokens": [50964, 341, 572, 574, 2286, 51064], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4730, "seek": 1521664, "start": 15230.64, "end": 15232.64, "text": " masking in the model state", "tokens": [51064, 31226, 294, 264, 2316, 1785, 51164], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4731, "seek": 1521664, "start": 15232.64, "end": 15234.64, "text": " so instead of having to re-initialize", "tokens": [51164, 370, 2602, 295, 1419, 281, 319, 12, 259, 270, 831, 1125, 51264], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4732, "seek": 1521664, "start": 15234.64, "end": 15236.64, "text": " this every single head for every", "tokens": [51264, 341, 633, 2167, 1378, 337, 633, 51364], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4733, "seek": 1521664, "start": 15236.64, "end": 15238.64, "text": " single forward and backward pass", "tokens": [51364, 2167, 2128, 293, 23897, 1320, 51464], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4734, "seek": 1521664, "start": 15238.64, "end": 15240.64, "text": " we're just going to add this to the model", "tokens": [51464, 321, 434, 445, 516, 281, 909, 341, 281, 264, 2316, 51564], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4735, "seek": 1521664, "start": 15240.64, "end": 15242.64, "text": " state so it's going to save us a lot of", "tokens": [51564, 1785, 370, 309, 311, 516, 281, 3155, 505, 257, 688, 295, 51664], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4736, "seek": 1521664, "start": 15242.64, "end": 15244.64, "text": " computation that way on our training", "tokens": [51664, 24903, 300, 636, 322, 527, 3097, 51764], "temperature": 0.0, "avg_logprob": -0.062294066962549244, "compression_ratio": 1.8127659574468085, "no_speech_prob": 0.0006666613626293838}, {"id": 4737, "seek": 1524464, "start": 15244.64, "end": 15246.64, "text": " so our training times can be reduced just because", "tokens": [50364, 370, 527, 3097, 1413, 393, 312, 9212, 445, 570, 50464], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4738, "seek": 1524464, "start": 15246.64, "end": 15248.64, "text": " we're registering this", "tokens": [50464, 321, 434, 47329, 341, 50564], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4739, "seek": 1524464, "start": 15248.64, "end": 15250.64, "text": " yeah", "tokens": [50564, 1338, 50664], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4740, "seek": 1524464, "start": 15250.64, "end": 15252.64, "text": " so it's just going to prevent some of that", "tokens": [50664, 370, 309, 311, 445, 516, 281, 4871, 512, 295, 300, 50764], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4741, "seek": 1524464, "start": 15252.64, "end": 15254.64, "text": " overhead computation of having to redo", "tokens": [50764, 19922, 24903, 295, 1419, 281, 29956, 50864], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4742, "seek": 1524464, "start": 15254.64, "end": 15256.64, "text": " this over and over again", "tokens": [50864, 341, 670, 293, 670, 797, 50964], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4743, "seek": 1524464, "start": 15256.64, "end": 15258.64, "text": " you could still do training without", "tokens": [50964, 291, 727, 920, 360, 3097, 1553, 51064], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4744, "seek": 1524464, "start": 15258.64, "end": 15260.64, "text": " this it would just take longer", "tokens": [51064, 341, 309, 576, 445, 747, 2854, 51164], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4745, "seek": 1524464, "start": 15260.64, "end": 15262.64, "text": " so that's what that's doing", "tokens": [51164, 370, 300, 311, 437, 300, 311, 884, 51264], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4746, "seek": 1524464, "start": 15262.64, "end": 15264.64, "text": " yeah", "tokens": [51264, 1338, 51364], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4747, "seek": 1524464, "start": 15264.64, "end": 15266.64, "text": " so now we have this drop-out", "tokens": [51364, 370, 586, 321, 362, 341, 3270, 12, 346, 51464], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4748, "seek": 1524464, "start": 15266.64, "end": 15268.64, "text": " of course and then in our forward pass", "tokens": [51464, 295, 1164, 293, 550, 294, 527, 2128, 1320, 51564], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4749, "seek": 1524464, "start": 15268.64, "end": 15270.64, "text": " let's", "tokens": [51564, 718, 311, 51664], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4750, "seek": 1524464, "start": 15270.64, "end": 15272.64, "text": " break this down step by step here", "tokens": [51664, 1821, 341, 760, 1823, 538, 1823, 510, 51764], "temperature": 0.0, "avg_logprob": -0.0766186540777033, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.000767250545322895}, {"id": 4751, "seek": 1527264, "start": 15272.64, "end": 15274.64, "text": " so we have a b by t by c", "tokens": [50364, 370, 321, 362, 257, 272, 538, 256, 538, 269, 50464], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4752, "seek": 1527264, "start": 15274.64, "end": 15276.64, "text": " so batch by time", "tokens": [50464, 370, 15245, 538, 565, 50564], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4753, "seek": 1527264, "start": 15276.64, "end": 15278.64, "text": " by channel is our shape", "tokens": [50564, 538, 2269, 307, 527, 3909, 50664], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4754, "seek": 1527264, "start": 15278.64, "end": 15280.64, "text": " we just unpack those numbers", "tokens": [50664, 321, 445, 26699, 729, 3547, 50764], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4755, "seek": 1527264, "start": 15280.64, "end": 15282.64, "text": " and then we have a key", "tokens": [50764, 293, 550, 321, 362, 257, 2141, 50864], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4756, "seek": 1527264, "start": 15282.64, "end": 15284.64, "text": " which is just calling this", "tokens": [50864, 597, 307, 445, 5141, 341, 50964], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4757, "seek": 1527264, "start": 15284.64, "end": 15286.64, "text": " linear transformation here on an input", "tokens": [50964, 8213, 9887, 510, 322, 364, 4846, 51064], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4758, "seek": 1527264, "start": 15286.64, "end": 15288.64, "text": " x", "tokens": [51064, 2031, 51164], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4759, "seek": 1527264, "start": 15288.64, "end": 15290.64, "text": " and then a query which is also", "tokens": [51164, 293, 550, 257, 14581, 597, 307, 611, 51264], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4760, "seek": 1527264, "start": 15290.64, "end": 15292.64, "text": " calling the same transformation but a different", "tokens": [51264, 5141, 264, 912, 9887, 457, 257, 819, 51364], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4761, "seek": 1527264, "start": 15292.64, "end": 15294.64, "text": " learnable transformation on x as well", "tokens": [51364, 1466, 712, 9887, 322, 2031, 382, 731, 51464], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4762, "seek": 1527264, "start": 15294.64, "end": 15296.64, "text": " so what we get", "tokens": [51464, 370, 437, 321, 483, 51564], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4763, "seek": 1527264, "start": 15296.64, "end": 15298.64, "text": " is this instead of b by t by c", "tokens": [51564, 307, 341, 2602, 295, 272, 538, 256, 538, 269, 51664], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4764, "seek": 1527264, "start": 15298.64, "end": 15300.64, "text": " we get b by t by head size", "tokens": [51664, 321, 483, 272, 538, 256, 538, 1378, 2744, 51764], "temperature": 0.0, "avg_logprob": -0.07847303204831824, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.011684438213706017}, {"id": 4765, "seek": 1530064, "start": 15300.64, "end": 15302.64, "text": " hence this transformation", "tokens": [50364, 16678, 341, 9887, 50464], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4766, "seek": 1530064, "start": 15302.64, "end": 15304.64, "text": " from 384 to 96", "tokens": [50464, 490, 12843, 19, 281, 24124, 50564], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4767, "seek": 1530064, "start": 15304.64, "end": 15306.64, "text": " so that's what that is", "tokens": [50564, 370, 300, 311, 437, 300, 307, 50664], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4768, "seek": 1530064, "start": 15306.64, "end": 15308.64, "text": " that's how these turn out here", "tokens": [50664, 300, 311, 577, 613, 1261, 484, 510, 50764], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4769, "seek": 1530064, "start": 15308.64, "end": 15310.64, "text": " so now we can actually compute", "tokens": [50764, 370, 586, 321, 393, 767, 14722, 50864], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4770, "seek": 1530064, "start": 15310.64, "end": 15312.64, "text": " the attention scores", "tokens": [50864, 264, 3202, 13444, 50964], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4771, "seek": 1530064, "start": 15312.64, "end": 15314.64, "text": " so what do we do", "tokens": [50964, 370, 437, 360, 321, 360, 51064], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4772, "seek": 1530064, "start": 15314.64, "end": 15316.64, "text": " we'll just say weights is our attention", "tokens": [51064, 321, 603, 445, 584, 17443, 307, 527, 3202, 51164], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4773, "seek": 1530064, "start": 15316.64, "end": 15318.64, "text": " weights are", "tokens": [51164, 17443, 366, 51264], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4774, "seek": 1530064, "start": 15318.64, "end": 15320.64, "text": " I guess you could say that", "tokens": [51264, 286, 2041, 291, 727, 584, 300, 51364], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4775, "seek": 1530064, "start": 15320.64, "end": 15322.64, "text": " we have our queries", "tokens": [51364, 321, 362, 527, 24109, 51464], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4776, "seek": 1530064, "start": 15322.64, "end": 15324.64, "text": " dot product matrix multiply", "tokens": [51464, 5893, 1674, 8141, 12972, 51564], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4777, "seek": 1530064, "start": 15324.64, "end": 15326.64, "text": " with the", "tokens": [51564, 365, 264, 51664], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4778, "seek": 1530064, "start": 15326.64, "end": 15328.64, "text": " keys transposed", "tokens": [51664, 9317, 7132, 1744, 51764], "temperature": 0.0, "avg_logprob": -0.07930046446779941, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.002286947565153241}, {"id": 4779, "seek": 1532864, "start": 15328.64, "end": 15330.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4780, "seek": 1532864, "start": 15330.64, "end": 15332.64, "text": " what does this actually look like", "tokens": [50464, 437, 775, 341, 767, 574, 411, 50564], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4781, "seek": 1532864, "start": 15332.64, "end": 15334.64, "text": " and I want to help you guys", "tokens": [50564, 293, 286, 528, 281, 854, 291, 1074, 50664], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4782, "seek": 1532864, "start": 15334.64, "end": 15336.64, "text": " sort of understand what transposing does here", "tokens": [50664, 1333, 295, 1223, 437, 7132, 6110, 775, 510, 50764], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4783, "seek": 1532864, "start": 15336.64, "end": 15338.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4784, "seek": 1532864, "start": 15338.64, "end": 15340.64, "text": " let's go back to here", "tokens": [50864, 718, 311, 352, 646, 281, 510, 50964], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4785, "seek": 1532864, "start": 15340.64, "end": 15342.64, "text": " and draw out what this is going to look like", "tokens": [50964, 293, 2642, 484, 437, 341, 307, 516, 281, 574, 411, 51064], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4786, "seek": 1532864, "start": 15342.64, "end": 15344.64, "text": " so", "tokens": [51064, 370, 51164], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4787, "seek": 1532864, "start": 15344.64, "end": 15346.64, "text": " essentially what transposing is going to do", "tokens": [51164, 4476, 437, 7132, 6110, 307, 516, 281, 360, 51264], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4788, "seek": 1532864, "start": 15346.64, "end": 15348.64, "text": " is", "tokens": [51264, 307, 51364], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4789, "seek": 1532864, "start": 15348.64, "end": 15350.64, "text": " it is just going to make sure", "tokens": [51364, 309, 307, 445, 516, 281, 652, 988, 51464], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4790, "seek": 1532864, "start": 15350.64, "end": 15352.64, "text": " let me draw this out", "tokens": [51464, 718, 385, 2642, 341, 484, 51564], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4791, "seek": 1532864, "start": 15352.64, "end": 15354.64, "text": " first", "tokens": [51564, 700, 51664], "temperature": 0.0, "avg_logprob": -0.07131069501241048, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.008574958890676498}, {"id": 4792, "seek": 1535464, "start": 15354.64, "end": 15356.64, "text": " so let's say you had", "tokens": [50364, 370, 718, 311, 584, 291, 632, 50464], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4793, "seek": 1535464, "start": 15356.64, "end": 15358.64, "text": " I don't know", "tokens": [50464, 286, 500, 380, 458, 50564], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4794, "seek": 1535464, "start": 15358.64, "end": 15360.64, "text": " maybe", "tokens": [50564, 1310, 50664], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4795, "seek": 1535464, "start": 15360.64, "end": 15362.64, "text": " a", "tokens": [50664, 257, 50764], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4796, "seek": 1535464, "start": 15366.64, "end": 15368.64, "text": " b", "tokens": [50964, 272, 51064], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4797, "seek": 1535464, "start": 15368.64, "end": 15370.64, "text": " c", "tokens": [51064, 269, 51164], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4798, "seek": 1535464, "start": 15370.64, "end": 15372.64, "text": " d", "tokens": [51164, 274, 51264], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4799, "seek": 1535464, "start": 15372.64, "end": 15374.64, "text": " and you have a", "tokens": [51264, 293, 291, 362, 257, 51364], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4800, "seek": 1535464, "start": 15374.64, "end": 15376.64, "text": " b", "tokens": [51364, 272, 51464], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4801, "seek": 1535464, "start": 15376.64, "end": 15378.64, "text": " c", "tokens": [51464, 269, 51564], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4802, "seek": 1535464, "start": 15378.64, "end": 15380.64, "text": " and d cool let's draw some lines", "tokens": [51564, 293, 274, 1627, 718, 311, 2642, 512, 3876, 51664], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4803, "seek": 1535464, "start": 15380.64, "end": 15382.64, "text": " to separate these", "tokens": [51664, 281, 4994, 613, 51764], "temperature": 0.0, "avg_logprob": -0.15803385617440208, "compression_ratio": 1.3295454545454546, "no_speech_prob": 0.010481470264494419}, {"id": 4804, "seek": 1538464, "start": 15384.64, "end": 15386.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4805, "seek": 1538464, "start": 15394.64, "end": 15396.64, "text": " awesome so essentially what this does", "tokens": [50864, 3476, 370, 4476, 437, 341, 775, 50964], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4806, "seek": 1538464, "start": 15396.64, "end": 15398.64, "text": " is the transposing", "tokens": [50964, 307, 264, 7132, 6110, 51064], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4807, "seek": 1538464, "start": 15398.64, "end": 15400.64, "text": " puts it into this form", "tokens": [51064, 8137, 309, 666, 341, 1254, 51164], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4808, "seek": 1538464, "start": 15400.64, "end": 15402.64, "text": " so if we didn't have", "tokens": [51164, 370, 498, 321, 994, 380, 362, 51264], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4809, "seek": 1538464, "start": 15402.64, "end": 15404.64, "text": " transposed then this would be in a different order", "tokens": [51264, 7132, 1744, 550, 341, 576, 312, 294, 257, 819, 1668, 51364], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4810, "seek": 1538464, "start": 15404.64, "end": 15406.64, "text": " it wouldn't be a b c d", "tokens": [51364, 309, 2759, 380, 312, 257, 272, 269, 274, 51464], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4811, "seek": 1538464, "start": 15406.64, "end": 15408.64, "text": " in both", "tokens": [51464, 294, 1293, 51564], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4812, "seek": 1538464, "start": 15408.64, "end": 15410.64, "text": " from like top to bottom left to right type of thing", "tokens": [51564, 490, 411, 1192, 281, 2767, 1411, 281, 558, 2010, 295, 551, 51664], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4813, "seek": 1538464, "start": 15410.64, "end": 15412.64, "text": " it would be in a different order", "tokens": [51664, 309, 576, 312, 294, 257, 819, 1668, 51764], "temperature": 0.0, "avg_logprob": -0.13070191406622167, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.0015976254362612963}, {"id": 4814, "seek": 1541264, "start": 15412.64, "end": 15414.64, "text": " but essentially not allow us", "tokens": [50364, 457, 4476, 406, 2089, 505, 50464], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4815, "seek": 1541264, "start": 15414.64, "end": 15416.64, "text": " to multiply them the same way", "tokens": [50464, 281, 12972, 552, 264, 912, 636, 50564], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4816, "seek": 1541264, "start": 15416.64, "end": 15418.64, "text": " so when we do a by a", "tokens": [50564, 370, 562, 321, 360, 257, 538, 257, 50664], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4817, "seek": 1541264, "start": 15418.64, "end": 15420.64, "text": " a times b", "tokens": [50664, 257, 1413, 272, 50764], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4818, "seek": 1541264, "start": 15420.64, "end": 15422.64, "text": " it's like sort of a direct", "tokens": [50764, 309, 311, 411, 1333, 295, 257, 2047, 50864], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4819, "seek": 1541264, "start": 15422.64, "end": 15424.64, "text": " multiply if you will", "tokens": [50864, 12972, 498, 291, 486, 50964], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4820, "seek": 1541264, "start": 15424.64, "end": 15426.64, "text": " I don't know if you remember times tables at all", "tokens": [50964, 286, 500, 380, 458, 498, 291, 1604, 1413, 8020, 412, 439, 51064], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4821, "seek": 1541264, "start": 15426.64, "end": 15428.64, "text": " from elementary school", "tokens": [51064, 490, 16429, 1395, 51164], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4822, "seek": 1541264, "start": 15428.64, "end": 15430.64, "text": " but that's pretty much what it is", "tokens": [51164, 457, 300, 311, 1238, 709, 437, 309, 307, 51264], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4823, "seek": 1541264, "start": 15430.64, "end": 15432.64, "text": " we're just setting up in a times table form", "tokens": [51264, 321, 434, 445, 3287, 493, 294, 257, 1413, 3199, 1254, 51364], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4824, "seek": 1541264, "start": 15432.64, "end": 15434.64, "text": " and we're computing attention scores that way", "tokens": [51364, 293, 321, 434, 15866, 3202, 13444, 300, 636, 51464], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4825, "seek": 1541264, "start": 15434.64, "end": 15436.64, "text": " so", "tokens": [51464, 370, 51564], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4826, "seek": 1541264, "start": 15436.64, "end": 15438.64, "text": " that's what that is", "tokens": [51564, 300, 311, 437, 300, 307, 51664], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4827, "seek": 1541264, "start": 15438.64, "end": 15440.64, "text": " that's what this transposing is doing", "tokens": [51664, 300, 311, 437, 341, 7132, 6110, 307, 884, 51764], "temperature": 0.0, "avg_logprob": -0.06499075067454371, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.004537362605333328}, {"id": 4828, "seek": 1544064, "start": 15440.64, "end": 15442.64, "text": " all this does is it just flips", "tokens": [50364, 439, 341, 775, 307, 309, 445, 40249, 50464], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4829, "seek": 1544064, "start": 15442.64, "end": 15444.64, "text": " the second last dimension", "tokens": [50464, 264, 1150, 1036, 10139, 50564], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4830, "seek": 1544064, "start": 15444.64, "end": 15446.64, "text": " with the last dimension", "tokens": [50564, 365, 264, 1036, 10139, 50664], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4831, "seek": 1544064, "start": 15446.64, "end": 15448.64, "text": " so", "tokens": [50664, 370, 50764], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4832, "seek": 1544064, "start": 15448.64, "end": 15450.64, "text": " in our case our second last", "tokens": [50764, 294, 527, 1389, 527, 1150, 1036, 50864], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4833, "seek": 1544064, "start": 15450.64, "end": 15452.64, "text": " is t and our last is head size", "tokens": [50864, 307, 256, 293, 527, 1036, 307, 1378, 2744, 50964], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4834, "seek": 1544064, "start": 15452.64, "end": 15454.64, "text": " so it just swaps these two", "tokens": [50964, 370, 309, 445, 1693, 2382, 613, 732, 51064], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4835, "seek": 1544064, "start": 15454.64, "end": 15456.64, "text": " so we get b by t by head size", "tokens": [51064, 370, 321, 483, 272, 538, 256, 538, 1378, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4836, "seek": 1544064, "start": 15456.64, "end": 15458.64, "text": " and then b by head size by t", "tokens": [51164, 293, 550, 272, 538, 1378, 2744, 538, 256, 51264], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4837, "seek": 1544064, "start": 15458.64, "end": 15460.64, "text": " we dot product these together", "tokens": [51264, 321, 5893, 1674, 613, 1214, 51364], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4838, "seek": 1544064, "start": 15460.64, "end": 15462.64, "text": " also keeping in mind our scaling", "tokens": [51364, 611, 5145, 294, 1575, 527, 21589, 51464], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4839, "seek": 1544064, "start": 15462.64, "end": 15464.64, "text": " here", "tokens": [51464, 510, 51564], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4840, "seek": 1544064, "start": 15464.64, "end": 15466.64, "text": " which is taking this", "tokens": [51564, 597, 307, 1940, 341, 51664], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4841, "seek": 1544064, "start": 15466.64, "end": 15468.64, "text": " we're just taking this scaling", "tokens": [51664, 321, 434, 445, 1940, 341, 21589, 51764], "temperature": 0.0, "avg_logprob": -0.08822563459288399, "compression_ratio": 1.9441340782122905, "no_speech_prob": 0.0020825830288231373}, {"id": 4842, "seek": 1546864, "start": 15468.64, "end": 15470.64, "text": " one", "tokens": [50364, 472, 50464], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4843, "seek": 1546864, "start": 15470.64, "end": 15472.64, "text": " over the square root of length", "tokens": [50464, 670, 264, 3732, 5593, 295, 4641, 50564], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4844, "seek": 1546864, "start": 15472.64, "end": 15474.64, "text": " of a row in the keys", "tokens": [50564, 295, 257, 5386, 294, 264, 9317, 50664], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4845, "seek": 1546864, "start": 15474.64, "end": 15476.64, "text": " if we look at this here", "tokens": [50664, 498, 321, 574, 412, 341, 510, 50764], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4846, "seek": 1546864, "start": 15476.64, "end": 15478.64, "text": " now there's little analogy", "tokens": [50764, 586, 456, 311, 707, 21663, 50864], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4847, "seek": 1546864, "start": 15478.64, "end": 15480.64, "text": " I'd like to provide for this scaling", "tokens": [50864, 286, 1116, 411, 281, 2893, 337, 341, 21589, 50964], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4848, "seek": 1546864, "start": 15480.64, "end": 15482.64, "text": " right here", "tokens": [50964, 558, 510, 51064], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4849, "seek": 1546864, "start": 15482.64, "end": 15484.64, "text": " so imagine in a room", "tokens": [51064, 370, 3811, 294, 257, 1808, 51164], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4850, "seek": 1546864, "start": 15484.64, "end": 15486.64, "text": " with a group of people and you're trying to understand", "tokens": [51164, 365, 257, 1594, 295, 561, 293, 291, 434, 1382, 281, 1223, 51264], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4851, "seek": 1546864, "start": 15486.64, "end": 15488.64, "text": " the overall conversation", "tokens": [51264, 264, 4787, 3761, 51364], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4852, "seek": 1546864, "start": 15488.64, "end": 15490.64, "text": " if everyone is talking at once", "tokens": [51364, 498, 1518, 307, 1417, 412, 1564, 51464], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4853, "seek": 1546864, "start": 15490.64, "end": 15492.64, "text": " it might be challenging to keep track", "tokens": [51464, 309, 1062, 312, 7595, 281, 1066, 2837, 51564], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4854, "seek": 1546864, "start": 15492.64, "end": 15494.64, "text": " of what's being said", "tokens": [51564, 295, 437, 311, 885, 848, 51664], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4855, "seek": 1546864, "start": 15494.64, "end": 15496.64, "text": " it would be more manageable if you could focus on", "tokens": [51664, 309, 576, 312, 544, 38798, 498, 291, 727, 1879, 322, 51764], "temperature": 0.0, "avg_logprob": -0.08935231561059351, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.001597621594555676}, {"id": 4856, "seek": 1549664, "start": 15496.64, "end": 15498.64, "text": " time right?", "tokens": [50364, 565, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4857, "seek": 1549664, "start": 15498.64, "end": 15500.64, "text": " so that's similar to how a multi head attention", "tokens": [50464, 370, 300, 311, 2531, 281, 577, 257, 4825, 1378, 3202, 50564], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4858, "seek": 1549664, "start": 15500.64, "end": 15502.64, "text": " in a transformer works", "tokens": [50564, 294, 257, 31782, 1985, 50664], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4859, "seek": 1549664, "start": 15502.64, "end": 15504.64, "text": " so each of these heads", "tokens": [50664, 370, 1184, 295, 613, 8050, 50764], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4860, "seek": 1549664, "start": 15504.64, "end": 15506.64, "text": " divides the original problem", "tokens": [50764, 41347, 264, 3380, 1154, 50864], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4861, "seek": 1549664, "start": 15506.64, "end": 15508.64, "text": " of understanding the entire conversation", "tokens": [50864, 295, 3701, 264, 2302, 3761, 50964], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4862, "seek": 1549664, "start": 15508.64, "end": 15510.64, "text": " i.e. the entire input sequence", "tokens": [50964, 741, 13, 68, 13, 264, 2302, 4846, 8310, 51064], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4863, "seek": 1549664, "start": 15510.64, "end": 15512.64, "text": " into smaller more manageable", "tokens": [51064, 666, 4356, 544, 38798, 51164], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4864, "seek": 1549664, "start": 15512.64, "end": 15514.64, "text": " sub problems", "tokens": [51164, 1422, 2740, 51264], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4865, "seek": 1549664, "start": 15514.64, "end": 15516.64, "text": " each of these sub problems is a head", "tokens": [51264, 1184, 295, 613, 1422, 2740, 307, 257, 1378, 51364], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4866, "seek": 1549664, "start": 15516.64, "end": 15518.64, "text": " so the head size", "tokens": [51364, 370, 264, 1378, 2744, 51464], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4867, "seek": 1549664, "start": 15518.64, "end": 15520.64, "text": " is the number of these sub problems", "tokens": [51464, 307, 264, 1230, 295, 613, 1422, 2740, 51564], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4868, "seek": 1549664, "start": 15520.64, "end": 15522.64, "text": " now consider what happens when each person", "tokens": [51564, 586, 1949, 437, 2314, 562, 1184, 954, 51664], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4869, "seek": 1549664, "start": 15522.64, "end": 15524.64, "text": " talks louder or quieter", "tokens": [51664, 6686, 22717, 420, 43339, 51764], "temperature": 0.0, "avg_logprob": -0.08754167102632068, "compression_ratio": 1.8, "no_speech_prob": 0.0010321500012651086}, {"id": 4870, "seek": 1552464, "start": 15524.64, "end": 15526.64, "text": " if someone speaks too loudly", "tokens": [50364, 498, 1580, 10789, 886, 22958, 50464], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4871, "seek": 1552464, "start": 15526.64, "end": 15528.64, "text": " or", "tokens": [50464, 420, 50564], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4872, "seek": 1552464, "start": 15528.64, "end": 15530.64, "text": " the values and the vectors are very large", "tokens": [50564, 264, 4190, 293, 264, 18875, 366, 588, 2416, 50664], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4873, "seek": 1552464, "start": 15530.64, "end": 15532.64, "text": " it might drown out the others", "tokens": [50664, 309, 1062, 20337, 484, 264, 2357, 50764], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4874, "seek": 1552464, "start": 15532.64, "end": 15534.64, "text": " this could make it difficult to understand the conversation", "tokens": [50764, 341, 727, 652, 309, 2252, 281, 1223, 264, 3761, 50864], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4875, "seek": 1552464, "start": 15534.64, "end": 15536.64, "text": " because you're only hearing one voice", "tokens": [50864, 570, 291, 434, 787, 4763, 472, 3177, 50964], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4876, "seek": 1552464, "start": 15536.64, "end": 15538.64, "text": " or most of one voice", "tokens": [50964, 420, 881, 295, 472, 3177, 51064], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4877, "seek": 1552464, "start": 15538.64, "end": 15540.64, "text": " to prevent this", "tokens": [51064, 281, 4871, 341, 51164], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4878, "seek": 1552464, "start": 15540.64, "end": 15542.64, "text": " we want to control how loud", "tokens": [51164, 321, 528, 281, 1969, 577, 6588, 51264], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4879, "seek": 1552464, "start": 15542.64, "end": 15544.64, "text": " or how quiet each person is talking", "tokens": [51264, 420, 577, 5677, 1184, 954, 307, 1417, 51364], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4880, "seek": 1552464, "start": 15544.64, "end": 15546.64, "text": " so we can hear everyone evenly", "tokens": [51364, 370, 321, 393, 1568, 1518, 17658, 51464], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4881, "seek": 1552464, "start": 15546.64, "end": 15548.64, "text": " the dot product of the", "tokens": [51464, 264, 5893, 1674, 295, 264, 51564], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4882, "seek": 1552464, "start": 15548.64, "end": 15550.64, "text": " query and key vectors", "tokens": [51564, 14581, 293, 2141, 18875, 51664], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4883, "seek": 1552464, "start": 15550.64, "end": 15552.64, "text": " in the attention mechanism", "tokens": [51664, 294, 264, 3202, 7513, 51764], "temperature": 0.0, "avg_logprob": -0.06608850551101397, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0030268423724919558}, {"id": 4884, "seek": 1555264, "start": 15552.64, "end": 15554.64, "text": " we want to check how loud each of voices", "tokens": [50364, 321, 528, 281, 1520, 577, 6588, 1184, 295, 9802, 50464], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4885, "seek": 1555264, "start": 15554.64, "end": 15556.64, "text": " if the vectors are very large", "tokens": [50464, 498, 264, 18875, 366, 588, 2416, 50564], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4886, "seek": 1555264, "start": 15556.64, "end": 15558.64, "text": " or high dimensional", "tokens": [50564, 420, 1090, 18795, 50664], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4887, "seek": 1555264, "start": 15558.64, "end": 15560.64, "text": " or many people are talking", "tokens": [50664, 420, 867, 561, 366, 1417, 50764], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4888, "seek": 1555264, "start": 15560.64, "end": 15562.64, "text": " the dot product can be very large", "tokens": [50764, 264, 5893, 1674, 393, 312, 588, 2416, 50864], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4889, "seek": 1555264, "start": 15562.64, "end": 15564.64, "text": " to control this volume", "tokens": [50864, 281, 1969, 341, 5523, 50964], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4890, "seek": 1555264, "start": 15564.64, "end": 15566.64, "text": " by scaling down the dot product", "tokens": [50964, 538, 21589, 760, 264, 5893, 1674, 51064], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4891, "seek": 1555264, "start": 15566.64, "end": 15568.64, "text": " using the square root of the head size", "tokens": [51064, 1228, 264, 3732, 5593, 295, 264, 1378, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4892, "seek": 1555264, "start": 15570.64, "end": 15572.64, "text": " this scaling helps ensure that no single", "tokens": [51264, 341, 21589, 3665, 5586, 300, 572, 2167, 51364], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4893, "seek": 1555264, "start": 15572.64, "end": 15574.64, "text": " voice is too dominant", "tokens": [51364, 3177, 307, 886, 15657, 51464], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4894, "seek": 1555264, "start": 15574.64, "end": 15576.64, "text": " allowing us to hear all the voices evenly", "tokens": [51464, 8293, 505, 281, 1568, 439, 264, 9802, 17658, 51564], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4895, "seek": 1555264, "start": 15576.64, "end": 15578.64, "text": " this is why we don't scale", "tokens": [51564, 341, 307, 983, 321, 500, 380, 4373, 51664], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4896, "seek": 1555264, "start": 15578.64, "end": 15580.64, "text": " by the number of heads", "tokens": [51664, 538, 264, 1230, 295, 8050, 51764], "temperature": 0.0, "avg_logprob": -0.08969504810939326, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.007008539978414774}, {"id": 4897, "seek": 1558064, "start": 15580.64, "end": 15582.64, "text": " time steps", "tokens": [50364, 565, 4439, 50464], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4898, "seek": 1558064, "start": 15582.64, "end": 15584.64, "text": " they don't directly affect how loud each voice is", "tokens": [50464, 436, 500, 380, 3838, 3345, 577, 6588, 1184, 3177, 307, 50564], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4899, "seek": 1558064, "start": 15584.64, "end": 15586.64, "text": " so in sum", "tokens": [50564, 370, 294, 2408, 50664], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4900, "seek": 1558064, "start": 15586.64, "end": 15588.64, "text": " multi head attention allows us to focus on", "tokens": [50664, 4825, 1378, 3202, 4045, 505, 281, 1879, 322, 50764], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4901, "seek": 1558064, "start": 15588.64, "end": 15590.64, "text": " different parts of the conversation", "tokens": [50764, 819, 3166, 295, 264, 3761, 50864], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4902, "seek": 1558064, "start": 15590.64, "end": 15592.64, "text": " and scaling helps us to hear", "tokens": [50864, 293, 21589, 3665, 505, 281, 1568, 50964], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4903, "seek": 1558064, "start": 15592.64, "end": 15594.64, "text": " all parts of the conversation evenly", "tokens": [50964, 439, 3166, 295, 264, 3761, 17658, 51064], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4904, "seek": 1558064, "start": 15594.64, "end": 15596.64, "text": " allowing us to understand", "tokens": [51064, 8293, 505, 281, 1223, 51164], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4905, "seek": 1558064, "start": 15596.64, "end": 15598.64, "text": " the overall conversation better", "tokens": [51164, 264, 4787, 3761, 1101, 51264], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4906, "seek": 1558064, "start": 15598.64, "end": 15600.64, "text": " so hopefully that helps you understand exactly", "tokens": [51264, 370, 4696, 300, 3665, 291, 1223, 2293, 51364], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4907, "seek": 1558064, "start": 15600.64, "end": 15602.64, "text": " what this scaling is doing", "tokens": [51364, 437, 341, 21589, 307, 884, 51464], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4908, "seek": 1558064, "start": 15602.64, "end": 15604.64, "text": " so now let's go into the rest of this here", "tokens": [51464, 370, 586, 718, 311, 352, 666, 264, 1472, 295, 341, 510, 51564], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4909, "seek": 1558064, "start": 15606.64, "end": 15608.64, "text": " so we have this scaling applied", "tokens": [51664, 370, 321, 362, 341, 21589, 6456, 51764], "temperature": 0.0, "avg_logprob": -0.09892810639880953, "compression_ratio": 1.9009009009009008, "no_speech_prob": 0.012231883592903614}, {"id": 4910, "seek": 1560864, "start": 15608.64, "end": 15610.64, "text": " for our head size", "tokens": [50364, 337, 527, 1378, 2744, 50464], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4911, "seek": 1560864, "start": 15610.64, "end": 15612.64, "text": " our head size dimension", "tokens": [50464, 527, 1378, 2744, 10139, 50564], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4912, "seek": 1560864, "start": 15612.64, "end": 15614.64, "text": " we're doing this", "tokens": [50564, 321, 434, 884, 341, 50664], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4913, "seek": 1560864, "start": 15614.64, "end": 15616.64, "text": " dot product matrix multiplication", "tokens": [50664, 5893, 1674, 8141, 27290, 50764], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4914, "seek": 1560864, "start": 15616.64, "end": 15618.64, "text": " here we get our B by T by T", "tokens": [50764, 510, 321, 483, 527, 363, 538, 314, 538, 314, 50864], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4915, "seek": 1560864, "start": 15618.64, "end": 15620.64, "text": " and then what is this", "tokens": [50864, 293, 550, 437, 307, 341, 50964], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4916, "seek": 1560864, "start": 15620.64, "end": 15622.64, "text": " masked fill doing", "tokens": [50964, 45249, 2836, 884, 51064], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4917, "seek": 1560864, "start": 15622.64, "end": 15624.64, "text": " so let me help you illustrate this here", "tokens": [51064, 370, 718, 385, 854, 291, 23221, 341, 510, 51164], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4918, "seek": 1560864, "start": 15624.64, "end": 15626.64, "text": " so mask fill", "tokens": [51164, 370, 6094, 2836, 51264], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4919, "seek": 1560864, "start": 15626.64, "end": 15628.64, "text": " is essentially", "tokens": [51264, 307, 4476, 51364], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4920, "seek": 1560864, "start": 15628.64, "end": 15630.64, "text": " we'll say block size", "tokens": [51364, 321, 603, 584, 3461, 2744, 51464], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4921, "seek": 1560864, "start": 15630.64, "end": 15632.64, "text": " is 3 here alright", "tokens": [51464, 307, 805, 510, 5845, 51564], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4922, "seek": 1560864, "start": 15632.64, "end": 15634.64, "text": " so we have", "tokens": [51564, 370, 321, 362, 51664], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4923, "seek": 1560864, "start": 15634.64, "end": 15636.64, "text": " initially", "tokens": [51664, 9105, 51764], "temperature": 0.0, "avg_logprob": -0.16507632009099038, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.071474090218544}, {"id": 4924, "seek": 1563664, "start": 15636.64, "end": 15638.64, "text": " like a 1", "tokens": [50364, 411, 257, 502, 50464], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4925, "seek": 1563664, "start": 15638.64, "end": 15640.64, "text": " a 0.6", "tokens": [50464, 257, 1958, 13, 21, 50564], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4926, "seek": 1563664, "start": 15640.64, "end": 15642.64, "text": " and then like a 0.4", "tokens": [50564, 293, 550, 411, 257, 1958, 13, 19, 50664], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4927, "seek": 1563664, "start": 15642.64, "end": 15644.64, "text": " then our next one is", "tokens": [50664, 550, 527, 958, 472, 307, 50764], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4928, "seek": 1563664, "start": 15646.64, "end": 15648.64, "text": " yeah we'll just say all of these are the same", "tokens": [50864, 1338, 321, 603, 445, 584, 439, 295, 613, 366, 264, 912, 50964], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4929, "seek": 1563664, "start": 15650.64, "end": 15652.64, "text": " so essentially", "tokens": [51064, 370, 4476, 51164], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4930, "seek": 1563664, "start": 15652.64, "end": 15654.64, "text": " in our first one", "tokens": [51164, 294, 527, 700, 472, 51264], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4931, "seek": 1563664, "start": 15654.64, "end": 15656.64, "text": " we want to mask out everything", "tokens": [51264, 321, 528, 281, 6094, 484, 1203, 51364], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4932, "seek": 1563664, "start": 15656.64, "end": 15658.64, "text": " except for the first time step", "tokens": [51364, 3993, 337, 264, 700, 565, 1823, 51464], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4933, "seek": 1563664, "start": 15658.64, "end": 15660.64, "text": " and then when we advance one", "tokens": [51464, 293, 550, 562, 321, 7295, 472, 51564], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4934, "seek": 1563664, "start": 15660.64, "end": 15662.64, "text": " so let's just change this here back to 0", "tokens": [51564, 370, 718, 311, 445, 1319, 341, 510, 646, 281, 1958, 51664], "temperature": 0.0, "avg_logprob": -0.09481324820682921, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.006094961892813444}, {"id": 4935, "seek": 1566664, "start": 15666.64, "end": 15668.64, "text": " when we go on to the next time step", "tokens": [50364, 562, 321, 352, 322, 281, 264, 958, 565, 1823, 50464], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4936, "seek": 1566664, "start": 15668.64, "end": 15670.64, "text": " we want to expose the next piece", "tokens": [50464, 321, 528, 281, 19219, 264, 958, 2522, 50564], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4937, "seek": 1566664, "start": 15670.64, "end": 15672.64, "text": " so 0.6 I believe it was", "tokens": [50564, 370, 1958, 13, 21, 286, 1697, 309, 390, 50664], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4938, "seek": 1566664, "start": 15672.64, "end": 15674.64, "text": " and then a 0 again", "tokens": [50664, 293, 550, 257, 1958, 797, 50764], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4939, "seek": 1566664, "start": 15674.64, "end": 15676.64, "text": " and then when we expose the next time step after that", "tokens": [50764, 293, 550, 562, 321, 19219, 264, 958, 565, 1823, 934, 300, 50864], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4940, "seek": 1566664, "start": 15676.64, "end": 15678.64, "text": " we want to expose all of them", "tokens": [50864, 321, 528, 281, 19219, 439, 295, 552, 50964], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4941, "seek": 1566664, "start": 15678.64, "end": 15680.64, "text": " so just kind of what this means is", "tokens": [50964, 370, 445, 733, 295, 437, 341, 1355, 307, 51064], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4942, "seek": 1566664, "start": 15680.64, "end": 15682.64, "text": " as we", "tokens": [51064, 382, 321, 51164], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4943, "seek": 1566664, "start": 15682.64, "end": 15684.64, "text": " as the time step advances", "tokens": [51164, 382, 264, 565, 1823, 25297, 51264], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4944, "seek": 1566664, "start": 15684.64, "end": 15686.64, "text": " in this sort of I guess vertical", "tokens": [51264, 294, 341, 1333, 295, 286, 2041, 9429, 51364], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4945, "seek": 1566664, "start": 15686.64, "end": 15688.64, "text": " part", "tokens": [51364, 644, 51464], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4946, "seek": 1566664, "start": 15688.64, "end": 15690.64, "text": " is every time this steps 1", "tokens": [51464, 307, 633, 565, 341, 4439, 502, 51564], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4947, "seek": 1566664, "start": 15690.64, "end": 15692.64, "text": " we just want to expose one more token", "tokens": [51564, 321, 445, 528, 281, 19219, 472, 544, 14862, 51664], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4948, "seek": 1566664, "start": 15692.64, "end": 15694.64, "text": " or one more", "tokens": [51664, 420, 472, 544, 51764], "temperature": 0.0, "avg_logprob": -0.0721985580574753, "compression_ratio": 1.9137055837563453, "no_speech_prob": 0.008845241740345955}, {"id": 4949, "seek": 1569464, "start": 15694.64, "end": 15696.64, "text": " and then we'll use sort of in like a staircase format", "tokens": [50364, 293, 550, 321, 603, 764, 1333, 295, 294, 411, 257, 35359, 7877, 50464], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4950, "seek": 1569464, "start": 15696.64, "end": 15698.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4951, "seek": 1569464, "start": 15698.64, "end": 15700.64, "text": " essentially what this mask fill is doing", "tokens": [50564, 4476, 437, 341, 6094, 2836, 307, 884, 50664], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4952, "seek": 1569464, "start": 15700.64, "end": 15702.64, "text": " is it's making this", "tokens": [50664, 307, 309, 311, 1455, 341, 50764], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4953, "seek": 1569464, "start": 15702.64, "end": 15704.64, "text": " T by T so block size by block size", "tokens": [50764, 314, 538, 314, 370, 3461, 2744, 538, 3461, 2744, 50864], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4954, "seek": 1569464, "start": 15704.64, "end": 15706.64, "text": " and", "tokens": [50864, 293, 50964], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4955, "seek": 1569464, "start": 15706.64, "end": 15708.64, "text": " for each of these values we're going to set them", "tokens": [50964, 337, 1184, 295, 613, 4190, 321, 434, 516, 281, 992, 552, 51064], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4956, "seek": 1569464, "start": 15708.64, "end": 15710.64, "text": " to negative infinity", "tokens": [51064, 281, 3671, 13202, 51164], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4957, "seek": 1569464, "start": 15710.64, "end": 15712.64, "text": " so for each value that's 0", "tokens": [51164, 370, 337, 1184, 2158, 300, 311, 1958, 51264], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4958, "seek": 1569464, "start": 15712.64, "end": 15714.64, "text": " we're going to make that the float value negative infinity", "tokens": [51264, 321, 434, 516, 281, 652, 300, 264, 15706, 2158, 3671, 13202, 51364], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4959, "seek": 1569464, "start": 15714.64, "end": 15716.64, "text": " so it's going to look like this", "tokens": [51364, 370, 309, 311, 516, 281, 574, 411, 341, 51464], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4960, "seek": 1569464, "start": 15716.64, "end": 15718.64, "text": " negative infinity", "tokens": [51464, 3671, 13202, 51564], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4961, "seek": 1569464, "start": 15720.64, "end": 15722.64, "text": " negative infinity", "tokens": [51664, 3671, 13202, 51764], "temperature": 0.0, "avg_logprob": -0.12850244090242205, "compression_ratio": 1.958762886597938, "no_speech_prob": 0.010486884042620659}, {"id": 4962, "seek": 1572464, "start": 15724.64, "end": 15726.64, "text": " just like that", "tokens": [50364, 445, 411, 300, 50464], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4963, "seek": 1572464, "start": 15726.64, "end": 15728.64, "text": " so essentially what happens after this", "tokens": [50464, 370, 4476, 437, 2314, 934, 341, 50564], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4964, "seek": 1572464, "start": 15728.64, "end": 15730.64, "text": " is our softmax", "tokens": [50564, 307, 527, 2787, 41167, 50664], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4965, "seek": 1572464, "start": 15730.64, "end": 15732.64, "text": " is going to take these values", "tokens": [50664, 307, 516, 281, 747, 613, 4190, 50764], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4966, "seek": 1572464, "start": 15732.64, "end": 15734.64, "text": " and it's going to exponentiate normalize them", "tokens": [50764, 293, 309, 311, 516, 281, 37871, 13024, 2710, 1125, 552, 50864], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4967, "seek": 1572464, "start": 15734.64, "end": 15736.64, "text": " we already went over the soft", "tokens": [50864, 321, 1217, 1437, 670, 264, 2787, 50964], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4968, "seek": 1572464, "start": 15736.64, "end": 15738.64, "text": " softmax previously", "tokens": [50964, 2787, 41167, 8046, 51064], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4969, "seek": 1572464, "start": 15738.64, "end": 15740.64, "text": " but", "tokens": [51064, 457, 51164], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4970, "seek": 1572464, "start": 15740.64, "end": 15742.64, "text": " essentially what this is going to do this", "tokens": [51164, 4476, 437, 341, 307, 516, 281, 360, 341, 51264], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4971, "seek": 1572464, "start": 15742.64, "end": 15744.64, "text": " this last dimension here", "tokens": [51264, 341, 1036, 10139, 510, 51364], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4972, "seek": 1572464, "start": 15744.64, "end": 15746.64, "text": " concatenate", "tokens": [51364, 1588, 7186, 473, 51464], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4973, "seek": 1572464, "start": 15746.64, "end": 15748.64, "text": " or not concatenate", "tokens": [51464, 420, 406, 1588, 7186, 473, 51564], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4974, "seek": 1572464, "start": 15748.64, "end": 15750.64, "text": " rather apply the softmax along the last dimension", "tokens": [51564, 2831, 3079, 264, 2787, 41167, 2051, 264, 1036, 10139, 51664], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4975, "seek": 1572464, "start": 15750.64, "end": 15752.64, "text": " is it's going to do that", "tokens": [51664, 307, 309, 311, 516, 281, 360, 300, 51764], "temperature": 0.0, "avg_logprob": -0.069776185353597, "compression_ratio": 1.9170984455958548, "no_speech_prob": 0.002182460855692625}, {"id": 4976, "seek": 1575264, "start": 15752.64, "end": 15754.64, "text": " in this sort of horizontal here", "tokens": [50364, 294, 341, 1333, 295, 12750, 510, 50464], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4977, "seek": 1575264, "start": 15754.64, "end": 15756.64, "text": " so this last", "tokens": [50464, 370, 341, 1036, 50564], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4978, "seek": 1575264, "start": 15756.64, "end": 15758.64, "text": " this last T", "tokens": [50564, 341, 1036, 314, 50664], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4979, "seek": 1575264, "start": 15758.64, "end": 15760.64, "text": " it's like blocks", "tokens": [50664, 309, 311, 411, 8474, 50764], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4980, "seek": 1575264, "start": 15760.64, "end": 15762.64, "text": " it's like block size by block size", "tokens": [50764, 309, 311, 411, 3461, 2744, 538, 3461, 2744, 50864], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4981, "seek": 1575264, "start": 15762.64, "end": 15764.64, "text": " so it's like we'll say", "tokens": [50864, 370, 309, 311, 411, 321, 603, 584, 50964], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4982, "seek": 1575264, "start": 15764.64, "end": 15766.64, "text": " T1 and T2", "tokens": [50964, 314, 16, 293, 314, 17, 51064], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4983, "seek": 1575264, "start": 15766.64, "end": 15768.64, "text": " each of these being like the block size", "tokens": [51064, 1184, 295, 613, 885, 411, 264, 3461, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4984, "seek": 1575264, "start": 15768.64, "end": 15770.64, "text": " we're just going to do it to this last T2 here", "tokens": [51164, 321, 434, 445, 516, 281, 360, 309, 281, 341, 1036, 314, 17, 510, 51264], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4985, "seek": 1575264, "start": 15770.64, "end": 15772.64, "text": " and this horizontal is T2", "tokens": [51264, 293, 341, 12750, 307, 314, 17, 51364], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4986, "seek": 1575264, "start": 15772.64, "end": 15774.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4987, "seek": 1575264, "start": 15774.64, "end": 15776.64, "text": " hopefully that makes sense", "tokens": [51464, 4696, 300, 1669, 2020, 51564], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4988, "seek": 1575264, "start": 15776.64, "end": 15778.64, "text": " and essentially", "tokens": [51564, 293, 4476, 51664], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4989, "seek": 1575264, "start": 15778.64, "end": 15780.64, "text": " what this exponentiation is going to do", "tokens": [51664, 437, 341, 37871, 6642, 307, 516, 281, 360, 51764], "temperature": 0.0, "avg_logprob": -0.07997843954298231, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.0032726661302149296}, {"id": 4990, "seek": 1578064, "start": 15780.64, "end": 15782.64, "text": " is it's going to turn these values to 0", "tokens": [50364, 307, 309, 311, 516, 281, 1261, 613, 4190, 281, 1958, 50464], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4991, "seek": 1578064, "start": 15782.64, "end": 15784.64, "text": " and", "tokens": [50464, 293, 50564], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4992, "seek": 1578064, "start": 15784.64, "end": 15786.64, "text": " this one is obviously going to remain a 1", "tokens": [50564, 341, 472, 307, 2745, 516, 281, 6222, 257, 502, 50664], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4993, "seek": 1578064, "start": 15786.64, "end": 15788.64, "text": " and then", "tokens": [50664, 293, 550, 50764], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4994, "seek": 1578064, "start": 15788.64, "end": 15790.64, "text": " it's going to turn these", "tokens": [50764, 309, 311, 516, 281, 1261, 613, 50864], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4995, "seek": 1578064, "start": 15790.64, "end": 15792.64, "text": " into 0", "tokens": [50864, 666, 1958, 50964], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4996, "seek": 1578064, "start": 15792.64, "end": 15794.64, "text": " and it's going to probably sharpen this 1 here", "tokens": [50964, 293, 309, 311, 516, 281, 1391, 31570, 341, 502, 510, 51064], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4997, "seek": 1578064, "start": 15794.64, "end": 15796.64, "text": " so this 1 is going to be more significant", "tokens": [51064, 370, 341, 502, 307, 516, 281, 312, 544, 4776, 51164], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4998, "seek": 1578064, "start": 15796.64, "end": 15798.64, "text": " it's going to grow more than the 0.6", "tokens": [51164, 309, 311, 516, 281, 1852, 544, 813, 264, 1958, 13, 21, 51264], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 4999, "seek": 1578064, "start": 15798.64, "end": 15800.64, "text": " because we're exponentiating", "tokens": [51264, 570, 321, 434, 37871, 72, 990, 51364], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 5000, "seek": 1578064, "start": 15800.64, "end": 15802.64, "text": " and then same here so this 1 is going to be", "tokens": [51364, 293, 550, 912, 510, 370, 341, 502, 307, 516, 281, 312, 51464], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 5001, "seek": 1578064, "start": 15802.64, "end": 15804.64, "text": " very, very sharp", "tokens": [51464, 588, 11, 588, 8199, 51564], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 5002, "seek": 1578064, "start": 15804.64, "end": 15806.64, "text": " compared to 0.6", "tokens": [51564, 5347, 281, 1958, 13, 21, 51664], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 5003, "seek": 1578064, "start": 15806.64, "end": 15808.64, "text": " or 0.4", "tokens": [51664, 420, 1958, 13, 19, 51764], "temperature": 0.0, "avg_logprob": -0.07203882535298665, "compression_ratio": 2.0054945054945055, "no_speech_prob": 0.002472364343702793}, {"id": 5004, "seek": 1580864, "start": 15808.64, "end": 15810.64, "text": " that's what the softmax does", "tokens": [50364, 300, 311, 437, 264, 2787, 41167, 775, 50464], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5005, "seek": 1580864, "start": 15810.64, "end": 15812.64, "text": " essentially the point of the softmax function", "tokens": [50464, 4476, 264, 935, 295, 264, 2787, 41167, 2445, 50564], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5006, "seek": 1580864, "start": 15812.64, "end": 15814.64, "text": " is to", "tokens": [50564, 307, 281, 50664], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5007, "seek": 1580864, "start": 15814.64, "end": 15816.64, "text": " make the values stand out more", "tokens": [50664, 652, 264, 4190, 1463, 484, 544, 50764], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5008, "seek": 1580864, "start": 15816.64, "end": 15818.64, "text": " it's to make the model more confident", "tokens": [50764, 309, 311, 281, 652, 264, 2316, 544, 6679, 50864], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5009, "seek": 1580864, "start": 15818.64, "end": 15820.64, "text": " in highlighting attention scores", "tokens": [50864, 294, 26551, 3202, 13444, 50964], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5010, "seek": 1580864, "start": 15820.64, "end": 15822.64, "text": " so when you have one value that's like very big", "tokens": [50964, 370, 562, 291, 362, 472, 2158, 300, 311, 411, 588, 955, 51064], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5011, "seek": 1580864, "start": 15822.64, "end": 15824.64, "text": " but not too big, not exploding", "tokens": [51064, 457, 406, 886, 955, 11, 406, 35175, 51164], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5012, "seek": 1580864, "start": 15824.64, "end": 15826.64, "text": " because of our scaling, right?", "tokens": [51164, 570, 295, 527, 21589, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5013, "seek": 1580864, "start": 15826.64, "end": 15828.64, "text": " we want to keep a minor scaling", "tokens": [51264, 321, 528, 281, 1066, 257, 6696, 21589, 51364], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5014, "seek": 1580864, "start": 15828.64, "end": 15830.64, "text": " but when a value is big, when a score", "tokens": [51364, 457, 562, 257, 2158, 307, 955, 11, 562, 257, 6175, 51464], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5015, "seek": 1580864, "start": 15830.64, "end": 15832.64, "text": " or attention score is very big", "tokens": [51464, 420, 3202, 6175, 307, 588, 955, 51564], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5016, "seek": 1580864, "start": 15832.64, "end": 15834.64, "text": " we want the model to put a lot of focus on that", "tokens": [51564, 321, 528, 264, 2316, 281, 829, 257, 688, 295, 1879, 322, 300, 51664], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5017, "seek": 1580864, "start": 15834.64, "end": 15836.64, "text": " and to say this", "tokens": [51664, 293, 281, 584, 341, 51764], "temperature": 0.0, "avg_logprob": -0.08968686687853909, "compression_ratio": 1.8577235772357723, "no_speech_prob": 0.0014549086336046457}, {"id": 5018, "seek": 1583664, "start": 15836.64, "end": 15838.64, "text": " the entire sentence or the entire thing of tokens", "tokens": [50364, 264, 2302, 8174, 420, 264, 2302, 551, 295, 22667, 50464], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5019, "seek": 1583664, "start": 15838.64, "end": 15840.64, "text": " and we just want it to learn the most", "tokens": [50464, 293, 321, 445, 528, 309, 281, 1466, 264, 881, 50564], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5020, "seek": 1583664, "start": 15840.64, "end": 15842.64, "text": " from that", "tokens": [50564, 490, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5021, "seek": 1583664, "start": 15842.64, "end": 15844.64, "text": " so essentially that's what softmax is doing", "tokens": [50664, 370, 4476, 300, 311, 437, 2787, 41167, 307, 884, 50764], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5022, "seek": 1583664, "start": 15844.64, "end": 15846.64, "text": " instead of just a normal", "tokens": [50764, 2602, 295, 445, 257, 2710, 50864], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5023, "seek": 1583664, "start": 15846.64, "end": 15848.64, "text": " normalizing mechanism", "tokens": [50864, 2710, 3319, 7513, 50964], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5024, "seek": 1583664, "start": 15848.64, "end": 15850.64, "text": " it's just doing some exponentiation", "tokens": [50964, 309, 311, 445, 884, 512, 37871, 6642, 51064], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5025, "seek": 1583664, "start": 15850.64, "end": 15852.64, "text": " to that to make the model more confident", "tokens": [51064, 281, 300, 281, 652, 264, 2316, 544, 6679, 51164], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5026, "seek": 1583664, "start": 15852.64, "end": 15854.64, "text": " in its predictions", "tokens": [51164, 294, 1080, 21264, 51264], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5027, "seek": 1583664, "start": 15854.64, "end": 15856.64, "text": " so this will help us score better", "tokens": [51264, 370, 341, 486, 854, 505, 6175, 1101, 51364], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5028, "seek": 1583664, "start": 15856.64, "end": 15858.64, "text": " in the long run if we just", "tokens": [51364, 294, 264, 938, 1190, 498, 321, 445, 51464], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5029, "seek": 1583664, "start": 15858.64, "end": 15860.64, "text": " highlight what tokens", "tokens": [51464, 5078, 437, 22667, 51564], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5030, "seek": 1583664, "start": 15860.64, "end": 15862.64, "text": " and what attention scores are more important", "tokens": [51564, 293, 437, 3202, 13444, 366, 544, 1021, 51664], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5031, "seek": 1583664, "start": 15862.64, "end": 15864.64, "text": " in the sequence", "tokens": [51664, 294, 264, 8310, 51764], "temperature": 0.0, "avg_logprob": -0.08147825513567243, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.008312681689858437}, {"id": 5032, "seek": 1586464, "start": 15864.64, "end": 15866.64, "text": " and then after this", "tokens": [50364, 293, 550, 934, 341, 50464], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5033, "seek": 1586464, "start": 15866.64, "end": 15868.64, "text": " softmax here", "tokens": [50464, 2787, 41167, 510, 50564], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5034, "seek": 1586464, "start": 15868.64, "end": 15870.64, "text": " we just apply a simple dropout", "tokens": [50564, 321, 445, 3079, 257, 2199, 3270, 346, 50664], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5035, "seek": 1586464, "start": 15870.64, "end": 15872.64, "text": " on this way variable", "tokens": [50664, 322, 341, 636, 7006, 50764], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5036, "seek": 1586464, "start": 15872.64, "end": 15874.64, "text": " this new", "tokens": [50764, 341, 777, 50864], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5037, "seek": 1586464, "start": 15874.64, "end": 15876.64, "text": " calculated way", "tokens": [50864, 15598, 636, 50964], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5038, "seek": 1586464, "start": 15876.64, "end": 15878.64, "text": " scale.product.attention", "tokens": [50964, 4373, 13, 33244, 13, 1591, 1251, 51064], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5039, "seek": 1586464, "start": 15878.64, "end": 15880.64, "text": " masked", "tokens": [51064, 45249, 51164], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5040, "seek": 1586464, "start": 15880.64, "end": 15882.64, "text": " and then softmaxed", "tokens": [51164, 293, 550, 2787, 41167, 292, 51264], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5041, "seek": 1586464, "start": 15882.64, "end": 15884.64, "text": " we apply a dropout on that", "tokens": [51264, 321, 3079, 257, 3270, 346, 322, 300, 51364], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5042, "seek": 1586464, "start": 15884.64, "end": 15886.64, "text": " and then we perform our final", "tokens": [51364, 293, 550, 321, 2042, 527, 2572, 51464], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5043, "seek": 1586464, "start": 15886.64, "end": 15888.64, "text": " weighted aggregation", "tokens": [51464, 32807, 16743, 399, 51564], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5044, "seek": 1586464, "start": 15888.64, "end": 15890.64, "text": " so this v", "tokens": [51564, 370, 341, 371, 51664], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5045, "seek": 1586464, "start": 15890.64, "end": 15892.64, "text": " multiplied by the output of the softmax", "tokens": [51664, 17207, 538, 264, 5598, 295, 264, 2787, 41167, 51764], "temperature": 0.0, "avg_logprob": -0.09374313564090939, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.008707381784915924}, {"id": 5046, "seek": 1589264, "start": 15892.64, "end": 15894.64, "text": " cool", "tokens": [50364, 1627, 50464], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5047, "seek": 1589264, "start": 15894.64, "end": 15896.64, "text": " so we get this v", "tokens": [50464, 370, 321, 483, 341, 371, 50564], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5048, "seek": 1589264, "start": 15896.64, "end": 15898.64, "text": " self.value of x so we just multiply that", "tokens": [50564, 2698, 13, 29155, 295, 2031, 370, 321, 445, 12972, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5049, "seek": 1589264, "start": 15898.64, "end": 15900.64, "text": " a little pointer", "tokens": [50664, 257, 707, 23918, 50764], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5050, "seek": 1589264, "start": 15900.64, "end": 15902.64, "text": " I wanted to add", "tokens": [50764, 286, 1415, 281, 909, 50864], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5051, "seek": 1589264, "start": 15902.64, "end": 15904.64, "text": " to this", "tokens": [50864, 281, 341, 50964], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5052, "seek": 1589264, "start": 15904.64, "end": 15906.64, "text": " module list", "tokens": [50964, 10088, 1329, 51064], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5053, "seek": 1589264, "start": 15906.64, "end": 15908.64, "text": " module list here", "tokens": [51064, 10088, 1329, 510, 51164], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5054, "seek": 1589264, "start": 15908.64, "end": 15910.64, "text": " and then our", "tokens": [51164, 293, 550, 527, 51264], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5055, "seek": 1589264, "start": 15910.64, "end": 15912.64, "text": " go", "tokens": [51264, 352, 51364], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5056, "seek": 1589264, "start": 15914.64, "end": 15916.64, "text": " yes our sequential network here", "tokens": [51464, 2086, 527, 42881, 3209, 510, 51564], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5057, "seek": 1589264, "start": 15916.64, "end": 15918.64, "text": " so we have this", "tokens": [51564, 370, 321, 362, 341, 51664], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5058, "seek": 1589264, "start": 15918.64, "end": 15920.64, "text": " sequential number of blocks here", "tokens": [51664, 42881, 1230, 295, 8474, 510, 51764], "temperature": 0.0, "avg_logprob": -0.08357090071627968, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.0026312065310776234}, {"id": 5059, "seek": 1592064, "start": 15920.64, "end": 15922.64, "text": " for n layers", "tokens": [50364, 337, 297, 7914, 50464], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5060, "seek": 1592064, "start": 15922.64, "end": 15924.64, "text": " and we have our module", "tokens": [50464, 293, 321, 362, 527, 10088, 50564], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5061, "seek": 1592064, "start": 15924.64, "end": 15926.64, "text": " list so what really is the difference here", "tokens": [50564, 1329, 370, 437, 534, 307, 264, 2649, 510, 50664], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5062, "seek": 1592064, "start": 15926.64, "end": 15928.64, "text": " well", "tokens": [50664, 731, 50764], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5063, "seek": 1592064, "start": 15928.64, "end": 15930.64, "text": " module list is not the same as n and not sequential", "tokens": [50764, 10088, 1329, 307, 406, 264, 912, 382, 297, 293, 406, 42881, 50864], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5064, "seek": 1592064, "start": 15930.64, "end": 15932.64, "text": " in terms of the", "tokens": [50864, 294, 2115, 295, 264, 50964], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5065, "seek": 1592064, "start": 15932.64, "end": 15934.64, "text": " asterisk usage that we see", "tokens": [50964, 257, 3120, 7797, 14924, 300, 321, 536, 51064], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5066, "seek": 1592064, "start": 15936.64, "end": 15938.64, "text": " in the language model class", "tokens": [51164, 294, 264, 2856, 2316, 1508, 51264], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5067, "seek": 1592064, "start": 15938.64, "end": 15940.64, "text": " module list doesn't run one layer", "tokens": [51264, 10088, 1329, 1177, 380, 1190, 472, 4583, 51364], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5068, "seek": 1592064, "start": 15940.64, "end": 15942.64, "text": " or head after another", "tokens": [51364, 420, 1378, 934, 1071, 51464], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5069, "seek": 1592064, "start": 15942.64, "end": 15944.64, "text": " but rather each is", "tokens": [51464, 457, 2831, 1184, 307, 51564], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5070, "seek": 1592064, "start": 15944.64, "end": 15946.64, "text": " isolated and gets its own unique perspective", "tokens": [51564, 14621, 293, 2170, 1080, 1065, 3845, 4585, 51664], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5071, "seek": 1592064, "start": 15946.64, "end": 15948.64, "text": " sequential processing", "tokens": [51664, 42881, 9007, 51764], "temperature": 0.0, "avg_logprob": -0.08716927965482076, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.004330119583755732}, {"id": 5072, "seek": 1594864, "start": 15948.64, "end": 15950.64, "text": " is where one block depends on another", "tokens": [50364, 307, 689, 472, 3461, 5946, 322, 1071, 50464], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5073, "seek": 1594864, "start": 15950.64, "end": 15952.64, "text": " to synchronously complete", "tokens": [50464, 281, 19331, 5098, 3566, 50564], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5074, "seek": 1594864, "start": 15952.64, "end": 15954.64, "text": " so that means we're waiting on one", "tokens": [50564, 370, 300, 1355, 321, 434, 3806, 322, 472, 50664], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5075, "seek": 1594864, "start": 15954.64, "end": 15956.64, "text": " to finish before we move on to the next", "tokens": [50664, 281, 2413, 949, 321, 1286, 322, 281, 264, 958, 50764], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5076, "seek": 1594864, "start": 15956.64, "end": 15958.64, "text": " so they're not completing asynchronously", "tokens": [50764, 370, 436, 434, 406, 19472, 42642, 5098, 50864], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5077, "seek": 1594864, "start": 15958.64, "end": 15960.64, "text": " or in parallel", "tokens": [50864, 420, 294, 8952, 50964], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5078, "seek": 1594864, "start": 15960.64, "end": 15962.64, "text": " so the multiple heads in a transformer model", "tokens": [50964, 370, 264, 3866, 8050, 294, 257, 31782, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5079, "seek": 1594864, "start": 15962.64, "end": 15964.64, "text": " operate independently", "tokens": [51064, 9651, 21761, 51164], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5080, "seek": 1594864, "start": 15964.64, "end": 15966.64, "text": " and their computations can be processed", "tokens": [51164, 293, 641, 2807, 763, 393, 312, 18846, 51264], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5081, "seek": 1594864, "start": 15966.64, "end": 15968.64, "text": " in parallel however this parallel", "tokens": [51264, 294, 8952, 4461, 341, 8952, 51364], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5082, "seek": 1594864, "start": 15968.64, "end": 15970.64, "text": " parallelism isn't due", "tokens": [51364, 8952, 1434, 1943, 380, 3462, 51464], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5083, "seek": 1594864, "start": 15970.64, "end": 15972.64, "text": " to the module list that stores", "tokens": [51464, 281, 264, 10088, 1329, 300, 9512, 51564], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5084, "seek": 1594864, "start": 15972.64, "end": 15974.64, "text": " the heads", "tokens": [51564, 264, 8050, 51664], "temperature": 0.0, "avg_logprob": -0.044229436628889326, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.002630847506225109}, {"id": 5085, "seek": 1597464, "start": 15974.64, "end": 15976.64, "text": " instead", "tokens": [50364, 2602, 50464], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5086, "seek": 1597464, "start": 15976.64, "end": 15978.64, "text": " it's because of how", "tokens": [50464, 309, 311, 570, 295, 577, 50564], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5087, "seek": 1597464, "start": 15978.64, "end": 15980.64, "text": " the computation are structured", "tokens": [50564, 264, 24903, 366, 18519, 50664], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5088, "seek": 1597464, "start": 15980.64, "end": 15982.64, "text": " to take advantage of the GPU's capabilities", "tokens": [50664, 281, 747, 5002, 295, 264, 18407, 311, 10862, 50764], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5089, "seek": 1597464, "start": 15982.64, "end": 15984.64, "text": " for simultaneous", "tokens": [50764, 337, 46218, 50864], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5090, "seek": 1597464, "start": 15984.64, "end": 15986.64, "text": " computation", "tokens": [50864, 24903, 50964], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5091, "seek": 1597464, "start": 15986.64, "end": 15988.64, "text": " and this is also how the deep learning", "tokens": [50964, 293, 341, 307, 611, 577, 264, 2452, 2539, 51064], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5092, "seek": 1597464, "start": 15988.64, "end": 15990.64, "text": " framework PyTorch", "tokens": [51064, 8388, 9953, 51, 284, 339, 51164], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5093, "seek": 1597464, "start": 15990.64, "end": 15992.64, "text": " interfaces with the GPU", "tokens": [51164, 28416, 365, 264, 18407, 51264], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5094, "seek": 1597464, "start": 15992.64, "end": 15994.64, "text": " so this isn't particularly something we have to worry", "tokens": [51264, 370, 341, 1943, 380, 4098, 746, 321, 362, 281, 3292, 51364], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5095, "seek": 1597464, "start": 15994.64, "end": 15996.64, "text": " about too much but", "tokens": [51364, 466, 886, 709, 457, 51464], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5096, "seek": 1597464, "start": 15996.64, "end": 15998.64, "text": " you could supposedly think", "tokens": [51464, 291, 727, 20581, 519, 51564], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5097, "seek": 1597464, "start": 15998.64, "end": 16000.64, "text": " that these are sort of running in parallel", "tokens": [51564, 300, 613, 366, 1333, 295, 2614, 294, 8952, 51664], "temperature": 0.0, "avg_logprob": -0.09900318021359651, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0076885404996573925}, {"id": 5098, "seek": 1600064, "start": 16000.64, "end": 16002.64, "text": " yeah", "tokens": [50364, 1338, 50464], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5099, "seek": 1600064, "start": 16002.64, "end": 16004.64, "text": " so if you want to get into hardware", "tokens": [50464, 370, 498, 291, 528, 281, 483, 666, 8837, 50564], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5100, "seek": 1600064, "start": 16004.64, "end": 16006.64, "text": " then that's like your whole realm there", "tokens": [50564, 550, 300, 311, 411, 428, 1379, 15355, 456, 50664], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5101, "seek": 1600064, "start": 16006.64, "end": 16008.64, "text": " but this is PyTorch, this is software", "tokens": [50664, 457, 341, 307, 9953, 51, 284, 339, 11, 341, 307, 4722, 50764], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5102, "seek": 1600064, "start": 16008.64, "end": 16010.64, "text": " not hardware at all", "tokens": [50764, 406, 8837, 412, 439, 50864], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5103, "seek": 1600064, "start": 16010.64, "end": 16012.64, "text": " I don't expect you have to have any hardware", "tokens": [50864, 286, 500, 380, 2066, 291, 362, 281, 362, 604, 8837, 50964], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5104, "seek": 1600064, "start": 16012.64, "end": 16014.64, "text": " knowledge about GPU, CPU", "tokens": [50964, 3601, 466, 18407, 11, 13199, 51064], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5105, "seek": 1600064, "start": 16014.64, "end": 16016.64, "text": " anything like that", "tokens": [51064, 1340, 411, 300, 51164], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5106, "seek": 1600064, "start": 16016.64, "end": 16018.64, "text": " anyways that's just kind of a background", "tokens": [51164, 13448, 300, 311, 445, 733, 295, 257, 3678, 51264], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5107, "seek": 1600064, "start": 16018.64, "end": 16020.64, "text": " of what's going on there", "tokens": [51264, 295, 437, 311, 516, 322, 456, 51364], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5108, "seek": 1600064, "start": 16020.64, "end": 16022.64, "text": " so cool", "tokens": [51364, 370, 1627, 51464], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5109, "seek": 1600064, "start": 16022.64, "end": 16024.64, "text": " so let's actually go over what is going on", "tokens": [51464, 370, 718, 311, 767, 352, 670, 437, 307, 516, 322, 51564], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5110, "seek": 1600064, "start": 16024.64, "end": 16026.64, "text": " from the ground up here", "tokens": [51564, 490, 264, 2727, 493, 510, 51664], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5111, "seek": 1600064, "start": 16026.64, "end": 16028.64, "text": " so we have this", "tokens": [51664, 370, 321, 362, 341, 51764], "temperature": 0.0, "avg_logprob": -0.10103498541790507, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.06088985130190849}, {"id": 5112, "seek": 1602864, "start": 16028.64, "end": 16030.64, "text": " GPT language model", "tokens": [50364, 26039, 51, 2856, 2316, 50464], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5113, "seek": 1602864, "start": 16030.64, "end": 16032.64, "text": " we got our token embeddings, positional embeddings", "tokens": [50464, 321, 658, 527, 14862, 12240, 29432, 11, 2535, 304, 12240, 29432, 50564], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5114, "seek": 1602864, "start": 16032.64, "end": 16034.64, "text": " we have these sequential blocks", "tokens": [50564, 321, 362, 613, 42881, 8474, 50664], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5115, "seek": 1602864, "start": 16034.64, "end": 16036.64, "text": " initialize our weights", "tokens": [50664, 5883, 1125, 527, 17443, 50764], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5116, "seek": 1602864, "start": 16036.64, "end": 16038.64, "text": " for each of these blocks", "tokens": [50764, 337, 1184, 295, 613, 8474, 50864], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5117, "seek": 1602864, "start": 16038.64, "end": 16040.64, "text": " we have a", "tokens": [50864, 321, 362, 257, 50964], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5118, "seek": 1602864, "start": 16040.64, "end": 16042.64, "text": " this class block", "tokens": [50964, 341, 1508, 3461, 51064], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5119, "seek": 1602864, "start": 16042.64, "end": 16044.64, "text": " so we get a head size parameter", "tokens": [51064, 370, 321, 483, 257, 1378, 2744, 13075, 51164], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5120, "seek": 1602864, "start": 16044.64, "end": 16046.64, "text": " which is n embedded of 384", "tokens": [51164, 597, 307, 297, 16741, 295, 12843, 19, 51264], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5121, "seek": 1602864, "start": 16046.64, "end": 16048.64, "text": " divided by n heads which is 4", "tokens": [51264, 6666, 538, 297, 8050, 597, 307, 1017, 51364], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5122, "seek": 1602864, "start": 16048.64, "end": 16050.64, "text": " so we get 96 from that", "tokens": [51364, 370, 321, 483, 24124, 490, 300, 51464], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5123, "seek": 1602864, "start": 16050.64, "end": 16052.64, "text": " that's the number of features we're capturing", "tokens": [51464, 300, 311, 264, 1230, 295, 4122, 321, 434, 23384, 51564], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5124, "seek": 1602864, "start": 16052.64, "end": 16054.64, "text": " self-attention", "tokens": [51564, 2698, 12, 1591, 1251, 51664], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5125, "seek": 1602864, "start": 16054.64, "end": 16056.64, "text": " we do a feed forward to layer norms", "tokens": [51664, 321, 360, 257, 3154, 2128, 281, 4583, 24357, 51764], "temperature": 0.0, "avg_logprob": -0.08685869031247839, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.0072306618094444275}, {"id": 5126, "seek": 1605664, "start": 16056.64, "end": 16058.64, "text": " self-attention, layer norm", "tokens": [50364, 2698, 12, 1591, 1251, 11, 4583, 2026, 50464], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5127, "seek": 1605664, "start": 16058.64, "end": 16060.64, "text": " feed forward", "tokens": [50464, 3154, 2128, 50564], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5128, "seek": 1605664, "start": 16060.64, "end": 16062.64, "text": " layer norm", "tokens": [50564, 4583, 2026, 50664], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5129, "seek": 1605664, "start": 16062.64, "end": 16064.64, "text": " in the post norm architecture", "tokens": [50664, 294, 264, 2183, 2026, 9482, 50764], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5130, "seek": 1605664, "start": 16064.64, "end": 16066.64, "text": " then we do a feed forward", "tokens": [50764, 550, 321, 360, 257, 3154, 2128, 50864], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5131, "seek": 1605664, "start": 16066.64, "end": 16068.64, "text": " just a linear", "tokens": [50864, 445, 257, 8213, 50964], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5132, "seek": 1605664, "start": 16068.64, "end": 16070.64, "text": " followed by a relu followed by a linear", "tokens": [50964, 6263, 538, 257, 1039, 84, 6263, 538, 257, 8213, 51064], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5133, "seek": 1605664, "start": 16070.64, "end": 16072.64, "text": " and then dropping that out", "tokens": [51064, 293, 550, 13601, 300, 484, 51164], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5134, "seek": 1605664, "start": 16072.64, "end": 16074.64, "text": " and then we have our multi-head attention", "tokens": [51164, 293, 550, 321, 362, 527, 4825, 12, 1934, 3202, 51264], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5135, "seek": 1605664, "start": 16074.64, "end": 16076.64, "text": " which just sort of structured", "tokens": [51264, 597, 445, 1333, 295, 18519, 51364], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5136, "seek": 1605664, "start": 16076.64, "end": 16078.64, "text": " these attention heads", "tokens": [51364, 613, 3202, 8050, 51464], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5137, "seek": 1605664, "start": 16078.64, "end": 16080.64, "text": " running in parallel and then concatenates the results", "tokens": [51464, 2614, 294, 8952, 293, 550, 1588, 7186, 1024, 264, 3542, 51564], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5138, "seek": 1605664, "start": 16080.64, "end": 16082.64, "text": " and then for each of these heads", "tokens": [51564, 293, 550, 337, 1184, 295, 613, 8050, 51664], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5139, "seek": 1605664, "start": 16082.64, "end": 16084.64, "text": " we have our keys, queries", "tokens": [51664, 321, 362, 527, 9317, 11, 24109, 51764], "temperature": 0.0, "avg_logprob": -0.1091239036770042, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.008843958377838135}, {"id": 5140, "seek": 1608464, "start": 16084.64, "end": 16086.64, "text": " and values", "tokens": [50364, 293, 4190, 50464], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5141, "seek": 1608464, "start": 16086.64, "end": 16088.64, "text": " we register a model state", "tokens": [50464, 321, 7280, 257, 2316, 1785, 50564], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5142, "seek": 1608464, "start": 16088.64, "end": 16090.64, "text": " to prevent overhead computation", "tokens": [50564, 281, 4871, 19922, 24903, 50664], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5143, "seek": 1608464, "start": 16090.64, "end": 16092.64, "text": " excessively", "tokens": [50664, 9310, 3413, 50764], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5144, "seek": 1608464, "start": 16092.64, "end": 16094.64, "text": " then we just", "tokens": [50764, 550, 321, 445, 50864], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5145, "seek": 1608464, "start": 16094.64, "end": 16096.64, "text": " do our scale dot product attention", "tokens": [50864, 360, 527, 4373, 5893, 1674, 3202, 50964], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5146, "seek": 1608464, "start": 16096.64, "end": 16098.64, "text": " in this line, we do our mast field", "tokens": [50964, 294, 341, 1622, 11, 321, 360, 527, 27055, 2519, 51064], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5147, "seek": 1608464, "start": 16098.64, "end": 16100.64, "text": " to prevent look ahead", "tokens": [51064, 281, 4871, 574, 2286, 51164], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5148, "seek": 1608464, "start": 16100.64, "end": 16102.64, "text": " we do our softmax to make our values", "tokens": [51164, 321, 360, 527, 2787, 41167, 281, 652, 527, 4190, 51264], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5149, "seek": 1608464, "start": 16102.64, "end": 16104.64, "text": " sharper and to make some of them stand out", "tokens": [51264, 44670, 293, 281, 652, 512, 295, 552, 1463, 484, 51364], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5150, "seek": 1608464, "start": 16104.64, "end": 16106.64, "text": " and then", "tokens": [51364, 293, 550, 51464], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5151, "seek": 1608464, "start": 16106.64, "end": 16108.64, "text": " we do a drop out finally", "tokens": [51464, 321, 360, 257, 3270, 484, 2721, 51564], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5152, "seek": 1608464, "start": 16108.64, "end": 16110.64, "text": " on that and just some weighted aggregation", "tokens": [51564, 322, 300, 293, 445, 512, 32807, 16743, 399, 51664], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5153, "seek": 1608464, "start": 16110.64, "end": 16112.64, "text": " we do our weights", "tokens": [51664, 321, 360, 527, 17443, 51764], "temperature": 0.0, "avg_logprob": -0.10590030151663475, "compression_ratio": 1.8, "no_speech_prob": 0.02095063403248787}, {"id": 5154, "seek": 1611264, "start": 16112.64, "end": 16114.64, "text": " this final", "tokens": [50364, 341, 2572, 50464], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5155, "seek": 1611264, "start": 16114.64, "end": 16116.64, "text": " weight variable", "tokens": [50464, 3364, 7006, 50564], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5156, "seek": 1611264, "start": 16116.64, "end": 16118.64, "text": " multiplied by our", "tokens": [50564, 17207, 538, 527, 50664], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5157, "seek": 1611264, "start": 16118.64, "end": 16120.64, "text": " weighted value", "tokens": [50664, 32807, 2158, 50764], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5158, "seek": 1611264, "start": 16120.64, "end": 16122.64, "text": " from this", "tokens": [50764, 490, 341, 50864], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5159, "seek": 1611264, "start": 16122.64, "end": 16124.64, "text": " initially this linear transformation", "tokens": [50864, 9105, 341, 8213, 9887, 50964], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5160, "seek": 1611264, "start": 16124.64, "end": 16126.64, "text": " so cool, that's what's happening", "tokens": [50964, 370, 1627, 11, 300, 311, 437, 311, 2737, 51064], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5161, "seek": 1611264, "start": 16126.64, "end": 16128.64, "text": " step by step", "tokens": [51064, 1823, 538, 1823, 51164], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5162, "seek": 1611264, "start": 16128.64, "end": 16130.64, "text": " in this GBT architecture", "tokens": [51164, 294, 341, 26809, 51, 9482, 51264], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5163, "seek": 1611264, "start": 16130.64, "end": 16132.64, "text": " amazing", "tokens": [51264, 2243, 51364], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5164, "seek": 1611264, "start": 16132.64, "end": 16134.64, "text": " give yourself a good pat on the back", "tokens": [51364, 976, 1803, 257, 665, 1947, 322, 264, 646, 51464], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5165, "seek": 1611264, "start": 16134.64, "end": 16136.64, "text": " go grab some coffee, do whatever you need to do", "tokens": [51464, 352, 4444, 512, 4982, 11, 360, 2035, 291, 643, 281, 360, 51564], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5166, "seek": 1611264, "start": 16136.64, "end": 16138.64, "text": " even get some sleep", "tokens": [51564, 754, 483, 512, 2817, 51664], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5167, "seek": 1611264, "start": 16138.64, "end": 16140.64, "text": " and get ready for the next section", "tokens": [51664, 293, 483, 1919, 337, 264, 958, 3541, 51764], "temperature": 0.0, "avg_logprob": -0.09087404997452446, "compression_ratio": 1.5776699029126213, "no_speech_prob": 0.006002301815897226}, {"id": 5168, "seek": 1614064, "start": 16140.64, "end": 16142.64, "text": " so there's actually another hyper parameter", "tokens": [50364, 370, 456, 311, 767, 1071, 9848, 13075, 50464], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5169, "seek": 1614064, "start": 16142.64, "end": 16144.64, "text": " I forgot to add", "tokens": [50464, 286, 5298, 281, 909, 50564], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5170, "seek": 1614064, "start": 16144.64, "end": 16146.64, "text": " which is n layer", "tokens": [50564, 597, 307, 297, 4583, 50664], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5171, "seek": 1614064, "start": 16146.64, "end": 16148.64, "text": " and n layer", "tokens": [50664, 293, 297, 4583, 50764], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5172, "seek": 1614064, "start": 16148.64, "end": 16150.64, "text": " is essentially equal to 4", "tokens": [50764, 307, 4476, 2681, 281, 1017, 50864], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5173, "seek": 1614064, "start": 16150.64, "end": 16152.64, "text": " n layer is essentially equal to", "tokens": [50864, 297, 4583, 307, 4476, 2681, 281, 50964], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5174, "seek": 1614064, "start": 16152.64, "end": 16154.64, "text": " the number", "tokens": [50964, 264, 1230, 51064], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5175, "seek": 1614064, "start": 16154.64, "end": 16156.64, "text": " of decoder blocks", "tokens": [51064, 295, 979, 19866, 8474, 51164], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5176, "seek": 1614064, "start": 16156.64, "end": 16158.64, "text": " we have", "tokens": [51164, 321, 362, 51264], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5177, "seek": 1614064, "start": 16158.64, "end": 16160.64, "text": " so instead of n block we just say n layers", "tokens": [51264, 370, 2602, 295, 297, 3461, 321, 445, 584, 297, 7914, 51364], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5178, "seek": 1614064, "start": 16160.64, "end": 16162.64, "text": " doesn't really matter what it's called", "tokens": [51364, 1177, 380, 534, 1871, 437, 309, 311, 1219, 51464], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5179, "seek": 1614064, "start": 16162.64, "end": 16164.64, "text": " but that's what it means", "tokens": [51464, 457, 300, 311, 437, 309, 1355, 51564], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5180, "seek": 1614064, "start": 16164.64, "end": 16166.64, "text": " and then number of heads is how many heads", "tokens": [51564, 293, 550, 1230, 295, 8050, 307, 577, 867, 8050, 51664], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5181, "seek": 1614064, "start": 16166.64, "end": 16168.64, "text": " we have running theoretically in parallel", "tokens": [51664, 321, 362, 2614, 29400, 294, 8952, 51764], "temperature": 0.0, "avg_logprob": -0.16258167770673643, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.06459153443574905}, {"id": 5182, "seek": 1616864, "start": 16168.64, "end": 16170.64, "text": " and then n embed", "tokens": [50364, 293, 550, 297, 12240, 50464], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5183, "seek": 1616864, "start": 16170.64, "end": 16172.64, "text": " is the number of total", "tokens": [50464, 307, 264, 1230, 295, 3217, 50564], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5184, "seek": 1616864, "start": 16172.64, "end": 16174.64, "text": " dimensions we want to capture", "tokens": [50564, 12819, 321, 528, 281, 7983, 50664], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5185, "seek": 1616864, "start": 16174.64, "end": 16176.64, "text": " from all the heads concatenated together", "tokens": [50664, 490, 439, 264, 8050, 1588, 7186, 770, 1214, 50764], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5186, "seek": 1616864, "start": 16176.64, "end": 16178.64, "text": " type of thing, we already went over that, so cool", "tokens": [50764, 2010, 295, 551, 11, 321, 1217, 1437, 670, 300, 11, 370, 1627, 50864], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5187, "seek": 1616864, "start": 16178.64, "end": 16180.64, "text": " hyper parameters", "tokens": [50864, 9848, 9834, 50964], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5188, "seek": 1616864, "start": 16180.64, "end": 16182.64, "text": " block size, sequence length", "tokens": [50964, 3461, 2744, 11, 8310, 4641, 51064], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5189, "seek": 1616864, "start": 16182.64, "end": 16184.64, "text": " batch sizes, how many", "tokens": [51064, 15245, 11602, 11, 577, 867, 51164], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5190, "seek": 1616864, "start": 16184.64, "end": 16186.64, "text": " of these do we want at the same time", "tokens": [51164, 295, 613, 360, 321, 528, 412, 264, 912, 565, 51264], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5191, "seek": 1616864, "start": 16186.64, "end": 16188.64, "text": " max itters is just training", "tokens": [51264, 11469, 309, 1559, 307, 445, 3097, 51364], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5192, "seek": 1616864, "start": 16188.64, "end": 16190.64, "text": " how many iterations we want to do", "tokens": [51364, 577, 867, 36540, 321, 528, 281, 360, 51464], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5193, "seek": 1616864, "start": 16190.64, "end": 16192.64, "text": " learning rate is", "tokens": [51464, 2539, 3314, 307, 51564], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5194, "seek": 1616864, "start": 16192.64, "end": 16194.64, "text": " what we covered that in", "tokens": [51564, 437, 321, 5343, 300, 294, 51664], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5195, "seek": 1616864, "start": 16194.64, "end": 16196.64, "text": " actually the Desmos calculator that I showed", "tokens": [51664, 767, 264, 3885, 3415, 24993, 300, 286, 4712, 51764], "temperature": 0.0, "avg_logprob": -0.0751605703119646, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.007118556182831526}, {"id": 5196, "seek": 1619664, "start": 16196.64, "end": 16198.64, "text": " a little while back", "tokens": [50364, 257, 707, 1339, 646, 50464], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5197, "seek": 1619664, "start": 16198.64, "end": 16200.64, "text": " just showing how", "tokens": [50464, 445, 4099, 577, 50564], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5198, "seek": 1619664, "start": 16200.64, "end": 16202.64, "text": " we update the model weights based on the derivative", "tokens": [50564, 321, 5623, 264, 2316, 17443, 2361, 322, 264, 13760, 50664], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5199, "seek": 1619664, "start": 16202.64, "end": 16204.64, "text": " of the loss function", "tokens": [50664, 295, 264, 4470, 2445, 50764], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5200, "seek": 1619664, "start": 16204.64, "end": 16206.64, "text": " and then", "tokens": [50764, 293, 550, 50864], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5201, "seek": 1619664, "start": 16206.64, "end": 16208.64, "text": " validators", "tokens": [50864, 7363, 3391, 50964], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5202, "seek": 1619664, "start": 16208.64, "end": 16210.64, "text": " which was just reporting the loss", "tokens": [50964, 597, 390, 445, 10031, 264, 4470, 51064], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5203, "seek": 1619664, "start": 16210.64, "end": 16212.64, "text": " and then lastly the", "tokens": [51064, 293, 550, 16386, 264, 51164], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5204, "seek": 1619664, "start": 16212.64, "end": 16214.64, "text": " dropout which is dropping out", "tokens": [51164, 3270, 346, 597, 307, 13601, 484, 51264], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5205, "seek": 1619664, "start": 16214.64, "end": 16216.64, "text": " 0.2 or 20%", "tokens": [51264, 1958, 13, 17, 420, 945, 4, 51364], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5206, "seek": 1619664, "start": 16216.64, "end": 16218.64, "text": " of the total neurons", "tokens": [51364, 295, 264, 3217, 22027, 51464], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5207, "seek": 1619664, "start": 16218.64, "end": 16220.64, "text": " so awesome", "tokens": [51464, 370, 3476, 51564], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5208, "seek": 1619664, "start": 16220.64, "end": 16222.64, "text": " that's pretty cool, let's go ahead and jump into", "tokens": [51564, 300, 311, 1238, 1627, 11, 718, 311, 352, 2286, 293, 3012, 666, 51664], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5209, "seek": 1619664, "start": 16222.64, "end": 16224.64, "text": " some data stuff", "tokens": [51664, 512, 1412, 1507, 51764], "temperature": 0.0, "avg_logprob": -0.1122300157841948, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.05996895208954811}, {"id": 5210, "seek": 1622464, "start": 16224.64, "end": 16226.64, "text": " I'm going to pull out a paper here", "tokens": [50364, 286, 478, 516, 281, 2235, 484, 257, 3035, 510, 50464], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5211, "seek": 1622464, "start": 16226.64, "end": 16228.64, "text": " so let's just make sure everything works here", "tokens": [50464, 370, 718, 311, 445, 652, 988, 1203, 1985, 510, 50564], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5212, "seek": 1622464, "start": 16228.64, "end": 16230.64, "text": " and then we're actually going to download our data", "tokens": [50564, 293, 550, 321, 434, 767, 516, 281, 5484, 527, 1412, 50664], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5213, "seek": 1622464, "start": 16230.64, "end": 16232.64, "text": " so I want to try to run some iterations", "tokens": [50664, 370, 286, 528, 281, 853, 281, 1190, 512, 36540, 50764], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5214, "seek": 1622464, "start": 16232.64, "end": 16234.64, "text": " and", "tokens": [50764, 293, 50864], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5215, "seek": 1622464, "start": 16234.64, "end": 16236.64, "text": " just make sure that our, actually I made some changes", "tokens": [50864, 445, 652, 988, 300, 527, 11, 767, 286, 1027, 512, 2962, 50964], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5216, "seek": 1622464, "start": 16238.64, "end": 16240.64, "text": " pretty much this was", "tokens": [51064, 1238, 709, 341, 390, 51164], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5217, "seek": 1622464, "start": 16240.64, "end": 16242.64, "text": " weird and didn't work so I just changed", "tokens": [51164, 3657, 293, 994, 380, 589, 370, 286, 445, 3105, 51264], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5218, "seek": 1622464, "start": 16242.64, "end": 16244.64, "text": " this around to", "tokens": [51264, 341, 926, 281, 51364], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5219, "seek": 1622464, "start": 16244.64, "end": 16246.64, "text": " making our characters empty", "tokens": [51364, 1455, 527, 4342, 6707, 51464], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5220, "seek": 1622464, "start": 16246.64, "end": 16248.64, "text": " opening this text file", "tokens": [51464, 5193, 341, 2487, 3991, 51564], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5221, "seek": 1622464, "start": 16248.64, "end": 16250.64, "text": " opening it", "tokens": [51564, 5193, 309, 51664], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5222, "seek": 1622464, "start": 16250.64, "end": 16252.64, "text": " storing it in a variable", "tokens": [51664, 26085, 309, 294, 257, 7006, 51764], "temperature": 0.0, "avg_logprob": -0.11645827470002351, "compression_ratio": 1.75, "no_speech_prob": 0.02883235178887844}, {"id": 5223, "seek": 1625264, "start": 16252.64, "end": 16254.64, "text": " format", "tokens": [50364, 7877, 50464], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5224, "seek": 1625264, "start": 16254.64, "end": 16256.64, "text": " and then just making our vocab", "tokens": [50464, 293, 550, 445, 1455, 527, 2329, 455, 50564], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5225, "seek": 1625264, "start": 16256.64, "end": 16258.64, "text": " this sorted list", "tokens": [50564, 341, 25462, 1329, 50664], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5226, "seek": 1625264, "start": 16258.64, "end": 16260.64, "text": " set of our text", "tokens": [50664, 992, 295, 527, 2487, 50764], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5227, "seek": 1625264, "start": 16260.64, "end": 16262.64, "text": " and then just making the vocab", "tokens": [50764, 293, 550, 445, 1455, 264, 2329, 455, 50864], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5228, "seek": 1625264, "start": 16262.64, "end": 16264.64, "text": " size the length of that", "tokens": [50864, 2744, 264, 4641, 295, 300, 50964], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5229, "seek": 1625264, "start": 16264.64, "end": 16266.64, "text": " so let's go ahead and actually run", "tokens": [50964, 370, 718, 311, 352, 2286, 293, 767, 1190, 51064], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5230, "seek": 1625264, "start": 16266.64, "end": 16268.64, "text": " this through, I did change the block size", "tokens": [51064, 341, 807, 11, 286, 630, 1319, 264, 3461, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5231, "seek": 1625264, "start": 16268.64, "end": 16270.64, "text": " to 64 batch size 128", "tokens": [51164, 281, 12145, 15245, 2744, 29810, 51264], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5232, "seek": 1625264, "start": 16270.64, "end": 16272.64, "text": " some other hype parameters here", "tokens": [51264, 512, 661, 24144, 9834, 510, 51364], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5233, "seek": 1625264, "start": 16272.64, "end": 16274.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5234, "seek": 1625264, "start": 16274.64, "end": 16276.64, "text": " honestly the block size and batch size will depend", "tokens": [51464, 6095, 264, 3461, 2744, 293, 15245, 2744, 486, 5672, 51564], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5235, "seek": 1625264, "start": 16276.64, "end": 16278.64, "text": " on your", "tokens": [51564, 322, 428, 51664], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5236, "seek": 1625264, "start": 16278.64, "end": 16280.64, "text": " computational resources", "tokens": [51664, 28270, 3593, 51764], "temperature": 0.0, "avg_logprob": -0.12278453671202368, "compression_ratio": 1.6798029556650247, "no_speech_prob": 0.01589602790772915}, {"id": 5237, "seek": 1628064, "start": 16280.64, "end": 16282.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5238, "seek": 1628064, "start": 16282.64, "end": 16284.64, "text": " just experiment with these", "tokens": [50464, 445, 5120, 365, 613, 50564], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5239, "seek": 1628064, "start": 16284.64, "end": 16286.64, "text": " I'm just going to try these out first", "tokens": [50564, 286, 478, 445, 516, 281, 853, 613, 484, 700, 50664], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5240, "seek": 1628064, "start": 16286.64, "end": 16288.64, "text": " just to show you guys what this looks like", "tokens": [50664, 445, 281, 855, 291, 1074, 437, 341, 1542, 411, 50764], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5241, "seek": 1628064, "start": 16292.64, "end": 16294.64, "text": " okay", "tokens": [50964, 1392, 51064], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5242, "seek": 1628064, "start": 16294.64, "end": 16296.64, "text": " so it looks like we're getting idx is not defined", "tokens": [51064, 370, 309, 1542, 411, 321, 434, 1242, 4496, 87, 307, 406, 7642, 51164], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5243, "seek": 1628064, "start": 16296.64, "end": 16298.64, "text": " where could that be", "tokens": [51164, 689, 727, 300, 312, 51264], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5244, "seek": 1628064, "start": 16298.64, "end": 16300.64, "text": " okay yep", "tokens": [51264, 1392, 18633, 51364], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5245, "seek": 1628064, "start": 16300.64, "end": 16302.64, "text": " so this is", "tokens": [51364, 370, 341, 307, 51464], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5246, "seek": 1628064, "start": 16302.64, "end": 16304.64, "text": " we could just change that", "tokens": [51464, 321, 727, 445, 1319, 300, 51564], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5247, "seek": 1628064, "start": 16304.64, "end": 16306.64, "text": " it's just saying idx is not defined", "tokens": [51564, 309, 311, 445, 1566, 4496, 87, 307, 406, 7642, 51664], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5248, "seek": 1628064, "start": 16306.64, "end": 16308.64, "text": " we're using index here idx there so that should work now", "tokens": [51664, 321, 434, 1228, 8186, 510, 4496, 87, 456, 370, 300, 820, 589, 586, 51764], "temperature": 0.0, "avg_logprob": -0.16801014143167084, "compression_ratio": 1.7900552486187846, "no_speech_prob": 0.0024718090426176786}, {"id": 5249, "seek": 1631064, "start": 16310.64, "end": 16312.64, "text": " and we're getting local variable", "tokens": [50364, 293, 321, 434, 1242, 2654, 7006, 50464], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5250, "seek": 1631064, "start": 16312.64, "end": 16314.64, "text": " t reference before assignment", "tokens": [50464, 256, 6408, 949, 15187, 50564], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5251, "seek": 1631064, "start": 16314.64, "end": 16316.64, "text": " okay so", "tokens": [50564, 1392, 370, 50664], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5252, "seek": 1631064, "start": 16316.64, "end": 16318.64, "text": " we have some", "tokens": [50664, 321, 362, 512, 50764], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5253, "seek": 1631064, "start": 16318.64, "end": 16320.64, "text": " we have t here and then we initialize", "tokens": [50764, 321, 362, 256, 510, 293, 550, 321, 5883, 1125, 50864], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5254, "seek": 1631064, "start": 16320.64, "end": 16322.64, "text": " t there so let's just bring up", "tokens": [50864, 256, 456, 370, 718, 311, 445, 1565, 493, 50964], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5255, "seek": 1631064, "start": 16324.64, "end": 16326.64, "text": " up to there cool", "tokens": [51064, 493, 281, 456, 1627, 51164], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5256, "seek": 1631064, "start": 16326.64, "end": 16328.64, "text": " now let's try and run this", "tokens": [51164, 586, 718, 311, 853, 293, 1190, 341, 51264], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5257, "seek": 1631064, "start": 16328.64, "end": 16330.64, "text": " oh shape is invalid", "tokens": [51264, 1954, 3909, 307, 34702, 51364], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5258, "seek": 1631064, "start": 16330.64, "end": 16332.64, "text": " for input size of", "tokens": [51364, 337, 4846, 2744, 295, 51464], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5259, "seek": 1631064, "start": 16332.64, "end": 16334.64, "text": " okay let's see what we got", "tokens": [51464, 1392, 718, 311, 536, 437, 321, 658, 51564], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5260, "seek": 1631064, "start": 16334.64, "end": 16336.64, "text": " it turns out we don't actually need", "tokens": [51564, 309, 4523, 484, 321, 500, 380, 767, 643, 51664], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5261, "seek": 1631064, "start": 16336.64, "end": 16338.64, "text": " two token embedding tables a little bit of a selling", "tokens": [51664, 732, 14862, 12240, 3584, 8020, 257, 707, 857, 295, 257, 6511, 51764], "temperature": 0.0, "avg_logprob": -0.11277074359712147, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.0039444491267204285}, {"id": 5262, "seek": 1633864, "start": 16338.64, "end": 16340.64, "text": " mistake but we don't need two of those", "tokens": [50364, 6146, 457, 321, 500, 380, 643, 732, 295, 729, 50464], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5263, "seek": 1633864, "start": 16340.64, "end": 16342.64, "text": " so I'll just delete that", "tokens": [50464, 370, 286, 603, 445, 12097, 300, 50564], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5264, "seek": 1633864, "start": 16342.64, "end": 16344.64, "text": " and then", "tokens": [50564, 293, 550, 50664], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5265, "seek": 1633864, "start": 16344.64, "end": 16346.64, "text": " what I'm going to do is go ahead and run this", "tokens": [50664, 437, 286, 478, 516, 281, 360, 307, 352, 2286, 293, 1190, 341, 50764], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5266, "seek": 1633864, "start": 16346.64, "end": 16348.64, "text": " again let's see a new error", "tokens": [50764, 797, 718, 311, 536, 257, 777, 6713, 50864], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5267, "seek": 1633864, "start": 16348.64, "end": 16350.64, "text": " local variable t reference before assignment", "tokens": [50864, 2654, 7006, 256, 6408, 949, 15187, 50964], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5268, "seek": 1633864, "start": 16350.64, "end": 16352.64, "text": " okay so our", "tokens": [50964, 1392, 370, 527, 51064], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5269, "seek": 1633864, "start": 16352.64, "end": 16354.64, "text": " t is referenced here", "tokens": [51064, 256, 307, 32734, 510, 51164], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5270, "seek": 1633864, "start": 16354.64, "end": 16356.64, "text": " and well how can we initialize this", "tokens": [51164, 293, 731, 577, 393, 321, 5883, 1125, 341, 51264], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5271, "seek": 1633864, "start": 16356.64, "end": 16358.64, "text": " what we can do is we could take this index", "tokens": [51264, 437, 321, 393, 360, 307, 321, 727, 747, 341, 8186, 51364], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5272, "seek": 1633864, "start": 16358.64, "end": 16360.64, "text": " here of shape", "tokens": [51364, 510, 295, 3909, 51464], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5273, "seek": 1633864, "start": 16360.64, "end": 16362.64, "text": " b by t because it goes b by t", "tokens": [51464, 272, 538, 256, 570, 309, 1709, 272, 538, 256, 51564], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5274, "seek": 1633864, "start": 16362.64, "end": 16364.64, "text": " plus 1 etc and just keeps growing", "tokens": [51564, 1804, 502, 5183, 293, 445, 5965, 4194, 51664], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5275, "seek": 1633864, "start": 16364.64, "end": 16366.64, "text": " so we could actually unpack that", "tokens": [51664, 370, 321, 727, 767, 26699, 300, 51764], "temperature": 0.0, "avg_logprob": -0.10645319985561684, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.008059781044721603}, {"id": 5276, "seek": 1636664, "start": 16366.64, "end": 16368.64, "text": " so we could go b", "tokens": [50364, 370, 321, 727, 352, 272, 50464], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5277, "seek": 1636664, "start": 16368.64, "end": 16370.64, "text": " b and", "tokens": [50464, 272, 293, 50564], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5278, "seek": 1636664, "start": 16370.64, "end": 16372.64, "text": " t is going to be index", "tokens": [50564, 256, 307, 516, 281, 312, 8186, 50664], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5279, "seek": 1636664, "start": 16372.64, "end": 16374.64, "text": " that shape just unpack that", "tokens": [50664, 300, 3909, 445, 26699, 300, 50764], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5280, "seek": 1636664, "start": 16374.64, "end": 16376.64, "text": " so cool", "tokens": [50764, 370, 1627, 50864], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5281, "seek": 1636664, "start": 16376.64, "end": 16378.64, "text": " so now we're going to run this training", "tokens": [50864, 370, 586, 321, 434, 516, 281, 1190, 341, 3097, 50964], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5282, "seek": 1636664, "start": 16378.64, "end": 16380.64, "text": " loop and", "tokens": [50964, 6367, 293, 51064], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5283, "seek": 1636664, "start": 16380.64, "end": 16382.64, "text": " it looks like it's working so far", "tokens": [51064, 309, 1542, 411, 309, 311, 1364, 370, 1400, 51164], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5284, "seek": 1636664, "start": 16382.64, "end": 16384.64, "text": " so that's amazing", "tokens": [51164, 370, 300, 311, 2243, 51264], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5285, "seek": 1636664, "start": 16384.64, "end": 16386.64, "text": " super cool", "tokens": [51264, 1687, 1627, 51364], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5286, "seek": 1636664, "start": 16386.64, "end": 16388.64, "text": " step 0 train last", "tokens": [51364, 1823, 1958, 3847, 1036, 51464], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5287, "seek": 1636664, "start": 16388.64, "end": 16390.64, "text": " 4.4 that's actually a pretty good training", "tokens": [51464, 1017, 13, 19, 300, 311, 767, 257, 1238, 665, 3097, 51564], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5288, "seek": 1636664, "start": 16390.64, "end": 16392.64, "text": " loss overall so", "tokens": [51564, 4470, 4787, 370, 51664], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5289, "seek": 1636664, "start": 16392.64, "end": 16394.64, "text": " we'll come back after this is done", "tokens": [51664, 321, 603, 808, 646, 934, 341, 307, 1096, 51764], "temperature": 0.0, "avg_logprob": -0.09308754920959472, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0023964333813637495}, {"id": 5290, "seek": 1639464, "start": 16394.64, "end": 16396.64, "text": " I've set it to train", "tokens": [50364, 286, 600, 992, 309, 281, 3847, 50464], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5291, "seek": 1639464, "start": 16396.64, "end": 16398.64, "text": " for", "tokens": [50464, 337, 50564], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5292, "seek": 1639464, "start": 16398.64, "end": 16400.64, "text": " 3,000 iterations printing every 500 iterations", "tokens": [50564, 805, 11, 1360, 36540, 14699, 633, 5923, 36540, 50664], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5293, "seek": 1639464, "start": 16400.64, "end": 16402.64, "text": " so we'll just see", "tokens": [50664, 370, 321, 603, 445, 536, 50764], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5294, "seek": 1639464, "start": 16402.64, "end": 16404.64, "text": " the loss six times over this entire", "tokens": [50764, 264, 4470, 2309, 1413, 670, 341, 2302, 50864], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5295, "seek": 1639464, "start": 16404.64, "end": 16406.64, "text": " training process", "tokens": [50864, 3097, 1399, 50964], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5296, "seek": 1639464, "start": 16406.64, "end": 16408.64, "text": " or we should", "tokens": [50964, 420, 321, 820, 51064], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5297, "seek": 1639464, "start": 16408.64, "end": 16410.64, "text": " I don't know why it's going to 100", "tokens": [51064, 286, 500, 380, 458, 983, 309, 311, 516, 281, 2319, 51164], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5298, "seek": 1639464, "start": 16410.64, "end": 16412.64, "text": " eval itters", "tokens": [51164, 1073, 304, 309, 1559, 51264], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5299, "seek": 1639464, "start": 16414.64, "end": 16416.64, "text": " eval itters", "tokens": [51364, 1073, 304, 309, 1559, 51464], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5300, "seek": 1639464, "start": 16416.64, "end": 16418.64, "text": " estimate loss is", "tokens": [51464, 12539, 4470, 307, 51564], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5301, "seek": 1639464, "start": 16420.64, "end": 16422.64, "text": " okay so we don't actually need", "tokens": [51664, 1392, 370, 321, 500, 380, 767, 643, 51764], "temperature": 0.0, "avg_logprob": -0.136417921199355, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.015419570729136467}, {"id": 5302, "seek": 1642264, "start": 16422.64, "end": 16424.64, "text": " eval interval", "tokens": [50364, 1073, 304, 15035, 50464], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5303, "seek": 1642264, "start": 16426.64, "end": 16428.64, "text": " we'll just make this", "tokens": [50564, 321, 603, 445, 652, 341, 50664], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5304, "seek": 1642264, "start": 16428.64, "end": 16430.64, "text": " sure why not 100", "tokens": [50664, 988, 983, 406, 2319, 50764], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5305, "seek": 1642264, "start": 16430.64, "end": 16432.64, "text": " we'll keep that", "tokens": [50764, 321, 603, 1066, 300, 50864], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5306, "seek": 1642264, "start": 16432.64, "end": 16434.64, "text": " and it's just going to keep going here", "tokens": [50864, 293, 309, 311, 445, 516, 281, 1066, 516, 510, 50964], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5307, "seek": 1642264, "start": 16434.64, "end": 16436.64, "text": " we'll see our loss over time", "tokens": [50964, 321, 603, 536, 527, 4470, 670, 565, 51064], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5308, "seek": 1642264, "start": 16436.64, "end": 16438.64, "text": " it's going to get smaller so", "tokens": [51064, 309, 311, 516, 281, 483, 4356, 370, 51164], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5309, "seek": 1642264, "start": 16438.64, "end": 16440.64, "text": " I'll come back when that's done", "tokens": [51164, 286, 603, 808, 646, 562, 300, 311, 1096, 51264], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5310, "seek": 1642264, "start": 16440.64, "end": 16442.64, "text": " as for the data we're going to be using", "tokens": [51264, 382, 337, 264, 1412, 321, 434, 516, 281, 312, 1228, 51364], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5311, "seek": 1642264, "start": 16442.64, "end": 16444.64, "text": " the open web text corpus", "tokens": [51364, 264, 1269, 3670, 2487, 1181, 31624, 51464], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5312, "seek": 1642264, "start": 16444.64, "end": 16446.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5313, "seek": 1642264, "start": 16446.64, "end": 16448.64, "text": " let's just go down here", "tokens": [51564, 718, 311, 445, 352, 760, 510, 51664], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5314, "seek": 1642264, "start": 16448.64, "end": 16450.64, "text": " so this is a paper called", "tokens": [51664, 370, 341, 307, 257, 3035, 1219, 51764], "temperature": 0.0, "avg_logprob": -0.12375885706681472, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002889149822294712}, {"id": 5315, "seek": 1645064, "start": 16450.64, "end": 16452.64, "text": " survey", "tokens": [50364, 8984, 50464], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5316, "seek": 1645064, "start": 16452.64, "end": 16454.64, "text": " survey of large language models", "tokens": [50464, 8984, 295, 2416, 2856, 5245, 50564], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5317, "seek": 1645064, "start": 16454.64, "end": 16456.64, "text": " so I'll just go back to open", "tokens": [50564, 370, 286, 603, 445, 352, 646, 281, 1269, 50664], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5318, "seek": 1645064, "start": 16456.64, "end": 16458.64, "text": " web", "tokens": [50664, 3670, 50764], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5319, "seek": 1645064, "start": 16458.64, "end": 16460.64, "text": " text", "tokens": [50764, 2487, 50864], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5320, "seek": 1645064, "start": 16460.64, "end": 16462.64, "text": " where that is", "tokens": [50864, 689, 300, 307, 50964], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5321, "seek": 1645064, "start": 16462.64, "end": 16464.64, "text": " up", "tokens": [50964, 493, 51064], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5322, "seek": 1645064, "start": 16464.64, "end": 16466.64, "text": " it's just fine", "tokens": [51064, 309, 311, 445, 2489, 51164], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5323, "seek": 1645064, "start": 16466.64, "end": 16468.64, "text": " so open web text", "tokens": [51164, 370, 1269, 3670, 2487, 51264], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5324, "seek": 1645064, "start": 16468.64, "end": 16470.64, "text": " this is consisted of a bunch of reddit links", "tokens": [51264, 341, 307, 38227, 295, 257, 3840, 295, 2182, 17975, 6123, 51364], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5325, "seek": 1645064, "start": 16470.64, "end": 16472.64, "text": " or just reddit upvotes", "tokens": [51364, 420, 445, 2182, 17975, 493, 85, 17251, 51464], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5326, "seek": 1645064, "start": 16472.64, "end": 16474.64, "text": " so if you go and reddit and you see", "tokens": [51464, 370, 498, 291, 352, 293, 2182, 17975, 293, 291, 536, 51564], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5327, "seek": 1645064, "start": 16474.64, "end": 16476.64, "text": " a bunch of those", "tokens": [51564, 257, 3840, 295, 729, 51664], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5328, "seek": 1645064, "start": 16476.64, "end": 16478.64, "text": " posts that are highly upvoted", "tokens": [51664, 12300, 300, 366, 5405, 493, 85, 23325, 51764], "temperature": 0.0, "avg_logprob": -0.12221466501553853, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0025110975839197636}, {"id": 5329, "seek": 1647864, "start": 16478.64, "end": 16480.64, "text": " or downvoted", "tokens": [50364, 420, 760, 85, 23325, 50464], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5330, "seek": 1647864, "start": 16480.64, "end": 16482.64, "text": " they're pretty much those", "tokens": [50464, 436, 434, 1238, 709, 729, 50564], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5331, "seek": 1647864, "start": 16482.64, "end": 16484.64, "text": " pieces of text are valuable", "tokens": [50564, 3755, 295, 2487, 366, 8263, 50664], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5332, "seek": 1647864, "start": 16484.64, "end": 16486.64, "text": " and they contain things that we can train them", "tokens": [50664, 293, 436, 5304, 721, 300, 321, 393, 3847, 552, 50764], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5333, "seek": 1647864, "start": 16486.64, "end": 16488.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5334, "seek": 1647864, "start": 16488.64, "end": 16490.64, "text": " pretty much web text is", "tokens": [50864, 1238, 709, 3670, 2487, 307, 50964], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5335, "seek": 1647864, "start": 16490.64, "end": 16492.64, "text": " just a corpus of all these upvoted links", "tokens": [50964, 445, 257, 1181, 31624, 295, 439, 613, 493, 85, 23325, 6123, 51064], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5336, "seek": 1647864, "start": 16492.64, "end": 16494.64, "text": " but it's not publicly available", "tokens": [51064, 457, 309, 311, 406, 14843, 2435, 51164], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5337, "seek": 1647864, "start": 16494.64, "end": 16496.64, "text": " so somebody created an open source version", "tokens": [51164, 370, 2618, 2942, 364, 1269, 4009, 3037, 51264], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5338, "seek": 1647864, "start": 16496.64, "end": 16498.64, "text": " called", "tokens": [51264, 1219, 51364], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5339, "seek": 1647864, "start": 16498.64, "end": 16500.64, "text": " open web text", "tokens": [51364, 1269, 3670, 2487, 51464], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5340, "seek": 1647864, "start": 16500.64, "end": 16502.64, "text": " hence open", "tokens": [51464, 16678, 1269, 51564], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5341, "seek": 1647864, "start": 16502.64, "end": 16504.64, "text": " and it's pretty much as an open version of this", "tokens": [51564, 293, 309, 311, 1238, 709, 382, 364, 1269, 3037, 295, 341, 51664], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5342, "seek": 1647864, "start": 16504.64, "end": 16506.64, "text": " so we're going to download that", "tokens": [51664, 370, 321, 434, 516, 281, 5484, 300, 51764], "temperature": 0.0, "avg_logprob": -0.09722063474566023, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.007118816487491131}, {"id": 5343, "seek": 1650664, "start": 16506.64, "end": 16508.64, "text": " for a here like common crawl which is", "tokens": [50364, 337, 257, 510, 411, 2689, 24767, 597, 307, 50464], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5344, "seek": 1650664, "start": 16508.64, "end": 16510.64, "text": " really really big so like", "tokens": [50464, 534, 534, 955, 370, 411, 50564], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5345, "seek": 1650664, "start": 16510.64, "end": 16512.64, "text": " petabyte scale data volume", "tokens": [50564, 3817, 34529, 4373, 1412, 5523, 50664], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5346, "seek": 1650664, "start": 16512.64, "end": 16514.64, "text": " you have a bunch of books", "tokens": [50664, 291, 362, 257, 3840, 295, 3642, 50764], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5347, "seek": 1650664, "start": 16514.64, "end": 16516.64, "text": " so this is a good paper to read over", "tokens": [50764, 370, 341, 307, 257, 665, 3035, 281, 1401, 670, 50864], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5348, "seek": 1650664, "start": 16516.64, "end": 16518.64, "text": " it's just called", "tokens": [50864, 309, 311, 445, 1219, 50964], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5349, "seek": 1650664, "start": 16518.64, "end": 16520.64, "text": " a survey", "tokens": [50964, 257, 8984, 51064], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5350, "seek": 1650664, "start": 16520.64, "end": 16522.64, "text": " of large language models you can search this up", "tokens": [51064, 295, 2416, 2856, 5245, 291, 393, 3164, 341, 493, 51164], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5351, "seek": 1650664, "start": 16522.64, "end": 16524.64, "text": " and it'll come up you can just download the pdf for", "tokens": [51164, 293, 309, 603, 808, 493, 291, 393, 445, 5484, 264, 280, 45953, 337, 51264], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5352, "seek": 1650664, "start": 16524.64, "end": 16526.64, "text": " so this is a really nice paper", "tokens": [51264, 370, 341, 307, 257, 534, 1481, 3035, 51364], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5353, "seek": 1650664, "start": 16526.64, "end": 16528.64, "text": " read over that if you'd like", "tokens": [51364, 1401, 670, 300, 498, 291, 1116, 411, 51464], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5354, "seek": 1650664, "start": 16528.64, "end": 16530.64, "text": " but anyways", "tokens": [51464, 457, 13448, 51564], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5355, "seek": 1650664, "start": 16530.64, "end": 16532.64, "text": " this is a download link for this open web text corpus", "tokens": [51564, 341, 307, 257, 5484, 2113, 337, 341, 1269, 3670, 2487, 1181, 31624, 51664], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5356, "seek": 1650664, "start": 16532.64, "end": 16534.64, "text": " so just go to this link", "tokens": [51664, 370, 445, 352, 281, 341, 2113, 51764], "temperature": 0.0, "avg_logprob": -0.10688925170898438, "compression_ratio": 1.7875, "no_speech_prob": 0.07151453197002411}, {"id": 5357, "seek": 1653464, "start": 16534.64, "end": 16536.64, "text": " I have it in the github repo", "tokens": [50364, 286, 362, 309, 294, 264, 290, 355, 836, 49040, 50464], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5358, "seek": 1653464, "start": 16536.64, "end": 16538.64, "text": " and you just go to download", "tokens": [50464, 293, 291, 445, 352, 281, 5484, 50564], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5359, "seek": 1653464, "start": 16538.64, "end": 16540.64, "text": " and it'll bring you to this drive", "tokens": [50564, 293, 309, 603, 1565, 291, 281, 341, 3332, 50664], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5360, "seek": 1653464, "start": 16540.64, "end": 16542.64, "text": " so you can go in and right click this", "tokens": [50664, 370, 291, 393, 352, 294, 293, 558, 2052, 341, 50764], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5361, "seek": 1653464, "start": 16542.64, "end": 16544.64, "text": " and just hit download", "tokens": [50764, 293, 445, 2045, 5484, 50864], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5362, "seek": 1653464, "start": 16544.64, "end": 16546.64, "text": " it'll say 12 gigabytes exceeds", "tokens": [50864, 309, 603, 584, 2272, 42741, 43305, 50964], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5363, "seek": 1653464, "start": 16546.64, "end": 16548.64, "text": " maximum file size that it can scan so it's like", "tokens": [50964, 6674, 3991, 2744, 300, 309, 393, 11049, 370, 309, 311, 411, 51064], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5364, "seek": 1653464, "start": 16548.64, "end": 16550.64, "text": " this might have a virus", "tokens": [51064, 341, 1062, 362, 257, 5752, 51164], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5365, "seek": 1653464, "start": 16550.64, "end": 16552.64, "text": " don't worry it doesn't have a virus this is actually", "tokens": [51164, 500, 380, 3292, 309, 1177, 380, 362, 257, 5752, 341, 307, 767, 51264], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5366, "seek": 1653464, "start": 16552.64, "end": 16554.64, "text": " created by a researcher so", "tokens": [51264, 2942, 538, 257, 21751, 370, 51364], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5367, "seek": 1653464, "start": 16554.64, "end": 16556.64, "text": " not really bad people are in charge of", "tokens": [51364, 406, 534, 1578, 561, 366, 294, 4602, 295, 51464], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5368, "seek": 1653464, "start": 16556.64, "end": 16558.64, "text": " creating text corpora", "tokens": [51464, 4084, 2487, 1181, 79, 3252, 51564], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5369, "seek": 1653464, "start": 16558.64, "end": 16560.64, "text": " so go in and download anyway", "tokens": [51564, 370, 352, 294, 293, 5484, 4033, 51664], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5370, "seek": 1653464, "start": 16560.64, "end": 16562.64, "text": " I've actually already downloaded this", "tokens": [51664, 286, 600, 767, 1217, 21748, 341, 51764], "temperature": 0.0, "avg_logprob": -0.09233818054199219, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.04081326350569725}, {"id": 5371, "seek": 1656264, "start": 16562.64, "end": 16564.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5372, "seek": 1656264, "start": 16564.64, "end": 16566.64, "text": " yeah I'll come back", "tokens": [50464, 1338, 286, 603, 808, 646, 50564], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5373, "seek": 1656264, "start": 16566.64, "end": 16568.64, "text": " when our training", "tokens": [50564, 562, 527, 3097, 50664], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5374, "seek": 1656264, "start": 16568.64, "end": 16570.64, "text": " is actually done here", "tokens": [50664, 307, 767, 1096, 510, 50764], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5375, "seek": 1656264, "start": 16570.64, "end": 16572.64, "text": " so I'm actually going to stop here iteration", "tokens": [50764, 370, 286, 478, 767, 516, 281, 1590, 510, 24784, 50864], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5376, "seek": 1656264, "start": 16572.64, "end": 16574.64, "text": " 2000 because", "tokens": [50864, 8132, 570, 50964], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5377, "seek": 1656264, "start": 16574.64, "end": 16576.64, "text": " we're not actually getting that much amazing progress", "tokens": [50964, 321, 434, 406, 767, 1242, 300, 709, 2243, 4205, 51064], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5378, "seek": 1656264, "start": 16576.64, "end": 16578.64, "text": " and the reason for this is because", "tokens": [51064, 293, 264, 1778, 337, 341, 307, 570, 51164], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5379, "seek": 1656264, "start": 16578.64, "end": 16580.64, "text": " our hyper parameters", "tokens": [51164, 527, 9848, 9834, 51264], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5380, "seek": 1656264, "start": 16580.64, "end": 16582.64, "text": " so batch size and block size", "tokens": [51264, 370, 15245, 2744, 293, 3461, 2744, 51364], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5381, "seek": 1656264, "start": 16582.64, "end": 16584.64, "text": " I mean these are okay", "tokens": [51364, 286, 914, 613, 366, 1392, 51464], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5382, "seek": 1656264, "start": 16584.64, "end": 16586.64, "text": " but we might want to change up as our learning rate", "tokens": [51464, 457, 321, 1062, 528, 281, 1319, 493, 382, 527, 2539, 3314, 51564], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5383, "seek": 1656264, "start": 16586.64, "end": 16588.64, "text": " so some combinations of learning rates that are really useful", "tokens": [51564, 370, 512, 21267, 295, 2539, 6846, 300, 366, 534, 4420, 51664], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5384, "seek": 1656264, "start": 16588.64, "end": 16590.64, "text": " is like", "tokens": [51664, 307, 411, 51764], "temperature": 0.0, "avg_logprob": -0.0886526107788086, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.006794861517846584}, {"id": 5385, "seek": 1659064, "start": 16590.64, "end": 16592.64, "text": " 3e to the negative 3", "tokens": [50364, 805, 68, 281, 264, 3671, 805, 50464], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5386, "seek": 1659064, "start": 16592.64, "end": 16594.64, "text": " you go 3e to the", "tokens": [50464, 291, 352, 805, 68, 281, 264, 50564], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5387, "seek": 1659064, "start": 16594.64, "end": 16596.64, "text": " negative 4", "tokens": [50564, 3671, 1017, 50664], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5388, "seek": 1659064, "start": 16596.64, "end": 16598.64, "text": " you go 1e to the negative 3", "tokens": [50664, 291, 352, 502, 68, 281, 264, 3671, 805, 50764], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5389, "seek": 1659064, "start": 16598.64, "end": 16600.64, "text": " 1e", "tokens": [50764, 502, 68, 50864], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5390, "seek": 1659064, "start": 16600.64, "end": 16602.64, "text": " 1e to the negative 4", "tokens": [50864, 502, 68, 281, 264, 3671, 1017, 50964], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5391, "seek": 1659064, "start": 16602.64, "end": 16604.64, "text": " so these are all learning rates that I like to play around with", "tokens": [50964, 370, 613, 366, 439, 2539, 6846, 300, 286, 411, 281, 862, 926, 365, 51064], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5392, "seek": 1659064, "start": 16604.64, "end": 16606.64, "text": " these are just sort of common ones", "tokens": [51064, 613, 366, 445, 1333, 295, 2689, 2306, 51164], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5393, "seek": 1659064, "start": 16606.64, "end": 16608.64, "text": " it's up to you if you want to use them or not but", "tokens": [51164, 309, 311, 493, 281, 291, 498, 291, 528, 281, 764, 552, 420, 406, 457, 51264], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5394, "seek": 1659064, "start": 16608.64, "end": 16610.64, "text": " what I might do actually", "tokens": [51264, 437, 286, 1062, 360, 767, 51364], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5395, "seek": 1659064, "start": 16610.64, "end": 16612.64, "text": " is just downgrade to 3e to the negative 4", "tokens": [51364, 307, 445, 760, 8692, 281, 805, 68, 281, 264, 3671, 1017, 51464], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5396, "seek": 1659064, "start": 16612.64, "end": 16614.64, "text": " and we'll retest it", "tokens": [51464, 293, 321, 603, 1533, 377, 309, 51564], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5397, "seek": 1659064, "start": 16614.64, "end": 16616.64, "text": " as well I'm going to bump up the", "tokens": [51564, 382, 731, 286, 478, 516, 281, 9961, 493, 264, 51664], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5398, "seek": 1659064, "start": 16616.64, "end": 16618.64, "text": " the number of heads", "tokens": [51664, 264, 1230, 295, 8050, 51764], "temperature": 0.0, "avg_logprob": -0.09923381953276406, "compression_ratio": 1.8792270531400965, "no_speech_prob": 0.01321573555469513}, {"id": 5399, "seek": 1661864, "start": 16618.64, "end": 16620.64, "text": " and the number of layers", "tokens": [50364, 293, 264, 1230, 295, 7914, 50464], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5400, "seek": 1661864, "start": 16620.64, "end": 16622.64, "text": " so that we can capture more", "tokens": [50464, 370, 300, 321, 393, 7983, 544, 50564], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5401, "seek": 1661864, "start": 16622.64, "end": 16624.64, "text": " complex relationships in the text", "tokens": [50564, 3997, 6159, 294, 264, 2487, 50664], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5402, "seek": 1661864, "start": 16624.64, "end": 16626.64, "text": " thus having it learn more", "tokens": [50664, 8807, 1419, 309, 1466, 544, 50764], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5403, "seek": 1661864, "start": 16626.64, "end": 16628.64, "text": " so I'm going to change each of these", "tokens": [50764, 370, 286, 478, 516, 281, 1319, 1184, 295, 613, 50864], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5404, "seek": 1661864, "start": 16628.64, "end": 16630.64, "text": " to 8", "tokens": [50864, 281, 1649, 50964], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5405, "seek": 1661864, "start": 16630.64, "end": 16632.64, "text": " go 8", "tokens": [50964, 352, 1649, 51064], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5406, "seek": 1661864, "start": 16632.64, "end": 16634.64, "text": " actually", "tokens": [51064, 767, 51164], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5407, "seek": 1661864, "start": 16634.64, "end": 16636.64, "text": " kernel will go", "tokens": [51164, 28256, 486, 352, 51264], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5408, "seek": 1661864, "start": 16636.64, "end": 16638.64, "text": " restart", "tokens": [51264, 21022, 51364], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5409, "seek": 1661864, "start": 16640.64, "end": 16642.64, "text": " now we'll just run this from the top", "tokens": [51464, 586, 321, 603, 445, 1190, 341, 490, 264, 1192, 51564], "temperature": 0.0, "avg_logprob": -0.1294206699854891, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00364754069596529}, {"id": 5410, "seek": 1664864, "start": 16648.64, "end": 16650.64, "text": " and", "tokens": [50364, 293, 50464], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5411, "seek": 1664864, "start": 16652.64, "end": 16654.64, "text": " and we'll run that", "tokens": [50564, 293, 321, 603, 1190, 300, 50664], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5412, "seek": 1664864, "start": 16654.64, "end": 16656.64, "text": " cool", "tokens": [50664, 1627, 50764], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5413, "seek": 1664864, "start": 16656.64, "end": 16658.64, "text": " so let's see", "tokens": [50764, 370, 718, 311, 536, 50864], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5414, "seek": 1664864, "start": 16658.64, "end": 16660.64, "text": " what we actually start off with and what our loss looks like over time", "tokens": [50864, 437, 321, 767, 722, 766, 365, 293, 437, 527, 4470, 1542, 411, 670, 565, 50964], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5415, "seek": 1664864, "start": 16670.64, "end": 16672.64, "text": " cool", "tokens": [51464, 1627, 51564], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5416, "seek": 1664864, "start": 16672.64, "end": 16674.64, "text": " so we got step 1 4.5 about the same as last time", "tokens": [51564, 370, 321, 658, 1823, 502, 1017, 13, 20, 466, 264, 912, 382, 1036, 565, 51664], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5417, "seek": 1664864, "start": 16674.64, "end": 16676.64, "text": " it's like 0.2 off", "tokens": [51664, 309, 311, 411, 1958, 13, 17, 766, 51764], "temperature": 0.0, "avg_logprob": -0.17057947012094352, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.017970824614167213}, {"id": 5418, "seek": 1667664, "start": 16676.64, "end": 16678.64, "text": " or something so it's pretty close", "tokens": [50364, 420, 746, 370, 309, 311, 1238, 1998, 50464], "temperature": 0.0, "avg_logprob": -0.12950503198724045, "compression_ratio": 1.0307692307692307, "no_speech_prob": 0.0465395413339138}, {"id": 5419, "seek": 1667664, "start": 16678.64, "end": 16680.64, "text": " let's see the next iteration here", "tokens": [50464, 718, 311, 536, 264, 958, 24784, 510, 50564], "temperature": 0.0, "avg_logprob": -0.12950503198724045, "compression_ratio": 1.0307692307692307, "no_speech_prob": 0.0465395413339138}, {"id": 5420, "seek": 1670664, "start": 16706.64, "end": 16708.64, "text": " that's wonderful", "tokens": [50364, 300, 311, 3715, 50464], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5421, "seek": 1670664, "start": 16708.64, "end": 16710.64, "text": " so before we were getting like 3.1 ish", "tokens": [50464, 370, 949, 321, 645, 1242, 411, 805, 13, 16, 307, 71, 50564], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5422, "seek": 1670664, "start": 16710.64, "end": 16712.64, "text": " or something around that range 3.15", "tokens": [50564, 420, 746, 926, 300, 3613, 805, 13, 5211, 50664], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5423, "seek": 1670664, "start": 16712.64, "end": 16714.64, "text": " now we're getting 2.2", "tokens": [50664, 586, 321, 434, 1242, 568, 13, 17, 50764], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5424, "seek": 1670664, "start": 16714.64, "end": 16716.64, "text": " so you can see that", "tokens": [50764, 370, 291, 393, 536, 300, 50864], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5425, "seek": 1670664, "start": 16716.64, "end": 16718.64, "text": " as we change hyper parameters", "tokens": [50864, 382, 321, 1319, 9848, 9834, 50964], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5426, "seek": 1670664, "start": 16718.64, "end": 16720.64, "text": " we can actually see a significant change", "tokens": [50964, 321, 393, 767, 536, 257, 4776, 1319, 51064], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5427, "seek": 1670664, "start": 16720.64, "end": 16722.64, "text": " in our loss", "tokens": [51064, 294, 527, 4470, 51164], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5428, "seek": 1670664, "start": 16722.64, "end": 16724.64, "text": " this is amazing", "tokens": [51164, 341, 307, 2243, 51264], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5429, "seek": 1670664, "start": 16724.64, "end": 16726.64, "text": " this is just to sort of prove how cool hyper", "tokens": [51264, 341, 307, 445, 281, 1333, 295, 7081, 577, 1627, 9848, 51364], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5430, "seek": 1670664, "start": 16726.64, "end": 16728.64, "text": " parameters are and what they do for you", "tokens": [51364, 9834, 366, 293, 437, 436, 360, 337, 291, 51464], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5431, "seek": 1670664, "start": 16728.64, "end": 16730.64, "text": " so", "tokens": [51464, 370, 51564], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5432, "seek": 1670664, "start": 16730.64, "end": 16732.64, "text": " let's start", "tokens": [51564, 718, 311, 722, 51664], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5433, "seek": 1670664, "start": 16732.64, "end": 16734.64, "text": " changing around some data stuff", "tokens": [51664, 4473, 926, 512, 1412, 1507, 51764], "temperature": 0.0, "avg_logprob": -0.107032959614325, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.5305830836296082}, {"id": 5434, "seek": 1673464, "start": 16734.64, "end": 16736.64, "text": " this right here is the Wizard of Oz text", "tokens": [50364, 341, 558, 510, 307, 264, 37449, 295, 29843, 2487, 50464], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5435, "seek": 1673464, "start": 16736.64, "end": 16738.64, "text": " just a simple text file", "tokens": [50464, 445, 257, 2199, 2487, 3991, 50564], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5436, "seek": 1673464, "start": 16738.64, "end": 16740.64, "text": " it's the size isn't", "tokens": [50564, 309, 311, 264, 2744, 1943, 380, 50664], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5437, "seek": 1673464, "start": 16740.64, "end": 16742.64, "text": " super large", "tokens": [50664, 1687, 2416, 50764], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5438, "seek": 1673464, "start": 16742.64, "end": 16744.64, "text": " so we can actually open it all into ram at once", "tokens": [50764, 370, 321, 393, 767, 1269, 309, 439, 666, 10211, 412, 1564, 50864], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5439, "seek": 1673464, "start": 16744.64, "end": 16746.64, "text": " but", "tokens": [50864, 457, 50964], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5440, "seek": 1673464, "start": 16746.64, "end": 16748.64, "text": " if we were to use the open web text", "tokens": [50964, 498, 321, 645, 281, 764, 264, 1269, 3670, 2487, 51064], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5441, "seek": 1673464, "start": 16748.64, "end": 16750.64, "text": " we cannot actually read", "tokens": [51064, 321, 2644, 767, 1401, 51164], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5442, "seek": 1673464, "start": 16750.64, "end": 16752.64, "text": " you know 45 gigabytes of", "tokens": [51164, 291, 458, 6905, 42741, 295, 51264], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5443, "seek": 1673464, "start": 16752.64, "end": 16754.64, "text": " utfa text in ram at once", "tokens": [51264, 2839, 11771, 2487, 294, 10211, 412, 1564, 51364], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5444, "seek": 1673464, "start": 16754.64, "end": 16756.64, "text": " just can't do that unless you have like maybe", "tokens": [51364, 445, 393, 380, 360, 300, 5969, 291, 362, 411, 1310, 51464], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5445, "seek": 1673464, "start": 16756.64, "end": 16758.64, "text": " 64 or 128 gigabytes", "tokens": [51464, 12145, 420, 29810, 42741, 51564], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5446, "seek": 1673464, "start": 16758.64, "end": 16760.64, "text": " of ram this is really just", "tokens": [51564, 295, 10211, 341, 307, 534, 445, 51664], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5447, "seek": 1673464, "start": 16760.64, "end": 16762.64, "text": " not feasible at all", "tokens": [51664, 406, 26648, 412, 439, 51764], "temperature": 0.0, "avg_logprob": -0.14600184955428133, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.01016711350530386}, {"id": 5448, "seek": 1676264, "start": 16762.64, "end": 16764.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5449, "seek": 1676264, "start": 16764.64, "end": 16766.64, "text": " we're going to do some data pre-processing", "tokens": [50464, 321, 434, 516, 281, 360, 512, 1412, 659, 12, 41075, 278, 50564], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5450, "seek": 1676264, "start": 16766.64, "end": 16768.64, "text": " here some data cleaning", "tokens": [50564, 510, 512, 1412, 8924, 50664], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5451, "seek": 1676264, "start": 16768.64, "end": 16770.64, "text": " and then just a way to simply load", "tokens": [50664, 293, 550, 445, 257, 636, 281, 2935, 3677, 50764], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5452, "seek": 1676264, "start": 16770.64, "end": 16772.64, "text": " data into the", "tokens": [50764, 1412, 666, 264, 50864], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5453, "seek": 1676264, "start": 16772.64, "end": 16774.64, "text": " GPT so let's go ahead and do that", "tokens": [50864, 26039, 51, 370, 718, 311, 352, 2286, 293, 360, 300, 50964], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5454, "seek": 1676264, "start": 16774.64, "end": 16776.64, "text": " so the model has actually gotten really good at predicting", "tokens": [50964, 370, 264, 2316, 575, 767, 5768, 534, 665, 412, 32884, 51064], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5455, "seek": 1676264, "start": 16776.64, "end": 16778.64, "text": " the next token as you can see", "tokens": [51064, 264, 958, 14862, 382, 291, 393, 536, 51164], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5456, "seek": 1676264, "start": 16778.64, "end": 16780.64, "text": " the train loss here is 1.01", "tokens": [51164, 264, 3847, 4470, 510, 307, 502, 13, 10607, 51264], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5457, "seek": 1676264, "start": 16780.64, "end": 16782.64, "text": " so let's actually", "tokens": [51264, 370, 718, 311, 767, 51364], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5458, "seek": 1676264, "start": 16782.64, "end": 16784.64, "text": " find", "tokens": [51364, 915, 51464], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5459, "seek": 1676264, "start": 16784.64, "end": 16786.64, "text": " what the prediction accuracy of that is", "tokens": [51464, 437, 264, 17630, 14170, 295, 300, 307, 51564], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5460, "seek": 1676264, "start": 16786.64, "end": 16788.64, "text": " so I might just go into GPT-4", "tokens": [51564, 370, 286, 1062, 445, 352, 666, 26039, 51, 12, 19, 51664], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5461, "seek": 1676264, "start": 16788.64, "end": 16790.64, "text": " here", "tokens": [51664, 510, 51764], "temperature": 0.0, "avg_logprob": -0.08435086789338485, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0015975740971043706}, {"id": 5462, "seek": 1679064, "start": 16790.64, "end": 16792.64, "text": " and", "tokens": [50364, 293, 50464], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5463, "seek": 1679064, "start": 16792.64, "end": 16794.64, "text": " just ask it", "tokens": [50464, 445, 1029, 309, 50564], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5464, "seek": 1679064, "start": 16794.64, "end": 16796.64, "text": " what is", "tokens": [50564, 437, 307, 50664], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5465, "seek": 1679064, "start": 16796.64, "end": 16798.64, "text": " the prediction", "tokens": [50664, 264, 17630, 50764], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5466, "seek": 1679064, "start": 16798.64, "end": 16800.64, "text": " accuracy", "tokens": [50764, 14170, 50864], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5467, "seek": 1679064, "start": 16800.64, "end": 16802.64, "text": " of", "tokens": [50864, 295, 50964], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5468, "seek": 1679064, "start": 16802.64, "end": 16804.64, "text": " loss 1.01", "tokens": [50964, 4470, 502, 13, 10607, 51064], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5469, "seek": 1679064, "start": 16806.64, "end": 16808.64, "text": " the loss value", "tokens": [51164, 264, 4470, 2158, 51264], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5470, "seek": 1679064, "start": 16808.64, "end": 16810.64, "text": " comes with a loss function during the pre-process", "tokens": [51264, 1487, 365, 257, 4470, 2445, 1830, 264, 659, 12, 41075, 51364], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5471, "seek": 1679064, "start": 16810.64, "end": 16812.64, "text": " okay so let's", "tokens": [51364, 1392, 370, 718, 311, 51464], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5472, "seek": 1679064, "start": 16812.64, "end": 16814.64, "text": " see", "tokens": [51464, 536, 51564], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5473, "seek": 1679064, "start": 16816.64, "end": 16818.64, "text": " cross entropy loss", "tokens": [51664, 3278, 30867, 4470, 51764], "temperature": 0.0, "avg_logprob": -0.0987076681168353, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.0016227291198447347}, {"id": 5474, "seek": 1681864, "start": 16818.64, "end": 16820.64, "text": " doesn't mean the model is 99% accurate", "tokens": [50364, 1177, 380, 914, 264, 2316, 307, 11803, 4, 8559, 50464], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5475, "seek": 1681864, "start": 16822.64, "end": 16824.64, "text": " okay", "tokens": [50564, 1392, 50664], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5476, "seek": 1681864, "start": 16824.64, "end": 16826.64, "text": " so", "tokens": [50664, 370, 50764], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5477, "seek": 1681864, "start": 16826.64, "end": 16828.64, "text": " that pretty much means that the model is really accurate", "tokens": [50764, 300, 1238, 709, 1355, 300, 264, 2316, 307, 534, 8559, 50864], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5478, "seek": 1681864, "start": 16828.64, "end": 16830.64, "text": " but I want to find a value here", "tokens": [50864, 457, 286, 528, 281, 915, 257, 2158, 510, 50964], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5479, "seek": 1681864, "start": 16830.64, "end": 16832.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5480, "seek": 1681864, "start": 16832.64, "end": 16834.64, "text": " if the", "tokens": [51064, 498, 264, 51164], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5481, "seek": 1681864, "start": 16834.64, "end": 16836.64, "text": " we'll go to Wolfram alpha", "tokens": [51164, 321, 603, 352, 281, 16634, 2356, 8961, 51264], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5482, "seek": 1681864, "start": 16838.64, "end": 16840.64, "text": " and just we'll just guess some values here", "tokens": [51364, 293, 445, 321, 603, 445, 2041, 512, 4190, 510, 51464], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5483, "seek": 1681864, "start": 16840.64, "end": 16842.64, "text": " so negative ln", "tokens": [51464, 370, 3671, 44166, 51564], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5484, "seek": 1681864, "start": 16842.64, "end": 16844.64, "text": " of let's say", "tokens": [51564, 295, 718, 311, 584, 51664], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5485, "seek": 1681864, "start": 16844.64, "end": 16846.64, "text": " 0.9", "tokens": [51664, 1958, 13, 24, 51764], "temperature": 0.0, "avg_logprob": -0.10106933684576125, "compression_ratio": 1.5375, "no_speech_prob": 0.004608410410583019}, {"id": 5486, "seek": 1684664, "start": 16846.64, "end": 16848.64, "text": " okay so probably not that", "tokens": [50364, 1392, 370, 1391, 406, 300, 50464], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5487, "seek": 1684664, "start": 16850.64, "end": 16852.64, "text": " 0.3", "tokens": [50564, 1958, 13, 18, 50664], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5488, "seek": 1684664, "start": 16852.64, "end": 16854.64, "text": " 0.2", "tokens": [50664, 1958, 13, 17, 50764], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5489, "seek": 1684664, "start": 16854.64, "end": 16856.64, "text": " 0.4", "tokens": [50764, 1958, 13, 19, 50864], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5490, "seek": 1684664, "start": 16856.64, "end": 16858.64, "text": " 0.35", "tokens": [50864, 1958, 13, 8794, 50964], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5491, "seek": 1684664, "start": 16858.64, "end": 16860.64, "text": " yep so the model", "tokens": [50964, 18633, 370, 264, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5492, "seek": 1684664, "start": 16860.64, "end": 16862.64, "text": " has about a 35% chance", "tokens": [51064, 575, 466, 257, 6976, 4, 2931, 51164], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5493, "seek": 1684664, "start": 16862.64, "end": 16864.64, "text": " of guessing the next token as of right now", "tokens": [51164, 295, 17939, 264, 958, 14862, 382, 295, 558, 586, 51264], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5494, "seek": 1684664, "start": 16864.64, "end": 16866.64, "text": " so that's actually pretty good", "tokens": [51264, 370, 300, 311, 767, 1238, 665, 51364], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5495, "seek": 1684664, "start": 16866.64, "end": 16868.64, "text": " so 1 in every 3 tokens", "tokens": [51364, 370, 502, 294, 633, 805, 22667, 51464], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5496, "seek": 1684664, "start": 16868.64, "end": 16870.64, "text": " are spot on", "tokens": [51464, 366, 4008, 322, 51564], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5497, "seek": 1684664, "start": 16870.64, "end": 16872.64, "text": " so that is wonderful", "tokens": [51564, 370, 300, 307, 3715, 51664], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5498, "seek": 1684664, "start": 16872.64, "end": 16874.64, "text": " this is converging even more", "tokens": [51664, 341, 307, 9652, 3249, 754, 544, 51764], "temperature": 0.0, "avg_logprob": -0.06047702103518368, "compression_ratio": 1.4518072289156627, "no_speech_prob": 0.0013041519559919834}, {"id": 5499, "seek": 1687464, "start": 16874.64, "end": 16876.64, "text": " we're getting 0.89 so now it's getting like", "tokens": [50364, 321, 434, 1242, 1958, 13, 21115, 370, 586, 309, 311, 1242, 411, 50464], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5500, "seek": 1687464, "start": 16876.64, "end": 16878.64, "text": " every", "tokens": [50464, 633, 50564], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5501, "seek": 1687464, "start": 16878.64, "end": 16880.64, "text": " 40% are being guessed properly", "tokens": [50564, 3356, 4, 366, 885, 21852, 6108, 50664], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5502, "seek": 1687464, "start": 16880.64, "end": 16882.64, "text": " our validation is not doing", "tokens": [50664, 527, 24071, 307, 406, 884, 50764], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5503, "seek": 1687464, "start": 16882.64, "end": 16884.64, "text": " amazing though", "tokens": [50764, 2243, 1673, 50864], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5504, "seek": 1687464, "start": 16884.64, "end": 16886.64, "text": " but we'll linger on that a little bit here", "tokens": [50864, 457, 321, 603, 45657, 322, 300, 257, 707, 857, 510, 50964], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5505, "seek": 1687464, "start": 16886.64, "end": 16888.64, "text": " and you'll see sort of how this changes", "tokens": [50964, 293, 291, 603, 536, 1333, 295, 577, 341, 2962, 51064], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5506, "seek": 1687464, "start": 16888.64, "end": 16890.64, "text": " as we scale our data", "tokens": [51064, 382, 321, 4373, 527, 1412, 51164], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5507, "seek": 1687464, "start": 16890.64, "end": 16892.64, "text": " but", "tokens": [51164, 457, 51264], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5508, "seek": 1687464, "start": 16892.64, "end": 16894.64, "text": " so I've installed this", "tokens": [51264, 370, 286, 600, 8899, 341, 51364], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5509, "seek": 1687464, "start": 16894.64, "end": 16896.64, "text": " webtext.tar file", "tokens": [51364, 3670, 25111, 13, 23480, 3991, 51464], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5510, "seek": 1687464, "start": 16896.64, "end": 16898.64, "text": " tar file is interesting", "tokens": [51464, 3112, 3991, 307, 1880, 51564], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5511, "seek": 1687464, "start": 16898.64, "end": 16900.64, "text": " so in order to actually extract these", "tokens": [51564, 370, 294, 1668, 281, 767, 8947, 613, 51664], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5512, "seek": 1687464, "start": 16900.64, "end": 16902.64, "text": " you simply just", "tokens": [51664, 291, 2935, 445, 51764], "temperature": 0.0, "avg_logprob": -0.08980415889195033, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.01639072224497795}, {"id": 5513, "seek": 1690264, "start": 16902.64, "end": 16904.64, "text": " right click on them", "tokens": [50364, 558, 2052, 322, 552, 50464], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5514, "seek": 1690264, "start": 16904.64, "end": 16906.64, "text": " you go extract to", "tokens": [50464, 291, 352, 8947, 281, 50564], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5515, "seek": 1690264, "start": 16906.64, "end": 16908.64, "text": " and then it'll just make a new file here", "tokens": [50564, 293, 550, 309, 603, 445, 652, 257, 777, 3991, 510, 50664], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5516, "seek": 1690264, "start": 16908.64, "end": 16910.64, "text": " so it'll process this", "tokens": [50664, 370, 309, 603, 1399, 341, 50764], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5517, "seek": 1690264, "start": 16910.64, "end": 16912.64, "text": " you have to make sure you have WinRAR or else this might not work", "tokens": [50764, 291, 362, 281, 652, 988, 291, 362, 10427, 49, 1899, 420, 1646, 341, 1062, 406, 589, 50864], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5518, "seek": 1690264, "start": 16912.64, "end": 16914.64, "text": " to the fullest extent", "tokens": [50864, 281, 264, 45154, 8396, 50964], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5519, "seek": 1690264, "start": 16914.64, "end": 16916.64, "text": " and yeah", "tokens": [50964, 293, 1338, 51064], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5520, "seek": 1690264, "start": 16916.64, "end": 16918.64, "text": " so we'll just wait for this to finish up here", "tokens": [51064, 370, 321, 603, 445, 1699, 337, 341, 281, 2413, 493, 510, 51164], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5521, "seek": 1690264, "start": 16918.64, "end": 16920.64, "text": " we should end up with something that looks like this", "tokens": [51164, 321, 820, 917, 493, 365, 746, 300, 1542, 411, 341, 51264], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5522, "seek": 1690264, "start": 16920.64, "end": 16922.64, "text": " so open webtext", "tokens": [51264, 370, 1269, 3670, 25111, 51364], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5523, "seek": 1690264, "start": 16922.64, "end": 16924.64, "text": " and inside of here", "tokens": [51364, 293, 1854, 295, 510, 51464], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5524, "seek": 1690264, "start": 16924.64, "end": 16926.64, "text": " you have a bunch of xz files", "tokens": [51464, 291, 362, 257, 3840, 295, 2031, 89, 7098, 51564], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5525, "seek": 1690264, "start": 16926.64, "end": 16928.64, "text": " cool so there's actually 20,000", "tokens": [51564, 1627, 370, 456, 311, 767, 945, 11, 1360, 51664], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5526, "seek": 1690264, "start": 16928.64, "end": 16930.64, "text": " of these so we're gonna have to do a lot of", "tokens": [51664, 295, 613, 370, 321, 434, 799, 362, 281, 360, 257, 688, 295, 51764], "temperature": 0.0, "avg_logprob": -0.08360213944406221, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.004753507673740387}, {"id": 5527, "seek": 1693064, "start": 16930.64, "end": 16932.64, "text": " there's definitely gonna be some for loops in here for sure", "tokens": [50364, 456, 311, 2138, 799, 312, 512, 337, 16121, 294, 510, 337, 988, 50464], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5528, "seek": 1693064, "start": 16932.64, "end": 16934.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5529, "seek": 1693064, "start": 16934.64, "end": 16936.64, "text": " let's just handle this", "tokens": [50564, 718, 311, 445, 4813, 341, 50664], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5530, "seek": 1693064, "start": 16936.64, "end": 16938.64, "text": " step by step in this data", "tokens": [50664, 1823, 538, 1823, 294, 341, 1412, 50764], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5531, "seek": 1693064, "start": 16938.64, "end": 16940.64, "text": " extract file", "tokens": [50764, 8947, 3991, 50864], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5532, "seek": 1693064, "start": 16940.64, "end": 16942.64, "text": " so first off", "tokens": [50864, 370, 700, 766, 50964], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5533, "seek": 1693064, "start": 16942.64, "end": 16944.64, "text": " we're gonna need to import some python modules", "tokens": [50964, 321, 434, 799, 643, 281, 974, 512, 38797, 16679, 51064], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5534, "seek": 1693064, "start": 16944.64, "end": 16946.64, "text": " we're gonna use OS for interacting with the operating system", "tokens": [51064, 321, 434, 799, 764, 12731, 337, 18017, 365, 264, 7447, 1185, 51164], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5535, "seek": 1693064, "start": 16946.64, "end": 16948.64, "text": " LZMA", "tokens": [51164, 441, 57, 9998, 51264], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5536, "seek": 1693064, "start": 16948.64, "end": 16950.64, "text": " for handling", "tokens": [51264, 337, 13175, 51364], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5537, "seek": 1693064, "start": 16950.64, "end": 16952.64, "text": " xz files which are a type of compressed file", "tokens": [51364, 2031, 89, 7098, 597, 366, 257, 2010, 295, 30353, 3991, 51464], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5538, "seek": 1693064, "start": 16952.64, "end": 16954.64, "text": " like 7zip for example", "tokens": [51464, 411, 1614, 27268, 337, 1365, 51564], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5539, "seek": 1693064, "start": 16954.64, "end": 16956.64, "text": " and then", "tokens": [51564, 293, 550, 51664], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5540, "seek": 1693064, "start": 16956.64, "end": 16958.64, "text": " TQDM for displaying a progress bar", "tokens": [51664, 314, 48, 35, 44, 337, 36834, 257, 4205, 2159, 51764], "temperature": 0.0, "avg_logprob": -0.11482031562111594, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.004536137916147709}, {"id": 5541, "seek": 1695864, "start": 16958.64, "end": 16960.64, "text": " so you see a progress bar left to right", "tokens": [50364, 370, 291, 536, 257, 4205, 2159, 1411, 281, 558, 50464], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5542, "seek": 1695864, "start": 16960.64, "end": 16962.64, "text": " in the terminal", "tokens": [50464, 294, 264, 14709, 50564], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5543, "seek": 1695864, "start": 16962.64, "end": 16964.64, "text": " and that's pretty much gonna show us how quick we are", "tokens": [50564, 293, 300, 311, 1238, 709, 799, 855, 505, 577, 1702, 321, 366, 50664], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5544, "seek": 1695864, "start": 16964.64, "end": 16966.64, "text": " at executing the script", "tokens": [50664, 412, 32368, 264, 5755, 50764], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5545, "seek": 1695864, "start": 16966.64, "end": 16968.64, "text": " so next up", "tokens": [50764, 370, 958, 493, 50864], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5546, "seek": 1695864, "start": 16968.64, "end": 16970.64, "text": " we're gonna define a function", "tokens": [50864, 321, 434, 799, 6964, 257, 2445, 50964], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5547, "seek": 1695864, "start": 16970.64, "end": 16972.64, "text": " called xz files in dir", "tokens": [50964, 1219, 2031, 89, 7098, 294, 4746, 51064], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5548, "seek": 1695864, "start": 16972.64, "end": 16974.64, "text": " it takes a directory as an input", "tokens": [51064, 309, 2516, 257, 21120, 382, 364, 4846, 51164], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5549, "seek": 1695864, "start": 16974.64, "end": 16976.64, "text": " returns a list of all of the xz file names", "tokens": [51164, 11247, 257, 1329, 295, 439, 295, 264, 2031, 89, 3991, 5288, 51264], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5550, "seek": 1695864, "start": 16976.64, "end": 16978.64, "text": " in that directory", "tokens": [51264, 294, 300, 21120, 51364], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5551, "seek": 1695864, "start": 16978.64, "end": 16980.64, "text": " it's gonna use os.listdir", "tokens": [51364, 309, 311, 799, 764, 3003, 13, 8264, 35043, 51464], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5552, "seek": 1695864, "start": 16980.64, "end": 16982.64, "text": " to get all the file names", "tokens": [51464, 281, 483, 439, 264, 3991, 5288, 51564], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5553, "seek": 1695864, "start": 16982.64, "end": 16984.64, "text": " and os", "tokens": [51564, 293, 3003, 51664], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5554, "seek": 1695864, "start": 16984.64, "end": 16986.64, "text": " path as file", "tokens": [51664, 3100, 382, 3991, 51764], "temperature": 0.0, "avg_logprob": -0.12069298735762064, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012234463356435299}, {"id": 5555, "seek": 1698664, "start": 16986.64, "end": 16988.64, "text": " to check if each one is a file", "tokens": [50364, 281, 1520, 498, 1184, 472, 307, 257, 3991, 50464], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5556, "seek": 1698664, "start": 16988.64, "end": 16990.64, "text": " and not a directory or", "tokens": [50464, 293, 406, 257, 21120, 420, 50564], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5557, "seek": 1698664, "start": 16990.64, "end": 16992.64, "text": " symbolic link", "tokens": [50564, 25755, 2113, 50664], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5558, "seek": 1698664, "start": 16992.64, "end": 16994.64, "text": " if a file name ends with .xz", "tokens": [50664, 498, 257, 3991, 1315, 5314, 365, 2411, 87, 89, 50764], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5559, "seek": 1698664, "start": 16994.64, "end": 16996.64, "text": " and it's a file", "tokens": [50764, 293, 309, 311, 257, 3991, 50864], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5560, "seek": 1698664, "start": 16996.64, "end": 16998.64, "text": " it'll be added to the list", "tokens": [50864, 309, 603, 312, 3869, 281, 264, 1329, 50964], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5561, "seek": 1698664, "start": 16998.64, "end": 17000.64, "text": " so we just have a bunch of these files", "tokens": [50964, 370, 321, 445, 362, 257, 3840, 295, 613, 7098, 51064], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5562, "seek": 1698664, "start": 17000.64, "end": 17002.64, "text": " each element", "tokens": [51064, 1184, 4478, 51164], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5563, "seek": 1698664, "start": 17002.64, "end": 17004.64, "text": " is just the title of each file in there", "tokens": [51164, 307, 445, 264, 4876, 295, 1184, 3991, 294, 456, 51264], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5564, "seek": 1698664, "start": 17004.64, "end": 17006.64, "text": " so that's pretty much what that does", "tokens": [51264, 370, 300, 311, 1238, 709, 437, 300, 775, 51364], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5565, "seek": 1698664, "start": 17006.64, "end": 17008.64, "text": " and next up here", "tokens": [51364, 293, 958, 493, 510, 51464], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5566, "seek": 1698664, "start": 17008.64, "end": 17010.64, "text": " we'll set up some variables", "tokens": [51464, 321, 603, 992, 493, 512, 9102, 51564], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5567, "seek": 1698664, "start": 17010.64, "end": 17012.64, "text": " folder path", "tokens": [51564, 10820, 3100, 51664], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5568, "seek": 1698664, "start": 17012.64, "end": 17014.64, "text": " it's just gonna be where our xz files are located", "tokens": [51664, 309, 311, 445, 799, 312, 689, 527, 2031, 89, 7098, 366, 6870, 51764], "temperature": 0.0, "avg_logprob": -0.06576563948291843, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0038835559971630573}, {"id": 5569, "seek": 1701464, "start": 17014.64, "end": 17016.64, "text": " so I'm actually gonna change this here", "tokens": [50364, 370, 286, 478, 767, 799, 1319, 341, 510, 50464], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5570, "seek": 1701464, "start": 17016.64, "end": 17018.64, "text": " because that's an incorrect file path", "tokens": [50464, 570, 300, 311, 364, 18424, 3991, 3100, 50564], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5571, "seek": 1701464, "start": 17018.64, "end": 17020.64, "text": " but", "tokens": [50564, 457, 50664], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5572, "seek": 1701464, "start": 17022.64, "end": 17024.64, "text": " yes", "tokens": [50764, 2086, 50864], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5573, "seek": 1701464, "start": 17024.64, "end": 17026.64, "text": " just like that", "tokens": [50864, 445, 411, 300, 50964], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5574, "seek": 1701464, "start": 17028.64, "end": 17030.64, "text": " you have to make sure that these", "tokens": [51064, 291, 362, 281, 652, 988, 300, 613, 51164], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5575, "seek": 1701464, "start": 17030.64, "end": 17032.64, "text": " slashes are actually forward slashes", "tokens": [51164, 1061, 12808, 366, 767, 2128, 1061, 12808, 51264], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5576, "seek": 1701464, "start": 17032.64, "end": 17034.64, "text": " or else you might get bytecode errors", "tokens": [51264, 420, 1646, 291, 1062, 483, 40846, 22332, 13603, 51364], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5577, "seek": 1701464, "start": 17034.64, "end": 17036.64, "text": " so when it actually tries to read the string", "tokens": [51364, 370, 562, 309, 767, 9898, 281, 1401, 264, 6798, 51464], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5578, "seek": 1701464, "start": 17036.64, "end": 17038.64, "text": " it doesn't think that", "tokens": [51464, 309, 1177, 380, 519, 300, 51564], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5579, "seek": 1701464, "start": 17038.64, "end": 17040.64, "text": " these are separated", "tokens": [51564, 613, 366, 12005, 51664], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5580, "seek": 1701464, "start": 17040.64, "end": 17042.64, "text": " the backward slashes do weird things", "tokens": [51664, 264, 23897, 1061, 12808, 360, 3657, 721, 51764], "temperature": 0.0, "avg_logprob": -0.07434594631195068, "compression_ratio": 1.7239583333333333, "no_speech_prob": 0.013842000626027584}, {"id": 5581, "seek": 1704264, "start": 17042.64, "end": 17044.64, "text": " so you could either do", "tokens": [50364, 370, 291, 727, 2139, 360, 50464], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5582, "seek": 1704264, "start": 17044.64, "end": 17046.64, "text": " a one forward slash", "tokens": [50464, 257, 472, 2128, 17330, 50564], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5583, "seek": 1704264, "start": 17046.64, "end": 17048.64, "text": " or two backward slashes", "tokens": [50564, 420, 732, 23897, 1061, 12808, 50664], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5584, "seek": 1704264, "start": 17048.64, "end": 17050.64, "text": " that should work", "tokens": [50664, 300, 820, 589, 50764], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5585, "seek": 1704264, "start": 17050.64, "end": 17052.64, "text": " just make sure you get forward slashes", "tokens": [50764, 445, 652, 988, 291, 483, 2128, 1061, 12808, 50864], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5586, "seek": 1704264, "start": 17052.64, "end": 17054.64, "text": " and you should be good", "tokens": [50864, 293, 291, 820, 312, 665, 50964], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5587, "seek": 1704264, "start": 17054.64, "end": 17056.64, "text": " so folder path is where all these files are located", "tokens": [50964, 370, 10820, 3100, 307, 689, 439, 613, 7098, 366, 6870, 51064], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5588, "seek": 1704264, "start": 17056.64, "end": 17058.64, "text": " all these xz files are located as you saw", "tokens": [51064, 439, 613, 2031, 89, 7098, 366, 6870, 382, 291, 1866, 51164], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5589, "seek": 1704264, "start": 17058.64, "end": 17060.64, "text": " output file", "tokens": [51164, 5598, 3991, 51264], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5590, "seek": 1704264, "start": 17060.64, "end": 17062.64, "text": " is the pattern for output file names", "tokens": [51264, 307, 264, 5102, 337, 5598, 3991, 5288, 51364], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5591, "seek": 1704264, "start": 17062.64, "end": 17064.64, "text": " in case we want to have more than one of them", "tokens": [51364, 294, 1389, 321, 528, 281, 362, 544, 813, 472, 295, 552, 51464], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5592, "seek": 1704264, "start": 17064.64, "end": 17066.64, "text": " so if you want to have 200 output files", "tokens": [51464, 370, 498, 291, 528, 281, 362, 2331, 5598, 7098, 51564], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5593, "seek": 1704264, "start": 17066.64, "end": 17068.64, "text": " instead of one then it'll just be like", "tokens": [51564, 2602, 295, 472, 550, 309, 603, 445, 312, 411, 51664], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5594, "seek": 1704264, "start": 17068.64, "end": 17070.64, "text": " output 0, output 1, output 2 etc", "tokens": [51664, 5598, 1958, 11, 5598, 502, 11, 5598, 568, 5183, 51764], "temperature": 0.0, "avg_logprob": -0.12134428850309116, "compression_ratio": 1.9224137931034482, "no_speech_prob": 0.020951047539711}, {"id": 5595, "seek": 1707064, "start": 17070.64, "end": 17072.64, "text": " and then vocab file is where we want to save", "tokens": [50364, 293, 550, 2329, 455, 3991, 307, 689, 321, 528, 281, 3155, 50464], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5596, "seek": 1707064, "start": 17072.64, "end": 17074.64, "text": " our vocabulary", "tokens": [50464, 527, 19864, 50564], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5597, "seek": 1707064, "start": 17074.64, "end": 17076.64, "text": " keep in mind in this giant corpus", "tokens": [50564, 1066, 294, 1575, 294, 341, 7410, 1181, 31624, 50664], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5598, "seek": 1707064, "start": 17076.64, "end": 17078.64, "text": " you can't push it on to ram at once", "tokens": [50664, 291, 393, 380, 2944, 309, 322, 281, 10211, 412, 1564, 50764], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5599, "seek": 1707064, "start": 17078.64, "end": 17080.64, "text": " so what we're gonna do is as we're", "tokens": [50764, 370, 437, 321, 434, 799, 360, 307, 382, 321, 434, 50864], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5600, "seek": 1707064, "start": 17080.64, "end": 17082.64, "text": " reading these little compressed files", "tokens": [50864, 3760, 613, 707, 30353, 7098, 50964], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5601, "seek": 1707064, "start": 17082.64, "end": 17084.64, "text": " 20,000 of them", "tokens": [50964, 945, 11, 1360, 295, 552, 51064], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5602, "seek": 1707064, "start": 17084.64, "end": 17086.64, "text": " we're gonna take all of the new characters", "tokens": [51064, 321, 434, 799, 747, 439, 295, 264, 777, 4342, 51164], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5603, "seek": 1707064, "start": 17086.64, "end": 17088.64, "text": " from them and just push them into some vocab file", "tokens": [51164, 490, 552, 293, 445, 2944, 552, 666, 512, 2329, 455, 3991, 51264], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5604, "seek": 1707064, "start": 17088.64, "end": 17090.64, "text": " containing all of the different", "tokens": [51264, 19273, 439, 295, 264, 819, 51364], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5605, "seek": 1707064, "start": 17090.64, "end": 17092.64, "text": " characters that we have", "tokens": [51364, 4342, 300, 321, 362, 51464], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5606, "seek": 1707064, "start": 17092.64, "end": 17094.64, "text": " so that way we can handle this later", "tokens": [51464, 370, 300, 636, 321, 393, 4813, 341, 1780, 51564], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5607, "seek": 1707064, "start": 17094.64, "end": 17096.64, "text": " and just pretty much sort it into some", "tokens": [51564, 293, 445, 1238, 709, 1333, 309, 666, 512, 51664], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5608, "seek": 1707064, "start": 17096.64, "end": 17098.64, "text": " list containing all of our vocabulary", "tokens": [51664, 1329, 19273, 439, 295, 527, 19864, 51764], "temperature": 0.0, "avg_logprob": -0.08004656704989346, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.022258292883634567}, {"id": 5609, "seek": 1709864, "start": 17098.64, "end": 17100.64, "text": " split files", "tokens": [50364, 7472, 7098, 50464], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5610, "seek": 1709864, "start": 17100.64, "end": 17102.64, "text": " how many files do we want to split this into", "tokens": [50464, 577, 867, 7098, 360, 321, 528, 281, 7472, 341, 666, 50564], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5611, "seek": 1709864, "start": 17102.64, "end": 17104.64, "text": " so pretty much this", "tokens": [50564, 370, 1238, 709, 341, 50664], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5612, "seek": 1709864, "start": 17104.64, "end": 17106.64, "text": " it ties back to output file", "tokens": [50664, 309, 14039, 646, 281, 5598, 3991, 50764], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5613, "seek": 1709864, "start": 17106.64, "end": 17108.64, "text": " and just these curly braces here", "tokens": [50764, 293, 445, 613, 32066, 41537, 510, 50864], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5614, "seek": 1709864, "start": 17108.64, "end": 17110.64, "text": " how many do we want to have", "tokens": [50864, 577, 867, 360, 321, 528, 281, 362, 50964], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5615, "seek": 1709864, "start": 17110.64, "end": 17112.64, "text": " if we want to have more than one then we would", "tokens": [50964, 498, 321, 528, 281, 362, 544, 813, 472, 550, 321, 576, 51064], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5616, "seek": 1709864, "start": 17112.64, "end": 17114.64, "text": " this would take effect", "tokens": [51064, 341, 576, 747, 1802, 51164], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5617, "seek": 1709864, "start": 17114.64, "end": 17116.64, "text": " so cool", "tokens": [51164, 370, 1627, 51264], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5618, "seek": 1709864, "start": 17116.64, "end": 17118.64, "text": " now we'll use our", "tokens": [51264, 586, 321, 603, 764, 527, 51364], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5619, "seek": 1709864, "start": 17118.64, "end": 17120.64, "text": " x files in dir", "tokens": [51364, 2031, 7098, 294, 4746, 51464], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5620, "seek": 1709864, "start": 17120.64, "end": 17122.64, "text": " to get a list of file names and store them in this variable", "tokens": [51464, 281, 483, 257, 1329, 295, 3991, 5288, 293, 3531, 552, 294, 341, 7006, 51564], "temperature": 0.0, "avg_logprob": -0.11952268600463867, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.010983164422214031}, {"id": 5621, "seek": 1712264, "start": 17122.64, "end": 17124.64, "text": " we'll count", "tokens": [50364, 321, 603, 1207, 50464], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5622, "seek": 1712264, "start": 17124.64, "end": 17126.64, "text": " the number of", "tokens": [50464, 264, 1230, 295, 50564], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5623, "seek": 1712264, "start": 17126.64, "end": 17128.64, "text": " total xd files", "tokens": [50564, 3217, 2031, 67, 7098, 50664], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5624, "seek": 1712264, "start": 17128.64, "end": 17130.64, "text": " simply the length of our file names", "tokens": [50664, 2935, 264, 4641, 295, 527, 3991, 5288, 50764], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5625, "seek": 1712264, "start": 17130.64, "end": 17132.64, "text": " now in here", "tokens": [50764, 586, 294, 510, 50864], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5626, "seek": 1712264, "start": 17132.64, "end": 17134.64, "text": " we'll calculate the number of files", "tokens": [50864, 321, 603, 8873, 264, 1230, 295, 7098, 50964], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5627, "seek": 1712264, "start": 17134.64, "end": 17136.64, "text": " to process for each output file", "tokens": [50964, 281, 1399, 337, 1184, 5598, 3991, 51064], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5628, "seek": 1712264, "start": 17136.64, "end": 17138.64, "text": " if the user is requested", "tokens": [51064, 498, 264, 4195, 307, 16436, 51164], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5629, "seek": 1712264, "start": 17138.64, "end": 17140.64, "text": " more than one output file", "tokens": [51164, 544, 813, 472, 5598, 3991, 51264], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5630, "seek": 1712264, "start": 17140.64, "end": 17142.64, "text": " request more than one output file", "tokens": [51264, 5308, 544, 813, 472, 5598, 3991, 51364], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5631, "seek": 1712264, "start": 17142.64, "end": 17144.64, "text": " this is the total number of", "tokens": [51364, 341, 307, 264, 3217, 1230, 295, 51464], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5632, "seek": 1712264, "start": 17144.64, "end": 17146.64, "text": " files divided by the number", "tokens": [51464, 7098, 6666, 538, 264, 1230, 51564], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5633, "seek": 1712264, "start": 17146.64, "end": 17148.64, "text": " output files rounded down", "tokens": [51564, 5598, 7098, 23382, 760, 51664], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5634, "seek": 1712264, "start": 17148.64, "end": 17150.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.09133456380743729, "compression_ratio": 1.9757575757575758, "no_speech_prob": 0.03902085870504379}, {"id": 5635, "seek": 1715064, "start": 17150.64, "end": 17152.64, "text": " if the user only wants one", "tokens": [50364, 498, 264, 4195, 787, 2738, 472, 50464], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5636, "seek": 1715064, "start": 17152.64, "end": 17154.64, "text": " output file max count is the same as total files", "tokens": [50464, 5598, 3991, 11469, 1207, 307, 264, 912, 382, 3217, 7098, 50564], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5637, "seek": 1715064, "start": 17154.64, "end": 17156.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5638, "seek": 1715064, "start": 17156.64, "end": 17158.64, "text": " that's how that works", "tokens": [50664, 300, 311, 577, 300, 1985, 50764], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5639, "seek": 1715064, "start": 17158.64, "end": 17160.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5640, "seek": 1715064, "start": 17160.64, "end": 17162.64, "text": " next up we'll just create a", "tokens": [50864, 958, 493, 321, 603, 445, 1884, 257, 50964], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5641, "seek": 1715064, "start": 17162.64, "end": 17164.64, "text": " set to store a vocabulary when we", "tokens": [50964, 992, 281, 3531, 257, 19864, 562, 321, 51064], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5642, "seek": 1715064, "start": 17164.64, "end": 17166.64, "text": " start appending these new characters into it", "tokens": [51064, 722, 724, 2029, 613, 777, 4342, 666, 309, 51164], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5643, "seek": 1715064, "start": 17166.64, "end": 17168.64, "text": " a set is a", "tokens": [51164, 257, 992, 307, 257, 51264], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5644, "seek": 1715064, "start": 17168.64, "end": 17170.64, "text": " collection of unique items in case you did not know", "tokens": [51264, 5765, 295, 3845, 4754, 294, 1389, 291, 630, 406, 458, 51364], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5645, "seek": 1715064, "start": 17170.64, "end": 17172.64, "text": " entirely what a set was", "tokens": [51364, 7696, 437, 257, 992, 390, 51464], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5646, "seek": 1715064, "start": 17174.64, "end": 17176.64, "text": " now", "tokens": [51564, 586, 51664], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5647, "seek": 1715064, "start": 17176.64, "end": 17178.64, "text": " this is where it gets interesting", "tokens": [51664, 341, 307, 689, 309, 2170, 1880, 51764], "temperature": 0.0, "avg_logprob": -0.07971605146774138, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.00045110489008948207}, {"id": 5648, "seek": 1717864, "start": 17178.64, "end": 17180.64, "text": " we're ready to process our", "tokens": [50364, 321, 434, 1919, 281, 1399, 527, 50464], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5649, "seek": 1717864, "start": 17180.64, "end": 17182.64, "text": " .xz files", "tokens": [50464, 2411, 87, 89, 7098, 50564], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5650, "seek": 1717864, "start": 17182.64, "end": 17184.64, "text": " for each output file we'll process", "tokens": [50564, 337, 1184, 5598, 3991, 321, 603, 1399, 50664], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5651, "seek": 1717864, "start": 17184.64, "end": 17186.64, "text": " max count files", "tokens": [50664, 11469, 1207, 7098, 50764], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5652, "seek": 1717864, "start": 17186.64, "end": 17188.64, "text": " for each file we'll open it", "tokens": [50764, 337, 1184, 3991, 321, 603, 1269, 309, 50864], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5653, "seek": 1717864, "start": 17188.64, "end": 17190.64, "text": " read its contents", "tokens": [50864, 1401, 1080, 15768, 50964], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5654, "seek": 1717864, "start": 17190.64, "end": 17192.64, "text": " and write the contents to the current output file", "tokens": [50964, 293, 2464, 264, 15768, 281, 264, 2190, 5598, 3991, 51064], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5655, "seek": 1717864, "start": 17192.64, "end": 17194.64, "text": " and then add any unique characters to our vocabulary", "tokens": [51064, 293, 550, 909, 604, 3845, 4342, 281, 527, 19864, 51164], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5656, "seek": 1717864, "start": 17194.64, "end": 17196.64, "text": " set", "tokens": [51164, 992, 51264], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5657, "seek": 1717864, "start": 17196.64, "end": 17198.64, "text": " after processing max count files", "tokens": [51264, 934, 9007, 11469, 1207, 7098, 51364], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5658, "seek": 1717864, "start": 17198.64, "end": 17200.64, "text": " remove them from our list of files", "tokens": [51364, 4159, 552, 490, 527, 1329, 295, 7098, 51464], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5659, "seek": 1717864, "start": 17200.64, "end": 17202.64, "text": " and then finally", "tokens": [51464, 293, 550, 2721, 51564], "temperature": 0.0, "avg_logprob": -0.1337085466706351, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.04074888676404953}, {"id": 5660, "seek": 1720864, "start": 17208.64, "end": 17210.64, "text": " we'll write all of our vocabulary to this file", "tokens": [50364, 321, 603, 2464, 439, 295, 527, 19864, 281, 341, 3991, 50464], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5661, "seek": 1720864, "start": 17210.64, "end": 17212.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5662, "seek": 1720864, "start": 17212.64, "end": 17214.64, "text": " we pretty much just open", "tokens": [50564, 321, 1238, 709, 445, 1269, 50664], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5663, "seek": 1720864, "start": 17214.64, "end": 17216.64, "text": " we just", "tokens": [50664, 321, 445, 50764], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5664, "seek": 1720864, "start": 17216.64, "end": 17218.64, "text": " write all of these characters in the vocab", "tokens": [50764, 2464, 439, 295, 613, 4342, 294, 264, 2329, 455, 50864], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5665, "seek": 1720864, "start": 17218.64, "end": 17220.64, "text": " to this", "tokens": [50864, 281, 341, 50964], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5666, "seek": 1720864, "start": 17220.64, "end": 17222.64, "text": " vocab file which is here vocab.txt", "tokens": [50964, 2329, 455, 3991, 597, 307, 510, 2329, 455, 13, 83, 734, 51064], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5667, "seek": 1720864, "start": 17222.64, "end": 17224.64, "text": " so awesome", "tokens": [51064, 370, 3476, 51164], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5668, "seek": 1720864, "start": 17224.64, "end": 17226.64, "text": " now", "tokens": [51164, 586, 51264], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5669, "seek": 1720864, "start": 17226.64, "end": 17228.64, "text": " honestly we could just go ahead", "tokens": [51264, 6095, 321, 727, 445, 352, 2286, 51364], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5670, "seek": 1720864, "start": 17228.64, "end": 17230.64, "text": " and run this", "tokens": [51364, 293, 1190, 341, 51464], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5671, "seek": 1720864, "start": 17230.64, "end": 17232.64, "text": " so let's go ahead and go in here", "tokens": [51464, 370, 718, 311, 352, 2286, 293, 352, 294, 510, 51564], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5672, "seek": 1720864, "start": 17232.64, "end": 17234.64, "text": " I'm going to go cls to clear that", "tokens": [51564, 286, 478, 516, 281, 352, 596, 82, 281, 1850, 300, 51664], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5673, "seek": 1720864, "start": 17234.64, "end": 17236.64, "text": " we'll go python", "tokens": [51664, 321, 603, 352, 38797, 51764], "temperature": 0.0, "avg_logprob": -0.12598543621244884, "compression_ratio": 1.690217391304348, "no_speech_prob": 0.019106928259134293}, {"id": 5674, "seek": 1723664, "start": 17236.64, "end": 17238.64, "text": " data extract", "tokens": [50364, 1412, 8947, 50464], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5675, "seek": 1723664, "start": 17238.64, "end": 17240.64, "text": " .py", "tokens": [50464, 2411, 8200, 50564], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5676, "seek": 1723664, "start": 17240.64, "end": 17242.64, "text": " let's see this work it's magic", "tokens": [50564, 718, 311, 536, 341, 589, 309, 311, 5585, 50664], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5677, "seek": 1723664, "start": 17244.64, "end": 17246.64, "text": " how many files would you like to split this into", "tokens": [50764, 577, 867, 7098, 576, 291, 411, 281, 7472, 341, 666, 50864], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5678, "seek": 1723664, "start": 17246.64, "end": 17248.64, "text": " we'll go one", "tokens": [50864, 321, 603, 352, 472, 50964], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5679, "seek": 1723664, "start": 17248.64, "end": 17250.64, "text": " then we get a progress bar", "tokens": [50964, 550, 321, 483, 257, 4205, 2159, 51064], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5680, "seek": 1723664, "start": 17250.64, "end": 17252.64, "text": " 20,000 files and we'll just let that load", "tokens": [51064, 945, 11, 1360, 7098, 293, 321, 603, 445, 718, 300, 3677, 51164], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5681, "seek": 1723664, "start": 17252.64, "end": 17254.64, "text": " I'll come back to you in", "tokens": [51164, 286, 603, 808, 646, 281, 291, 294, 51264], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5682, "seek": 1723664, "start": 17254.64, "end": 17256.64, "text": " about 30 minutes to check up on this", "tokens": [51264, 466, 2217, 2077, 281, 1520, 493, 322, 341, 51364], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5683, "seek": 1723664, "start": 17256.64, "end": 17258.64, "text": " okay so there's not a little", "tokens": [51364, 1392, 370, 456, 311, 406, 257, 707, 51464], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5684, "seek": 1723664, "start": 17258.64, "end": 17260.64, "text": " one thing we want to consider for", "tokens": [51464, 472, 551, 321, 528, 281, 1949, 337, 51564], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5685, "seek": 1723664, "start": 17260.64, "end": 17262.64, "text": " and", "tokens": [51564, 293, 51664], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5686, "seek": 1723664, "start": 17262.64, "end": 17264.64, "text": " it's actually quite important is our splits", "tokens": [51664, 309, 311, 767, 1596, 1021, 307, 527, 37741, 51764], "temperature": 0.0, "avg_logprob": -0.11701084276951781, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009119869209825993}, {"id": 5687, "seek": 1726464, "start": 17264.64, "end": 17266.64, "text": " for train and file splits", "tokens": [50364, 337, 3847, 293, 3991, 37741, 50464], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5688, "seek": 1726464, "start": 17266.64, "end": 17268.64, "text": " it would be really inefficient", "tokens": [50464, 309, 576, 312, 534, 43495, 50564], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5689, "seek": 1726464, "start": 17268.64, "end": 17270.64, "text": " to just get blocks and then creating", "tokens": [50564, 281, 445, 483, 8474, 293, 550, 4084, 50664], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5690, "seek": 1726464, "start": 17270.64, "end": 17272.64, "text": " train and file splits as we go", "tokens": [50664, 3847, 293, 3991, 37741, 382, 321, 352, 50764], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5691, "seek": 1726464, "start": 17272.64, "end": 17274.64, "text": " every new batch we get", "tokens": [50764, 633, 777, 15245, 321, 483, 50864], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5692, "seek": 1726464, "start": 17274.64, "end": 17276.64, "text": " so in turn", "tokens": [50864, 370, 294, 1261, 50964], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5693, "seek": 1726464, "start": 17276.64, "end": 17278.64, "text": " what we might be better off doing", "tokens": [50964, 437, 321, 1062, 312, 1101, 766, 884, 51064], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5694, "seek": 1726464, "start": 17278.64, "end": 17280.64, "text": " is just creating an", "tokens": [51064, 307, 445, 4084, 364, 51164], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5695, "seek": 1726464, "start": 17280.64, "end": 17282.64, "text": " output train file and an output file file", "tokens": [51164, 5598, 3847, 3991, 293, 364, 5598, 3991, 3991, 51264], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5696, "seek": 1726464, "start": 17282.64, "end": 17284.64, "text": " so just two of them instead of one", "tokens": [51264, 370, 445, 732, 295, 552, 2602, 295, 472, 51364], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5697, "seek": 1726464, "start": 17284.64, "end": 17286.64, "text": " train is 90% of our data", "tokens": [51364, 3847, 307, 4289, 4, 295, 527, 1412, 51464], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5698, "seek": 1726464, "start": 17286.64, "end": 17288.64, "text": " file is 10% of our data", "tokens": [51464, 3991, 307, 1266, 4, 295, 527, 1412, 51564], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5699, "seek": 1726464, "start": 17288.64, "end": 17290.64, "text": " if that makes sense", "tokens": [51564, 498, 300, 1669, 2020, 51664], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5700, "seek": 1726464, "start": 17290.64, "end": 17292.64, "text": " so pretty much what I did", "tokens": [51664, 370, 1238, 709, 437, 286, 630, 51764], "temperature": 0.0, "avg_logprob": -0.09140613859733648, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.00668741948902607}, {"id": 5701, "seek": 1729264, "start": 17292.64, "end": 17294.64, "text": " is I got the output line for how many", "tokens": [50364, 307, 286, 658, 264, 5598, 1622, 337, 577, 867, 50464], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5702, "seek": 1729264, "start": 17294.64, "end": 17296.64, "text": " files do you want", "tokens": [50464, 7098, 360, 291, 528, 50564], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5703, "seek": 1729264, "start": 17296.64, "end": 17298.64, "text": " so you can see I got quite a bit of files", "tokens": [50564, 370, 291, 393, 536, 286, 658, 1596, 257, 857, 295, 7098, 50664], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5704, "seek": 1729264, "start": 17298.64, "end": 17300.64, "text": " produced here", "tokens": [50664, 7126, 510, 50764], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5705, "seek": 1729264, "start": 17300.64, "end": 17302.64, "text": " by not doing that correctly", "tokens": [50764, 538, 406, 884, 300, 8944, 50864], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5706, "seek": 1729264, "start": 17302.64, "end": 17304.64, "text": " so don't do that", "tokens": [50864, 370, 500, 380, 360, 300, 50964], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5707, "seek": 1729264, "start": 17304.64, "end": 17306.64, "text": " and", "tokens": [50964, 293, 51064], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5708, "seek": 1729264, "start": 17306.64, "end": 17308.64, "text": " yeah", "tokens": [51064, 1338, 51164], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5709, "seek": 1729264, "start": 17308.64, "end": 17310.64, "text": " essentially we're just", "tokens": [51164, 4476, 321, 434, 445, 51264], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5710, "seek": 1729264, "start": 17310.64, "end": 17312.64, "text": " we're pretty much just doing that", "tokens": [51264, 321, 434, 1238, 709, 445, 884, 300, 51364], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5711, "seek": 1729264, "start": 17312.64, "end": 17314.64, "text": " so we're processing some training files", "tokens": [51364, 370, 321, 434, 9007, 512, 3097, 7098, 51464], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5712, "seek": 1729264, "start": 17314.64, "end": 17316.64, "text": " we're separating 90%", "tokens": [51464, 321, 434, 29279, 4289, 4, 51564], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5713, "seek": 1729264, "start": 17316.64, "end": 17318.64, "text": " of the names on the left side", "tokens": [51564, 295, 264, 5288, 322, 264, 1411, 1252, 51664], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5714, "seek": 1729264, "start": 17318.64, "end": 17320.64, "text": " and then 10% of the names", "tokens": [51664, 293, 550, 1266, 4, 295, 264, 5288, 51764], "temperature": 0.0, "avg_logprob": -0.16575680687313987, "compression_ratio": 1.7035175879396984, "no_speech_prob": 0.013841845095157623}, {"id": 5715, "seek": 1732064, "start": 17320.64, "end": 17322.64, "text": " we're just separating those in the two different", "tokens": [50364, 321, 434, 445, 29279, 729, 294, 264, 732, 819, 50464], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5716, "seek": 1732064, "start": 17322.64, "end": 17324.64, "text": " arrays, file names", "tokens": [50464, 41011, 11, 3991, 5288, 50564], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5717, "seek": 1732064, "start": 17324.64, "end": 17326.64, "text": " and then we're just processing each of those", "tokens": [50564, 293, 550, 321, 434, 445, 9007, 1184, 295, 729, 50664], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5718, "seek": 1732064, "start": 17326.64, "end": 17328.64, "text": " arrays based on the file names", "tokens": [50664, 41011, 2361, 322, 264, 3991, 5288, 50764], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5719, "seek": 1732064, "start": 17328.64, "end": 17330.64, "text": " so I took away that little bit", "tokens": [50764, 370, 286, 1890, 1314, 300, 707, 857, 50864], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5720, "seek": 1732064, "start": 17330.64, "end": 17332.64, "text": " that was asking", "tokens": [50864, 300, 390, 3365, 50964], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5721, "seek": 1732064, "start": 17332.64, "end": 17334.64, "text": " how many files per", "tokens": [50964, 577, 867, 7098, 680, 51064], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5722, "seek": 1732064, "start": 17334.64, "end": 17336.64, "text": " split do you want", "tokens": [51064, 7472, 360, 291, 528, 51164], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5723, "seek": 1732064, "start": 17336.64, "end": 17338.64, "text": " so I took that away", "tokens": [51164, 370, 286, 1890, 300, 1314, 51264], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5724, "seek": 1732064, "start": 17338.64, "end": 17340.64, "text": " and this is effectively the same code", "tokens": [51264, 293, 341, 307, 8659, 264, 912, 3089, 51364], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5725, "seek": 1732064, "start": 17340.64, "end": 17342.64, "text": " just a little bit of tweaks", "tokens": [51364, 445, 257, 707, 857, 295, 46664, 51464], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5726, "seek": 1732064, "start": 17342.64, "end": 17344.64, "text": " and yeah", "tokens": [51464, 293, 1338, 51564], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5727, "seek": 1732064, "start": 17344.64, "end": 17346.64, "text": " so I'm going to go ahead and run this", "tokens": [51564, 370, 286, 478, 516, 281, 352, 2286, 293, 1190, 341, 51664], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5728, "seek": 1732064, "start": 17346.64, "end": 17348.64, "text": " data extract", "tokens": [51664, 1412, 8947, 51764], "temperature": 0.0, "avg_logprob": -0.09427881240844727, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0017542215064167976}, {"id": 5729, "seek": 1734864, "start": 17348.64, "end": 17350.64, "text": " cool", "tokens": [50364, 1627, 50464], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5730, "seek": 1734864, "start": 17350.64, "end": 17352.64, "text": " so we got an output train", "tokens": [50464, 370, 321, 658, 364, 5598, 3847, 50564], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5731, "seek": 1734864, "start": 17352.64, "end": 17354.64, "text": " and then after this it's going to do", "tokens": [50564, 293, 550, 934, 341, 309, 311, 516, 281, 360, 50664], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5732, "seek": 1734864, "start": 17354.64, "end": 17356.64, "text": " the output validation set", "tokens": [50664, 264, 5598, 24071, 992, 50764], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5733, "seek": 1734864, "start": 17356.64, "end": 17358.64, "text": " so I'll come back after this is done", "tokens": [50764, 370, 286, 603, 808, 646, 934, 341, 307, 1096, 50864], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5734, "seek": 1734864, "start": 17358.64, "end": 17360.64, "text": " so awesome I have just downloaded", "tokens": [50864, 370, 3476, 286, 362, 445, 21748, 50964], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5735, "seek": 1734864, "start": 17360.64, "end": 17362.64, "text": " both or I've both", "tokens": [50964, 1293, 420, 286, 600, 1293, 51064], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5736, "seek": 1734864, "start": 17362.64, "end": 17364.64, "text": " got both these splits", "tokens": [51064, 658, 1293, 613, 37741, 51164], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5737, "seek": 1734864, "start": 17364.64, "end": 17366.64, "text": " output train and val train so just to", "tokens": [51164, 5598, 3847, 293, 1323, 3847, 370, 445, 281, 51264], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5738, "seek": 1734864, "start": 17366.64, "end": 17368.64, "text": " confirm that they're", "tokens": [51264, 9064, 300, 436, 434, 51364], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5739, "seek": 1734864, "start": 17368.64, "end": 17370.64, "text": " actually the right size got 38.9", "tokens": [51364, 767, 264, 558, 2744, 658, 12843, 13, 24, 51464], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5740, "seek": 1734864, "start": 17370.64, "end": 17372.64, "text": " and then 4.27", "tokens": [51464, 293, 550, 1017, 13, 10076, 51564], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5741, "seek": 1734864, "start": 17372.64, "end": 17374.64, "text": " so if we do this divided by", "tokens": [51564, 370, 498, 321, 360, 341, 6666, 538, 51664], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5742, "seek": 1734864, "start": 17374.64, "end": 17376.64, "text": " 9 so about 30", "tokens": [51664, 1722, 370, 466, 2217, 51764], "temperature": 0.0, "avg_logprob": -0.10134713433005593, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.013217654079198837}, {"id": 5743, "seek": 1737664, "start": 17376.64, "end": 17378.64, "text": " 8.9 divided by 9", "tokens": [50364, 1649, 13, 24, 6666, 538, 1722, 50464], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5744, "seek": 1737664, "start": 17378.64, "end": 17380.64, "text": " 4.32 and it's", "tokens": [50464, 1017, 13, 11440, 293, 309, 311, 50564], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5745, "seek": 1737664, "start": 17380.64, "end": 17382.64, "text": " pretty close to 4.27 so", "tokens": [50564, 1238, 1998, 281, 1017, 13, 10076, 370, 50664], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5746, "seek": 1737664, "start": 17382.64, "end": 17384.64, "text": " we can confirm that these are pretty much", "tokens": [50664, 321, 393, 9064, 300, 613, 366, 1238, 709, 50764], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5747, "seek": 1737664, "start": 17384.64, "end": 17386.64, "text": " the", "tokens": [50764, 264, 50864], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5748, "seek": 1737664, "start": 17386.64, "end": 17388.64, "text": " length that we expect them to be", "tokens": [50864, 4641, 300, 321, 2066, 552, 281, 312, 50964], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5749, "seek": 1737664, "start": 17388.64, "end": 17390.64, "text": " so awesome we have this vocab.txt", "tokens": [50964, 370, 3476, 321, 362, 341, 2329, 455, 13, 83, 734, 51064], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5750, "seek": 1737664, "start": 17390.64, "end": 17392.64, "text": " file wonderful so now", "tokens": [51064, 3991, 3715, 370, 586, 51164], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5751, "seek": 1737664, "start": 17392.64, "end": 17394.64, "text": " we have to focus on is", "tokens": [51164, 321, 362, 281, 1879, 322, 307, 51264], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5752, "seek": 1737664, "start": 17394.64, "end": 17396.64, "text": " getting this into", "tokens": [51264, 1242, 341, 666, 51364], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5753, "seek": 1737664, "start": 17396.64, "end": 17398.64, "text": " our batches so when we call", "tokens": [51364, 527, 15245, 279, 370, 562, 321, 818, 51464], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5754, "seek": 1737664, "start": 17398.64, "end": 17400.64, "text": " our get batch function actually", "tokens": [51464, 527, 483, 15245, 2445, 767, 51564], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5755, "seek": 1737664, "start": 17400.64, "end": 17402.64, "text": " cd out of this open this in", "tokens": [51564, 269, 67, 484, 295, 341, 1269, 341, 294, 51664], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5756, "seek": 1737664, "start": 17402.64, "end": 17404.64, "text": " a Jupyter notebook", "tokens": [51664, 257, 22125, 88, 391, 21060, 51764], "temperature": 0.0, "avg_logprob": -0.10234817436763219, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.012046565301716328}, {"id": 5757, "seek": 1740664, "start": 17406.64, "end": 17408.64, "text": " copy my desktop", "tokens": [50364, 5055, 452, 14502, 50464], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5758, "seek": 1740664, "start": 17410.64, "end": 17412.64, "text": " paste it over here", "tokens": [50564, 9163, 309, 670, 510, 50664], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5759, "seek": 1740664, "start": 17412.64, "end": 17414.64, "text": " and perfect", "tokens": [50664, 293, 2176, 50764], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5760, "seek": 1740664, "start": 17414.64, "end": 17416.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5761, "seek": 1740664, "start": 17416.64, "end": 17418.64, "text": " this open when web text folder with", "tokens": [50864, 341, 1269, 562, 3670, 2487, 10820, 365, 50964], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5762, "seek": 1740664, "start": 17418.64, "end": 17420.64, "text": " these files awesome", "tokens": [50964, 613, 7098, 3476, 51064], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5763, "seek": 1740664, "start": 17420.64, "end": 17422.64, "text": " and our GPTV", "tokens": [51064, 293, 527, 26039, 12586, 51164], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5764, "seek": 1740664, "start": 17422.64, "end": 17424.64, "text": " one", "tokens": [51164, 472, 51264], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5765, "seek": 1740664, "start": 17424.64, "end": 17426.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5766, "seek": 1740664, "start": 17426.64, "end": 17428.64, "text": " this get batch function", "tokens": [51364, 341, 483, 15245, 2445, 51464], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5767, "seek": 1740664, "start": 17428.64, "end": 17430.64, "text": " is going to have to", "tokens": [51464, 307, 516, 281, 362, 281, 51564], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5768, "seek": 1740664, "start": 17430.64, "end": 17432.64, "text": " change also these", "tokens": [51564, 1319, 611, 613, 51664], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5769, "seek": 1740664, "start": 17432.64, "end": 17434.64, "text": " are going to have to change as well", "tokens": [51664, 366, 516, 281, 362, 281, 1319, 382, 731, 51764], "temperature": 0.0, "avg_logprob": -0.180026067269815, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0049811070784926414}, {"id": 5770, "seek": 1743464, "start": 17434.64, "end": 17436.64, "text": " and this one too these are probably", "tokens": [50364, 293, 341, 472, 886, 613, 366, 1391, 50464], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5771, "seek": 1743464, "start": 17436.64, "end": 17438.64, "text": " not going to be here", "tokens": [50464, 406, 516, 281, 312, 510, 50564], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5772, "seek": 1743464, "start": 17438.64, "end": 17440.64, "text": " but pretty much", "tokens": [50564, 457, 1238, 709, 50664], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5773, "seek": 1743464, "start": 17440.64, "end": 17442.64, "text": " let's go ahead and first of all", "tokens": [50664, 718, 311, 352, 2286, 293, 700, 295, 439, 50764], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5774, "seek": 1743464, "start": 17442.64, "end": 17444.64, "text": " get this vocab.txt", "tokens": [50764, 483, 341, 2329, 455, 13, 83, 734, 50864], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5775, "seek": 1743464, "start": 17444.64, "end": 17446.64, "text": " in so what I'm going to do", "tokens": [50864, 294, 370, 437, 286, 478, 516, 281, 360, 50964], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5776, "seek": 1743464, "start": 17446.64, "end": 17448.64, "text": " I'm just going to go", "tokens": [50964, 286, 478, 445, 516, 281, 352, 51064], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5777, "seek": 1743464, "start": 17448.64, "end": 17450.64, "text": " we're going to go", "tokens": [51064, 321, 434, 516, 281, 352, 51164], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5778, "seek": 1743464, "start": 17450.64, "end": 17452.64, "text": " open web text slash", "tokens": [51164, 1269, 3670, 2487, 17330, 51264], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5779, "seek": 1743464, "start": 17452.64, "end": 17454.64, "text": " vocab.txt", "tokens": [51264, 2329, 455, 13, 83, 734, 51364], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5780, "seek": 1743464, "start": 17454.64, "end": 17456.64, "text": " cool so that's our vocab", "tokens": [51364, 1627, 370, 300, 311, 527, 2329, 455, 51464], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5781, "seek": 1743464, "start": 17456.64, "end": 17458.64, "text": " right there text read", "tokens": [51464, 558, 456, 2487, 1401, 51564], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5782, "seek": 1743464, "start": 17458.64, "end": 17460.64, "text": " vocab size the length of that nice", "tokens": [51564, 2329, 455, 2744, 264, 4641, 295, 300, 1481, 51664], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5783, "seek": 1743464, "start": 17460.64, "end": 17462.64, "text": " so that's what our vocab is", "tokens": [51664, 370, 300, 311, 437, 527, 2329, 455, 307, 51764], "temperature": 0.0, "avg_logprob": -0.127808604033097, "compression_ratio": 1.7880434782608696, "no_speech_prob": 0.0037640396039932966}, {"id": 5784, "seek": 1746264, "start": 17462.64, "end": 17464.64, "text": " and then", "tokens": [50364, 293, 550, 50464], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5785, "seek": 1746264, "start": 17464.64, "end": 17466.64, "text": " what we're going to do next", "tokens": [50464, 437, 321, 434, 516, 281, 360, 958, 50564], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5786, "seek": 1746264, "start": 17466.64, "end": 17468.64, "text": " is change this get batch function", "tokens": [50564, 307, 1319, 341, 483, 15245, 2445, 50664], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5787, "seek": 1746264, "start": 17468.64, "end": 17470.64, "text": " around", "tokens": [50664, 926, 50764], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5788, "seek": 1746264, "start": 17470.64, "end": 17472.64, "text": " so first of all I'm going to go ahead", "tokens": [50764, 370, 700, 295, 439, 286, 478, 516, 281, 352, 2286, 50864], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5789, "seek": 1746264, "start": 17472.64, "end": 17474.64, "text": " and get rid of this here", "tokens": [50864, 293, 483, 3973, 295, 341, 510, 50964], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5790, "seek": 1746264, "start": 17474.64, "end": 17476.64, "text": " and then", "tokens": [50964, 293, 550, 51064], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5791, "seek": 1746264, "start": 17476.64, "end": 17478.64, "text": " I've actually produced", "tokens": [51064, 286, 600, 767, 7126, 51164], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5792, "seek": 1746264, "start": 17478.64, "end": 17480.64, "text": " some code specifically for", "tokens": [51164, 512, 3089, 4682, 337, 51264], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5793, "seek": 1746264, "start": 17480.64, "end": 17482.64, "text": " this so I'm just going to go back", "tokens": [51264, 341, 370, 286, 478, 445, 516, 281, 352, 646, 51364], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5794, "seek": 1746264, "start": 17482.64, "end": 17484.64, "text": " to my", "tokens": [51364, 281, 452, 51464], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5795, "seek": 1746264, "start": 17484.64, "end": 17486.64, "text": " I'm just going to find", "tokens": [51464, 286, 478, 445, 516, 281, 915, 51564], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5796, "seek": 1746264, "start": 17486.64, "end": 17488.64, "text": " this folder", "tokens": [51564, 341, 10820, 51664], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5797, "seek": 1746264, "start": 17488.64, "end": 17490.64, "text": " okay so I've", "tokens": [51664, 1392, 370, 286, 600, 51764], "temperature": 0.0, "avg_logprob": -0.128866225794742, "compression_ratio": 1.6783625730994152, "no_speech_prob": 0.003375898115336895}, {"id": 5798, "seek": 1749064, "start": 17490.64, "end": 17492.64, "text": " actually produced some", "tokens": [50364, 767, 7126, 512, 50464], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5799, "seek": 1749064, "start": 17492.64, "end": 17494.64, "text": " code here", "tokens": [50464, 3089, 510, 50564], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5800, "seek": 1749064, "start": 17494.64, "end": 17496.64, "text": " I produced this off camera", "tokens": [50564, 286, 7126, 341, 766, 2799, 50664], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5801, "seek": 1749064, "start": 17496.64, "end": 17498.64, "text": " but", "tokens": [50664, 457, 50764], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5802, "seek": 1749064, "start": 17498.64, "end": 17500.64, "text": " pretty much what this is going to do", "tokens": [50764, 1238, 709, 437, 341, 307, 516, 281, 360, 50864], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5803, "seek": 1749064, "start": 17500.64, "end": 17502.64, "text": " it's going to let us call a split", "tokens": [50864, 309, 311, 516, 281, 718, 505, 818, 257, 7472, 50964], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5804, "seek": 1749064, "start": 17502.64, "end": 17504.64, "text": " okay so we have our get batch", "tokens": [50964, 1392, 370, 321, 362, 527, 483, 15245, 51064], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5805, "seek": 1749064, "start": 17504.64, "end": 17506.64, "text": " function all of this down here is the", "tokens": [51064, 2445, 439, 295, 341, 760, 510, 307, 264, 51164], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5806, "seek": 1749064, "start": 17506.64, "end": 17508.64, "text": " same as our GPTV", "tokens": [51164, 912, 382, 527, 26039, 12586, 51264], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5807, "seek": 1749064, "start": 17508.64, "end": 17510.64, "text": " one file and then", "tokens": [51264, 472, 3991, 293, 550, 51364], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5808, "seek": 1749064, "start": 17510.64, "end": 17512.64, "text": " this data is", "tokens": [51364, 341, 1412, 307, 51464], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5809, "seek": 1749064, "start": 17512.64, "end": 17514.64, "text": " just going to get a random chunk of text", "tokens": [51464, 445, 516, 281, 483, 257, 4974, 16635, 295, 2487, 51564], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5810, "seek": 1749064, "start": 17514.64, "end": 17516.64, "text": " with giant block of text", "tokens": [51564, 365, 7410, 3461, 295, 2487, 51664], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5811, "seek": 1749064, "start": 17516.64, "end": 17518.64, "text": " and the way", "tokens": [51664, 293, 264, 636, 51764], "temperature": 0.0, "avg_logprob": -0.11752461452110141, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00038592974306084216}, {"id": 5812, "seek": 1751864, "start": 17518.64, "end": 17520.64, "text": " that we get it is actually pretty interesting", "tokens": [50364, 300, 321, 483, 309, 307, 767, 1238, 1880, 50464], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5813, "seek": 1751864, "start": 17520.64, "end": 17522.64, "text": " so the way that we get this text is", "tokens": [50464, 370, 264, 636, 300, 321, 483, 341, 2487, 307, 50564], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5814, "seek": 1751864, "start": 17522.64, "end": 17524.64, "text": " something called memory mapping", "tokens": [50564, 746, 1219, 4675, 18350, 50664], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5815, "seek": 1751864, "start": 17524.64, "end": 17526.64, "text": " so memory mapping is a way", "tokens": [50664, 370, 4675, 18350, 307, 257, 636, 50764], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5816, "seek": 1751864, "start": 17526.64, "end": 17528.64, "text": " to look at disk files", "tokens": [50764, 281, 574, 412, 12355, 7098, 50864], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5817, "seek": 1751864, "start": 17528.64, "end": 17530.64, "text": " or to open them and look at pieces of them", "tokens": [50864, 420, 281, 1269, 552, 293, 574, 412, 3755, 295, 552, 50964], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5818, "seek": 1751864, "start": 17530.64, "end": 17532.64, "text": " without opening the entire thing at once", "tokens": [50964, 1553, 5193, 264, 2302, 551, 412, 1564, 51064], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5819, "seek": 1751864, "start": 17532.64, "end": 17534.64, "text": " so memory mapping", "tokens": [51064, 370, 4675, 18350, 51164], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5820, "seek": 1751864, "start": 17534.64, "end": 17536.64, "text": " I'm not a hardware guy so I can't", "tokens": [51164, 286, 478, 406, 257, 8837, 2146, 370, 286, 393, 380, 51264], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5821, "seek": 1751864, "start": 17536.64, "end": 17538.64, "text": " really talk about that", "tokens": [51264, 534, 751, 466, 300, 51364], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5822, "seek": 1751864, "start": 17538.64, "end": 17540.64, "text": " memory mapping is pretty", "tokens": [51364, 4675, 18350, 307, 1238, 51464], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5823, "seek": 1751864, "start": 17540.64, "end": 17542.64, "text": " cool and allows us to look at little", "tokens": [51464, 1627, 293, 4045, 505, 281, 574, 412, 707, 51564], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5824, "seek": 1751864, "start": 17542.64, "end": 17544.64, "text": " chunks at a time in very large text files", "tokens": [51564, 24004, 412, 257, 565, 294, 588, 2416, 2487, 7098, 51664], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5825, "seek": 1751864, "start": 17544.64, "end": 17546.64, "text": " so that's essentially what we're doing here", "tokens": [51664, 370, 300, 311, 4476, 437, 321, 434, 884, 510, 51764], "temperature": 0.0, "avg_logprob": -0.05175464115445576, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.002082555554807186}, {"id": 5826, "seek": 1754664, "start": 17546.64, "end": 17548.64, "text": " we're passing this split", "tokens": [50364, 321, 434, 8437, 341, 7472, 50464], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5827, "seek": 1754664, "start": 17548.64, "end": 17550.64, "text": " split", "tokens": [50464, 7472, 50564], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5828, "seek": 1754664, "start": 17550.64, "end": 17552.64, "text": " file name is equal to train split", "tokens": [50564, 3991, 1315, 307, 2681, 281, 3847, 7472, 50664], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5829, "seek": 1754664, "start": 17552.64, "end": 17554.64, "text": " this is just an example text file", "tokens": [50664, 341, 307, 445, 364, 1365, 2487, 3991, 50764], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5830, "seek": 1754664, "start": 17556.64, "end": 17558.64, "text": " if the split is equal to train then this", "tokens": [50864, 498, 264, 7472, 307, 2681, 281, 3847, 550, 341, 50964], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5831, "seek": 1754664, "start": 17558.64, "end": 17560.64, "text": " is our file name else", "tokens": [50964, 307, 527, 3991, 1315, 1646, 51064], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5832, "seek": 1754664, "start": 17560.64, "end": 17562.64, "text": " file split and then we're going to", "tokens": [51064, 3991, 7472, 293, 550, 321, 434, 516, 281, 51164], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5833, "seek": 1754664, "start": 17562.64, "end": 17564.64, "text": " open this file name in binary mode", "tokens": [51164, 1269, 341, 3991, 1315, 294, 17434, 4391, 51264], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5834, "seek": 1754664, "start": 17564.64, "end": 17566.64, "text": " this has to be in binary mode", "tokens": [51264, 341, 575, 281, 312, 294, 17434, 4391, 51364], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5835, "seek": 1754664, "start": 17566.64, "end": 17568.64, "text": " it's also a lot more efficient in binary", "tokens": [51364, 309, 311, 611, 257, 688, 544, 7148, 294, 17434, 51464], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5836, "seek": 1754664, "start": 17568.64, "end": 17570.64, "text": " mode and then", "tokens": [51464, 4391, 293, 550, 51564], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5837, "seek": 1754664, "start": 17570.64, "end": 17572.64, "text": " we're going to open this with a mem map", "tokens": [51564, 321, 434, 516, 281, 1269, 341, 365, 257, 1334, 4471, 51664], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5838, "seek": 1754664, "start": 17572.64, "end": 17574.64, "text": " so I don't expect you to memorize all the mem map syntax", "tokens": [51664, 370, 286, 500, 380, 2066, 291, 281, 27478, 439, 264, 1334, 4471, 28431, 51764], "temperature": 0.0, "avg_logprob": -0.10403511303813517, "compression_ratio": 2.0964467005076144, "no_speech_prob": 0.01204850897192955}, {"id": 5839, "seek": 1757464, "start": 17574.64, "end": 17576.64, "text": " you can look at the docs if you would like", "tokens": [50364, 291, 393, 574, 412, 264, 45623, 498, 291, 576, 411, 50464], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5840, "seek": 1757464, "start": 17576.64, "end": 17578.64, "text": " but I'm just going to explain", "tokens": [50464, 457, 286, 478, 445, 516, 281, 2903, 50564], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5841, "seek": 1757464, "start": 17578.64, "end": 17580.64, "text": " logically what's happening", "tokens": [50564, 38887, 437, 311, 2737, 50664], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5842, "seek": 1757464, "start": 17580.64, "end": 17582.64, "text": " so we're going to open this", "tokens": [50664, 370, 321, 434, 516, 281, 1269, 341, 50764], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5843, "seek": 1757464, "start": 17582.64, "end": 17584.64, "text": " with the mem map library", "tokens": [50764, 365, 264, 1334, 4471, 6405, 50864], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5844, "seek": 1757464, "start": 17584.64, "end": 17586.64, "text": " and we're going to open this as", "tokens": [50864, 293, 321, 434, 516, 281, 1269, 341, 382, 50964], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5845, "seek": 1757464, "start": 17586.64, "end": 17588.64, "text": " mm so", "tokens": [50964, 11169, 370, 51064], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5846, "seek": 1757464, "start": 17588.64, "end": 17590.64, "text": " the file size is literally", "tokens": [51064, 264, 3991, 2744, 307, 3736, 51164], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5847, "seek": 1757464, "start": 17590.64, "end": 17592.64, "text": " just the length of it so determining", "tokens": [51164, 445, 264, 4641, 295, 309, 370, 23751, 51264], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5848, "seek": 1757464, "start": 17592.64, "end": 17594.64, "text": " the file size and", "tokens": [51264, 264, 3991, 2744, 293, 51364], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5849, "seek": 1757464, "start": 17594.64, "end": 17596.64, "text": " all we're doing from this point is we're just finding", "tokens": [51364, 439, 321, 434, 884, 490, 341, 935, 307, 321, 434, 445, 5006, 51464], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5850, "seek": 1757464, "start": 17596.64, "end": 17598.64, "text": " a position so we're using the random library", "tokens": [51464, 257, 2535, 370, 321, 434, 1228, 264, 4974, 6405, 51564], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5851, "seek": 1757464, "start": 17598.64, "end": 17600.64, "text": " and we're finding", "tokens": [51564, 293, 321, 434, 5006, 51664], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5852, "seek": 1757464, "start": 17600.64, "end": 17602.64, "text": " a position between", "tokens": [51664, 257, 2535, 1296, 51764], "temperature": 0.0, "avg_logprob": -0.08956123417259282, "compression_ratio": 1.9615384615384615, "no_speech_prob": 0.010320751927793026}, {"id": 5853, "seek": 1760264, "start": 17602.64, "end": 17604.64, "text": " 0", "tokens": [50364, 1958, 50464], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5854, "seek": 1760264, "start": 17604.64, "end": 17606.64, "text": " and the file size", "tokens": [50464, 293, 264, 3991, 2744, 50564], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5855, "seek": 1760264, "start": 17606.64, "end": 17608.64, "text": " minus block size times batch size", "tokens": [50564, 3175, 3461, 2744, 1413, 15245, 2744, 50664], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5856, "seek": 1760264, "start": 17608.64, "end": 17610.64, "text": " so pretty much we have this", "tokens": [50664, 370, 1238, 709, 321, 362, 341, 50764], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5857, "seek": 1760264, "start": 17610.64, "end": 17612.64, "text": " giant text", "tokens": [50764, 7410, 2487, 50864], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5858, "seek": 1760264, "start": 17612.64, "end": 17614.64, "text": " file we could either", "tokens": [50864, 3991, 321, 727, 2139, 50964], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5859, "seek": 1760264, "start": 17614.64, "end": 17616.64, "text": " what we want to do is we want to start", "tokens": [50964, 437, 321, 528, 281, 360, 307, 321, 528, 281, 722, 51064], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5860, "seek": 1760264, "start": 17616.64, "end": 17618.64, "text": " from 0 and go up to like", "tokens": [51064, 490, 1958, 293, 352, 493, 281, 411, 51164], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5861, "seek": 1760264, "start": 17618.64, "end": 17620.64, "text": " just before the end because if we", "tokens": [51164, 445, 949, 264, 917, 570, 498, 321, 51264], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5862, "seek": 1760264, "start": 17620.64, "end": 17622.64, "text": " actually sample", "tokens": [51264, 767, 6889, 51364], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5863, "seek": 1760264, "start": 17622.64, "end": 17624.64, "text": " that last piece then it's still", "tokens": [51364, 300, 1036, 2522, 550, 309, 311, 920, 51464], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5864, "seek": 1760264, "start": 17624.64, "end": 17626.64, "text": " going to have some wiggle room to", "tokens": [51464, 516, 281, 362, 512, 33377, 1808, 281, 51564], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5865, "seek": 1760264, "start": 17626.64, "end": 17628.64, "text": " reach further into the file", "tokens": [51564, 2524, 3052, 666, 264, 3991, 51664], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5866, "seek": 1760264, "start": 17628.64, "end": 17630.64, "text": " if we just made it from like", "tokens": [51664, 498, 321, 445, 1027, 309, 490, 411, 51764], "temperature": 0.0, "avg_logprob": -0.061911641983758835, "compression_ratio": 1.6990291262135921, "no_speech_prob": 0.007932319305837154}, {"id": 5867, "seek": 1763064, "start": 17630.64, "end": 17632.64, "text": " the first", "tokens": [50364, 264, 700, 50464], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5868, "seek": 1763064, "start": 17632.64, "end": 17634.64, "text": " the very start of the file to the very end", "tokens": [50464, 264, 588, 722, 295, 264, 3991, 281, 264, 588, 917, 50564], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5869, "seek": 1763064, "start": 17634.64, "end": 17636.64, "text": " then it would want to do", "tokens": [50564, 550, 309, 576, 528, 281, 360, 50664], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5870, "seek": 1763064, "start": 17636.64, "end": 17638.64, "text": " is it would want to look past the end", "tokens": [50664, 307, 309, 576, 528, 281, 574, 1791, 264, 917, 50764], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5871, "seek": 1763064, "start": 17638.64, "end": 17640.64, "text": " because it would want to look at more tokens from that", "tokens": [50764, 570, 309, 576, 528, 281, 574, 412, 544, 22667, 490, 300, 50864], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5872, "seek": 1763064, "start": 17640.64, "end": 17642.64, "text": " and then we would just get errors", "tokens": [50864, 293, 550, 321, 576, 445, 483, 13603, 50964], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5873, "seek": 1763064, "start": 17642.64, "end": 17644.64, "text": " because you can't read more than", "tokens": [50964, 570, 291, 393, 380, 1401, 544, 813, 51064], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5874, "seek": 1763064, "start": 17644.64, "end": 17646.64, "text": " the file size if that makes sense", "tokens": [51064, 264, 3991, 2744, 498, 300, 1669, 2020, 51164], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5875, "seek": 1763064, "start": 17646.64, "end": 17648.64, "text": " so that's why I'm just making this little threshold here", "tokens": [51164, 370, 300, 311, 983, 286, 478, 445, 1455, 341, 707, 14678, 510, 51264], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5876, "seek": 1763064, "start": 17648.64, "end": 17650.64, "text": " and", "tokens": [51264, 293, 51364], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5877, "seek": 1763064, "start": 17650.64, "end": 17652.64, "text": " yeah so that's what that does", "tokens": [51364, 1338, 370, 300, 311, 437, 300, 775, 51464], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5878, "seek": 1763064, "start": 17652.64, "end": 17654.64, "text": " that's the starting position could be a random", "tokens": [51464, 300, 311, 264, 2891, 2535, 727, 312, 257, 4974, 51564], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5879, "seek": 1763064, "start": 17654.64, "end": 17656.64, "text": " number between the start and", "tokens": [51564, 1230, 1296, 264, 722, 293, 51664], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5880, "seek": 1763064, "start": 17656.64, "end": 17658.64, "text": " a little bit a little margin from the end", "tokens": [51664, 257, 707, 857, 257, 707, 10270, 490, 264, 917, 51764], "temperature": 0.0, "avg_logprob": -0.07013549949183609, "compression_ratio": 2.0253164556962027, "no_speech_prob": 0.0015976459253579378}, {"id": 5881, "seek": 1765864, "start": 17658.64, "end": 17660.64, "text": " here so", "tokens": [50364, 510, 370, 50464], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5882, "seek": 1765864, "start": 17660.64, "end": 17662.64, "text": " next up we have", "tokens": [50464, 958, 493, 321, 362, 50564], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5883, "seek": 1765864, "start": 17662.64, "end": 17664.64, "text": " this seek function so seek is going to", "tokens": [50564, 341, 8075, 2445, 370, 8075, 307, 516, 281, 50664], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5884, "seek": 1765864, "start": 17664.64, "end": 17666.64, "text": " go to the start position and then", "tokens": [50664, 352, 281, 264, 722, 2535, 293, 550, 50764], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5885, "seek": 1765864, "start": 17666.64, "end": 17668.64, "text": " block is going to", "tokens": [50764, 3461, 307, 516, 281, 50864], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5886, "seek": 1765864, "start": 17668.64, "end": 17670.64, "text": " read we're going to", "tokens": [50864, 1401, 321, 434, 516, 281, 50964], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5887, "seek": 1765864, "start": 17670.64, "end": 17672.64, "text": " go up to the start position it's going to", "tokens": [50964, 352, 493, 281, 264, 722, 2535, 309, 311, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5888, "seek": 1765864, "start": 17672.64, "end": 17674.64, "text": " seek up to there that's where it's going to start", "tokens": [51064, 8075, 493, 281, 456, 300, 311, 689, 309, 311, 516, 281, 722, 51164], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5889, "seek": 1765864, "start": 17674.64, "end": 17676.64, "text": " it's going to go up to it and then the read", "tokens": [51164, 309, 311, 516, 281, 352, 493, 281, 309, 293, 550, 264, 1401, 51264], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5890, "seek": 1765864, "start": 17676.64, "end": 17678.64, "text": " function is going to", "tokens": [51264, 2445, 307, 516, 281, 51364], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5891, "seek": 1765864, "start": 17678.64, "end": 17680.64, "text": " find a block of text that is", "tokens": [51364, 915, 257, 3461, 295, 2487, 300, 307, 51464], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5892, "seek": 1765864, "start": 17680.64, "end": 17682.64, "text": " block size times batch size so it's", "tokens": [51464, 3461, 2744, 1413, 15245, 2744, 370, 309, 311, 51564], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5893, "seek": 1765864, "start": 17682.64, "end": 17684.64, "text": " going to find a little snippet", "tokens": [51564, 516, 281, 915, 257, 707, 35623, 302, 51664], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5894, "seek": 1765864, "start": 17684.64, "end": 17686.64, "text": " of text in there at the starting", "tokens": [51664, 295, 2487, 294, 456, 412, 264, 2891, 51764], "temperature": 0.0, "avg_logprob": -0.07006383129930872, "compression_ratio": 2.320441988950276, "no_speech_prob": 0.005383898504078388}, {"id": 5895, "seek": 1768664, "start": 17686.64, "end": 17688.64, "text": " position and it's going to be of size", "tokens": [50364, 2535, 293, 309, 311, 516, 281, 312, 295, 2744, 50464], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5896, "seek": 1768664, "start": 17688.64, "end": 17690.64, "text": " it's going to have this the same amount of", "tokens": [50464, 309, 311, 516, 281, 362, 341, 264, 912, 2372, 295, 50564], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5897, "seek": 1768664, "start": 17690.64, "end": 17692.64, "text": " I guess bytes as", "tokens": [50564, 286, 2041, 36088, 382, 50664], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5898, "seek": 1768664, "start": 17692.64, "end": 17694.64, "text": " block size time times batch size", "tokens": [50664, 3461, 2744, 565, 1413, 15245, 2744, 50764], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5899, "seek": 1768664, "start": 17694.64, "end": 17696.64, "text": " then all that minus one", "tokens": [50764, 550, 439, 300, 3175, 472, 50864], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5900, "seek": 1768664, "start": 17696.64, "end": 17698.64, "text": " just so that it fits into this start position", "tokens": [50864, 445, 370, 300, 309, 9001, 666, 341, 722, 2535, 50964], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5901, "seek": 1768664, "start": 17698.64, "end": 17700.64, "text": " we don't get errors here that's why I put the minus one", "tokens": [50964, 321, 500, 380, 483, 13603, 510, 300, 311, 983, 286, 829, 264, 3175, 472, 51064], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5902, "seek": 1768664, "start": 17700.64, "end": 17702.64, "text": " but", "tokens": [51064, 457, 51164], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5903, "seek": 1768664, "start": 17702.64, "end": 17704.64, "text": " yeah so we'll get a pretty", "tokens": [51164, 1338, 370, 321, 603, 483, 257, 1238, 51264], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5904, "seek": 1768664, "start": 17704.64, "end": 17706.64, "text": " we'll get a pretty decent", "tokens": [51264, 321, 603, 483, 257, 1238, 8681, 51364], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5905, "seek": 1768664, "start": 17706.64, "end": 17708.64, "text": " text amount I guess you could say", "tokens": [51364, 2487, 2372, 286, 2041, 291, 727, 584, 51464], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5906, "seek": 1768664, "start": 17708.64, "end": 17710.64, "text": " it's going to be enough to work with you could", "tokens": [51464, 309, 311, 516, 281, 312, 1547, 281, 589, 365, 291, 727, 51564], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5907, "seek": 1768664, "start": 17710.64, "end": 17712.64, "text": " you could of course increases if you", "tokens": [51564, 291, 727, 295, 1164, 8637, 498, 291, 51664], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5908, "seek": 1768664, "start": 17712.64, "end": 17714.64, "text": " wanted to you could do like", "tokens": [51664, 1415, 281, 291, 727, 360, 411, 51764], "temperature": 0.0, "avg_logprob": -0.08803269357392282, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.00870965700596571}, {"id": 5909, "seek": 1771464, "start": 17714.64, "end": 17716.64, "text": " times eight if you wanted", "tokens": [50364, 1413, 3180, 498, 291, 1415, 50464], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5910, "seek": 1771464, "start": 17716.64, "end": 17718.64, "text": " times eight and then times eight up here but", "tokens": [50464, 1413, 3180, 293, 550, 1413, 3180, 493, 510, 457, 50564], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5911, "seek": 1771464, "start": 17718.64, "end": 17720.64, "text": " we're not going to do that", "tokens": [50564, 321, 434, 406, 516, 281, 360, 300, 50664], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5912, "seek": 1771464, "start": 17720.64, "end": 17722.64, "text": " based on my experience this is performed pretty well", "tokens": [50664, 2361, 322, 452, 1752, 341, 307, 10332, 1238, 731, 50764], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5913, "seek": 1771464, "start": 17722.64, "end": 17724.64, "text": " so we're going to stick with this method here", "tokens": [50764, 370, 321, 434, 516, 281, 2897, 365, 341, 3170, 510, 50864], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5914, "seek": 1771464, "start": 17724.64, "end": 17726.64, "text": " and then", "tokens": [50864, 293, 550, 50964], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5915, "seek": 1771464, "start": 17726.64, "end": 17728.64, "text": " we just decode this", "tokens": [50964, 321, 445, 979, 1429, 341, 51064], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5916, "seek": 1771464, "start": 17728.64, "end": 17730.64, "text": " bit of text the reason we decode it is it's", "tokens": [51064, 857, 295, 2487, 264, 1778, 321, 979, 1429, 309, 307, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5917, "seek": 1771464, "start": 17730.64, "end": 17732.64, "text": " it's because it's", "tokens": [51164, 309, 311, 570, 309, 311, 51264], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5918, "seek": 1771464, "start": 17732.64, "end": 17734.64, "text": " we read it in binary form", "tokens": [51264, 321, 1401, 309, 294, 17434, 1254, 51364], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5919, "seek": 1771464, "start": 17734.64, "end": 17736.64, "text": " so once we have this block of", "tokens": [51364, 370, 1564, 321, 362, 341, 3461, 295, 51464], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5920, "seek": 1771464, "start": 17736.64, "end": 17738.64, "text": " text we actually have to decode this to", "tokens": [51464, 2487, 321, 767, 362, 281, 979, 1429, 341, 281, 51564], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5921, "seek": 1771464, "start": 17738.64, "end": 17740.64, "text": " UFA format or UTF", "tokens": [51564, 624, 19684, 7877, 420, 624, 20527, 51664], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5922, "seek": 1771464, "start": 17740.64, "end": 17742.64, "text": " format and then any like", "tokens": [51664, 7877, 293, 550, 604, 411, 51764], "temperature": 0.0, "avg_logprob": -0.0864281766996609, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.021279850974678993}, {"id": 5923, "seek": 1774264, "start": 17742.64, "end": 17744.64, "text": " bytecode errors we get we're just going to ignore", "tokens": [50364, 40846, 22332, 13603, 321, 483, 321, 434, 445, 516, 281, 11200, 50464], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5924, "seek": 1774264, "start": 17744.64, "end": 17746.64, "text": " that this is something you learn", "tokens": [50464, 300, 341, 307, 746, 291, 1466, 50564], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5925, "seek": 1774264, "start": 17746.64, "end": 17748.64, "text": " through practice is when you start dealing", "tokens": [50564, 807, 3124, 307, 562, 291, 722, 6260, 50664], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5926, "seek": 1774264, "start": 17748.64, "end": 17750.64, "text": " with like really weird data or if it has", "tokens": [50664, 365, 411, 534, 3657, 1412, 420, 498, 309, 575, 50764], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5927, "seek": 1774264, "start": 17750.64, "end": 17752.64, "text": " like corruptions in it you'll get errors", "tokens": [50764, 411, 17366, 626, 294, 309, 291, 603, 483, 13603, 50864], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5928, "seek": 1774264, "start": 17752.64, "end": 17754.64, "text": " so all you want to do is all", "tokens": [50864, 370, 439, 291, 528, 281, 360, 307, 439, 50964], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5929, "seek": 1774264, "start": 17754.64, "end": 17756.64, "text": " this does is it pretty much says", "tokens": [50964, 341, 775, 307, 309, 1238, 709, 1619, 51064], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5930, "seek": 1774264, "start": 17756.64, "end": 17758.64, "text": " okay we're just going to ignore this", "tokens": [51064, 1392, 321, 434, 445, 516, 281, 11200, 341, 51164], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5931, "seek": 1774264, "start": 17758.64, "end": 17760.64, "text": " bit of text and we're just going to sample", "tokens": [51164, 857, 295, 2487, 293, 321, 434, 445, 516, 281, 6889, 51264], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5932, "seek": 1774264, "start": 17760.64, "end": 17762.64, "text": " everything around it and not include that", "tokens": [51264, 1203, 926, 309, 293, 406, 4090, 300, 51364], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5933, "seek": 1774264, "start": 17762.64, "end": 17764.64, "text": " part and plus since we're doing so many", "tokens": [51364, 644, 293, 1804, 1670, 321, 434, 884, 370, 867, 51464], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5934, "seek": 1774264, "start": 17764.64, "end": 17766.64, "text": " iterations it won't actually interfere", "tokens": [51464, 36540, 309, 1582, 380, 767, 23946, 51564], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5935, "seek": 1774264, "start": 17766.64, "end": 17768.64, "text": " that much so we should", "tokens": [51564, 300, 709, 370, 321, 820, 51664], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5936, "seek": 1774264, "start": 17768.64, "end": 17770.64, "text": " be all right and then for this replace", "tokens": [51664, 312, 439, 558, 293, 550, 337, 341, 7406, 51764], "temperature": 0.0, "avg_logprob": -0.0662571838923863, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.013219347223639488}, {"id": 5937, "seek": 1777064, "start": 17770.64, "end": 17772.64, "text": " go function here I was noticing", "tokens": [50364, 352, 2445, 510, 286, 390, 21814, 50464], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5938, "seek": 1777064, "start": 17772.64, "end": 17774.64, "text": " I got errors about this slash R", "tokens": [50464, 286, 658, 13603, 466, 341, 17330, 497, 50564], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5939, "seek": 1777064, "start": 17774.64, "end": 17776.64, "text": " so all this does is it just replaces that", "tokens": [50564, 370, 439, 341, 775, 307, 309, 445, 46734, 300, 50664], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5940, "seek": 1777064, "start": 17776.64, "end": 17778.64, "text": " with an empty string and then finally", "tokens": [50664, 365, 364, 6707, 6798, 293, 550, 2721, 50764], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5941, "seek": 1777064, "start": 17778.64, "end": 17780.64, "text": " we have all this", "tokens": [50764, 321, 362, 439, 341, 50864], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5942, "seek": 1777064, "start": 17780.64, "end": 17782.64, "text": " we have all this decoded data", "tokens": [50864, 321, 362, 439, 341, 979, 12340, 1412, 50964], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5943, "seek": 1777064, "start": 17782.64, "end": 17784.64, "text": " so all we're going to do is just encode", "tokens": [50964, 370, 439, 321, 434, 516, 281, 360, 307, 445, 2058, 1429, 51064], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5944, "seek": 1777064, "start": 17784.64, "end": 17786.64, "text": " this into the", "tokens": [51064, 341, 666, 264, 51164], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5945, "seek": 1777064, "start": 17786.64, "end": 17788.64, "text": " tokenized form so it's all in", "tokens": [51164, 14862, 1602, 1254, 370, 309, 311, 439, 294, 51264], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5946, "seek": 1777064, "start": 17788.64, "end": 17790.64, "text": " it's all in the tokenized form", "tokens": [51264, 309, 311, 439, 294, 264, 14862, 1602, 1254, 51364], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5947, "seek": 1777064, "start": 17790.64, "end": 17792.64, "text": " integers or torch.longs", "tokens": [51364, 41674, 420, 27822, 13, 13025, 82, 51464], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5948, "seek": 1777064, "start": 17792.64, "end": 17794.64, "text": " data type", "tokens": [51464, 1412, 2010, 51564], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5949, "seek": 1777064, "start": 17794.64, "end": 17796.64, "text": " and we just that's what our data is", "tokens": [51564, 293, 321, 445, 300, 311, 437, 527, 1412, 307, 51664], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5950, "seek": 1777064, "start": 17796.64, "end": 17798.64, "text": " instead of a bunch of characters it's just a bunch", "tokens": [51664, 2602, 295, 257, 3840, 295, 4342, 309, 311, 445, 257, 3840, 51764], "temperature": 0.0, "avg_logprob": -0.10389311611652374, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.005909755825996399}, {"id": 5951, "seek": 1779864, "start": 17798.64, "end": 17800.64, "text": " of numbers and then we", "tokens": [50364, 295, 3547, 293, 550, 321, 50464], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5952, "seek": 1779864, "start": 17800.64, "end": 17802.64, "text": " return that into our get batch", "tokens": [50464, 2736, 300, 666, 527, 483, 15245, 50564], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5953, "seek": 1779864, "start": 17802.64, "end": 17804.64, "text": " and this is what our data is", "tokens": [50564, 293, 341, 307, 437, 527, 1412, 307, 50664], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5954, "seek": 1779864, "start": 17804.64, "end": 17806.64, "text": " so that's pretty cool", "tokens": [50664, 370, 300, 311, 1238, 1627, 50764], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5955, "seek": 1779864, "start": 17806.64, "end": 17808.64, "text": " we can get either train or a valve", "tokens": [50764, 321, 393, 483, 2139, 3847, 420, 257, 15294, 50864], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5956, "seek": 1779864, "start": 17808.64, "end": 17810.64, "text": " split and", "tokens": [50864, 7472, 293, 50964], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5957, "seek": 1779864, "start": 17810.64, "end": 17812.64, "text": " that's sort of what it looks like in practice", "tokens": [50964, 300, 311, 1333, 295, 437, 309, 1542, 411, 294, 3124, 51064], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5958, "seek": 1779864, "start": 17812.64, "end": 17814.64, "text": " that's how we sample from", "tokens": [51064, 300, 311, 577, 321, 6889, 490, 51164], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5959, "seek": 1779864, "start": 17814.64, "end": 17816.64, "text": " very large text files at a smaller", "tokens": [51164, 588, 2416, 2487, 7098, 412, 257, 4356, 51264], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5960, "seek": 1779864, "start": 17816.64, "end": 17818.64, "text": " scale bit by bit so", "tokens": [51264, 4373, 857, 538, 857, 370, 51364], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5961, "seek": 1779864, "start": 17818.64, "end": 17820.64, "text": " let's go ahead and implement this here", "tokens": [51364, 718, 311, 352, 2286, 293, 4445, 341, 510, 51464], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5962, "seek": 1779864, "start": 17820.64, "end": 17822.64, "text": " and go grab this entire", "tokens": [51464, 293, 352, 4444, 341, 2302, 51564], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5963, "seek": 1779864, "start": 17822.64, "end": 17824.64, "text": " thing", "tokens": [51564, 551, 51664], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5964, "seek": 1779864, "start": 17824.64, "end": 17826.64, "text": " and pop over to here", "tokens": [51664, 293, 1665, 670, 281, 510, 51764], "temperature": 0.0, "avg_logprob": -0.09737852269952947, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0046809096820652485}, {"id": 5965, "seek": 1782664, "start": 17826.64, "end": 17828.64, "text": " we're just going to replace that", "tokens": [50364, 321, 434, 445, 516, 281, 7406, 300, 50464], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5966, "seek": 1782664, "start": 17828.64, "end": 17830.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5967, "seek": 1782664, "start": 17830.64, "end": 17832.64, "text": " get random chunk, get batch", "tokens": [50564, 483, 4974, 16635, 11, 483, 15245, 50664], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5968, "seek": 1782664, "start": 17832.64, "end": 17834.64, "text": " cool", "tokens": [50664, 1627, 50764], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5969, "seek": 1782664, "start": 17834.64, "end": 17836.64, "text": " so now we can actually go ahead and", "tokens": [50764, 370, 586, 321, 393, 767, 352, 2286, 293, 50864], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5970, "seek": 1782664, "start": 17836.64, "end": 17838.64, "text": " perhaps run this", "tokens": [50864, 4317, 1190, 341, 50964], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5971, "seek": 1782664, "start": 17838.64, "end": 17840.64, "text": " actually before we run this there's a little something we need to", "tokens": [50964, 767, 949, 321, 1190, 341, 456, 311, 257, 707, 746, 321, 643, 281, 51064], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5972, "seek": 1782664, "start": 17840.64, "end": 17842.64, "text": " add in here", "tokens": [51064, 909, 294, 510, 51164], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5973, "seek": 1782664, "start": 17842.64, "end": 17844.64, "text": " so I have this", "tokens": [51164, 370, 286, 362, 341, 51264], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5974, "seek": 1782664, "start": 17844.64, "end": 17846.64, "text": " train split.txt and a valve split.txt", "tokens": [51264, 3847, 7472, 13, 83, 734, 293, 257, 15294, 7472, 13, 83, 734, 51364], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5975, "seek": 1782664, "start": 17846.64, "end": 17848.64, "text": " so I actually need to", "tokens": [51364, 370, 286, 767, 643, 281, 51464], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5976, "seek": 1782664, "start": 17848.64, "end": 17850.64, "text": " change these", "tokens": [51464, 1319, 613, 51564], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5977, "seek": 1782664, "start": 17850.64, "end": 17852.64, "text": " so let's go rename we'll go", "tokens": [51564, 370, 718, 311, 352, 36741, 321, 603, 352, 51664], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5978, "seek": 1782664, "start": 17852.64, "end": 17854.64, "text": " train split.txt", "tokens": [51664, 3847, 7472, 13, 83, 734, 51764], "temperature": 0.0, "avg_logprob": -0.1345120006137424, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.007575487717986107}, {"id": 5979, "seek": 1785464, "start": 17854.64, "end": 17856.64, "text": " and then", "tokens": [50364, 293, 550, 50464], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5980, "seek": 1785464, "start": 17856.64, "end": 17858.64, "text": " a valve split.txt", "tokens": [50464, 257, 15294, 7472, 13, 83, 734, 50564], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5981, "seek": 1785464, "start": 17858.64, "end": 17860.64, "text": " cool", "tokens": [50564, 1627, 50664], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5982, "seek": 1785464, "start": 17860.64, "end": 17862.64, "text": " and then we could just go", "tokens": [50664, 293, 550, 321, 727, 445, 352, 50764], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5983, "seek": 1785464, "start": 17862.64, "end": 17864.64, "text": " open web text", "tokens": [50764, 1269, 3670, 2487, 50864], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5984, "seek": 1785464, "start": 17864.64, "end": 17866.64, "text": " forward slash", "tokens": [50864, 2128, 17330, 50964], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5985, "seek": 1785464, "start": 17866.64, "end": 17868.64, "text": " and then same thing for here", "tokens": [50964, 293, 550, 912, 551, 337, 510, 51064], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5986, "seek": 1785464, "start": 17868.64, "end": 17870.64, "text": " cool let's go ahead", "tokens": [51064, 1627, 718, 311, 352, 2286, 51164], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5987, "seek": 1785464, "start": 17870.64, "end": 17872.64, "text": " and run this now", "tokens": [51164, 293, 1190, 341, 586, 51264], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5988, "seek": 1785464, "start": 17876.64, "end": 17878.64, "text": " and we're getting errors", "tokens": [51464, 293, 321, 434, 1242, 13603, 51564], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5989, "seek": 1785464, "start": 17878.64, "end": 17880.64, "text": " mem map is not defined", "tokens": [51564, 1334, 4471, 307, 406, 7642, 51664], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5990, "seek": 1785464, "start": 17880.64, "end": 17882.64, "text": " so that's another thing we need to probably", "tokens": [51664, 370, 300, 311, 1071, 551, 321, 643, 281, 1391, 51764], "temperature": 0.0, "avg_logprob": -0.12648499011993408, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.004467316437512636}, {"id": 5991, "seek": 1788264, "start": 17882.64, "end": 17884.64, "text": " add in then", "tokens": [50364, 909, 294, 550, 50464], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5992, "seek": 1788264, "start": 17884.64, "end": 17886.64, "text": " so I'm actually just going to", "tokens": [50464, 370, 286, 478, 767, 445, 516, 281, 50564], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5993, "seek": 1788264, "start": 17886.64, "end": 17888.64, "text": " stop this process from running here", "tokens": [50564, 1590, 341, 1399, 490, 2614, 510, 50664], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5994, "seek": 1788264, "start": 17888.64, "end": 17890.64, "text": " we're going to go pip", "tokens": [50664, 321, 434, 516, 281, 352, 8489, 50764], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5995, "seek": 1788264, "start": 17890.64, "end": 17892.64, "text": " install", "tokens": [50764, 3625, 50864], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5996, "seek": 1788264, "start": 17892.64, "end": 17894.64, "text": " mem map", "tokens": [50864, 1334, 4471, 50964], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5997, "seek": 1788264, "start": 17896.64, "end": 17898.64, "text": " mem map is not defined", "tokens": [51064, 1334, 4471, 307, 406, 7642, 51164], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5998, "seek": 1788264, "start": 17898.64, "end": 17900.64, "text": " we don't actually need to install this", "tokens": [51164, 321, 500, 380, 767, 643, 281, 3625, 341, 51264], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 5999, "seek": 1788264, "start": 17900.64, "end": 17902.64, "text": " by default comes with the operating system", "tokens": [51264, 538, 7576, 1487, 365, 264, 7447, 1185, 51364], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 6000, "seek": 1788264, "start": 17902.64, "end": 17904.64, "text": " so", "tokens": [51364, 370, 51464], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 6001, "seek": 1788264, "start": 17904.64, "end": 17906.64, "text": " what we actually need to do", "tokens": [51464, 437, 321, 767, 643, 281, 360, 51564], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 6002, "seek": 1788264, "start": 17906.64, "end": 17908.64, "text": " is", "tokens": [51564, 307, 51664], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 6003, "seek": 1788264, "start": 17908.64, "end": 17910.64, "text": " just close this", "tokens": [51664, 445, 1998, 341, 51764], "temperature": 0.0, "avg_logprob": -0.08554439317612421, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004133409354835749}, {"id": 6004, "seek": 1791064, "start": 17910.64, "end": 17912.64, "text": " gptv1", "tokens": [50364, 290, 662, 85, 16, 50464], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6005, "seek": 1791064, "start": 17912.64, "end": 17914.64, "text": " awesome", "tokens": [50464, 3476, 50564], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6006, "seek": 1791064, "start": 17914.64, "end": 17916.64, "text": " everything is good", "tokens": [50564, 1203, 307, 665, 50664], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6007, "seek": 1791064, "start": 17916.64, "end": 17918.64, "text": " nothing is broken", "tokens": [50664, 1825, 307, 5463, 50764], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6008, "seek": 1791064, "start": 17918.64, "end": 17920.64, "text": " so what I actually need to do up here", "tokens": [50764, 370, 437, 286, 767, 643, 281, 360, 493, 510, 50864], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6009, "seek": 1791064, "start": 17920.64, "end": 17922.64, "text": " is import this", "tokens": [50864, 307, 974, 341, 50964], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6010, "seek": 1791064, "start": 17922.64, "end": 17924.64, "text": " so I need to go", "tokens": [50964, 370, 286, 643, 281, 352, 51064], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6011, "seek": 1791064, "start": 17924.64, "end": 17926.64, "text": " import mem map", "tokens": [51064, 974, 1334, 4471, 51164], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6012, "seek": 1791064, "start": 17926.64, "end": 17928.64, "text": " just like that", "tokens": [51164, 445, 411, 300, 51264], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6013, "seek": 1791064, "start": 17928.64, "end": 17930.64, "text": " and", "tokens": [51264, 293, 51364], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6014, "seek": 1791064, "start": 17930.64, "end": 17932.64, "text": " should be good to start running this script", "tokens": [51364, 820, 312, 665, 281, 722, 2614, 341, 5755, 51464], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6015, "seek": 1791064, "start": 17932.64, "end": 17934.64, "text": " name random is not defined", "tokens": [51464, 1315, 4974, 307, 406, 7642, 51564], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6016, "seek": 1791064, "start": 17934.64, "end": 17936.64, "text": " again another importation we have to make", "tokens": [51564, 797, 1071, 974, 399, 321, 362, 281, 652, 51664], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6017, "seek": 1791064, "start": 17936.64, "end": 17938.64, "text": " import", "tokens": [51664, 974, 51764], "temperature": 0.0, "avg_logprob": -0.1708340809263032, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.0026312542613595724}, {"id": 6018, "seek": 1793864, "start": 17938.64, "end": 17940.64, "text": " import", "tokens": [50364, 974, 50464], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6019, "seek": 1793864, "start": 17940.64, "end": 17942.64, "text": " random", "tokens": [50464, 4974, 50564], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6020, "seek": 1793864, "start": 17946.64, "end": 17948.64, "text": " and we should start seeing some", "tokens": [50764, 293, 321, 820, 722, 2577, 512, 50864], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6021, "seek": 1793864, "start": 17948.64, "end": 17950.64, "text": " progress going here so once we see the first iteration", "tokens": [50864, 4205, 516, 510, 370, 1564, 321, 536, 264, 700, 24784, 50964], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6022, "seek": 1793864, "start": 17950.64, "end": 17952.64, "text": " I'm going to stop it come back", "tokens": [50964, 286, 478, 516, 281, 1590, 309, 808, 646, 51064], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6023, "seek": 1793864, "start": 17952.64, "end": 17954.64, "text": " at the last iteration and", "tokens": [51064, 412, 264, 1036, 24784, 293, 51164], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6024, "seek": 1793864, "start": 17954.64, "end": 17956.64, "text": " then we'll start adding some little bits and pieces", "tokens": [51164, 550, 321, 603, 722, 5127, 512, 707, 9239, 293, 3755, 51264], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6025, "seek": 1793864, "start": 17956.64, "end": 17958.64, "text": " onto our script here to make it better", "tokens": [51264, 3911, 527, 5755, 510, 281, 652, 309, 1101, 51364], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6026, "seek": 1793864, "start": 17958.64, "end": 17960.64, "text": " so we're already about 600 iterations", "tokens": [51364, 370, 321, 434, 1217, 466, 11849, 36540, 51464], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6027, "seek": 1793864, "start": 17960.64, "end": 17962.64, "text": " in and you can see how the training loss", "tokens": [51464, 294, 293, 291, 393, 536, 577, 264, 3097, 4470, 51564], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6028, "seek": 1793864, "start": 17962.64, "end": 17964.64, "text": " is actually done really well so far", "tokens": [51564, 307, 767, 1096, 534, 731, 370, 1400, 51664], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6029, "seek": 1793864, "start": 17964.64, "end": 17966.64, "text": " it's gone from 10.5 drop all the way to", "tokens": [51664, 309, 311, 2780, 490, 1266, 13, 20, 3270, 439, 264, 636, 281, 51764], "temperature": 0.0, "avg_logprob": -0.08269514604048296, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.0027142439503222704}, {"id": 6030, "seek": 1796664, "start": 17966.64, "end": 17968.64, "text": " 2.38", "tokens": [50364, 568, 13, 12625, 50464], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6031, "seek": 1796664, "start": 17968.64, "end": 17970.64, "text": " and", "tokens": [50464, 293, 50564], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6032, "seek": 1796664, "start": 17970.64, "end": 17972.64, "text": " we can actually see that", "tokens": [50564, 321, 393, 767, 536, 300, 50664], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6033, "seek": 1796664, "start": 17972.64, "end": 17974.64, "text": " we might be able to actually get a", "tokens": [50664, 321, 1062, 312, 1075, 281, 767, 483, 257, 50764], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6034, "seek": 1796664, "start": 17974.64, "end": 17976.64, "text": " val loss that is lower than the", "tokens": [50764, 1323, 4470, 300, 307, 3126, 813, 264, 50864], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6035, "seek": 1796664, "start": 17976.64, "end": 17978.64, "text": " train because keep in mind", "tokens": [50864, 3847, 570, 1066, 294, 1575, 50964], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6036, "seek": 1796664, "start": 17978.64, "end": 17980.64, "text": " in train mode", "tokens": [50964, 294, 3847, 4391, 51064], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6037, "seek": 1796664, "start": 17980.64, "end": 17982.64, "text": " the dropout takes effect but in val", "tokens": [51064, 264, 3270, 346, 2516, 1802, 457, 294, 1323, 51164], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6038, "seek": 1796664, "start": 17982.64, "end": 17984.64, "text": " in eval mode", "tokens": [51164, 294, 1073, 304, 4391, 51264], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6039, "seek": 1796664, "start": 17984.64, "end": 17986.64, "text": " let me just scroll up to this here", "tokens": [51264, 718, 385, 445, 11369, 493, 281, 341, 510, 51364], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6040, "seek": 1796664, "start": 17986.64, "end": 17988.64, "text": " yes", "tokens": [51364, 2086, 51464], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6041, "seek": 1796664, "start": 17988.64, "end": 17990.64, "text": " so model about eval what this does", "tokens": [51464, 370, 2316, 466, 1073, 304, 437, 341, 775, 51564], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6042, "seek": 1796664, "start": 17990.64, "end": 17992.64, "text": " is it turns off the dropout", "tokens": [51564, 307, 309, 4523, 766, 264, 3270, 346, 51664], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6043, "seek": 1796664, "start": 17992.64, "end": 17994.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.10380963604859632, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.01941097341477871}, {"id": 6044, "seek": 1799464, "start": 17994.64, "end": 17996.64, "text": " we don't lose any of the neurons", "tokens": [50364, 321, 500, 380, 3624, 604, 295, 264, 22027, 50464], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6045, "seek": 1799464, "start": 17996.64, "end": 17998.64, "text": " and they're all sort of showing the same", "tokens": [50464, 293, 436, 434, 439, 1333, 295, 4099, 264, 912, 50564], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6046, "seek": 1799464, "start": 17998.64, "end": 18000.64, "text": " features and giving all the information that they're supposed", "tokens": [50564, 4122, 293, 2902, 439, 264, 1589, 300, 436, 434, 3442, 50664], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6047, "seek": 1799464, "start": 18000.64, "end": 18002.64, "text": " to because they're all active but in train mode", "tokens": [50664, 281, 570, 436, 434, 439, 4967, 457, 294, 3847, 4391, 50764], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6048, "seek": 1799464, "start": 18002.64, "end": 18004.64, "text": " 20% of them are off so", "tokens": [50764, 945, 4, 295, 552, 366, 766, 370, 50864], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6049, "seek": 1799464, "start": 18004.64, "end": 18006.64, "text": " once you actually see", "tokens": [50864, 1564, 291, 767, 536, 50964], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6050, "seek": 1799464, "start": 18006.64, "end": 18008.64, "text": " in eval mode it does better", "tokens": [50964, 294, 1073, 304, 4391, 309, 775, 1101, 51064], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6051, "seek": 1799464, "start": 18008.64, "end": 18010.64, "text": " that means", "tokens": [51064, 300, 1355, 51164], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6052, "seek": 1799464, "start": 18010.64, "end": 18012.64, "text": " that the network has started to", "tokens": [51164, 300, 264, 3209, 575, 1409, 281, 51264], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6053, "seek": 1799464, "start": 18012.64, "end": 18014.64, "text": " form a sense", "tokens": [51264, 1254, 257, 2020, 51364], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6054, "seek": 1799464, "start": 18014.64, "end": 18016.64, "text": " of completeness in its learning", "tokens": [51364, 295, 1557, 15264, 294, 1080, 2539, 51464], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6055, "seek": 1799464, "start": 18016.64, "end": 18018.64, "text": " so it's just adjusting things a little bit", "tokens": [51464, 370, 309, 311, 445, 23559, 721, 257, 707, 857, 51564], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6056, "seek": 1799464, "start": 18018.64, "end": 18020.64, "text": " once it hits that point", "tokens": [51564, 1564, 309, 8664, 300, 935, 51664], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6057, "seek": 1799464, "start": 18020.64, "end": 18022.64, "text": " and we might see this happen", "tokens": [51664, 293, 321, 1062, 536, 341, 1051, 51764], "temperature": 0.0, "avg_logprob": -0.0889927520126593, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.003763570450246334}, {"id": 6058, "seek": 1802264, "start": 18022.64, "end": 18024.64, "text": " momentarily but this is", "tokens": [50364, 1623, 3289, 457, 341, 307, 50464], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6059, "seek": 1802264, "start": 18024.64, "end": 18026.64, "text": " really good progress so far a loss of", "tokens": [50464, 534, 665, 4205, 370, 1400, 257, 4470, 295, 50564], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6060, "seek": 1802264, "start": 18026.64, "end": 18028.64, "text": " 1.8 is amazing", "tokens": [50564, 502, 13, 23, 307, 2243, 50664], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6061, "seek": 1802264, "start": 18028.64, "end": 18030.64, "text": " so", "tokens": [50664, 370, 50764], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6062, "seek": 1802264, "start": 18030.64, "end": 18032.64, "text": " in the meantime", "tokens": [50764, 294, 264, 14991, 50864], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6063, "seek": 1802264, "start": 18032.64, "end": 18034.64, "text": " I'm just going to add some little tweaks", "tokens": [50864, 286, 478, 445, 516, 281, 909, 512, 707, 46664, 50964], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6064, "seek": 1802264, "start": 18034.64, "end": 18036.64, "text": " here and there to improve this script", "tokens": [50964, 510, 293, 456, 281, 3470, 341, 5755, 51064], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6065, "seek": 1802264, "start": 18036.64, "end": 18038.64, "text": " so I've actually stopped the iteration process", "tokens": [51064, 370, 286, 600, 767, 5936, 264, 24784, 1399, 51164], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6066, "seek": 1802264, "start": 18038.64, "end": 18040.64, "text": " but we've gotten to 700 steps and we can already", "tokens": [51164, 457, 321, 600, 5768, 281, 15204, 4439, 293, 321, 393, 1217, 51264], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6067, "seek": 1802264, "start": 18040.64, "end": 18042.64, "text": " see that val loss", "tokens": [51264, 536, 300, 1323, 4470, 51364], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6068, "seek": 1802264, "start": 18042.64, "end": 18044.64, "text": " is becoming a less than train loss", "tokens": [51364, 307, 5617, 257, 1570, 813, 3847, 4470, 51464], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6069, "seek": 1802264, "start": 18044.64, "end": 18046.64, "text": " which is showing that the model is actually converging", "tokens": [51464, 597, 307, 4099, 300, 264, 2316, 307, 767, 9652, 3249, 51564], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6070, "seek": 1802264, "start": 18046.64, "end": 18048.64, "text": " and doing very well", "tokens": [51564, 293, 884, 588, 731, 51664], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6071, "seek": 1802264, "start": 18048.64, "end": 18050.64, "text": " so this architecture is amazing", "tokens": [51664, 370, 341, 9482, 307, 2243, 51764], "temperature": 0.0, "avg_logprob": -0.07692777193509616, "compression_ratio": 1.6796875, "no_speech_prob": 0.042681414633989334}, {"id": 6072, "seek": 1805064, "start": 18050.64, "end": 18052.64, "text": " we've pretty much covered", "tokens": [50364, 321, 600, 1238, 709, 5343, 50464], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6073, "seek": 1805064, "start": 18052.64, "end": 18054.64, "text": " every", "tokens": [50464, 633, 50564], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6074, "seek": 1805064, "start": 18054.64, "end": 18056.64, "text": " architectural, math, pie torch part", "tokens": [50564, 26621, 11, 5221, 11, 1730, 27822, 644, 50664], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6075, "seek": 1805064, "start": 18056.64, "end": 18058.64, "text": " that this script has to offer", "tokens": [50664, 300, 341, 5755, 575, 281, 2626, 50764], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6076, "seek": 1805064, "start": 18058.64, "end": 18060.64, "text": " the only thing I want to add", "tokens": [50764, 264, 787, 551, 286, 528, 281, 909, 50864], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6077, "seek": 1805064, "start": 18060.64, "end": 18062.64, "text": " actually a few things I want to add", "tokens": [50864, 767, 257, 1326, 721, 286, 528, 281, 909, 50964], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6078, "seek": 1805064, "start": 18062.64, "end": 18064.64, "text": " one of them being torch.load", "tokens": [50964, 472, 295, 552, 885, 27822, 13, 2907, 51064], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6079, "seek": 1805064, "start": 18064.64, "end": 18066.64, "text": " and torch.save", "tokens": [51064, 293, 27822, 13, 82, 946, 51164], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6080, "seek": 1805064, "start": 18066.64, "end": 18068.64, "text": " so one thing that's going to be really important", "tokens": [51164, 370, 472, 551, 300, 311, 516, 281, 312, 534, 1021, 51264], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6081, "seek": 1805064, "start": 18068.64, "end": 18070.64, "text": " when you start to scale up", "tokens": [51264, 562, 291, 722, 281, 4373, 493, 51364], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6082, "seek": 1805064, "start": 18070.64, "end": 18072.64, "text": " your iterations", "tokens": [51364, 428, 36540, 51464], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6083, "seek": 1805064, "start": 18072.64, "end": 18074.64, "text": " is you don't just want to run a script", "tokens": [51464, 307, 291, 500, 380, 445, 528, 281, 1190, 257, 5755, 51564], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6084, "seek": 1805064, "start": 18074.64, "end": 18076.64, "text": " that executes a training loop", "tokens": [51564, 300, 4454, 1819, 257, 3097, 6367, 51664], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6085, "seek": 1805064, "start": 18076.64, "end": 18078.64, "text": " with an architecture and", "tokens": [51664, 365, 364, 9482, 293, 51764], "temperature": 0.0, "avg_logprob": -0.07323183929711058, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.02296079508960247}, {"id": 6086, "seek": 1807864, "start": 18078.64, "end": 18080.64, "text": " that's it. You won't have some way to", "tokens": [50364, 300, 311, 309, 13, 509, 1582, 380, 362, 512, 636, 281, 50464], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6087, "seek": 1807864, "start": 18080.64, "end": 18082.64, "text": " store those learning parameters", "tokens": [50464, 3531, 729, 2539, 9834, 50564], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6088, "seek": 1807864, "start": 18082.64, "end": 18084.64, "text": " so that's what torch.load and torch.save does", "tokens": [50564, 370, 300, 311, 437, 27822, 13, 2907, 293, 27822, 13, 82, 946, 775, 50664], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6089, "seek": 1807864, "start": 18086.64, "end": 18088.64, "text": " save some file", "tokens": [50764, 3155, 512, 3991, 50864], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6090, "seek": 1807864, "start": 18088.64, "end": 18090.64, "text": " right and", "tokens": [50864, 558, 293, 50964], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6091, "seek": 1807864, "start": 18090.64, "end": 18092.64, "text": " you can pretty much", "tokens": [50964, 291, 393, 1238, 709, 51064], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6092, "seek": 1807864, "start": 18092.64, "end": 18094.64, "text": " you could put it into like a serialized", "tokens": [51064, 291, 727, 829, 309, 666, 411, 257, 17436, 1602, 51164], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6093, "seek": 1807864, "start": 18094.64, "end": 18096.64, "text": " format when you", "tokens": [51164, 7877, 562, 291, 51264], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6094, "seek": 1807864, "start": 18096.64, "end": 18098.64, "text": " save it you take your initial", "tokens": [51264, 3155, 309, 291, 747, 428, 5883, 51364], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6095, "seek": 1807864, "start": 18098.64, "end": 18100.64, "text": " architecture in our case it would actually", "tokens": [51364, 9482, 294, 527, 1389, 309, 576, 767, 51464], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6096, "seek": 1807864, "start": 18100.64, "end": 18102.64, "text": " be the GPT language model so you would", "tokens": [51464, 312, 264, 26039, 51, 2856, 2316, 370, 291, 576, 51564], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6097, "seek": 1807864, "start": 18102.64, "end": 18104.64, "text": " save this because it contains", "tokens": [51564, 3155, 341, 570, 309, 8306, 51664], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6098, "seek": 1807864, "start": 18104.64, "end": 18106.64, "text": " everything all these other classes", "tokens": [51664, 1203, 439, 613, 661, 5359, 51764], "temperature": 0.0, "avg_logprob": -0.10711263079162038, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.005726019851863384}, {"id": 6099, "seek": 1810664, "start": 18106.64, "end": 18108.64, "text": " as well they're all inside of GPT", "tokens": [50364, 382, 731, 436, 434, 439, 1854, 295, 26039, 51, 50464], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6100, "seek": 1810664, "start": 18108.64, "end": 18110.64, "text": " language model would save that architecture", "tokens": [50464, 2856, 2316, 576, 3155, 300, 9482, 50564], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6101, "seek": 1810664, "start": 18110.64, "end": 18112.64, "text": " and you essentially", "tokens": [50564, 293, 291, 4476, 50664], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6102, "seek": 1810664, "start": 18112.64, "end": 18114.64, "text": " serialize it into some pickled file", "tokens": [50664, 17436, 1125, 309, 666, 512, 38076, 3991, 50764], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6103, "seek": 1810664, "start": 18114.64, "end": 18116.64, "text": " that would have", "tokens": [50764, 300, 576, 362, 50864], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6104, "seek": 1810664, "start": 18116.64, "end": 18118.64, "text": " the file extension .pkl", "tokens": [50864, 264, 3991, 10320, 2411, 79, 7837, 50964], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6105, "seek": 1810664, "start": 18118.64, "end": 18120.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6106, "seek": 1810664, "start": 18120.64, "end": 18122.64, "text": " essentially", "tokens": [51064, 4476, 51164], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6107, "seek": 1810664, "start": 18122.64, "end": 18124.64, "text": " instead of using torch we're just going to use", "tokens": [51164, 2602, 295, 1228, 27822, 321, 434, 445, 516, 281, 764, 51264], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6108, "seek": 1810664, "start": 18124.64, "end": 18126.64, "text": " a library called pickle because", "tokens": [51264, 257, 6405, 1219, 31433, 570, 51364], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6109, "seek": 1810664, "start": 18126.64, "end": 18128.64, "text": " they're essentially the same thing", "tokens": [51364, 436, 434, 4476, 264, 912, 551, 51464], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6110, "seek": 1810664, "start": 18128.64, "end": 18130.64, "text": " pickle is a little bit easier", "tokens": [51464, 31433, 307, 257, 707, 857, 3571, 51564], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6111, "seek": 1810664, "start": 18130.64, "end": 18132.64, "text": " to use or at least a little bit easier to understand", "tokens": [51564, 281, 764, 420, 412, 1935, 257, 707, 857, 3571, 281, 1223, 51664], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6112, "seek": 1810664, "start": 18132.64, "end": 18134.64, "text": " there's less to it", "tokens": [51664, 456, 311, 1570, 281, 309, 51764], "temperature": 0.0, "avg_logprob": -0.10985857412355755, "compression_ratio": 1.8035714285714286, "no_speech_prob": 0.005728187970817089}, {"id": 6113, "seek": 1813464, "start": 18134.64, "end": 18136.64, "text": " pickle will only work", "tokens": [50364, 31433, 486, 787, 589, 50464], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6114, "seek": 1813464, "start": 18136.64, "end": 18138.64, "text": " on one GPU", "tokens": [50464, 322, 472, 18407, 50564], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6115, "seek": 1813464, "start": 18138.64, "end": 18140.64, "text": " so if you have like 8 GPUs at the same time", "tokens": [50564, 370, 498, 291, 362, 411, 1649, 18407, 82, 412, 264, 912, 565, 50664], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6116, "seek": 1813464, "start": 18140.64, "end": 18142.64, "text": " you're going to want to learn a little bit more", "tokens": [50664, 291, 434, 516, 281, 528, 281, 1466, 257, 707, 857, 544, 50764], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6117, "seek": 1813464, "start": 18142.64, "end": 18144.64, "text": " about hardware stuff and", "tokens": [50764, 466, 8837, 1507, 293, 50864], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6118, "seek": 1813464, "start": 18144.64, "end": 18146.64, "text": " some PyTorch docs but", "tokens": [50864, 512, 9953, 51, 284, 339, 45623, 457, 50964], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6119, "seek": 1813464, "start": 18146.64, "end": 18148.64, "text": " pretty much", "tokens": [50964, 1238, 709, 51064], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6120, "seek": 1813464, "start": 18148.64, "end": 18150.64, "text": " if we want to", "tokens": [51064, 498, 321, 528, 281, 51164], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6121, "seek": 1813464, "start": 18150.64, "end": 18152.64, "text": " save this after training", "tokens": [51164, 3155, 341, 934, 3097, 51264], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6122, "seek": 1813464, "start": 18152.64, "end": 18154.64, "text": " what we're going to do is we're going to use", "tokens": [51264, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 51364], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6123, "seek": 1813464, "start": 18154.64, "end": 18156.64, "text": " a little library called pickle and this comes", "tokens": [51364, 257, 707, 6405, 1219, 31433, 293, 341, 1487, 51464], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6124, "seek": 1813464, "start": 18156.64, "end": 18158.64, "text": " pre-installed with windows", "tokens": [51464, 659, 12, 13911, 8907, 365, 9309, 51564], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6125, "seek": 1813464, "start": 18160.64, "end": 18162.64, "text": " import pickle", "tokens": [51664, 974, 31433, 51764], "temperature": 0.0, "avg_logprob": -0.10867956642792603, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.010815284214913845}, {"id": 6126, "seek": 1816264, "start": 18162.64, "end": 18164.64, "text": " okay so what we want to do is", "tokens": [50364, 1392, 370, 437, 321, 528, 281, 360, 307, 50464], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6127, "seek": 1816264, "start": 18164.64, "end": 18166.64, "text": " implement this after the training loop", "tokens": [50464, 4445, 341, 934, 264, 3097, 6367, 50564], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6128, "seek": 1816264, "start": 18166.64, "end": 18168.64, "text": " after all these parameters have been updated", "tokens": [50564, 934, 439, 613, 9834, 362, 668, 10588, 50664], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6129, "seek": 1816264, "start": 18168.64, "end": 18170.64, "text": " and learned to the fullest extent", "tokens": [50664, 293, 3264, 281, 264, 45154, 8396, 50764], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6130, "seek": 1816264, "start": 18170.64, "end": 18172.64, "text": " so after this training loop", "tokens": [50764, 370, 934, 341, 3097, 6367, 50864], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6131, "seek": 1816264, "start": 18172.64, "end": 18174.64, "text": " we're simply going to open", "tokens": [50864, 321, 434, 2935, 516, 281, 1269, 50964], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6132, "seek": 1816264, "start": 18174.64, "end": 18176.64, "text": " we're going to do with open", "tokens": [50964, 321, 434, 516, 281, 360, 365, 1269, 51064], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6133, "seek": 1816264, "start": 18176.64, "end": 18178.64, "text": " and we could just go", "tokens": [51064, 293, 321, 727, 445, 352, 51164], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6134, "seek": 1816264, "start": 18178.64, "end": 18180.64, "text": " model 01 like that", "tokens": [51164, 2316, 23185, 411, 300, 51264], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6135, "seek": 1816264, "start": 18180.64, "end": 18182.64, "text": " and then", "tokens": [51264, 293, 550, 51364], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6136, "seek": 1816264, "start": 18182.64, "end": 18184.64, "text": " just that .pkl is the file extension", "tokens": [51364, 445, 300, 2411, 79, 7837, 307, 264, 3991, 10320, 51464], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6137, "seek": 1816264, "start": 18184.64, "end": 18186.64, "text": " for it", "tokens": [51464, 337, 309, 51564], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6138, "seek": 1816264, "start": 18186.64, "end": 18188.64, "text": " and then since we're writing to it we're going to go", "tokens": [51564, 293, 550, 1670, 321, 434, 3579, 281, 309, 321, 434, 516, 281, 352, 51664], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6139, "seek": 1816264, "start": 18188.64, "end": 18190.64, "text": " write binary", "tokens": [51664, 2464, 17434, 51764], "temperature": 0.0, "avg_logprob": -0.11800806862967354, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0025904548820108175}, {"id": 6140, "seek": 1819064, "start": 18190.64, "end": 18192.64, "text": " F", "tokens": [50364, 479, 50464], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6141, "seek": 1819064, "start": 18192.64, "end": 18194.64, "text": " and then in order to actually save this", "tokens": [50464, 293, 550, 294, 1668, 281, 767, 3155, 341, 50564], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6142, "seek": 1819064, "start": 18194.64, "end": 18196.64, "text": " we just go pickle.dump", "tokens": [50564, 321, 445, 352, 31433, 13, 67, 1420, 50664], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6143, "seek": 1819064, "start": 18196.64, "end": 18198.64, "text": " and then we can use", "tokens": [50664, 293, 550, 321, 393, 764, 50764], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6144, "seek": 1819064, "start": 18198.64, "end": 18200.64, "text": " model and then", "tokens": [50764, 2316, 293, 550, 50864], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6145, "seek": 1819064, "start": 18200.64, "end": 18202.64, "text": " just F like that", "tokens": [50864, 445, 479, 411, 300, 50964], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6146, "seek": 1819064, "start": 18202.64, "end": 18204.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6147, "seek": 1819064, "start": 18204.64, "end": 18206.64, "text": " if I start recording this", "tokens": [51064, 498, 286, 722, 6613, 341, 51164], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6148, "seek": 1819064, "start": 18206.64, "end": 18208.64, "text": " it's going to make", "tokens": [51164, 309, 311, 516, 281, 652, 51264], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6149, "seek": 1819064, "start": 18208.64, "end": 18210.64, "text": " if I start recording this training process", "tokens": [51264, 498, 286, 722, 6613, 341, 3097, 1399, 51364], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6150, "seek": 1819064, "start": 18210.64, "end": 18212.64, "text": " it's going to make my clip", "tokens": [51364, 309, 311, 516, 281, 652, 452, 7353, 51464], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6151, "seek": 1819064, "start": 18212.64, "end": 18214.64, "text": " like so", "tokens": [51464, 411, 370, 51564], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6152, "seek": 1819064, "start": 18214.64, "end": 18216.64, "text": " I'm going to come back to this after we've done", "tokens": [51564, 286, 478, 516, 281, 808, 646, 281, 341, 934, 321, 600, 1096, 51664], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6153, "seek": 1819064, "start": 18216.64, "end": 18218.64, "text": " let's just say about", "tokens": [51664, 718, 311, 445, 584, 466, 51764], "temperature": 0.0, "avg_logprob": -0.09738598617852903, "compression_ratio": 1.7976878612716762, "no_speech_prob": 0.01405703742057085}, {"id": 6154, "seek": 1821864, "start": 18218.64, "end": 18220.64, "text": " 100 iterations", "tokens": [50364, 2319, 36540, 50464], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6155, "seek": 1821864, "start": 18220.64, "end": 18222.64, "text": " we're going to do 100 editors", "tokens": [50464, 321, 434, 516, 281, 360, 2319, 31446, 50564], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6156, "seek": 1821864, "start": 18222.64, "end": 18224.64, "text": " and I'm going to come back and", "tokens": [50564, 293, 286, 478, 516, 281, 808, 646, 293, 50664], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6157, "seek": 1821864, "start": 18224.64, "end": 18226.64, "text": " show you guys", "tokens": [50664, 855, 291, 1074, 50764], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6158, "seek": 1821864, "start": 18226.64, "end": 18228.64, "text": " what the model file looks like", "tokens": [50764, 437, 264, 2316, 3991, 1542, 411, 50864], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6159, "seek": 1821864, "start": 18228.64, "end": 18230.64, "text": " what I actually did is I changed some of the model", "tokens": [50864, 437, 286, 767, 630, 307, 286, 3105, 512, 295, 264, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6160, "seek": 1821864, "start": 18230.64, "end": 18232.64, "text": " hyper parameters because", "tokens": [50964, 9848, 9834, 570, 51064], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6161, "seek": 1821864, "start": 18232.64, "end": 18234.64, "text": " it was taking way too long", "tokens": [51064, 309, 390, 1940, 636, 886, 938, 51164], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6162, "seek": 1821864, "start": 18234.64, "end": 18236.64, "text": " to perform what we wanted it to so I changed", "tokens": [51164, 281, 2042, 437, 321, 1415, 309, 281, 370, 286, 3105, 51264], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6163, "seek": 1821864, "start": 18236.64, "end": 18238.64, "text": " and head to one and layer to one", "tokens": [51264, 293, 1378, 281, 472, 293, 4583, 281, 472, 51364], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6164, "seek": 1821864, "start": 18238.64, "end": 18240.64, "text": " and I had half batch size", "tokens": [51364, 293, 286, 632, 1922, 15245, 2744, 51464], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6165, "seek": 1821864, "start": 18240.64, "end": 18242.64, "text": " all the way down from 64 to 32", "tokens": [51464, 439, 264, 636, 760, 490, 12145, 281, 8858, 51564], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6166, "seek": 1821864, "start": 18242.64, "end": 18244.64, "text": " so what I'm actually going to add here is just", "tokens": [51564, 370, 437, 286, 478, 767, 516, 281, 909, 510, 307, 445, 51664], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6167, "seek": 1821864, "start": 18244.64, "end": 18246.64, "text": " to make sure I like to print this out at the beginning", "tokens": [51664, 281, 652, 988, 286, 411, 281, 4482, 341, 484, 412, 264, 2863, 51764], "temperature": 0.0, "avg_logprob": -0.13459808407849028, "compression_ratio": 1.7490494296577948, "no_speech_prob": 0.03959529474377632}, {"id": 6168, "seek": 1824664, "start": 18246.64, "end": 18248.64, "text": " of this", "tokens": [50364, 295, 341, 50464], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6169, "seek": 1824664, "start": 18248.64, "end": 18250.64, "text": " make sure that the device is CUDA", "tokens": [50464, 652, 988, 300, 264, 4302, 307, 29777, 7509, 50564], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6170, "seek": 1824664, "start": 18250.64, "end": 18252.64, "text": " let's go back down", "tokens": [50564, 718, 311, 352, 646, 760, 50664], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6171, "seek": 1824664, "start": 18252.64, "end": 18254.64, "text": " so it did in fact train the model", "tokens": [50664, 370, 309, 630, 294, 1186, 3847, 264, 2316, 50764], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6172, "seek": 1824664, "start": 18254.64, "end": 18256.64, "text": " so we got all this done", "tokens": [50764, 370, 321, 658, 439, 341, 1096, 50864], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6173, "seek": 1824664, "start": 18256.64, "end": 18258.64, "text": " and yeah", "tokens": [50864, 293, 1338, 50964], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6174, "seek": 1824664, "start": 18258.64, "end": 18260.64, "text": " so I don't know why I did 2.54", "tokens": [50964, 370, 286, 500, 380, 458, 983, 286, 630, 568, 13, 19563, 51064], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6175, "seek": 1824664, "start": 18260.64, "end": 18262.64, "text": " whatever that", "tokens": [51064, 2035, 300, 51164], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6176, "seek": 1824664, "start": 18262.64, "end": 18264.64, "text": " that was just the entire loss", "tokens": [51164, 300, 390, 445, 264, 2302, 4470, 51264], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6177, "seek": 1824664, "start": 18264.64, "end": 18266.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6178, "seek": 1824664, "start": 18266.64, "end": 18268.64, "text": " model saved awesome", "tokens": [51364, 2316, 6624, 3476, 51464], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6179, "seek": 1824664, "start": 18268.64, "end": 18270.64, "text": " what does this actually look like here", "tokens": [51464, 437, 775, 341, 767, 574, 411, 510, 51564], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6180, "seek": 1824664, "start": 18270.64, "end": 18272.64, "text": " so this model.pkl", "tokens": [51564, 370, 341, 2316, 13, 79, 7837, 51664], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6181, "seek": 1824664, "start": 18272.64, "end": 18274.64, "text": " 106 megabytes isn't that wonderful", "tokens": [51664, 1266, 21, 10816, 24538, 1943, 380, 300, 3715, 51764], "temperature": 0.0, "avg_logprob": -0.12822538103376116, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.01384197361767292}, {"id": 6182, "seek": 1827464, "start": 18274.64, "end": 18276.64, "text": " so this is our model file this is what they look like", "tokens": [50364, 370, 341, 307, 527, 2316, 3991, 341, 307, 437, 436, 574, 411, 50464], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6183, "seek": 1827464, "start": 18276.64, "end": 18278.64, "text": " it's just a serialized", "tokens": [50464, 309, 311, 445, 257, 17436, 1602, 50564], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6184, "seek": 1827464, "start": 18278.64, "end": 18280.64, "text": " pretty much the entire architecture", "tokens": [50564, 1238, 709, 264, 2302, 9482, 50664], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6185, "seek": 1827464, "start": 18280.64, "end": 18282.64, "text": " all the parameters of the model the state", "tokens": [50664, 439, 264, 9834, 295, 264, 2316, 264, 1785, 50764], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6186, "seek": 1827464, "start": 18282.64, "end": 18284.64, "text": " everything that it contains", "tokens": [50764, 1203, 300, 309, 8306, 50864], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6187, "seek": 1827464, "start": 18284.64, "end": 18286.64, "text": " and we just compress that", "tokens": [50864, 293, 321, 445, 14778, 300, 50964], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6188, "seek": 1827464, "start": 18286.64, "end": 18288.64, "text": " into a little pkl file take that out", "tokens": [50964, 666, 257, 707, 280, 7837, 3991, 747, 300, 484, 51064], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6189, "seek": 1827464, "start": 18288.64, "end": 18290.64, "text": " decompress it and then just use it again", "tokens": [51064, 22867, 735, 309, 293, 550, 445, 764, 309, 797, 51164], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6190, "seek": 1827464, "start": 18290.64, "end": 18292.64, "text": " with all those same parameters so", "tokens": [51164, 365, 439, 729, 912, 9834, 370, 51264], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6191, "seek": 1827464, "start": 18292.64, "end": 18294.64, "text": " awesome", "tokens": [51264, 3476, 51364], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6192, "seek": 1827464, "start": 18294.64, "end": 18296.64, "text": " and all this really took was", "tokens": [51364, 293, 439, 341, 534, 1890, 390, 51464], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6193, "seek": 1827464, "start": 18296.64, "end": 18298.64, "text": " we just open", "tokens": [51464, 321, 445, 1269, 51564], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6194, "seek": 1827464, "start": 18298.64, "end": 18300.64, "text": " as this", "tokens": [51564, 382, 341, 51664], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6195, "seek": 1827464, "start": 18300.64, "end": 18302.64, "text": " we do a pickle.dump", "tokens": [51664, 321, 360, 257, 31433, 13, 67, 1420, 51764], "temperature": 0.0, "avg_logprob": -0.07668657218460488, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.0026724047493189573}, {"id": 6196, "seek": 1830264, "start": 18302.64, "end": 18304.64, "text": " to make sure that actually save I just like to add", "tokens": [50364, 281, 652, 988, 300, 767, 3155, 286, 445, 411, 281, 909, 50464], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6197, "seek": 1830264, "start": 18304.64, "end": 18306.64, "text": " a little print statement there cool", "tokens": [50464, 257, 707, 4482, 5629, 456, 1627, 50564], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6198, "seek": 1830264, "start": 18306.64, "end": 18308.64, "text": " so next", "tokens": [50564, 370, 958, 50664], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6199, "seek": 1830264, "start": 18308.64, "end": 18310.64, "text": " what I'd like to add is a little", "tokens": [50664, 437, 286, 1116, 411, 281, 909, 307, 257, 707, 50764], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6200, "seek": 1830264, "start": 18310.64, "end": 18312.64, "text": " wait for us to", "tokens": [50764, 1699, 337, 505, 281, 50864], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6201, "seek": 1830264, "start": 18312.64, "end": 18314.64, "text": " instead of just doing all of our training at once", "tokens": [50864, 2602, 295, 445, 884, 439, 295, 527, 3097, 412, 1564, 50964], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6202, "seek": 1830264, "start": 18314.64, "end": 18316.64, "text": " and then saving the model being able to", "tokens": [50964, 293, 550, 6816, 264, 2316, 885, 1075, 281, 51064], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6203, "seek": 1830264, "start": 18316.64, "end": 18318.64, "text": " train multiple times", "tokens": [51064, 3847, 3866, 1413, 51164], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6204, "seek": 1830264, "start": 18318.64, "end": 18320.64, "text": " so I'm gonna go up here", "tokens": [51164, 370, 286, 478, 799, 352, 493, 510, 51264], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6205, "seek": 1830264, "start": 18320.64, "end": 18322.64, "text": " to our", "tokens": [51264, 281, 527, 51364], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6206, "seek": 1830264, "start": 18322.64, "end": 18324.64, "text": " GPT language model here", "tokens": [51364, 26039, 51, 2856, 2316, 510, 51464], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6207, "seek": 1830264, "start": 18324.64, "end": 18326.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6208, "seek": 1830264, "start": 18326.64, "end": 18328.64, "text": " let's just see", "tokens": [51564, 718, 311, 445, 536, 51664], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6209, "seek": 1830264, "start": 18328.64, "end": 18330.64, "text": " what I'm gonna do", "tokens": [51664, 437, 286, 478, 799, 360, 51764], "temperature": 0.0, "avg_logprob": -0.09939755234762887, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.01614260859787464}, {"id": 6210, "seek": 1833064, "start": 18330.64, "end": 18332.64, "text": " with open", "tokens": [50364, 365, 1269, 50464], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6211, "seek": 1833064, "start": 18332.64, "end": 18334.64, "text": " and we're gonna go", "tokens": [50464, 293, 321, 434, 799, 352, 50564], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6212, "seek": 1833064, "start": 18334.64, "end": 18336.64, "text": " model 01", "tokens": [50564, 2316, 23185, 50664], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6213, "seek": 1833064, "start": 18336.64, "end": 18338.64, "text": " pkl", "tokens": [50664, 280, 7837, 50764], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6214, "seek": 1833064, "start": 18338.64, "end": 18340.64, "text": " and we're gonna go read binary", "tokens": [50764, 293, 321, 434, 799, 352, 1401, 17434, 50864], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6215, "seek": 1833064, "start": 18340.64, "end": 18342.64, "text": " so actually gonna read it we're gonna", "tokens": [50864, 370, 767, 799, 1401, 309, 321, 434, 799, 50964], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6216, "seek": 1833064, "start": 18342.64, "end": 18344.64, "text": " load this into", "tokens": [50964, 3677, 341, 666, 51064], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6217, "seek": 1833064, "start": 18344.64, "end": 18346.64, "text": " our script here", "tokens": [51064, 527, 5755, 510, 51164], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6218, "seek": 1833064, "start": 18346.64, "end": 18348.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6219, "seek": 1833064, "start": 18348.64, "end": 18350.64, "text": " we're gonna go as f", "tokens": [51264, 321, 434, 799, 352, 382, 283, 51364], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6220, "seek": 1833064, "start": 18350.64, "end": 18352.64, "text": " and then", "tokens": [51364, 293, 550, 51464], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6221, "seek": 1833064, "start": 18352.64, "end": 18354.64, "text": " I believe it's pickle.load", "tokens": [51464, 286, 1697, 309, 311, 31433, 13, 2907, 51564], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6222, "seek": 1833064, "start": 18356.64, "end": 18358.64, "text": " you just go yeah", "tokens": [51664, 291, 445, 352, 1338, 51764], "temperature": 0.0, "avg_logprob": -0.13093039989471436, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0414302721619606}, {"id": 6223, "seek": 1835864, "start": 18358.64, "end": 18360.64, "text": " model equals", "tokens": [50364, 2316, 6915, 50464], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6224, "seek": 1835864, "start": 18360.64, "end": 18362.64, "text": " pickle.load and then we'll just", "tokens": [50464, 31433, 13, 2907, 293, 550, 321, 603, 445, 50564], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6225, "seek": 1835864, "start": 18362.64, "end": 18364.64, "text": " essentially dump that", "tokens": [50564, 4476, 11430, 300, 50664], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6226, "seek": 1835864, "start": 18364.64, "end": 18366.64, "text": " right in there", "tokens": [50664, 558, 294, 456, 50764], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6227, "seek": 1835864, "start": 18366.64, "end": 18368.64, "text": " go print", "tokens": [50764, 352, 4482, 50864], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6228, "seek": 1835864, "start": 18368.64, "end": 18370.64, "text": " loading", "tokens": [50864, 15114, 50964], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6229, "seek": 1835864, "start": 18370.64, "end": 18372.64, "text": " model", "tokens": [50964, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6230, "seek": 1835864, "start": 18372.64, "end": 18374.64, "text": " parameters", "tokens": [51064, 9834, 51164], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6231, "seek": 1835864, "start": 18374.64, "end": 18376.64, "text": " dot dot dot", "tokens": [51164, 5893, 5893, 5893, 51264], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6232, "seek": 1835864, "start": 18376.64, "end": 18378.64, "text": " and then", "tokens": [51264, 293, 550, 51364], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6233, "seek": 1835864, "start": 18378.64, "end": 18380.64, "text": " just put f in there", "tokens": [51364, 445, 829, 283, 294, 456, 51464], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6234, "seek": 1835864, "start": 18380.64, "end": 18382.64, "text": " and then once it is loaded", "tokens": [51464, 293, 550, 1564, 309, 307, 13210, 51564], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6235, "seek": 1835864, "start": 18382.64, "end": 18384.64, "text": " we'll do print", "tokens": [51564, 321, 603, 360, 4482, 51664], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6236, "seek": 1835864, "start": 18384.64, "end": 18386.64, "text": " loaded", "tokens": [51664, 13210, 51764], "temperature": 0.0, "avg_logprob": -0.10153939988878039, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.00926516018807888}, {"id": 6237, "seek": 1838664, "start": 18386.64, "end": 18388.64, "text": " successfully", "tokens": [50364, 10727, 50464], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6238, "seek": 1838664, "start": 18388.64, "end": 18390.64, "text": " cool", "tokens": [50464, 1627, 50564], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6239, "seek": 1838664, "start": 18390.64, "end": 18392.64, "text": " so I'm actually gonna try this out now", "tokens": [50564, 370, 286, 478, 767, 799, 853, 341, 484, 586, 50664], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6240, "seek": 1838664, "start": 18392.64, "end": 18394.64, "text": " go", "tokens": [50664, 352, 50764], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6241, "seek": 1838664, "start": 18394.64, "end": 18396.64, "text": " do that", "tokens": [50764, 360, 300, 50864], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6242, "seek": 1838664, "start": 18396.64, "end": 18398.64, "text": " boom", "tokens": [50864, 9351, 50964], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6243, "seek": 1838664, "start": 18398.64, "end": 18400.64, "text": " and boom", "tokens": [50964, 293, 9351, 51064], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6244, "seek": 1838664, "start": 18400.64, "end": 18402.64, "text": " okay", "tokens": [51064, 1392, 51164], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6245, "seek": 1838664, "start": 18402.64, "end": 18404.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6246, "seek": 1838664, "start": 18404.64, "end": 18406.64, "text": " loading model parameters loaded successfully", "tokens": [51264, 15114, 2316, 9834, 13210, 10727, 51364], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6247, "seek": 1838664, "start": 18406.64, "end": 18408.64, "text": " and we'll actually see this", "tokens": [51364, 293, 321, 603, 767, 536, 341, 51464], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6248, "seek": 1838664, "start": 18408.64, "end": 18410.64, "text": " start to work on its own now", "tokens": [51464, 722, 281, 589, 322, 1080, 1065, 586, 51564], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6249, "seek": 1838664, "start": 18410.64, "end": 18412.64, "text": " so", "tokens": [51564, 370, 51664], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6250, "seek": 1838664, "start": 18412.64, "end": 18414.64, "text": " is it going to begin or is it not going to begin", "tokens": [51664, 307, 309, 516, 281, 1841, 420, 307, 309, 406, 516, 281, 1841, 51764], "temperature": 0.0, "avg_logprob": -0.07932966351509094, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005466704722493887}, {"id": 6251, "seek": 1841464, "start": 18414.64, "end": 18416.64, "text": " let's run that", "tokens": [50364, 718, 311, 1190, 300, 50464], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6252, "seek": 1841464, "start": 18416.64, "end": 18418.64, "text": " okay perfect", "tokens": [50464, 1392, 2176, 50564], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6253, "seek": 1841464, "start": 18418.64, "end": 18420.64, "text": " so now we should take the loss", "tokens": [50564, 370, 586, 321, 820, 747, 264, 4470, 50664], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6254, "seek": 1841464, "start": 18420.64, "end": 18422.64, "text": " that we had before which was about", "tokens": [50664, 300, 321, 632, 949, 597, 390, 466, 50764], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6255, "seek": 1841464, "start": 18422.64, "end": 18424.64, "text": " 2.54 I believe", "tokens": [50764, 568, 13, 19563, 286, 1697, 50864], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6256, "seek": 1841464, "start": 18424.64, "end": 18426.64, "text": " something around those, something along those lines", "tokens": [50864, 746, 926, 729, 11, 746, 2051, 729, 3876, 50964], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6257, "seek": 1841464, "start": 18426.64, "end": 18428.64, "text": " you can see that our training process", "tokens": [50964, 291, 393, 536, 300, 527, 3097, 1399, 51064], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6258, "seek": 1841464, "start": 18428.64, "end": 18430.64, "text": " is greatly accelerated", "tokens": [51064, 307, 14147, 29763, 51164], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6259, "seek": 1841464, "start": 18432.64, "end": 18434.64, "text": " so we had 100", "tokens": [51264, 370, 321, 632, 2319, 51364], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6260, "seek": 1841464, "start": 18434.64, "end": 18436.64, "text": " now it's just gonna do an estimate loss", "tokens": [51364, 586, 309, 311, 445, 799, 360, 364, 12539, 4470, 51464], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6261, "seek": 1841464, "start": 18436.64, "end": 18438.64, "text": " cool", "tokens": [51464, 1627, 51564], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6262, "seek": 1841464, "start": 18440.64, "end": 18442.64, "text": " and we're almost done", "tokens": [51664, 293, 321, 434, 1920, 1096, 51764], "temperature": 0.0, "avg_logprob": -0.09754060328691855, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00235922122374177}, {"id": 6263, "seek": 1844464, "start": 18445.64, "end": 18447.64, "text": " 1.96 awesome", "tokens": [50414, 502, 13, 22962, 3476, 50514], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6264, "seek": 1844464, "start": 18447.64, "end": 18449.64, "text": " and the model saved", "tokens": [50514, 293, 264, 2316, 6624, 50614], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6265, "seek": 1844464, "start": 18449.64, "end": 18451.64, "text": " so essentially what we can do with this", "tokens": [50614, 370, 4476, 437, 321, 393, 360, 365, 341, 50714], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6266, "seek": 1844464, "start": 18451.64, "end": 18453.64, "text": " is we can now", "tokens": [50714, 307, 321, 393, 586, 50814], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6267, "seek": 1844464, "start": 18453.64, "end": 18455.64, "text": " save models", "tokens": [50814, 3155, 5245, 50914], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6268, "seek": 1844464, "start": 18455.64, "end": 18457.64, "text": " and then we can load them and then iterate further", "tokens": [50914, 293, 550, 321, 393, 3677, 552, 293, 550, 44497, 3052, 51014], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6269, "seek": 1844464, "start": 18457.64, "end": 18459.64, "text": " so if you wanted to", "tokens": [51014, 370, 498, 291, 1415, 281, 51114], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6270, "seek": 1844464, "start": 18459.64, "end": 18461.64, "text": " you could create a super cool", "tokens": [51114, 291, 727, 1884, 257, 1687, 1627, 51214], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6271, "seek": 1844464, "start": 18461.64, "end": 18463.64, "text": " GPT language model", "tokens": [51214, 26039, 51, 2856, 2316, 51314], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6272, "seek": 1844464, "start": 18463.64, "end": 18465.64, "text": " script here and", "tokens": [51314, 5755, 510, 293, 51414], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6273, "seek": 1844464, "start": 18465.64, "end": 18467.64, "text": " you could essentially give it like 10,000 or 20,000", "tokens": [51414, 291, 727, 4476, 976, 309, 411, 1266, 11, 1360, 420, 945, 11, 1360, 51514], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6274, "seek": 1844464, "start": 18467.64, "end": 18469.64, "text": " iterations to run overnight", "tokens": [51514, 36540, 281, 1190, 13935, 51614], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6275, "seek": 1844464, "start": 18469.64, "end": 18471.64, "text": " you'd be able to save it", "tokens": [51614, 291, 1116, 312, 1075, 281, 3155, 309, 51714], "temperature": 0.0, "avg_logprob": -0.06600694562874589, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006795044057071209}, {"id": 6276, "seek": 1847164, "start": 18471.64, "end": 18473.64, "text": " and then import that into say a chat bot", "tokens": [50364, 293, 550, 974, 300, 666, 584, 257, 5081, 10592, 50464], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6277, "seek": 1847164, "start": 18473.64, "end": 18475.64, "text": " if you want", "tokens": [50464, 498, 291, 528, 50564], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6278, "seek": 1847164, "start": 18475.64, "end": 18477.64, "text": " so that's pretty cool and that's just kind of", "tokens": [50564, 370, 300, 311, 1238, 1627, 293, 300, 311, 445, 733, 295, 50664], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6279, "seek": 1847164, "start": 18477.64, "end": 18479.64, "text": " a good thing", "tokens": [50664, 257, 665, 551, 50764], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6280, "seek": 1847164, "start": 18479.64, "end": 18481.64, "text": " good little, it's kind of", "tokens": [50764, 665, 707, 11, 309, 311, 733, 295, 50864], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6281, "seek": 1847164, "start": 18481.64, "end": 18483.64, "text": " essential for language modeling because", "tokens": [50864, 7115, 337, 2856, 15983, 570, 50964], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6282, "seek": 1847164, "start": 18483.64, "end": 18485.64, "text": " what's the point", "tokens": [50964, 437, 311, 264, 935, 51064], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6283, "seek": 1847164, "start": 18485.64, "end": 18487.64, "text": " in having a machine learning model if you can't", "tokens": [51064, 294, 1419, 257, 3479, 2539, 2316, 498, 291, 393, 380, 51164], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6284, "seek": 1847164, "start": 18487.64, "end": 18489.64, "text": " actually use it and deploy it", "tokens": [51164, 767, 764, 309, 293, 7274, 309, 51264], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6285, "seek": 1847164, "start": 18489.64, "end": 18491.64, "text": " so you need to save for this stuff to work", "tokens": [51264, 370, 291, 643, 281, 3155, 337, 341, 1507, 281, 589, 51364], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6286, "seek": 1847164, "start": 18491.64, "end": 18493.64, "text": " alright", "tokens": [51364, 5845, 51464], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6287, "seek": 1847164, "start": 18493.64, "end": 18495.64, "text": " now let's move on to", "tokens": [51464, 586, 718, 311, 1286, 322, 281, 51564], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6288, "seek": 1847164, "start": 18495.64, "end": 18497.64, "text": " a little something in this task manager", "tokens": [51564, 257, 707, 746, 294, 341, 5633, 6598, 51664], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6289, "seek": 1847164, "start": 18497.64, "end": 18499.64, "text": " here which I'd like to go over", "tokens": [51664, 510, 597, 286, 1116, 411, 281, 352, 670, 51764], "temperature": 0.0, "avg_logprob": -0.0918461799621582, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.00898056011646986}, {"id": 6290, "seek": 1849964, "start": 18499.64, "end": 18501.64, "text": " so this shared GPU memory here", "tokens": [50364, 370, 341, 5507, 18407, 4675, 510, 50464], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6291, "seek": 1849964, "start": 18501.64, "end": 18503.64, "text": " and this dedicated GPU memory", "tokens": [50464, 293, 341, 8374, 18407, 4675, 50564], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6292, "seek": 1849964, "start": 18503.64, "end": 18505.64, "text": " so dedicated", "tokens": [50564, 370, 8374, 50664], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6293, "seek": 1849964, "start": 18505.64, "end": 18507.64, "text": " means how much", "tokens": [50664, 1355, 577, 709, 50764], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6294, "seek": 1849964, "start": 18507.64, "end": 18509.64, "text": " VRAM, video RAM", "tokens": [50764, 13722, 2865, 11, 960, 14561, 50864], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6295, "seek": 1849964, "start": 18509.64, "end": 18511.64, "text": " does your GPU actually have", "tokens": [50864, 775, 428, 18407, 767, 362, 50964], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6296, "seek": 1849964, "start": 18511.64, "end": 18513.64, "text": " on the card", "tokens": [50964, 322, 264, 2920, 51064], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6297, "seek": 1849964, "start": 18513.64, "end": 18515.64, "text": " so on the card it's going to be very quick memory", "tokens": [51064, 370, 322, 264, 2920, 309, 311, 516, 281, 312, 588, 1702, 4675, 51164], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6298, "seek": 1849964, "start": 18515.64, "end": 18517.64, "text": " because it doesn't have to", "tokens": [51164, 570, 309, 1177, 380, 362, 281, 51264], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6299, "seek": 1849964, "start": 18517.64, "end": 18519.64, "text": " the electrons don't have to travel as quickly", "tokens": [51264, 264, 14265, 500, 380, 362, 281, 3147, 382, 2661, 51364], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6300, "seek": 1849964, "start": 18519.64, "end": 18521.64, "text": " that's kind of the logic of it", "tokens": [51364, 300, 311, 733, 295, 264, 9952, 295, 309, 51464], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6301, "seek": 1849964, "start": 18521.64, "end": 18523.64, "text": " the electrons don't have to travel", "tokens": [51464, 264, 14265, 500, 380, 362, 281, 3147, 51564], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6302, "seek": 1849964, "start": 18523.64, "end": 18525.64, "text": " they don't have to travel as far", "tokens": [51564, 436, 500, 380, 362, 281, 3147, 382, 1400, 51664], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6303, "seek": 1849964, "start": 18525.64, "end": 18527.64, "text": " because", "tokens": [51664, 570, 51764], "temperature": 0.0, "avg_logprob": -0.09129395051435991, "compression_ratio": 1.917948717948718, "no_speech_prob": 0.006190300453454256}, {"id": 6304, "seek": 1852764, "start": 18527.64, "end": 18529.64, "text": " the little RAM chip is right there", "tokens": [50364, 264, 707, 14561, 11409, 307, 558, 456, 50464], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6305, "seek": 1852764, "start": 18529.64, "end": 18531.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6306, "seek": 1852764, "start": 18531.64, "end": 18533.64, "text": " dedicated GPU memory is a lot faster", "tokens": [50564, 8374, 18407, 4675, 307, 257, 688, 4663, 50664], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6307, "seek": 1852764, "start": 18533.64, "end": 18535.64, "text": " shared GPU memory", "tokens": [50664, 5507, 18407, 4675, 50764], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6308, "seek": 1852764, "start": 18535.64, "end": 18537.64, "text": " is essentially if this gets overloaded", "tokens": [50764, 307, 4476, 498, 341, 2170, 28777, 292, 50864], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6309, "seek": 1852764, "start": 18537.64, "end": 18539.64, "text": " it'll use some of the RAM on your", "tokens": [50864, 309, 603, 764, 512, 295, 264, 14561, 322, 428, 50964], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6310, "seek": 1852764, "start": 18539.64, "end": 18541.64, "text": " computer instead", "tokens": [50964, 3820, 2602, 51064], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6311, "seek": 1852764, "start": 18541.64, "end": 18543.64, "text": " so this will typically be about half of your", "tokens": [51064, 370, 341, 486, 5850, 312, 466, 1922, 295, 428, 51164], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6312, "seek": 1852764, "start": 18543.64, "end": 18545.64, "text": " computer's RAM", "tokens": [51164, 3820, 311, 14561, 51264], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6313, "seek": 1852764, "start": 18545.64, "end": 18547.64, "text": " I have 32 gigabytes of RAM on my computer", "tokens": [51264, 286, 362, 8858, 42741, 295, 14561, 322, 452, 3820, 51364], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6314, "seek": 1852764, "start": 18547.64, "end": 18549.64, "text": " so 16.0 makes sense", "tokens": [51364, 370, 3165, 13, 15, 1669, 2020, 51464], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6315, "seek": 1852764, "start": 18549.64, "end": 18551.64, "text": " half 32", "tokens": [51464, 1922, 8858, 51564], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6316, "seek": 1852764, "start": 18551.64, "end": 18553.64, "text": " and yeah", "tokens": [51564, 293, 1338, 51664], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6317, "seek": 1852764, "start": 18553.64, "end": 18555.64, "text": " so you want to make sure you're only using dedicated", "tokens": [51664, 370, 291, 528, 281, 652, 988, 291, 434, 787, 1228, 8374, 51764], "temperature": 0.0, "avg_logprob": -0.05676607290903727, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.005058590788394213}, {"id": 6318, "seek": 1855564, "start": 18555.64, "end": 18557.64, "text": " GPU memory", "tokens": [50364, 18407, 4675, 50464], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6319, "seek": 1855564, "start": 18557.64, "end": 18559.64, "text": " having your shared GPU memory go up", "tokens": [50464, 1419, 428, 5507, 18407, 4675, 352, 493, 50564], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6320, "seek": 1855564, "start": 18559.64, "end": 18561.64, "text": " is not usually a good thing", "tokens": [50564, 307, 406, 2673, 257, 665, 551, 50664], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6321, "seek": 1855564, "start": 18561.64, "end": 18563.64, "text": " a little bit is fine", "tokens": [50664, 257, 707, 857, 307, 2489, 50764], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6322, "seek": 1855564, "start": 18563.64, "end": 18565.64, "text": " but", "tokens": [50764, 457, 50864], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6323, "seek": 1855564, "start": 18565.64, "end": 18567.64, "text": " dedicated GPU memory is the fastest", "tokens": [50864, 8374, 18407, 4675, 307, 264, 14573, 50964], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6324, "seek": 1855564, "start": 18567.64, "end": 18569.64, "text": " and you want everything to stick on there", "tokens": [50964, 293, 291, 528, 1203, 281, 2897, 322, 456, 51064], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6325, "seek": 1855564, "start": 18569.64, "end": 18571.64, "text": " just try to make sure all of your parameters", "tokens": [51064, 445, 853, 281, 652, 988, 439, 295, 428, 9834, 51164], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6326, "seek": 1855564, "start": 18571.64, "end": 18573.64, "text": " sort of fit around this", "tokens": [51164, 1333, 295, 3318, 926, 341, 51264], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6327, "seek": 1855564, "start": 18573.64, "end": 18575.64, "text": " whatever your max capacity is", "tokens": [51264, 2035, 428, 11469, 6042, 307, 51364], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6328, "seek": 1855564, "start": 18575.64, "end": 18577.64, "text": " maybe it's 4, maybe it's 8", "tokens": [51364, 1310, 309, 311, 1017, 11, 1310, 309, 311, 1649, 51464], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6329, "seek": 1855564, "start": 18577.64, "end": 18579.64, "text": " maybe it's 48", "tokens": [51464, 1310, 309, 311, 11174, 51564], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6330, "seek": 1855564, "start": 18579.64, "end": 18581.64, "text": " who knows", "tokens": [51564, 567, 3255, 51664], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6331, "seek": 1855564, "start": 18581.64, "end": 18583.64, "text": " and a good way to figure out", "tokens": [51664, 293, 257, 665, 636, 281, 2573, 484, 51764], "temperature": 0.0, "avg_logprob": -0.04507527711256495, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.03406478837132454}, {"id": 6332, "seek": 1858364, "start": 18583.64, "end": 18585.64, "text": " what you can use on your GPU", "tokens": [50364, 437, 291, 393, 764, 322, 428, 18407, 50464], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6333, "seek": 1858364, "start": 18585.64, "end": 18587.64, "text": " without it getting memory errors", "tokens": [50464, 1553, 309, 1242, 4675, 13603, 50564], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6334, "seek": 1858364, "start": 18587.64, "end": 18589.64, "text": " or using shared memory", "tokens": [50564, 420, 1228, 5507, 4675, 50664], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6335, "seek": 1858364, "start": 18589.64, "end": 18591.64, "text": " is to actually play around", "tokens": [50664, 307, 281, 767, 862, 926, 50764], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6336, "seek": 1858364, "start": 18591.64, "end": 18593.64, "text": " with", "tokens": [50764, 365, 50864], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6337, "seek": 1858364, "start": 18593.64, "end": 18595.64, "text": " these parameters up here", "tokens": [50864, 613, 9834, 493, 510, 50964], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6338, "seek": 1858364, "start": 18595.64, "end": 18597.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6339, "seek": 1858364, "start": 18597.64, "end": 18599.64, "text": " block size and batch size", "tokens": [51064, 3461, 2744, 293, 15245, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6340, "seek": 1858364, "start": 18599.64, "end": 18601.64, "text": " actually let me switch those around", "tokens": [51164, 767, 718, 385, 3679, 729, 926, 51264], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6341, "seek": 1858364, "start": 18601.64, "end": 18603.64, "text": " these are not supposed to be in that order", "tokens": [51264, 613, 366, 406, 3442, 281, 312, 294, 300, 1668, 51364], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6342, "seek": 1858364, "start": 18603.64, "end": 18605.64, "text": " but", "tokens": [51364, 457, 51464], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6343, "seek": 1858364, "start": 18605.64, "end": 18607.64, "text": " all good", "tokens": [51464, 439, 665, 51564], "temperature": 0.0, "avg_logprob": -0.10370221263483952, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00505923992022872}, {"id": 6344, "seek": 1860764, "start": 18607.64, "end": 18609.64, "text": " we'll make our batch size", "tokens": [50364, 321, 603, 652, 527, 15245, 2744, 50464], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6345, "seek": 1860764, "start": 18609.64, "end": 18611.64, "text": " 64", "tokens": [50464, 12145, 50564], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6346, "seek": 1860764, "start": 18611.64, "end": 18613.64, "text": " that's 128", "tokens": [50564, 300, 311, 29810, 50664], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6347, "seek": 1860764, "start": 18613.64, "end": 18615.64, "text": " okay", "tokens": [50664, 1392, 50764], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6348, "seek": 1860764, "start": 18615.64, "end": 18617.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6349, "seek": 1860764, "start": 18617.64, "end": 18619.64, "text": " batch size and block size", "tokens": [50864, 15245, 2744, 293, 3461, 2744, 50964], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6350, "seek": 1860764, "start": 18619.64, "end": 18621.64, "text": " are very big contributors to how much memory you're going to use", "tokens": [50964, 366, 588, 955, 45627, 281, 577, 709, 4675, 291, 434, 516, 281, 764, 51064], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6351, "seek": 1860764, "start": 18621.64, "end": 18623.64, "text": " learning rate is not", "tokens": [51064, 2539, 3314, 307, 406, 51164], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6352, "seek": 1860764, "start": 18623.64, "end": 18625.64, "text": " max iterations is not", "tokens": [51164, 11469, 36540, 307, 406, 51264], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6353, "seek": 1860764, "start": 18625.64, "end": 18627.64, "text": " evaluators is not", "tokens": [51264, 6133, 3391, 307, 406, 51364], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6354, "seek": 1860764, "start": 18627.64, "end": 18629.64, "text": " but these three will", "tokens": [51364, 457, 613, 1045, 486, 51464], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6355, "seek": 1860764, "start": 18629.64, "end": 18631.64, "text": " the amount of features that you store", "tokens": [51464, 264, 2372, 295, 4122, 300, 291, 3531, 51564], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6356, "seek": 1860764, "start": 18631.64, "end": 18633.64, "text": " the amount of heads you have running in parallel", "tokens": [51564, 264, 2372, 295, 8050, 291, 362, 2614, 294, 8952, 51664], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6357, "seek": 1860764, "start": 18633.64, "end": 18635.64, "text": " and then also", "tokens": [51664, 293, 550, 611, 51764], "temperature": 0.0, "avg_logprob": -0.11845833627801193, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.004828763660043478}, {"id": 6358, "seek": 1863564, "start": 18635.64, "end": 18637.64, "text": " layers so", "tokens": [50364, 7914, 370, 50464], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6359, "seek": 1863564, "start": 18637.64, "end": 18639.64, "text": " some of these will not", "tokens": [50464, 512, 295, 613, 486, 406, 50564], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6360, "seek": 1863564, "start": 18639.64, "end": 18641.64, "text": " affect you as much because they're more", "tokens": [50564, 3345, 291, 382, 709, 570, 436, 434, 544, 50664], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6361, "seek": 1863564, "start": 18641.64, "end": 18643.64, "text": " sort of restrained to computation", "tokens": [50664, 1333, 295, 25508, 2001, 281, 24903, 50764], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6362, "seek": 1863564, "start": 18643.64, "end": 18645.64, "text": " how quickly you can do operations if something is sequential", "tokens": [50764, 577, 2661, 291, 393, 360, 7705, 498, 746, 307, 42881, 50864], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6363, "seek": 1863564, "start": 18647.64, "end": 18649.64, "text": " so N layer won't strain you", "tokens": [50964, 370, 426, 4583, 1582, 380, 14249, 291, 51064], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6364, "seek": 1863564, "start": 18649.64, "end": 18651.64, "text": " as much as something like batch and block size", "tokens": [51064, 382, 709, 382, 746, 411, 15245, 293, 3461, 2744, 51164], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6365, "seek": 1863564, "start": 18651.64, "end": 18653.64, "text": " but", "tokens": [51164, 457, 51264], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6366, "seek": 1863564, "start": 18653.64, "end": 18655.64, "text": " those are just good little things to", "tokens": [51264, 729, 366, 445, 665, 707, 721, 281, 51364], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6367, "seek": 1863564, "start": 18655.64, "end": 18657.64, "text": " sort of tweak and play around with", "tokens": [51364, 1333, 295, 29879, 293, 862, 926, 365, 51464], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6368, "seek": 1863564, "start": 18657.64, "end": 18659.64, "text": " so I found the optimal", "tokens": [51464, 370, 286, 1352, 264, 16252, 51564], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6369, "seek": 1863564, "start": 18659.64, "end": 18661.64, "text": " sort of set of", "tokens": [51564, 1333, 295, 992, 295, 51664], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6370, "seek": 1863564, "start": 18661.64, "end": 18663.64, "text": " hyper parameters for my PC", "tokens": [51664, 9848, 9834, 337, 452, 6465, 51764], "temperature": 0.0, "avg_logprob": -0.1137911998308622, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.03256448730826378}, {"id": 6371, "seek": 1866364, "start": 18663.64, "end": 18665.64, "text": " that happens to be", "tokens": [50364, 300, 2314, 281, 312, 50464], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6372, "seek": 1866364, "start": 18665.64, "end": 18667.64, "text": " 8, 8, 3, 8, 4", "tokens": [50464, 1649, 11, 1649, 11, 805, 11, 1649, 11, 1017, 50564], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6373, "seek": 1866364, "start": 18667.64, "end": 18669.64, "text": " learning rates is the same", "tokens": [50564, 2539, 6846, 307, 264, 912, 50664], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6374, "seek": 1866364, "start": 18669.64, "end": 18671.64, "text": " and then 64, 128 for this", "tokens": [50664, 293, 550, 12145, 11, 29810, 337, 341, 50764], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6375, "seek": 1866364, "start": 18671.64, "end": 18673.64, "text": " so that happened to be the optimal", "tokens": [50764, 370, 300, 2011, 281, 312, 264, 16252, 50864], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6376, "seek": 1866364, "start": 18673.64, "end": 18675.64, "text": " hyper parameters for my computer", "tokens": [50864, 9848, 9834, 337, 452, 3820, 50964], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6377, "seek": 1866364, "start": 18675.64, "end": 18677.64, "text": " it'll probably be different for yours", "tokens": [50964, 309, 603, 1391, 312, 819, 337, 6342, 51064], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6378, "seek": 1866364, "start": 18677.64, "end": 18679.64, "text": " if you don't have 8 gigabytes of RAM on your GPU", "tokens": [51064, 498, 291, 500, 380, 362, 1649, 42741, 295, 14561, 322, 428, 18407, 51164], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6379, "seek": 1866364, "start": 18681.64, "end": 18683.64, "text": " so anyways", "tokens": [51264, 370, 13448, 51364], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6380, "seek": 1866364, "start": 18683.64, "end": 18685.64, "text": " that's a little something you have to pay attention to", "tokens": [51364, 300, 311, 257, 707, 746, 291, 362, 281, 1689, 3202, 281, 51464], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6381, "seek": 1866364, "start": 18685.64, "end": 18687.64, "text": " to make sure you don't run out of errors", "tokens": [51464, 281, 652, 988, 291, 500, 380, 1190, 484, 295, 13603, 51564], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6382, "seek": 1866364, "start": 18687.64, "end": 18689.64, "text": " and a technique you can use", "tokens": [51564, 293, 257, 6532, 291, 393, 764, 51664], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6383, "seek": 1866364, "start": 18689.64, "end": 18691.64, "text": " which I'm not actually going to show you in this course", "tokens": [51664, 597, 286, 478, 406, 767, 516, 281, 855, 291, 294, 341, 1164, 51764], "temperature": 0.0, "avg_logprob": -0.11877371215820312, "compression_ratio": 1.5962962962962963, "no_speech_prob": 0.014948231168091297}, {"id": 6384, "seek": 1869164, "start": 18691.64, "end": 18693.64, "text": " but it's quite useful is something called auto tuning", "tokens": [50364, 457, 309, 311, 1596, 4420, 307, 746, 1219, 8399, 15164, 50464], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6385, "seek": 1869164, "start": 18693.64, "end": 18695.64, "text": " and what auto tuning does", "tokens": [50464, 293, 437, 8399, 15164, 775, 50564], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6386, "seek": 1869164, "start": 18695.64, "end": 18697.64, "text": " is it pretty much runs", "tokens": [50564, 307, 309, 1238, 709, 6676, 50664], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6387, "seek": 1869164, "start": 18697.64, "end": 18699.64, "text": " a bunch of these", "tokens": [50664, 257, 3840, 295, 613, 50764], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6388, "seek": 1869164, "start": 18699.64, "end": 18701.64, "text": " a bunch of models with different", "tokens": [50764, 257, 3840, 295, 5245, 365, 819, 50864], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6389, "seek": 1869164, "start": 18701.64, "end": 18703.64, "text": " sets of hyper parameters", "tokens": [50864, 6352, 295, 9848, 9834, 50964], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6390, "seek": 1869164, "start": 18703.64, "end": 18705.64, "text": " so to run like batch size 64", "tokens": [50964, 370, 281, 1190, 411, 15245, 2744, 12145, 51064], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6391, "seek": 1869164, "start": 18705.64, "end": 18707.64, "text": " batch size 32, batch size 16", "tokens": [51064, 15245, 2744, 8858, 11, 15245, 2744, 3165, 51164], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6392, "seek": 1869164, "start": 18707.64, "end": 18709.64, "text": " batch size maybe 256", "tokens": [51164, 15245, 2744, 1310, 38882, 51264], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6393, "seek": 1869164, "start": 18709.64, "end": 18711.64, "text": " we'll be like okay which ones are throwing errors and which ones aren't", "tokens": [51264, 321, 603, 312, 411, 1392, 597, 2306, 366, 10238, 13603, 293, 597, 2306, 3212, 380, 51364], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6394, "seek": 1869164, "start": 18711.64, "end": 18713.64, "text": " so what it'll do", "tokens": [51364, 370, 437, 309, 603, 360, 51464], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6395, "seek": 1869164, "start": 18713.64, "end": 18715.64, "text": " if you properly", "tokens": [51464, 498, 291, 6108, 51564], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6396, "seek": 1869164, "start": 18715.64, "end": 18717.64, "text": " if you properly set up an auto tuning script", "tokens": [51564, 498, 291, 6108, 992, 493, 364, 8399, 15164, 5755, 51664], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6397, "seek": 1869164, "start": 18717.64, "end": 18719.64, "text": " is", "tokens": [51664, 307, 51764], "temperature": 0.0, "avg_logprob": -0.11421687084695567, "compression_ratio": 1.8177777777777777, "no_speech_prob": 0.009262402541935444}, {"id": 6398, "seek": 1871964, "start": 18719.64, "end": 18721.64, "text": " you will be able to find", "tokens": [50364, 291, 486, 312, 1075, 281, 915, 50464], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6399, "seek": 1871964, "start": 18721.64, "end": 18723.64, "text": " the most optimal", "tokens": [50464, 264, 881, 16252, 50564], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6400, "seek": 1871964, "start": 18723.64, "end": 18725.64, "text": " set of parameters for your computer", "tokens": [50564, 992, 295, 9834, 337, 428, 3820, 50664], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6401, "seek": 1871964, "start": 18725.64, "end": 18727.64, "text": " most optimal set of hyper parameters", "tokens": [50664, 881, 16252, 992, 295, 9848, 9834, 50764], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6402, "seek": 1871964, "start": 18727.64, "end": 18729.64, "text": " that is possible", "tokens": [50764, 300, 307, 1944, 50864], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6403, "seek": 1871964, "start": 18729.64, "end": 18731.64, "text": " so auto tuning is cool", "tokens": [50864, 370, 8399, 15164, 307, 1627, 50964], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6404, "seek": 1871964, "start": 18731.64, "end": 18733.64, "text": " you can definitely look more into that", "tokens": [50964, 291, 393, 2138, 574, 544, 666, 300, 51064], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6405, "seek": 1871964, "start": 18733.64, "end": 18735.64, "text": " there's tons of research on it", "tokens": [51064, 456, 311, 9131, 295, 2132, 322, 309, 51164], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6406, "seek": 1871964, "start": 18735.64, "end": 18737.64, "text": " and yeah so", "tokens": [51164, 293, 1338, 370, 51264], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6407, "seek": 1871964, "start": 18737.64, "end": 18739.64, "text": " auto tuning is cool let's dig into the next part", "tokens": [51264, 8399, 15164, 307, 1627, 718, 311, 2528, 666, 264, 958, 644, 51364], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6408, "seek": 1871964, "start": 18739.64, "end": 18741.64, "text": " the next little trick we use in practice", "tokens": [51364, 264, 958, 707, 4282, 321, 764, 294, 3124, 51464], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6409, "seek": 1871964, "start": 18741.64, "end": 18743.64, "text": " especially by machine learning engineers", "tokens": [51464, 2318, 538, 3479, 2539, 11955, 51564], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6410, "seek": 1871964, "start": 18743.64, "end": 18745.64, "text": " it's a little something called arguments", "tokens": [51564, 309, 311, 257, 707, 746, 1219, 12869, 51664], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6411, "seek": 1871964, "start": 18745.64, "end": 18747.64, "text": " so you pass an argument into", "tokens": [51664, 370, 291, 1320, 364, 6770, 666, 51764], "temperature": 0.0, "avg_logprob": -0.07229699497729276, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.001700185821391642}, {"id": 6412, "seek": 1874764, "start": 18747.64, "end": 18749.64, "text": " not necessarily a function but into the command line", "tokens": [50364, 406, 4725, 257, 2445, 457, 666, 264, 5622, 1622, 50464], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6413, "seek": 1874764, "start": 18749.64, "end": 18751.64, "text": " so this is what it'll look like", "tokens": [50464, 370, 341, 307, 437, 309, 603, 574, 411, 50564], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6414, "seek": 1874764, "start": 18751.64, "end": 18753.64, "text": " this is just a basic example", "tokens": [50564, 341, 307, 445, 257, 3875, 1365, 50664], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6415, "seek": 1874764, "start": 18753.64, "end": 18755.64, "text": " of what arg parsing will look like", "tokens": [50664, 295, 437, 3882, 21156, 278, 486, 574, 411, 50764], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6416, "seek": 1874764, "start": 18755.64, "end": 18757.64, "text": " so just go", "tokens": [50764, 370, 445, 352, 50864], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6417, "seek": 1874764, "start": 18757.64, "end": 18759.64, "text": " python, arg parsing", "tokens": [50864, 38797, 11, 3882, 21156, 278, 50964], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6418, "seek": 1874764, "start": 18759.64, "end": 18761.64, "text": " because that's a script's name", "tokens": [50964, 570, 300, 311, 257, 5755, 311, 1315, 51064], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6419, "seek": 1874764, "start": 18761.64, "end": 18763.64, "text": " I go dash", "tokens": [51064, 286, 352, 8240, 51164], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6420, "seek": 1874764, "start": 18763.64, "end": 18765.64, "text": " llms because that's what it says", "tokens": [51164, 287, 75, 2592, 570, 300, 311, 437, 309, 1619, 51264], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6421, "seek": 1874764, "start": 18765.64, "end": 18767.64, "text": " right here this is what the argument is", "tokens": [51264, 558, 510, 341, 307, 437, 264, 6770, 307, 51364], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6422, "seek": 1874764, "start": 18767.64, "end": 18769.64, "text": " and then we can just pass in a string", "tokens": [51364, 293, 550, 321, 393, 445, 1320, 294, 257, 6798, 51464], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6423, "seek": 1874764, "start": 18769.64, "end": 18771.64, "text": " say hello", "tokens": [51464, 584, 7751, 51564], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6424, "seek": 1874764, "start": 18771.64, "end": 18773.64, "text": " the provided", "tokens": [51564, 264, 5649, 51664], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6425, "seek": 1874764, "start": 18773.64, "end": 18775.64, "text": " whatever is hello", "tokens": [51664, 2035, 307, 7751, 51764], "temperature": 0.0, "avg_logprob": -0.12516515595572336, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.044626932591199875}, {"id": 6426, "seek": 1877564, "start": 18775.64, "end": 18777.64, "text": " cool you can add little arguments to this", "tokens": [50364, 1627, 291, 393, 909, 707, 12869, 281, 341, 50464], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6427, "seek": 1877564, "start": 18777.64, "end": 18779.64, "text": " and I'm even going to change this around", "tokens": [50464, 293, 286, 478, 754, 516, 281, 1319, 341, 926, 50564], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6428, "seek": 1877564, "start": 18779.64, "end": 18781.64, "text": " I could say", "tokens": [50564, 286, 727, 584, 50664], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6429, "seek": 1877564, "start": 18785.64, "end": 18787.64, "text": " batch size", "tokens": [50864, 15245, 2744, 50964], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6430, "seek": 1877564, "start": 18787.64, "end": 18789.64, "text": " and then", "tokens": [50964, 293, 550, 51064], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6431, "seek": 1877564, "start": 18789.64, "end": 18791.64, "text": " let's go like that", "tokens": [51064, 718, 311, 352, 411, 300, 51164], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6432, "seek": 1877564, "start": 18791.64, "end": 18793.64, "text": " batch", "tokens": [51164, 15245, 51264], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6433, "seek": 1877564, "start": 18793.64, "end": 18795.64, "text": " batch size", "tokens": [51264, 15245, 2744, 51364], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6434, "seek": 1877564, "start": 18799.64, "end": 18801.64, "text": " please", "tokens": [51564, 1767, 51664], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6435, "seek": 1877564, "start": 18801.64, "end": 18803.64, "text": " provide", "tokens": [51664, 2893, 51764], "temperature": 0.0, "avg_logprob": -0.14844982964651926, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.012045923620462418}, {"id": 6436, "seek": 1880364, "start": 18803.64, "end": 18805.64, "text": " a batch size", "tokens": [50364, 257, 15245, 2744, 50464], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6437, "seek": 1880364, "start": 18807.64, "end": 18809.64, "text": " I can do the same thing again", "tokens": [50564, 286, 393, 360, 264, 912, 551, 797, 50664], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6438, "seek": 1880364, "start": 18811.64, "end": 18813.64, "text": " and see it says", "tokens": [50764, 293, 536, 309, 1619, 50864], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6439, "seek": 1880364, "start": 18813.64, "end": 18815.64, "text": " following arguments required are batch size", "tokens": [50864, 3480, 12869, 4739, 366, 15245, 2744, 50964], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6440, "seek": 1880364, "start": 18815.64, "end": 18817.64, "text": " so that obviously didn't work", "tokens": [50964, 370, 300, 2745, 994, 380, 589, 51064], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6441, "seek": 1880364, "start": 18817.64, "end": 18819.64, "text": " and if we actually tried the correct way", "tokens": [51064, 293, 498, 321, 767, 3031, 264, 3006, 636, 51164], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6442, "seek": 1880364, "start": 18819.64, "end": 18821.64, "text": " our parsing.py then we go", "tokens": [51164, 527, 21156, 278, 13, 8200, 550, 321, 352, 51264], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6443, "seek": 1880364, "start": 18821.64, "end": 18823.64, "text": " dash, batch size", "tokens": [51264, 8240, 11, 15245, 2744, 51364], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6444, "seek": 1880364, "start": 18823.64, "end": 18825.64, "text": " we can make it 32", "tokens": [51364, 321, 393, 652, 309, 8858, 51464], "temperature": 0.0, "avg_logprob": -0.16483231665382922, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.003481716150417924}, {"id": 6445, "seek": 1882564, "start": 18825.64, "end": 18827.64, "text": " oops", "tokens": [50364, 34166, 50464], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6446, "seek": 1882564, "start": 18831.64, "end": 18833.64, "text": " that's because it's not a string", "tokens": [50664, 300, 311, 570, 309, 311, 406, 257, 6798, 50764], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6447, "seek": 1882564, "start": 18833.64, "end": 18835.64, "text": " so", "tokens": [50764, 370, 50864], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6448, "seek": 1882564, "start": 18835.64, "end": 18837.64, "text": " what we need to actually do", "tokens": [50864, 437, 321, 643, 281, 767, 360, 50964], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6449, "seek": 1882564, "start": 18837.64, "end": 18839.64, "text": " is it's bs somewhere", "tokens": [50964, 307, 309, 311, 272, 82, 4079, 51064], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6450, "seek": 1882564, "start": 18839.64, "end": 18841.64, "text": " okay", "tokens": [51064, 1392, 51164], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6451, "seek": 1882564, "start": 18841.64, "end": 18843.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6452, "seek": 1882564, "start": 18843.64, "end": 18845.64, "text": " args", "tokens": [51264, 3882, 82, 51364], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6453, "seek": 1882564, "start": 18845.64, "end": 18847.64, "text": " parse args", "tokens": [51364, 48377, 3882, 82, 51464], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6454, "seek": 1882564, "start": 18847.64, "end": 18849.64, "text": " so we need to change this", "tokens": [51464, 370, 321, 643, 281, 1319, 341, 51564], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6455, "seek": 1882564, "start": 18849.64, "end": 18851.64, "text": " to bs like that", "tokens": [51564, 281, 272, 82, 411, 300, 51664], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6456, "seek": 1882564, "start": 18851.64, "end": 18853.64, "text": " let me go batch size", "tokens": [51664, 718, 385, 352, 15245, 2744, 51764], "temperature": 0.0, "avg_logprob": -0.15247316763434612, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.01825617626309395}, {"id": 6457, "seek": 1885364, "start": 18853.64, "end": 18855.64, "text": " batch size is 32", "tokens": [50364, 15245, 2744, 307, 8858, 50464], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6458, "seek": 1885364, "start": 18855.64, "end": 18857.64, "text": " okay", "tokens": [50464, 1392, 50564], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6459, "seek": 1885364, "start": 18857.64, "end": 18859.64, "text": " so even I'm a little bit new to arguments as well", "tokens": [50564, 370, 754, 286, 478, 257, 707, 857, 777, 281, 12869, 382, 731, 50664], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6460, "seek": 1885364, "start": 18859.64, "end": 18861.64, "text": " but", "tokens": [50664, 457, 50764], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6461, "seek": 1885364, "start": 18861.64, "end": 18863.64, "text": " this is something that comes in very handy", "tokens": [50764, 341, 307, 746, 300, 1487, 294, 588, 13239, 50864], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6462, "seek": 1885364, "start": 18863.64, "end": 18865.64, "text": " when you're trying to know each time", "tokens": [50864, 562, 291, 434, 1382, 281, 458, 1184, 565, 50964], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6463, "seek": 1885364, "start": 18865.64, "end": 18867.64, "text": " you're trying to change some parameters", "tokens": [50964, 291, 434, 1382, 281, 1319, 512, 9834, 51064], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6464, "seek": 1885364, "start": 18867.64, "end": 18869.64, "text": " if you add", "tokens": [51064, 498, 291, 909, 51164], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6465, "seek": 1885364, "start": 18869.64, "end": 18871.64, "text": " new gpu or whatever and you're like oh I want to double my batch size", "tokens": [51164, 777, 290, 34859, 420, 2035, 293, 291, 434, 411, 1954, 286, 528, 281, 3834, 452, 15245, 2744, 51264], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6466, "seek": 1885364, "start": 18871.64, "end": 18873.64, "text": " it's like sure you can easily do that", "tokens": [51264, 309, 311, 411, 988, 291, 393, 3612, 360, 300, 51364], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6467, "seek": 1885364, "start": 18873.64, "end": 18875.64, "text": " so a lot of the times", "tokens": [51364, 370, 257, 688, 295, 264, 1413, 51464], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6468, "seek": 1885364, "start": 18875.64, "end": 18877.64, "text": " it won't just have one but you'll have like", "tokens": [51464, 309, 1582, 380, 445, 362, 472, 457, 291, 603, 362, 411, 51564], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6469, "seek": 1885364, "start": 18877.64, "end": 18879.64, "text": " many meaning like maybe a dozen", "tokens": [51564, 867, 3620, 411, 1310, 257, 16654, 51664], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6470, "seek": 1885364, "start": 18879.64, "end": 18881.64, "text": " or so of these", "tokens": [51664, 420, 370, 295, 613, 51764], "temperature": 0.0, "avg_logprob": -0.10388926258237344, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.005058811511844397}, {"id": 6471, "seek": 1888164, "start": 18881.64, "end": 18883.64, "text": " of these little arguments", "tokens": [50364, 295, 613, 707, 12869, 50464], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6472, "seek": 1888164, "start": 18883.64, "end": 18885.64, "text": " so that is what this looks like", "tokens": [50464, 370, 300, 307, 437, 341, 1542, 411, 50564], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6473, "seek": 1888164, "start": 18885.64, "end": 18887.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6474, "seek": 1888164, "start": 18887.64, "end": 18889.64, "text": " we're going to go ahead and implement this", "tokens": [50664, 321, 434, 516, 281, 352, 2286, 293, 4445, 341, 50764], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6475, "seek": 1888164, "start": 18889.64, "end": 18891.64, "text": " into our little script here", "tokens": [50764, 666, 527, 707, 5755, 510, 50864], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6476, "seek": 1888164, "start": 18891.64, "end": 18893.64, "text": " so", "tokens": [50864, 370, 50964], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6477, "seek": 1888164, "start": 18893.64, "end": 18895.64, "text": " I'm just going to", "tokens": [50964, 286, 478, 445, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6478, "seek": 1888164, "start": 18895.64, "end": 18897.64, "text": " pop over to gpt1", "tokens": [51064, 1665, 670, 281, 290, 662, 16, 51164], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6479, "seek": 1888164, "start": 18897.64, "end": 18899.64, "text": " I'm going to pull this up on my", "tokens": [51164, 286, 478, 516, 281, 2235, 341, 493, 322, 452, 51264], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6480, "seek": 1888164, "start": 18899.64, "end": 18901.64, "text": " second monitor here", "tokens": [51264, 1150, 6002, 510, 51364], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6481, "seek": 1888164, "start": 18903.64, "end": 18905.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6482, "seek": 1888164, "start": 18905.64, "end": 18907.64, "text": " in terms of these", "tokens": [51564, 294, 2115, 295, 613, 51664], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6483, "seek": 1888164, "start": 18907.64, "end": 18909.64, "text": " I'm just going to start off", "tokens": [51664, 286, 478, 445, 516, 281, 722, 766, 51764], "temperature": 0.0, "avg_logprob": -0.0891509903801812, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.011152977123856544}, {"id": 6484, "seek": 1890964, "start": 18909.64, "end": 18911.64, "text": " making a", "tokens": [50364, 1455, 257, 50464], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6485, "seek": 1890964, "start": 18911.64, "end": 18913.64, "text": " importation", "tokens": [50464, 974, 399, 50564], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6486, "seek": 1890964, "start": 18913.64, "end": 18915.64, "text": " arg", "tokens": [50564, 3882, 50664], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6487, "seek": 1890964, "start": 18915.64, "end": 18917.64, "text": " arg parser", "tokens": [50664, 3882, 21156, 260, 50764], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6488, "seek": 1890964, "start": 18917.64, "end": 18919.64, "text": " or arg parse rather", "tokens": [50764, 420, 3882, 48377, 2831, 50864], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6489, "seek": 1890964, "start": 18919.64, "end": 18921.64, "text": " that's what it's called", "tokens": [50864, 300, 311, 437, 309, 311, 1219, 50964], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6490, "seek": 1890964, "start": 18921.64, "end": 18923.64, "text": " and then we go", "tokens": [50964, 293, 550, 321, 352, 51064], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6491, "seek": 1890964, "start": 18923.64, "end": 18925.64, "text": " parser is equal to", "tokens": [51064, 21156, 260, 307, 2681, 281, 51164], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6492, "seek": 1890964, "start": 18925.64, "end": 18927.64, "text": " I'll just", "tokens": [51164, 286, 603, 445, 51264], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6493, "seek": 1890964, "start": 18927.64, "end": 18929.64, "text": " copy and paste this entire thing", "tokens": [51264, 5055, 293, 9163, 341, 2302, 551, 51364], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6494, "seek": 1890964, "start": 18929.64, "end": 18931.64, "text": " and why not", "tokens": [51364, 293, 983, 406, 51464], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6495, "seek": 1890964, "start": 18931.64, "end": 18933.64, "text": " cool", "tokens": [51464, 1627, 51564], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6496, "seek": 1890964, "start": 18935.64, "end": 18937.64, "text": " okay", "tokens": [51664, 1392, 51764], "temperature": 0.0, "avg_logprob": -0.09986538817917091, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.010167645290493965}, {"id": 6497, "seek": 1893764, "start": 18937.64, "end": 18939.64, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6498, "seek": 1893764, "start": 18939.64, "end": 18941.64, "text": " we get a batch size", "tokens": [50464, 321, 483, 257, 15245, 2744, 50564], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6499, "seek": 1893764, "start": 18941.64, "end": 18943.64, "text": " or something", "tokens": [50564, 420, 746, 50664], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6500, "seek": 1893764, "start": 18943.64, "end": 18945.64, "text": " and then", "tokens": [50664, 293, 550, 50764], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6501, "seek": 1893764, "start": 18945.64, "end": 18947.64, "text": " we'll add in the second part here", "tokens": [50764, 321, 603, 909, 294, 264, 1150, 644, 510, 50864], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6502, "seek": 1893764, "start": 18947.64, "end": 18949.64, "text": " so", "tokens": [50864, 370, 50964], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6503, "seek": 1893764, "start": 18955.64, "end": 18957.64, "text": " args parse the arguments", "tokens": [51264, 3882, 82, 48377, 264, 12869, 51364], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6504, "seek": 1893764, "start": 18957.64, "end": 18959.64, "text": " here", "tokens": [51364, 510, 51464], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6505, "seek": 1893764, "start": 18961.64, "end": 18963.64, "text": " and the little scope", "tokens": [51564, 293, 264, 707, 11923, 51664], "temperature": 0.0, "avg_logprob": -0.19053023202078684, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.0009848617482930422}, {"id": 6506, "seek": 1896364, "start": 18963.64, "end": 18965.64, "text": " of", "tokens": [50364, 295, 50464], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6507, "seek": 1896364, "start": 18965.64, "end": 18967.64, "text": " batch size like that", "tokens": [50464, 15245, 2744, 411, 300, 50564], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6508, "seek": 1896364, "start": 18967.64, "end": 18969.64, "text": " our batch size is equal to", "tokens": [50564, 527, 15245, 2744, 307, 2681, 281, 50664], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6509, "seek": 1896364, "start": 18969.64, "end": 18971.64, "text": " whatever that was", "tokens": [50664, 2035, 300, 390, 50764], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6510, "seek": 1896364, "start": 18971.64, "end": 18973.64, "text": " and we'll just go args", "tokens": [50764, 293, 321, 603, 445, 352, 3882, 82, 50864], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6511, "seek": 1896364, "start": 18973.64, "end": 18975.64, "text": " dot", "tokens": [50864, 5893, 50964], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6512, "seek": 1896364, "start": 18975.64, "end": 18977.64, "text": " args dot batch size so cool", "tokens": [50964, 3882, 82, 5893, 15245, 2744, 370, 1627, 51064], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6513, "seek": 1896364, "start": 18979.64, "end": 18981.64, "text": " we're going to run this", "tokens": [51164, 321, 434, 516, 281, 1190, 341, 51264], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6514, "seek": 1896364, "start": 18981.64, "end": 18983.64, "text": " and", "tokens": [51264, 293, 51364], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6515, "seek": 1896364, "start": 18983.64, "end": 18985.64, "text": " not defined", "tokens": [51364, 406, 7642, 51464], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6516, "seek": 1896364, "start": 18985.64, "end": 18987.64, "text": " so I got a little not defined thing here", "tokens": [51464, 370, 286, 658, 257, 707, 406, 7642, 551, 510, 51564], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6517, "seek": 1896364, "start": 18987.64, "end": 18989.64, "text": " and pretty much", "tokens": [51564, 293, 1238, 709, 51664], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6518, "seek": 1896364, "start": 18989.64, "end": 18991.64, "text": " all I missed was that", "tokens": [51664, 439, 286, 6721, 390, 300, 51764], "temperature": 0.0, "avg_logprob": -0.12881418021328478, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0023593646474182606}, {"id": 6519, "seek": 1899164, "start": 18991.64, "end": 18993.64, "text": " we're doing this so essentially", "tokens": [50364, 321, 434, 884, 341, 370, 4476, 50464], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6520, "seek": 1899164, "start": 18993.64, "end": 18995.64, "text": " this", "tokens": [50464, 341, 50564], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6521, "seek": 1899164, "start": 18995.64, "end": 18997.64, "text": " should be equal to this right here", "tokens": [50564, 820, 312, 2681, 281, 341, 558, 510, 50664], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6522, "seek": 1899164, "start": 18997.64, "end": 18999.64, "text": " so I'm just going to go ahead and copy that", "tokens": [50664, 370, 286, 478, 445, 516, 281, 352, 2286, 293, 5055, 300, 50764], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6523, "seek": 1899164, "start": 18999.64, "end": 19001.64, "text": " and", "tokens": [50764, 293, 50864], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6524, "seek": 1899164, "start": 19003.64, "end": 19005.64, "text": " boot parse args", "tokens": [50964, 11450, 48377, 3882, 82, 51064], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6525, "seek": 1899164, "start": 19005.64, "end": 19007.64, "text": " except", "tokens": [51064, 3993, 51164], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6526, "seek": 1899164, "start": 19007.64, "end": 19009.64, "text": " we don't have a parse args function", "tokens": [51164, 321, 500, 380, 362, 257, 48377, 3882, 82, 2445, 51264], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6527, "seek": 1899164, "start": 19009.64, "end": 19011.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6528, "seek": 1899164, "start": 19011.64, "end": 19013.64, "text": " what do we need to do instead", "tokens": [51364, 437, 360, 321, 643, 281, 360, 2602, 51464], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6529, "seek": 1899164, "start": 19013.64, "end": 19015.64, "text": " well it", "tokens": [51464, 731, 309, 51564], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6530, "seek": 1899164, "start": 19015.64, "end": 19017.64, "text": " actually that might just work on it so let's try it out", "tokens": [51564, 767, 300, 1062, 445, 589, 322, 309, 370, 718, 311, 853, 309, 484, 51664], "temperature": 0.0, "avg_logprob": -0.14266953685066916, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.014497951604425907}, {"id": 6531, "seek": 1902164, "start": 19021.64, "end": 19023.64, "text": " okay so it looks like", "tokens": [50364, 1392, 370, 309, 1542, 411, 50464], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6532, "seek": 1902164, "start": 19023.64, "end": 19025.64, "text": " it's actually expecting some input here", "tokens": [50464, 309, 311, 767, 9650, 512, 4846, 510, 50564], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6533, "seek": 1902164, "start": 19025.64, "end": 19027.64, "text": " in code so", "tokens": [50564, 294, 3089, 370, 50664], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6534, "seek": 1902164, "start": 19027.64, "end": 19029.64, "text": " that's probably working", "tokens": [50664, 300, 311, 1391, 1364, 50764], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6535, "seek": 1902164, "start": 19029.64, "end": 19031.64, "text": " and if we", "tokens": [50764, 293, 498, 321, 50864], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6536, "seek": 1902164, "start": 19031.64, "end": 19033.64, "text": " ported this into a script", "tokens": [50864, 2436, 292, 341, 666, 257, 5755, 50964], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6537, "seek": 1902164, "start": 19033.64, "end": 19035.64, "text": " then it would simply ask us for some input", "tokens": [50964, 550, 309, 576, 2935, 1029, 505, 337, 512, 4846, 51064], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6538, "seek": 1902164, "start": 19035.64, "end": 19037.64, "text": " so I believe we're doing this correctly", "tokens": [51064, 370, 286, 1697, 321, 434, 884, 341, 8944, 51164], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6539, "seek": 1902164, "start": 19037.64, "end": 19039.64, "text": " let's go ahead", "tokens": [51164, 718, 311, 352, 2286, 51264], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6540, "seek": 1902164, "start": 19039.64, "end": 19041.64, "text": " and actually switch over", "tokens": [51264, 293, 767, 3679, 670, 51364], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6541, "seek": 1902164, "start": 19041.64, "end": 19043.64, "text": " and pour all of this into some code", "tokens": [51364, 293, 2016, 439, 295, 341, 666, 512, 3089, 51464], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6542, "seek": 1902164, "start": 19043.64, "end": 19045.64, "text": " so I'm going to make", "tokens": [51464, 370, 286, 478, 516, 281, 652, 51564], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6543, "seek": 1902164, "start": 19045.64, "end": 19047.64, "text": " a training file", "tokens": [51564, 257, 3097, 3991, 51664], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6544, "seek": 1902164, "start": 19047.64, "end": 19049.64, "text": " and a chat file", "tokens": [51664, 293, 257, 5081, 3991, 51764], "temperature": 0.0, "avg_logprob": -0.06481824012903067, "compression_ratio": 1.6780487804878048, "no_speech_prob": 0.022957926616072655}, {"id": 6545, "seek": 1904964, "start": 19049.64, "end": 19051.64, "text": " the training file is going to be all of our parameters", "tokens": [50364, 264, 3097, 3991, 307, 516, 281, 312, 439, 295, 527, 9834, 50464], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6546, "seek": 1904964, "start": 19051.64, "end": 19053.64, "text": " whatever all of our architecture", "tokens": [50464, 2035, 439, 295, 527, 9482, 50564], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6547, "seek": 1904964, "start": 19053.64, "end": 19055.64, "text": " and then the actual training loop itself", "tokens": [50564, 293, 550, 264, 3539, 3097, 6367, 2564, 50664], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6548, "seek": 1904964, "start": 19055.64, "end": 19057.64, "text": " we're going to have some arguments in there", "tokens": [50664, 321, 434, 516, 281, 362, 512, 12869, 294, 456, 50764], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6549, "seek": 1904964, "start": 19057.64, "end": 19059.64, "text": " and then the chat bot is going to be", "tokens": [50764, 293, 550, 264, 5081, 10592, 307, 516, 281, 312, 50864], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6550, "seek": 1904964, "start": 19059.64, "end": 19061.64, "text": " pretty much just a question-answer", "tokens": [50864, 1238, 709, 445, 257, 1168, 12, 43904, 50964], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6551, "seek": 1904964, "start": 19061.64, "end": 19063.64, "text": " thing that just reproduces text", "tokens": [50964, 551, 300, 445, 11408, 887, 2487, 51064], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6552, "seek": 1904964, "start": 19063.64, "end": 19065.64, "text": " so it'll just be like prompt, completion", "tokens": [51064, 370, 309, 603, 445, 312, 411, 12391, 11, 19372, 51164], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6553, "seek": 1904964, "start": 19065.64, "end": 19067.64, "text": " type of thing and", "tokens": [51164, 2010, 295, 551, 293, 51264], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6554, "seek": 1904964, "start": 19067.64, "end": 19069.64, "text": " yeah so let's go ahead and implement that here", "tokens": [51264, 1338, 370, 718, 311, 352, 2286, 293, 4445, 300, 510, 51364], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6555, "seek": 1904964, "start": 19069.64, "end": 19071.64, "text": " so in our", "tokens": [51364, 370, 294, 527, 51464], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6556, "seek": 1904964, "start": 19071.64, "end": 19073.64, "text": " GPT course", "tokens": [51464, 26039, 51, 1164, 51564], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6557, "seek": 1904964, "start": 19073.64, "end": 19075.64, "text": " here I'm going to go", "tokens": [51564, 510, 286, 478, 516, 281, 352, 51664], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6558, "seek": 1904964, "start": 19075.64, "end": 19077.64, "text": " training.py", "tokens": [51664, 3097, 13, 8200, 51764], "temperature": 0.0, "avg_logprob": -0.10807832342679383, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.0021482487209141254}, {"id": 6559, "seek": 1907764, "start": 19077.64, "end": 19079.64, "text": " and we're going to go", "tokens": [50364, 293, 321, 434, 516, 281, 352, 50464], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6560, "seek": 1907764, "start": 19079.64, "end": 19081.64, "text": " chatbot.py", "tokens": [50464, 5081, 18870, 13, 8200, 50564], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6561, "seek": 1907764, "start": 19081.64, "end": 19083.64, "text": " just like that", "tokens": [50564, 445, 411, 300, 50664], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6562, "seek": 1907764, "start": 19083.64, "end": 19085.64, "text": " so in training", "tokens": [50664, 370, 294, 3097, 50764], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6563, "seek": 1907764, "start": 19085.64, "end": 19087.64, "text": " let's go ahead and drag everything in here", "tokens": [50764, 718, 311, 352, 2286, 293, 5286, 1203, 294, 510, 50864], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6564, "seek": 1907764, "start": 19089.64, "end": 19091.64, "text": " I'm just going to", "tokens": [50964, 286, 478, 445, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6565, "seek": 1907764, "start": 19091.64, "end": 19093.64, "text": " move this over to the second screen", "tokens": [51064, 1286, 341, 670, 281, 264, 1150, 2568, 51164], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6566, "seek": 1907764, "start": 19093.64, "end": 19095.64, "text": " and just copy and paste", "tokens": [51164, 293, 445, 5055, 293, 9163, 51264], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6567, "seek": 1907764, "start": 19095.64, "end": 19097.64, "text": " everything in order here", "tokens": [51264, 1203, 294, 1668, 510, 51364], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6568, "seek": 1907764, "start": 19097.64, "end": 19099.64, "text": " so next up we have our", "tokens": [51364, 370, 958, 493, 321, 362, 527, 51464], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6569, "seek": 1907764, "start": 19099.64, "end": 19101.64, "text": " characters", "tokens": [51464, 4342, 51564], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6570, "seek": 1907764, "start": 19101.64, "end": 19103.64, "text": " and then we have our", "tokens": [51564, 293, 550, 321, 362, 527, 51664], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6571, "seek": 1907764, "start": 19103.64, "end": 19105.64, "text": " tokenizer", "tokens": [51664, 14862, 6545, 51764], "temperature": 0.0, "avg_logprob": -0.0716737454587763, "compression_ratio": 1.6347305389221556, "no_speech_prob": 0.005218507256358862}, {"id": 6572, "seek": 1910564, "start": 19105.64, "end": 19107.64, "text": " and then our", "tokens": [50364, 293, 550, 527, 50464], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6573, "seek": 1910564, "start": 19107.64, "end": 19109.64, "text": " getRandomChunk and getBatches", "tokens": [50464, 483, 49, 4606, 6546, 3197, 293, 483, 33, 852, 279, 50564], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6574, "seek": 1910564, "start": 19113.64, "end": 19115.64, "text": " suite", "tokens": [50764, 14205, 50864], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6575, "seek": 1910564, "start": 19115.64, "end": 19117.64, "text": " our estimateLoss function", "tokens": [50864, 527, 12539, 43, 772, 2445, 50964], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6576, "seek": 1910564, "start": 19121.64, "end": 19123.64, "text": " and then this giant piece", "tokens": [51164, 293, 550, 341, 7410, 2522, 51264], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6577, "seek": 1910564, "start": 19123.64, "end": 19125.64, "text": " of code", "tokens": [51264, 295, 3089, 51364], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6578, "seek": 1910564, "start": 19125.64, "end": 19127.64, "text": " containing", "tokens": [51364, 19273, 51464], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6579, "seek": 1910564, "start": 19127.64, "end": 19129.64, "text": " most of the architecture we built up", "tokens": [51464, 881, 295, 264, 9482, 321, 3094, 493, 51564], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6580, "seek": 1910564, "start": 19131.64, "end": 19133.64, "text": " we're just going to add that in there", "tokens": [51664, 321, 434, 445, 516, 281, 909, 300, 294, 456, 51764], "temperature": 0.0, "avg_logprob": -0.12153943379720052, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.001064854208379984}, {"id": 6581, "seek": 1913364, "start": 19133.64, "end": 19135.64, "text": " we're not getting any warnings", "tokens": [50364, 321, 434, 406, 1242, 604, 30009, 50464], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6582, "seek": 1913364, "start": 19137.64, "end": 19139.64, "text": " and then the training loop", "tokens": [50564, 293, 550, 264, 3097, 6367, 50664], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6583, "seek": 1913364, "start": 19141.64, "end": 19143.64, "text": " and the optimizer", "tokens": [50764, 293, 264, 5028, 6545, 50864], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6584, "seek": 1913364, "start": 19143.64, "end": 19145.64, "text": " awesome", "tokens": [50864, 3476, 50964], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6585, "seek": 1913364, "start": 19145.64, "end": 19147.64, "text": " then after this", "tokens": [50964, 550, 934, 341, 51064], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6586, "seek": 1913364, "start": 19147.64, "end": 19149.64, "text": " we would simply have this context", "tokens": [51064, 321, 576, 2935, 362, 341, 4319, 51164], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6587, "seek": 1913364, "start": 19149.64, "end": 19151.64, "text": " but the point of this is that we want to have this in our", "tokens": [51164, 457, 264, 935, 295, 341, 307, 300, 321, 528, 281, 362, 341, 294, 527, 51264], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6588, "seek": 1913364, "start": 19151.64, "end": 19153.64, "text": " chatbot script", "tokens": [51264, 5081, 18870, 5755, 51364], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6589, "seek": 1913364, "start": 19153.64, "end": 19155.64, "text": " so what I'm going to do", "tokens": [51364, 370, 437, 286, 478, 516, 281, 360, 51464], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6590, "seek": 1913364, "start": 19155.64, "end": 19157.64, "text": " is in this training.py", "tokens": [51464, 307, 294, 341, 3097, 13, 8200, 51564], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6591, "seek": 1913364, "start": 19157.64, "end": 19159.64, "text": " I'm going to keep", "tokens": [51564, 286, 478, 516, 281, 1066, 51664], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6592, "seek": 1913364, "start": 19159.64, "end": 19161.64, "text": " all of these the same I'm going to keep this entire thing", "tokens": [51664, 439, 295, 613, 264, 912, 286, 478, 516, 281, 1066, 341, 2302, 551, 51764], "temperature": 0.0, "avg_logprob": -0.08047680902962732, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.002630844945088029}, {"id": 6593, "seek": 1916164, "start": 19161.64, "end": 19163.64, "text": " the same", "tokens": [50364, 264, 912, 50464], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6594, "seek": 1916164, "start": 19163.64, "end": 19165.64, "text": " get rid of this little block of code", "tokens": [50464, 483, 3973, 295, 341, 707, 3461, 295, 3089, 50564], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6595, "seek": 1916164, "start": 19165.64, "end": 19167.64, "text": " and we're going to go into", "tokens": [50564, 293, 321, 434, 516, 281, 352, 666, 50664], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6596, "seek": 1916164, "start": 19167.64, "end": 19169.64, "text": " the chatbot", "tokens": [50664, 264, 5081, 18870, 50764], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6597, "seek": 1916164, "start": 19169.64, "end": 19171.64, "text": " here so loadingMile", "tokens": [50764, 510, 370, 15114, 44, 794, 50864], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6598, "seek": 1916164, "start": 19171.64, "end": 19173.64, "text": " parameters good we want to load some in", "tokens": [50864, 9834, 665, 321, 528, 281, 3677, 512, 294, 50964], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6599, "seek": 1916164, "start": 19173.64, "end": 19175.64, "text": " train some more and then dump it", "tokens": [50964, 3847, 512, 544, 293, 550, 11430, 309, 51064], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6600, "seek": 1916164, "start": 19175.64, "end": 19177.64, "text": " chatbot is not going to dump anything", "tokens": [51064, 5081, 18870, 307, 406, 516, 281, 11430, 1340, 51164], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6601, "seek": 1916164, "start": 19177.64, "end": 19179.64, "text": " it's just going to save so I'm going to take", "tokens": [51164, 309, 311, 445, 516, 281, 3155, 370, 286, 478, 516, 281, 747, 51264], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6602, "seek": 1916164, "start": 19179.64, "end": 19181.64, "text": " all of our training here", "tokens": [51264, 439, 295, 527, 3097, 510, 51364], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6603, "seek": 1916164, "start": 19183.64, "end": 19185.64, "text": " and instead of dumping", "tokens": [51464, 293, 2602, 295, 42224, 51564], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6604, "seek": 1916164, "start": 19185.64, "end": 19187.64, "text": " take that away we'll also take", "tokens": [51564, 747, 300, 1314, 321, 603, 611, 747, 51664], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6605, "seek": 1916164, "start": 19187.64, "end": 19189.64, "text": " away the training", "tokens": [51664, 1314, 264, 3097, 51764], "temperature": 0.0, "avg_logprob": -0.09959736048618209, "compression_ratio": 1.7673267326732673, "no_speech_prob": 0.016644978895783424}, {"id": 6606, "seek": 1918964, "start": 19189.64, "end": 19191.64, "text": " loop as well", "tokens": [50364, 6367, 382, 731, 50464], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6607, "seek": 1918964, "start": 19195.64, "end": 19197.64, "text": " I don't believe we have anything", "tokens": [50664, 286, 500, 380, 1697, 321, 362, 1340, 50764], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6608, "seek": 1918964, "start": 19197.64, "end": 19199.64, "text": " else to actually bring in", "tokens": [50764, 1646, 281, 767, 1565, 294, 50864], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6609, "seek": 1918964, "start": 19199.64, "end": 19201.64, "text": " we don't need our getBatch", "tokens": [50864, 321, 500, 380, 643, 527, 483, 33, 852, 50964], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6610, "seek": 1918964, "start": 19201.64, "end": 19203.64, "text": " we do not need our getRandomChunks", "tokens": [50964, 321, 360, 406, 643, 527, 483, 49, 4606, 6546, 17627, 51064], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6611, "seek": 1918964, "start": 19203.64, "end": 19205.64, "text": " so awesome", "tokens": [51064, 370, 3476, 51164], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6612, "seek": 1918964, "start": 19205.64, "end": 19207.64, "text": " we're just importing these parameters", "tokens": [51164, 321, 434, 445, 43866, 613, 9834, 51264], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6613, "seek": 1918964, "start": 19207.64, "end": 19209.64, "text": " by default like that", "tokens": [51264, 538, 7576, 411, 300, 51364], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6614, "seek": 1918964, "start": 19209.64, "end": 19211.64, "text": " awesome", "tokens": [51364, 3476, 51464], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6615, "seek": 1918964, "start": 19211.64, "end": 19213.64, "text": " so from this point", "tokens": [51464, 370, 490, 341, 935, 51564], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6616, "seek": 1918964, "start": 19213.64, "end": 19215.64, "text": " we have imported", "tokens": [51564, 321, 362, 25524, 51664], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6617, "seek": 1918964, "start": 19215.64, "end": 19217.64, "text": " we've imported our model", "tokens": [51664, 321, 600, 25524, 527, 2316, 51764], "temperature": 0.0, "avg_logprob": -0.09235602901095435, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.014945614151656628}, {"id": 6618, "seek": 1921764, "start": 19217.64, "end": 19219.64, "text": " cool so let's go ahead", "tokens": [50364, 1627, 370, 718, 311, 352, 2286, 50464], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6619, "seek": 1921764, "start": 19219.64, "end": 19221.64, "text": " and port in our little", "tokens": [50464, 293, 2436, 294, 527, 707, 50564], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6620, "seek": 1921764, "start": 19221.64, "end": 19223.64, "text": " chatbot here", "tokens": [50564, 5081, 18870, 510, 50664], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6621, "seek": 1921764, "start": 19223.64, "end": 19225.64, "text": " this little end piece", "tokens": [50664, 341, 707, 917, 2522, 50764], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6622, "seek": 1921764, "start": 19225.64, "end": 19227.64, "text": " which is going to allow us to", "tokens": [50764, 597, 307, 516, 281, 2089, 505, 281, 50864], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6623, "seek": 1921764, "start": 19227.64, "end": 19229.64, "text": " essentially chat with the model", "tokens": [50864, 4476, 5081, 365, 264, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6624, "seek": 1921764, "start": 19229.64, "end": 19231.64, "text": " this is what it looks like a little wild loop", "tokens": [50964, 341, 307, 437, 309, 1542, 411, 257, 707, 4868, 6367, 51064], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6625, "seek": 1921764, "start": 19231.64, "end": 19233.64, "text": " we have a prompt we just input", "tokens": [51064, 321, 362, 257, 12391, 321, 445, 4846, 51164], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6626, "seek": 1921764, "start": 19233.64, "end": 19235.64, "text": " something", "tokens": [51164, 746, 51264], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6627, "seek": 1921764, "start": 19235.64, "end": 19237.64, "text": " prompt next line that should be fairly self explanatory", "tokens": [51264, 12391, 958, 1622, 300, 820, 312, 6457, 2698, 9045, 4745, 51364], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6628, "seek": 1921764, "start": 19237.64, "end": 19239.64, "text": " and we have this tensor", "tokens": [51364, 293, 321, 362, 341, 40863, 51464], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6629, "seek": 1921764, "start": 19239.64, "end": 19241.64, "text": " we're going to encode this prompt into a bunch", "tokens": [51464, 321, 434, 516, 281, 2058, 1429, 341, 12391, 666, 257, 3840, 51564], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6630, "seek": 1921764, "start": 19241.64, "end": 19243.64, "text": " of integers or torch.long data types", "tokens": [51564, 295, 41674, 420, 27822, 13, 13025, 1412, 3467, 51664], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6631, "seek": 1921764, "start": 19243.64, "end": 19245.64, "text": " on the GPU", "tokens": [51664, 322, 264, 18407, 51764], "temperature": 0.0, "avg_logprob": -0.11977920532226563, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.009406591765582561}, {"id": 6632, "seek": 1924564, "start": 19245.64, "end": 19247.64, "text": " devices CUDA", "tokens": [50364, 5759, 29777, 7509, 50464], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6633, "seek": 1924564, "start": 19247.64, "end": 19249.64, "text": " and then after", "tokens": [50464, 293, 550, 934, 50564], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6634, "seek": 1924564, "start": 19249.64, "end": 19251.64, "text": " after we've actually generated these", "tokens": [50564, 934, 321, 600, 767, 10833, 613, 50664], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6635, "seek": 1924564, "start": 19251.64, "end": 19253.64, "text": " so model.generate", "tokens": [50664, 370, 2316, 13, 21848, 473, 50764], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6636, "seek": 1924564, "start": 19253.64, "end": 19255.64, "text": " we're going to unsqueeze these", "tokens": [50764, 321, 434, 516, 281, 2693, 1077, 10670, 613, 50864], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6637, "seek": 1924564, "start": 19255.64, "end": 19257.64, "text": " remember it's a torch.tensor", "tokens": [50864, 1604, 309, 311, 257, 27822, 13, 83, 23153, 50964], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6638, "seek": 1924564, "start": 19257.64, "end": 19259.64, "text": " so it's going to be in the matrices form", "tokens": [50964, 370, 309, 311, 516, 281, 312, 294, 264, 32284, 1254, 51064], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6639, "seek": 1924564, "start": 19259.64, "end": 19261.64, "text": " so it's going to look like this", "tokens": [51064, 370, 309, 311, 516, 281, 574, 411, 341, 51164], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6640, "seek": 1924564, "start": 19261.64, "end": 19263.64, "text": " it's going to look like this or whatever", "tokens": [51164, 309, 311, 516, 281, 574, 411, 341, 420, 2035, 51264], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6641, "seek": 1924564, "start": 19263.64, "end": 19265.64, "text": " that's essentially what the shape is", "tokens": [51264, 300, 311, 4476, 437, 264, 3909, 307, 51364], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6642, "seek": 1924564, "start": 19265.64, "end": 19267.64, "text": " so all we're doing when we unsqueeze it", "tokens": [51364, 370, 439, 321, 434, 884, 562, 321, 2693, 1077, 10670, 309, 51464], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6643, "seek": 1924564, "start": 19267.64, "end": 19269.64, "text": " is we're just taking away this wrapping", "tokens": [51464, 307, 321, 434, 445, 1940, 1314, 341, 21993, 51564], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6644, "seek": 1924564, "start": 19269.64, "end": 19271.64, "text": " around it", "tokens": [51564, 926, 309, 51664], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6645, "seek": 1924564, "start": 19271.64, "end": 19273.64, "text": " so awesome", "tokens": [51664, 370, 3476, 51764], "temperature": 0.0, "avg_logprob": -0.10797555446624756, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.013629259541630745}, {"id": 6646, "seek": 1927364, "start": 19273.64, "end": 19275.64, "text": " we're just going to do some", "tokens": [50364, 321, 434, 445, 516, 281, 360, 512, 50464], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6647, "seek": 1927364, "start": 19275.64, "end": 19277.64, "text": " tokens for example 150 here", "tokens": [50464, 22667, 337, 1365, 8451, 510, 50564], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6648, "seek": 1927364, "start": 19277.64, "end": 19279.64, "text": " and then to a list format", "tokens": [50564, 293, 550, 281, 257, 1329, 7877, 50664], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6649, "seek": 1927364, "start": 19279.64, "end": 19281.64, "text": " and then we can just print these out", "tokens": [50664, 293, 550, 321, 393, 445, 4482, 613, 484, 50764], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6650, "seek": 1927364, "start": 19281.64, "end": 19283.64, "text": " as January characters", "tokens": [50764, 382, 7061, 4342, 50864], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6651, "seek": 1927364, "start": 19283.64, "end": 19285.64, "text": " awesome so we're just going to ask this prompt", "tokens": [50864, 3476, 370, 321, 434, 445, 516, 281, 1029, 341, 12391, 50964], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6652, "seek": 1927364, "start": 19285.64, "end": 19287.64, "text": " and then do some compute give us a completion", "tokens": [50964, 293, 550, 360, 512, 14722, 976, 505, 257, 19372, 51064], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6653, "seek": 1927364, "start": 19287.64, "end": 19289.64, "text": " so on and so forth", "tokens": [51064, 370, 322, 293, 370, 5220, 51164], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6654, "seek": 1927364, "start": 19289.64, "end": 19291.64, "text": " so that's what this is doing here", "tokens": [51164, 370, 300, 311, 437, 341, 307, 884, 510, 51264], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6655, "seek": 1927364, "start": 19291.64, "end": 19293.64, "text": " and another thing I wanted to point out", "tokens": [51264, 293, 1071, 551, 286, 1415, 281, 935, 484, 51364], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6656, "seek": 1927364, "start": 19293.64, "end": 19295.64, "text": " is actually when we load these", "tokens": [51364, 307, 767, 562, 321, 3677, 613, 51464], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6657, "seek": 1927364, "start": 19295.64, "end": 19297.64, "text": " parameters in", "tokens": [51464, 9834, 294, 51564], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6658, "seek": 1927364, "start": 19297.64, "end": 19299.64, "text": " at least on training", "tokens": [51564, 412, 1935, 322, 3097, 51664], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6659, "seek": 1927364, "start": 19299.64, "end": 19301.64, "text": " it's going to initially give us errors", "tokens": [51664, 309, 311, 516, 281, 9105, 976, 505, 13603, 51764], "temperature": 0.0, "avg_logprob": -0.18294688633510045, "compression_ratio": 1.8033472803347281, "no_speech_prob": 0.010813245549798012}, {"id": 6660, "seek": 1930164, "start": 19301.64, "end": 19303.64, "text": " from we're going to get errors from that", "tokens": [50364, 490, 321, 434, 516, 281, 483, 13603, 490, 300, 50464], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6661, "seek": 1930164, "start": 19303.64, "end": 19305.64, "text": " because the model will just not be", "tokens": [50464, 570, 264, 2316, 486, 445, 406, 312, 50564], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6662, "seek": 1930164, "start": 19305.64, "end": 19307.64, "text": " anything and we won't be able to import stuff", "tokens": [50564, 1340, 293, 321, 1582, 380, 312, 1075, 281, 974, 1507, 50664], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6663, "seek": 1930164, "start": 19307.64, "end": 19309.64, "text": " so that's going to give you errors first of all", "tokens": [50664, 370, 300, 311, 516, 281, 976, 291, 13603, 700, 295, 439, 50764], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6664, "seek": 1930164, "start": 19309.64, "end": 19311.64, "text": " another thing you want to pay attention to", "tokens": [50764, 1071, 551, 291, 528, 281, 1689, 3202, 281, 50864], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6665, "seek": 1930164, "start": 19311.64, "end": 19313.64, "text": " is to make sure that when you've actually trained", "tokens": [50864, 307, 281, 652, 988, 300, 562, 291, 600, 767, 8895, 50964], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6666, "seek": 1930164, "start": 19313.64, "end": 19315.64, "text": " this initial model that it matches", "tokens": [50964, 341, 5883, 2316, 300, 309, 10676, 51064], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6667, "seek": 1930164, "start": 19315.64, "end": 19317.64, "text": " all of the architectural", "tokens": [51064, 439, 295, 264, 26621, 51164], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6668, "seek": 1930164, "start": 19317.64, "end": 19319.64, "text": " stuff and the hyper parameters", "tokens": [51164, 1507, 293, 264, 9848, 9834, 51264], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6669, "seek": 1930164, "start": 19319.64, "end": 19321.64, "text": " that you used", "tokens": [51264, 300, 291, 1143, 51364], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6670, "seek": 1930164, "start": 19321.64, "end": 19323.64, "text": " that when you're using to load up again", "tokens": [51364, 300, 562, 291, 434, 1228, 281, 3677, 493, 797, 51464], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6671, "seek": 1930164, "start": 19323.64, "end": 19325.64, "text": " so", "tokens": [51464, 370, 51564], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6672, "seek": 1930164, "start": 19325.64, "end": 19327.64, "text": " when you're running your forward pass and whatnot", "tokens": [51564, 562, 291, 434, 2614, 428, 2128, 1320, 293, 25882, 51664], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6673, "seek": 1930164, "start": 19327.64, "end": 19329.64, "text": " you just want to make sure that this architecture", "tokens": [51664, 291, 445, 528, 281, 652, 988, 300, 341, 9482, 51764], "temperature": 0.0, "avg_logprob": -0.05367259032853687, "compression_ratio": 1.9391634980988592, "no_speech_prob": 0.01565159671008587}, {"id": 6674, "seek": 1932964, "start": 19329.64, "end": 19331.64, "text": " sort of lines up with it", "tokens": [50364, 1333, 295, 3876, 493, 365, 309, 50464], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6675, "seek": 1932964, "start": 19331.64, "end": 19333.64, "text": " just so that you don't get any architectural errors", "tokens": [50464, 445, 370, 300, 291, 500, 380, 483, 604, 26621, 13603, 50564], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6676, "seek": 1932964, "start": 19333.64, "end": 19335.64, "text": " those can be really confusing to debug", "tokens": [50564, 729, 393, 312, 534, 13181, 281, 24083, 50664], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6677, "seek": 1932964, "start": 19335.64, "end": 19337.64, "text": " so yeah", "tokens": [50664, 370, 1338, 50764], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6678, "seek": 1932964, "start": 19337.64, "end": 19339.64, "text": " and the way we can do this is actually just", "tokens": [50764, 293, 264, 636, 321, 393, 360, 341, 307, 767, 445, 50864], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6679, "seek": 1932964, "start": 19339.64, "end": 19341.64, "text": " commenting it out here", "tokens": [50864, 29590, 309, 484, 510, 50964], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6680, "seek": 1932964, "start": 19341.64, "end": 19343.64, "text": " awesome, we're able to save load models", "tokens": [50964, 3476, 11, 321, 434, 1075, 281, 3155, 3677, 5245, 51064], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6681, "seek": 1932964, "start": 19343.64, "end": 19345.64, "text": " and", "tokens": [51064, 293, 51164], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6682, "seek": 1932964, "start": 19345.64, "end": 19347.64, "text": " we're able to use a little loop", "tokens": [51164, 321, 434, 1075, 281, 764, 257, 707, 6367, 51264], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6683, "seek": 1932964, "start": 19347.64, "end": 19349.64, "text": " to create a sort of", "tokens": [51264, 281, 1884, 257, 1333, 295, 51364], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6684, "seek": 1932964, "start": 19349.64, "end": 19351.64, "text": " chat-up that's not really helpful", "tokens": [51364, 5081, 12, 1010, 300, 311, 406, 534, 4961, 51464], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6685, "seek": 1932964, "start": 19351.64, "end": 19353.64, "text": " because we haven't trained it", "tokens": [51464, 570, 321, 2378, 380, 8895, 309, 51564], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6686, "seek": 1932964, "start": 19353.64, "end": 19355.64, "text": " an insane amount on", "tokens": [51564, 364, 10838, 2372, 322, 51664], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6687, "seek": 1932964, "start": 19355.64, "end": 19357.64, "text": " data that actually is useful", "tokens": [51664, 1412, 300, 767, 307, 4420, 51764], "temperature": 0.0, "avg_logprob": -0.10326868969461192, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0067946589551866055}, {"id": 6688, "seek": 1935764, "start": 19357.64, "end": 19359.64, "text": " so another little detail that's very important", "tokens": [50364, 370, 1071, 707, 2607, 300, 311, 588, 1021, 50464], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6689, "seek": 1935764, "start": 19359.64, "end": 19361.64, "text": " is to actually", "tokens": [50464, 307, 281, 767, 50564], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6690, "seek": 1935764, "start": 19361.64, "end": 19363.64, "text": " make sure that you have nn-module in all", "tokens": [50564, 652, 988, 300, 291, 362, 297, 77, 12, 8014, 2271, 294, 439, 50664], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6691, "seek": 1935764, "start": 19363.64, "end": 19365.64, "text": " of these classes and subclasses", "tokens": [50664, 295, 613, 5359, 293, 1422, 11665, 279, 50764], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6692, "seek": 1935764, "start": 19365.64, "end": 19367.64, "text": " nn.module basically works", "tokens": [50764, 297, 77, 13, 8014, 2271, 1936, 1985, 50864], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6693, "seek": 1935764, "start": 19367.64, "end": 19369.64, "text": " as a tracker for all of your", "tokens": [50864, 382, 257, 37516, 337, 439, 295, 428, 50964], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6694, "seek": 1935764, "start": 19369.64, "end": 19371.64, "text": " parameters it makes", "tokens": [50964, 9834, 309, 1669, 51064], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6695, "seek": 1935764, "start": 19371.64, "end": 19373.64, "text": " make sure that all of your", "tokens": [51064, 652, 988, 300, 439, 295, 428, 51164], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6696, "seek": 1935764, "start": 19373.64, "end": 19375.64, "text": " nn extensions run correctly", "tokens": [51164, 297, 77, 25129, 1190, 8944, 51264], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6697, "seek": 1935764, "start": 19375.64, "end": 19377.64, "text": " and just overall a cornerstone", "tokens": [51264, 293, 445, 4787, 257, 4538, 11243, 51364], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6698, "seek": 1935764, "start": 19377.64, "end": 19379.64, "text": " for PyTorch like you need it", "tokens": [51364, 337, 9953, 51, 284, 339, 411, 291, 643, 309, 51464], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6699, "seek": 1935764, "start": 19379.64, "end": 19381.64, "text": " so make sure you have nn-module in all of these classes", "tokens": [51464, 370, 652, 988, 291, 362, 297, 77, 12, 8014, 2271, 294, 439, 295, 613, 5359, 51564], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6700, "seek": 1935764, "start": 19381.64, "end": 19383.64, "text": " I know that", "tokens": [51564, 286, 458, 300, 51664], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6701, "seek": 1935764, "start": 19383.64, "end": 19385.64, "text": " block sort of comes out of GPT", "tokens": [51664, 3461, 1333, 295, 1487, 484, 295, 26039, 51, 51764], "temperature": 0.0, "avg_logprob": -0.12067061431648195, "compression_ratio": 1.8391304347826087, "no_speech_prob": 0.004330288618803024}, {"id": 6702, "seek": 1938564, "start": 19385.64, "end": 19387.64, "text": " language model and so on and so forth", "tokens": [50364, 2856, 2316, 293, 370, 322, 293, 370, 5220, 50464], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6703, "seek": 1938564, "start": 19387.64, "end": 19389.64, "text": " but just all of these", "tokens": [50464, 457, 445, 439, 295, 613, 50564], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6704, "seek": 1938564, "start": 19389.64, "end": 19391.64, "text": " classes with nn", "tokens": [50564, 5359, 365, 297, 77, 50664], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6705, "seek": 1938564, "start": 19391.64, "end": 19393.64, "text": " or any learnable parameters", "tokens": [50664, 420, 604, 1466, 712, 9834, 50764], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6706, "seek": 1938564, "start": 19393.64, "end": 19395.64, "text": " you will need it in it's overall just", "tokens": [50764, 291, 486, 643, 309, 294, 309, 311, 4787, 445, 50864], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6707, "seek": 1938564, "start": 19395.64, "end": 19397.64, "text": " a good practice to have nn-module in all", "tokens": [50864, 257, 665, 3124, 281, 362, 297, 77, 12, 8014, 2271, 294, 439, 50964], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6708, "seek": 1938564, "start": 19397.64, "end": 19399.64, "text": " of your classes overall", "tokens": [50964, 295, 428, 5359, 4787, 51064], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6709, "seek": 1938564, "start": 19399.64, "end": 19401.64, "text": " just to sort of avoid those errors", "tokens": [51064, 445, 281, 1333, 295, 5042, 729, 13603, 51164], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6710, "seek": 1938564, "start": 19401.64, "end": 19403.64, "text": " so cool", "tokens": [51164, 370, 1627, 51264], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6711, "seek": 1938564, "start": 19403.64, "end": 19405.64, "text": " I didn't explicitly go over that", "tokens": [51264, 286, 994, 380, 20803, 352, 670, 300, 51364], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6712, "seek": 1938564, "start": 19405.64, "end": 19407.64, "text": " at the beginning but that's just a heads up", "tokens": [51364, 412, 264, 2863, 457, 300, 311, 445, 257, 8050, 493, 51464], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6713, "seek": 1938564, "start": 19407.64, "end": 19409.64, "text": " you always want to make sure nn-module is inside of these", "tokens": [51464, 291, 1009, 528, 281, 652, 988, 297, 77, 12, 8014, 2271, 307, 1854, 295, 613, 51564], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6714, "seek": 1938564, "start": 19409.64, "end": 19411.64, "text": " so cool", "tokens": [51564, 370, 1627, 51664], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6715, "seek": 1938564, "start": 19411.64, "end": 19413.64, "text": " now", "tokens": [51664, 586, 51764], "temperature": 0.0, "avg_logprob": -0.07492237248696572, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.018250035122036934}, {"id": 6716, "seek": 1941364, "start": 19413.64, "end": 19415.64, "text": " something I'd like to highlight", "tokens": [50364, 746, 286, 1116, 411, 281, 5078, 50464], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6717, "seek": 1941364, "start": 19415.64, "end": 19417.64, "text": " is a little error that we get", "tokens": [50464, 307, 257, 707, 6713, 300, 321, 483, 50564], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6718, "seek": 1941364, "start": 19417.64, "end": 19419.64, "text": " we try to generate when we have max new", "tokens": [50564, 321, 853, 281, 8460, 562, 321, 362, 11469, 777, 50664], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6719, "seek": 1941364, "start": 19419.64, "end": 19421.64, "text": " tokens above block size so let me show you", "tokens": [50664, 22667, 3673, 3461, 2744, 370, 718, 385, 855, 291, 50764], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6720, "seek": 1941364, "start": 19421.64, "end": 19423.64, "text": " that right now", "tokens": [50764, 300, 558, 586, 50864], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6721, "seek": 1941364, "start": 19423.64, "end": 19425.64, "text": " you just go python, chat bot", "tokens": [50864, 291, 445, 352, 38797, 11, 5081, 10592, 50964], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6722, "seek": 1941364, "start": 19425.64, "end": 19427.64, "text": " and then batch size 32", "tokens": [50964, 293, 550, 15245, 2744, 8858, 51064], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6723, "seek": 1941364, "start": 19427.64, "end": 19429.64, "text": " so we could say", "tokens": [51064, 370, 321, 727, 584, 51164], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6724, "seek": 1941364, "start": 19429.64, "end": 19431.64, "text": " we could say hello", "tokens": [51164, 321, 727, 584, 7751, 51264], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6725, "seek": 1941364, "start": 19431.64, "end": 19433.64, "text": " for example", "tokens": [51264, 337, 1365, 51364], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6726, "seek": 1941364, "start": 19435.64, "end": 19437.64, "text": " okay so it's going to give us", "tokens": [51464, 1392, 370, 309, 311, 516, 281, 976, 505, 51564], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6727, "seek": 1941364, "start": 19437.64, "end": 19439.64, "text": " some errors here and what exactly", "tokens": [51564, 512, 13603, 510, 293, 437, 2293, 51664], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6728, "seek": 1941364, "start": 19439.64, "end": 19441.64, "text": " does this error mean", "tokens": [51664, 775, 341, 6713, 914, 51764], "temperature": 0.0, "avg_logprob": -0.14112451964733647, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.00453704409301281}, {"id": 6729, "seek": 1944164, "start": 19441.64, "end": 19443.64, "text": " well when we try to", "tokens": [50364, 731, 562, 321, 853, 281, 50464], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6730, "seek": 1944164, "start": 19443.64, "end": 19445.64, "text": " generate 150 new", "tokens": [50464, 8460, 8451, 777, 50564], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6731, "seek": 1944164, "start": 19445.64, "end": 19447.64, "text": " tokens what it's doing", "tokens": [50564, 22667, 437, 309, 311, 884, 50664], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6732, "seek": 1944164, "start": 19447.64, "end": 19449.64, "text": " is it's taking the previous", "tokens": [50664, 307, 309, 311, 1940, 264, 3894, 50764], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6733, "seek": 1944164, "start": 19449.64, "end": 19451.64, "text": " you know", "tokens": [50764, 291, 458, 50864], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6734, "seek": 1944164, "start": 19451.64, "end": 19453.64, "text": " H-E-L-L-O", "tokens": [50864, 389, 12, 36, 12, 43, 12, 43, 12, 46, 50964], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6735, "seek": 1944164, "start": 19453.64, "end": 19455.64, "text": " exclamation mark 6 tokens", "tokens": [50964, 1624, 43233, 1491, 1386, 22667, 51064], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6736, "seek": 1944164, "start": 19455.64, "end": 19457.64, "text": " and it's pretty much adding up 150", "tokens": [51064, 293, 309, 311, 1238, 709, 5127, 493, 8451, 51164], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6737, "seek": 1944164, "start": 19457.64, "end": 19459.64, "text": " on top of that so we have", "tokens": [51164, 322, 1192, 295, 300, 370, 321, 362, 51264], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6738, "seek": 1944164, "start": 19459.64, "end": 19461.64, "text": " 156 tokens", "tokens": [51264, 2119, 21, 22667, 51364], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6739, "seek": 1944164, "start": 19461.64, "end": 19463.64, "text": " that we're now trying to fit inside of block size", "tokens": [51364, 300, 321, 434, 586, 1382, 281, 3318, 1854, 295, 3461, 2744, 51464], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6740, "seek": 1944164, "start": 19463.64, "end": 19465.64, "text": " which in our case is", "tokens": [51464, 597, 294, 527, 1389, 307, 51564], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6741, "seek": 1944164, "start": 19465.64, "end": 19467.64, "text": " 128", "tokens": [51564, 29810, 51664], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6742, "seek": 1944164, "start": 19467.64, "end": 19469.64, "text": " so of course", "tokens": [51664, 370, 295, 1164, 51764], "temperature": 0.0, "avg_logprob": -0.09120088410608977, "compression_ratio": 1.46, "no_speech_prob": 0.013217082247138023}, {"id": 6743, "seek": 1946964, "start": 19469.64, "end": 19471.64, "text": " 156 does not fit", "tokens": [50364, 2119, 21, 775, 406, 3318, 50464], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6744, "seek": 1946964, "start": 19471.64, "end": 19473.64, "text": " into 128 and that's", "tokens": [50464, 666, 29810, 293, 300, 311, 50564], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6745, "seek": 1946964, "start": 19473.64, "end": 19475.64, "text": " why we get some errors here", "tokens": [50564, 983, 321, 483, 512, 13603, 510, 50664], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6746, "seek": 1946964, "start": 19475.64, "end": 19477.64, "text": " so", "tokens": [50664, 370, 50764], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6747, "seek": 1946964, "start": 19477.64, "end": 19479.64, "text": " all we have to do is make sure", "tokens": [50764, 439, 321, 362, 281, 360, 307, 652, 988, 50864], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6748, "seek": 1946964, "start": 19479.64, "end": 19481.64, "text": " that", "tokens": [50864, 300, 50964], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6749, "seek": 1946964, "start": 19481.64, "end": 19483.64, "text": " we essentially", "tokens": [50964, 321, 4476, 51064], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6750, "seek": 1946964, "start": 19483.64, "end": 19485.64, "text": " what we could do is make sure that max new tokens", "tokens": [51064, 437, 321, 727, 360, 307, 652, 988, 300, 11469, 777, 22667, 51164], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6751, "seek": 1946964, "start": 19485.64, "end": 19487.64, "text": " is small enough and then be sort of", "tokens": [51164, 307, 1359, 1547, 293, 550, 312, 1333, 295, 51264], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6752, "seek": 1946964, "start": 19487.64, "end": 19489.64, "text": " paying attention when we make prompts", "tokens": [51264, 6229, 3202, 562, 321, 652, 41095, 51364], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6753, "seek": 1946964, "start": 19489.64, "end": 19491.64, "text": " or", "tokens": [51364, 420, 51464], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6754, "seek": 1946964, "start": 19491.64, "end": 19493.64, "text": " we could actually make a little", "tokens": [51464, 321, 727, 767, 652, 257, 707, 51564], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6755, "seek": 1946964, "start": 19493.64, "end": 19495.64, "text": " cropping", "tokens": [51564, 4848, 3759, 51664], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6756, "seek": 1946964, "start": 19495.64, "end": 19497.64, "text": " cropping tool here so what this will do", "tokens": [51664, 4848, 3759, 2290, 510, 370, 437, 341, 486, 360, 51764], "temperature": 0.0, "avg_logprob": -0.05854076914267965, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.004069629590958357}, {"id": 6757, "seek": 1949764, "start": 19497.64, "end": 19499.64, "text": " is it will pretty much crop", "tokens": [50364, 307, 309, 486, 1238, 709, 9086, 50464], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6758, "seek": 1949764, "start": 19499.64, "end": 19501.64, "text": " through the last block size tokens", "tokens": [50464, 807, 264, 1036, 3461, 2744, 22667, 50564], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6759, "seek": 1949764, "start": 19501.64, "end": 19503.64, "text": " and", "tokens": [50564, 293, 50664], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6760, "seek": 1949764, "start": 19503.64, "end": 19505.64, "text": " this is super useful because it", "tokens": [50664, 341, 307, 1687, 4420, 570, 309, 50764], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6761, "seek": 1949764, "start": 19505.64, "end": 19507.64, "text": " pretty much doesn't make us have to pay", "tokens": [50764, 1238, 709, 1177, 380, 652, 505, 362, 281, 1689, 50864], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6762, "seek": 1949764, "start": 19507.64, "end": 19509.64, "text": " attention to max new tokens all the time", "tokens": [50864, 3202, 281, 11469, 777, 22667, 439, 264, 565, 50964], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6763, "seek": 1949764, "start": 19509.64, "end": 19511.64, "text": " and it just essentially", "tokens": [50964, 293, 309, 445, 4476, 51064], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6764, "seek": 1949764, "start": 19511.64, "end": 19513.64, "text": " crops it around that 128 limit", "tokens": [51064, 16829, 309, 926, 300, 29810, 4948, 51164], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6765, "seek": 1949764, "start": 19513.64, "end": 19515.64, "text": " so", "tokens": [51164, 370, 51264], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6766, "seek": 1949764, "start": 19515.64, "end": 19517.64, "text": " I'm going to go ahead and replace index here", "tokens": [51264, 286, 478, 516, 281, 352, 2286, 293, 7406, 8186, 510, 51364], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6767, "seek": 1949764, "start": 19517.64, "end": 19519.64, "text": " with index con or index condition", "tokens": [51364, 365, 8186, 416, 420, 8186, 4188, 51464], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6768, "seek": 1949764, "start": 19519.64, "end": 19521.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6769, "seek": 1949764, "start": 19521.64, "end": 19523.64, "text": " we go ahead and run this again", "tokens": [51564, 321, 352, 2286, 293, 1190, 341, 797, 51664], "temperature": 0.0, "avg_logprob": -0.0835592795391472, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0038828286342322826}, {"id": 6770, "seek": 1952764, "start": 19527.64, "end": 19529.64, "text": " so I could say hello", "tokens": [50364, 370, 286, 727, 584, 7751, 50464], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6771, "seek": 1952764, "start": 19531.64, "end": 19533.64, "text": " and we get a successful", "tokens": [50564, 293, 321, 483, 257, 4406, 50664], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6772, "seek": 1952764, "start": 19533.64, "end": 19535.64, "text": " completion awesome", "tokens": [50664, 19372, 3476, 50764], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6773, "seek": 1952764, "start": 19535.64, "end": 19537.64, "text": " we can keep asking new prompts like this", "tokens": [50764, 321, 393, 1066, 3365, 777, 41095, 411, 341, 50864], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6774, "seek": 1952764, "start": 19537.64, "end": 19539.64, "text": " right", "tokens": [50864, 558, 50964], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6775, "seek": 1952764, "start": 19543.64, "end": 19545.64, "text": " and awesome so", "tokens": [51164, 293, 3476, 370, 51264], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6776, "seek": 1952764, "start": 19545.64, "end": 19547.64, "text": " yeah we're not really getting any of these", "tokens": [51264, 1338, 321, 434, 406, 534, 1242, 604, 295, 613, 51364], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6777, "seek": 1952764, "start": 19547.64, "end": 19549.64, "text": " dimensionality like", "tokens": [51364, 10139, 1860, 411, 51464], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6778, "seek": 1952764, "start": 19549.64, "end": 19551.64, "text": " architecture fitting type errors if you want to call them", "tokens": [51464, 9482, 15669, 2010, 13603, 498, 291, 528, 281, 818, 552, 51564], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6779, "seek": 1952764, "start": 19551.64, "end": 19553.64, "text": " if you want to make it super fancy that way", "tokens": [51564, 498, 291, 528, 281, 652, 309, 1687, 10247, 300, 636, 51664], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6780, "seek": 1952764, "start": 19553.64, "end": 19555.64, "text": " but yeah", "tokens": [51664, 457, 1338, 51764], "temperature": 0.0, "avg_logprob": -0.12438975310907131, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0021153930574655533}, {"id": 6781, "seek": 1955564, "start": 19555.64, "end": 19557.64, "text": " not really that much else to do", "tokens": [50364, 406, 534, 300, 709, 1646, 281, 360, 50464], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6782, "seek": 1955564, "start": 19557.64, "end": 19559.64, "text": " yeah there's a few points I want to go over", "tokens": [50464, 1338, 456, 311, 257, 1326, 2793, 286, 528, 281, 352, 670, 50564], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6783, "seek": 1955564, "start": 19559.64, "end": 19561.64, "text": " including fine tuning", "tokens": [50564, 3009, 2489, 15164, 50664], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6784, "seek": 1955564, "start": 19561.64, "end": 19563.64, "text": " so I'm going to go over a little", "tokens": [50664, 370, 286, 478, 516, 281, 352, 670, 257, 707, 50764], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6785, "seek": 1955564, "start": 19563.64, "end": 19565.64, "text": " illustrative example as to what", "tokens": [50764, 8490, 30457, 1365, 382, 281, 437, 50864], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6786, "seek": 1955564, "start": 19565.64, "end": 19567.64, "text": " fine tuning actually looks like in practice", "tokens": [50864, 2489, 15164, 767, 1542, 411, 294, 3124, 50964], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6787, "seek": 1955564, "start": 19567.64, "end": 19569.64, "text": " so in pre-training", "tokens": [50964, 370, 294, 659, 12, 17227, 1760, 51064], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6788, "seek": 1955564, "start": 19569.64, "end": 19571.64, "text": " which is what this course is based off of", "tokens": [51064, 597, 307, 437, 341, 1164, 307, 2361, 766, 295, 51164], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6789, "seek": 1955564, "start": 19571.64, "end": 19573.64, "text": " in pre-training you have this", "tokens": [51164, 294, 659, 12, 17227, 1760, 291, 362, 341, 51264], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6790, "seek": 1955564, "start": 19573.64, "end": 19575.64, "text": " giant text corpus right you have this", "tokens": [51264, 7410, 2487, 1181, 31624, 558, 291, 362, 341, 51364], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6791, "seek": 1955564, "start": 19575.64, "end": 19577.64, "text": " giant corpus here", "tokens": [51364, 7410, 1181, 31624, 510, 51464], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6792, "seek": 1955564, "start": 19579.64, "end": 19581.64, "text": " some text in it", "tokens": [51564, 512, 2487, 294, 309, 51664], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6793, "seek": 1955564, "start": 19581.64, "end": 19583.64, "text": " and essentially", "tokens": [51664, 293, 4476, 51764], "temperature": 0.0, "avg_logprob": -0.0918172938483102, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0037638230714946985}, {"id": 6794, "seek": 1958364, "start": 19583.64, "end": 19585.64, "text": " what you do is you take out little snippets", "tokens": [50364, 437, 291, 360, 307, 291, 747, 484, 707, 35623, 1385, 50464], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6795, "seek": 1958364, "start": 19585.64, "end": 19587.64, "text": " these are called", "tokens": [50464, 613, 366, 1219, 50564], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6796, "seek": 1958364, "start": 19587.64, "end": 19589.64, "text": " blocks or batches", "tokens": [50564, 8474, 420, 15245, 279, 50664], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6797, "seek": 1958364, "start": 19589.64, "end": 19591.64, "text": " or chunks you could say you take out little batches", "tokens": [50664, 420, 24004, 291, 727, 584, 291, 747, 484, 707, 15245, 279, 50764], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6798, "seek": 1958364, "start": 19591.64, "end": 19593.64, "text": " of these you sample", "tokens": [50764, 295, 613, 291, 6889, 50864], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6799, "seek": 1958364, "start": 19593.64, "end": 19595.64, "text": " random little blocks and you take multiple batches", "tokens": [50864, 4974, 707, 8474, 293, 291, 747, 3866, 15245, 279, 50964], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6800, "seek": 1958364, "start": 19595.64, "end": 19597.64, "text": " of them and", "tokens": [50964, 295, 552, 293, 51064], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6801, "seek": 1958364, "start": 19597.64, "end": 19599.64, "text": " you essentially have this", "tokens": [51064, 291, 4476, 362, 341, 51164], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6802, "seek": 1958364, "start": 19599.64, "end": 19601.64, "text": " let's just say", "tokens": [51164, 718, 311, 445, 584, 51264], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6803, "seek": 1958364, "start": 19601.64, "end": 19603.64, "text": " H E L L O", "tokens": [51264, 389, 462, 441, 441, 422, 51364], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6804, "seek": 1958364, "start": 19603.64, "end": 19605.64, "text": " and maybe the next", "tokens": [51364, 293, 1310, 264, 958, 51464], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6805, "seek": 1958364, "start": 19605.64, "end": 19607.64, "text": " predict maybe the outputs", "tokens": [51464, 6069, 1310, 264, 23930, 51564], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6806, "seek": 1958364, "start": 19607.64, "end": 19609.64, "text": " or the targets rather", "tokens": [51564, 420, 264, 12911, 2831, 51664], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6807, "seek": 1958364, "start": 19609.64, "end": 19611.64, "text": " or", "tokens": [51664, 420, 51764], "temperature": 0.0, "avg_logprob": -0.10655563354492187, "compression_ratio": 1.7956989247311828, "no_speech_prob": 0.024029040709137917}, {"id": 6808, "seek": 1961164, "start": 19611.64, "end": 19613.64, "text": " the L L O", "tokens": [50364, 264, 441, 441, 422, 50464], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6809, "seek": 1961164, "start": 19613.64, "end": 19615.64, "text": " exclamation mark", "tokens": [50464, 1624, 43233, 1491, 50564], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6810, "seek": 1961164, "start": 19615.64, "end": 19617.64, "text": " so it's just shifted over by one", "tokens": [50564, 370, 309, 311, 445, 18892, 670, 538, 472, 50664], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6811, "seek": 1961164, "start": 19617.64, "end": 19619.64, "text": " and so given this", "tokens": [50664, 293, 370, 2212, 341, 50764], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6812, "seek": 1961164, "start": 19619.64, "end": 19621.64, "text": " sequence of characters", "tokens": [50764, 8310, 295, 4342, 50864], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6813, "seek": 1961164, "start": 19621.64, "end": 19623.64, "text": " you want to predict this which is just", "tokens": [50864, 291, 528, 281, 6069, 341, 597, 307, 445, 50964], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6814, "seek": 1961164, "start": 19623.64, "end": 19625.64, "text": " the input shifted by one", "tokens": [50964, 264, 4846, 18892, 538, 472, 51064], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6815, "seek": 1961164, "start": 19625.64, "end": 19627.64, "text": " that's what pre-training is", "tokens": [51064, 300, 311, 437, 659, 12, 17227, 1760, 307, 51164], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6816, "seek": 1961164, "start": 19627.64, "end": 19629.64, "text": " and keep in mind that these are the same size", "tokens": [51164, 293, 1066, 294, 1575, 300, 613, 366, 264, 912, 2744, 51264], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6817, "seek": 1961164, "start": 19629.64, "end": 19631.64, "text": " this is one, two,", "tokens": [51264, 341, 307, 472, 11, 732, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6818, "seek": 1961164, "start": 19631.64, "end": 19633.64, "text": " three, four, and five", "tokens": [51364, 1045, 11, 1451, 11, 293, 1732, 51464], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6819, "seek": 1961164, "start": 19633.64, "end": 19635.64, "text": " same thing here these are both", "tokens": [51464, 912, 551, 510, 613, 366, 1293, 51564], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6820, "seek": 1961164, "start": 19635.64, "end": 19637.64, "text": " five characters long", "tokens": [51564, 1732, 4342, 938, 51664], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6821, "seek": 1961164, "start": 19637.64, "end": 19639.64, "text": " fine tuning however is not completely the same", "tokens": [51664, 2489, 15164, 4461, 307, 406, 2584, 264, 912, 51764], "temperature": 0.0, "avg_logprob": -0.10122491632189069, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.0029795467853546143}, {"id": 6822, "seek": 1963964, "start": 19639.64, "end": 19641.64, "text": " so I could have", "tokens": [50364, 370, 286, 727, 362, 50464], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6823, "seek": 1963964, "start": 19641.64, "end": 19643.64, "text": " hello", "tokens": [50464, 7751, 50564], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6824, "seek": 1963964, "start": 19643.64, "end": 19645.64, "text": " and then maybe like a question mark", "tokens": [50564, 293, 550, 1310, 411, 257, 1168, 1491, 50664], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6825, "seek": 1963964, "start": 19645.64, "end": 19647.64, "text": " and it would respond", "tokens": [50664, 293, 309, 576, 4196, 50764], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6826, "seek": 1963964, "start": 19647.64, "end": 19649.64, "text": " you know", "tokens": [50764, 291, 458, 50864], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6827, "seek": 1963964, "start": 19651.64, "end": 19653.64, "text": " the model might respond", "tokens": [50964, 264, 2316, 1062, 4196, 51064], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6828, "seek": 1963964, "start": 19653.64, "end": 19655.64, "text": " L R U", "tokens": [51064, 441, 497, 624, 51164], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6829, "seek": 1963964, "start": 19655.64, "end": 19657.64, "text": " maybe that's just a", "tokens": [51164, 1310, 300, 311, 445, 257, 51264], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6830, "seek": 1963964, "start": 19657.64, "end": 19659.64, "text": " a response that it gives us", "tokens": [51264, 257, 4134, 300, 309, 2709, 505, 51364], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6831, "seek": 1963964, "start": 19659.64, "end": 19661.64, "text": " we can obviously see that hello does not have the same amount of characters", "tokens": [51364, 321, 393, 2745, 536, 300, 7751, 775, 406, 362, 264, 912, 2372, 295, 4342, 51464], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6832, "seek": 1963964, "start": 19661.64, "end": 19663.64, "text": " with the same amount of indices", "tokens": [51464, 365, 264, 912, 2372, 295, 43840, 51564], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6833, "seek": 1963964, "start": 19663.64, "end": 19665.64, "text": " as how are you", "tokens": [51564, 382, 577, 366, 291, 51664], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6834, "seek": 1963964, "start": 19665.64, "end": 19667.64, "text": " so", "tokens": [51664, 370, 51764], "temperature": 0.0, "avg_logprob": -0.08556841196638815, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.011149204336106777}, {"id": 6835, "seek": 1966764, "start": 19667.64, "end": 19669.64, "text": " this is essentially the difference between", "tokens": [50364, 341, 307, 4476, 264, 2649, 1296, 50464], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6836, "seek": 1966764, "start": 19669.64, "end": 19671.64, "text": " fine tuning and pre-training", "tokens": [50464, 2489, 15164, 293, 659, 12, 17227, 1760, 50564], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6837, "seek": 1966764, "start": 19671.64, "end": 19673.64, "text": " with fine tuning you just have to add a little bit of", "tokens": [50564, 365, 2489, 15164, 291, 445, 362, 281, 909, 257, 707, 857, 295, 50664], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6838, "seek": 1966764, "start": 19673.64, "end": 19675.64, "text": " different things in your generate function", "tokens": [50664, 819, 721, 294, 428, 8460, 2445, 50764], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6839, "seek": 1966764, "start": 19675.64, "end": 19677.64, "text": " to compensate for not having", "tokens": [50764, 281, 29458, 337, 406, 1419, 50864], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6840, "seek": 1966764, "start": 19677.64, "end": 19679.64, "text": " the same", "tokens": [50864, 264, 912, 50964], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6841, "seek": 1966764, "start": 19679.64, "end": 19681.64, "text": " amount of indices in your inputs", "tokens": [50964, 2372, 295, 43840, 294, 428, 15743, 51064], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6842, "seek": 1966764, "start": 19681.64, "end": 19683.64, "text": " and targets and rather just", "tokens": [51064, 293, 12911, 293, 2831, 445, 51164], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6843, "seek": 1966764, "start": 19683.64, "end": 19685.64, "text": " generate until you receive an end token", "tokens": [51164, 8460, 1826, 291, 4774, 364, 917, 14862, 51264], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6844, "seek": 1966764, "start": 19685.64, "end": 19687.64, "text": " so", "tokens": [51264, 370, 51364], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6845, "seek": 1966764, "start": 19687.64, "end": 19689.64, "text": " what they don't explicitly say here is at the", "tokens": [51364, 437, 436, 500, 380, 20803, 584, 510, 307, 412, 264, 51464], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6846, "seek": 1966764, "start": 19689.64, "end": 19691.64, "text": " end of this question", "tokens": [51464, 917, 295, 341, 1168, 51564], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6847, "seek": 1966764, "start": 19691.64, "end": 19693.64, "text": " there's actually a little end token which we usually", "tokens": [51564, 456, 311, 767, 257, 707, 917, 14862, 597, 321, 2673, 51664], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6848, "seek": 1966764, "start": 19693.64, "end": 19695.64, "text": " do", "tokens": [51664, 360, 51764], "temperature": 0.0, "avg_logprob": -0.0664644283907754, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0016735059907659888}, {"id": 6849, "seek": 1969564, "start": 19695.64, "end": 19697.64, "text": " looks like this", "tokens": [50364, 1542, 411, 341, 50464], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6850, "seek": 1969564, "start": 19697.64, "end": 19699.64, "text": " like that", "tokens": [50464, 411, 300, 50564], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6851, "seek": 1969564, "start": 19699.64, "end": 19701.64, "text": " or", "tokens": [50564, 420, 50664], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6852, "seek": 1969564, "start": 19701.64, "end": 19703.64, "text": " like this", "tokens": [50664, 411, 341, 50764], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6853, "seek": 1969564, "start": 19703.64, "end": 19705.64, "text": " these are end tokens and then you typically", "tokens": [50764, 613, 366, 917, 22667, 293, 550, 291, 5850, 50864], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6854, "seek": 1969564, "start": 19705.64, "end": 19707.64, "text": " have the same for start tokens like an s", "tokens": [50864, 362, 264, 912, 337, 722, 22667, 411, 364, 262, 50964], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6855, "seek": 1969564, "start": 19707.64, "end": 19709.64, "text": " or", "tokens": [50964, 420, 51064], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6856, "seek": 1969564, "start": 19709.64, "end": 19711.64, "text": " start", "tokens": [51064, 722, 51164], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6857, "seek": 1969564, "start": 19711.64, "end": 19713.64, "text": " like that, pretty simple", "tokens": [51164, 411, 300, 11, 1238, 2199, 51264], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6858, "seek": 1969564, "start": 19713.64, "end": 19715.64, "text": " and essentially you would just append them", "tokens": [51264, 293, 4476, 291, 576, 445, 34116, 552, 51364], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6859, "seek": 1969564, "start": 19717.64, "end": 19719.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6860, "seek": 1969564, "start": 19719.64, "end": 19721.64, "text": " a start token", "tokens": [51564, 257, 722, 14862, 51664], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6861, "seek": 1969564, "start": 19721.64, "end": 19723.64, "text": " the start token doesn't matter as much", "tokens": [51664, 264, 722, 14862, 1177, 380, 1871, 382, 709, 51764], "temperature": 0.0, "avg_logprob": -0.1416445864906794, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.017702413722872734}, {"id": 6862, "seek": 1972364, "start": 19723.64, "end": 19725.64, "text": " as we essentially just are looking at", "tokens": [50364, 382, 321, 4476, 445, 366, 1237, 412, 50464], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6863, "seek": 1972364, "start": 19725.64, "end": 19727.64, "text": " what this does and then", "tokens": [50464, 437, 341, 775, 293, 550, 50564], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6864, "seek": 1972364, "start": 19727.64, "end": 19729.64, "text": " we start generating the start doesn't really", "tokens": [50564, 321, 722, 17746, 264, 722, 1177, 380, 534, 50664], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6865, "seek": 1972364, "start": 19729.64, "end": 19731.64, "text": " matter because", "tokens": [50664, 1871, 570, 50764], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6866, "seek": 1972364, "start": 19731.64, "end": 19733.64, "text": " we don't really need to know when to start generating", "tokens": [50764, 321, 500, 380, 534, 643, 281, 458, 562, 281, 722, 17746, 50864], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6867, "seek": 1972364, "start": 19733.64, "end": 19735.64, "text": " it just happens but the end token is", "tokens": [50864, 309, 445, 2314, 457, 264, 917, 14862, 307, 50964], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6868, "seek": 1972364, "start": 19735.64, "end": 19737.64, "text": " important because we don't want to just generate", "tokens": [50964, 1021, 570, 321, 500, 380, 528, 281, 445, 8460, 51064], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6869, "seek": 1972364, "start": 19737.64, "end": 19739.64, "text": " an infinite number of tokens", "tokens": [51064, 364, 13785, 1230, 295, 22667, 51164], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6870, "seek": 1972364, "start": 19739.64, "end": 19741.64, "text": " because these aren't the same size", "tokens": [51164, 570, 613, 3212, 380, 264, 912, 2744, 51264], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6871, "seek": 1972364, "start": 19741.64, "end": 19743.64, "text": " it could theoretically generate a really", "tokens": [51264, 309, 727, 29400, 8460, 257, 534, 51364], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6872, "seek": 1972364, "start": 19743.64, "end": 19745.64, "text": " really long completion", "tokens": [51364, 534, 938, 19372, 51464], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6873, "seek": 1972364, "start": 19745.64, "end": 19747.64, "text": " so all we want to make sure", "tokens": [51464, 370, 439, 321, 528, 281, 652, 988, 51564], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6874, "seek": 1972364, "start": 19747.64, "end": 19749.64, "text": " is that it's not generating an infinite amount of tokens", "tokens": [51564, 307, 300, 309, 311, 406, 17746, 364, 13785, 2372, 295, 22667, 51664], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6875, "seek": 1972364, "start": 19749.64, "end": 19751.64, "text": " consuming an infinite amount of computation", "tokens": [51664, 19867, 364, 13785, 2372, 295, 24903, 51764], "temperature": 0.0, "avg_logprob": -0.1001375961303711, "compression_ratio": 2.0803212851405624, "no_speech_prob": 0.006899756845086813}, {"id": 6876, "seek": 1975164, "start": 19751.64, "end": 19753.64, "text": " and just to prevent that loop", "tokens": [50364, 293, 445, 281, 4871, 300, 6367, 50464], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6877, "seek": 1975164, "start": 19753.64, "end": 19755.64, "text": " so that's why we append this end token", "tokens": [50464, 370, 300, 311, 983, 321, 34116, 341, 917, 14862, 50564], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6878, "seek": 1975164, "start": 19755.64, "end": 19757.64, "text": " to the end here", "tokens": [50564, 281, 264, 917, 510, 50664], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6879, "seek": 1975164, "start": 19759.64, "end": 19761.64, "text": " we have this little end bit", "tokens": [50764, 321, 362, 341, 707, 917, 857, 50864], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6880, "seek": 1975164, "start": 19761.64, "end": 19763.64, "text": " and", "tokens": [50864, 293, 50964], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6881, "seek": 1975164, "start": 19763.64, "end": 19765.64, "text": " essentially once this end token is sampled", "tokens": [50964, 4476, 1564, 341, 917, 14862, 307, 3247, 15551, 51064], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6882, "seek": 1975164, "start": 19765.64, "end": 19767.64, "text": " you would end the generation", "tokens": [51064, 291, 576, 917, 264, 5125, 51164], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6883, "seek": 1975164, "start": 19767.64, "end": 19769.64, "text": " simple as that", "tokens": [51164, 2199, 382, 300, 51264], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6884, "seek": 1975164, "start": 19769.64, "end": 19771.64, "text": " and we don't actually", "tokens": [51264, 293, 321, 500, 380, 767, 51364], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6885, "seek": 1975164, "start": 19771.64, "end": 19773.64, "text": " sample from the token itself", "tokens": [51364, 6889, 490, 264, 14862, 2564, 51464], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6886, "seek": 1975164, "start": 19773.64, "end": 19775.64, "text": " but rather the actual", "tokens": [51464, 457, 2831, 264, 3539, 51564], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6887, "seek": 1975164, "start": 19775.64, "end": 19777.64, "text": " the", "tokens": [51564, 264, 51664], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6888, "seek": 1975164, "start": 19777.64, "end": 19779.64, "text": " I guess you could say index", "tokens": [51664, 286, 2041, 291, 727, 584, 8186, 51764], "temperature": 0.0, "avg_logprob": -0.10094895205654941, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007812219206243753}, {"id": 6889, "seek": 1977964, "start": 19779.64, "end": 19781.64, "text": " or the miracle value", "tokens": [50364, 420, 264, 14660, 2158, 50464], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6890, "seek": 1977964, "start": 19781.64, "end": 19783.64, "text": " the encoded version of end", "tokens": [50464, 264, 2058, 12340, 3037, 295, 917, 50564], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6891, "seek": 1977964, "start": 19783.64, "end": 19785.64, "text": " which", "tokens": [50564, 597, 50664], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6892, "seek": 1977964, "start": 19785.64, "end": 19787.64, "text": " is usually just going to be the length of your vocab", "tokens": [50664, 307, 2673, 445, 516, 281, 312, 264, 4641, 295, 428, 2329, 455, 50764], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6893, "seek": 1977964, "start": 19787.64, "end": 19789.64, "text": " size", "tokens": [50764, 2744, 50864], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6894, "seek": 1977964, "start": 19789.64, "end": 19791.64, "text": " plus one", "tokens": [50864, 1804, 472, 50964], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6895, "seek": 1977964, "start": 19791.64, "end": 19793.64, "text": " so if your vocab size in our case", "tokens": [50964, 370, 498, 428, 2329, 455, 2744, 294, 527, 1389, 51064], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6896, "seek": 1977964, "start": 19793.64, "end": 19795.64, "text": " is like maybe 32,000", "tokens": [51064, 307, 411, 1310, 8858, 11, 1360, 51164], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6897, "seek": 1977964, "start": 19795.64, "end": 19797.64, "text": " your end token would be at index", "tokens": [51164, 428, 917, 14862, 576, 312, 412, 8186, 51264], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6898, "seek": 1977964, "start": 19797.64, "end": 19799.64, "text": " 32,001", "tokens": [51264, 8858, 11, 628, 16, 51364], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6899, "seek": 1977964, "start": 19799.64, "end": 19801.64, "text": " so that way when you sample", "tokens": [51364, 370, 300, 636, 562, 291, 6889, 51464], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6900, "seek": 1977964, "start": 19801.64, "end": 19803.64, "text": " when you sample an end token", "tokens": [51464, 562, 291, 6889, 364, 917, 14862, 51564], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6901, "seek": 1977964, "start": 19803.64, "end": 19805.64, "text": " when you sample that", "tokens": [51564, 562, 291, 6889, 300, 51664], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6902, "seek": 1977964, "start": 19805.64, "end": 19807.64, "text": " 32,001 token", "tokens": [51664, 8858, 11, 628, 16, 14862, 51764], "temperature": 0.0, "avg_logprob": -0.08130422292971144, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.003822696628049016}, {"id": 6903, "seek": 1980964, "start": 19809.64, "end": 19811.64, "text": " you actually just end the sequence", "tokens": [50364, 291, 767, 445, 917, 264, 8310, 50464], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6904, "seek": 1980964, "start": 19811.64, "end": 19813.64, "text": " and of course when you train", "tokens": [50464, 293, 295, 1164, 562, 291, 3847, 50564], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6905, "seek": 1980964, "start": 19813.64, "end": 19815.64, "text": " your model you're always", "tokens": [50564, 428, 2316, 291, 434, 1009, 50664], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6906, "seek": 1980964, "start": 19815.64, "end": 19817.64, "text": " appending this end token to the end", "tokens": [50664, 724, 2029, 341, 917, 14862, 281, 264, 917, 50764], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6907, "seek": 1980964, "start": 19817.64, "end": 19819.64, "text": " so you get your initial inputs", "tokens": [50764, 370, 291, 483, 428, 5883, 15743, 50864], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6908, "seek": 1980964, "start": 19819.64, "end": 19821.64, "text": " and then inside of either your", "tokens": [50864, 293, 550, 1854, 295, 2139, 428, 50964], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6909, "seek": 1980964, "start": 19821.64, "end": 19823.64, "text": " training data", "tokens": [50964, 3097, 1412, 51064], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6910, "seek": 1980964, "start": 19823.64, "end": 19825.64, "text": " or when you actually are processing it", "tokens": [51064, 420, 562, 291, 767, 366, 9007, 309, 51164], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6911, "seek": 1980964, "start": 19825.64, "end": 19827.64, "text": " and feeding it into that transformer", "tokens": [51164, 293, 12919, 309, 666, 300, 31782, 51264], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6912, "seek": 1980964, "start": 19827.64, "end": 19829.64, "text": " you have some sort of function that's just appending", "tokens": [51264, 291, 362, 512, 1333, 295, 2445, 300, 311, 445, 724, 2029, 51364], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6913, "seek": 1980964, "start": 19829.64, "end": 19831.64, "text": " that little", "tokens": [51364, 300, 707, 51464], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6914, "seek": 1980964, "start": 19831.64, "end": 19833.64, "text": " 32,001 token index", "tokens": [51464, 8858, 11, 628, 16, 14862, 8186, 51564], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6915, "seek": 1980964, "start": 19833.64, "end": 19835.64, "text": " to it", "tokens": [51564, 281, 309, 51664], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6916, "seek": 1980964, "start": 19835.64, "end": 19837.64, "text": " so that's pretty much what fine tuning is", "tokens": [51664, 370, 300, 311, 1238, 709, 437, 2489, 15164, 307, 51764], "temperature": 0.0, "avg_logprob": -0.0612010657787323, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.004262465052306652}, {"id": 6917, "seek": 1983764, "start": 19837.64, "end": 19839.64, "text": " it comes up fine tuning", "tokens": [50364, 309, 1487, 493, 2489, 15164, 50464], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6918, "seek": 1983764, "start": 19839.64, "end": 19841.64, "text": " and the whole process of creating", "tokens": [50464, 293, 264, 1379, 1399, 295, 4084, 50564], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6919, "seek": 1983764, "start": 19841.64, "end": 19843.64, "text": " these giant language models", "tokens": [50564, 613, 7410, 2856, 5245, 50664], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6920, "seek": 1983764, "start": 19843.64, "end": 19845.64, "text": " is to of course help people", "tokens": [50664, 307, 281, 295, 1164, 854, 561, 50764], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6921, "seek": 1983764, "start": 19845.64, "end": 19847.64, "text": " and there's no better way to do that", "tokens": [50764, 293, 456, 311, 572, 1101, 636, 281, 360, 300, 50864], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6922, "seek": 1983764, "start": 19847.64, "end": 19849.64, "text": " than to", "tokens": [50864, 813, 281, 50964], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6923, "seek": 1983764, "start": 19849.64, "end": 19851.64, "text": " literally have all the information", "tokens": [50964, 3736, 362, 439, 264, 1589, 51064], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6924, "seek": 1983764, "start": 19851.64, "end": 19853.64, "text": " that humans have ever known meaning like common crawl", "tokens": [51064, 300, 6255, 362, 1562, 2570, 3620, 411, 2689, 24767, 51164], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6925, "seek": 1983764, "start": 19853.64, "end": 19855.64, "text": " open web text or Wikipedia", "tokens": [51164, 1269, 3670, 2487, 420, 28999, 51264], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6926, "seek": 1983764, "start": 19855.64, "end": 19857.64, "text": " and even research papers", "tokens": [51264, 293, 754, 2132, 10577, 51364], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6927, "seek": 1983764, "start": 19857.64, "end": 19859.64, "text": " pre-training on all of that", "tokens": [51364, 659, 12, 17227, 1760, 322, 439, 295, 300, 51464], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6928, "seek": 1983764, "start": 19859.64, "end": 19861.64, "text": " so just doing again the same size", "tokens": [51464, 370, 445, 884, 797, 264, 912, 2744, 51564], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6929, "seek": 1983764, "start": 19861.64, "end": 19863.64, "text": " and then shift over for targets", "tokens": [51564, 293, 550, 5513, 670, 337, 12911, 51664], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6930, "seek": 1983764, "start": 19863.64, "end": 19865.64, "text": " and then after you've iterated on that", "tokens": [51664, 293, 550, 934, 291, 600, 17138, 770, 322, 300, 51764], "temperature": 0.0, "avg_logprob": -0.09877518363620924, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.04081139713525772}, {"id": 6931, "seek": 1986564, "start": 19865.64, "end": 19867.64, "text": " many many times you switch over to fine tuning", "tokens": [50364, 867, 867, 1413, 291, 3679, 670, 281, 2489, 15164, 50464], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6932, "seek": 1986564, "start": 19867.64, "end": 19869.64, "text": " where you have these", "tokens": [50464, 689, 291, 362, 613, 50564], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6933, "seek": 1986564, "start": 19869.64, "end": 19871.64, "text": " specifically picked out", "tokens": [50564, 4682, 6183, 484, 50664], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6934, "seek": 1986564, "start": 19871.64, "end": 19873.64, "text": " prompt and completion pairs", "tokens": [50664, 12391, 293, 19372, 15494, 50764], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6935, "seek": 1986564, "start": 19873.64, "end": 19875.64, "text": " and you just train on those for a really long time", "tokens": [50764, 293, 291, 445, 3847, 322, 729, 337, 257, 534, 938, 565, 50864], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6936, "seek": 1986564, "start": 19875.64, "end": 19877.64, "text": " until you are satisfied", "tokens": [50864, 1826, 291, 366, 11239, 50964], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6937, "seek": 1986564, "start": 19877.64, "end": 19879.64, "text": " with your result", "tokens": [50964, 365, 428, 1874, 51064], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6938, "seek": 1986564, "start": 19879.64, "end": 19881.64, "text": " and yeah that's what language modeling is", "tokens": [51064, 293, 1338, 300, 311, 437, 2856, 15983, 307, 51164], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6939, "seek": 1986564, "start": 19881.64, "end": 19883.64, "text": " there are a few key pointers I want to leave you with", "tokens": [51164, 456, 366, 257, 1326, 2141, 44548, 286, 528, 281, 1856, 291, 365, 51264], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6940, "seek": 1986564, "start": 19883.64, "end": 19885.64, "text": " before you head on your way to", "tokens": [51264, 949, 291, 1378, 322, 428, 636, 281, 51364], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6941, "seek": 1986564, "start": 19885.64, "end": 19887.64, "text": " research and development and machine learning", "tokens": [51364, 2132, 293, 3250, 293, 3479, 2539, 51464], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6942, "seek": 1986564, "start": 19887.64, "end": 19889.64, "text": " so first things first", "tokens": [51464, 370, 700, 721, 700, 51564], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6943, "seek": 1986564, "start": 19889.64, "end": 19891.64, "text": " there's a little something called", "tokens": [51564, 456, 311, 257, 707, 746, 1219, 51664], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6944, "seek": 1986564, "start": 19891.64, "end": 19893.64, "text": " efficiency testing", "tokens": [51664, 10493, 4997, 51764], "temperature": 0.0, "avg_logprob": -0.0656215406097142, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.028413714841008186}, {"id": 6945, "seek": 1989364, "start": 19893.64, "end": 19895.64, "text": " or just finding out how quickly", "tokens": [50364, 420, 445, 5006, 484, 577, 2661, 50464], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6946, "seek": 1989364, "start": 19895.64, "end": 19897.64, "text": " certain operations takes", "tokens": [50464, 1629, 7705, 2516, 50564], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6947, "seek": 1989364, "start": 19897.64, "end": 19899.64, "text": " we'll just call this", "tokens": [50564, 321, 603, 445, 818, 341, 50664], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6948, "seek": 1989364, "start": 19899.64, "end": 19901.64, "text": " efficiency testing and I'll show you", "tokens": [50664, 10493, 4997, 293, 286, 603, 855, 291, 50764], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6949, "seek": 1989364, "start": 19901.64, "end": 19903.64, "text": " exactly how to do this right here", "tokens": [50764, 2293, 577, 281, 360, 341, 558, 510, 50864], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6950, "seek": 1989364, "start": 19903.64, "end": 19905.64, "text": " efficiency", "tokens": [50864, 10493, 50964], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6951, "seek": 1989364, "start": 19905.64, "end": 19907.64, "text": " yeah", "tokens": [50964, 1338, 51064], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6952, "seek": 1989364, "start": 19907.64, "end": 19909.64, "text": " I don't know if I spelled that correctly", "tokens": [51064, 286, 500, 380, 458, 498, 286, 34388, 300, 8944, 51164], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6953, "seek": 1989364, "start": 19909.64, "end": 19911.64, "text": " I don't know what it's doing now", "tokens": [51164, 286, 500, 380, 458, 437, 309, 311, 884, 586, 51264], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6954, "seek": 1989364, "start": 19911.64, "end": 19913.64, "text": " anyways", "tokens": [51264, 13448, 51364], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6955, "seek": 1989364, "start": 19913.64, "end": 19915.64, "text": " we'll just pop into code here", "tokens": [51364, 321, 603, 445, 1665, 666, 3089, 510, 51464], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6956, "seek": 1989364, "start": 19915.64, "end": 19917.64, "text": " and", "tokens": [51464, 293, 51564], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6957, "seek": 1989364, "start": 19917.64, "end": 19919.64, "text": " essentially", "tokens": [51564, 4476, 51664], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6958, "seek": 1989364, "start": 19919.64, "end": 19921.64, "text": " we'll just do", "tokens": [51664, 321, 603, 445, 360, 51764], "temperature": 0.0, "avg_logprob": -0.11540094665859056, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.04597766324877739}, {"id": 6959, "seek": 1992164, "start": 19921.64, "end": 19923.64, "text": " I don't know", "tokens": [50364, 286, 500, 380, 458, 50464], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6960, "seek": 1992164, "start": 19923.64, "end": 19925.64, "text": " I'm testing", "tokens": [50464, 286, 478, 4997, 50564], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6961, "seek": 1992164, "start": 19925.64, "end": 19927.64, "text": " import time", "tokens": [50564, 974, 565, 50664], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6962, "seek": 1992164, "start": 19927.64, "end": 19929.64, "text": " and", "tokens": [50664, 293, 50764], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6963, "seek": 1992164, "start": 19929.64, "end": 19931.64, "text": " essentially", "tokens": [50764, 4476, 50864], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6964, "seek": 1992164, "start": 19931.64, "end": 19933.64, "text": " all we're going to do is just", "tokens": [50864, 439, 321, 434, 516, 281, 360, 307, 445, 50964], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6965, "seek": 1992164, "start": 19933.64, "end": 19935.64, "text": " time how long operations take", "tokens": [50964, 565, 577, 938, 7705, 747, 51064], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6966, "seek": 1992164, "start": 19935.64, "end": 19937.64, "text": " so", "tokens": [51064, 370, 51164], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6967, "seek": 1992164, "start": 19937.64, "end": 19939.64, "text": " in here you can go", "tokens": [51164, 294, 510, 291, 393, 352, 51264], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6968, "seek": 1992164, "start": 19939.64, "end": 19941.64, "text": " you can go start time", "tokens": [51264, 291, 393, 352, 722, 565, 51364], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6969, "seek": 1992164, "start": 19941.64, "end": 19943.64, "text": " equals time dot time", "tokens": [51364, 6915, 565, 5893, 565, 51464], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6970, "seek": 1992164, "start": 19943.64, "end": 19945.64, "text": " and essentially what this function does", "tokens": [51464, 293, 4476, 437, 341, 2445, 775, 51564], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6971, "seek": 1992164, "start": 19945.64, "end": 19947.64, "text": " is it just takes a look at the current time right now", "tokens": [51564, 307, 309, 445, 2516, 257, 574, 412, 264, 2190, 565, 558, 586, 51664], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6972, "seek": 1992164, "start": 19947.64, "end": 19949.64, "text": " the current like millisecond", "tokens": [51664, 264, 2190, 411, 27940, 18882, 51764], "temperature": 0.0, "avg_logprob": -0.0983826906784721, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.006586804986000061}, {"id": 6973, "seek": 1994964, "start": 19949.64, "end": 19951.64, "text": " very precise", "tokens": [50364, 588, 13600, 50464], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6974, "seek": 1994964, "start": 19951.64, "end": 19953.64, "text": " and we can do some little", "tokens": [50464, 293, 321, 393, 360, 512, 707, 50564], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6975, "seek": 1994964, "start": 19953.64, "end": 19955.64, "text": " operation like", "tokens": [50564, 6916, 411, 50664], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6976, "seek": 1994964, "start": 19955.64, "end": 19957.64, "text": " I don't know 4", "tokens": [50664, 286, 500, 380, 458, 1017, 50764], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6977, "seek": 1994964, "start": 19957.64, "end": 19959.64, "text": " I in range", "tokens": [50764, 286, 294, 3613, 50864], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6978, "seek": 1994964, "start": 19961.64, "end": 19963.64, "text": " we'll just go", "tokens": [50964, 321, 603, 445, 352, 51064], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6979, "seek": 1994964, "start": 19963.64, "end": 19965.64, "text": " 10,000", "tokens": [51064, 1266, 11, 1360, 51164], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6980, "seek": 1994964, "start": 19965.64, "end": 19967.64, "text": " go", "tokens": [51164, 352, 51264], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6981, "seek": 1994964, "start": 19967.64, "end": 19969.64, "text": " print", "tokens": [51264, 4482, 51364], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6982, "seek": 1994964, "start": 19969.64, "end": 19971.64, "text": " I", "tokens": [51364, 286, 51464], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6983, "seek": 1994964, "start": 19971.64, "end": 19973.64, "text": " print I times 2", "tokens": [51464, 4482, 286, 1413, 568, 51564], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6984, "seek": 1994964, "start": 19973.64, "end": 19975.64, "text": " and then we can just end the time here", "tokens": [51564, 293, 550, 321, 393, 445, 917, 264, 565, 510, 51664], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6985, "seek": 1994964, "start": 19975.64, "end": 19977.64, "text": " so go end time", "tokens": [51664, 370, 352, 917, 565, 51764], "temperature": 0.0, "avg_logprob": -0.15377193607696116, "compression_ratio": 1.4140625, "no_speech_prob": 0.016144759953022003}, {"id": 6986, "seek": 1997764, "start": 19977.64, "end": 19979.64, "text": " equals time dot time again", "tokens": [50364, 6915, 565, 5893, 565, 797, 50464], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6987, "seek": 1997764, "start": 19979.64, "end": 19981.64, "text": " calling the current time so we're doing", "tokens": [50464, 5141, 264, 2190, 565, 370, 321, 434, 884, 50564], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6988, "seek": 1997764, "start": 19981.64, "end": 19983.64, "text": " right now versus back then", "tokens": [50564, 558, 586, 5717, 646, 550, 50664], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6989, "seek": 1997764, "start": 19983.64, "end": 19985.64, "text": " and that little difference is how long it took to execute", "tokens": [50664, 293, 300, 707, 2649, 307, 577, 938, 309, 1890, 281, 14483, 50764], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6990, "seek": 1997764, "start": 19985.64, "end": 19987.64, "text": " so all we can do", "tokens": [50764, 370, 439, 321, 393, 360, 50864], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6991, "seek": 1997764, "start": 19987.64, "end": 19989.64, "text": " is just do we can say total time", "tokens": [50864, 307, 445, 360, 321, 393, 584, 3217, 565, 50964], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6992, "seek": 1997764, "start": 19989.64, "end": 19991.64, "text": " we can say total time equals", "tokens": [50964, 321, 393, 584, 3217, 565, 6915, 51064], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6993, "seek": 1997764, "start": 19991.64, "end": 19993.64, "text": " end time", "tokens": [51064, 917, 565, 51164], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6994, "seek": 1997764, "start": 19993.64, "end": 19995.64, "text": " minus start time", "tokens": [51164, 3175, 722, 565, 51264], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6995, "seek": 1997764, "start": 19995.64, "end": 19997.64, "text": " and we'll just go print", "tokens": [51264, 293, 321, 603, 445, 352, 4482, 51364], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6996, "seek": 1997764, "start": 19997.64, "end": 19999.64, "text": " end time", "tokens": [51364, 917, 565, 51464], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6997, "seek": 1997764, "start": 19999.64, "end": 20001.64, "text": " or", "tokens": [51464, 420, 51564], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6998, "seek": 1997764, "start": 20003.64, "end": 20005.64, "text": " I'm taking", "tokens": [51664, 286, 478, 1940, 51764], "temperature": 0.0, "avg_logprob": -0.12512733090308406, "compression_ratio": 1.7514450867052023, "no_speech_prob": 0.004263537470251322}, {"id": 6999, "seek": 2000564, "start": 20005.64, "end": 20007.64, "text": " let's go total", "tokens": [50364, 718, 311, 352, 3217, 50464], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7000, "seek": 2000564, "start": 20009.64, "end": 20011.64, "text": " total time like that", "tokens": [50564, 3217, 565, 411, 300, 50664], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7001, "seek": 2000564, "start": 20011.64, "end": 20013.64, "text": " just execute this", "tokens": [50664, 445, 14483, 341, 50764], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7002, "seek": 2000564, "start": 20015.64, "end": 20017.64, "text": " Python", "tokens": [50864, 15329, 50964], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7003, "seek": 2000564, "start": 20017.64, "end": 20019.64, "text": " time testing", "tokens": [50964, 565, 4997, 51064], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7004, "seek": 2000564, "start": 20019.64, "end": 20021.64, "text": " cool", "tokens": [51064, 1627, 51164], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7005, "seek": 2000564, "start": 20021.64, "end": 20023.64, "text": " time taken 1.32 seconds", "tokens": [51164, 565, 2726, 502, 13, 11440, 3949, 51264], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7006, "seek": 2000564, "start": 20023.64, "end": 20025.64, "text": " so you can essentially time every single operation", "tokens": [51264, 370, 291, 393, 4476, 565, 633, 2167, 6916, 51364], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7007, "seek": 2000564, "start": 20025.64, "end": 20027.64, "text": " you do with this method", "tokens": [51364, 291, 360, 365, 341, 3170, 51464], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7008, "seek": 2000564, "start": 20027.64, "end": 20029.64, "text": " and you can see even in your", "tokens": [51464, 293, 291, 393, 536, 754, 294, 428, 51564], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7009, "seek": 2000564, "start": 20029.64, "end": 20031.64, "text": " I encourage you to actually try this out", "tokens": [51564, 286, 5373, 291, 281, 767, 853, 341, 484, 51664], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7010, "seek": 2000564, "start": 20031.64, "end": 20033.64, "text": " I'm not going to but I encourage you to try out", "tokens": [51664, 286, 478, 406, 516, 281, 457, 286, 5373, 291, 281, 853, 484, 51764], "temperature": 0.0, "avg_logprob": -0.10840668075386135, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.008184041827917099}, {"id": 7011, "seek": 2003364, "start": 20033.64, "end": 20035.64, "text": " how long the model actually takes", "tokens": [50364, 577, 938, 264, 2316, 767, 2516, 50464], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7012, "seek": 2003364, "start": 20035.64, "end": 20037.64, "text": " to do certain things like how long does it take", "tokens": [50464, 281, 360, 1629, 721, 411, 577, 938, 775, 309, 747, 50564], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7013, "seek": 2003364, "start": 20037.64, "end": 20039.64, "text": " to load a model how does it take to save a model", "tokens": [50564, 281, 3677, 257, 2316, 577, 775, 309, 747, 281, 3155, 257, 2316, 50664], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7014, "seek": 2003364, "start": 20039.64, "end": 20041.64, "text": " how long does it take to estimate the loss", "tokens": [50664, 577, 938, 775, 309, 747, 281, 12539, 264, 4470, 50764], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7015, "seek": 2003364, "start": 20041.64, "end": 20043.64, "text": " right", "tokens": [50764, 558, 50864], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7016, "seek": 2003364, "start": 20043.64, "end": 20045.64, "text": " play around with hyperparameters see how long things take", "tokens": [50864, 862, 926, 365, 9848, 2181, 335, 6202, 536, 577, 938, 721, 747, 50964], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7017, "seek": 2003364, "start": 20045.64, "end": 20047.64, "text": " and maybe you'll figure out something new who knows", "tokens": [50964, 293, 1310, 291, 603, 2573, 484, 746, 777, 567, 3255, 51064], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7018, "seek": 2003364, "start": 20047.64, "end": 20049.64, "text": " but this is a little something we use", "tokens": [51064, 457, 341, 307, 257, 707, 746, 321, 764, 51164], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7019, "seek": 2003364, "start": 20049.64, "end": 20051.64, "text": " to pretty much test how long something", "tokens": [51164, 281, 1238, 709, 1500, 577, 938, 746, 51264], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7020, "seek": 2003364, "start": 20051.64, "end": 20053.64, "text": " takes how efficient it is", "tokens": [51264, 2516, 577, 7148, 309, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7021, "seek": 2003364, "start": 20053.64, "end": 20055.64, "text": " and then to also see if", "tokens": [51364, 293, 550, 281, 611, 536, 498, 51464], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7022, "seek": 2003364, "start": 20055.64, "end": 20057.64, "text": " it's worth investigating a new way of approaching", "tokens": [51464, 309, 311, 3163, 22858, 257, 777, 636, 295, 14908, 51564], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7023, "seek": 2003364, "start": 20057.64, "end": 20059.64, "text": " something in case it takes", "tokens": [51564, 746, 294, 1389, 309, 2516, 51664], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7024, "seek": 2003364, "start": 20059.64, "end": 20061.64, "text": " ridiculous amount of time", "tokens": [51664, 11083, 2372, 295, 565, 51764], "temperature": 0.0, "avg_logprob": -0.08435779543065314, "compression_ratio": 1.9511278195488722, "no_speech_prob": 0.031112372875213623}, {"id": 7025, "seek": 2006164, "start": 20061.64, "end": 20063.64, "text": " so that's time testing", "tokens": [50364, 370, 300, 311, 565, 4997, 50464], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7026, "seek": 2006164, "start": 20063.64, "end": 20065.64, "text": " and efficiency testing for you", "tokens": [50464, 293, 10493, 4997, 337, 291, 50564], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7027, "seek": 2006164, "start": 20065.64, "end": 20067.64, "text": " the next little bit I want to cover", "tokens": [50564, 264, 958, 707, 857, 286, 528, 281, 2060, 50664], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7028, "seek": 2006164, "start": 20067.64, "end": 20069.64, "text": " is the history", "tokens": [50664, 307, 264, 2503, 50764], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7029, "seek": 2006164, "start": 20069.64, "end": 20071.64, "text": " I'm not going to go over the entire history", "tokens": [50764, 286, 478, 406, 516, 281, 352, 670, 264, 2302, 2503, 50864], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7030, "seek": 2006164, "start": 20071.64, "end": 20073.64, "text": " of AI and LLMs", "tokens": [50864, 295, 7318, 293, 441, 43, 26386, 50964], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7031, "seek": 2006164, "start": 20073.64, "end": 20075.64, "text": " but essentially", "tokens": [50964, 457, 4476, 51064], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7032, "seek": 2006164, "start": 20075.64, "end": 20077.64, "text": " we originated with something called RNNs", "tokens": [51064, 321, 31129, 365, 746, 1219, 45702, 45, 82, 51164], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7033, "seek": 2006164, "start": 20077.64, "end": 20079.64, "text": " okay RNNs are called", "tokens": [51164, 1392, 45702, 45, 82, 366, 1219, 51264], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7034, "seek": 2006164, "start": 20079.64, "end": 20081.64, "text": " recurrent neural networks", "tokens": [51264, 18680, 1753, 18161, 9590, 51364], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7035, "seek": 2006164, "start": 20081.64, "end": 20083.64, "text": " and they're really inefficient", "tokens": [51364, 293, 436, 434, 534, 43495, 51464], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7036, "seek": 2006164, "start": 20083.64, "end": 20085.64, "text": " at least for scaled", "tokens": [51464, 412, 1935, 337, 36039, 51564], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7037, "seek": 2006164, "start": 20085.64, "end": 20087.64, "text": " AI systems so RNNs", "tokens": [51564, 7318, 3652, 370, 45702, 45, 82, 51664], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7038, "seek": 2006164, "start": 20087.64, "end": 20089.64, "text": " are a little essentially think of it as a little loop", "tokens": [51664, 366, 257, 707, 4476, 519, 295, 309, 382, 257, 707, 6367, 51764], "temperature": 0.0, "avg_logprob": -0.08598767339655783, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0012445844477042556}, {"id": 7039, "seek": 2008964, "start": 20089.64, "end": 20091.64, "text": " keeps learning and learning", "tokens": [50364, 5965, 2539, 293, 2539, 50464], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7040, "seek": 2008964, "start": 20091.64, "end": 20093.64, "text": " and this is sequential right", "tokens": [50464, 293, 341, 307, 42881, 558, 50564], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7041, "seek": 2008964, "start": 20093.64, "end": 20095.64, "text": " it does this and then this and then this", "tokens": [50564, 309, 775, 341, 293, 550, 341, 293, 550, 341, 50664], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7042, "seek": 2008964, "start": 20095.64, "end": 20097.64, "text": " has to wait for each completion", "tokens": [50664, 575, 281, 1699, 337, 1184, 19372, 50764], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7043, "seek": 2008964, "start": 20097.64, "end": 20099.64, "text": " synchronous you can't have multiple of them at once", "tokens": [50764, 44743, 291, 393, 380, 362, 3866, 295, 552, 412, 1564, 50864], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7044, "seek": 2008964, "start": 20099.64, "end": 20101.64, "text": " because they're complex", "tokens": [50864, 570, 436, 434, 3997, 50964], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7045, "seek": 2008964, "start": 20101.64, "end": 20103.64, "text": " GPUs cannot run complex things", "tokens": [50964, 18407, 82, 2644, 1190, 3997, 721, 51064], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7046, "seek": 2008964, "start": 20103.64, "end": 20105.64, "text": " they're only designed for just", "tokens": [51064, 436, 434, 787, 4761, 337, 445, 51164], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7047, "seek": 2008964, "start": 20105.64, "end": 20107.64, "text": " matrix multiplication and very simple", "tokens": [51164, 8141, 27290, 293, 588, 2199, 51264], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7048, "seek": 2008964, "start": 20107.64, "end": 20109.64, "text": " math like that", "tokens": [51264, 5221, 411, 300, 51364], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7049, "seek": 2008964, "start": 20109.64, "end": 20111.64, "text": " so RNNs are essentially", "tokens": [51364, 370, 45702, 45, 82, 366, 4476, 51464], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7050, "seek": 2008964, "start": 20111.64, "end": 20113.64, "text": " a little bit dumber than transformers", "tokens": [51464, 257, 707, 857, 274, 4182, 813, 4088, 433, 51564], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7051, "seek": 2008964, "start": 20113.64, "end": 20115.64, "text": " and they", "tokens": [51564, 293, 436, 51664], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7052, "seek": 2008964, "start": 20115.64, "end": 20117.64, "text": " are run on the CPU", "tokens": [51664, 366, 1190, 322, 264, 13199, 51764], "temperature": 0.0, "avg_logprob": -0.08789424721254122, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.03672515228390694}, {"id": 7053, "seek": 2011764, "start": 20117.64, "end": 20119.64, "text": " so RNNs was where we last sort of stopped at", "tokens": [50364, 370, 45702, 45, 82, 390, 689, 321, 1036, 1333, 295, 5936, 412, 50464], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7054, "seek": 2011764, "start": 20119.64, "end": 20121.64, "text": " and what I encourage you to do", "tokens": [50464, 293, 437, 286, 5373, 291, 281, 360, 50564], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7055, "seek": 2011764, "start": 20121.64, "end": 20123.64, "text": " is look into more of the language", "tokens": [50564, 307, 574, 666, 544, 295, 264, 2856, 50664], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7056, "seek": 2011764, "start": 20123.64, "end": 20125.64, "text": " modeling and AI", "tokens": [50664, 15983, 293, 7318, 50764], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7057, "seek": 2011764, "start": 20125.64, "end": 20127.64, "text": " history and research that has led up to this", "tokens": [50764, 2503, 293, 2132, 300, 575, 4684, 493, 281, 341, 50864], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7058, "seek": 2011764, "start": 20127.64, "end": 20129.64, "text": " point so you can have an idea", "tokens": [50864, 935, 370, 291, 393, 362, 364, 1558, 50964], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7059, "seek": 2011764, "start": 20129.64, "end": 20131.64, "text": " as to how researchers", "tokens": [50964, 382, 281, 577, 10309, 51064], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7060, "seek": 2011764, "start": 20131.64, "end": 20133.64, "text": " have been able to quickly innovate", "tokens": [51064, 362, 668, 1075, 281, 2661, 33444, 51164], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7061, "seek": 2011764, "start": 20133.64, "end": 20135.64, "text": " given", "tokens": [51164, 2212, 51264], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7062, "seek": 2011764, "start": 20135.64, "end": 20137.64, "text": " all these historical innovations", "tokens": [51264, 439, 613, 8584, 24283, 51364], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7063, "seek": 2011764, "start": 20137.64, "end": 20139.64, "text": " so you have like all these things leading up to the transformer", "tokens": [51364, 370, 291, 362, 411, 439, 613, 721, 5775, 493, 281, 264, 31782, 51464], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7064, "seek": 2011764, "start": 20139.64, "end": 20141.64, "text": " well how did they all", "tokens": [51464, 731, 577, 630, 436, 439, 51564], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7065, "seek": 2011764, "start": 20141.64, "end": 20143.64, "text": " philosophize", "tokens": [51564, 14529, 1125, 51664], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7066, "seek": 2011764, "start": 20143.64, "end": 20145.64, "text": " up to that point", "tokens": [51664, 493, 281, 300, 935, 51764], "temperature": 0.0, "avg_logprob": -0.08391054541663787, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.006091612856835127}, {"id": 7067, "seek": 2014564, "start": 20145.64, "end": 20147.64, "text": " and yeah it's just", "tokens": [50364, 293, 1338, 309, 311, 445, 50464], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7068, "seek": 2014564, "start": 20147.64, "end": 20149.64, "text": " something good to sort of be confident", "tokens": [50464, 746, 665, 281, 1333, 295, 312, 6679, 50564], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7069, "seek": 2014564, "start": 20149.64, "end": 20151.64, "text": " in is innovating", "tokens": [50564, 294, 307, 5083, 990, 50664], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7070, "seek": 2014564, "start": 20151.64, "end": 20153.64, "text": " as both a researcher", "tokens": [50664, 382, 1293, 257, 21751, 50764], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7071, "seek": 2014564, "start": 20153.64, "end": 20155.64, "text": " and engineer and a", "tokens": [50764, 293, 11403, 293, 257, 50864], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7072, "seek": 2014564, "start": 20155.64, "end": 20157.64, "text": " business person", "tokens": [50864, 1606, 954, 50964], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7073, "seek": 2014564, "start": 20157.64, "end": 20159.64, "text": " so cool", "tokens": [50964, 370, 1627, 51064], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7074, "seek": 2014564, "start": 20159.64, "end": 20161.64, "text": " RNNs were where we sort of", "tokens": [51064, 45702, 45, 82, 645, 689, 321, 1333, 295, 51164], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7075, "seek": 2014564, "start": 20161.64, "end": 20163.64, "text": " finished off and now it's transformers and GPTs", "tokens": [51164, 4335, 766, 293, 586, 309, 311, 4088, 433, 293, 26039, 33424, 51264], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7076, "seek": 2014564, "start": 20163.64, "end": 20165.64, "text": " that's the current state of AI", "tokens": [51264, 300, 311, 264, 2190, 1785, 295, 7318, 51364], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7077, "seek": 2014564, "start": 20165.64, "end": 20167.64, "text": " next up I", "tokens": [51364, 958, 493, 286, 51464], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7078, "seek": 2014564, "start": 20167.64, "end": 20169.64, "text": " would like to go over something called", "tokens": [51464, 576, 411, 281, 352, 670, 746, 1219, 51564], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7079, "seek": 2014564, "start": 20169.64, "end": 20171.64, "text": " quantization", "tokens": [51564, 4426, 2144, 51664], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7080, "seek": 2014564, "start": 20171.64, "end": 20173.64, "text": " so quantization is essentially", "tokens": [51664, 370, 4426, 2144, 307, 4476, 51764], "temperature": 0.0, "avg_logprob": -0.0968923002186388, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.0028440130408853292}, {"id": 7081, "seek": 2017364, "start": 20173.64, "end": 20175.64, "text": " a way to reduce the memory", "tokens": [50364, 257, 636, 281, 5407, 264, 4675, 50464], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7082, "seek": 2017364, "start": 20175.64, "end": 20177.64, "text": " usage by your parameters", "tokens": [50464, 14924, 538, 428, 9834, 50564], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7083, "seek": 2017364, "start": 20177.64, "end": 20179.64, "text": " so there's actually a paper here", "tokens": [50564, 370, 456, 311, 767, 257, 3035, 510, 50664], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7084, "seek": 2017364, "start": 20179.64, "end": 20181.64, "text": " called QLaura Efficient Fine", "tokens": [50664, 1219, 1249, 5478, 2991, 462, 7816, 12024, 50764], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7085, "seek": 2017364, "start": 20181.64, "end": 20183.64, "text": " Tuning of Quantized", "tokens": [50764, 21363, 278, 295, 26968, 1602, 50864], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7086, "seek": 2017364, "start": 20183.64, "end": 20185.64, "text": " LLMs so", "tokens": [50864, 441, 43, 26386, 370, 50964], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7087, "seek": 2017364, "start": 20185.64, "end": 20187.64, "text": " all this does in simple", "tokens": [50964, 439, 341, 775, 294, 2199, 51064], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7088, "seek": 2017364, "start": 20187.64, "end": 20189.64, "text": " form is pretty much instead of", "tokens": [51064, 1254, 307, 1238, 709, 2602, 295, 51164], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7089, "seek": 2017364, "start": 20189.64, "end": 20191.64, "text": " using 32 bit floating", "tokens": [51164, 1228, 8858, 857, 12607, 51264], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7090, "seek": 2017364, "start": 20191.64, "end": 20193.64, "text": " point numbers it goes not only", "tokens": [51264, 935, 3547, 309, 1709, 406, 787, 51364], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7091, "seek": 2017364, "start": 20193.64, "end": 20195.64, "text": " to 16 bit of half precision", "tokens": [51364, 281, 3165, 857, 295, 1922, 18356, 51464], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7092, "seek": 2017364, "start": 20195.64, "end": 20197.64, "text": " but all the way down to 4", "tokens": [51464, 457, 439, 264, 636, 760, 281, 1017, 51564], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7093, "seek": 2017364, "start": 20197.64, "end": 20199.64, "text": " so what this actually", "tokens": [51564, 370, 437, 341, 767, 51664], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7094, "seek": 2017364, "start": 20199.64, "end": 20201.64, "text": " looks like is in binary code", "tokens": [51664, 1542, 411, 307, 294, 17434, 3089, 51764], "temperature": 0.0, "avg_logprob": -0.09963725437627774, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.020012136548757553}, {"id": 7095, "seek": 2020164, "start": 20201.64, "end": 20203.64, "text": " or in bytecode", "tokens": [50364, 420, 294, 40846, 22332, 50464], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7096, "seek": 2020164, "start": 20203.64, "end": 20205.64, "text": " it will look", "tokens": [50464, 309, 486, 574, 50564], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7097, "seek": 2020164, "start": 20205.64, "end": 20207.64, "text": " here there's some array", "tokens": [50564, 510, 456, 311, 512, 10225, 50664], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7098, "seek": 2020164, "start": 20207.64, "end": 20209.64, "text": " of numbers", "tokens": [50664, 295, 3547, 50764], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7099, "seek": 2020164, "start": 20209.64, "end": 20211.64, "text": " that it uses", "tokens": [50764, 300, 309, 4960, 50864], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7100, "seek": 2020164, "start": 20217.64, "end": 20219.64, "text": " okay I can't find it", "tokens": [51164, 1392, 286, 393, 380, 915, 309, 51264], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7101, "seek": 2020164, "start": 20219.64, "end": 20221.64, "text": " but pretty much what it is", "tokens": [51264, 457, 1238, 709, 437, 309, 307, 51364], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7102, "seek": 2020164, "start": 20221.64, "end": 20223.64, "text": " it is a bunch of", "tokens": [51364, 309, 307, 257, 3840, 295, 51464], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7103, "seek": 2020164, "start": 20223.64, "end": 20225.64, "text": " it's a bunch of floating point numbers", "tokens": [51464, 309, 311, 257, 3840, 295, 12607, 935, 3547, 51564], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7104, "seek": 2020164, "start": 20225.64, "end": 20227.64, "text": " and they're all between", "tokens": [51564, 293, 436, 434, 439, 1296, 51664], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7105, "seek": 2020164, "start": 20227.64, "end": 20229.64, "text": " negative one and one", "tokens": [51664, 3671, 472, 293, 472, 51764], "temperature": 0.0, "avg_logprob": -0.09127804438273111, "compression_ratio": 1.5448275862068965, "no_speech_prob": 0.009408404119312763}, {"id": 7106, "seek": 2022964, "start": 20229.64, "end": 20231.64, "text": " and there are 16 of them", "tokens": [50364, 293, 456, 366, 3165, 295, 552, 50464], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7107, "seek": 2022964, "start": 20231.64, "end": 20233.64, "text": " if you have a 4 bit number", "tokens": [50464, 498, 291, 362, 257, 1017, 857, 1230, 50564], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7108, "seek": 2022964, "start": 20233.64, "end": 20235.64, "text": " that means it can hold 16 different", "tokens": [50564, 300, 1355, 309, 393, 1797, 3165, 819, 50664], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7109, "seek": 2022964, "start": 20235.64, "end": 20237.64, "text": " values 0 through 15", "tokens": [50664, 4190, 1958, 807, 2119, 50764], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7110, "seek": 2022964, "start": 20237.64, "end": 20239.64, "text": " which is 16 values", "tokens": [50764, 597, 307, 3165, 4190, 50864], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7111, "seek": 2022964, "start": 20239.64, "end": 20241.64, "text": " and all you pretty much do is you have this", "tokens": [50864, 293, 439, 291, 1238, 709, 360, 307, 291, 362, 341, 50964], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7112, "seek": 2022964, "start": 20241.64, "end": 20243.64, "text": " array of floating point numbers", "tokens": [50964, 10225, 295, 12607, 935, 3547, 51064], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7113, "seek": 2022964, "start": 20243.64, "end": 20245.64, "text": " you use the bytecode of", "tokens": [51064, 291, 764, 264, 40846, 22332, 295, 51164], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7114, "seek": 2022964, "start": 20245.64, "end": 20247.64, "text": " that 4 bit", "tokens": [51164, 300, 1017, 857, 51264], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7115, "seek": 2022964, "start": 20247.64, "end": 20249.64, "text": " number to look up the index", "tokens": [51264, 1230, 281, 574, 493, 264, 8186, 51364], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7116, "seek": 2022964, "start": 20249.64, "end": 20251.64, "text": " in that array and that is your weight", "tokens": [51364, 294, 300, 10225, 293, 300, 307, 428, 3364, 51464], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7117, "seek": 2022964, "start": 20251.64, "end": 20253.64, "text": " that is the weight they use", "tokens": [51464, 300, 307, 264, 3364, 436, 764, 51564], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7118, "seek": 2022964, "start": 20253.64, "end": 20255.64, "text": " in your model", "tokens": [51564, 294, 428, 2316, 51664], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7119, "seek": 2022964, "start": 20255.64, "end": 20257.64, "text": " so this way instead of using 32 bit", "tokens": [51664, 370, 341, 636, 2602, 295, 1228, 8858, 857, 51764], "temperature": 0.0, "avg_logprob": -0.05757094273525002, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.006386028602719307}, {"id": 7120, "seek": 2025764, "start": 20257.64, "end": 20259.64, "text": " having these super long numbers", "tokens": [50364, 1419, 613, 1687, 938, 3547, 50464], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7121, "seek": 2025764, "start": 20259.64, "end": 20261.64, "text": " that are super precise", "tokens": [50464, 300, 366, 1687, 13600, 50564], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7122, "seek": 2025764, "start": 20261.64, "end": 20263.64, "text": " you can have super precise numbers", "tokens": [50564, 291, 393, 362, 1687, 13600, 3547, 50664], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7123, "seek": 2025764, "start": 20263.64, "end": 20265.64, "text": " that are just generally good parameters", "tokens": [50664, 300, 366, 445, 5101, 665, 9834, 50764], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7124, "seek": 2025764, "start": 20265.64, "end": 20267.64, "text": " to have that just perform", "tokens": [50764, 281, 362, 300, 445, 2042, 50864], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7125, "seek": 2025764, "start": 20267.64, "end": 20269.64, "text": " decently", "tokens": [50864, 979, 2276, 50964], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7126, "seek": 2025764, "start": 20269.64, "end": 20271.64, "text": " they're just sort of well spread out", "tokens": [50964, 436, 434, 445, 1333, 295, 731, 3974, 484, 51064], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7127, "seek": 2025764, "start": 20271.64, "end": 20273.64, "text": " and experimented on and they just", "tokens": [51064, 293, 5120, 292, 322, 293, 436, 445, 51164], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7128, "seek": 2025764, "start": 20273.64, "end": 20275.64, "text": " happen to work and you have 16 of them", "tokens": [51164, 1051, 281, 589, 293, 291, 362, 3165, 295, 552, 51264], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7129, "seek": 2025764, "start": 20275.64, "end": 20277.64, "text": " instead of a lot", "tokens": [51264, 2602, 295, 257, 688, 51364], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7130, "seek": 2025764, "start": 20277.64, "end": 20279.64, "text": " so that's", "tokens": [51364, 370, 300, 311, 51464], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7131, "seek": 2025764, "start": 20279.64, "end": 20281.64, "text": " another cool little thing that's going on right now", "tokens": [51464, 1071, 1627, 707, 551, 300, 311, 516, 322, 558, 586, 51564], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7132, "seek": 2025764, "start": 20281.64, "end": 20283.64, "text": " is 4 bit quantizations", "tokens": [51564, 307, 1017, 857, 4426, 14455, 51664], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7133, "seek": 2025764, "start": 20283.64, "end": 20285.64, "text": " it's a little bit harder", "tokens": [51664, 309, 311, 257, 707, 857, 6081, 51764], "temperature": 0.0, "avg_logprob": -0.13308434052900833, "compression_ratio": 1.7901785714285714, "no_speech_prob": 0.0502798855304718}, {"id": 7134, "seek": 2028564, "start": 20285.64, "end": 20287.64, "text": " to implement", "tokens": [50364, 281, 4445, 50464], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7135, "seek": 2028564, "start": 20287.64, "end": 20289.64, "text": " I would encourage you to experiment with half precision", "tokens": [50464, 286, 576, 5373, 291, 281, 5120, 365, 1922, 18356, 50564], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7136, "seek": 2028564, "start": 20289.64, "end": 20291.64, "text": " meaning 16 bit", "tokens": [50564, 3620, 3165, 857, 50664], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7137, "seek": 2028564, "start": 20291.64, "end": 20293.64, "text": " floating point numbers", "tokens": [50664, 12607, 935, 3547, 50764], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7138, "seek": 2028564, "start": 20293.64, "end": 20295.64, "text": " so that means it occupies", "tokens": [50764, 370, 300, 1355, 309, 8073, 530, 50864], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7139, "seek": 2028564, "start": 20295.64, "end": 20297.64, "text": " 16 on and off switches", "tokens": [50864, 3165, 322, 293, 766, 19458, 50964], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7140, "seek": 2028564, "start": 20297.64, "end": 20299.64, "text": " or capacitors on your GPU", "tokens": [50964, 420, 4637, 9862, 322, 428, 18407, 51064], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7141, "seek": 2028564, "start": 20299.64, "end": 20301.64, "text": " and", "tokens": [51064, 293, 51164], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7142, "seek": 2028564, "start": 20301.64, "end": 20303.64, "text": " so quantization is cool to", "tokens": [51164, 370, 4426, 2144, 307, 1627, 281, 51264], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7143, "seek": 2028564, "start": 20303.64, "end": 20305.64, "text": " sort of scale down the memory", "tokens": [51264, 1333, 295, 4373, 760, 264, 4675, 51364], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7144, "seek": 2028564, "start": 20305.64, "end": 20307.64, "text": " so that way you can scale up all of your hyper parameters", "tokens": [51364, 370, 300, 636, 291, 393, 4373, 493, 439, 295, 428, 9848, 9834, 51464], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7145, "seek": 2028564, "start": 20307.64, "end": 20309.64, "text": " and have a more complex model", "tokens": [51464, 293, 362, 257, 544, 3997, 2316, 51564], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7146, "seek": 2028564, "start": 20309.64, "end": 20311.64, "text": " with these", "tokens": [51564, 365, 613, 51664], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7147, "seek": 2028564, "start": 20311.64, "end": 20313.64, "text": " yeah just essentially to have bigger models", "tokens": [51664, 1338, 445, 4476, 281, 362, 3801, 5245, 51764], "temperature": 0.0, "avg_logprob": -0.076404869556427, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.007693090476095676}, {"id": 7148, "seek": 2031364, "start": 20313.64, "end": 20315.64, "text": " with less space", "tokens": [50364, 365, 1570, 1901, 50464], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7149, "seek": 2031364, "start": 20315.64, "end": 20317.64, "text": " take it up", "tokens": [50464, 747, 309, 493, 50564], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7150, "seek": 2031364, "start": 20317.64, "end": 20319.64, "text": " so that is", "tokens": [50564, 370, 300, 307, 50664], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7151, "seek": 2031364, "start": 20319.64, "end": 20321.64, "text": " quantization", "tokens": [50664, 4426, 2144, 50764], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7152, "seek": 2031364, "start": 20321.64, "end": 20323.64, "text": " and this is the paper for it", "tokens": [50764, 293, 341, 307, 264, 3035, 337, 309, 50864], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7153, "seek": 2031364, "start": 20323.64, "end": 20325.64, "text": " it's a little link you can search out if you want to get", "tokens": [50864, 309, 311, 257, 707, 2113, 291, 393, 3164, 484, 498, 291, 528, 281, 483, 50964], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7154, "seek": 2031364, "start": 20325.64, "end": 20327.64, "text": " more familiar with this see", "tokens": [50964, 544, 4963, 365, 341, 536, 51064], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7155, "seek": 2031364, "start": 20327.64, "end": 20329.64, "text": " sort of performance standards and what not", "tokens": [51064, 1333, 295, 3389, 7787, 293, 437, 406, 51164], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7156, "seek": 2031364, "start": 20329.64, "end": 20331.64, "text": " the next thing I'd like to cover", "tokens": [51164, 264, 958, 551, 286, 1116, 411, 281, 2060, 51264], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7157, "seek": 2031364, "start": 20331.64, "end": 20333.64, "text": " is gradient accumulation", "tokens": [51264, 307, 16235, 35647, 51364], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7158, "seek": 2031364, "start": 20333.64, "end": 20335.64, "text": " so you might have heard of this you might not have heard of this", "tokens": [51364, 370, 291, 1062, 362, 2198, 295, 341, 291, 1062, 406, 362, 2198, 295, 341, 51464], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7159, "seek": 2031364, "start": 20335.64, "end": 20337.64, "text": " gradient accumulation", "tokens": [51464, 16235, 35647, 51564], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7160, "seek": 2031364, "start": 20337.64, "end": 20339.64, "text": " will", "tokens": [51564, 486, 51664], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7161, "seek": 2031364, "start": 20339.64, "end": 20341.64, "text": " what gradient accumulation does", "tokens": [51664, 437, 16235, 35647, 775, 51764], "temperature": 0.0, "avg_logprob": -0.09524066493196308, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.009856251068413258}, {"id": 7162, "seek": 2034164, "start": 20341.64, "end": 20343.64, "text": " is it will accumulate", "tokens": [50364, 307, 309, 486, 33384, 50464], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7163, "seek": 2034164, "start": 20343.64, "end": 20345.64, "text": " gradients", "tokens": [50464, 2771, 2448, 50564], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7164, "seek": 2034164, "start": 20345.64, "end": 20347.64, "text": " over say we just set a variable", "tokens": [50564, 670, 584, 321, 445, 992, 257, 7006, 50664], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7165, "seek": 2034164, "start": 20347.64, "end": 20349.64, "text": " x so every x iterations", "tokens": [50664, 2031, 370, 633, 2031, 36540, 50764], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7166, "seek": 2034164, "start": 20349.64, "end": 20351.64, "text": " it'll just accumulate those", "tokens": [50764, 309, 603, 445, 33384, 729, 50864], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7167, "seek": 2034164, "start": 20351.64, "end": 20353.64, "text": " iterations, average them", "tokens": [50864, 36540, 11, 4274, 552, 50964], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7168, "seek": 2034164, "start": 20353.64, "end": 20355.64, "text": " and what this allows you to do", "tokens": [50964, 293, 437, 341, 4045, 291, 281, 360, 51064], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7169, "seek": 2034164, "start": 20355.64, "end": 20357.64, "text": " is instead of", "tokens": [51064, 307, 2602, 295, 51164], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7170, "seek": 2034164, "start": 20357.64, "end": 20359.64, "text": " updating each iteration", "tokens": [51164, 25113, 1184, 24784, 51264], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7171, "seek": 2034164, "start": 20359.64, "end": 20361.64, "text": " you're updating every x iterations", "tokens": [51264, 291, 434, 25113, 633, 2031, 36540, 51364], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7172, "seek": 2034164, "start": 20361.64, "end": 20363.64, "text": " so that allows you to fit", "tokens": [51364, 370, 300, 4045, 291, 281, 3318, 51464], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7173, "seek": 2034164, "start": 20363.64, "end": 20365.64, "text": " more parameters and more info", "tokens": [51464, 544, 9834, 293, 544, 13614, 51564], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7174, "seek": 2034164, "start": 20365.64, "end": 20367.64, "text": " or generalization into this one piece", "tokens": [51564, 420, 2674, 2144, 666, 341, 472, 2522, 51664], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7175, "seek": 2034164, "start": 20367.64, "end": 20369.64, "text": " so that way when you", "tokens": [51664, 370, 300, 636, 562, 291, 51764], "temperature": 0.0, "avg_logprob": -0.10355551555903271, "compression_ratio": 1.8223350253807107, "no_speech_prob": 0.019394688308238983}, {"id": 7176, "seek": 2036964, "start": 20369.64, "end": 20371.64, "text": " update your parameters", "tokens": [50364, 5623, 428, 9834, 50464], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7177, "seek": 2036964, "start": 20371.64, "end": 20373.64, "text": " it's able to generalize more", "tokens": [50464, 309, 311, 1075, 281, 2674, 1125, 544, 50564], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7178, "seek": 2036964, "start": 20373.64, "end": 20375.64, "text": " over maybe a higher batch size", "tokens": [50564, 670, 1310, 257, 2946, 15245, 2744, 50664], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7179, "seek": 2036964, "start": 20375.64, "end": 20377.64, "text": " or a higher block size", "tokens": [50664, 420, 257, 2946, 3461, 2744, 50764], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7180, "seek": 2036964, "start": 20377.64, "end": 20379.64, "text": " so when you distribute this", "tokens": [50764, 370, 562, 291, 20594, 341, 50864], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7181, "seek": 2036964, "start": 20379.64, "end": 20381.64, "text": " over many", "tokens": [50864, 670, 867, 50964], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7182, "seek": 2036964, "start": 20381.64, "end": 20383.64, "text": " iterations and average them", "tokens": [50964, 36540, 293, 4274, 552, 51064], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7183, "seek": 2036964, "start": 20383.64, "end": 20385.64, "text": " you can fit more into each iteration", "tokens": [51064, 291, 393, 3318, 544, 666, 1184, 24784, 51164], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7184, "seek": 2036964, "start": 20385.64, "end": 20387.64, "text": " because it's sort of calculating", "tokens": [51164, 570, 309, 311, 1333, 295, 28258, 51264], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7185, "seek": 2036964, "start": 20387.64, "end": 20389.64, "text": " all of them combined", "tokens": [51264, 439, 295, 552, 9354, 51364], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7186, "seek": 2036964, "start": 20389.64, "end": 20391.64, "text": " so yeah that's a cool little trick", "tokens": [51364, 370, 1338, 300, 311, 257, 1627, 707, 4282, 51464], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7187, "seek": 2036964, "start": 20391.64, "end": 20393.64, "text": " you can use if", "tokens": [51464, 291, 393, 764, 498, 51564], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7188, "seek": 2036964, "start": 20393.64, "end": 20395.64, "text": " your GPU maybe isn't", "tokens": [51564, 428, 18407, 1310, 1943, 380, 51664], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7189, "seek": 2036964, "start": 20395.64, "end": 20397.64, "text": " as big if it doesn't have as much", "tokens": [51664, 382, 955, 498, 309, 1177, 380, 362, 382, 709, 51764], "temperature": 0.0, "avg_logprob": -0.04440387090047201, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.0037637026980519295}, {"id": 7190, "seek": 2039764, "start": 20397.64, "end": 20399.64, "text": " VRAM on it", "tokens": [50364, 13722, 2865, 322, 309, 50464], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7191, "seek": 2039764, "start": 20399.64, "end": 20401.64, "text": " so gradient accumulation is wonderful", "tokens": [50464, 370, 16235, 35647, 307, 3715, 50564], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7192, "seek": 2039764, "start": 20401.64, "end": 20403.64, "text": " and it's used lots in practice", "tokens": [50564, 293, 309, 311, 1143, 3195, 294, 3124, 50664], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7193, "seek": 2039764, "start": 20403.64, "end": 20405.64, "text": " the final thing I'd like to leave", "tokens": [50664, 264, 2572, 551, 286, 1116, 411, 281, 1856, 50764], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7194, "seek": 2039764, "start": 20405.64, "end": 20407.64, "text": " you guys off with is something called", "tokens": [50764, 291, 1074, 766, 365, 307, 746, 1219, 50864], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7195, "seek": 2039764, "start": 20407.64, "end": 20409.64, "text": " hugging face and you've probably", "tokens": [50864, 41706, 1851, 293, 291, 600, 1391, 50964], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7196, "seek": 2039764, "start": 20409.64, "end": 20411.64, "text": " heard a lot about this so far", "tokens": [50964, 2198, 257, 688, 466, 341, 370, 1400, 51064], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7197, "seek": 2039764, "start": 20411.64, "end": 20413.64, "text": " but let me just guide you through", "tokens": [51064, 457, 718, 385, 445, 5934, 291, 807, 51164], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7198, "seek": 2039764, "start": 20413.64, "end": 20415.64, "text": " and show you how absolutely explosive", "tokens": [51164, 293, 855, 291, 577, 3122, 24630, 51264], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7199, "seek": 2039764, "start": 20415.64, "end": 20417.64, "text": " hugging face is", "tokens": [51264, 41706, 1851, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7200, "seek": 2039764, "start": 20417.64, "end": 20419.64, "text": " for machine learning so you have", "tokens": [51364, 337, 3479, 2539, 370, 291, 362, 51464], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7201, "seek": 2039764, "start": 20419.64, "end": 20421.64, "text": " a bunch of models, data sets", "tokens": [51464, 257, 3840, 295, 5245, 11, 1412, 6352, 51564], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7202, "seek": 2039764, "start": 20421.64, "end": 20423.64, "text": " spaces, docs, etc", "tokens": [51564, 7673, 11, 45623, 11, 5183, 51664], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7203, "seek": 2039764, "start": 20423.64, "end": 20425.64, "text": " and", "tokens": [51664, 293, 51764], "temperature": 0.0, "avg_logprob": -0.08099303989235414, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01261749304831028}, {"id": 7204, "seek": 2042564, "start": 20425.64, "end": 20427.64, "text": " let's go to models for example", "tokens": [50364, 718, 311, 352, 281, 5245, 337, 1365, 50464], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7205, "seek": 2042564, "start": 20427.64, "end": 20429.64, "text": " so let's just showcase how cool this is", "tokens": [50464, 370, 718, 311, 445, 20388, 577, 1627, 341, 307, 50564], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7206, "seek": 2042564, "start": 20429.64, "end": 20431.64, "text": " you have multimodal AIs which could be", "tokens": [50564, 291, 362, 32972, 378, 304, 316, 6802, 597, 727, 312, 50664], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7207, "seek": 2042564, "start": 20431.64, "end": 20433.64, "text": " like", "tokens": [50664, 411, 50764], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7208, "seek": 2042564, "start": 20433.64, "end": 20435.64, "text": " image and text or video", "tokens": [50764, 3256, 293, 2487, 420, 960, 50864], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7209, "seek": 2042564, "start": 20435.64, "end": 20437.64, "text": " etc you have multiple different modes", "tokens": [50864, 5183, 291, 362, 3866, 819, 14068, 50964], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7210, "seek": 2042564, "start": 20437.64, "end": 20439.64, "text": " so it's not just text or not just video", "tokens": [50964, 370, 309, 311, 406, 445, 2487, 420, 406, 445, 960, 51064], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7211, "seek": 2042564, "start": 20439.64, "end": 20441.64, "text": " it's many different ones at the same time", "tokens": [51064, 309, 311, 867, 819, 2306, 412, 264, 912, 565, 51164], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7212, "seek": 2042564, "start": 20441.64, "end": 20443.64, "text": " so you have multimodal models", "tokens": [51164, 370, 291, 362, 32972, 378, 304, 5245, 51264], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7213, "seek": 2042564, "start": 20443.64, "end": 20445.64, "text": " you have computer vision", "tokens": [51264, 291, 362, 3820, 5201, 51364], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7214, "seek": 2042564, "start": 20445.64, "end": 20447.64, "text": " you have natural language processing", "tokens": [51364, 291, 362, 3303, 2856, 9007, 51464], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7215, "seek": 2042564, "start": 20447.64, "end": 20449.64, "text": " and we're actually doing natural language", "tokens": [51464, 293, 321, 434, 767, 884, 3303, 2856, 51564], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7216, "seek": 2042564, "start": 20449.64, "end": 20451.64, "text": " processing in this course", "tokens": [51564, 9007, 294, 341, 1164, 51664], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7217, "seek": 2042564, "start": 20451.64, "end": 20453.64, "text": " we have audio, a tabular", "tokens": [51664, 321, 362, 6278, 11, 257, 4421, 1040, 51764], "temperature": 0.0, "avg_logprob": -0.08371647921475497, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.016379892826080322}, {"id": 7218, "seek": 2045364, "start": 20453.64, "end": 20455.64, "text": " and reinforcement learning", "tokens": [50364, 293, 29280, 2539, 50464], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7219, "seek": 2045364, "start": 20455.64, "end": 20457.64, "text": " so this is really cool", "tokens": [50464, 370, 341, 307, 534, 1627, 50564], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7220, "seek": 2045364, "start": 20457.64, "end": 20459.64, "text": " and you can actually just download these models", "tokens": [50564, 293, 291, 393, 767, 445, 5484, 613, 5245, 50664], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7221, "seek": 2045364, "start": 20459.64, "end": 20461.64, "text": " and host them on your own computer", "tokens": [50664, 293, 3975, 552, 322, 428, 1065, 3820, 50764], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7222, "seek": 2045364, "start": 20461.64, "end": 20463.64, "text": " that is really cool", "tokens": [50764, 300, 307, 534, 1627, 50864], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7223, "seek": 2045364, "start": 20463.64, "end": 20465.64, "text": " you also have data sets which are even cooler", "tokens": [50864, 291, 611, 362, 1412, 6352, 597, 366, 754, 15566, 50964], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7224, "seek": 2045364, "start": 20465.64, "end": 20467.64, "text": " and these are pretty much", "tokens": [50964, 293, 613, 366, 1238, 709, 51064], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7225, "seek": 2045364, "start": 20467.64, "end": 20469.64, "text": " just really high quality data sets", "tokens": [51064, 445, 534, 1090, 3125, 1412, 6352, 51164], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7226, "seek": 2045364, "start": 20469.64, "end": 20471.64, "text": " of prompt and answer completions", "tokens": [51164, 295, 12391, 293, 1867, 1557, 626, 51264], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7227, "seek": 2045364, "start": 20471.64, "end": 20473.64, "text": " at least for our purpose", "tokens": [51264, 412, 1935, 337, 527, 4334, 51364], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7228, "seek": 2045364, "start": 20473.64, "end": 20475.64, "text": " if you want to use those", "tokens": [51364, 498, 291, 528, 281, 764, 729, 51464], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7229, "seek": 2045364, "start": 20475.64, "end": 20477.64, "text": " so you have", "tokens": [51464, 370, 291, 362, 51564], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7230, "seek": 2045364, "start": 20477.64, "end": 20479.64, "text": " question answering", "tokens": [51564, 1168, 13430, 51664], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7231, "seek": 2045364, "start": 20479.64, "end": 20481.64, "text": " or conversational", "tokens": [51664, 420, 2615, 1478, 51764], "temperature": 0.0, "avg_logprob": -0.07835182021645938, "compression_ratio": 1.7853881278538812, "no_speech_prob": 0.04526527598500252}, {"id": 7232, "seek": 2048164, "start": 20481.64, "end": 20483.64, "text": " work data set for example", "tokens": [50364, 589, 1412, 992, 337, 1365, 50464], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7233, "seek": 2048164, "start": 20483.64, "end": 20485.64, "text": " as 9000 downloads", "tokens": [50464, 382, 1722, 1360, 36553, 50564], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7234, "seek": 2048164, "start": 20485.64, "end": 20487.64, "text": " 500 likes", "tokens": [50564, 5923, 5902, 50664], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7235, "seek": 2048164, "start": 20487.64, "end": 20489.64, "text": " it has a bunch of", "tokens": [50664, 309, 575, 257, 3840, 295, 50764], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7236, "seek": 2048164, "start": 20489.64, "end": 20491.64, "text": " IDs, system prompts", "tokens": [50764, 48212, 11, 1185, 41095, 50864], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7237, "seek": 2048164, "start": 20491.64, "end": 20493.64, "text": " so you're an AI assistant or whatever", "tokens": [50864, 370, 291, 434, 364, 7318, 10994, 420, 2035, 50964], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7238, "seek": 2048164, "start": 20493.64, "end": 20495.64, "text": " and then you have the cool stuff which is", "tokens": [50964, 293, 550, 291, 362, 264, 1627, 1507, 597, 307, 51064], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7239, "seek": 2048164, "start": 20495.64, "end": 20497.64, "text": " you'll be given a definition of a task first", "tokens": [51064, 291, 603, 312, 2212, 257, 7123, 295, 257, 5633, 700, 51164], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7240, "seek": 2048164, "start": 20497.64, "end": 20499.64, "text": " and some input of the task etc", "tokens": [51164, 293, 512, 4846, 295, 264, 5633, 5183, 51264], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7241, "seek": 2048164, "start": 20499.64, "end": 20501.64, "text": " and then the response it's like oh", "tokens": [51264, 293, 550, 264, 4134, 309, 311, 411, 1954, 51364], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7242, "seek": 2048164, "start": 20501.64, "end": 20503.64, "text": " we just gave it an input and asked it to answer", "tokens": [51364, 321, 445, 2729, 309, 364, 4846, 293, 2351, 309, 281, 1867, 51464], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7243, "seek": 2048164, "start": 20503.64, "end": 20505.64, "text": " in a format and actually did that", "tokens": [51464, 294, 257, 7877, 293, 767, 630, 300, 51564], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7244, "seek": 2048164, "start": 20505.64, "end": 20507.64, "text": " correctly so", "tokens": [51564, 8944, 370, 51664], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7245, "seek": 2048164, "start": 20507.64, "end": 20509.64, "text": " you could pretty much train these", "tokens": [51664, 291, 727, 1238, 709, 3847, 613, 51764], "temperature": 0.0, "avg_logprob": -0.11896094629320048, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.007342113181948662}, {"id": 7246, "seek": 2050964, "start": 20509.64, "end": 20511.64, "text": " on a bunch of", "tokens": [50364, 322, 257, 3840, 295, 50464], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7247, "seek": 2050964, "start": 20511.64, "end": 20513.64, "text": " prompts that you would be able to feed into GPT-4", "tokens": [50464, 41095, 300, 291, 576, 312, 1075, 281, 3154, 666, 26039, 51, 12, 19, 50564], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7248, "seek": 2050964, "start": 20513.64, "end": 20515.64, "text": " and try to make your model perform that way", "tokens": [50564, 293, 853, 281, 652, 428, 2316, 2042, 300, 636, 50664], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7249, "seek": 2050964, "start": 20515.64, "end": 20517.64, "text": " and this actually has", "tokens": [50664, 293, 341, 767, 575, 50764], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7250, "seek": 2050964, "start": 20517.64, "end": 20519.64, "text": " 4.23 million rows", "tokens": [50764, 1017, 13, 9356, 2459, 13241, 50864], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7251, "seek": 2050964, "start": 20519.64, "end": 20521.64, "text": " in the training split which is amazing", "tokens": [50864, 294, 264, 3097, 7472, 597, 307, 2243, 50964], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7252, "seek": 2050964, "start": 20521.64, "end": 20523.64, "text": " so", "tokens": [50964, 370, 51064], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7253, "seek": 2050964, "start": 20523.64, "end": 20525.64, "text": " data sets are wonderful", "tokens": [51064, 1412, 6352, 366, 3715, 51164], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7254, "seek": 2050964, "start": 20525.64, "end": 20527.64, "text": " and you can find the best ones", "tokens": [51164, 293, 291, 393, 915, 264, 1151, 2306, 51264], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7255, "seek": 2050964, "start": 20527.64, "end": 20529.64, "text": " at least the best fine tuning data sets on OpenORCA", "tokens": [51264, 412, 1935, 264, 1151, 2489, 15164, 1412, 6352, 322, 7238, 2483, 15515, 51364], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7256, "seek": 2050964, "start": 20529.64, "end": 20531.64, "text": " really good", "tokens": [51364, 534, 665, 51464], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7257, "seek": 2050964, "start": 20531.64, "end": 20533.64, "text": " as for pre-training", "tokens": [51464, 382, 337, 659, 12, 17227, 1760, 51564], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7258, "seek": 2050964, "start": 20533.64, "end": 20535.64, "text": " I believe I mentioned this earlier", "tokens": [51564, 286, 1697, 286, 2835, 341, 3071, 51664], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7259, "seek": 2050964, "start": 20535.64, "end": 20537.64, "text": " in this survey of large language models", "tokens": [51664, 294, 341, 8984, 295, 2416, 2856, 5245, 51764], "temperature": 0.0, "avg_logprob": -0.07476336324316823, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.007227055262774229}, {"id": 7260, "seek": 2053764, "start": 20537.64, "end": 20539.64, "text": " that we just", "tokens": [50364, 300, 321, 445, 50464], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7261, "seek": 2053764, "start": 20539.64, "end": 20541.64, "text": " put down through Reddit links", "tokens": [50464, 829, 760, 807, 32210, 6123, 50564], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7262, "seek": 2053764, "start": 20545.64, "end": 20547.64, "text": " yep so you could use like OpenWebText", "tokens": [50764, 18633, 370, 291, 727, 764, 411, 7238, 4360, 65, 50198, 50864], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7263, "seek": 2053764, "start": 20547.64, "end": 20549.64, "text": " you could use CommonCrawl", "tokens": [50864, 291, 727, 764, 18235, 34, 5131, 75, 50964], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7264, "seek": 2053764, "start": 20549.64, "end": 20551.64, "text": " you could use Books", "tokens": [50964, 291, 727, 764, 33843, 51064], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7265, "seek": 2053764, "start": 20551.64, "end": 20553.64, "text": " you could use Wikipedia", "tokens": [51064, 291, 727, 764, 28999, 51164], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7266, "seek": 2053764, "start": 20553.64, "end": 20555.64, "text": " these are all pre-training data sources", "tokens": [51164, 613, 366, 439, 659, 12, 17227, 1760, 1412, 7139, 51264], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7267, "seek": 2053764, "start": 20555.64, "end": 20557.64, "text": " so yeah", "tokens": [51264, 370, 1338, 51364], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7268, "seek": 2053764, "start": 20557.64, "end": 20559.64, "text": " hopefully that leaves you with a better understanding", "tokens": [51364, 4696, 300, 5510, 291, 365, 257, 1101, 3701, 51464], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7269, "seek": 2053764, "start": 20559.64, "end": 20561.64, "text": " on how to create GPTs, transformers", "tokens": [51464, 322, 577, 281, 1884, 26039, 33424, 11, 4088, 433, 51564], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7270, "seek": 2053764, "start": 20561.64, "end": 20563.64, "text": " and", "tokens": [51564, 293, 51664], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7271, "seek": 2053764, "start": 20563.64, "end": 20565.64, "text": " pretty good large language models from scratch", "tokens": [51664, 1238, 665, 2416, 2856, 5245, 490, 8459, 51764], "temperature": 0.0, "avg_logprob": -0.16261797453227797, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.013839698396623135}, {"id": 7272, "seek": 2056564, "start": 20565.64, "end": 20567.64, "text": " with your own data that you scraped", "tokens": [50364, 365, 428, 1065, 1412, 300, 291, 13943, 3452, 50464], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7273, "seek": 2056564, "start": 20567.64, "end": 20569.64, "text": " or that you downloaded", "tokens": [50464, 420, 300, 291, 21748, 50564], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7274, "seek": 2056564, "start": 20569.64, "end": 20571.64, "text": " and yeah", "tokens": [50564, 293, 1338, 50664], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7275, "seek": 2056564, "start": 20571.64, "end": 20573.64, "text": " that's it, thanks for watching", "tokens": [50664, 300, 311, 309, 11, 3231, 337, 1976, 50764], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7276, "seek": 2056564, "start": 20573.64, "end": 20575.64, "text": " so you've learned a ton in this course", "tokens": [50764, 370, 291, 600, 3264, 257, 2952, 294, 341, 1164, 50864], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7277, "seek": 2056564, "start": 20575.64, "end": 20577.64, "text": " about language modeling", "tokens": [50864, 466, 2856, 15983, 50964], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7278, "seek": 2056564, "start": 20577.64, "end": 20579.64, "text": " how to use data, how to create architecture", "tokens": [50964, 577, 281, 764, 1412, 11, 577, 281, 1884, 9482, 51064], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7279, "seek": 2056564, "start": 20579.64, "end": 20581.64, "text": " from scratch", "tokens": [51064, 490, 8459, 51164], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7280, "seek": 2056564, "start": 20581.64, "end": 20583.64, "text": " maybe even how to look at research papers", "tokens": [51164, 1310, 754, 577, 281, 574, 412, 2132, 10577, 51264], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7281, "seek": 2056564, "start": 20583.64, "end": 20585.64, "text": " so if you really enjoy this content", "tokens": [51264, 370, 498, 291, 534, 2103, 341, 2701, 51364], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7282, "seek": 2056564, "start": 20585.64, "end": 20587.64, "text": " I would encourage you to maybe subscribe", "tokens": [51364, 286, 576, 5373, 291, 281, 1310, 3022, 51464], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7283, "seek": 2056564, "start": 20587.64, "end": 20589.64, "text": " and like on my YouTube channel", "tokens": [51464, 293, 411, 322, 452, 3088, 2269, 51564], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7284, "seek": 2056564, "start": 20589.64, "end": 20591.64, "text": " which is in the description", "tokens": [51564, 597, 307, 294, 264, 3855, 51664], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7285, "seek": 2056564, "start": 20591.64, "end": 20593.64, "text": " I make many videos about AI", "tokens": [51664, 286, 652, 867, 2145, 466, 7318, 51764], "temperature": 0.0, "avg_logprob": -0.05401112548017924, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.09252911806106567}, {"id": 7286, "seek": 2059364, "start": 20593.64, "end": 20595.64, "text": " and computer science in general", "tokens": [50364, 293, 3820, 3497, 294, 2674, 50464], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7287, "seek": 2059364, "start": 20595.64, "end": 20597.64, "text": " so", "tokens": [50464, 370, 50564], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7288, "seek": 2059364, "start": 20597.64, "end": 20599.64, "text": " you could totally feel free to subscribe there", "tokens": [50564, 291, 727, 3879, 841, 1737, 281, 3022, 456, 50664], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7289, "seek": 2059364, "start": 20599.64, "end": 20601.64, "text": " if you don't want to subscribe, that's fine", "tokens": [50664, 498, 291, 500, 380, 528, 281, 3022, 11, 300, 311, 2489, 50764], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7290, "seek": 2059364, "start": 20601.64, "end": 20603.64, "text": " you could always unsubscribe later if you want to", "tokens": [50764, 291, 727, 1009, 2693, 9493, 1780, 498, 291, 528, 281, 50864], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7291, "seek": 2059364, "start": 20603.64, "end": 20605.64, "text": " it's completely free", "tokens": [50864, 309, 311, 2584, 1737, 50964], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7292, "seek": 2059364, "start": 20605.64, "end": 20607.64, "text": " but yeah, also have a GitHub repo in the description", "tokens": [50964, 457, 1338, 11, 611, 362, 257, 23331, 49040, 294, 264, 3855, 51064], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7293, "seek": 2059364, "start": 20607.64, "end": 20609.64, "text": " for all the code that we used", "tokens": [51064, 337, 439, 264, 3089, 300, 321, 1143, 51164], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7294, "seek": 2059364, "start": 20609.64, "end": 20611.64, "text": " not the data because it's way too big", "tokens": [51164, 406, 264, 1412, 570, 309, 311, 636, 886, 955, 51264], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7295, "seek": 2059364, "start": 20611.64, "end": 20613.64, "text": " but", "tokens": [51264, 457, 51364], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7296, "seek": 2059364, "start": 20613.64, "end": 20615.64, "text": " all of the code and the Wizard of Oz", "tokens": [51364, 439, 295, 264, 3089, 293, 264, 37449, 295, 29843, 51464], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7297, "seek": 2059364, "start": 20615.64, "end": 20617.64, "text": " Text file", "tokens": [51464, 18643, 3991, 51564], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7298, "seek": 2059364, "start": 20617.64, "end": 20619.64, "text": " so that's all in the GitHub repo in the description", "tokens": [51564, 370, 300, 311, 439, 294, 264, 23331, 49040, 294, 264, 3855, 51664], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}, {"id": 7299, "seek": 2059364, "start": 20619.64, "end": 20621.64, "text": " thanks for watching", "tokens": [51664, 3231, 337, 1976, 51764], "temperature": 0.0, "avg_logprob": -0.09745475674463698, "compression_ratio": 1.8410041841004183, "no_speech_prob": 0.02671939879655838}], "language": "en"}