WEBVTT

00:00.000 --> 00:04.000
Learn how to build your own large language model from scratch.

00:04.360 --> 00:09.840
This course goes into the data handling, math and transformers behind large language models.

00:09.960 --> 00:11.800
Elliot Arledge created this course.

00:12.040 --> 00:18.640
He will help you gain a deep understanding of how LLMs work and how they can be used in various applications.

00:19.040 --> 00:20.600
So let's get started.

00:21.000 --> 00:22.880
Welcome to Intro to Language Modeling.

00:22.920 --> 00:25.280
In this course, you're going to learn a lot of crazy stuff.

00:25.320 --> 00:27.080
Okay, I'm just going to give you a heads up.

00:27.120 --> 00:28.960
It's going to be a lot of crazy stuff we learn here.

00:29.240 --> 00:31.400
However, it will not be insanely hard.

00:31.640 --> 00:35.720
I don't expect you have any any experience in calculus or linear algebra.

00:36.640 --> 00:39.040
A lot of courses out there do assume that, but I will not.

00:39.920 --> 00:41.200
We're going to build up from square one.

00:41.320 --> 00:46.240
We're going to take baby steps when it comes to new fundamental concepts in math and machine learning.

00:46.600 --> 00:51.240
And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out.

00:51.920 --> 00:55.520
That way we don't take forever just taking baby steps through every little concept.

00:56.000 --> 00:58.560
This course is inspired by Andre Karpathy's.

00:59.400 --> 01:01.320
Building a GPT from scratch lecture.

01:02.240 --> 01:03.240
So shout out to him.

01:03.880 --> 01:09.680
And yeah, we don't assume you have any experience, maybe three months of Python experience.

01:10.480 --> 01:13.000
Just so the syntax is sort of familiar and you can.

01:14.080 --> 01:19.120
You're able to follow along that way, but no matter how smart you are, how quick you learn.

01:20.080 --> 01:25.240
The willingness to put in the hours is the most important because this is material that you won't normally come across.

01:26.200 --> 01:35.800
So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water.

01:36.240 --> 01:37.280
Water is very important.

01:37.760 --> 01:41.440
And yeah, hopefully you can make it to the end of this.

01:42.400 --> 01:43.040
You can do it.

01:44.280 --> 01:51.640
Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing.

01:52.280 --> 01:57.600
We'll be scaling the data to about 45 gigabytes for the entire training data set.

01:58.160 --> 02:03.560
So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45.

02:04.440 --> 02:09.240
So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine.

02:09.240 --> 02:16.640
You can just download a different data set and sort of follow the same data pipeline that I do in this video.

02:17.640 --> 02:21.040
Through the course, you may see me switch between Mac OS and Windows.

02:21.440 --> 02:26.640
The code still works all the same, both operating systems, and I'll be using a tool called SSH.

02:26.640 --> 02:42.640
It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook.

02:43.640 --> 02:49.640
So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording.

02:50.640 --> 02:53.640
So why am I creating this course?

02:53.640 --> 03:03.640
Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space.

03:03.640 --> 03:09.640
So I intend to build up from baby steps and then larger steps when things are fairly simple to work with.

03:10.640 --> 03:19.640
And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you.

03:19.640 --> 03:24.640
So with that being said, let's go ahead and jump in to the good stuff.

03:24.640 --> 03:29.640
So in order to develop this project step by step, we're going to use something called Jupyter notebooks.

03:29.640 --> 03:34.640
And you can sort of play with these in the Anaconda prompt or at least launch them from here.

03:34.640 --> 03:38.640
So Anaconda prompt is just great for anything machine learning related.

03:38.640 --> 03:40.640
So make sure to have this installed.

03:40.640 --> 03:47.640
I will link a video in the description so that you can sort of set this up and install it step by step guide in there.

03:47.640 --> 03:53.640
So we can do from this point is sort of just set up our project and initialize everything.

03:53.640 --> 04:03.640
So I'm going to do is just head over into my directory that I want to be Python testing.

04:03.640 --> 04:10.640
We're going to make a directory free code camp GPT course.

04:10.640 --> 04:14.640
And then from this point, we're going to go and make a virtual environment.

04:14.640 --> 04:26.640
So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around.

04:26.640 --> 04:29.640
And what the virtual environment does is it sort of separates that.

04:29.640 --> 04:34.640
So you have this isolated environment over here, and you can just play around with this however you want.

04:34.640 --> 04:43.640
And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system.

04:43.640 --> 04:46.640
When you're not in a virtual environment, if that makes sense.

04:46.640 --> 04:51.640
So we're going to go ahead and set that up right now by using Python dash M.

04:51.640 --> 04:55.640
And then we're going to go V and V for virtual V and V and then CUDA.

04:55.640 --> 05:06.640
So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs.

05:06.640 --> 05:09.640
GPUs are going to accelerate this a ton.

05:09.640 --> 05:13.640
And basically CUDA is just that little feature in the GPU that lets us do that.

05:13.640 --> 05:16.640
So we're going to make an environment called CUDA.

05:16.640 --> 05:17.640
I'm going to go and press enter.

05:17.640 --> 05:20.640
It's going to do that for us going to take a few seconds.

05:20.640 --> 05:23.640
So now that's done, we can go ahead and do CUDA.

05:23.640 --> 05:28.640
And we're just going to basically activate this environment so we can start developing in it.

05:28.640 --> 05:33.640
I'm going to go backslash, we're going to go scripts, and then activate.

05:33.640 --> 05:35.640
So now you can see it says CUDA base.

05:35.640 --> 05:39.640
So we're in CUDA and then secondary base.

05:39.640 --> 05:41.640
So it's going to prioritize CUDA.

05:41.640 --> 05:46.640
So from this point, we can actually start installing some stuff, some libraries here.

05:46.640 --> 05:53.640
So we can go pip three install Matt plot lib numpy.

05:54.640 --> 06:01.640
We're going to use p y l m z a l z m a.

06:01.640 --> 06:04.640
And then what are some other ones?

06:04.640 --> 06:07.640
We're going to do IPY kernel.

06:07.640 --> 06:14.640
This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks.

06:14.640 --> 06:16.640
So that's why that's important.

06:16.640 --> 06:19.640
And then just the actual Jupyter notebook feature.

06:19.640 --> 06:21.640
So go and press enter.

06:21.640 --> 06:22.640
Those are going to install.

06:22.640 --> 06:25.640
That's going to take a few seconds to do.

06:25.640 --> 06:33.640
So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm.

06:33.640 --> 06:37.640
And don't quote me on this, but I'm pretty sure it's based in C plus plus.

06:37.640 --> 06:40.640
So you actually need some build tools for this.

06:40.640 --> 06:45.640
And you can get that with visual studio build tools.

06:45.640 --> 06:50.640
So what you're you might see, you might see a little error and basically go to that website.

06:50.640 --> 06:51.640
You're going to get this right here.

06:51.640 --> 06:54.640
So just go ahead and download build tools.

06:54.640 --> 06:57.640
What's going to download here, you're going to click on that.

06:57.640 --> 06:59.640
It's going to, it's going to set up.

06:59.640 --> 07:05.640
And then you're going to go ahead and click continue.

07:05.640 --> 07:09.640
And then at this point, you can go ahead and click modify if you see this here.

07:09.640 --> 07:14.640
And then you might get to a little workloads section here.

07:14.640 --> 07:16.640
So once you're at workloads, that's good.

07:16.640 --> 07:21.640
What you're going to make sure is that you have these two checked off right here.

07:21.640 --> 07:23.640
Just make sure that you have these two.

07:23.640 --> 07:25.640
I'm not sure what desktop particularly does.

07:25.640 --> 07:32.640
It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects.

07:32.640 --> 07:35.640
So just get these two for now.

07:35.640 --> 07:36.640
That'll be good.

07:36.640 --> 07:40.640
And then you can click modify over here if you wanted to modify just like that.

07:40.640 --> 07:45.640
And then you should be good to rerun that command.

07:45.640 --> 07:57.640
So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch.

07:57.640 --> 07:59.640
We're not going to do it like this.

07:59.640 --> 08:07.640
What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch.

08:07.640 --> 08:12.640
So it's going to install the CUDA extension, which will allow us to utilize the GPU.

08:12.640 --> 08:14.640
So it's just this command right here.

08:14.640 --> 08:25.640
And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started.

08:25.640 --> 08:29.640
And then you'll be able to see this right here.

08:29.640 --> 08:35.640
So we have stable windows pip Python and CUDA 11.7 or 11.8.

08:35.640 --> 08:36.640
So I just clicked on this.

08:36.640 --> 08:43.640
And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch.

08:43.640 --> 08:49.640
And then with this index URL for the CUDA 11.8.

08:49.640 --> 08:53.640
So that's pretty much all we're doing there to install CUDA.

08:53.640 --> 08:54.640
That's part of our torch.

08:54.640 --> 08:58.640
So we can go ahead and click enter on this.

08:58.640 --> 08:59.640
So great.

08:59.640 --> 09:04.640
We've installed a lot of things, a lot of libraries, a lot of setup has been done already.

09:04.640 --> 09:08.640
What I want to check now is just to make sure that our Python version is what we want.

09:08.640 --> 09:11.640
So I've done version 3.10.9.

09:11.640 --> 09:12.640
That's great.

09:12.640 --> 09:17.640
If you're between 3.9, 3.10, 3.11, that's perfect.

09:17.640 --> 09:20.640
So if you're in between those, it should be fine.

09:20.640 --> 09:24.640
At this point, we can just jump right into our Jupyter Notebook.

09:24.640 --> 09:27.640
So the command for that is just Jupyter Notebook.

09:27.640 --> 09:28.640
It's about like that.

09:28.640 --> 09:31.640
Click enter.

09:31.640 --> 09:34.640
It's going to send us into here.

09:35.640 --> 09:42.640
And I've created this little biogram.ipynb here in my VS Code.

09:42.640 --> 09:46.640
So pretty much you need to actually type some stuff in it.

09:46.640 --> 09:52.640
And you need to make sure that it has the ipynb extension or else it won't work.

09:52.640 --> 09:58.640
So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason.

09:58.640 --> 10:00.640
So just make sure you type some stuff in it.

10:00.640 --> 10:01.640
Open that in VS Code.

10:01.640 --> 10:05.640
Type, I don't know, a equals 3 or str equals banana.

10:05.640 --> 10:07.640
I don't care.

10:07.640 --> 10:12.640
At this point, let's go ahead and pop into here.

10:12.640 --> 10:14.640
So this is what our notebook is going to look like.

10:14.640 --> 10:19.640
And we're going to be working with this quite a bit throughout this course.

10:19.640 --> 10:27.640
So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook.

10:27.640 --> 10:33.640
And make sure that we can interact with it from this kernel rather than just through the command prompt.

10:33.640 --> 10:35.640
So we're going to go ahead and check here.

10:35.640 --> 10:37.640
And I have a virtual environment here.

10:37.640 --> 10:42.640
You may not, but all we're going to do is basically go into here.

10:42.640 --> 10:44.640
We're going to end this.

10:44.640 --> 10:56.640
And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install.

10:57.640 --> 11:04.640
User, you'll see why we're doing this in the second user name equals CUDA.

11:04.640 --> 11:07.640
This is from the virtual environment we initialized before.

11:07.640 --> 11:09.640
So that's the name of the virtual environment.

11:09.640 --> 11:20.640
And then the display name, how it's actually going to look in the terminal is going to be display name.

11:21.640 --> 11:28.640
We'll just call it CUDA GPT.

11:28.640 --> 11:29.640
I don't know.

11:29.640 --> 11:30.640
That sounds like a cool name.

11:30.640 --> 11:32.640
And I'm going to press enter.

11:32.640 --> 11:37.640
It's going to make this environment for us great installed.

11:37.640 --> 11:38.640
Good.

11:38.640 --> 11:45.640
So we can go and run our notebook again and we'll see if this changes.

11:45.640 --> 11:52.640
So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT.

11:52.640 --> 11:54.640
Let's click that.

11:54.640 --> 11:55.640
Sweet.

11:55.640 --> 12:01.640
So now we can actually start doing more and just sort of experimenting with how the notebooks

12:01.640 --> 12:08.640
work and actually how we can build up this bi-gram model and sort of learning how language

12:08.640 --> 12:10.640
models work from scratch.

12:10.640 --> 12:11.640
So let's go ahead and do that.

12:11.640 --> 12:19.640
Now that we jump into this actual code here, what I want to do is delete all of these.

12:19.640 --> 12:20.640
Good.

12:20.640 --> 12:25.640
So now what I'm going to do is just get a small little data set, just very small for us to

12:25.640 --> 12:30.640
work with that we can sort of try to make a bi-gram out of, something very small.

12:30.640 --> 12:36.640
So what we can do is go to this website called Project Gutenberg and they basically just

12:36.640 --> 12:41.640
have a bunch of free books that are licensed under Creative Commons.

12:41.640 --> 12:45.640
So we can use all of these for free.

12:45.640 --> 12:49.640
So let's use the Wizard of Oz.

12:49.640 --> 12:53.640
Put it at the end of Wizard of Oz.

12:53.640 --> 12:55.640
Great.

12:55.640 --> 12:58.640
So what we're going to want to do is just click on plain text here.

12:58.640 --> 12:59.640
Great.

12:59.640 --> 13:09.640
So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore

13:09.640 --> 13:11.640
of underscore Oz.

13:11.640 --> 13:12.640
Good.

13:12.640 --> 13:24.640
So now what I'm going to do is we should probably drag this into, we should drag this into our

13:24.640 --> 13:25.640
folder here.

13:25.640 --> 13:30.640
I'm just going to pop that into there.

13:30.640 --> 13:31.640
Good stuff.

13:31.640 --> 13:33.640
Did that work?

13:33.640 --> 13:34.640
Sweet.

13:34.640 --> 13:37.640
So now we have our Wizard of Oz text in here, we can open that.

13:37.640 --> 13:41.640
What we can do is start of this book.

13:41.640 --> 13:42.640
Okay.

13:42.640 --> 13:49.640
So we can go ahead and go down to when it starts.

13:49.640 --> 13:58.640
Sweet.

13:58.640 --> 14:01.640
So maybe we'll just cut it here.

14:01.640 --> 14:03.640
That'd be a good place to start.

14:03.640 --> 14:04.640
Just like that.

14:04.640 --> 14:07.640
I'll put a few spaces.

14:07.640 --> 14:08.640
Good.

14:08.640 --> 14:10.640
So now we have this book.

14:10.640 --> 14:17.640
We go to the bottom here just to get rid of some of this other licensing stuff, which

14:17.640 --> 14:23.640
might get in the way with our predictions in the context of the entire book.

14:23.640 --> 14:26.640
So let's just go down to when that starts.

14:26.640 --> 14:29.640
End of the book.

14:29.640 --> 14:31.640
Okay.

14:31.640 --> 14:37.640
So we've gotten all that.

14:37.640 --> 14:38.640
That is done.

14:38.640 --> 14:41.640
Get rid of the illustration there.

14:41.640 --> 14:42.640
Perfect.

14:42.640 --> 14:45.640
So now we have this Wizard of Oz text that we can work with.

14:45.640 --> 14:47.640
Let's close that up.

14:47.640 --> 14:48.640
233 kilobytes.

14:48.640 --> 14:49.640
Awesome.

14:49.640 --> 14:50.640
Very small size.

14:50.640 --> 14:51.640
We can work with this.

14:51.640 --> 14:52.640
This is great.

14:52.640 --> 14:54.640
So we have this wizard of Oz dot txt file.

14:54.640 --> 14:55.640
And what are we going to do with that?

14:55.640 --> 15:00.640
Well, we're going to try to train a transformer or at least a background language model on

15:00.640 --> 15:01.640
this text.

15:01.640 --> 15:05.640
So in order to do that, we need to sort of learn how to manage this text file, how to

15:05.640 --> 15:07.640
open it, et cetera.

15:07.640 --> 15:15.640
So we're going to go ahead and open this and do wizard of Oz.

15:15.640 --> 15:16.640
Like that.

15:16.640 --> 15:17.640
And we're going to open in read mode.

15:17.640 --> 15:22.640
And then we're going to use the encoding utf 8 just like that.

15:22.640 --> 15:25.640
So this is the file mode that you're going to open in.

15:25.640 --> 15:26.640
There's read mode.

15:26.640 --> 15:27.640
There's write mode.

15:27.640 --> 15:28.640
There's read binary.

15:28.640 --> 15:29.640
There's write binary.

15:29.640 --> 15:36.640
And those are really the only ones we're going to be worrying about for this video.

15:36.640 --> 15:40.640
The other ones you can look into in your spare time if you'd like to.

15:40.640 --> 15:43.640
I've already seen using those four for now.

15:43.640 --> 15:47.640
And then the encoding is just what type of character coding are we using?

15:47.640 --> 15:48.640
That's pretty much it.

15:48.640 --> 15:51.640
We can just open this as F short for file.

15:51.640 --> 15:54.640
I'm going to go text equals f dot read.

15:55.640 --> 15:58.640
I'm going to read this file stored in a string variable.

15:58.640 --> 16:01.640
And then we can print some stuff about it.

16:01.640 --> 16:05.640
So we can go print the length of this text.

16:05.640 --> 16:06.640
Run that.

16:06.640 --> 16:10.640
We get the length of the text.

16:10.640 --> 16:16.640
We could print the first 200 characters of the text.

16:16.640 --> 16:17.640
Sure.

16:17.640 --> 16:19.640
So you have the first 200 characters.

16:19.640 --> 16:20.640
Great.

16:21.640 --> 16:24.640
So now we know how to, you know, just play with characters.

16:24.640 --> 16:28.640
At least just see what the characters actually look like.

16:28.640 --> 16:33.640
So now we can do a little bit more from this point, which is going to be encoders.

16:33.640 --> 16:40.640
And before we get into that, what I'm going to do is put these into a little vocabulary

16:40.640 --> 16:42.640
list that we can work with.

16:42.640 --> 16:48.640
So all I'm going to do is I'm going to say we're going to make a charge variable.

16:48.640 --> 16:54.640
So the charge is going to be all the charge or all the characters in this text piece.

16:54.640 --> 17:08.640
So we're going to make a sorted set of text here, and we're going to just print out charge.

17:08.640 --> 17:09.640
So look at that.

17:09.640 --> 17:13.640
We have a giant array of all these characters.

17:13.640 --> 17:19.640
So now we can, what we can do is we can use something called a tokenizer and a tokenizer

17:19.640 --> 17:22.640
consists of an encoder and a decoder.

17:22.640 --> 17:28.640
What an encoder does is it's actually going to convert each character or sorry, each element

17:28.640 --> 17:31.640
of this array to an integer.

17:31.640 --> 17:34.640
So maybe this would be a zero.

17:34.640 --> 17:36.640
This would be a one, right?

17:36.640 --> 17:42.640
So a new, a new line or an enter would be a zero, a space would be a one exclamation

17:42.640 --> 17:44.640
mark would be a two, et cetera, right?

17:44.640 --> 17:46.640
All the way to the length of them.

17:46.640 --> 17:51.640
And then what we could do is we could even, we could even print the length of these characters.

17:51.640 --> 17:53.640
So you can see how many there actually are.

17:53.640 --> 17:58.640
So there's 81 characters in the entire, in the entire Wizard of Oz book.

17:58.640 --> 18:02.640
So I've written some code here that is going to do that job for us, the job of tokenizers.

18:02.640 --> 18:07.640
So what we do is we just use a little generator, some generator for loops here,

18:07.640 --> 18:13.640
a generator for loops rather, and we make a little mapping from strings to integers

18:13.640 --> 18:16.640
and integers to strings, given the vocabulary.

18:16.640 --> 18:19.640
So we just enumerate through each of these.

18:19.640 --> 18:24.640
We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right?

18:24.640 --> 18:26.640
That's basically all we're doing here.

18:26.640 --> 18:28.640
And we have an encoder and a decoder.

18:28.640 --> 18:34.640
So let's say we wanted to convert the string hello to integers.

18:34.640 --> 18:40.640
So we go encode, and we could do hello, just like that.

18:40.640 --> 18:45.640
And then we could go ahead and print this out.

18:45.640 --> 18:46.640
Perfect.

18:46.640 --> 18:48.640
Let's go ahead and run that.

18:48.640 --> 18:49.640
Boom.

18:49.640 --> 18:53.640
So now we have a conversion from characters to integers.

18:53.640 --> 18:59.640
And then if we wanted to maybe convert this back, so decode it,

18:59.640 --> 19:05.640
sort this in a little, maybe decoded hello equals that.

19:05.640 --> 19:13.640
And then we could go or encoded rather encoded hello.

19:13.640 --> 19:18.640
And then we could go decoded.

19:18.640 --> 19:24.640
Hello is equal to we go decode and we can use the encoded hello.

19:24.640 --> 19:28.640
So we're going to go ahead and encode this into integers.

19:28.640 --> 19:33.640
And then we're going to decode the integers back to a character format.

19:33.640 --> 19:36.640
So let's go ahead and print that out.

19:36.640 --> 19:41.640
We're going to go ahead and print the decoded hello.

19:41.640 --> 19:42.640
Perfect.

19:42.640 --> 19:44.640
So now we get that.

19:44.640 --> 19:48.640
So I'm going to fill you in on a little background information about these tokenizers.

19:48.640 --> 19:53.640
So right now we're using the character level tokenizer, which takes basically each character

19:53.640 --> 19:56.640
and converts it to an integer equivalent.

19:56.640 --> 20:03.640
So we have a very small vocabulary and a very large amount of tokens to convert.

20:03.640 --> 20:08.640
So if we have 40,000 individual characters, it means we have a small vocabulary to work

20:08.640 --> 20:12.640
with, but a lot of characters to encode and decode, right?

20:12.640 --> 20:19.640
If we have, if we work with maybe a word level tokenizer, that means we have a ton, like

20:19.640 --> 20:24.640
every single word in the English language, I mean, if you're working with multiple languages,

20:24.640 --> 20:30.640
this could be like, you know, a lot, very large amount of tokens.

20:30.640 --> 20:35.640
So you're going to have like maybe millions or billions or trillions if you're, if you're

20:35.640 --> 20:36.640
doing something weird.

20:36.640 --> 20:42.640
But in that case, you're going to have a way smaller set to work with.

20:42.640 --> 20:48.640
So you're going to have very large vocabulary, but a very small amount to encode and decode.

20:48.640 --> 20:54.640
So if you have a subword tokenizer, that means you're going to be somewhere in between a character

20:54.640 --> 20:57.640
level and a word level tokenizer, if that makes sense.

20:57.640 --> 21:02.640
So in the context of language models, it's really important that we're efficient with our data

21:02.640 --> 21:05.640
and just having a giant string might not work the best.

21:05.640 --> 21:10.640
And we're going to be using a machine learning framework called pi torch or torch.

21:10.640 --> 21:13.640
So I've imported this right here.

21:14.640 --> 21:21.640
And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well.

21:21.640 --> 21:28.640
A lot of the linear algebra, which involves a type of data structure called tensors.

21:28.640 --> 21:30.640
So tensors are pretty much matrices.

21:30.640 --> 21:32.640
If you're not familiar with those, that's fine.

21:32.640 --> 21:34.640
We'll go over them more in the course.

21:34.640 --> 21:41.640
But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with.

21:41.640 --> 21:43.640
So I'm going to go ahead and delete these here.

21:43.640 --> 21:49.640
And all we can do is just make our data element.

21:49.640 --> 21:53.640
We could this is going to be the entire text data of the entire Wizard of Oz.

21:53.640 --> 22:03.640
So we could go ahead and make this data equals and we're going to go torch tensor.

22:03.640 --> 22:06.640
And then we're going to go and code.

22:06.640 --> 22:08.640
We're going to put the text inside of that.

22:08.640 --> 22:11.640
So we're going to go ahead and encode this text right here.

22:11.640 --> 22:24.640
And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long.

22:24.640 --> 22:30.640
This basically means we're just going to have this as a super long sequence of integers.

22:30.640 --> 22:37.640
And yeah, let's go see what we can do with this torch tensor element right here.

22:37.640 --> 22:44.640
So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data.

22:44.640 --> 22:48.640
So it's pretty much the same thing in terms of working with arrays.

22:48.640 --> 22:59.640
It's just a different type of data structure in the context of pi torch sort of easier to work within that way.

22:59.640 --> 23:11.640
Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot.

23:11.640 --> 23:16.640
But we're going to go over some of this stuff later in the course just about how to do all this math.

23:16.640 --> 23:28.640
We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff.

23:28.640 --> 23:33.640
So next I'm going to talk about is something called validation and training splits.

23:33.640 --> 23:40.640
So why don't we just, you know, use the entire text document and only train on that entire text corpus?

23:40.640 --> 23:41.640
Why don't we train on that?

23:41.640 --> 23:47.640
Well, the reason we actually split into training and validation sets, I'm going to show you right here.

23:47.640 --> 23:49.640
So we have this giant text corpus.

23:49.640 --> 23:50.640
It's a super long text file.

23:50.640 --> 23:54.640
Think of it as a, you know, an essay, but a lot of pages.

23:54.640 --> 24:00.640
So this is our entire corpus and we make our training set, you know, 80% of it.

24:00.640 --> 24:01.640
So maybe this much.

24:01.640 --> 24:05.640
And then the other validation is this 20% right here.

24:05.640 --> 24:06.640
Okay.

24:06.640 --> 24:17.640
So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out.

24:17.640 --> 24:19.640
It would have it in the entire thing memorized.

24:19.640 --> 24:21.640
It wouldn't really get anything useful out of that.

24:21.640 --> 24:24.640
You would only know this document.

24:24.640 --> 24:29.640
But what the purpose of language modeling is, is to generate text that's like the training data.

24:29.640 --> 24:32.640
And this is exactly why we put into splits.

24:32.640 --> 24:40.640
So if we, if we run our training split right here, it's only going to know 80% of that entire corpus.

24:40.640 --> 24:44.640
And it's only going to generate on that 80% instead of the entire thing.

24:44.640 --> 24:49.640
And then we have our other 20%, which only knows 20% of the entire corpus.

24:49.640 --> 24:56.640
So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document.

24:56.640 --> 25:00.640
We're trying to generate text that's like the document.

25:00.640 --> 25:08.640
Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare.

25:08.640 --> 25:15.640
And the point is to generate Shakespearean like text, but not exactly what it looked like.

25:15.640 --> 25:21.640
Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right?

25:21.640 --> 25:23.640
We're trying to generate text that's like it.

25:23.640 --> 25:29.640
So that's the entire reason, or at least that's most of the reason why we use train and vowel splits.

25:29.640 --> 25:33.640
So you might be wondering, you know, like, why is this even called the bi-gram language model?

25:33.640 --> 25:36.640
I'm actually going to show you how that works right now.

25:36.640 --> 25:40.640
So if we go back to our whiteboard here, I've drawn a little sketch.

25:40.640 --> 25:47.640
So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now.

25:47.640 --> 25:49.640
We're just working with characters.

25:49.640 --> 25:52.640
Pretty much we have two, right?

25:52.640 --> 25:55.640
So by means to the by prefix means two.

25:55.640 --> 25:58.640
So we're going to, we're going to have a bi-gram.

25:58.640 --> 26:03.640
So given maybe, I mean, there's nothing before an H in this content.

26:03.640 --> 26:08.640
So we just assume that's the start of content, and then that's going to point to an H.

26:08.640 --> 26:12.640
So H is the most likely to come after the start.

26:12.640 --> 26:19.640
And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L,

26:19.640 --> 26:22.640
we're going to have another L, and then L leads to O, right?

26:22.640 --> 26:28.640
So maybe there's going to be some probabilities associated with these.

26:28.640 --> 26:31.640
So that's pretty much how it's how it's going to predict right now.

26:31.640 --> 26:35.640
It's only going to consider the previous character to predict the next.

26:35.640 --> 26:38.640
So we have given this one, we predict the next.

26:38.640 --> 26:41.640
So there's two, which is why it's called bi-gram language model.

26:41.640 --> 26:48.640
So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want,

26:48.640 --> 26:55.640
how we can actually implement this into a neural network, an artificial neural network, and train it.

26:55.640 --> 27:04.640
So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here,

27:04.640 --> 27:11.640
just a small snippet, and we're going to make some predictions and we're going to make some targets out of that.

27:11.640 --> 27:18.640
So our block size is just a bunch of encoded characters or integers that we have predictions and targets.

27:18.640 --> 27:24.640
So let's say we take a small little size of maybe block size of five, okay?

27:24.640 --> 27:30.640
So we have this tiny little tensor of five integers and these are our predictions.

27:30.640 --> 27:42.640
So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one.

27:42.640 --> 27:48.640
So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside.

27:48.640 --> 27:56.640
So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one.

27:56.640 --> 27:59.640
So we're going to be accessing the same indices.

27:59.640 --> 28:03.640
So at index zero, it's going to be five, index zero is going to be 67, right?

28:03.640 --> 28:07.640
So 67 is following five in the bi-gram language model.

28:07.640 --> 28:10.640
So that's pretty much all we do.

28:10.640 --> 28:20.640
So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target.

28:20.640 --> 28:25.640
And then we can optimize for reducing that error.

28:25.640 --> 28:39.640
So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here.

28:39.640 --> 28:43.640
We would take a little snippet random.

28:43.640 --> 28:58.640
It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size.

28:58.640 --> 29:03.640
So five.

29:03.640 --> 29:06.640
Ignore my terrible writing again.

29:06.640 --> 29:22.640
And then this one would just be it would just be one up to block size or five plus one.

29:22.640 --> 29:24.640
So we'll be up to six, right?

29:24.640 --> 29:26.640
And that's that's pretty much all we do.

29:26.640 --> 29:28.640
This is exactly what it's going to look like in the code.

29:28.640 --> 29:33.640
So I've written some code here that does exactly what we just talked about in Python.

29:33.640 --> 29:41.640
So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger.

29:41.640 --> 29:51.640
And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one.

29:51.640 --> 29:56.640
So we make it position one up to block size plus one little offset there.

29:56.640 --> 30:08.640
And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are.

30:08.640 --> 30:12.640
So this is what this looks like in Python, right, we can do predictions.

30:12.640 --> 30:15.640
But this isn't really scalable yet.

30:15.640 --> 30:18.640
This is sequential right sequential.

30:18.640 --> 30:25.640
It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly.

30:25.640 --> 30:30.640
That only happens sequentially it's this one and this task and this task and this task, right.

30:30.640 --> 30:37.640
But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel.

30:37.640 --> 30:52.640
So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them.

30:52.640 --> 31:05.640
So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot.

31:05.640 --> 31:08.640
So I'm going to illustrate that for you right now.

31:08.640 --> 31:10.640
So let's just say we have a block.

31:10.640 --> 31:12.640
Okay, block looks like this.

31:12.640 --> 31:19.640
And we have some we have some integers in between here.

31:19.640 --> 31:22.640
So this is a block.

31:22.640 --> 31:24.640
Okay.

31:24.640 --> 31:28.640
Now, if we want to make multiple of these, we're just going to stack them.

31:28.640 --> 31:32.640
So we're going to make another one.

31:32.640 --> 31:35.640
Another one.

31:35.640 --> 31:36.640
Another one.

31:36.640 --> 31:38.640
So let's say we have four batches.

31:38.640 --> 31:39.640
Okay.

31:39.640 --> 31:40.640
Or sorry, four blocks.

31:40.640 --> 31:45.640
So we have four different blocks that are just stacked on top of each other.

31:45.640 --> 31:50.640
And we can represent this as a new hyper parameter called batch size.

31:50.640 --> 31:55.640
This is going to tell us how many of these sequences can we actually process in parallel.

31:55.640 --> 32:03.640
So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time.

32:03.640 --> 32:07.640
So this is a really good way to scale language models.

32:07.640 --> 32:12.640
And without these, you can't really expect any fast training or good performance at all.

32:12.640 --> 32:19.640
So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process.

32:19.640 --> 32:24.640
And we can, it just takes one line to do this actually.

32:24.640 --> 32:37.640
So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation.

32:38.640 --> 32:44.640
And if it's available, like it says it's available, we'll set the device to CUDA else CPU.

32:44.640 --> 32:47.640
So we're going to go and print out the device here.

32:47.640 --> 32:50.640
So that's going to run and we get CUDA.

32:50.640 --> 32:55.640
So that means we can use the GPU for a lot of our processing here.

32:55.640 --> 33:02.640
And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size.

33:02.640 --> 33:07.640
And then we're going to use batch size, which is how many blocks we're doing in parallel.

33:07.640 --> 33:10.640
And we're just going to make this four for now.

33:10.640 --> 33:14.640
So these are our two hyper parameters that are very, very important for training.

33:14.640 --> 33:28.640
And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it.

33:28.640 --> 33:39.640
And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first.

33:39.640 --> 33:43.640
It'll shut down and you just go up Jupyter notebook again and then enter.

33:43.640 --> 33:52.640
And then after this is done, you should be able to just restart that and it will work.

33:52.640 --> 33:53.640
Hopefully.

33:53.640 --> 33:55.640
There we go.

33:55.640 --> 34:00.640
So I go in restart and clear outputs.

34:00.640 --> 34:02.640
And we can run that.

34:02.640 --> 34:03.640
See, we get boo.

34:03.640 --> 34:05.640
So awesome.

34:05.640 --> 34:10.640
Now, let's try to do some actual cool pie torch stop.

34:10.640 --> 34:14.640
So we're going to go in and import torch here.

34:14.640 --> 34:19.640
And then let's go ahead and try this random feature.

34:19.640 --> 34:21.640
So you go random.

34:21.640 --> 34:24.640
We'll do equals torch dot random.

34:24.640 --> 34:30.640
And then let's say we go minus 100 to 100.

34:30.640 --> 34:34.640
And then in brackets, we go six, just like that.

34:34.640 --> 34:40.640
So if we want to print this out here, or we could just go random like that.

34:40.640 --> 34:42.640
Run this block first.

34:42.640 --> 34:43.640
Good.

34:43.640 --> 34:44.640
And boom.

34:44.640 --> 34:47.640
So we get a tensor type.

34:47.640 --> 34:51.640
And all these numbers are we have we have six of them.

34:51.640 --> 34:53.640
So 123456.

34:53.640 --> 34:56.640
And they're between negative 100 and 100.

34:56.640 --> 35:02.640
So we're going to have to keep this in mind right here when we're getting our random

35:02.640 --> 35:05.640
batches from this giant text corpus.

35:05.640 --> 35:07.640
So let's try out a new one.

35:07.640 --> 35:09.640
Let's just try.

35:09.640 --> 35:11.640
We can make we can make tensors.

35:11.640 --> 35:12.640
We've done this before.

35:12.640 --> 35:17.640
So you do tensor equals torch dot tensor.

35:17.640 --> 35:30.640
So if you go 0.1, 1.2, here, I'll just copy and paste one right here.

35:30.640 --> 35:32.640
So we do this.

35:32.640 --> 35:38.640
And we can just do tensor and we'll get exactly this.

35:38.640 --> 35:45.640
So we get a three by two matrix.

35:45.640 --> 35:48.640
Now we're going to try a different one called zeros.

35:48.640 --> 35:51.640
So zeros is just torch dot zeros.

35:51.640 --> 35:56.640
And then inside of here, we could just do the dimensions or the shape of this.

35:56.640 --> 36:00.640
So two by three, and then we could just do zeros.

36:00.640 --> 36:02.640
And then go ahead and run that.

36:02.640 --> 36:06.640
So we get a two by three of zeros.

36:06.640 --> 36:10.640
And these are all floating point numbers, by the way.

36:10.640 --> 36:11.640
Maybe we could try ones.

36:11.640 --> 36:13.640
Now I know ones is pretty fun ones.

36:13.640 --> 36:17.640
So we both torch dot ones.

36:17.640 --> 36:19.640
It's pretty much the same as zeros.

36:19.640 --> 36:25.640
We could just do like maybe three by four and then print that ones out.

36:25.640 --> 36:28.640
So we have a three by four of ones.

36:28.640 --> 36:29.640
Sweet.

36:29.640 --> 36:37.640
So what if we do input equals torch dot empty.

36:37.640 --> 36:45.640
We can make this two by three.

36:45.640 --> 36:48.640
So these are interesting.

36:48.640 --> 36:55.640
These are pretty much a bunch of very either very large or very small numbers.

36:55.640 --> 36:58.640
I haven't particularly found a use case for this yet,

36:58.640 --> 37:01.640
but just another feature that PyTorch has.

37:01.640 --> 37:03.640
We have a range.

37:03.640 --> 37:09.640
So we go arrange equals torch dot arrange.

37:09.640 --> 37:14.640
I could do like five, for example, just do range.

37:14.640 --> 37:22.640
So now we have a tensor just sorted zero or rather starting at zero up to four.

37:22.640 --> 37:27.640
So five, just just like that.

37:27.640 --> 37:36.640
Line space equals torch dot line line space.

37:36.640 --> 37:44.640
Spelling is weird to three, 10, and then steps, for example, equals five.

37:44.640 --> 37:47.640
So it makes sense in a second here, go run.

37:47.640 --> 37:51.640
And we got a line space of steps equals five.

37:51.640 --> 37:54.640
So we have five different ones, boom, boom, boom, boom, boom.

37:54.640 --> 37:56.640
And we go all the way from three to 10.

37:56.640 --> 38:03.640
So pretty much getting all of the constant increments from three all the way up to 10 over five steps.

38:03.640 --> 38:07.640
So you're doing, you're basically adding the same amount every time.

38:07.640 --> 38:15.640
So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right?

38:15.640 --> 38:19.640
So just over five steps, we want to find what that constant increment is.

38:19.640 --> 38:22.640
So that's a pretty cool one.

38:22.640 --> 38:27.640
And then we have, we'll do log space, which is interesting.

38:27.640 --> 38:31.640
Log space equals torch dot log space.

38:31.640 --> 38:48.640
And then we'll go start, start equals negative 10 and equals 10.

38:48.640 --> 38:50.640
These are both start and end.

38:50.640 --> 38:51.640
You can either put these here.

38:51.640 --> 38:55.640
You can either put the start with them, start equals, or you don't have to.

38:55.640 --> 38:57.640
It's honestly up to you.

38:57.640 --> 39:00.640
And then we can put our steps again.

39:00.640 --> 39:03.640
So steps equals every five.

39:03.640 --> 39:05.640
Let's go ahead and run that.

39:05.640 --> 39:10.640
Oops, need to put log space there.

39:10.640 --> 39:11.640
So we get that.

39:11.640 --> 39:15.640
So we start at one of the negative 10.

39:15.640 --> 39:17.640
And then we just do this little increments here.

39:17.640 --> 39:19.640
So it goes 10, negative five, zero plus five times.

39:19.640 --> 39:20.640
Just over five steps.

39:20.640 --> 39:22.640
So that's pretty cool.

39:22.640 --> 39:24.640
What else do we have here?

39:24.640 --> 39:28.640
So we have I, torch dot I.

39:28.640 --> 39:30.640
I just have all these on my second screen here.

39:30.640 --> 39:36.640
So a bunch of examples just written out and we're just kind of visualizing what these can do.

39:36.640 --> 39:47.640
And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do.

39:47.640 --> 39:50.640
So we're just kind of experimenting with these.

39:50.640 --> 39:55.640
What we can do with the basics of pytorch and some of the very basic functions.

39:55.640 --> 40:01.640
So first I will print this out here.

40:01.640 --> 40:09.640
So we get pretty much just a diagonal line and it's in five.

40:09.640 --> 40:19.640
So you get a five by five matrix and pretty much just reduced row each long form.

40:19.640 --> 40:23.640
I don't know how to pronounce it, but that's pretty much what it looks like.

40:23.640 --> 40:26.640
So pretty cool stuff.

40:26.640 --> 40:29.640
Let's see what else we have.

40:29.640 --> 40:35.640
We have empty like.

40:35.640 --> 40:53.640
We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty.

40:53.640 --> 41:09.640
And then we can go two by three and then data type torch dot int 64 64 bit integers.

41:09.640 --> 41:17.640
And then let's see what happens here empty.

41:17.640 --> 41:20.640
So that's pretty cool.

41:20.640 --> 41:21.640
What else do we have?

41:21.640 --> 41:23.640
Yes, we can do timing as well.

41:23.640 --> 41:27.640
So I'm just going to erase all of these.

41:27.640 --> 41:38.640
You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit.

41:38.640 --> 41:41.640
But yeah, I'm actually going to delete all of these here.

41:41.640 --> 41:57.640
And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment.

41:57.640 --> 42:16.640
Cuda if torch dot Cuda underscore is dot Cuda is available.

42:16.640 --> 42:28.640
And then else you print out our device here and run this Cuda suite.

42:28.640 --> 42:44.640
So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality.

42:44.640 --> 42:49.640
We're just doing different experiments with a bunch of different tensors.

42:49.640 --> 42:56.640
So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time.

42:56.640 --> 43:00.640
So this comes with the operating system or sorry with with Python.

43:00.640 --> 43:03.640
You don't have to actually install this manually.

43:03.640 --> 43:13.640
So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now.

43:13.640 --> 43:20.640
So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds.

43:20.640 --> 43:26.640
So if we subtract end time, start time will get a three second difference and that would be the total elapsed time.

43:26.640 --> 43:34.640
And then this little number here, this four will be just how many decimal places we have.

43:34.640 --> 43:36.640
So I can go ahead and run this here.

43:36.640 --> 43:40.640
Time is not defined. Let's run that first.

43:40.640 --> 43:45.640
It's going to take, you know, almost no time at all.

43:45.640 --> 43:49.640
So we can actually increase this if we want to 10 and then run that again.

43:49.640 --> 43:53.640
Again, it's, you know, we're making up pretty much a one by one matrix.

43:53.640 --> 43:55.640
So just a just a zero.

43:55.640 --> 44:01.640
So we're not really going to get anything significant from that.

44:01.640 --> 44:17.640
But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network.

44:17.640 --> 44:27.640
That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem.

44:27.640 --> 44:31.640
It'll take maybe a few seconds at most like maybe 30 seconds to do that.

44:31.640 --> 44:35.640
And that's not going to be any time at all in the entire training process.

44:35.640 --> 44:46.640
So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU.

44:46.640 --> 44:48.640
So I have some code for that right here.

44:48.640 --> 44:52.640
So we're going to initialize a bunch of matrices here.

44:52.640 --> 44:58.640
So our sorry, tensors, and we have just basically random ones.

44:58.640 --> 45:03.640
So we have a 10,000 by 10,000, all random, all random floating point numbers.

45:03.640 --> 45:05.640
And then we're going to push these to the GPU.

45:05.640 --> 45:09.640
And we have two of these and then same thing for NumPy.

45:10.640 --> 45:17.640
So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.

45:17.640 --> 45:29.640
So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply.

45:29.640 --> 45:37.640
So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds.

45:37.640 --> 45:40.640
And then we have, see, look at that.

45:40.640 --> 45:45.640
So for the GPU, it took a little while to do that.

45:45.640 --> 45:49.640
And then for the CPU, it didn't take as long.

45:49.640 --> 45:56.640
So this is because there's the shape of these matrices are not really that big.

45:56.640 --> 45:58.640
They're just two dimensional, right?

45:58.640 --> 46:04.640
So it's see, this is something that the CPU can do very quickly because there's not that much to do.

46:04.640 --> 46:07.640
But let's say we want to bump it up a notch.

46:07.640 --> 46:15.640
So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.

46:15.640 --> 46:16.640
Hopefully that works.

46:16.640 --> 46:19.640
And then we can do, we'll just do the same thing.

46:19.640 --> 46:21.640
So just paste this.

46:22.640 --> 46:38.640
Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did.

46:38.640 --> 46:43.640
And this is because there's, you know, a lot more going on here.

46:43.640 --> 46:47.640
There's a lot more simple multiplication to do.

46:47.640 --> 46:55.640
So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model,

46:55.640 --> 47:01.640
we're not going to be doing very complex operations between all these tensors.

47:01.640 --> 47:04.640
They're going to be very similar to what we saw in here.

47:04.640 --> 47:09.640
The dimensionality and shape is going to be very similar to what we're seeing right now.

47:09.640 --> 47:11.640
You know, maybe three or four dimensions.

47:11.640 --> 47:15.640
And it's going to be very easy for a GPU to do this.

47:15.640 --> 47:18.640
They're not complex tasks that we need the CPU to do.

47:18.640 --> 47:20.640
They're not very hard at all.

47:20.640 --> 47:28.640
So when we give this task to parallel processing, it's going to be a ton quicker.

47:28.640 --> 47:31.640
So you're going to see why this matters later in the course.

47:31.640 --> 47:35.640
You're going to see this with some of the hyper parameters we're going to use,

47:35.640 --> 47:40.640
which I'm not going to get into quite yet, but over the next little bit,

47:40.640 --> 47:48.640
you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process.

47:48.640 --> 47:49.640
So this is great.

47:49.640 --> 47:57.640
Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency.

47:57.640 --> 48:03.640
So there's actually another term that we can use called a percentage percentage time.

48:03.640 --> 48:07.640
I don't know if that's exactly how you're supposed to call it, but that's what it is.

48:07.640 --> 48:12.640
And pretty much what it'll do is time how long it takes to execute a block.

48:12.640 --> 48:17.640
So we can see here there's CPU times zero nanoseconds.

48:17.640 --> 48:22.640
The end is for nano billionth of a second is a nanosecond and then wall time.

48:22.640 --> 48:28.640
So CPU time is how long it takes to execute on the CPU.

48:28.640 --> 48:36.640
The time that it's doing operations for and then the wall time would be how long it actually takes like in real time.

48:36.640 --> 48:40.640
How long do you have to wait? Do you have to wait until it's finished?

48:40.640 --> 48:45.640
So the only thing that the CPU CPU time doesn't include is waiting.

48:45.640 --> 48:50.640
So in an entire process, there's going to be some operations and there's going to be some waiting.

48:50.640 --> 48:56.640
Wall time is going to have both of those and CPU time is just the execution.

48:56.640 --> 49:02.640
So let's go ahead and continue with some of the basic PyTorch functions.

49:02.640 --> 49:06.640
So I've written some stuff down here.

49:06.640 --> 49:14.640
So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu.

49:14.640 --> 49:17.640
I don't think that's how you pronounce it, but we'll get into that more.

49:17.640 --> 49:24.640
Transposing, linear, concatenating, and the softmax function.

49:24.640 --> 49:28.640
So let's first start off here with the Torch.multinomial.

49:28.640 --> 49:34.640
So this is essentially a probability distribution based on the index that you give it.

49:34.640 --> 49:36.640
So we have probabilities here.

49:36.640 --> 49:39.640
We say 0.1 and 0.9.

49:39.640 --> 49:42.640
These numbers have to add up to one to make 100%.

49:42.640 --> 49:44.640
100% is one, one whole.

49:44.640 --> 49:47.640
So I have 10% and 90%.

49:47.640 --> 49:49.640
This is an index zero.

49:49.640 --> 49:55.640
So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one.

49:55.640 --> 50:05.640
So if I go ahead and run these up here.

50:05.640 --> 50:07.640
Give this a second to do its thing.

50:07.640 --> 50:11.640
So you can see that in the end we have our numSample set to 10.

50:11.640 --> 50:13.640
So it's going to give us 10 of these.

50:13.640 --> 50:15.640
1, 2, 3, 4, 5, 6, 7, 9, 10.

50:15.640 --> 50:17.640
And all of them are ones.

50:17.640 --> 50:20.640
If we run it again, we make it slightly different results.

50:20.640 --> 50:22.640
So now we have some zeros in there.

50:22.640 --> 50:25.640
But the zeros have very low probability of happening.

50:25.640 --> 50:29.640
As a matter of fact, exactly a 10% probability of happening.

50:29.640 --> 50:36.640
So we're going to use this later in predicting what word is going to come next.

50:36.640 --> 50:42.640
Let's move on to Torch.cat or short for Torch.concatenate.

50:42.640 --> 50:46.640
So this will essentially concatenate two tensors into one.

50:46.640 --> 50:50.640
So I initialize this tensor here, torch.tensor, 1, 2, 3, 4.

50:50.640 --> 50:52.640
It's one dimensional.

50:52.640 --> 50:56.640
And we have another tensor here that just contains five.

50:56.640 --> 51:03.640
So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5.

51:03.640 --> 51:08.640
We just combine them together and this is what will come out in the end.

51:08.640 --> 51:10.640
So we run that 1, 2, 3, 4, 5.

51:10.640 --> 51:12.640
Perfect.

51:12.640 --> 51:15.640
So we're going to actually use this when we're generating.

51:15.640 --> 51:18.640
When we're generating text given a context.

51:18.640 --> 51:22.640
So it's going to start from zero.

51:22.640 --> 51:25.640
We're going to use our probability distribution to pick the first one.

51:25.640 --> 51:33.640
And then based on the first one, we're going to, you know, we're going to predict the next character.

51:33.640 --> 51:41.640
And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted.

51:41.640 --> 51:44.640
So we have this, maybe like 100 characters over here.

51:44.640 --> 51:46.640
And then the next character that we're predicting is over here.

51:46.640 --> 51:48.640
We just concatenate these.

51:48.640 --> 51:54.640
And by the end, we will have all of the integers that we've predicted.

51:54.640 --> 51:57.640
So next up, we have torch.trill.

51:57.640 --> 52:03.640
And what this stands for, what the trail stands for is a triangle lower.

52:03.640 --> 52:07.640
So it's going to be in a sort of a triangle formation like this diagonal.

52:07.640 --> 52:12.640
It's going to be going from top left to bottom right.

52:12.640 --> 52:16.640
And so you're going to see a little bit more why later in this course.

52:16.640 --> 52:30.640
But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history.

52:30.640 --> 52:32.640
We're trying to predict the future.

52:32.640 --> 52:36.640
So giving the answers in the future isn't what we want to do at all.

52:36.640 --> 52:40.640
So maybe we've just predicted one and the rest of them we haven't predicted yet.

52:40.640 --> 52:42.640
So we set all these to zero.

52:42.640 --> 52:44.640
And then we predicted another one.

52:44.640 --> 52:45.640
And these are still zero.

52:45.640 --> 52:47.640
So these are talking to each other in history.

52:47.640 --> 52:58.640
And as and as our predictions add up, we have more and more history to look back to and less future, right?

52:58.640 --> 53:04.640
Basically, the premise of this is just making sure we can't communicate with the answer.

53:04.640 --> 53:11.640
We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet.

53:11.640 --> 53:13.640
They don't give you the answer sheet.

53:13.640 --> 53:19.640
So you have to know based on your history of knowledge, which answers to predict.

53:19.640 --> 53:22.640
And that's all that's going on here.

53:22.640 --> 53:26.640
And we have, I mean, you could probably guess this triangle upper.

53:26.640 --> 53:28.640
So we have all the upper ones.

53:28.640 --> 53:32.640
These are, you know, lower on the lower side and then these are on the upper side.

53:32.640 --> 53:34.640
So same concept there.

53:34.640 --> 53:37.640
And then we have a masked fill.

53:37.640 --> 53:48.640
So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here.

53:48.640 --> 53:53.640
So if you exponentiate zero, if you exponentiate zero, it'll become one.

53:53.640 --> 53:57.640
If you exponentiate negative infinity, it'll become zero.

53:57.640 --> 54:03.640
All that's going on here is we're doing approximately 2.71.

54:03.640 --> 54:08.640
And this is a constant that we use in the dot exp function.

54:08.640 --> 54:14.640
And then we're putting this to whatever power is in that current slot.

54:14.640 --> 54:16.640
So we have a zero here.

54:16.640 --> 54:26.640
So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71.

54:27.640 --> 54:40.640
And then 2.71 to the negative infinity is, of course, zero.

54:40.640 --> 54:43.640
So that's pretty much how we get from this to this.

54:43.640 --> 54:48.640
And we're just, we're simply just masking these over.

54:48.640 --> 54:50.640
So that's great.

54:50.640 --> 54:54.640
And I sort of showcase what the exp does.

54:54.640 --> 54:56.640
And we're just using this one right here.

54:56.640 --> 55:00.640
We're using this output and we're just plugging it into here.

55:00.640 --> 55:05.640
So it'll go from negative infinity to zero and then zero to one.

55:05.640 --> 55:08.640
So that's how we get from here to here.

55:08.640 --> 55:11.640
Now we have transposing.

55:11.640 --> 55:16.640
So transposing is when we sort of flip or swap the dimensions of a tensor.

55:16.640 --> 55:23.640
So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four.

55:23.640 --> 55:30.640
And we can use the transpose function to essentially flip any dimensions that we want.

55:30.640 --> 55:37.640
So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension,

55:37.640 --> 55:41.640
but we're pretty much swapping the zero with position with the second.

55:41.640 --> 55:45.640
So zero, one, two, we're swapping this one with this one.

55:45.640 --> 55:51.640
So the end result, like you would probably guess the shape of this is going to be 432 instead of 234.

55:51.640 --> 55:55.640
So you kind of just take a look at this and see, you know, which ones are being flipped.

55:55.640 --> 55:59.640
And those are the dimensions and that's the output.

55:59.640 --> 56:01.640
So hopefully that makes sense.

56:01.640 --> 56:03.640
Next up, we have torch dot stack.

56:03.640 --> 56:06.640
And this is where we're actually going to go.

56:06.640 --> 56:08.640
We're going to we're going to do more of this.

56:08.640 --> 56:15.640
We're actually going to use torch dot stack stack very shortly here when we're getting our batches.

56:15.640 --> 56:23.640
So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant,

56:23.640 --> 56:28.640
a giant length of integers or tokens.

56:28.640 --> 56:34.640
And all we're doing is we're just stacking them together in blocks or to make a batch.

56:34.640 --> 56:37.640
So that's pretty much what we're going to end up doing.

56:37.640 --> 56:39.640
And that's what torch dot stack does.

56:39.640 --> 56:46.640
We can take something that's one dimensional and then we can stack it to make it two dimensional.

56:46.640 --> 56:52.640
We can take something that's two dimensional and stack it a bunch of times to make it three dimensional.

56:52.640 --> 56:58.640
Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other.

56:58.640 --> 56:59.640
Now it's four dimensional.

56:59.640 --> 57:01.640
So hopefully that makes sense.

57:01.640 --> 57:05.640
All we're doing is we're just passing in each tensor that we're going to stack in order.

57:05.640 --> 57:09.640
So this is our little output here and that's pretty much all it is.

57:09.640 --> 57:17.640
The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish.

57:17.640 --> 57:18.640
It's really important.

57:18.640 --> 57:20.640
It's called the nn dot linear function.

57:20.640 --> 57:25.640
So it is a pretty much a function of the nn dot module.

57:25.640 --> 57:35.640
And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters.

57:35.640 --> 57:41.640
So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false.

57:41.640 --> 57:50.640
But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better.

57:50.640 --> 57:59.640
And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output.

57:59.640 --> 58:06.640
So pretty much anything with nn dot linear is going to be very important and it's going to be learnable.

58:06.640 --> 58:08.640
So we can see over here.

58:08.640 --> 58:12.640
This is the tors.nn little site here on the docs.

58:12.640 --> 58:19.640
So we have containers, a bunch of different layers like activations, layers, pretty much just layers.

58:19.640 --> 58:20.640
That's all it is.

58:20.640 --> 58:23.640
And so these are these are important.

58:23.640 --> 58:26.640
We're going to, we're basically going to learn from these.

58:26.640 --> 58:32.640
And you're going to see why we're going to use something called keys and values, keys, values and queers later on.

58:32.640 --> 58:33.640
You'll see why those are important.

58:33.640 --> 58:38.640
But if that doesn't make sense yet, help me, let me illustrate value for you right now.

58:38.640 --> 58:40.640
So I drew this out here.

58:40.640 --> 58:48.640
So if we look back at our examples, we have a, we make, we initialize a term.

58:48.640 --> 58:51.640
We make, we initialize a tensor.

58:51.640 --> 58:53.640
It's 10, 10 and 10.

58:53.640 --> 58:56.640
What we're going to do is we're going to do a linear transformation.

58:56.640 --> 58:58.640
This linear stands for linear transformation.

58:58.640 --> 59:04.640
So pretty much we're just going to apply a weight and a bias through each of these layers here.

59:04.640 --> 59:09.640
So we have an input and we have an output x is our input, y is our output.

59:09.640 --> 59:13.640
And this is of size three and this is of size three.

59:13.640 --> 59:16.640
So pretty much we just need to make sure that these are lining up.

59:16.640 --> 59:26.640
And for more context, the nn.sequential is sort of built off nn.linear.

59:26.640 --> 59:32.640
So if we go ahead and search that up right now, this will make sense in a second here.

59:32.640 --> 59:37.640
This is also some good prerequisite knowledge in general for machine learning.

59:37.640 --> 59:45.640
So let's see nn.sequential doesn't show it here, but pretty much.

59:45.640 --> 59:53.640
If you have, let's say, two, you have two input neurons and maybe you have one output neuron.

59:53.640 --> 59:55.640
Okay, you have a bunch of hidden layers in between here.

59:55.640 --> 01:00:02.640
Let's say we have one, two, three, four, and then one, two, three.

01:00:02.640 --> 01:00:09.640
So pretty much you need to make sure that the inputs aligns with this hidden layer.

01:00:09.640 --> 01:00:12.640
This hidden layer aligns with this one and this one aligns with this one.

01:00:12.640 --> 01:00:17.640
So you're going to have a transformation of two to four.

01:00:17.640 --> 01:00:25.640
So two, four, and then this one's going to be four to three, four to three,

01:00:25.640 --> 01:00:27.640
and then you're going to have a final one.

01:00:27.640 --> 01:00:31.640
This is two to four right here, four to three here, and then this final one.

01:00:31.640 --> 01:00:33.640
It's going to be three to one.

01:00:33.640 --> 01:00:37.640
So you pretty much just need to make sure that these are lining up.

01:00:37.640 --> 01:00:43.640
So we can see that we have two, four, and then this four is carried on from this output here.

01:00:43.640 --> 01:00:47.640
And pretty much this will just make sure that our shapes are consistent.

01:00:47.640 --> 01:00:52.640
And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work.

01:00:52.640 --> 01:00:54.640
So we need to make sure that our shapes are consistent.

01:00:54.640 --> 01:00:59.640
If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets,

01:00:59.640 --> 01:01:04.640
but if you're really interested, you could use chatGPT, of course.

01:01:04.640 --> 01:01:09.640
And that's a really good learning resource, chatGPT, going on to get up discussions, maybe,

01:01:09.640 --> 01:01:12.640
or just looking at documentation.

01:01:12.640 --> 01:01:19.640
And if you're not good at reading documentation, then you could take maybe some little keywords from here,

01:01:19.640 --> 01:01:22.640
like a sequential container.

01:01:22.640 --> 01:01:24.640
Well, what is a sequential container?

01:01:24.640 --> 01:01:29.640
You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation

01:01:29.640 --> 01:01:31.640
and figure things out step by step.

01:01:31.640 --> 01:01:38.640
It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on.

01:01:38.640 --> 01:01:40.640
You don't need to memorize them.

01:01:40.640 --> 01:01:44.640
But while you're working with them, it's important to understand what they're really doing behind the scenes,

01:01:44.640 --> 01:01:50.640
especially if you want to make an efficient and popular working neural net.

01:01:50.640 --> 01:01:53.640
So that's that.

01:01:53.640 --> 01:02:00.640
And pretty much what's going to happen here with these linear layers is we're just going to simply transform

01:02:00.640 --> 01:02:03.640
from one to the other input to output, no hidden layers.

01:02:03.640 --> 01:02:06.640
And we're just going to be able to learn best parameters for doing that.

01:02:06.640 --> 01:02:10.640
You're going to see why that's useful later.

01:02:10.640 --> 01:02:12.640
Now we have the softmax function.

01:02:12.640 --> 01:02:14.640
So that sounds scary.

01:02:14.640 --> 01:02:18.640
And the softmax function isn't actually what it sounds like at all.

01:02:18.640 --> 01:02:20.640
Let me illustrate that for you right now.

01:02:20.640 --> 01:02:25.640
So let's go ahead and change the color here.

01:02:25.640 --> 01:02:35.640
So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc.

01:02:35.640 --> 01:02:37.640
Right, floating points, whatever.

01:02:37.640 --> 01:02:48.640
So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these.

01:02:48.640 --> 01:02:53.640
And we're going to divide them by the sum of all of these exponentiated.

01:02:53.640 --> 01:02:57.640
So pretty much what's going to happen, let's say we exponentiate one.

01:02:57.640 --> 01:03:05.640
So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp.

01:03:05.640 --> 01:03:08.640
And I think I talked about this up here.

01:03:08.640 --> 01:03:15.640
This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating.

01:03:15.640 --> 01:03:23.640
So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71.

01:03:23.640 --> 01:03:35.640
And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two.

01:03:35.640 --> 01:03:37.640
Okay, so we're going to get 7.34.

01:03:38.640 --> 01:03:40.640
I'm going to get 7.34.

01:03:40.640 --> 01:03:42.640
Gorg my writing, it's terrible.

01:03:42.640 --> 01:03:47.640
2.71 to 3 cubed.

01:03:47.640 --> 01:03:48.640
So 19.9.

01:03:51.640 --> 01:03:55.640
So pretty much what's going to happen is we can rearrange this in a new array.

01:03:55.640 --> 01:04:00.640
7.34 and 19.9.

01:04:00.640 --> 01:04:06.640
So if we add all these up together, we add all these up together, we're going to get 2.71 plus this.

01:04:06.640 --> 01:04:08.640
Let's do this math real quick.

01:04:08.640 --> 01:04:12.640
I'm just going to walk you through this to help you understand what the softmax function is doing.

01:04:12.640 --> 01:04:20.640
7.34 plus 19.9.

01:04:20.640 --> 01:04:23.640
That's going to give us a total of 29.95.

01:04:23.640 --> 01:04:24.640
Great.

01:04:24.640 --> 01:04:29.640
29.95.

01:04:29.640 --> 01:04:36.640
So all we do is we just divide each of these elements by the total.

01:04:36.640 --> 01:04:40.640
So 2.71 divided by this is going to give us maybe x.

01:04:40.640 --> 01:04:44.640
And we do 7.34 divided by this is going to give us y.

01:04:44.640 --> 01:04:49.640
And then we have 19.9 divided by this is going to give us z.

01:04:49.640 --> 01:04:54.640
So pretty much you're going to exponentiate all of these.

01:04:54.640 --> 01:04:57.640
You're going to add them together to create a total.

01:04:57.640 --> 01:05:02.640
And then you're going to divide each of those exponentiate elements by the exponentiated total.

01:05:02.640 --> 01:05:09.640
So after that, this x right here is just, we're just going to wrap these again.

01:05:09.640 --> 01:05:17.640
And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z.

01:05:17.640 --> 01:05:19.640
That's all it's doing.

01:05:19.640 --> 01:05:22.640
And yeah, it's not really crazy.

01:05:22.640 --> 01:05:25.640
There's a weird formula for it.

01:05:25.640 --> 01:05:30.640
Softmax, softmax function.

01:05:30.640 --> 01:05:35.640
So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here

01:05:35.640 --> 01:05:39.640
and a lot of math that's above the high school level.

01:05:39.640 --> 01:05:43.640
But yeah, like this formula here, I believe this is what it is.

01:05:43.640 --> 01:05:46.640
Or standard unit, softmax function, there you go.

01:05:46.640 --> 01:05:48.640
So pretty much this is what it does.

01:05:48.640 --> 01:05:51.640
And there's your easy explanation of what it does.

01:05:51.640 --> 01:05:56.640
So you're going to see why this is useful later, but it's just important to know what's going on

01:05:56.640 --> 01:06:02.640
so that you won't lag behind later in the course when this background knowledge becomes important.

01:06:02.640 --> 01:06:08.640
So if we go over a little example of that, of the softmax function in code, it looks like this right here.

01:06:08.640 --> 01:06:13.640
So we import torsha and n dot functional as f, f short for functional.

01:06:13.640 --> 01:06:17.640
And we pretty much just do f dot softmax and then plug in a tensor.

01:06:17.640 --> 01:06:23.640
And what we want the dimension to be the output dimension.

01:06:23.640 --> 01:06:29.640
So if we plug this into here and we print it out, we go and print it out.

01:06:29.640 --> 01:06:35.640
It's going to take a second.

01:06:35.640 --> 01:06:39.640
Torch is not defined. So let's run this from the top here.

01:06:39.640 --> 01:06:42.640
Boom.

01:06:42.640 --> 01:06:44.640
And let's try that again. Boom. There we go.

01:06:44.640 --> 01:06:48.640
So if you took all those values, let's actually do this again from scratch.

01:06:48.640 --> 01:06:56.640
So we do 2.71, 2.71 divided by 29.95.

01:06:56.640 --> 01:07:02.640
We get 0.09, 0.09. Good.

01:07:02.640 --> 01:07:12.640
And then if we do 7.34 divided by 29.95, we get 0.245.

01:07:12.640 --> 01:07:16.640
So 0.245. Well, it's kind of close.

01:07:16.640 --> 01:07:22.640
Really close actually. And then 66.52. So if we go, what was that last one there?

01:07:22.640 --> 01:07:30.640
19.9. So we do 19.9 divided by 29.95.

01:07:30.640 --> 01:07:35.640
66.4. So 66.5. It's pretty close.

01:07:35.640 --> 01:07:41.640
Again, we're rounding, so it's not perfectly accurate.

01:07:41.640 --> 01:07:47.640
As you can see, they're very close and for only having two decimal places, we did pretty good.

01:07:47.640 --> 01:07:52.640
So that's just sort of illustrating what the softmax function does and what it looks like in code.

01:07:52.640 --> 01:08:01.640
We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line.

01:08:01.640 --> 01:08:04.640
It's just like that.

01:08:04.640 --> 01:08:07.640
So now we're going to go over embeddings.

01:08:07.640 --> 01:08:10.640
And I'm not actually, I don't have any code for this yet.

01:08:10.640 --> 01:08:16.640
We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills

01:08:16.640 --> 01:08:23.640
and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning.

01:08:23.640 --> 01:08:33.640
So if we pop in a chat GPT here, we say, what is an end dot embedding?

01:08:33.640 --> 01:08:45.640
And then dots. Let me type in a non-bedding class in the PyTorch library.

01:08:45.640 --> 01:08:51.640
Okay, actual language processing max maps each discrete input to a dense vector representation.

01:08:51.640 --> 01:08:54.640
Okay, how does this work? Let's see.

01:08:54.640 --> 01:08:58.640
So we have some vocab. So that's probably our vocabulary size.

01:08:58.640 --> 01:09:05.640
I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set.

01:09:05.640 --> 01:09:11.640
That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter.

01:09:11.640 --> 01:09:16.640
So let's see. This doesn't quite make sense to me yet.

01:09:16.640 --> 01:09:19.640
So maybe I want to learn what does this actually look like?

01:09:19.640 --> 01:09:33.640
Can you explain this to a, maybe an eighth grader and provide a visualization?

01:09:33.640 --> 01:09:37.640
Certainly. Okay.

01:09:37.640 --> 01:09:42.640
Little secret codes that represent the meaning of the words. Okay, that helps.

01:09:42.640 --> 01:09:47.640
So if we have cat, okay, so cat, cat's a word.

01:09:47.640 --> 01:09:53.640
So maybe we want to know what it would look like on a character level.

01:09:53.640 --> 01:10:03.640
What about on a character level instead of word level?

01:10:03.640 --> 01:10:05.640
So it's probably going to look very similar.

01:10:05.640 --> 01:10:10.640
We have this little vector here storing some information about whatever this is.

01:10:10.640 --> 01:10:18.640
So a, it means this here. Okay, so as your point to, and this is really useful.

01:10:18.640 --> 01:10:21.640
So we've pretty much just learned what embedding vectors does.

01:10:21.640 --> 01:10:30.640
And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character.

01:10:30.640 --> 01:10:34.640
And we don't even know what each of these elements mean.

01:10:34.640 --> 01:10:35.640
We don't know what they mean.

01:10:35.640 --> 01:10:41.640
This could be maybe positivity or should be the start of a word or it could be any piece of information,

01:10:41.640 --> 01:10:44.640
maybe something we can't even comprehend yet.

01:10:44.640 --> 01:10:55.640
But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before,

01:10:55.640 --> 01:11:02.640
nn.embedding right here is a part of the nn.module.

01:11:02.640 --> 01:11:05.640
So these are learnable parameters, which is great.

01:11:05.640 --> 01:11:08.640
So it's actually going to learn the importance of each letter,

01:11:08.640 --> 01:11:11.640
and it's going to be able to produce some amazing results.

01:11:11.640 --> 01:11:21.640
So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter.

01:11:21.640 --> 01:11:25.640
In our case, it's character level, not subword, not word, it's character level.

01:11:25.640 --> 01:11:28.640
So it's going to represent some meaning about those.

01:11:28.640 --> 01:11:30.640
So that's what embedding vectors are.

01:11:30.640 --> 01:11:32.640
Let's go figure out how they work in code.

01:11:32.640 --> 01:11:38.640
We have this little character level embedding vector and it contains a list.

01:11:38.640 --> 01:11:45.640
There's five elements in here, one, two, three, four, five, and it's by the vocab size.

01:11:45.640 --> 01:11:51.640
So we have all of our vocabulary by the length of each embedding vector.

01:11:51.640 --> 01:11:56.640
So this actually makes sense because our vocab size by the embedding dimension,

01:11:56.640 --> 01:12:02.640
which is how much information is actually being stored in each of these characters.

01:12:02.640 --> 01:12:04.640
So this now is very easy to understand.

01:12:04.640 --> 01:12:09.640
I'm just going to copy this code from here and I'm going to paste it down here.

01:12:09.640 --> 01:12:16.640
And let's just get rid of the torch because we already initialized that above.

01:12:17.640 --> 01:12:22.640
So if we just run this, actually, let's turn that down to maybe a thousand characters.

01:12:22.640 --> 01:12:24.640
Let's try that out.

01:12:25.640 --> 01:12:27.640
And it's not defined.

01:12:27.640 --> 01:12:29.640
We did not initialize it.

01:12:40.640 --> 01:12:43.640
So let's go back down here and look at that.

01:12:43.640 --> 01:12:50.640
So this dot shape is going to essentially show the shape of it this much by this much.

01:12:50.640 --> 01:12:52.640
So it's four by a hundred.

01:12:52.640 --> 01:12:59.640
And yeah, so we can we can work with these and we can store stuff about characters in them.

01:12:59.640 --> 01:13:05.640
And you're going to see this in the next lecture, how we actually use embedding vectors.

01:13:05.640 --> 01:13:09.640
So no need to worry if a lot of this doesn't make sense yet.

01:13:09.640 --> 01:13:10.640
That's fine.

01:13:10.640 --> 01:13:13.640
You're going to learn a little bit more about how we use these over the course.

01:13:13.640 --> 01:13:17.640
You're going to get more confident with using them even in your own projects.

01:13:17.640 --> 01:13:19.640
So don't don't stress about it too much right now.

01:13:19.640 --> 01:13:23.640
Embeddings are pretty tricky at first to learn.

01:13:23.640 --> 01:13:25.640
So don't worry about that too much.

01:13:25.640 --> 01:13:37.640
But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks.

01:13:37.640 --> 01:13:49.640
So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say.

01:13:49.640 --> 01:13:53.640
But that's the architecture of this multilayer perceptron.

01:13:53.640 --> 01:13:58.640
But pretty much what's happening is we have a little input here and we have a white matrix.

01:13:58.640 --> 01:14:01.640
So white matrix is looks like this.

01:14:01.640 --> 01:14:11.640
It's like this and we have some, we have some values in between X1, Y1 and maybe Z1.

01:14:11.640 --> 01:14:17.640
So a bunch of weights and maybe biases to that we add to it.

01:14:17.640 --> 01:14:23.640
So the tricky part is how do we actually multiply our input by this white matrix?

01:14:23.640 --> 01:14:25.640
We're just doing one matrix times another.

01:14:25.640 --> 01:14:27.640
Well, that's called matrix multiplication.

01:14:27.640 --> 01:14:30.640
And I'm going to show you how to do that right now.

01:14:30.640 --> 01:14:34.640
So first off, we have to learn something called dot products.

01:14:34.640 --> 01:14:40.640
So dot products are actually pretty easy and you might have actually done them before.

01:14:40.640 --> 01:14:46.640
So let's say we go ahead and take, we go ahead and take this right here we go.

01:14:47.640 --> 01:14:49.640
One, two, three.

01:14:49.640 --> 01:14:51.640
That's going to be what A is.

01:14:51.640 --> 01:14:55.640
And then we have four, five, six.

01:14:55.640 --> 01:15:04.640
So if we want to find the dot product between these two, all we have to do is simply take the index of both of these,

01:15:04.640 --> 01:15:08.640
the first ones and the second ones and third ones, multiply them together and then add.

01:15:08.640 --> 01:15:20.640
So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five,

01:15:20.640 --> 01:15:24.640
and then add it to three times six.

01:15:26.640 --> 01:15:32.640
So one times four is four, two times five is ten, three times six is eighteen.

01:15:32.640 --> 01:15:36.640
So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two.

01:15:36.640 --> 01:15:42.640
So the dot product of this is going to be thirty-two.

01:15:42.640 --> 01:15:45.640
And that's pretty much how simple dot products are.

01:15:45.640 --> 01:15:54.640
It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up.

01:15:54.640 --> 01:15:55.640
That's a dot product.

01:15:55.640 --> 01:15:59.640
So we actually need dot products for matrix multiplication.

01:15:59.640 --> 01:16:01.640
So let's go ahead and jump into that right now.

01:16:01.640 --> 01:16:06.640
So I'm just going to create two matrices that are going to be pretty easy to work with.

01:16:06.640 --> 01:16:12.640
So let's say we have A and have one matrix over here.

01:16:12.640 --> 01:16:20.640
It's going to be one, two, three, four, five and six.

01:16:20.640 --> 01:16:26.640
This is going to be equal to A and then B is going to be another matrix.

01:16:26.640 --> 01:16:35.640
So we're going to have seven, eight, nine, ten, eleven, twelve.

01:16:35.640 --> 01:16:37.640
Ignore my terrible writing.

01:16:37.640 --> 01:16:41.640
Pretty much what we do is to multiply these together.

01:16:41.640 --> 01:16:45.640
First we need to make sure that they can multiply together.

01:16:45.640 --> 01:16:49.640
So we need to take a look at the amount of rows and columns at this half.

01:16:49.640 --> 01:16:52.640
So this one right here is three rows, one, two, three.

01:16:52.640 --> 01:16:54.640
Three rows and two columns.

01:16:54.640 --> 01:16:57.640
So this is going to be a three by two matrix.

01:16:57.640 --> 01:17:01.640
And this one has two rows and three columns.

01:17:01.640 --> 01:17:04.640
So it's a two by three matrix.

01:17:04.640 --> 01:17:11.640
So all we have to make sure that if we're multiplying A dot product with B,

01:17:11.640 --> 01:17:15.640
and this is the PyTorch syntax for multiplying matrices,

01:17:15.640 --> 01:17:21.640
if we're multiplying A by B, then we have to make sure the following is true.

01:17:21.640 --> 01:17:29.640
So if we use three by two and then dot product with two times three,

01:17:29.640 --> 01:17:34.640
we have to make sure that these two inner values are the same.

01:17:34.640 --> 01:17:37.640
So two is equal to two, so we cross these out,

01:17:37.640 --> 01:17:40.640
and then the ones that we have left over are three by three.

01:17:40.640 --> 01:17:44.640
So the resulting matrix would be A three by three.

01:17:44.640 --> 01:17:52.640
Or if you had like a three by four times A five by five by one,

01:17:52.640 --> 01:17:55.640
that doesn't work because these values aren't the same.

01:17:55.640 --> 01:17:58.640
So these two matrices couldn't multiply.

01:17:58.640 --> 01:18:02.640
And sometimes you actually have to flip these to make them work.

01:18:02.640 --> 01:18:07.640
So maybe we change this value here to A three.

01:18:07.640 --> 01:18:09.640
We change this value to a three.

01:18:09.640 --> 01:18:12.640
In this order, they do not multiply.

01:18:12.640 --> 01:18:25.640
But if we switch them around, we have a three by five with A five by three,

01:18:25.640 --> 01:18:29.640
sorry, five by three with A three by four.

01:18:29.640 --> 01:18:31.640
So these two numbers are the same.

01:18:31.640 --> 01:18:32.640
That works.

01:18:32.640 --> 01:18:34.640
The resulting matrix is a five by four.

01:18:34.640 --> 01:18:38.640
So that's how you make sure that two matrices are compatible.

01:18:38.640 --> 01:18:41.640
So now to actually multiply these together,

01:18:41.640 --> 01:18:43.640
what we're going to do, I'm going to make a new line here.

01:18:43.640 --> 01:18:47.640
So we're going to rewrite these.

01:18:47.640 --> 01:18:49.640
Now we don't have to rewrite them.

01:18:49.640 --> 01:18:51.640
Let's just cross that out here.

01:18:51.640 --> 01:19:01.640
So pretty much what we have to do is we have to take these two and dot product with these two.

01:19:01.640 --> 01:19:08.640
And then once we're done that, we do the same with these and these, these and these.

01:19:08.640 --> 01:19:14.640
So we start with the first, the first row in the A matrix.

01:19:14.640 --> 01:19:18.640
And we iterate through all of the columns in the B matrix.

01:19:18.640 --> 01:19:24.640
And then after we're done that, we just go to the next row in the A matrix and then et cetera, right?

01:19:24.640 --> 01:19:26.640
So let's go ahead and do this right now.

01:19:26.640 --> 01:19:30.640
That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here.

01:19:30.640 --> 01:19:40.640
So we have our one times, our one times seven plus two times 10.

01:19:40.640 --> 01:19:47.640
So one times seven plus two times 10.

01:19:47.640 --> 01:19:50.640
And this is equal to 27.

01:19:50.640 --> 01:19:56.640
So that's the first dot product of one and two and seven and 10.

01:19:56.640 --> 01:20:01.640
So what this is actually going to look like in our new matrix, I'm going to write this out here.

01:20:01.640 --> 01:20:04.640
So this is our new matrix here.

01:20:04.640 --> 01:20:08.640
This 27 is going to go right here.

01:20:08.640 --> 01:20:10.640
Let's continue.

01:20:10.640 --> 01:20:16.640
So next up, we're going to do one and two and then eight and 11.

01:20:16.640 --> 01:20:32.640
So we're going to go one, one times eight plus two, or sorry, two and 11.

01:20:32.640 --> 01:20:35.640
So one times eight is eight and then two times 11 is 22.

01:20:35.640 --> 01:20:41.640
So our result here is 30 and 30 is just going to go right here.

01:20:41.640 --> 01:20:45.640
So 27, 30, and you can see how this is going to work, right?

01:20:45.640 --> 01:20:53.640
So in our first row of A, we're going to get the first row of this resulting matrix.

01:20:53.640 --> 01:20:58.640
So let's go ahead and do the rest here.

01:20:58.640 --> 01:21:03.640
So we have one and two and then nine and 12.

01:21:03.640 --> 01:21:10.640
One times nine, two times 12.

01:21:10.640 --> 01:21:13.640
One times nine is nine, two times 12 is 24.

01:21:13.640 --> 01:21:17.640
So if we do, that's like 33, I believe.

01:21:17.640 --> 01:21:22.640
So 33 and we can go ahead and write that here.

01:21:22.640 --> 01:21:25.640
So now let's move on to the next.

01:21:25.640 --> 01:21:34.640
We have three and four, three, three and four dot product with seven and 10.

01:21:34.640 --> 01:21:39.640
So three will multiply seven.

01:21:39.640 --> 01:21:48.640
And then we're going to go ahead and add that to four times 10.

01:21:48.640 --> 01:21:54.640
Three times seven, three times seven is 21, and then four times 10 is 40.

01:21:54.640 --> 01:21:56.640
So we're going to get 47.

01:21:56.640 --> 01:22:02.640
So I'll put there so we can go in and write 47 right there.

01:22:02.640 --> 01:22:09.640
And our next one is going to be three and four dot product with eight and 11.

01:22:09.640 --> 01:22:20.640
So eight plus four times 11.

01:22:20.640 --> 01:22:21.640
Perfect.

01:22:21.640 --> 01:22:27.640
So we get three times eight is 24 and then plus 44.

01:22:27.640 --> 01:22:32.640
So 24 plus 44, that's 68.

01:22:32.640 --> 01:22:39.640
So we get 68 and we can go in and write that here.

01:22:39.640 --> 01:22:53.640
So next up, we have three and four and nine and 12.

01:22:53.640 --> 01:22:56.640
So three times nine is 27.

01:22:56.640 --> 01:22:57.640
And then four times 12.

01:22:57.640 --> 01:22:59.640
So let's just, let's just do that.

01:22:59.640 --> 01:23:01.640
I'm not doing that in my head.

01:23:01.640 --> 01:23:03.640
27 plus was four times 12.

01:23:03.640 --> 01:23:04.640
So that's 48.

01:23:04.640 --> 01:23:10.640
27 plus 48 gives us 75.

01:23:10.640 --> 01:23:14.640
Let's go ahead and write our 75 here.

01:23:14.640 --> 01:23:18.640
Then we can go ahead and slide down to this row since we're done, since we're done that.

01:23:18.640 --> 01:23:36.640
And then we go five, five and six dot product was seven and 10.

01:23:36.640 --> 01:23:42.640
So our result from this five times seven is 35 and then six times 10 is 60.

01:23:42.640 --> 01:23:44.640
So we're going to get 95.

01:23:44.640 --> 01:23:49.640
We can go in and write our 95 here.

01:23:49.640 --> 01:24:06.640
And then five and six dot product with eight and 11.

01:24:06.640 --> 01:24:10.640
So five times eight is 40 and then six times 11 is 66.

01:24:10.640 --> 01:24:19.640
So we get 104.

01:24:19.640 --> 01:24:35.640
And then the last one, so five and six dot product with nine and 12.

01:24:35.640 --> 01:24:38.640
So five, five times nine is 45.

01:24:38.640 --> 01:24:44.640
And then six times 12 is what six times 12, 72, I think.

01:24:44.640 --> 01:24:47.640
So six times 12, 72.

01:24:47.640 --> 01:24:48.640
Yeah.

01:24:48.640 --> 01:24:56.640
So 45 plus 72, 117.

01:24:56.640 --> 01:25:04.640
And that is how you do a three by two matrix and a two by three matrix multiplying them together.

01:25:04.640 --> 01:25:12.640
So the result would be C equals that.

01:25:12.640 --> 01:25:21.640
So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far.

01:25:21.640 --> 01:25:30.640
So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU.

01:25:30.640 --> 01:25:36.640
So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training.

01:25:36.640 --> 01:25:41.640
But pretty much that's how you do dot products and matrix multiplication.

01:25:41.640 --> 01:25:44.640
So I actually realized I messed up a little bit on the math there.

01:25:44.640 --> 01:25:48.640
So this 104, that's actually 106.

01:25:48.640 --> 01:25:52.640
So I messed up there if you caught that.

01:25:52.640 --> 01:25:53.640
Good job.

01:25:53.640 --> 01:25:58.640
But pretty much this is what this looks like in three lines of code.

01:25:58.640 --> 01:26:05.640
So all of this up here that we just covered all of this is in three lines.

01:26:05.640 --> 01:26:09.640
So we initialize an A tensor and a B tensor.

01:26:09.640 --> 01:26:11.640
Each one of these is a row.

01:26:11.640 --> 01:26:16.640
Each one of these is a row and it'll pretty much multiply these together.

01:26:16.640 --> 01:26:23.640
So this at symbol, this is a shorthand how you multiply two matrices in pytorch together.

01:26:23.640 --> 01:26:31.640
Another way to do this is to use the torch dot matrix multiply function or math mall for short.

01:26:31.640 --> 01:26:34.640
And then you can do A and B.

01:26:34.640 --> 01:26:38.640
So these will print literally the same thing.

01:26:38.640 --> 01:26:39.640
Look at that.

01:26:39.640 --> 01:26:43.640
So I'm not too sure on the differences between them.

01:26:43.640 --> 01:26:47.640
I use A at B for short.

01:26:47.640 --> 01:26:56.640
But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that.

01:26:56.640 --> 01:27:06.640
But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks.

01:27:06.640 --> 01:27:09.640
So there's our network here if I go up.

01:27:09.640 --> 01:27:19.640
Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number.

01:27:19.640 --> 01:27:25.640
So if it's like a one, it would be like one dot zero or something or just like a one dot.

01:27:25.640 --> 01:27:27.640
That's what it would look like as a floating point number.

01:27:27.640 --> 01:27:32.640
But if it were an integer, say B is full of ones with integers, it would just be a one.

01:27:32.640 --> 01:27:35.640
There wouldn't be any decimal zero zero center, right?

01:27:35.640 --> 01:27:37.640
It would just be one.

01:27:37.640 --> 01:27:45.640
So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type.

01:27:45.640 --> 01:27:48.640
So I showcase this right here.

01:27:48.640 --> 01:27:50.640
We have an int 64.

01:27:50.640 --> 01:27:56.640
So type of it is an integer and a float 32, 64 and 32 don't mean anything.

01:27:56.640 --> 01:27:59.640
All we have to know is an integer and floating point number.

01:27:59.640 --> 01:28:12.640
So I've initialized a torch.randint, I covered above and set above here.

01:28:12.640 --> 01:28:19.640
And maybe not.

01:28:19.640 --> 01:28:27.640
Anyways, this pretty much does torch.randint is going the first parameter here is anything.

01:28:27.640 --> 01:28:29.640
It's pretty much your range.

01:28:29.640 --> 01:28:34.640
So I could do like zero to five, or I could just do like one.

01:28:34.640 --> 01:28:40.640
So it'll do zero up to one, and then your shape of the matrix that it generates.

01:28:40.640 --> 01:28:43.640
So I said it's a random int.

01:28:43.640 --> 01:28:49.640
So that means it's going to generate a tensor with the data type integer 64.

01:28:49.640 --> 01:28:58.640
So we have a three by two, and then I initialize another random key detail here.

01:28:58.640 --> 01:29:01.640
We don't have the int suffix.

01:29:01.640 --> 01:29:05.640
So this just generates floating point numbers.

01:29:05.640 --> 01:29:08.640
And if we actually return the types of each of these.

01:29:08.640 --> 01:29:20.640
So five print int 64 dot d type, and then float 32 dot d type.

01:29:20.640 --> 01:29:21.640
Save that.

01:29:21.640 --> 01:29:25.640
I'm going to comment this out for now.

01:29:25.640 --> 01:29:28.640
We get a in 64 and float 32.

01:29:28.640 --> 01:29:37.640
So if we just try to multiply these together, try to multiply these together.

01:29:37.640 --> 01:29:40.640
Expected scalar type long above found float.

01:29:40.640 --> 01:29:43.640
So long is pretty much when you have a sequence of integers.

01:29:43.640 --> 01:29:47.640
And float is, of course, you have the decimal place.

01:29:47.640 --> 01:29:49.640
So you can actually multiply this together.

01:29:49.640 --> 01:29:57.640
So pretty much what you can do is cast the float method on this.

01:29:57.640 --> 01:30:04.640
If you just do dot float, and then parentheses, and then run this, it'll actually work.

01:30:04.640 --> 01:30:07.640
So you can cast integers to floats.

01:30:07.640 --> 01:30:11.640
And then I think there's a way you can cast floats to integers, but it has some rounding in there.

01:30:11.640 --> 01:30:17.640
So probably not the best for input and weights, matrix multiplication.

01:30:17.640 --> 01:30:26.640
But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise.

01:30:26.640 --> 01:30:30.640
So you want to make sure that they have sort of room to float around.

01:30:30.640 --> 01:30:33.640
So that's pretty much how you avoid that error.

01:30:33.640 --> 01:30:34.640
Let's move on.

01:30:34.640 --> 01:30:35.640
So congratulations.

01:30:35.640 --> 01:30:38.640
You probably made it further than quite a few people already.

01:30:38.640 --> 01:30:40.640
So congratulations on that.

01:30:40.640 --> 01:30:44.640
That was one of the most comprehensive parts of this entire course.

01:30:44.640 --> 01:30:48.640
Understanding the math is going on behind the scenes.

01:30:48.640 --> 01:30:52.640
For some people, it's very hard to grasp if you're not very fluent with math.

01:30:52.640 --> 01:30:58.640
But yeah, let's continue the biogram language model and let's pump out some code here.

01:30:58.640 --> 01:31:03.640
So to recap, we're using CUDA to accelerate the training process.

01:31:03.640 --> 01:31:10.640
We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel.

01:31:10.640 --> 01:31:12.640
Two hyperparameters.

01:31:12.640 --> 01:31:14.640
We open our text.

01:31:14.640 --> 01:31:16.640
We make a vocabulary out of it.

01:31:16.640 --> 01:31:19.640
We initialize our encoder and decoder.

01:31:19.640 --> 01:31:26.640
We get our data encoding all this text, and then we get our train and bow splits.

01:31:26.640 --> 01:31:28.640
And then this next function here, get batch.

01:31:28.640 --> 01:31:34.640
So before I jump into this, go ahead and run this here.

01:31:34.640 --> 01:31:42.640
So this is pretty much just taking the first little, I don't know, we have eight characters.

01:31:42.640 --> 01:31:49.640
So it's taking the first eight characters and then index one all the way to index nine.

01:31:50.640 --> 01:31:58.640
And we can pretty much use this to show what the current input is and then what the target would be.

01:31:58.640 --> 01:32:06.640
So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right?

01:32:06.640 --> 01:32:09.640
So this is the premise of the biogram language model.

01:32:09.640 --> 01:32:11.640
Given this character, we're going to predict the next.

01:32:11.640 --> 01:32:14.640
It doesn't know anything else in the entire history.

01:32:14.640 --> 01:32:18.640
It just knows what's before it or just knows what the current character is.

01:32:18.640 --> 01:32:22.640
And based on that, we're going to predict the next one.

01:32:22.640 --> 01:32:28.640
So we have this get batch function here, and this part right here is the most important piece of code.

01:32:28.640 --> 01:32:35.640
This is going to work a little bit more later with our train and bow splits, making sure that, you know,

01:32:35.640 --> 01:32:38.640
I'll try to explain this in a different way with our training bow splits.

01:32:38.640 --> 01:32:42.640
So imagine you take a course, as you take a math course, okay?

01:32:42.640 --> 01:32:49.640
And 90% of all your work is done just learning how the course works, learning all about the math.

01:32:49.640 --> 01:32:52.640
So that's like 90% of data you get from it.

01:32:52.640 --> 01:32:54.640
And then maybe another 10%.

01:32:54.640 --> 01:33:00.640
Another 10% at the end is that final exam, which might have some questions you've never seen before.

01:33:00.640 --> 01:33:05.640
So the point is in that first 90%, you're tested on based on what you know.

01:33:05.640 --> 01:33:09.640
And then this other 10% is what you don't know.

01:33:09.640 --> 01:33:15.640
And this pretty much means you can't memorize everything and then just start generating based on your memory.

01:33:15.640 --> 01:33:21.640
You generate something that's alike or something that's close based on what you already know and the patterns you captured

01:33:21.640 --> 01:33:24.640
in that 90% of the course.

01:33:24.640 --> 01:33:26.640
So you can write your final exam successfully.

01:33:26.640 --> 01:33:28.640
So that's pretty much what's going on here.

01:33:28.640 --> 01:33:36.640
The training is the course, learning everything about it and then validation is validating the final exam.

01:33:36.640 --> 01:33:49.640
So pretty much what we're doing here is initialize IX and that'll take a random manager between zero

01:33:49.640 --> 01:33:54.640
and then length of the length of the entire text minus block size.

01:33:54.640 --> 01:34:04.640
So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data.

01:34:04.640 --> 01:34:06.640
So that's kind of how that works.

01:34:06.640 --> 01:34:10.640
And if we print this out here, it'll just give us this right here.

01:34:10.640 --> 01:34:11.640
So we get some random integers.

01:34:11.640 --> 01:34:20.640
These are some random indices in the entire text that we can start generating from.

01:34:20.640 --> 01:34:24.640
So print this out and then torch.stack.

01:34:24.640 --> 01:34:25.640
We covered this before.

01:34:25.640 --> 01:34:28.640
Pretty much what this does, it's going to stack them in batches.

01:34:28.640 --> 01:34:30.640
This is the entire point of batches.

01:34:30.640 --> 01:34:33.640
So that's what we do there.

01:34:33.640 --> 01:34:39.640
We get X and then Y is just the same thing, but offset by one like this.

01:34:39.640 --> 01:34:42.640
So that's what happens there.

01:34:42.640 --> 01:34:47.640
And let's get into actually, I'm going to add something here.

01:34:47.640 --> 01:34:48.640
This is going to be very important.

01:34:48.640 --> 01:34:56.640
We're going to go X and Y is equal to model dot.

01:34:56.640 --> 01:35:02.640
We're going to go X dot to device.

01:35:02.640 --> 01:35:06.640
So notice how, no, we didn't do it up here.

01:35:06.640 --> 01:35:15.640
Okay, we'll cover this later, but pretty much you're going to see what this does in a second here.

01:35:16.640 --> 01:35:23.640
We return these and you can see that the device changed.

01:35:23.640 --> 01:35:24.640
So now we're actually on CUDA.

01:35:24.640 --> 01:35:33.640
And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU.

01:35:33.640 --> 01:35:39.640
They're no longer going to be processed sequentially, but rather in our batches in parallel.

01:35:39.640 --> 01:35:48.640
So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here.

01:35:48.640 --> 01:35:52.640
So now we can go ahead and actually initialize our neural net.

01:35:52.640 --> 01:35:59.640
So what I'm going to do is I'm going to go back up here and we're going to import some more stuff.

01:35:59.640 --> 01:36:07.640
So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second.

01:36:07.640 --> 01:36:09.640
I'm going to explain this here.

01:36:09.640 --> 01:36:20.640
I just want to get some code out first.

01:36:20.640 --> 01:36:22.640
And down here we can initialize this.

01:36:22.640 --> 01:36:23.640
So it's a class.

01:36:23.640 --> 01:36:33.640
We're going to make it a by-gram language model subclass of nn.module.

01:36:33.640 --> 01:36:42.640
And the reason why we do nn.module here is because it's going to take an nn.module.

01:36:42.640 --> 01:36:57.640
I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters.

01:36:57.640 --> 01:37:02.640
So I'm going to go ahead and look at the documentation here so you can sort of understand this better.

01:37:03.640 --> 01:37:05.640
We go to nn.

01:37:05.640 --> 01:37:16.640
So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before.

01:37:16.640 --> 01:37:18.640
So we have nn.linear.

01:37:18.640 --> 01:37:26.640
So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable.

01:37:26.640 --> 01:37:31.640
So that weight matrix will be changed through gradient descent.

01:37:31.640 --> 01:37:34.640
And actually, I think I should probably cover gradient descent right now.

01:37:34.640 --> 01:37:42.640
So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better.

01:37:42.640 --> 01:37:46.640
So I'm going to go ahead and set up a little graph for that right now.

01:37:46.640 --> 01:37:49.640
So I'm going to be using a little tool called Desmos.

01:37:49.640 --> 01:37:51.640
Desmos is actually great.

01:37:51.640 --> 01:37:53.640
It acts as a graphing calculator.

01:37:53.640 --> 01:37:56.640
So you can plug in formulas and move things around.

01:37:56.640 --> 01:37:59.640
You sort of visualize how math functions work.

01:37:59.640 --> 01:38:06.640
So I've written some functions out here that will basically calculate the derivative of a sine wave.

01:38:06.640 --> 01:38:10.640
So if I move A around, you'll see that changes.

01:38:10.640 --> 01:38:17.640
So before I get into what's really going on here, I need to first tell you what the loss actually is.

01:38:17.640 --> 01:38:23.640
If you're not familiar with the loss, let's say we have 80 characters in our vocabulary.

01:38:23.640 --> 01:38:28.640
And we have just started our model, no training at all, completely random weights.

01:38:28.640 --> 01:38:34.640
And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully.

01:38:34.640 --> 01:38:42.640
So how we can measure the loss of this is by taking the negative log likelihood.

01:38:42.640 --> 01:38:44.640
So the likelihood is one out of 80.

01:38:44.640 --> 01:38:47.640
We take the log of that and then negative.

01:38:47.640 --> 01:38:51.640
So if we plug this in here, we'll get 4.38.

01:38:51.640 --> 01:38:53.640
So that's a terrible loss.

01:38:53.640 --> 01:38:55.640
Obviously, that's one out of 80.

01:38:55.640 --> 01:38:59.640
So it's like, you know, not even 2% chance.

01:38:59.640 --> 01:39:01.640
So that's not great.

01:39:01.640 --> 01:39:08.640
So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss.

01:39:08.640 --> 01:39:10.640
And that's how we train our network.

01:39:10.640 --> 01:39:11.640
So how does this actually work?

01:39:11.640 --> 01:39:13.640
How does this actually work out in code, you ask?

01:39:13.640 --> 01:39:17.640
So pretty much, let's say we have a loss here, okay?

01:39:17.640 --> 01:39:20.640
Start off with a loss of 2, just arbitrary loss, whatever.

01:39:20.640 --> 01:39:24.640
And what we're trying to do is decrease it.

01:39:24.640 --> 01:39:29.640
So over time, it's going to become smaller and smaller if we move in this direction.

01:39:29.640 --> 01:39:32.640
So how do we know if we're moving in the right direction?

01:39:32.640 --> 01:39:37.640
Well, we take the derivative of what the current point is at right now,

01:39:37.640 --> 01:39:39.640
and then we try moving it in a different direction.

01:39:39.640 --> 01:39:42.640
So if we move it this way, sure, it'll go down.

01:39:42.640 --> 01:39:43.640
That's great.

01:39:43.640 --> 01:39:47.640
We can hit the local bottom over there, or we can move to this side.

01:39:47.640 --> 01:39:51.640
And then we can see that the slope is increasing in a negative direction.

01:39:51.640 --> 01:39:56.640
So we're going to keep adjusting the parameters in favor of this direction.

01:39:56.640 --> 01:39:59.640
So that's pretty much what gradient descent is.

01:39:59.640 --> 01:40:03.640
We're descending with the gradient.

01:40:03.640 --> 01:40:05.640
So pretty self-explanatory.

01:40:05.640 --> 01:40:07.640
That's what the loss function does.

01:40:07.640 --> 01:40:11.640
And gradient descent is an optimizer.

01:40:11.640 --> 01:40:13.640
So it's an optimizer for the network.

01:40:13.640 --> 01:40:17.640
Optimizes our parameters, our weight, matrices, etc.

01:40:17.640 --> 01:40:20.640
So these are some common optimizers that are used.

01:40:20.640 --> 01:40:25.640
And this is just by going to torch.optim, short for optimizer.

01:40:25.640 --> 01:40:29.640
And these are just a list of a bunch of optimizers that PyTorch provides.

01:40:29.640 --> 01:40:34.640
So what we're going to be using is something called AdamW.

01:40:34.640 --> 01:40:40.640
And what AdamW is, is it pretty much...

01:40:40.640 --> 01:40:42.640
I'm just going to read off my little script here,

01:40:42.640 --> 01:40:46.640
because I can't memorize every optimizer that exists.

01:40:46.640 --> 01:40:52.640
So Adam, without Adam, just Adam, not AdamW,

01:40:52.640 --> 01:40:57.640
Adam is a popular optimization algorithm that combines ideas of momentum.

01:40:57.640 --> 01:41:03.640
And it uses a moving average of both the gradient and its squared value

01:41:03.640 --> 01:41:06.640
to adapt the learning rate of each parameter.

01:41:06.640 --> 01:41:10.640
And the learning rate is something that we should also go over.

01:41:10.640 --> 01:41:15.640
So let's say I figure out I need to move in this direction.

01:41:15.640 --> 01:41:17.640
I move, I take a step like that.

01:41:17.640 --> 01:41:20.640
Okay, that's a very big step that I say,

01:41:20.640 --> 01:41:22.640
okay, we need to keep moving in that direction.

01:41:22.640 --> 01:41:26.640
So what happens is I go like this, and then I end up there.

01:41:26.640 --> 01:41:29.640
And it's like, whoa, we're going up now, what happened?

01:41:29.640 --> 01:41:31.640
So that's because you have a very high learning rate.

01:41:31.640 --> 01:41:35.640
If you have a lower learning rate, what will happen is you'll start here.

01:41:35.640 --> 01:41:38.640
It'll take little one-pixel steps or very, very small steps.

01:41:38.640 --> 01:41:41.640
Okay, that's good. That's better. It's even better.

01:41:41.640 --> 01:41:44.640
Keep going in this direction. This is great.

01:41:44.640 --> 01:41:47.640
And you keep going down. You're like, okay, this is good.

01:41:47.640 --> 01:41:50.640
We're descending. And it's starting to flatten out.

01:41:50.640 --> 01:41:53.640
So we know that we're hitting a local bottom here.

01:41:53.640 --> 01:41:56.640
And then we stop because it starts ascending again.

01:41:56.640 --> 01:42:03.640
So that means this is our best set of parameters because of what that loss is

01:42:03.640 --> 01:42:08.640
or what the derivative is of that particular point.

01:42:08.640 --> 01:42:12.640
So pretty much this is what the learning rate is.

01:42:12.640 --> 01:42:17.640
So you want to have a small learning rate so that you don't take too large steps

01:42:17.640 --> 01:42:21.640
so that the parameters don't change dramatically and end up messing you up.

01:42:21.640 --> 01:42:24.640
So you want to make them small enough so that you can still have efficient training.

01:42:24.640 --> 01:42:33.640
You don't want to be moving in a millionth of one or something.

01:42:33.640 --> 01:42:38.640
That would be ridiculous. You'd have to do so many iterations to even get this far.

01:42:38.640 --> 01:42:45.640
So maybe you'd make it decently high but not too high that it'll go like that, right?

01:42:45.640 --> 01:42:50.640
So that's what the learning rate is, just how fast it learns pretty much.

01:42:50.640 --> 01:42:59.640
And yeah, so AtomW is a modification of the Atom Optimizer.

01:42:59.640 --> 01:43:05.640
And it adds weight to K. So pretty much there's just some features that you add on to gradient descent

01:43:05.640 --> 01:43:09.640
and then AtomW is the same thing except that has weight to K.

01:43:09.640 --> 01:43:13.640
And what this pretty much means is it generalizes the parameters more.

01:43:13.640 --> 01:43:17.640
So instead of having very high level of performance or very low level,

01:43:17.640 --> 01:43:20.640
it takes a little generalized in between.

01:43:20.640 --> 01:43:27.640
So the weight significance will actually shrink as it flans out.

01:43:27.640 --> 01:43:32.640
So this will pretty much make sure that certain parameters in your network,

01:43:32.640 --> 01:43:39.640
certain parameters in your weight matrices aren't affecting the output of this model drastically.

01:43:39.640 --> 01:43:42.640
That could be in a positive or negative direction.

01:43:42.640 --> 01:43:47.640
You can have insanely high performance from some lucky parameters in your weight matrices.

01:43:47.640 --> 01:43:53.640
So pretty much the point is to minimize those, to decay those values.

01:43:53.640 --> 01:44:00.640
That's what weight to K is, to prevent it from having that insane or super low performance.

01:44:00.640 --> 01:44:02.640
That's what weight to K is.

01:44:02.640 --> 01:44:07.640
So that's a little background on gradient descent and optimizers.

01:44:07.640 --> 01:44:10.640
Let's go ahead and finish typing this out.

01:44:10.640 --> 01:44:17.640
So next up, we actually, we need to initialize some things.

01:44:17.640 --> 01:44:27.640
So we have our init self, of course, since it's a class, vocab size.

01:44:27.640 --> 01:44:32.640
I want to make sure that's correct, vocabulary size.

01:44:32.640 --> 01:44:47.640
I might actually shrink this just a vocab size because it sounds way easier to type out.

01:44:47.640 --> 01:44:49.640
And vocab size, good.

01:44:49.640 --> 01:44:53.640
So we're going to pump out some R code here.

01:44:53.640 --> 01:44:57.640
And this is just assuming that you have some sort of a background in Python.

01:44:57.640 --> 01:45:01.640
If not, it's all good.

01:45:01.640 --> 01:45:04.640
Just understanding the premise of what's going on here.

01:45:04.640 --> 01:45:09.640
So we're going to make something called an embedding table.

01:45:09.640 --> 01:45:19.640
And I'm going to explain this to you in a second here, why the embedding table is really important.

01:45:19.640 --> 01:45:23.640
Notice that we use the nn.

01:45:23.640 --> 01:45:26.640
We use the nn module in this.

01:45:26.640 --> 01:45:30.640
So that means this is going to be a learnable parameter, the init.embedding.

01:45:30.640 --> 01:45:36.640
So we're going to make this vocab size by vocab size.

01:45:36.640 --> 01:45:41.640
So let's say you have all eight characters here and you have all eight characters here.

01:45:41.640 --> 01:45:46.640
I'm going to actually show you what this looks like in a second here and why this is really important.

01:45:46.640 --> 01:45:51.640
So first off, we're going to finish typing out this background language model.

01:45:51.640 --> 01:45:55.640
So we're going to define our forward pass here.

01:45:55.640 --> 01:46:02.640
So the reason why we type this forward pass out, instead of just using what it offers by default,

01:46:02.640 --> 01:46:12.640
is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task.

01:46:12.640 --> 01:46:18.640
This is a really good practice because we want to actually know what's going on behind the scenes in our model.

01:46:18.640 --> 01:46:20.640
We want to know exactly what's going on.

01:46:20.640 --> 01:46:29.640
We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug.

01:46:29.640 --> 01:46:37.640
So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch?

01:46:37.640 --> 01:46:42.640
Well, like I said, understanding the process, what are all the transformations that are actually going on,

01:46:42.640 --> 01:46:50.640
all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output?

01:46:50.640 --> 01:47:03.640
Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices

01:47:03.640 --> 01:47:12.640
If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that.

01:47:12.640 --> 01:47:21.640
So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up.

01:47:21.640 --> 01:47:28.640
So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here.

01:47:28.640 --> 01:47:34.640
Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right?

01:47:34.640 --> 01:47:41.640
So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right?

01:47:41.640 --> 01:47:48.640
And optimization, of course. So that's pretty much why we write out the forward pass from scratch.

01:47:48.640 --> 01:47:55.640
It's also just best practice. So it's never really a good idea to not write this.

01:47:55.640 --> 01:48:03.640
But let's continue. So self, and it will do index and targets.

01:48:03.640 --> 01:48:12.640
So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here.

01:48:12.640 --> 01:48:17.640
Before we do logits, I'm going to explain to you this embedding table here.

01:48:18.640 --> 01:48:23.640
Paste that in.

01:48:29.640 --> 01:48:35.640
Return logits. You're going to see why we return logits in a second here.

01:48:35.640 --> 01:48:41.640
So this an end on embedding here is pretty much just a lookup table.

01:48:41.640 --> 01:48:45.640
So what we're going to have, I'm actually going to pull up my notebook here.

01:48:45.640 --> 01:48:53.640
So we have a giant sort of grid of what the predictions are going to look like.

01:48:53.640 --> 01:48:57.640
It's going to look, can I drag it in here? No.

01:48:57.640 --> 01:49:03.640
So go ahead and download this full screen. Boom.

01:49:03.640 --> 01:49:06.640
This is my notion here, but pretty much this is what it looks like.

01:49:06.640 --> 01:49:09.640
And I took this picture from Andrei Karpathy's lecture.

01:49:09.640 --> 01:49:14.640
But what this is, is it has start tokens and end tokens.

01:49:14.640 --> 01:49:18.640
So start is at the start of the block, and end tokens are at the end of the block.

01:49:18.640 --> 01:49:29.640
And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character.

01:49:29.640 --> 01:49:45.640
So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.

01:49:45.640 --> 01:49:51.640
So if we just add up all these, if we normalize them, and we get a little probability of this happening,

01:49:51.640 --> 01:49:55.640
I don't know, if we add up all these together, I don't know what that is.

01:49:55.640 --> 01:49:59.640
It's some crazy number, maybe 20,000 or something, something crazy.

01:49:59.640 --> 01:50:07.640
Pretty much that percentage is the percentage of the end token coming after the character A.

01:50:07.640 --> 01:50:15.640
And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind.

01:50:15.640 --> 01:50:24.640
That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that.

01:50:24.640 --> 01:50:30.640
To that entire row. So this one's pretty significant in proportion to the others.

01:50:30.640 --> 01:50:33.640
So this one's going to be a fairly high probability of coming next.

01:50:33.640 --> 01:50:37.640
A lot of the times you're going to have an I coming after an R.

01:50:37.640 --> 01:50:40.640
And that's pretty much what that is. That's the embedding table.

01:50:40.640 --> 01:50:43.640
So that's why we make it vocab size by vocab size.

01:50:43.640 --> 01:50:48.640
So that's a little background on what we're doing here.

01:50:48.640 --> 01:50:53.640
So let's continue with the term logits.

01:50:53.640 --> 01:50:57.640
So what exactly are the logits? You're probably asking that.

01:50:57.640 --> 01:51:04.640
So let's actually go back to a little notebook I had over here.

01:51:04.640 --> 01:51:10.640
So remember our softmax function, right? Our softmax right here.

01:51:10.640 --> 01:51:15.640
So we exponentiated each of these values and then we normalized them.

01:51:15.640 --> 01:51:21.640
Normalized. We took its contribution to the sum of everything. That's what normalizing is.

01:51:21.640 --> 01:51:28.640
So you can think of logits as just a bunch of floating point numbers that are normalized, right?

01:51:28.640 --> 01:51:33.640
So you have a total, I'll write this out.

01:51:33.640 --> 01:51:43.640
So let's say we have, that's a terrible line. Let's draw a new one.

01:51:44.640 --> 01:52:03.640
So let's say we have 2, 4, and 6. And we want to normalize these.

01:52:03.640 --> 01:52:09.640
So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12.

01:52:09.640 --> 01:52:15.640
So 2 divided by 12. We take the percentage of that.

01:52:15.640 --> 01:52:21.640
2 out of 12 is 0.16 something, okay?

01:52:21.640 --> 01:52:27.640
So 0.16, we'll just do 1.167.

01:52:27.640 --> 01:52:31.640
And then 4 out of 12 would be double that.

01:52:31.640 --> 01:52:38.640
So 4 out of 12 would be 33, 33%.

01:52:38.640 --> 01:52:42.640
And then 6 out of 12, that's 50. So 0.5.

01:52:42.640 --> 01:52:50.640
So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution.

01:52:50.640 --> 01:53:04.640
So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d.

01:53:04.640 --> 01:53:08.640
We know that from this distribution, a followed by d is most likely to come next.

01:53:08.640 --> 01:53:15.640
So this is what the logits are. They're pretty much a probability distribution of what we want to predict.

01:53:15.640 --> 01:53:21.640
So given that, let's hop back into here. We're going to mess around with these a little bit.

01:53:21.640 --> 01:53:27.640
So we have this embedding table, and I already showed you what that looked like.

01:53:27.640 --> 01:53:30.640
It looked like this right here. This is our embedding table.

01:53:30.640 --> 01:53:38.640
So let's use something called, we're going to use a function called dot view.

01:53:38.640 --> 01:53:43.640
So this is going to help us sort of reshape what our logits look like.

01:53:43.640 --> 01:53:46.640
And I'm going to go over an example of what this looks like in a second here.

01:53:46.640 --> 01:53:51.640
I'm just going to pump out some code. So we have our batch by our time.

01:53:51.640 --> 01:53:56.640
So the time is, you can think of time as that sequence of integers.

01:53:56.640 --> 01:53:59.640
That's the time dimension, right? You start from here.

01:53:59.640 --> 01:54:03.640
Maybe through the generating process, we don't know what's here next.

01:54:03.640 --> 01:54:06.640
We don't know what's on the, we don't know what the next token is.

01:54:06.640 --> 01:54:10.640
So that's why we say it's time because there's some we don't know yet and there's some that we already do know.

01:54:10.640 --> 01:54:12.640
That's what we call the time dimension.

01:54:12.640 --> 01:54:18.640
And then channels would just be, how many different channels are, what's the vocabulary size?

01:54:18.640 --> 01:54:20.640
Channels is the vocabulary size.

01:54:20.640 --> 01:54:24.640
So we can make this the logits dot shape.

01:54:24.640 --> 01:54:27.640
This is what logits going to return here is B by T by C.

01:54:27.640 --> 01:54:29.640
That's the shape of it.

01:54:29.640 --> 01:54:38.640
And then our targets do, actually, no, we won't do that yet.

01:54:38.640 --> 01:54:46.640
We'll do do logits equals logits dot view.

01:54:46.640 --> 01:54:52.640
And then we'll, this is very important, B by T.

01:54:52.640 --> 01:55:02.640
So because we're particularly paying attention to the channels, the vocabulary, the batch and time,

01:55:02.640 --> 01:55:05.640
they, I mean, they're not as important here.

01:55:05.640 --> 01:55:07.640
So we can sort of blend these together.

01:55:07.640 --> 01:55:15.640
And as long as the logits and the targets have the same batch and time, we should be all right.

01:55:15.640 --> 01:55:21.640
So we're going to do B, B times T by C.

01:55:21.640 --> 01:55:26.640
And then we can go to initialize our targets.

01:55:26.640 --> 01:55:33.640
It's going to be targets dot view.

01:55:33.640 --> 01:55:39.640
And it's going to be just a B by T.

01:55:39.640 --> 01:55:43.640
And then we can make our loss, remember the loss function, right?

01:55:43.640 --> 01:55:49.640
We do the functional of cross entropy, just a way of measuring the loss.

01:55:49.640 --> 01:55:52.640
And we basically take where there's two parameters here.

01:55:52.640 --> 01:55:58.640
So we have the logits and the targets.

01:55:58.640 --> 01:56:02.640
So I'm going to go over exactly what's going on here in a second.

01:56:02.640 --> 01:56:05.640
But first, you might be asking, what does this view mean?

01:56:05.640 --> 01:56:06.640
What exactly does this do?

01:56:06.640 --> 01:56:08.640
So I'm going to show you that right now.

01:56:08.640 --> 01:56:14.640
There's some code here that initializes a random tensor of shape 2 by 3 by 5.

01:56:14.640 --> 01:56:22.640
And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape.

01:56:22.640 --> 01:56:27.640
So shape takes the, you know, it takes the 2 by 3 by 5.

01:56:27.640 --> 01:56:31.640
We get x equals 2, y equals 3, and z equals 5.

01:56:31.640 --> 01:56:42.640
So then we can do dot view, and that'll pretty much make that tensor again with those dimensions.

01:56:42.640 --> 01:56:45.640
So then we can just print that out afterwards.

01:56:45.640 --> 01:56:53.640
We go, we could print out, I don't know, print x, y, z.

01:56:53.640 --> 01:56:56.640
We have 2, 3, 5.

01:56:56.640 --> 01:57:00.640
Print, print a dot shape.

01:57:00.640 --> 01:57:08.640
And actually, I'll print out a dot shape right here first so you can see that this actually does line up.

01:57:08.640 --> 01:57:11.640
A dot shape.

01:57:11.640 --> 01:57:13.640
And then down here as well.

01:57:13.640 --> 01:57:15.640
Same exact thing.

01:57:15.640 --> 01:57:20.640
This also view does, basically allows us to unpack with the dot shape,

01:57:20.640 --> 01:57:24.640
and then we can use view to put them back together into a tensor.

01:57:24.640 --> 01:57:30.640
So you might be asking, why in this notebook did we, did we have to reshape these?

01:57:30.640 --> 01:57:32.640
Why do we do that?

01:57:32.640 --> 01:57:38.640
Well, the answer sort of falls into what the shape needs to be here with cross entropy.

01:57:38.640 --> 01:57:40.640
What does it expect?

01:57:40.640 --> 01:57:43.640
What does PyTorch expect the actual shape to be?

01:57:43.640 --> 01:57:51.640
So I looked at the documentation here, and it pretty much says that we want either one dimension,

01:57:51.640 --> 01:57:58.640
which is channels, or 2, which is n, which I believe n is also the batch.

01:57:58.640 --> 01:58:02.640
So you have n, n different blocks or batches.

01:58:02.640 --> 01:58:06.640
And then you have some other dimensions here.

01:58:06.640 --> 01:58:15.640
So pretty much what it's expecting is a b by c by t instead of a b by t by c,

01:58:15.640 --> 01:58:19.640
which is precisely what we get out of here.

01:58:19.640 --> 01:58:22.640
It's the logits dot shape is b by t by c.

01:58:22.640 --> 01:58:25.640
We want it in a b by c by t.

01:58:25.640 --> 01:58:30.640
So pretty much what we're doing is we're just putting this into,

01:58:30.640 --> 01:58:33.640
we're just making this one parameter by multiplying those.

01:58:33.640 --> 01:58:34.640
That's what's going on here.

01:58:34.640 --> 01:58:37.640
And then that means the second one is going to be c.

01:58:37.640 --> 01:58:43.640
So you get like a b times t equals n, and then c, just the way that it expects it, right?

01:58:43.640 --> 01:58:44.640
Just like that.

01:58:44.640 --> 01:58:47.640
So that's pretty much what we're doing there.

01:58:47.640 --> 01:58:57.640
And a lot of the times you might get errors from passing it into a functional function in PyTorch.

01:58:57.640 --> 01:59:02.640
So it's important to pay attention to how PyTorch expects the shapes to be,

01:59:02.640 --> 01:59:04.640
because you're going to get errors from that.

01:59:04.640 --> 01:59:07.640
And I mean, it's not very hard to reshape them.

01:59:07.640 --> 01:59:12.640
You just use the dot view and dot shape and you unpack them, reshape them together.

01:59:12.640 --> 01:59:19.640
It's overall pretty simple for a beginner to intermediate level projects.

01:59:19.640 --> 01:59:22.640
So it shouldn't really be a trouble there, but just watch out for that,

01:59:22.640 --> 01:59:27.640
because it will come back and get you if you're not aware at some point.

01:59:27.640 --> 01:59:30.640
So I've added a new function here called generate,

01:59:30.640 --> 01:59:33.640
and this is pretty much going to generate tokens for us.

01:59:33.640 --> 01:59:39.640
So we pass an index, which is the current index or the context,

01:59:39.640 --> 01:59:43.640
and then we have max new tokens, and this is passed in through here.

01:59:43.640 --> 01:59:51.640
So we have our context, we make it a single zero, just the next line character.

01:59:51.640 --> 01:59:56.640
And then we generate based on that, and then our max new tokens, second parameter,

01:59:56.640 --> 01:59:59.640
we just make it 500 second parameter.

01:59:59.640 --> 02:00:00.640
So cool.

02:00:00.640 --> 02:00:02.640
What do we do inside of here?

02:00:02.640 --> 02:00:15.640
We have a little loop that pretty much it generates based on the range of the max new tokens.

02:00:15.640 --> 02:00:21.640
So we're going to generate max new tokens, tokens, if that makes sense.

02:00:21.640 --> 02:00:28.640
Pretty much what we do is we call forward pass based on the current state of the model parameters.

02:00:28.640 --> 02:00:33.640
And I want to be explicit here and say self dot forward, rather than just self index,

02:00:33.640 --> 02:00:35.640
it will call self dot forward when we do this.

02:00:35.640 --> 02:00:39.640
But let's just be explicit and say self dot forward here.

02:00:39.640 --> 02:00:42.640
So we get the logic and the loss from this.

02:00:42.640 --> 02:00:44.640
We focus on the last time step.

02:00:44.640 --> 02:00:47.640
That's the only one we care about diagram language model.

02:00:47.640 --> 02:00:52.640
We only care about the single previous character, only one doesn't have context before.

02:00:52.640 --> 02:00:57.640
And then we apply the softmax to get probability distribution.

02:00:57.640 --> 02:01:00.640
And we already went over the softmax function before.

02:01:00.640 --> 02:01:07.640
The reason why we use negative one here is because we're focusing on the last dimension.

02:01:07.640 --> 02:01:13.640
And in case you aren't familiar with negative indexing, which is what this is here and same with here,

02:01:13.640 --> 02:01:16.640
is imagine you have a little number line.

02:01:16.640 --> 02:01:21.640
It starts at index zero, one, two, three, four, five, et cetera.

02:01:21.640 --> 02:01:29.640
So if you go before zero, it's just going to loop to the very end of that array.

02:01:29.640 --> 02:01:34.640
So when we call negative one, it's going to do the last element, negative two,

02:01:34.640 --> 02:01:37.640
second last element, negative three, third last element, et cetera.

02:01:37.640 --> 02:01:39.640
So that's pretty much all this is here.

02:01:39.640 --> 02:01:41.640
And you can do this for anything in Python.

02:01:41.640 --> 02:01:44.640
Negative indexing is quite common.

02:01:44.640 --> 02:01:47.640
So that's what we do here.

02:01:47.640 --> 02:01:51.640
We apply softmax to the last dimension.

02:01:51.640 --> 02:01:54.640
And then we sample from the distribution.

02:01:54.640 --> 02:01:59.640
So we already went over torch dot monomial, we get one sample.

02:01:59.640 --> 02:02:09.640
And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate.

02:02:09.640 --> 02:02:17.640
It concatenates the previous context or the previous tokens with the newly generated one.

02:02:17.640 --> 02:02:19.640
And then we just combine them together.

02:02:19.640 --> 02:02:21.640
So they're one thing.

02:02:21.640 --> 02:02:26.640
And we do this on a B by T plus one.

02:02:26.640 --> 02:02:29.640
And if that doesn't make sense, let me help you out here.

02:02:29.640 --> 02:02:34.640
So we have this time dimension, let's say we have, you know, maybe just one element here.

02:02:34.640 --> 02:02:37.640
So we have something in the zero position.

02:02:37.640 --> 02:02:43.640
And then whenever we generate a token, we're going to take the information from the zero position.

02:02:43.640 --> 02:02:45.640
And then we're going to add one to it.

02:02:45.640 --> 02:02:47.640
So it becomes a B by T.

02:02:47.640 --> 02:02:51.640
Since there was only one element, the length of that was one, it is now two.

02:02:51.640 --> 02:02:54.640
Then we have this two, we make it three.

02:02:54.640 --> 02:02:57.640
And then we have this three, we make it four.

02:02:57.640 --> 02:03:04.640
So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it.

02:03:04.640 --> 02:03:08.640
And then we, you know, after this loop, we just return the index.

02:03:08.640 --> 02:03:12.640
So this is all the generated tokens for max new tokens.

02:03:12.640 --> 02:03:16.640
And that's pretty much what that does.

02:03:16.640 --> 02:03:24.640
Model up to device here, this is just going to push our parameters to the GPU for more efficient training.

02:03:24.640 --> 02:03:29.640
I'm not sure if this makes a huge difference right now because we're only doing background language modeling.

02:03:29.640 --> 02:03:33.640
But yeah, it's handy to have this here.

02:03:33.640 --> 02:03:37.640
And then, I mean, this is, this is pretty self explanatory here.

02:03:37.640 --> 02:03:40.640
We generate based on a context.

02:03:40.640 --> 02:03:44.640
This is the context, which is a single zero or a next line character.

02:03:44.640 --> 02:03:47.640
We pass in our max new tokens.

02:03:47.640 --> 02:03:49.640
And then we pretty much just decode this.

02:03:49.640 --> 02:03:52.640
So that's how that works.

02:03:52.640 --> 02:03:58.640
Let's move on to the optimizer and the training loop, the actual training process.

02:03:58.640 --> 02:04:02.640
So I actually skipped something and probably left you a little bit confused.

02:04:02.640 --> 02:04:12.640
But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here?

02:04:12.640 --> 02:04:18.640
Because the logits only returns two dimensions, right?

02:04:18.640 --> 02:04:23.640
You have a B by T, or you have a B times T by C.

02:04:23.640 --> 02:04:26.640
So how exactly does this work?

02:04:26.640 --> 02:04:31.640
Well, when we call this forward pass, all we're passing in is the index here.

02:04:31.640 --> 02:04:35.640
So that means targets defaults to none.

02:04:35.640 --> 02:04:41.640
So because targets is none, the loss is none, and this code does not execute.

02:04:41.640 --> 02:04:46.640
And it just uses this logits here, which is three dimensional.

02:04:46.640 --> 02:04:48.640
So that's how that works.

02:04:48.640 --> 02:05:00.640
And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there.

02:05:00.640 --> 02:05:04.640
And that will make sure targets is not none.

02:05:04.640 --> 02:05:11.640
So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits.

02:05:11.640 --> 02:05:16.640
So that's just a little clarification there, if that was confusing to anybody.

02:05:16.640 --> 02:05:24.640
Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type.

02:05:24.640 --> 02:05:35.640
So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes.

02:05:35.640 --> 02:05:47.640
So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference

02:05:47.640 --> 02:05:51.640
between float and int is float has decimals, it's a floating point number.

02:05:51.640 --> 02:05:56.640
And then integers just, just a single integer doesn't, it's not really anything more than that.

02:05:56.640 --> 02:06:01.640
It can just be bigger based on the amount of bits that occupies.

02:06:01.640 --> 02:06:05.640
So that's just a overview on tors dot long.

02:06:05.640 --> 02:06:08.640
It's the exact same thing as in 64.

02:06:08.640 --> 02:06:11.640
So that's that.

02:06:11.640 --> 02:06:14.640
Now we have this, we have this training loop here.

02:06:14.640 --> 02:06:18.640
So we define our optimizer.

02:06:18.640 --> 02:06:26.640
And I already meant over optimizers previously, Adam W, which is Adam weight decay.

02:06:26.640 --> 02:06:28.640
So we have weight decay in here.

02:06:28.640 --> 02:06:33.640
And then all of our model parameters, and then our learning rates.

02:06:33.640 --> 02:06:36.640
So I actually wrote to learning rate up here.

02:06:36.640 --> 02:06:41.640
So I would add this and then just rerun this part of the code here if you're typing along.

02:06:41.640 --> 02:06:48.640
So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop.

02:06:48.640 --> 02:06:55.640
And the learning rate is special, because sometimes you're learning what will be too high.

02:06:55.640 --> 02:06:58.640
And some said, sometimes it'll be too low.

02:06:58.640 --> 02:07:09.640
So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time.

02:07:09.640 --> 02:07:15.640
So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip.

02:07:15.640 --> 02:07:22.640
So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly.

02:07:22.640 --> 02:07:32.640
You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that.

02:07:33.640 --> 02:07:42.640
So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs.

02:07:42.640 --> 02:07:47.640
So now we have this training loop here, which is going to iterate over the max iterations.

02:07:47.640 --> 02:07:50.640
We just give each iteration the term iter.

02:07:50.640 --> 02:07:57.640
And I don't think we use this yet, but we will later for just reporting on the loss over time.

02:07:58.640 --> 02:08:06.640
But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training.

02:08:06.640 --> 02:08:08.640
This is the training loop, we don't care about validation.

02:08:08.640 --> 02:08:16.640
So we're going to call train on this, we're going to get some x inputs and some y targets.

02:08:16.640 --> 02:08:24.640
So we go in and do a model dot forward here, we got our logits and our loss.

02:08:24.640 --> 02:08:30.640
And then we're going to do our optimizer dot zero grad and I'll explain this in the second here.

02:08:30.640 --> 02:08:32.640
It's a little bit confusing.

02:08:32.640 --> 02:08:42.640
But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops.

02:08:42.640 --> 02:08:44.640
I know I can go by this a little bit quickly.

02:08:44.640 --> 02:08:50.640
But this is the standard training loop architecture for basic models.

02:08:50.640 --> 02:08:53.640
And this is what it'll usually look like.

02:08:53.640 --> 02:08:58.640
So you'll, you know, you'll get your data, get your inputs or outputs, whatever.

02:08:58.640 --> 02:09:00.640
You'll do a forward pass.

02:09:00.640 --> 02:09:03.640
You'll define some thing about the optimizer here.

02:09:03.640 --> 02:09:04.640
In our case, it's your grad.

02:09:04.640 --> 02:09:10.640
And then you'll have a loss dot backward, which is backward pass.

02:09:10.640 --> 02:09:15.640
And the optimizer dot step, which lets gradient descent work its magic.

02:09:15.640 --> 02:09:19.640
So back to optimizer dot zero grad.

02:09:19.640 --> 02:09:27.640
So by default, PyTorch will accumulate the gradients over time via adding them.

02:09:27.640 --> 02:09:34.640
And what we do by by putting a zero grad is we make sure that they do not add over time.

02:09:34.640 --> 02:09:38.640
So the previous gradients do not affect the current one.

02:09:38.640 --> 02:09:43.640
And the reason we don't want this is because previous gradients are from previous data.

02:09:43.640 --> 02:09:48.640
And the data is, you know, kind of weird sometimes, sometimes it's biased.

02:09:48.640 --> 02:09:55.640
And we don't want that determining, you know, how much like what our error is, right?

02:09:55.640 --> 02:10:02.640
So we only want to decide, we only want to optimize based on the current gradient of our current data.

02:10:02.640 --> 02:10:05.640
And this little parameter in here, we go set to none.

02:10:05.640 --> 02:10:11.640
This pretty much means we're going to set, we're going to set the gradients instead of zero,

02:10:11.640 --> 02:10:15.640
instead of zero gradient, we're going to set it to none.

02:10:15.640 --> 02:10:20.640
And the reason why we set it to none is because none occupies a lot less space.

02:10:20.640 --> 02:10:24.640
It just, yeah, just occupies a lot less space when you have a zero.

02:10:24.640 --> 02:10:28.640
That's, that's probably an int 64 or something that's going to take up space.

02:10:28.640 --> 02:10:34.640
And because, you know, we might have a lot of these accumulating that takes up space over time.

02:10:34.640 --> 02:10:41.640
So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to.

02:10:41.640 --> 02:10:46.640
And that's pretty much what that does.

02:10:46.640 --> 02:10:56.640
It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets,

02:10:56.640 --> 02:11:01.640
which need to understand previous context because they're recurrent.

02:11:01.640 --> 02:11:10.640
I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation.

02:11:10.640 --> 02:11:17.640
We'll simply take an average of all the accumulation steps and just averages the gradients together.

02:11:17.640 --> 02:11:21.640
So you get a more effective, maybe block size, right?

02:11:21.640 --> 02:11:23.640
You get more context that way.

02:11:23.640 --> 02:11:25.640
And you can have the same batch size.

02:11:25.640 --> 02:11:27.640
So just little neat tricks like that.

02:11:27.640 --> 02:11:34.640
We'll talk about gradient accumulation more later in the course, but pretty much what's going on here.

02:11:34.640 --> 02:11:37.640
We define an optimizer, Adam W.

02:11:37.640 --> 02:11:39.640
We iterate over max editors.

02:11:39.640 --> 02:11:41.640
We get a batch training split.

02:11:41.640 --> 02:11:47.640
We do a forward pass, zero grad, backward pass, and then we get a step in the right direction.

02:11:47.640 --> 02:11:49.640
So we're gradient descent works as magic.

02:11:49.640 --> 02:11:52.640
And at the end, we could just print out the loss here.

02:11:52.640 --> 02:11:54.640
So I've run this a few times.

02:11:54.640 --> 02:12:00.640
And over time, I've gotten the loss of 2.55, which is okay.

02:12:00.640 --> 02:12:10.640
And if we generate based on that loss, we get still pretty garbage tokens.

02:12:10.640 --> 02:12:14.640
But then again, this is a background language model.

02:12:14.640 --> 02:12:17.640
So actually, I might need to retrain this here.

02:12:17.640 --> 02:12:19.640
It's not trained yet.

02:12:19.640 --> 02:12:26.640
So what I'm actually going to do is run this, run this, run this, boom.

02:12:26.640 --> 02:12:30.640
And then what I'll do, oh, it looks like we're printing out a lot of stuff here.

02:12:30.640 --> 02:12:33.640
So that's coming from our get batch.

02:12:33.640 --> 02:12:35.640
So I'll just comment that.

02:12:35.640 --> 02:12:37.640
Or we can just delete it overall.

02:12:37.640 --> 02:12:39.640
Cool.

02:12:39.640 --> 02:12:48.640
And now if we run this again.

02:12:48.640 --> 02:12:53.640
Give it a second.

02:12:53.640 --> 02:12:56.640
Perfect.

02:12:56.640 --> 02:13:01.640
So I don't know why it's still doing that.

02:13:01.640 --> 02:13:06.640
If we run it again, let's see.

02:13:06.640 --> 02:13:15.640
Where are we printing stuff?

02:13:16.640 --> 02:13:18.640
No.

02:13:18.640 --> 02:13:20.640
Ah, yes.

02:13:20.640 --> 02:13:22.640
We have to run this again after changing it.

02:13:22.640 --> 02:13:26.640
Silly me.

02:13:26.640 --> 02:13:30.640
And of course, 10,000 steps is a lot.

02:13:30.640 --> 02:13:32.640
So it takes a little while.

02:13:32.640 --> 02:13:34.640
It takes a few seconds, which is actually quite quick.

02:13:34.640 --> 02:13:38.640
So after the first one, we get a loss of 3.15.

02:13:38.640 --> 02:13:40.640
We can generate from that.

02:13:40.640 --> 02:13:42.640
And we get something that is less garbage.

02:13:42.640 --> 02:13:44.640
You know, it has some next line characters.

02:13:44.640 --> 02:13:46.640
It understands a little bit more to, you know,

02:13:46.640 --> 02:13:48.640
space things out and whatnot.

02:13:48.640 --> 02:13:52.640
So that's like slightly less garbage than before.

02:13:52.640 --> 02:13:56.640
But yeah, this, this is pretty good.

02:13:56.640 --> 02:13:58.640
So I lied.

02:13:58.640 --> 02:14:02.640
There aren't actually any lectures previously where I talked about optimizers.

02:14:02.640 --> 02:14:05.640
So might as well talk about it now.

02:14:05.640 --> 02:14:07.640
So a bunch of common ones.

02:14:07.640 --> 02:14:12.640
And honestly, you don't really need to know anything more than the common ones

02:14:12.640 --> 02:14:16.640
because most of them are just built off of these.

02:14:16.640 --> 02:14:19.640
So you have your mean squared error,

02:14:19.640 --> 02:14:22.640
common loss function using regression, regression problems,

02:14:22.640 --> 02:14:25.640
where it's like, you know, you have a bunch of data points,

02:14:25.640 --> 02:14:27.640
find the best fit line, right?

02:14:27.640 --> 02:14:29.640
That's a common regression problem.

02:14:29.640 --> 02:14:31.640
Goals to predict a continuous output

02:14:31.640 --> 02:14:33.640
and measures the average squared difference

02:14:33.640 --> 02:14:36.640
between the predicted and actual values,

02:14:36.640 --> 02:14:39.640
often used to train neural networks for regression tasks.

02:14:39.640 --> 02:14:41.640
So cool.

02:14:41.640 --> 02:14:43.640
That's the most basic one.

02:14:43.640 --> 02:14:45.640
You can look into that more if you'd like,

02:14:45.640 --> 02:14:48.640
but that's our most basic optimizer.

02:14:48.640 --> 02:14:50.640
Gradient descent is a step up from that.

02:14:50.640 --> 02:14:53.640
It's used to minimize the loss function in a model,

02:14:53.640 --> 02:14:55.640
measures how well the model,

02:14:55.640 --> 02:14:59.640
the gradient measures how well the model is able to predict

02:14:59.640 --> 02:15:02.640
the target variable based on the input features.

02:15:02.640 --> 02:15:04.640
So we have some input X,

02:15:04.640 --> 02:15:08.640
we have some weights and biases maybe, WX plus B.

02:15:08.640 --> 02:15:14.640
And all we're trying to do is make sure that the inputs

02:15:14.640 --> 02:15:21.640
or make sure that we make the inputs become the desired outputs

02:15:21.640 --> 02:15:25.640
and based on how far it is away from the desired outputs,

02:15:25.640 --> 02:15:28.640
we can change the parameters of the model.

02:15:28.640 --> 02:15:32.640
So we went over gradient descent recently or previously,

02:15:32.640 --> 02:15:35.640
but that's pretty much what's going on here.

02:15:35.640 --> 02:15:40.640
And momentum is just a little extension of gradient descent

02:15:40.640 --> 02:15:43.640
that adds the momentum term.

02:15:43.640 --> 02:15:46.640
So it helps smooth out the training

02:15:46.640 --> 02:15:51.640
and allows it to continue moving in the right direction,

02:15:51.640 --> 02:15:54.640
even if the gradient changes direction or varies in magnitude.

02:15:54.640 --> 02:15:57.640
It's particularly useful for training deep neural nets.

02:15:57.640 --> 02:16:00.640
So momentum is when you have, you know,

02:16:00.640 --> 02:16:04.640
you consider some of the other gradients.

02:16:04.640 --> 02:16:07.640
So you have something that's like maybe passed on from here

02:16:07.640 --> 02:16:10.640
and then it might include a little bit of the current one.

02:16:10.640 --> 02:16:13.640
So like 90%, like a good momentum coefficient

02:16:13.640 --> 02:16:16.640
would be like 90% previous gradients

02:16:16.640 --> 02:16:18.640
and then 10% of the current one.

02:16:18.640 --> 02:16:23.640
So it kind of like lags behind and makes it converge sort of smoothly.

02:16:23.640 --> 02:16:25.640
That makes sense.

02:16:25.640 --> 02:16:27.640
Arm as prop, I've never used this,

02:16:27.640 --> 02:16:31.640
but it's an algorithm that use the moving average of the squared gradient

02:16:31.640 --> 02:16:33.640
to adapt learning rates of each parameter,

02:16:33.640 --> 02:16:36.640
helps to avoid oscillations in the parameter updates

02:16:36.640 --> 02:16:39.640
and can move and can improve convergence in some cases.

02:16:39.640 --> 02:16:42.640
So you can look more into that if you'd like.

02:16:42.640 --> 02:16:44.640
Adam, very popular,

02:16:44.640 --> 02:16:48.640
combines the ideas of momentum and arm as prop.

02:16:48.640 --> 02:16:50.640
He uses a moving average,

02:16:50.640 --> 02:16:52.640
both the gradient and its squared value

02:16:52.640 --> 02:16:54.640
to adapt learning rate of each parameter.

02:16:54.640 --> 02:16:58.640
So often uses the default optimizer for deep learning models.

02:16:58.640 --> 02:17:01.640
And in our case, when we continue to build this out,

02:17:01.640 --> 02:17:04.640
it's going to be quite a deep net.

02:17:04.640 --> 02:17:08.640
And Adam W is just a modification of the item optimizer

02:17:08.640 --> 02:17:10.640
that adds weight decay to the parameter updates.

02:17:10.640 --> 02:17:15.640
So helps to regularize and improve generalization performance.

02:17:15.640 --> 02:17:19.640
Using this optimizer as it best suits the properties of the model

02:17:19.640 --> 02:17:21.640
we'll train in this video.

02:17:21.640 --> 02:17:24.640
So, of course, I'm reading off the script here.

02:17:24.640 --> 02:17:28.640
There's no really other better way to say how these optimizers work.

02:17:28.640 --> 02:17:31.640
But, yeah, if you want to look more into, you know,

02:17:31.640 --> 02:17:34.640
concepts like momentum or weight decay

02:17:34.640 --> 02:17:39.640
or, you know, oscillations and just some statistic stuff, you can.

02:17:39.640 --> 02:17:43.640
But honestly, the only thing that really matters

02:17:43.640 --> 02:17:47.640
is just knowing which optimizers are used for certain things.

02:17:47.640 --> 02:17:50.640
So, like, what is the momentum used for?

02:17:50.640 --> 02:17:53.640
What is Adam W great for?

02:17:53.640 --> 02:17:56.640
What is MSC good for, right?

02:17:56.640 --> 02:18:00.640
Just knowing what the differences and similarities are,

02:18:00.640 --> 02:18:05.640
as well as when is the best case to use the optimizer.

02:18:05.640 --> 02:18:10.640
So, yeah, you can find more information about that at torch.optim.

02:18:10.640 --> 02:18:12.640
So when we develop language models,

02:18:12.640 --> 02:18:15.640
something really important in language modeling,

02:18:15.640 --> 02:18:18.640
data science, machine learning, at all,

02:18:18.640 --> 02:18:20.640
is just being able to report a loss

02:18:20.640 --> 02:18:23.640
or get an idea of how well our model is performing

02:18:23.640 --> 02:18:25.640
over, you know, the first 1,000 iterations

02:18:25.640 --> 02:18:27.640
and then the first 2,000 iterations

02:18:27.640 --> 02:18:29.640
and 4,000 iterations, right?

02:18:29.640 --> 02:18:31.640
So we want to get a general idea

02:18:31.640 --> 02:18:33.640
of how our model is converging over time.

02:18:33.640 --> 02:18:36.640
But we don't want to just print every single step of this.

02:18:36.640 --> 02:18:37.640
That wouldn't make sense.

02:18:37.640 --> 02:18:41.640
So what we actually could do is print every, you know,

02:18:41.640 --> 02:18:43.640
200 iterations, 500.

02:18:43.640 --> 02:18:45.640
We could print every 10,000 iterations

02:18:45.640 --> 02:18:48.640
if you're running a crazy big language model if you wanted to.

02:18:49.640 --> 02:18:51.640
And that's exactly what we're going to implement right here.

02:18:51.640 --> 02:18:57.640
So, actually, this doesn't require an insane amount of Python syntax.

02:18:57.640 --> 02:19:01.640
This is just, I'm actually just going to add it into our for loop here.

02:19:01.640 --> 02:19:05.640
And what this is going to do is it's going to do what I just said,

02:19:05.640 --> 02:19:09.640
is print every, you know, every certain number of iterations.

02:19:09.640 --> 02:19:16.640
So we can add a new hyper parameter up here called eval-itters.

02:19:16.640 --> 02:19:21.640
And I'm going to make this 250 just for,

02:19:21.640 --> 02:19:24.640
just to make things sort of easy here.

02:19:24.640 --> 02:19:28.640
And we're going to go ahead and add this in here.

02:19:28.640 --> 02:19:33.640
So I'm going to go if-iter.

02:19:33.640 --> 02:19:35.640
And we're going to do the module operator.

02:19:35.640 --> 02:19:38.640
You can look more into this if you want later.

02:19:38.640 --> 02:19:43.640
And we're going to do eval-itters equals equals zero.

02:19:43.640 --> 02:19:50.640
What this is going to do is it's going to check if the current iteration

02:19:50.640 --> 02:19:57.640
divided by, or sorry, if the remainder of the current iteration

02:19:57.640 --> 02:20:00.640
divided by our eval-itters parameter,

02:20:00.640 --> 02:20:05.640
if the remainder of that is zero, then we continue with it.

02:20:05.640 --> 02:20:07.640
So hopefully that made sense.

02:20:07.640 --> 02:20:12.640
If you want to, you could just ask GPT4

02:20:12.640 --> 02:20:16.640
or GPT3.5, whatever you have, just this module operator,

02:20:16.640 --> 02:20:19.640
and you should get a good general understanding of what it does.

02:20:19.640 --> 02:20:21.640
Cool.

02:20:21.640 --> 02:20:24.640
So all we can do now is we'll just say,

02:20:24.640 --> 02:20:26.640
we'll just have a filler statement here.

02:20:26.640 --> 02:20:31.640
We'll just do print, we've been f-string,

02:20:31.640 --> 02:20:40.640
and then we'll go losses, losses, maybe that.

02:20:40.640 --> 02:20:42.640
Or actually, I'm going to change this here.

02:20:42.640 --> 02:20:49.640
We can go step-iter.

02:20:49.640 --> 02:20:53.640
Add a little colon in there.

02:20:53.640 --> 02:21:01.640
And then I'll go split.

02:21:01.640 --> 02:21:07.640
Actually, I'll just go loss, and then losses like that.

02:21:07.640 --> 02:21:11.640
And then we'll have some sort of put in here.

02:21:11.640 --> 02:21:14.640
Something soon.

02:21:14.640 --> 02:21:16.640
I don't know.

02:21:16.640 --> 02:21:20.640
And all I've done is I've actually added a little function in here

02:21:20.640 --> 02:21:21.640
behind the scenes.

02:21:21.640 --> 02:21:23.640
You guys didn't see me do this yet.

02:21:23.640 --> 02:21:28.640
But pretty much, I'm not going to go through the actual function itself,

02:21:28.640 --> 02:21:32.640
but what is important is that you know this decorator right here.

02:21:32.640 --> 02:21:34.640
This probably isn't very common to you.

02:21:34.640 --> 02:21:36.640
This is torch.nograt.

02:21:36.640 --> 02:21:39.640
And what this is going to do is it's going to make sure that

02:21:39.640 --> 02:21:41.640
PyTorch doesn't use gradients at all in here.

02:21:41.640 --> 02:21:43.640
That'll reduce computation.

02:21:43.640 --> 02:21:45.640
It'll reduce memory usage.

02:21:45.640 --> 02:21:47.640
It's just overall better for performance.

02:21:47.640 --> 02:21:49.640
And because we're just reporting a loss,

02:21:49.640 --> 02:21:53.640
we don't really need to do any optimizing or gradient computation here.

02:21:53.640 --> 02:21:55.640
We're just getting losses.

02:21:55.640 --> 02:21:57.640
We're feeding some stuff into the model.

02:21:57.640 --> 02:22:01.640
We're getting a loss out of it, and we're going from there.

02:22:01.640 --> 02:22:06.640
So that's pretty much what's happening with this torch.nograt.

02:22:06.640 --> 02:22:11.640
And, you know, for things like, I don't know,

02:22:11.640 --> 02:22:14.640
if you have other classes or other outside functions,

02:22:14.640 --> 02:22:17.640
like, I mean, get batched by default isn't using this

02:22:17.640 --> 02:22:20.640
because it doesn't have the model thing passed into it.

02:22:20.640 --> 02:22:25.640
But estimate loss does have model pass into it right here.

02:22:25.640 --> 02:22:30.640
So we just kind of want to make sure that it's not using any gradients.

02:22:30.640 --> 02:22:32.640
We're going to reduce computation that way.

02:22:32.640 --> 02:22:34.640
So anyways, if you want,

02:22:34.640 --> 02:22:37.640
you can just take a quick readover of this,

02:22:37.640 --> 02:22:40.640
and it should overall make sense.

02:22:40.640 --> 02:22:44.640
Terms like .item.me are pretty common.

02:22:44.640 --> 02:22:47.640
A lot of the other things here, like model, X and Y,

02:22:47.640 --> 02:22:49.640
we get our logits and our loss.

02:22:49.640 --> 02:22:51.640
This stuff should make sense.

02:22:51.640 --> 02:22:53.640
It should be pretty straightforward.

02:22:53.640 --> 02:22:56.640
And only two other things I want to touch on

02:22:56.640 --> 02:22:58.640
is model.eval and model.train,

02:22:58.640 --> 02:23:01.640
because you probably have not seen these yet.

02:23:01.640 --> 02:23:08.640
So model.train essentially puts the model in the training mode.

02:23:08.640 --> 02:23:10.640
The model learns from the data,

02:23:10.640 --> 02:23:12.640
meaning the weights and biases,

02:23:12.640 --> 02:23:14.640
if we have, well, sometimes you only have weights,

02:23:14.640 --> 02:23:16.640
sometimes you, you know,

02:23:16.640 --> 02:23:19.640
sometimes you have weights and biases, whatever it is,

02:23:19.640 --> 02:23:21.640
those are updated during this phase.

02:23:21.640 --> 02:23:23.640
And then some layers of the model,

02:23:23.640 --> 02:23:25.640
like dropout and batch normalization,

02:23:25.640 --> 02:23:28.640
which you may not be familiar with yet,

02:23:28.640 --> 02:23:30.640
operate differently in training mode.

02:23:30.640 --> 02:23:32.640
For example, dropout is active,

02:23:32.640 --> 02:23:35.640
and what dropout does is this little hyperparameter

02:23:35.640 --> 02:23:37.640
that we add up here.

02:23:37.640 --> 02:23:39.640
It'll look like this.

02:23:39.640 --> 02:23:41.640
Dropout would be like 0.2.

02:23:41.640 --> 02:23:43.640
So pretty much what dropout does

02:23:43.640 --> 02:23:46.640
is it's going to drop out random neurons in the network

02:23:46.640 --> 02:23:48.640
so that we don't overfit.

02:23:48.640 --> 02:23:52.640
And this is actually disabled in validation mode,

02:23:52.640 --> 02:23:54.640
or eval mode.

02:23:54.640 --> 02:23:58.640
So this will just help our model sort of learn better

02:23:58.640 --> 02:24:00.640
when it has little, like, pieces of noise

02:24:00.640 --> 02:24:03.640
and when things aren't in quite the right place

02:24:03.640 --> 02:24:06.640
so that you don't have, you know, certain neurons in the network

02:24:06.640 --> 02:24:10.640
taking priority and just making a lot of the heavy decisions.

02:24:10.640 --> 02:24:11.640
We don't want that.

02:24:11.640 --> 02:24:14.640
So dropout will just sort of help our model train better

02:24:14.640 --> 02:24:18.640
by taking 20% of the neurons out, 0.2, at random.

02:24:18.640 --> 02:24:20.640
And that's all dropout does.

02:24:20.640 --> 02:24:23.640
So I'm just going to delete that for now.

02:24:24.640 --> 02:24:27.640
And then, yeah, model that train.

02:24:27.640 --> 02:24:30.640
Well, dropout is active during this phase,

02:24:30.640 --> 02:24:33.640
during training, randomly turning off,

02:24:33.640 --> 02:24:35.640
random neurons in the network.

02:24:35.640 --> 02:24:37.640
And this is to prevent overfitting.

02:24:37.640 --> 02:24:39.640
We went over overfitting earlier, I believe.

02:24:39.640 --> 02:24:42.640
And as for evaluation mode,

02:24:42.640 --> 02:24:46.640
evaluation mode is used when the model's being evaluated

02:24:46.640 --> 02:24:48.640
or tested just like it sounds.

02:24:48.640 --> 02:24:49.640
It's being trained.

02:24:49.640 --> 02:24:52.640
What the other mode is being validated or tested.

02:24:52.640 --> 02:24:56.640
And layers like dropout and batch normalization

02:24:56.640 --> 02:24:58.640
behave differently in this mode.

02:24:58.640 --> 02:25:00.640
Like dropout is turned off in the evaluation, right?

02:25:00.640 --> 02:25:02.640
Because what we're actually doing

02:25:02.640 --> 02:25:04.640
is we're using the entire network.

02:25:04.640 --> 02:25:07.640
We want everything to be working sort of together.

02:25:07.640 --> 02:25:10.640
And we want to actually see how well does it perform.

02:25:10.640 --> 02:25:12.640
Training mode is when we're just, you know,

02:25:12.640 --> 02:25:15.640
sampling, doing weird things to try to challenge the network

02:25:15.640 --> 02:25:16.640
as we're training it.

02:25:16.640 --> 02:25:19.640
And then evaluating or validation would be

02:25:19.640 --> 02:25:22.640
when we just get the network in its optimal form

02:25:22.640 --> 02:25:25.640
and we're trying to see how good of results it produces.

02:25:25.640 --> 02:25:27.640
So that's what a val is.

02:25:27.640 --> 02:25:29.640
And the reason we switched into a val here

02:25:29.640 --> 02:25:32.640
is just because, well, we are testing the model.

02:25:32.640 --> 02:25:34.640
We want to see, you know, how well it does

02:25:34.640 --> 02:25:38.640
with any given set of data from a get batch.

02:25:38.640 --> 02:25:40.640
And we don't actually need to train here.

02:25:40.640 --> 02:25:43.640
If there was no training, this would not be here

02:25:43.640 --> 02:25:46.640
because we would not be using any gradients.

02:25:47.640 --> 02:25:50.640
So we would be using gradients if training was on.

02:25:50.640 --> 02:25:53.640
Anyways, that's estimate loss for you.

02:25:53.640 --> 02:25:57.640
This function is, you know, just generally good

02:25:57.640 --> 02:25:59.640
to have a data science.

02:25:59.640 --> 02:26:02.640
Your train and validation splits, whatnot.

02:26:02.640 --> 02:26:05.640
And yeah, good for reporting.

02:26:05.640 --> 02:26:07.640
You know how it is.

02:26:07.640 --> 02:26:09.640
And we can go ahead and add this down here.

02:26:09.640 --> 02:26:11.640
So there's something soon.

02:26:11.640 --> 02:26:18.640
We'll go losses is equal to estimates loss.

02:26:18.640 --> 02:26:24.640
And then we can go ahead and put a...

02:26:24.640 --> 02:26:28.640
Yeah, we don't actually have to put anything in here.

02:26:28.640 --> 02:26:29.640
Cool.

02:26:29.640 --> 02:26:32.640
So now let's go ahead and run this.

02:26:32.640 --> 02:26:34.640
Let me run from the start here.

02:26:34.640 --> 02:26:38.640
Boom, boom, boom, boom, boom, boom.

02:26:42.640 --> 02:26:44.640
Perfect.

02:26:51.640 --> 02:26:53.640
I'm running for 10,000 iterations.

02:26:53.640 --> 02:26:55.640
That's interesting.

02:26:55.640 --> 02:26:57.640
Okay.

02:26:57.640 --> 02:27:01.640
So, yes.

02:27:01.640 --> 02:27:03.640
So what I'm going to do actually here

02:27:03.640 --> 02:27:05.640
is you can see this loss part is weird.

02:27:05.640 --> 02:27:08.640
So I'm actually going to change this up.

02:27:08.640 --> 02:27:12.640
And I'm just going to switch it to...

02:27:12.640 --> 02:27:15.640
We're going to go train loss.

02:27:15.640 --> 02:27:17.640
And we're going to go losses.

02:27:17.640 --> 02:27:20.640
And we're going to do the train split.

02:27:20.640 --> 02:27:23.640
And then we're going to go over here

02:27:23.640 --> 02:27:27.640
and just do the validation loss.

02:27:27.640 --> 02:27:30.640
We can do validation or just val for short.

02:27:30.640 --> 02:27:35.640
And I'm going to make it consistent here.

02:27:35.640 --> 02:27:38.640
So we have a colon there, a colon here.

02:27:38.640 --> 02:27:44.640
And then we just go losses and do val.

02:27:44.640 --> 02:27:45.640
Cool.

02:27:45.640 --> 02:27:50.640
So I'm going to reduce these max editors up here to only 1,000.

02:27:50.640 --> 02:27:52.640
Run that.

02:27:52.640 --> 02:27:54.640
Run this.

02:27:54.640 --> 02:27:57.640
Oh, somebody did a match.

02:28:06.640 --> 02:28:08.640
Okay.

02:28:08.640 --> 02:28:10.640
Okay.

02:28:10.640 --> 02:28:12.640
Okay.

02:28:34.640 --> 02:28:36.640
Yes.

02:28:36.640 --> 02:28:38.640
So what actually happened here was

02:28:38.640 --> 02:28:41.640
when we were doing these little ticks,

02:28:41.640 --> 02:28:44.640
what was happening is these were matching up with these.

02:28:44.640 --> 02:28:47.640
And it was telling us, oh, you can't do that.

02:28:47.640 --> 02:28:49.640
You can't start here and then end there

02:28:49.640 --> 02:28:51.640
and have all this weird stuff.

02:28:51.640 --> 02:28:53.640
Like, you can't do that.

02:28:53.640 --> 02:28:56.640
So pretty much we just need to make sure that these are different.

02:28:56.640 --> 02:28:58.640
So I'm going to do a double quote instead of single

02:28:58.640 --> 02:29:00.640
and then double quote to finish it off.

02:29:00.640 --> 02:29:03.640
And as you can see, this worked out here.

02:29:03.640 --> 02:29:05.640
So I'll just run that again

02:29:05.640 --> 02:29:07.640
so you guys can see what this looks like.

02:29:07.640 --> 02:29:08.640
Okay.

02:29:08.640 --> 02:29:11.640
Because we have, you know, a lot of decimal places.

02:29:11.640 --> 02:29:16.640
So what we can actually do here is we can add in a little format

02:29:16.640 --> 02:29:19.640
or a little decimal place reducer, if you call it,

02:29:19.640 --> 02:29:23.640
just for, you know, so you can read it.

02:29:23.640 --> 02:29:25.640
So it's not like some weird decimal number

02:29:25.640 --> 02:29:27.640
and you're like, oh, does this eight matter?

02:29:27.640 --> 02:29:28.640
Probably not.

02:29:28.640 --> 02:29:30.640
Just like the first three digits, maybe.

02:29:30.640 --> 02:29:33.640
So all we can do here is just add in,

02:29:33.640 --> 02:29:35.640
I believe this is how it goes.

02:29:37.640 --> 02:29:40.640
I don't think it's the other way.

02:29:40.640 --> 02:29:42.640
We'll find out.

02:29:42.640 --> 02:29:46.640
Some stuff in Python is extremely confusing to me.

02:29:46.640 --> 02:29:49.640
But there we go.

02:29:49.640 --> 02:29:50.640
So I got it right.

02:29:50.640 --> 02:29:52.640
Go on and then period.

02:29:52.640 --> 02:29:54.640
And as you can see, we have those digits reduced.

02:29:54.640 --> 02:29:57.640
So I can actually put this down to 3F.

02:30:03.640 --> 02:30:04.640
Wonderful.

02:30:04.640 --> 02:30:09.640
So we have our train loss and our validation loss.

02:30:09.640 --> 02:30:11.640
Great job you made it this far.

02:30:11.640 --> 02:30:13.640
This is absolutely amazing.

02:30:13.640 --> 02:30:14.640
This is insane.

02:30:14.640 --> 02:30:16.640
You've gotten this far in the video.

02:30:16.640 --> 02:30:19.640
We've covered all the basics, everything you need to know

02:30:19.640 --> 02:30:22.640
about background language models, optimizers,

02:30:22.640 --> 02:30:25.640
training loops, reporting losses.

02:30:25.640 --> 02:30:28.640
I can't even name everything we've done because it's so much.

02:30:28.640 --> 02:30:31.640
So congratulations that you made it this far.

02:30:31.640 --> 02:30:33.640
You should go take a quick break.

02:30:33.640 --> 02:30:37.640
Give yourself a pat on the back and get ready for the next part

02:30:37.640 --> 02:30:39.640
here because it's going to be absolutely insane.

02:30:39.640 --> 02:30:43.640
We're going to dig into literally state of the art language

02:30:43.640 --> 02:30:47.640
models and how we can build them from scratch,

02:30:47.640 --> 02:30:49.640
or at least how we can pre-train them.

02:30:49.640 --> 02:30:52.640
And some of these terms are going to seem a little bit out

02:30:52.640 --> 02:30:56.640
there, but I can ensure you by the end of this next section

02:30:56.640 --> 02:31:00.640
here, you're going to have a pretty good understanding

02:31:00.640 --> 02:31:03.640
about the state of language models right now.

02:31:03.640 --> 02:31:07.640
So go take a quick break and I'll see you back in a little bit.

02:31:07.640 --> 02:31:11.640
So there's something I'd like to clear up and actually sort of

02:31:11.640 --> 02:31:15.640
lied to you a little bit, a little while back in this course

02:31:15.640 --> 02:31:17.640
about what normalizing is.

02:31:17.640 --> 02:31:21.640
So I recall we were talking about the softmax function

02:31:21.640 --> 02:31:24.640
and normalizing vectors.

02:31:24.640 --> 02:31:29.640
So the softmax is definitely a form of normalization,

02:31:29.640 --> 02:31:31.640
but there are many forms.

02:31:31.640 --> 02:31:35.640
There are not just a few or like there's not just one or two normalizations.

02:31:35.640 --> 02:31:40.640
There are actually many of them and I have them on my second monitor here,

02:31:40.640 --> 02:31:44.640
but I don't want to just dump that library of information on your head

02:31:44.640 --> 02:31:46.640
because that's not how you learn.

02:31:46.640 --> 02:31:49.640
So what we're going to do is we're going to plug this into GPT-4.

02:31:50.640 --> 02:32:06.640
I'm going to say, can you list all the forms of normalizing in machine learning?

02:32:06.640 --> 02:32:16.640
And how are they different from one another?

02:32:16.640 --> 02:32:18.640
GPT-4 is a great tool.

02:32:18.640 --> 02:32:22.640
If you don't already use it, I highly suggest you use it,

02:32:22.640 --> 02:32:26.640
or even GPT-3.5, which is the free version.

02:32:26.640 --> 02:32:30.640
But yeah, it's a great tool for just quickly learning anything

02:32:30.640 --> 02:32:35.640
and then have it give you example practice questions with answers

02:32:35.640 --> 02:32:38.640
so you can learn topics in literally minutes

02:32:38.640 --> 02:32:43.640
that would take you several lectures to learn in a university course.

02:32:43.640 --> 02:32:47.640
But anyways, there's a few here.

02:32:47.640 --> 02:32:51.640
So min-max normalization, yep.

02:32:51.640 --> 02:32:54.640
z-score, decimal scaling, mean normalization,

02:32:54.640 --> 02:32:59.640
unit vector, or layer 2, robust scaling, power transformations.

02:32:59.640 --> 02:33:00.640
Okay.

02:33:00.640 --> 02:33:03.640
So yeah, and then softmax would be another one.

02:33:03.640 --> 02:33:09.640
What about softmax?

02:33:09.640 --> 02:33:11.640
It is in data type normalization,

02:33:11.640 --> 02:33:19.640
but it's not typically using from normalizing input data.

02:33:19.640 --> 02:33:21.640
It's commonly used in the output layer.

02:33:21.640 --> 02:33:24.640
So softmax is a type of normalization,

02:33:24.640 --> 02:33:29.640
but it's not used for normalizing input data.

02:33:29.640 --> 02:33:36.640
And honestly, we've proved that here by actually producing some probabilities.

02:33:36.640 --> 02:33:39.640
So this isn't something we used in our forward pass.

02:33:39.640 --> 02:33:41.640
This is something we used in our generate function

02:33:41.640 --> 02:33:44.640
to get a bunch of probabilities from our logits.

02:33:44.640 --> 02:33:47.640
So this is, yeah, interesting.

02:33:47.640 --> 02:33:50.640
It's good to just figure little things like these out for, you know,

02:33:50.640 --> 02:33:54.640
just to be, put you on the edge a little bit more

02:33:54.640 --> 02:33:57.640
for the future when it comes to engineering these kind of things.

02:33:57.640 --> 02:33:59.640
All right, great.

02:33:59.640 --> 02:34:02.640
So the next thing I want to touch on is activation functions.

02:34:02.640 --> 02:34:06.640
And activation functions are extremely important

02:34:06.640 --> 02:34:12.640
in offering new ways of changing our inputs that are not linear.

02:34:12.640 --> 02:34:16.640
So, for example, if we were to have a bunch of linear layers,

02:34:16.640 --> 02:34:19.640
a bunch of, let me erase this,

02:34:19.640 --> 02:34:23.640
if we were to have a bunch of, you know,

02:34:23.640 --> 02:34:26.640
nn.linears in a row,

02:34:26.640 --> 02:34:30.640
what would actually happen is they would all just, you know,

02:34:30.640 --> 02:34:32.640
they would all squeeze together

02:34:32.640 --> 02:34:37.640
and essentially apply one transformation that sums up all of them kind of.

02:34:37.640 --> 02:34:41.640
They all sort of multiply together and it gives us one transformation

02:34:41.640 --> 02:34:45.640
that is kind of just a waste of computation

02:34:45.640 --> 02:34:49.640
because let's say you have 100 of these nn.linear layers

02:34:49.640 --> 02:34:52.640
and nothing else.

02:34:52.640 --> 02:34:54.640
You're essentially going from inputs to outputs,

02:34:54.640 --> 02:34:59.640
but you're doing 100 times the computation for just one multiplication.

02:34:59.640 --> 02:35:01.640
That doesn't really make sense.

02:35:01.640 --> 02:35:06.640
So what can we do to actually make these deep neural networks important

02:35:06.640 --> 02:35:10.640
and what can we offer that's more than just linear transformations?

02:35:10.640 --> 02:35:13.640
Well, that's where activation functions come in

02:35:13.640 --> 02:35:16.640
and I'm going to go over these in a quick second here.

02:35:16.640 --> 02:35:19.640
So let's go navigate over to the PyTorch docs.

02:35:19.640 --> 02:35:23.640
So the three activation functions I'm going to cover

02:35:23.640 --> 02:35:27.640
in this little part of the video are the relu, the sigmoid,

02:35:27.640 --> 02:35:29.640
and the tanh activation functions.

02:35:29.640 --> 02:35:35.640
So let's start off with the relu or rectified linear unit.

02:35:35.640 --> 02:35:37.640
So we're going to use functional relu

02:35:37.640 --> 02:35:40.640
and the reason why we're not just going to use torch.n

02:35:40.640 --> 02:35:43.640
is because we're not doing any forward passes here.

02:35:43.640 --> 02:35:47.640
I'm just going to add these into our,

02:35:47.640 --> 02:35:52.640
I'm going to add these, let me clear this, clear this output.

02:35:52.640 --> 02:35:53.640
That's fine.

02:35:53.640 --> 02:35:56.640
I'm actually going to add these into here and there's no forward pass.

02:35:56.640 --> 02:35:58.640
We're just going to simply run them through a function

02:35:58.640 --> 02:36:01.640
and get an output just so we can see what it looks like.

02:36:01.640 --> 02:36:06.640
So I've actually added this up here from torch.n

02:36:06.640 --> 02:36:08.640
and import functional as capital F.

02:36:08.640 --> 02:36:12.640
It's just kind of a common PyTorch practice, capital F.

02:36:12.640 --> 02:36:17.640
And let's go ahead and start off with the relu here.

02:36:17.640 --> 02:36:25.640
So we can go, I don't know, x equals torch.tensor

02:36:25.640 --> 02:36:31.640
and then we'll make it a negative 0.05, for example.

02:36:31.640 --> 02:36:38.640
And then we'll go dtype equals torch.flurp32

02:36:38.640 --> 02:36:45.640
and we can go y equals f.relu of x.

02:36:45.640 --> 02:36:52.640
And then we'll go ahead and print y.

02:36:52.640 --> 02:36:54.640
It has no attribute relu.

02:36:54.640 --> 02:36:56.640
Okay, let's try nn then.

02:36:56.640 --> 02:37:01.640
Let's try nn and see if that works.

02:37:01.640 --> 02:37:04.640
Okay, well that didn't work and that's fine

02:37:04.640 --> 02:37:07.640
because we can simply take a look at this

02:37:07.640 --> 02:37:10.640
and it'll help us understand.

02:37:10.640 --> 02:37:12.640
We don't actually need to,

02:37:12.640 --> 02:37:13.640
we don't need to write this out in code

02:37:13.640 --> 02:37:15.640
as long as it sort of makes sense.

02:37:15.640 --> 02:37:17.640
We don't need to write this in the forward pass, really.

02:37:17.640 --> 02:37:19.640
You're not going to use it anywhere else.

02:37:19.640 --> 02:37:22.640
So yeah, I'm not going to be too discouraged

02:37:22.640 --> 02:37:26.640
that that does not work in the functional library.

02:37:26.640 --> 02:37:29.640
But yeah, so pretty much what this does

02:37:29.640 --> 02:37:31.640
is if a number is below,

02:37:31.640 --> 02:37:34.640
if a number is 0 or below 0,

02:37:34.640 --> 02:37:36.640
it will turn that number into 0.

02:37:36.640 --> 02:37:40.640
And then if it's above 0, it'll stay the same.

02:37:40.640 --> 02:37:43.640
So this graph sort of helps you visualize that.

02:37:43.640 --> 02:37:45.640
There's a little function here.

02:37:45.640 --> 02:37:47.640
That might make sense to some people.

02:37:47.640 --> 02:37:49.640
I don't really care about the functions too much

02:37:49.640 --> 02:37:52.640
as long as I can sort of visualize what the function means,

02:37:52.640 --> 02:37:55.640
what it does, what are some applications it can be used.

02:37:55.640 --> 02:37:59.640
That usually covers enough for like any function at all.

02:37:59.640 --> 02:38:02.640
So that's the Relu function.

02:38:02.640 --> 02:38:03.640
Pretty cool.

02:38:03.640 --> 02:38:07.640
It simply offers a non-linearity to our linear networks.

02:38:07.640 --> 02:38:09.640
So if you have 100 layers deep

02:38:09.640 --> 02:38:11.640
and every, I don't know,

02:38:11.640 --> 02:38:14.640
every second step you put a Relu,

02:38:14.640 --> 02:38:16.640
that network is going to learn a lot more things.

02:38:16.640 --> 02:38:19.640
It's going to learn a lot more linearity, non-linearity.

02:38:19.640 --> 02:38:22.640
Then if you were to just have 100 layers

02:38:22.640 --> 02:38:25.640
multiplying all into one transformation.

02:38:25.640 --> 02:38:27.640
So that's what that is.

02:38:27.640 --> 02:38:28.640
That's the Relu.

02:38:28.640 --> 02:38:30.640
Now let's go over to Sigmoid.

02:38:30.640 --> 02:38:34.640
So here we can actually use the functional library.

02:38:34.640 --> 02:38:38.640
And all Sigmoid does is we go 1 over 1 plus

02:38:38.640 --> 02:38:41.640
exponentiated of negative x.

02:38:41.640 --> 02:38:44.640
So I'm going to add that here.

02:38:44.640 --> 02:38:47.640
We could, yeah, why not do that?

02:38:47.640 --> 02:38:51.640
Negative 0.05 float 32.

02:38:51.640 --> 02:38:52.640
Sure.

02:38:52.640 --> 02:38:56.640
We'll go f dot Sigmoid.

02:38:56.640 --> 02:38:59.640
And then we'll just go x and then we'll print y.

02:38:59.640 --> 02:39:00.640
Cool.

02:39:00.640 --> 02:39:04.640
So we get a tensor 0.4875.

02:39:04.640 --> 02:39:05.640
Interesting.

02:39:05.640 --> 02:39:09.640
So this little negative 0.05 here

02:39:09.640 --> 02:39:13.640
is essentially being plugged into this negative x.

02:39:13.640 --> 02:39:21.640
So 1 over 1 plus 2.71 to the power of negative 0.05.

02:39:21.640 --> 02:39:23.640
So it's essentially,

02:39:23.640 --> 02:39:33.640
if we do 2.71, 2.71 to the power of negative 0.05,

02:39:33.640 --> 02:39:35.640
we're just going to get positive.

02:39:35.640 --> 02:39:40.640
So 1.05 and then 1 plus that.

02:39:40.640 --> 02:39:44.640
So that's 2.05.

02:39:44.640 --> 02:39:46.640
We just do 1 over that.

02:39:46.640 --> 02:39:47.640
2.05.

02:39:47.640 --> 02:39:50.640
So we get about 0.487.

02:39:50.640 --> 02:39:53.640
And what do we get here?

02:39:53.640 --> 02:39:54.640
0.4 at 7.

02:39:54.640 --> 02:39:56.640
Cool.

02:39:56.640 --> 02:39:57.640
So that's interesting.

02:39:57.640 --> 02:40:00.640
And let's actually look, is there a graph here?

02:40:00.640 --> 02:40:05.640
Let's look at the Sigmoid activation function.

02:40:05.640 --> 02:40:06.640
Wikipedia.

02:40:06.640 --> 02:40:08.640
Don't get too scared by this math here.

02:40:08.640 --> 02:40:09.640
I don't like it either,

02:40:09.640 --> 02:40:13.640
but I like the graphs they're cool to look at.

02:40:13.640 --> 02:40:15.640
So this is pretty much what it's doing here.

02:40:15.640 --> 02:40:19.640
So yeah, it's just a little curve.

02:40:19.640 --> 02:40:23.640
Kind of looks like a, it's kind of just like a wave,

02:40:23.640 --> 02:40:26.640
but it's cool looking.

02:40:26.640 --> 02:40:28.640
That's what the Sigmoid function does.

02:40:28.640 --> 02:40:31.640
It's used to just generalize over this line.

02:40:31.640 --> 02:40:36.640
And yeah, Sigmoid function is pretty cool.

02:40:36.640 --> 02:40:38.640
So now let's move on to the tanh.

02:40:38.640 --> 02:40:40.640
The tanh function.

02:40:40.640 --> 02:40:43.640
Google Bing is, or Microsoft Bing is giving me

02:40:43.640 --> 02:40:44.640
a nice description of that.

02:40:44.640 --> 02:40:46.640
Cool.

02:40:46.640 --> 02:40:47.640
Perfect.

02:40:47.640 --> 02:40:48.640
E to the negative x.

02:40:48.640 --> 02:40:51.640
I like that.

02:40:51.640 --> 02:40:53.640
So tanh is a little bit different.

02:40:53.640 --> 02:40:56.640
There's a lot more exponentiating going on here.

02:40:56.640 --> 02:41:01.640
So you have, I'll just say expo or exp of x

02:41:01.640 --> 02:41:03.640
minus exp of negative x

02:41:03.640 --> 02:41:06.640
divided by exp of x plus exp of negative x.

02:41:06.640 --> 02:41:09.640
There's a lot of positives and negatives in here.

02:41:09.640 --> 02:41:13.640
Positive, positive, negative, negative, negative, positive.

02:41:13.640 --> 02:41:16.640
So that's interesting.

02:41:16.640 --> 02:41:19.640
Let's go ahead and put this into code here.

02:41:19.640 --> 02:41:23.640
So I'll go torch dot examples, or torch examples.

02:41:23.640 --> 02:41:25.640
This is our file here.

02:41:25.640 --> 02:41:30.640
And I'll just go tanh.

02:41:30.640 --> 02:41:32.640
Cool.

02:41:32.640 --> 02:41:35.640
So negative 0.05.

02:41:35.640 --> 02:41:36.640
Cool.

02:41:36.640 --> 02:41:37.640
What if we do a one?

02:41:37.640 --> 02:41:41.640
What will that produce?

02:41:41.640 --> 02:41:45.640
Oh, 0.76.

02:41:45.640 --> 02:41:50.640
What if we do a 10?

02:41:50.640 --> 02:41:51.640
1.0.

02:41:51.640 --> 02:41:53.640
Interesting.

02:41:53.640 --> 02:41:56.640
So this is sort of similar to the sigmoid,

02:41:56.640 --> 02:41:58.640
except it's, you know,

02:41:58.640 --> 02:42:05.640
it's actually asked to attach a BT what the difference is.

02:42:06.640 --> 02:42:13.640
When would you use tanh over sigmoid?

02:42:13.640 --> 02:42:16.640
Let's see here.

02:42:16.640 --> 02:42:20.640
Sigmoid function and hyperbolic tangent or tanh function

02:42:20.640 --> 02:42:23.640
are activations functions used in neural networks.

02:42:23.640 --> 02:42:25.640
They have a similar s-shaped curve,

02:42:25.640 --> 02:42:27.640
but have different ranges.

02:42:27.640 --> 02:42:30.640
So sigmoid output values between a 0 and a 1

02:42:30.640 --> 02:42:33.640
while tanh is between a negative 1 and a 1.

02:42:33.640 --> 02:42:35.640
So if you're, you know,

02:42:35.640 --> 02:42:38.640
if you're rating maybe the,

02:42:38.640 --> 02:42:42.640
maybe if you're getting a probability distribution,

02:42:42.640 --> 02:42:46.640
for example, you want it to be between 0 and 1,

02:42:46.640 --> 02:42:48.640
meaning percentages or decimal places.

02:42:48.640 --> 02:42:53.640
So like a 0.5 would be 50%, 0.87 would be 87%.

02:42:53.640 --> 02:42:56.640
And that's what the sigmoid function does.

02:42:56.640 --> 02:42:59.640
It's quite close to the softmax function, actually.

02:42:59.640 --> 02:43:02.640
Except the softmax just, you know,

02:43:02.640 --> 02:43:05.640
it prioritizes the bigger values

02:43:05.640 --> 02:43:08.640
and puts the smaller values to our priority.

02:43:08.640 --> 02:43:10.640
That's all the softmax says.

02:43:10.640 --> 02:43:12.640
It's kind of a sigmoid on steroids.

02:43:12.640 --> 02:43:16.640
And the tanh outputs between negative 1 and 1.

02:43:16.640 --> 02:43:20.640
So, yeah, you could maybe even start theorycrafting

02:43:20.640 --> 02:43:23.640
and thinking of some ways you could use

02:43:23.640 --> 02:43:26.640
even the tanh function and sigmoid in different use cases.

02:43:26.640 --> 02:43:29.640
So that's kind of a general overview on those.

02:43:29.640 --> 02:43:31.640
So biogram language models are finished.

02:43:31.640 --> 02:43:33.640
All of this we finished here is now done.

02:43:33.640 --> 02:43:35.640
You're back from your break.

02:43:35.640 --> 02:43:38.640
If you took one, if you didn't, that's fine too.

02:43:38.640 --> 02:43:41.640
But pretty much we're going to dig into

02:43:41.640 --> 02:43:44.640
the transformer architecture now.

02:43:44.640 --> 02:43:47.640
And we're actually going to build it from scratch.

02:43:47.640 --> 02:43:50.640
So there was recently a paper proposed

02:43:50.640 --> 02:43:53.640
called the transformer model.

02:43:53.640 --> 02:43:57.640
And this uses a mechanism called self-attention.

02:43:57.640 --> 02:44:00.640
Self-attention is used in these multi-head attention,

02:44:00.640 --> 02:44:02.640
little bricks here.

02:44:02.640 --> 02:44:05.640
And there's a lot that happens.

02:44:05.640 --> 02:44:07.640
So there's something I want to clarify

02:44:07.640 --> 02:44:09.640
before we jump right into this architecture

02:44:09.640 --> 02:44:12.640
and just dump a bunch of information

02:44:12.640 --> 02:44:15.640
on your poor little brain right now.

02:44:15.640 --> 02:44:18.640
But a lot of these networks, at first,

02:44:18.640 --> 02:44:21.640
can be extremely confusing to beginners.

02:44:21.640 --> 02:44:23.640
So I want to make it clear.

02:44:23.640 --> 02:44:26.640
It's perfectly okay if you don't understand this at first.

02:44:26.640 --> 02:44:29.640
I'm going to try to explain this in the best way possible.

02:44:29.640 --> 02:44:31.640
Believe me, I've seen tons of videos

02:44:31.640 --> 02:44:34.640
on people explaining the transformer architecture.

02:44:34.640 --> 02:44:37.640
And all of them have been, to some degree,

02:44:37.640 --> 02:44:39.640
a bit confusing to me as well.

02:44:39.640 --> 02:44:41.640
So I'm going to try to clarify

02:44:41.640 --> 02:44:46.640
all those little pieces of confusion.

02:44:46.640 --> 02:44:48.640
Like what does that mean?

02:44:48.640 --> 02:44:49.640
You didn't cover that piece.

02:44:49.640 --> 02:44:51.640
I don't know what's going on here.

02:44:51.640 --> 02:44:53.640
I'm going to cover all those little bits

02:44:53.640 --> 02:44:56.640
and make sure that nothing is left behind.

02:44:56.640 --> 02:44:58.640
So you're going to want to sit tight

02:44:58.640 --> 02:45:01.640
and pay attention for this next part here.

02:45:01.640 --> 02:45:04.640
So yeah, let's go ahead and dive into

02:45:04.640 --> 02:45:07.640
just the general transformer architecture

02:45:07.640 --> 02:45:09.640
and why it's important.

02:45:09.640 --> 02:45:11.640
So in the transformer network,

02:45:11.640 --> 02:45:14.640
you have a lot of computation going on.

02:45:14.640 --> 02:45:17.640
You have some adding and normalizing.

02:45:17.640 --> 02:45:19.640
You have some multi-hat attention.

02:45:19.640 --> 02:45:21.640
You have some feed forward networks.

02:45:21.640 --> 02:45:23.640
There's a lot going on here.

02:45:23.640 --> 02:45:25.640
There's a lot of computation, a lot of multiplying,

02:45:25.640 --> 02:45:27.640
there's a lot going on.

02:45:27.640 --> 02:45:29.640
So the question I actually had at first was,

02:45:29.640 --> 02:45:32.640
well, if you're just multiplying these inputs

02:45:32.640 --> 02:45:34.640
by a bunch of different things along,

02:45:34.640 --> 02:45:37.640
you should just end up with some random value at the end

02:45:37.640 --> 02:45:40.640
that maybe doesn't really mean that much

02:45:40.640 --> 02:45:42.640
of the initial input.

02:45:42.640 --> 02:45:44.640
And that's actually correct.

02:45:44.640 --> 02:45:47.640
For the first few iterations,

02:45:47.640 --> 02:45:49.640
the model has absolutely no context

02:45:49.640 --> 02:45:50.640
as to what's going on.

02:45:50.640 --> 02:45:51.640
It is clueless.

02:45:51.640 --> 02:45:53.640
It is going in random directions

02:45:53.640 --> 02:45:57.640
and it's just trying to find the best way to converge.

02:45:57.640 --> 02:45:59.640
So this is what machine learning and deep learning

02:45:59.640 --> 02:46:01.640
is actually all about,

02:46:01.640 --> 02:46:04.640
is having all these little parameters in,

02:46:04.640 --> 02:46:06.640
you know, the adding and normalizing,

02:46:06.640 --> 02:46:09.640
the feed forward networks, even multi-hat attention.

02:46:09.640 --> 02:46:12.640
We're trying to optimize the parameters

02:46:12.640 --> 02:46:15.640
for producing an output that is meaningful

02:46:15.640 --> 02:46:17.640
that will actually help us produce

02:46:17.640 --> 02:46:21.640
almost perfectly like English text.

02:46:21.640 --> 02:46:23.640
And so this is the entire process of pre-training.

02:46:23.640 --> 02:46:26.640
You send a bunch of inputs into a transformer

02:46:26.640 --> 02:46:29.640
and you get some output probabilities

02:46:29.640 --> 02:46:31.640
that you used to generate from.

02:46:31.640 --> 02:46:34.640
And what attention does

02:46:34.640 --> 02:46:37.640
is it sets little different scores

02:46:37.640 --> 02:46:41.640
to, you know, each little token in a sentence.

02:46:41.640 --> 02:46:44.640
For tokens you have character, subword,

02:46:44.640 --> 02:46:46.640
and word-level tokens.

02:46:46.640 --> 02:46:49.640
So you're pretty much just mapping

02:46:49.640 --> 02:46:51.640
bits of attention to each of these,

02:46:51.640 --> 02:46:53.640
as well as, you know,

02:46:53.640 --> 02:46:56.640
what is the position also mean as well.

02:46:56.640 --> 02:47:00.640
So you could have two words that are right next to each other,

02:47:00.640 --> 02:47:02.640
but then if you don't actually, you know,

02:47:02.640 --> 02:47:04.640
positionally encode them,

02:47:04.640 --> 02:47:06.640
it doesn't really mean much,

02:47:06.640 --> 02:47:09.640
because it's like, oh, these could be like 4,000 characters apart.

02:47:09.640 --> 02:47:11.640
So that's why you need both

02:47:11.640 --> 02:47:14.640
to put attention scores on these tokens

02:47:14.640 --> 02:47:17.640
and to positionally encode them.

02:47:17.640 --> 02:47:19.640
And that's what's happening here.

02:47:19.640 --> 02:47:24.640
So what we do is we get to our inputs.

02:47:24.640 --> 02:47:26.640
We got our inputs.

02:47:26.640 --> 02:47:28.640
So, I mean, we went over this with

02:47:28.640 --> 02:47:30.640
diagram language models.

02:47:30.640 --> 02:47:33.640
We feed our X and Y,

02:47:33.640 --> 02:47:35.640
so X would be our inputs,

02:47:35.640 --> 02:47:38.640
Y would be our targets or outputs.

02:47:38.640 --> 02:47:41.640
And what we're going to do

02:47:41.640 --> 02:47:44.640
is give these little embeddings.

02:47:44.640 --> 02:47:47.640
So I believe we went over embeddings a little while ago,

02:47:47.640 --> 02:47:49.640
and pretty much what those mean

02:47:49.640 --> 02:47:51.640
is it's going to have a little row

02:47:51.640 --> 02:47:53.640
for each token on that table,

02:47:53.640 --> 02:47:55.640
and that's going to store, you know,

02:47:55.640 --> 02:47:58.640
some vector as to what that token means.

02:47:58.640 --> 02:48:01.640
So let's say you had, like, you know,

02:48:01.640 --> 02:48:04.640
the character E, for example,

02:48:04.640 --> 02:48:09.640
the sentiment or the vector of the character E

02:48:09.640 --> 02:48:11.640
is probably going to be vastly different

02:48:11.640 --> 02:48:13.640
than the sentiment of Z, right?

02:48:13.640 --> 02:48:15.640
Because E is a very common vowel,

02:48:15.640 --> 02:48:17.640
and Z is one of the most uncommon,

02:48:17.640 --> 02:48:21.640
if not the most uncommon letter in the English language.

02:48:21.640 --> 02:48:24.640
So these embeddings are learned.

02:48:24.640 --> 02:48:27.640
We have these both for our inputs and our outputs.

02:48:27.640 --> 02:48:29.640
We give them positional encodings

02:48:29.640 --> 02:48:31.640
like I was talking about,

02:48:31.640 --> 02:48:33.640
and there's ways we can do that.

02:48:33.640 --> 02:48:36.640
We can actually use learnable parameters

02:48:36.640 --> 02:48:38.640
to assign these encodings.

02:48:38.640 --> 02:48:41.640
A lot of these are learnable parameters, by the way,

02:48:41.640 --> 02:48:43.640
and you'll see that as you, you know,

02:48:43.640 --> 02:48:45.640
delve more and more into transformers.

02:48:45.640 --> 02:48:50.640
But, yeah, so after we've given these inputs,

02:48:50.640 --> 02:48:52.640
embeddings, and positional encodings,

02:48:52.640 --> 02:48:54.640
and same thing with the outputs,

02:48:54.640 --> 02:48:56.640
which are essentially just shifted right,

02:48:56.640 --> 02:48:59.640
you have, you know, I up to block size for inputs,

02:48:59.640 --> 02:49:04.640
and then I plus one up to block size plus one, right?

02:49:04.640 --> 02:49:08.640
Or whatever little thing we employed here

02:49:08.640 --> 02:49:10.640
in our background language models.

02:49:10.640 --> 02:49:12.640
Quite what it was.

02:49:12.640 --> 02:49:14.640
Or even if we did that at all.

02:49:18.640 --> 02:49:20.640
No.

02:49:20.640 --> 02:49:22.640
I'm just speaking gibberish right now,

02:49:22.640 --> 02:49:24.640
but that's fine because it's going to make sense

02:49:24.640 --> 02:49:26.640
in a little bit here.

02:49:28.640 --> 02:49:31.640
So what I'm going to actually do

02:49:31.640 --> 02:49:33.640
is I'm not going to read off of this right here

02:49:33.640 --> 02:49:35.640
because this is really confusing.

02:49:35.640 --> 02:49:38.640
So I'm going to switch over to a little,

02:49:39.640 --> 02:49:42.640
like a little sketch that I drew out.

02:49:42.640 --> 02:49:44.640
And this is pretty much the entire transformer

02:49:44.640 --> 02:49:46.640
with a lot of other things considered

02:49:46.640 --> 02:49:50.640
that this initial image does not really put into perspective.

02:49:50.640 --> 02:49:53.640
So let's go ahead and jump into

02:49:53.640 --> 02:49:57.640
sort of what's going on in here from the ground up.

02:49:57.640 --> 02:49:59.640
So like I was talking about before,

02:49:59.640 --> 02:50:01.640
we have some inputs and we have some outputs

02:50:01.640 --> 02:50:03.640
which are shifted right,

02:50:03.640 --> 02:50:07.640
and we give each of them some embedding vectors

02:50:07.640 --> 02:50:09.640
and positional encodings.

02:50:09.640 --> 02:50:12.640
So from here, let's say we have n layers.

02:50:12.640 --> 02:50:14.640
This is going to make sense in a second.

02:50:14.640 --> 02:50:16.640
n layers is set to four.

02:50:16.640 --> 02:50:18.640
So the amount of layers we have is set to four.

02:50:18.640 --> 02:50:21.640
So you can see we have an encoder, encoder.

02:50:21.640 --> 02:50:24.640
Like we have four of these and we have four decoders.

02:50:24.640 --> 02:50:27.640
So four is actually the amount of encoders

02:50:27.640 --> 02:50:29.640
and decoders we have.

02:50:29.640 --> 02:50:31.640
We always have the same amount of each.

02:50:31.640 --> 02:50:34.640
So if we have, you know, ten layers,

02:50:34.640 --> 02:50:37.640
that means we have ten encoders and ten decoders.

02:50:37.640 --> 02:50:39.640
And pretty much what would happen

02:50:39.640 --> 02:50:42.640
is after this input,

02:50:42.640 --> 02:50:44.640
embedding and positional embedding,

02:50:44.640 --> 02:50:47.640
we feed that into the first encoder layer

02:50:47.640 --> 02:50:49.640
and then the next, and then next,

02:50:49.640 --> 02:50:51.640
and then right as soon as we hit the last one,

02:50:51.640 --> 02:50:56.640
we feed these into each of these decoders here,

02:50:56.640 --> 02:50:58.640
each of these decoder layers.

02:50:58.640 --> 02:51:03.640
So only the last encoder will feed into these decoders.

02:51:03.640 --> 02:51:08.640
And pretty much these decoders will all run.

02:51:08.640 --> 02:51:10.640
They'll all learn different things.

02:51:10.640 --> 02:51:12.640
And then they'll turn what they learned.

02:51:12.640 --> 02:51:15.640
They'll do, they'll apply a linear transformation

02:51:15.640 --> 02:51:16.640
at the end of it.

02:51:16.640 --> 02:51:18.640
This is not in the decoder function.

02:51:18.640 --> 02:51:20.640
This is actually after the last decoder.

02:51:20.640 --> 02:51:22.640
It'll apply a linear transformation

02:51:22.640 --> 02:51:26.640
to pretty much sort of simplify

02:51:26.640 --> 02:51:28.640
or give a summary of what it learned.

02:51:28.640 --> 02:51:33.640
And then we apply a softmax on that new, you know, tensor

02:51:33.640 --> 02:51:36.640
to get some probabilities to sample from,

02:51:36.640 --> 02:51:38.640
like we talked about in the generate function

02:51:38.640 --> 02:51:39.640
in our biogram.

02:51:39.640 --> 02:51:42.640
And then once we get these probabilities,

02:51:42.640 --> 02:51:47.640
we can then sample from them and generate tokens.

02:51:47.640 --> 02:51:51.640
And that's kind of like the first little step here.

02:51:51.640 --> 02:51:52.640
That's what's going on.

02:51:52.640 --> 02:51:53.640
We have some encoders.

02:51:53.640 --> 02:51:55.640
We have some decoders.

02:51:55.640 --> 02:51:57.640
We do a transformation to summarize.

02:51:57.640 --> 02:51:59.640
We have a softmax to get probabilities.

02:51:59.640 --> 02:52:01.640
And then we generate based on those probabilities.

02:52:01.640 --> 02:52:04.640
Cool.

02:52:04.640 --> 02:52:06.640
Next up, in the encoder,

02:52:06.640 --> 02:52:09.640
in each of these encoders, this is what it's going to look like.

02:52:09.640 --> 02:52:11.640
So we have multi-hat attention,

02:52:11.640 --> 02:52:14.640
which I'm going to dub into a second here.

02:52:14.640 --> 02:52:17.640
So after this multi-hat attention,

02:52:17.640 --> 02:52:19.640
we have a residual connection.

02:52:19.640 --> 02:52:22.640
So in case you aren't familiar with residual connections,

02:52:22.640 --> 02:52:24.640
I might have went over this before.

02:52:24.640 --> 02:52:26.640
But pretty much what they do is

02:52:26.640 --> 02:52:28.640
it's a little connector.

02:52:28.640 --> 02:52:30.640
So I don't know.

02:52:30.640 --> 02:52:32.640
Let's say you get some inputs X,

02:52:32.640 --> 02:52:34.640
you have some inputs X down here,

02:52:34.640 --> 02:52:38.640
and you put them into some sort of function here,

02:52:38.640 --> 02:52:40.640
some sort of like feedforward network, whatever it is.

02:52:40.640 --> 02:52:43.640
A feedforward network is essentially just a linear,

02:52:43.640 --> 02:52:45.640
a RELU, and then a linear.

02:52:45.640 --> 02:52:47.640
That's all feedforward network is right here.

02:52:47.640 --> 02:52:49.640
Linear, really, really linear.

02:52:49.640 --> 02:52:55.640
And all you do is you wrap those inputs

02:52:55.640 --> 02:52:58.640
around so you don't actually put them

02:52:58.640 --> 02:53:00.640
into that feedforward network.

02:53:00.640 --> 02:53:02.640
You actually wrap them around,

02:53:02.640 --> 02:53:05.640
and then you can add them to the output.

02:53:05.640 --> 02:53:07.640
So you had some X values here,

02:53:07.640 --> 02:53:10.640
go through the RELU, and then you had some wrap around.

02:53:10.640 --> 02:53:14.640
And then right here, you simply add them together

02:53:14.640 --> 02:53:17.640
and you normalize them using some encod layer norm,

02:53:17.640 --> 02:53:19.640
which we're going to cover in a little bit.

02:53:19.640 --> 02:53:23.640
And the reason why residual connections

02:53:23.640 --> 02:53:26.640
are so useful in transformers

02:53:26.640 --> 02:53:29.640
is because when you have a really deep neural network,

02:53:29.640 --> 02:53:32.640
a lot of the information is actually forgotten

02:53:32.640 --> 02:53:34.640
in the first steps.

02:53:34.640 --> 02:53:37.640
So if you have your first view encoder layers

02:53:37.640 --> 02:53:39.640
and your first view decoder layers,

02:53:39.640 --> 02:53:42.640
a lot of the information here is going to be forgotten

02:53:42.640 --> 02:53:44.640
because it's not being carried through.

02:53:44.640 --> 02:53:48.640
The first steps of it aren't explicitly being carried through

02:53:48.640 --> 02:53:52.640
and sort of skipped through the functions.

02:53:52.640 --> 02:53:56.640
And yeah, you can sort of see how they would just be forgotten.

02:53:56.640 --> 02:53:59.640
So residual connections are sort of just a cheat

02:53:59.640 --> 02:54:01.640
for getting around that,

02:54:01.640 --> 02:54:03.640
for not having deep neural networks forget things

02:54:03.640 --> 02:54:05.640
from the beginning,

02:54:05.640 --> 02:54:07.640
and having them all sort of work together to the same degree.

02:54:07.640 --> 02:54:10.640
So residual connections are great that way.

02:54:10.640 --> 02:54:13.640
And then, you know, at the end there,

02:54:13.640 --> 02:54:16.640
you would add them together and then normalize.

02:54:16.640 --> 02:54:19.640
And there's two different ways that you can do this add a norm.

02:54:19.640 --> 02:54:22.640
There's add a norm and then norm and add.

02:54:22.640 --> 02:54:27.640
So these are two different separate architectures

02:54:27.640 --> 02:54:30.640
that you can do in transformers.

02:54:30.640 --> 02:54:34.640
And both of these are sort of like meta architectures.

02:54:34.640 --> 02:54:40.640
But pretty much pre-norm is the normalize then add,

02:54:40.640 --> 02:54:42.640
and then post-norm is add then normalize.

02:54:42.640 --> 02:54:46.640
So in this attention is all you need paper

02:54:46.640 --> 02:54:51.640
proposed by a bunch of research scientists was

02:54:51.640 --> 02:54:55.640
initially you want to add these,

02:54:55.640 --> 02:54:59.640
you want to add these together and then normalize them.

02:54:59.640 --> 02:55:05.640
So that is what we call the post-norm architecture.

02:55:05.640 --> 02:55:08.640
And then pre-norm is just flip them around.

02:55:08.640 --> 02:55:13.640
So I've actually done some testing with pre-norm and post-norm

02:55:13.640 --> 02:55:17.640
and the original transformer paper

02:55:17.640 --> 02:55:21.640
turned out to be quite actually a lot better,

02:55:21.640 --> 02:55:24.640
at least for training very small language models.

02:55:24.640 --> 02:55:26.640
If you're training bigger ones, it might be different,

02:55:26.640 --> 02:55:31.640
but essentially we're just going to go by the rules that we use in here.

02:55:31.640 --> 02:55:32.640
So add a norm.

02:55:32.640 --> 02:55:33.640
We're not going to do norm and add.

02:55:33.640 --> 02:55:37.640
Add a norm in this video specifically because it works better

02:55:37.640 --> 02:55:41.640
and we just don't want to break any of the rules and go outside of it

02:55:41.640 --> 02:55:42.640
because then that starts to get confusing.

02:55:42.640 --> 02:55:45.640
And actually if you watch the Andre Carpathi lecture

02:55:45.640 --> 02:55:47.640
on building GPTs from scratch,

02:55:47.640 --> 02:55:53.640
he actually implemented it in the pre-norm way.

02:55:53.640 --> 02:55:55.640
So normalize then add.

02:55:55.640 --> 02:55:58.640
So yeah, based on my experience,

02:55:58.640 --> 02:56:05.640
what I've done on my computer here is the post-norm architecture works quite better.

02:56:05.640 --> 02:56:07.640
So that's why we're going to use it.

02:56:07.640 --> 02:56:10.640
We're going to do add then normalize.

02:56:10.640 --> 02:56:15.640
So then we essentially feed this into a feedforward network

02:56:15.640 --> 02:56:16.640
which we covered earlier.

02:56:16.640 --> 02:56:20.640
And then how did it go?

02:56:20.640 --> 02:56:23.640
So we're encoder.

02:56:23.640 --> 02:56:28.640
We do a residual connection from here to here

02:56:28.640 --> 02:56:35.640
and then another residual connection from outside of our feedforward network.

02:56:35.640 --> 02:56:39.640
So each time we're doing some other things like some, you know,

02:56:39.640 --> 02:56:41.640
some computation blocks in here,

02:56:41.640 --> 02:56:43.640
we're going to have a rest connection.

02:56:43.640 --> 02:56:46.640
Same with our feedforward rest connection.

02:56:46.640 --> 02:56:49.640
And then of course the output from here,

02:56:49.640 --> 02:56:51.640
just when it exits,

02:56:51.640 --> 02:56:53.640
it's going to feed into the next encoder block

02:56:53.640 --> 02:56:55.640
if it's not the last encoder.

02:56:55.640 --> 02:56:57.640
So this one is going to do all this.

02:56:57.640 --> 02:56:58.640
It's going to feed into that one.

02:56:58.640 --> 02:56:59.640
It's going to do the same thing.

02:56:59.640 --> 02:57:00.640
Feed into this one.

02:57:00.640 --> 02:57:02.640
Going to feed into that one.

02:57:02.640 --> 02:57:06.640
And then the output of this is going to feed into each of these decoders,

02:57:06.640 --> 02:57:09.640
all the same information.

02:57:09.640 --> 02:57:15.640
And yeah, so that's a little bit scoped in as to what these encoders look like.

02:57:15.640 --> 02:57:18.640
So now that you know what the encoder looks like,

02:57:18.640 --> 02:57:19.640
what the feedforward looks like,

02:57:19.640 --> 02:57:21.640
we're going to go into multi-head attention,

02:57:21.640 --> 02:57:23.640
sort of the premise,

02:57:23.640 --> 02:57:26.640
sort of the highlight of the transformer architecture

02:57:26.640 --> 02:57:28.640
and why it's so important.

02:57:28.640 --> 02:57:31.640
So multi-head attention,

02:57:31.640 --> 02:57:33.640
we call it multi-head attention

02:57:33.640 --> 02:57:35.640
because there are a bunch of these different heads

02:57:35.640 --> 02:57:37.640
learning different semantic info

02:57:37.640 --> 02:57:39.640
from a unique perspective.

02:57:39.640 --> 02:57:42.640
So let's say you have 10 different people

02:57:42.640 --> 02:57:45.640
looking at the same book.

02:57:45.640 --> 02:57:47.640
If you have 10 different people,

02:57:47.640 --> 02:57:52.640
let's say they're all reading the same Harry Potter book.

02:57:52.640 --> 02:57:54.640
These different people,

02:57:54.640 --> 02:57:57.640
they might have different cognitive abilities.

02:57:57.640 --> 02:57:59.640
They might have different IQs.

02:57:59.640 --> 02:58:01.640
They might have been raised in different ways.

02:58:01.640 --> 02:58:03.640
So they might interpret things differently.

02:58:03.640 --> 02:58:06.640
They might look at little things in that book

02:58:06.640 --> 02:58:08.640
and their mind will,

02:58:08.640 --> 02:58:10.640
they'll imagine different scenarios,

02:58:10.640 --> 02:58:12.640
different environments from the book.

02:58:12.640 --> 02:58:16.640
And essentially why this is so valuable

02:58:16.640 --> 02:58:19.640
is because we don't just want to have one person,

02:58:19.640 --> 02:58:21.640
just one perspective on this.

02:58:21.640 --> 02:58:24.640
We want to have a bunch of different heads in parallel

02:58:24.640 --> 02:58:30.640
looking at this same piece of data

02:58:30.640 --> 02:58:33.640
because they're all going to capture different things about it.

02:58:33.640 --> 02:58:36.640
And keep in mind each of these heads,

02:58:36.640 --> 02:58:38.640
each of these heads in parallel,

02:58:38.640 --> 02:58:40.640
these different perspectives,

02:58:40.640 --> 02:58:42.640
they have different learnable parameters.

02:58:42.640 --> 02:58:44.640
So they're not all the same one

02:58:44.640 --> 02:58:46.640
looking at this piece of data.

02:58:46.640 --> 02:58:49.640
They're actually,

02:58:49.640 --> 02:58:51.640
they all have different learnable parameters.

02:58:51.640 --> 02:58:54.640
So you have a bunch of these

02:58:54.640 --> 02:58:56.640
at the same time learning different things

02:58:56.640 --> 02:58:58.640
and that's why it's so powerful.

02:58:58.640 --> 02:59:04.640
So this scale.product attention runs in parallel,

02:59:04.640 --> 02:59:06.640
which means we can scale that to the GPU,

02:59:06.640 --> 02:59:08.640
which is very useful.

02:59:08.640 --> 02:59:10.640
It's good to touch on that.

02:59:10.640 --> 02:59:12.640
Anything with the GPU that you can accelerate

02:59:12.640 --> 02:59:14.640
is just an automatic win

02:59:14.640 --> 02:59:19.640
because parallelism is great in machine learning.

02:59:19.640 --> 02:59:21.640
Why not have parallelism, right?

02:59:21.640 --> 02:59:23.640
If it's just going to be running the CPU, what's the point?

02:59:23.640 --> 02:59:25.640
That's why we love GPUs.

02:59:25.640 --> 02:59:27.640
Anyways, yeah.

02:59:27.640 --> 02:59:29.640
So you're going to have these different,

02:59:29.640 --> 02:59:31.640
you're going to have these things that are called keys,

02:59:31.640 --> 02:59:33.640
queries and values.

02:59:33.640 --> 02:59:35.640
I'll touch on those in a second here

02:59:35.640 --> 02:59:37.640
because keys, queries and values

02:59:37.640 --> 02:59:39.640
sort of point to self-attention,

02:59:39.640 --> 02:59:41.640
which is literally the entire point of the transformer.

02:59:41.640 --> 02:59:43.640
Transformer wouldn't really mean anything

02:59:43.640 --> 02:59:45.640
without self-attention.

02:59:45.640 --> 02:59:47.640
So I'll touch on those in a second here

02:59:47.640 --> 02:59:49.640
and we'll actually delve deeper

02:59:49.640 --> 02:59:51.640
as we hit this sort of block.

02:59:51.640 --> 02:59:53.640
But yeah, you have these keys, queries and values.

02:59:53.640 --> 02:59:55.640
They go into scale.product attention.

02:59:55.640 --> 02:59:57.640
So a bunch of these running in parallel

02:59:57.640 --> 02:59:59.640
and then you concatenate the results

02:59:59.640 --> 03:00:01.640
from all these different heads running in parallel.

03:00:01.640 --> 03:00:03.640
You have all these different people.

03:00:03.640 --> 03:00:05.640
You concatenate all of them,

03:00:05.640 --> 03:00:07.640
you generalize it,

03:00:07.640 --> 03:00:09.640
and then you apply a transformation

03:00:09.640 --> 03:00:11.640
to a linear transformation

03:00:11.640 --> 03:00:13.640
to pretty much summarize that

03:00:13.640 --> 03:00:16.640
and then do your add a norm,

03:00:16.640 --> 03:00:18.640
then pay for a network.

03:00:18.640 --> 03:00:20.640
So that's what's going on in multi-head attention.

03:00:20.640 --> 03:00:22.640
You're just doing a bunch of self-attentions

03:00:22.640 --> 03:00:24.640
in parallel, concatenating,

03:00:24.640 --> 03:00:26.640
and then continuing on with this part.

03:00:26.640 --> 03:00:28.640
So scale.product attention.

03:00:28.640 --> 03:00:30.640
What is that?

03:00:30.640 --> 03:00:32.640
So let's just start from the ground up here.

03:00:32.640 --> 03:00:34.640
We'll just go from left to right.

03:00:34.640 --> 03:00:36.640
So you have your keys, queries and values.

03:00:36.640 --> 03:00:38.640
What do your keys do?

03:00:38.640 --> 03:00:40.640
Well, a key is

03:00:40.640 --> 03:00:42.640
let's just say you have a token and a sentence.

03:00:42.640 --> 03:00:44.640
Okay?

03:00:44.640 --> 03:00:46.640
So if you have

03:00:46.640 --> 03:00:48.640
let me just

03:00:48.640 --> 03:00:50.640
roll down here to a good example.

03:00:50.640 --> 03:00:52.640
So

03:00:52.640 --> 03:00:54.640
self-attention

03:00:54.640 --> 03:00:56.640
uses

03:00:56.640 --> 03:00:58.640
keys, queries and values.

03:00:58.640 --> 03:01:00.640
Self-attention helps

03:01:00.640 --> 03:01:02.640
identify

03:01:02.640 --> 03:01:04.640
which of these tokens in a sentence

03:01:04.640 --> 03:01:06.640
in any given sentence are more important

03:01:06.640 --> 03:01:08.640
and how much attention

03:01:08.640 --> 03:01:10.640
you should pay

03:01:10.640 --> 03:01:12.640
to each of those characters or words, whatever you're using.

03:01:12.640 --> 03:01:14.640
We'll just use words

03:01:14.640 --> 03:01:16.640
to

03:01:16.640 --> 03:01:18.640
make it easier to understand for the purpose of this video.

03:01:18.640 --> 03:01:20.640
But

03:01:20.640 --> 03:01:22.640
essentially imagine you have

03:01:24.640 --> 03:01:26.640
these two sentences here.

03:01:26.640 --> 03:01:28.640
So you have

03:01:28.640 --> 03:01:30.640
let me bring out my little piece of text.

03:01:30.640 --> 03:01:32.640
So you have

03:01:34.640 --> 03:01:36.640
that didn't work.

03:01:38.640 --> 03:01:40.640
So imagine you have

03:01:42.640 --> 03:01:44.640
server, can I have the check?

03:01:44.640 --> 03:01:46.640
And then you have

03:01:48.640 --> 03:01:50.640
and you have

03:01:50.640 --> 03:01:52.640
looks like I crashed the server.

03:01:52.640 --> 03:01:54.640
So

03:01:54.640 --> 03:01:56.640
I mean, both of these have

03:01:56.640 --> 03:01:58.640
the word server in them, but they mean different things.

03:01:58.640 --> 03:02:00.640
Server meaning like the waiter

03:02:00.640 --> 03:02:02.640
or the waitress or whoever

03:02:02.640 --> 03:02:04.640
is billing

03:02:04.640 --> 03:02:06.640
you at the end of your restaurant visit.

03:02:06.640 --> 03:02:08.640
And then looks like I crashed the server

03:02:08.640 --> 03:02:10.640
is like, oh, there's actually a server running

03:02:10.640 --> 03:02:12.640
in the cloud, not like a person

03:02:12.640 --> 03:02:14.640
that's billing me, but an actual server.

03:02:14.640 --> 03:02:16.640
That's maybe running a video game.

03:02:16.640 --> 03:02:18.640
And

03:02:18.640 --> 03:02:20.640
these are two different things. So what attention can do

03:02:20.640 --> 03:02:22.640
is it can actually identify

03:02:22.640 --> 03:02:24.640
which words would get attention here.

03:02:24.640 --> 03:02:26.640
So it can say

03:02:26.640 --> 03:02:28.640
server, can I have the check?

03:02:28.640 --> 03:02:30.640
Can I have?

03:02:30.640 --> 03:02:32.640
So it's maybe you're looking

03:02:32.640 --> 03:02:34.640
for something you're looking for the check

03:02:34.640 --> 03:02:36.640
and then server

03:02:36.640 --> 03:02:38.640
is like, oh, well in this

03:02:38.640 --> 03:02:40.640
in this particular sequence or in this

03:02:40.640 --> 03:02:42.640
in the sentiment of this sentence here

03:02:42.640 --> 03:02:44.640
server

03:02:44.640 --> 03:02:46.640
is specifically tied to

03:02:46.640 --> 03:02:48.640
this one meaning, maybe a human

03:02:48.640 --> 03:02:50.640
someone at a restaurant

03:02:50.640 --> 03:02:52.640
and then crash

03:02:52.640 --> 03:02:54.640
the server

03:02:54.640 --> 03:02:56.640
crash is going to get a very high attention

03:02:56.640 --> 03:02:58.640
score because

03:02:58.640 --> 03:03:00.640
you don't normally

03:03:00.640 --> 03:03:02.640
crash a server at a restaurant

03:03:02.640 --> 03:03:04.640
that doesn't particularly make sense.

03:03:04.640 --> 03:03:06.640
So

03:03:06.640 --> 03:03:08.640
if you have different words like this

03:03:08.640 --> 03:03:10.640
what self-attention will do

03:03:10.640 --> 03:03:12.640
is it will learn

03:03:12.640 --> 03:03:14.640
which words in the sentence

03:03:14.640 --> 03:03:16.640
are actually more important

03:03:16.640 --> 03:03:18.640
and which words should

03:03:18.640 --> 03:03:20.640
pay more attention to.

03:03:20.640 --> 03:03:22.640
So that's really all that's going on here

03:03:22.640 --> 03:03:24.640
and

03:03:24.640 --> 03:03:26.640
the key

03:03:26.640 --> 03:03:28.640
is essentially going to emit

03:03:28.640 --> 03:03:30.640
a different

03:03:30.640 --> 03:03:32.640
it's going to emit

03:03:32.640 --> 03:03:34.640
a little tensor

03:03:34.640 --> 03:03:36.640
here saying

03:03:36.640 --> 03:03:38.640
what do I contain

03:03:38.640 --> 03:03:40.640
and then query

03:03:40.640 --> 03:03:42.640
is going to say

03:03:42.640 --> 03:03:44.640
what am I looking for?

03:03:44.640 --> 03:03:46.640
So what's going to happen

03:03:46.640 --> 03:03:48.640
is if these, let's say

03:03:48.640 --> 03:03:50.640
server, it's going to look for things like

03:03:50.640 --> 03:03:52.640
check or crashed

03:03:52.640 --> 03:03:54.640
so if it sees crashed

03:03:54.640 --> 03:03:56.640
then that means the key and the query

03:03:56.640 --> 03:03:58.640
are going to multiply

03:03:58.640 --> 03:04:00.640
and it's going to get a very high attention score

03:04:00.640 --> 03:04:02.640
but if you have something

03:04:02.640 --> 03:04:04.640
like

03:04:04.640 --> 03:04:06.640
it's like

03:04:06.640 --> 03:04:08.640
there's literally almost any sentence

03:04:08.640 --> 03:04:10.640
so that doesn't mean much.

03:04:10.640 --> 03:04:12.640
We're not going to pay attention to those words

03:04:12.640 --> 03:04:14.640
so that's going to get a very low attention score

03:04:14.640 --> 03:04:16.640
and all attention

03:04:16.640 --> 03:04:18.640
is you're just dot-producting

03:04:18.640 --> 03:04:20.640
these vectors together.

03:04:20.640 --> 03:04:22.640
So you get a key

03:04:22.640 --> 03:04:24.640
and a query, you dot-product them

03:04:24.640 --> 03:04:26.640
we already went over dot-products

03:04:26.640 --> 03:04:28.640
in this course before

03:04:28.640 --> 03:04:30.640
and then

03:04:30.640 --> 03:04:32.640
this is a little bit of a confusing part

03:04:32.640 --> 03:04:34.640
is you just scale

03:04:34.640 --> 03:04:36.640
by one over the

03:04:36.640 --> 03:04:38.640
square root

03:04:38.640 --> 03:04:40.640
of the length of a row

03:04:40.640 --> 03:04:42.640
in the keys or queries matrix

03:04:42.640 --> 03:04:44.640
otherwise known as

03:04:44.640 --> 03:04:46.640
DK.

03:04:46.640 --> 03:04:48.640
So let's say we have

03:04:48.640 --> 03:04:50.640
our key and our query

03:04:50.640 --> 03:04:52.640
these are all going to be the same length by the way.

03:04:52.640 --> 03:04:54.640
Let's say our keys

03:04:54.640 --> 03:04:56.640
is

03:04:56.640 --> 03:04:58.640
maybe our keys is going to be like

03:04:58.640 --> 03:05:00.640
10 characters long

03:05:00.640 --> 03:05:02.640
our keys are going to be 10 characters long as well

03:05:02.640 --> 03:05:04.640
so it's going to do

03:05:04.640 --> 03:05:06.640
one over the square root of 10

03:05:06.640 --> 03:05:08.640
if that makes sense

03:05:08.640 --> 03:05:10.640
and so

03:05:10.640 --> 03:05:12.640
that's just

03:05:12.640 --> 03:05:14.640
essentially a way of preventing

03:05:14.640 --> 03:05:16.640
these dot-products

03:05:16.640 --> 03:05:18.640
from exploding

03:05:18.640 --> 03:05:20.640
we want to scale them because

03:05:20.640 --> 03:05:22.640
as we have

03:05:22.640 --> 03:05:24.640
as the length of it increases

03:05:24.640 --> 03:05:26.640
so will the

03:05:26.640 --> 03:05:28.640
ending dot-product

03:05:28.640 --> 03:05:30.640
because there's more of these to multiply

03:05:30.640 --> 03:05:32.640
so we pretty much just want to

03:05:32.640 --> 03:05:34.640
scale it by using

03:05:34.640 --> 03:05:36.640
an inverse square root

03:05:36.640 --> 03:05:38.640
and that will just help us with

03:05:38.640 --> 03:05:40.640
scaling make sure nothing explodes

03:05:40.640 --> 03:05:42.640
in unnecessary ways

03:05:42.640 --> 03:05:44.640
and then

03:05:44.640 --> 03:05:46.640
the next little important part

03:05:46.640 --> 03:05:48.640
is using tort.trill

03:05:48.640 --> 03:05:50.640
which I imagine we went over in our examples here

03:05:50.640 --> 03:05:52.640
trill

03:05:52.640 --> 03:05:54.640
yeah

03:05:54.640 --> 03:05:56.640
so

03:05:56.640 --> 03:05:58.640
you can see that

03:05:58.640 --> 03:06:00.640
it's a diagonal

03:06:00.640 --> 03:06:02.640
it's a left triangular

03:06:02.640 --> 03:06:04.640
matrix of ones

03:06:04.640 --> 03:06:06.640
and these aren't going to be ones

03:06:06.640 --> 03:06:08.640
in our self-attention here

03:06:08.640 --> 03:06:10.640
in our tort.trill or masking

03:06:10.640 --> 03:06:12.640
what this is going to be

03:06:12.640 --> 03:06:14.640
is

03:06:14.640 --> 03:06:16.640
the scores at each time step

03:06:16.640 --> 03:06:18.640
the combination of scores

03:06:18.640 --> 03:06:20.640
at each time step

03:06:20.640 --> 03:06:22.640
so

03:06:22.640 --> 03:06:24.640
if we've only gone

03:06:24.640 --> 03:06:26.640
if we're only looking at the first

03:06:26.640 --> 03:06:28.640
time step

03:06:28.640 --> 03:06:30.640
we should not have access to the rest of things

03:06:30.640 --> 03:06:32.640
or else that would be cheating

03:06:32.640 --> 03:06:34.640
we shouldn't be allowed to look ahead

03:06:34.640 --> 03:06:36.640
because we haven't actually produced these yet

03:06:36.640 --> 03:06:38.640
we need to produce these before we can

03:06:38.640 --> 03:06:40.640
put them into perspective

03:06:40.640 --> 03:06:42.640
and put a weight on them

03:06:42.640 --> 03:06:44.640
so we're going to set all these to zero

03:06:44.640 --> 03:06:46.640
and then we go to the next time step

03:06:46.640 --> 03:06:48.640
so now we've just generated this

03:06:48.640 --> 03:06:50.640
one we haven't generated these yet

03:06:50.640 --> 03:06:52.640
so we can't look at them

03:06:52.640 --> 03:06:54.640
and then as we go more and more

03:06:54.640 --> 03:06:56.640
as the time step increases

03:06:56.640 --> 03:06:58.640
we know more and more context

03:06:58.640 --> 03:07:00.640
about all of these tokens

03:07:00.640 --> 03:07:02.640
so

03:07:02.640 --> 03:07:04.640
that's all that's doing

03:07:04.640 --> 03:07:06.640
mask attention is pretty much just saying

03:07:06.640 --> 03:07:08.640
we don't want to look into the future

03:07:08.640 --> 03:07:10.640
we want to only guess with what we currently know

03:07:10.640 --> 03:07:12.640
in our current time step

03:07:12.640 --> 03:07:14.640
and everything before it

03:07:14.640 --> 03:07:16.640
you can't jump into the future

03:07:16.640 --> 03:07:18.640
look at what happened in the past

03:07:18.640 --> 03:07:20.640
and do stuff based on that

03:07:20.640 --> 03:07:22.640
same thing applies to life

03:07:22.640 --> 03:07:24.640
you can't really skip to the future and say

03:07:24.640 --> 03:07:26.640
hey if you do this you're going to be a billionaire

03:07:26.640 --> 03:07:28.640
no that would be cheating

03:07:28.640 --> 03:07:30.640
you're not allowed to do that

03:07:30.640 --> 03:07:32.640
you can only look at the mistakes you made

03:07:32.640 --> 03:07:34.640
and say how can I become a billionaire

03:07:34.640 --> 03:07:36.640
based on all these other mistakes that I made

03:07:36.640 --> 03:07:38.640
how can I become as close to perfect as possible

03:07:38.640 --> 03:07:40.640
which no one I can ever be perfect

03:07:40.640 --> 03:07:42.640
but that's my little analogy for the day

03:07:42.640 --> 03:07:44.640
so that's mask attention

03:07:44.640 --> 03:07:46.640
pretty much just not letting us skip time steps

03:07:46.640 --> 03:07:48.640
so that's fun

03:07:48.640 --> 03:07:50.640
let's continue

03:07:50.640 --> 03:07:52.640
two more little things I want to touch on before I jump forward here

03:07:52.640 --> 03:07:54.640
so

03:07:54.640 --> 03:07:56.640
these keys, queries and values

03:07:56.640 --> 03:07:58.640
each of these are learned through a linear transformation

03:07:58.640 --> 03:08:00.640
just an end dot linear

03:08:00.640 --> 03:08:02.640
is applied

03:08:02.640 --> 03:08:04.640
and that's how we get our keys, queries and values

03:08:04.640 --> 03:08:06.640
so that's just a little

03:08:06.640 --> 03:08:08.640
touching there if you're wondering how do we get those

03:08:08.640 --> 03:08:10.640
it's just an end dot linear transformation

03:08:10.640 --> 03:08:12.640
and then as for our

03:08:12.640 --> 03:08:14.640
masking we don't actually apply this all the time

03:08:14.640 --> 03:08:16.640
you might have seen right here

03:08:16.640 --> 03:08:18.640
we have

03:08:18.640 --> 03:08:20.640
multi-head attention

03:08:20.640 --> 03:08:22.640
multi-head attention and then mask

03:08:22.640 --> 03:08:24.640
multi-head attention

03:08:24.640 --> 03:08:26.640
so this masked attention isn't used all the time

03:08:26.640 --> 03:08:28.640
it's only used

03:08:28.640 --> 03:08:30.640
actually one out of the three attentions

03:08:30.640 --> 03:08:32.640
we have per layer

03:08:32.640 --> 03:08:34.640
so

03:08:34.640 --> 03:08:36.640
I'll give you a little bit more information

03:08:36.640 --> 03:08:38.640
about that as we

03:08:38.640 --> 03:08:40.640
progress more and more into the architecture

03:08:40.640 --> 03:08:42.640
as we learn more about it

03:08:42.640 --> 03:08:44.640
I'm not going to dive into that

03:08:44.640 --> 03:08:46.640
quite yet though

03:08:46.640 --> 03:08:48.640
so let's just continue on with what's going on

03:08:48.640 --> 03:08:50.640
so we have a softmax

03:08:50.640 --> 03:08:52.640
and why softmax important

03:08:52.640 --> 03:08:54.640
well

03:08:54.640 --> 03:08:56.640
I actually mentioned earlier

03:08:56.640 --> 03:08:58.640
softmax is not commonly used

03:08:58.640 --> 03:09:00.640
as a normalization method

03:09:00.640 --> 03:09:02.640
but here we're actually using

03:09:02.640 --> 03:09:04.640
softmax to normalize

03:09:04.640 --> 03:09:06.640
so when you have all of these

03:09:06.640 --> 03:09:08.640
when you have all of these

03:09:08.640 --> 03:09:10.640
attention scores

03:09:10.640 --> 03:09:12.640
essentially what the softmax is doing

03:09:12.640 --> 03:09:14.640
is it's going to

03:09:14.640 --> 03:09:16.640
exponentiate and normalize all of these

03:09:16.640 --> 03:09:18.640
so

03:09:18.640 --> 03:09:20.640
all of the attention scores that have scored

03:09:20.640 --> 03:09:22.640
high like maybe 50 to

03:09:22.640 --> 03:09:24.640
90% or whatever it is

03:09:24.640 --> 03:09:26.640
those are going to take a massive effect

03:09:26.640 --> 03:09:28.640
in that entire

03:09:28.640 --> 03:09:30.640
attention

03:09:30.640 --> 03:09:32.640
I guess tensor if you want to call it that

03:09:34.640 --> 03:09:36.640
and that's important

03:09:36.640 --> 03:09:38.640
it might not seem important

03:09:38.640 --> 03:09:40.640
but it's essentially just giving the model

03:09:40.640 --> 03:09:42.640
more confidence

03:09:42.640 --> 03:09:44.640
as to which tokens matter more

03:09:44.640 --> 03:09:46.640
so for example

03:09:46.640 --> 03:09:48.640
if we just

03:09:48.640 --> 03:09:50.640
did a normalization

03:09:50.640 --> 03:09:52.640
we would

03:09:52.640 --> 03:09:54.640
have words like server and crash

03:09:54.640 --> 03:09:56.640
and then server and check

03:09:56.640 --> 03:09:58.640
and then

03:09:58.640 --> 03:10:00.640
you would just know

03:10:00.640 --> 03:10:02.640
a decent amount about those

03:10:02.640 --> 03:10:04.640
those would pay attention to a decent amount

03:10:04.640 --> 03:10:06.640
because they multiply together quite well

03:10:06.640 --> 03:10:08.640
but if you softmax those

03:10:08.640 --> 03:10:10.640
then it's like

03:10:10.640 --> 03:10:12.640
those are almost the only characters that matter

03:10:12.640 --> 03:10:14.640
so it's looking at the context

03:10:14.640 --> 03:10:16.640
of those two

03:10:16.640 --> 03:10:18.640
and then we're sort of filling in

03:10:18.640 --> 03:10:20.640
like we're learning about the rest of the sentence

03:10:20.640 --> 03:10:22.640
based on just the

03:10:22.640 --> 03:10:24.640
sentiment of those attention scores

03:10:24.640 --> 03:10:26.640
because they're so high priority

03:10:26.640 --> 03:10:28.640
because they multiply together

03:10:28.640 --> 03:10:30.640
to such a high degree

03:10:30.640 --> 03:10:32.640
we want to emphasize them

03:10:32.640 --> 03:10:34.640
basically let the model learn more

03:10:34.640 --> 03:10:36.640
about which words matter more together

03:10:36.640 --> 03:10:38.640
so

03:10:38.640 --> 03:10:40.640
that's pretty much just what the softmax does

03:10:40.640 --> 03:10:42.640
it increases our confidence in

03:10:42.640 --> 03:10:44.640
attention

03:10:44.640 --> 03:10:46.640
and then a matrix multiply

03:10:46.640 --> 03:10:48.640
we go back to our V here

03:10:48.640 --> 03:10:50.640
and this is a value

03:10:50.640 --> 03:10:52.640
so essentially what this is

03:10:52.640 --> 03:10:54.640
is just a linear transformation

03:10:54.640 --> 03:10:56.640
and we apply this on our

03:10:56.640 --> 03:10:58.640
we apply this on our inputs

03:10:58.640 --> 03:11:00.640
and

03:11:00.640 --> 03:11:02.640
we have some value about

03:11:02.640 --> 03:11:04.640
you know

03:11:04.640 --> 03:11:06.640
what exactly those tokens are

03:11:06.640 --> 03:11:08.640
and after we've gotten all of our attention

03:11:08.640 --> 03:11:10.640
our softmax everything done

03:11:10.640 --> 03:11:12.640
it's just going to multiply

03:11:12.640 --> 03:11:14.640
the original values

03:11:14.640 --> 03:11:16.640
by everything we've gotten so far

03:11:16.640 --> 03:11:18.640
just so that you don't have any information

03:11:18.640 --> 03:11:20.640
that's really lost or we don't have anything scrambled

03:11:20.640 --> 03:11:22.640
just that we have like a general idea

03:11:22.640 --> 03:11:24.640
of okay these are actually

03:11:24.640 --> 03:11:26.640
all the tokens we have

03:11:26.640 --> 03:11:28.640
and then these are

03:11:28.640 --> 03:11:30.640
we found interesting the attention scores

03:11:32.640 --> 03:11:34.640
so

03:11:34.640 --> 03:11:36.640
we have an output which is a blend of input

03:11:36.640 --> 03:11:38.640
vector values and attention placed on each token

03:11:38.640 --> 03:11:40.640
and

03:11:40.640 --> 03:11:42.640
that's pretty much what's happening in scaled dot

03:11:42.640 --> 03:11:44.640
product attention in parallel

03:11:44.640 --> 03:11:46.640
so we have a bunch of these that are just happening

03:11:46.640 --> 03:11:48.640
at the same time

03:11:48.640 --> 03:11:50.640
many of these happening at the same time

03:11:50.640 --> 03:11:52.640
and yeah so

03:11:52.640 --> 03:11:54.640
that's what attention is

03:11:54.640 --> 03:11:56.640
that's what feedforward networks are

03:11:56.640 --> 03:11:58.640
residual connections are

03:12:00.640 --> 03:12:02.640
and yeah

03:12:02.640 --> 03:12:04.640
and then so after this after we've

03:12:04.640 --> 03:12:06.640
fed these into our decoders

03:12:06.640 --> 03:12:08.640
we get an output

03:12:08.640 --> 03:12:10.640
we apply linear transformation to summarize

03:12:10.640 --> 03:12:12.640
softmax probabilities

03:12:12.640 --> 03:12:14.640
and then we generate based on that

03:12:14.640 --> 03:12:16.640
based on everything that we learned

03:12:16.640 --> 03:12:18.640
and

03:12:18.640 --> 03:12:20.640
actually what I didn't quite write a lot about

03:12:20.640 --> 03:12:22.640
was the decoder

03:12:22.640 --> 03:12:24.640
so what I'm actually going to talk about next

03:12:24.640 --> 03:12:26.640
is something I didn't fill in yet

03:12:26.640 --> 03:12:28.640
which is why

03:12:28.640 --> 03:12:30.640
why the heck do we

03:12:30.640 --> 03:12:32.640
use mass attention here

03:12:32.640 --> 03:12:34.640
but not in these places so why the heck

03:12:34.640 --> 03:12:36.640
do we have a multi attention here

03:12:36.640 --> 03:12:38.640
all that attention here but mass attention here

03:12:38.640 --> 03:12:40.640
so why is this

03:12:40.640 --> 03:12:42.640
well the purpose of the encoder

03:12:42.640 --> 03:12:44.640
is to pretty much learn

03:12:44.640 --> 03:12:46.640
the present

03:12:46.640 --> 03:12:48.640
past and future

03:12:48.640 --> 03:12:50.640
and put that into a vector representation

03:12:50.640 --> 03:12:52.640
for the decoder

03:12:52.640 --> 03:12:54.640
that's what the encoder does

03:12:54.640 --> 03:12:56.640
so it's okay if we look into the future

03:12:56.640 --> 03:12:58.640
and understand tokens that way

03:12:58.640 --> 03:13:00.640
because we're technically not cheating

03:13:00.640 --> 03:13:02.640
we're just learning the different attention scores

03:13:02.640 --> 03:13:04.640
and yeah we're just using that

03:13:04.640 --> 03:13:06.640
to help us predict based on

03:13:06.640 --> 03:13:08.640
what the sentence looks like

03:13:08.640 --> 03:13:10.640
but not explicitly giving it away

03:13:10.640 --> 03:13:12.640
just giving it an idea of

03:13:12.640 --> 03:13:14.640
what to look for type of thing

03:13:14.640 --> 03:13:16.640
and then

03:13:16.640 --> 03:13:18.640
we use mass attention here because

03:13:18.640 --> 03:13:20.640
well we don't want to look ahead

03:13:20.640 --> 03:13:22.640
we want to look at the present and the past

03:13:22.640 --> 03:13:24.640
and

03:13:24.640 --> 03:13:26.640
later on

03:13:26.640 --> 03:13:28.640
we're not giving anything explicit

03:13:28.640 --> 03:13:30.640
here we're not giving anything yet

03:13:30.640 --> 03:13:32.640
so we want to make some raw guesses

03:13:32.640 --> 03:13:34.640
they're not going to be very good guesses at first

03:13:34.640 --> 03:13:36.640
we want to make some raw guesses

03:13:36.640 --> 03:13:38.640
and then later on

03:13:38.640 --> 03:13:40.640
we can feed these

03:13:40.640 --> 03:13:42.640
the added and normalized guesses

03:13:42.640 --> 03:13:44.640
into

03:13:44.640 --> 03:13:46.640
this next multi attention

03:13:46.640 --> 03:13:48.640
which isn't masked

03:13:48.640 --> 03:13:50.640
and then we can use

03:13:50.640 --> 03:13:52.640
this max multi head attention

03:13:52.640 --> 03:13:54.640
with the vector representation

03:13:54.640 --> 03:13:56.640
given by the encoder

03:13:56.640 --> 03:13:58.640
and then we can sort of do

03:13:58.640 --> 03:14:00.640
more useful things with that

03:14:00.640 --> 03:14:02.640
rather than just being forced to guess

03:14:02.640 --> 03:14:04.640
raw attention scores

03:14:04.640 --> 03:14:06.640
and then being judged for that

03:14:06.640 --> 03:14:08.640
we can sort of introduce more

03:14:08.640 --> 03:14:10.640
more and more elements

03:14:10.640 --> 03:14:12.640
in this decoder block to help us learn more meaningful things

03:14:12.640 --> 03:14:14.640
so

03:14:14.640 --> 03:14:16.640
we start off with

03:14:16.640 --> 03:14:18.640
making this

03:14:18.640 --> 03:14:20.640
mass multi head attention

03:14:20.640 --> 03:14:22.640
and then combining that

03:14:22.640 --> 03:14:24.640
with

03:14:24.640 --> 03:14:26.640
our

03:14:26.640 --> 03:14:28.640
then afterwards we do a multi head attention

03:14:28.640 --> 03:14:30.640
with the

03:14:30.640 --> 03:14:32.640
vector representation from the encoder

03:14:32.640 --> 03:14:34.640
and then we can make decisions on that

03:14:34.640 --> 03:14:36.640
so that's kind of why that works

03:14:36.640 --> 03:14:38.640
this way

03:14:38.640 --> 03:14:40.640
if you don't think I explain it like amazingly

03:14:40.640 --> 03:14:42.640
well you can totally just

03:14:42.640 --> 03:14:44.640
ask GPT4

03:14:44.640 --> 03:14:46.640
or GPT3.5

03:14:46.640 --> 03:14:48.640
and get a pretty decent answer

03:14:48.640 --> 03:14:50.640
but that's how that works

03:14:50.640 --> 03:14:52.640
and

03:14:52.640 --> 03:14:54.640
another thing I kind of wanted to point out here

03:14:54.640 --> 03:14:56.640
is these linear transformations

03:14:56.640 --> 03:14:58.640
that you see

03:14:58.640 --> 03:15:00.640
I mean there's a lot of them

03:15:00.640 --> 03:15:02.640
in the

03:15:02.640 --> 03:15:04.640
scaled dot project attention

03:15:04.640 --> 03:15:06.640
so you have your linears

03:15:06.640 --> 03:15:08.640
for your value or key value

03:15:08.640 --> 03:15:10.640
and key query and values

03:15:10.640 --> 03:15:12.640
so

03:15:12.640 --> 03:15:14.640
as well as the one up here

03:15:14.640 --> 03:15:16.640
linears are great

03:15:16.640 --> 03:15:18.640
for just expanding or shrinking

03:15:18.640 --> 03:15:20.640
a bunch of important info

03:15:20.640 --> 03:15:22.640
into something easier to work with

03:15:22.640 --> 03:15:24.640
so if you have a bunch of

03:15:24.640 --> 03:15:26.640
if you have a large vector containing a bunch

03:15:26.640 --> 03:15:28.640
of info learned from this

03:15:28.640 --> 03:15:30.640
scaled dot project attention

03:15:30.640 --> 03:15:32.640
you can

03:15:32.640 --> 03:15:34.640
you can sort of just compress

03:15:34.640 --> 03:15:36.640
that into something more manageable

03:15:36.640 --> 03:15:38.640
through a linear transformation

03:15:38.640 --> 03:15:40.640
and it's essentially what's just happening here

03:15:40.640 --> 03:15:42.640
with Softmax as well as

03:15:42.640 --> 03:15:44.640
in our

03:15:44.640 --> 03:15:46.640
scaled dot project attention here

03:15:46.640 --> 03:15:48.640
for these linear transformations

03:15:48.640 --> 03:15:50.640
from our inputs

03:15:50.640 --> 03:15:52.640
to

03:15:52.640 --> 03:15:54.640
quick keys, queries and values

03:15:54.640 --> 03:15:56.640
that's all that's happening

03:15:56.640 --> 03:15:58.640
if you want to read more about

03:15:58.640 --> 03:16:00.640
linear transformations the importance of them

03:16:00.640 --> 03:16:02.640
you can totally go out of your way to do that

03:16:02.640 --> 03:16:04.640
but that's just sort of a brief summary

03:16:04.640 --> 03:16:06.640
as to why they're important

03:16:06.640 --> 03:16:08.640
just shrinking or expanding

03:16:08.640 --> 03:16:10.640
so that's sort of a brief overview on how

03:16:10.640 --> 03:16:12.640
transformers work

03:16:12.640 --> 03:16:14.640
however in this

03:16:14.640 --> 03:16:16.640
course we will not be building the transformer

03:16:16.640 --> 03:16:18.640
architecture we'll be building

03:16:18.640 --> 03:16:20.640
something called a GPT which you're probably familiar

03:16:20.640 --> 03:16:22.640
with and GPT stands for

03:16:22.640 --> 03:16:24.640
Generatively Pre-Trained Transformer

03:16:24.640 --> 03:16:26.640
or Generative Pre-Trained Transformer

03:16:26.640 --> 03:16:28.640
one of the two

03:16:28.640 --> 03:16:30.640
and pretty much what this is

03:16:30.640 --> 03:16:32.640
it's pretty close to the transformer

03:16:32.640 --> 03:16:34.640
this architecture here except

03:16:34.640 --> 03:16:36.640
it only adopts

03:16:36.640 --> 03:16:38.640
the decoder blocks and it takes away

03:16:38.640 --> 03:16:40.640
this multi-head attention here

03:16:40.640 --> 03:16:42.640
so all we're doing is we're removing

03:16:42.640 --> 03:16:44.640
the encoder

03:16:44.640 --> 03:16:46.640
as well as what the encoder plugs into

03:16:46.640 --> 03:16:48.640
so all we have left

03:16:48.640 --> 03:16:50.640
is just some inputs

03:16:50.640 --> 03:16:52.640
our max multi-head

03:16:52.640 --> 03:16:54.640
attention

03:16:54.640 --> 03:16:56.640
our post-norm architecture

03:16:56.640 --> 03:16:58.640
and then

03:16:58.640 --> 03:17:00.640
right after this we're not going to

03:17:00.640 --> 03:17:02.640
a non-mass multi-head attention

03:17:02.640 --> 03:17:04.640
but rather to a feed forward network

03:17:04.640 --> 03:17:06.640
and then a post-norm

03:17:06.640 --> 03:17:08.640
so that's all it is, it's just 1, 2, 3, 4

03:17:08.640 --> 03:17:10.640
that's all it's going to look like

03:17:10.640 --> 03:17:12.640
that's all the blocks are going to be

03:17:12.640 --> 03:17:14.640
it is still important

03:17:14.640 --> 03:17:16.640
to understand the transformer architecture itself

03:17:16.640 --> 03:17:18.640
because you might need that in the future

03:17:18.640 --> 03:17:20.640
and it is sort of a good practice in language

03:17:20.640 --> 03:17:22.640
modeling to

03:17:22.640 --> 03:17:24.640
have a grasp on and to understand

03:17:24.640 --> 03:17:26.640
you know why we use mass multi-head

03:17:26.640 --> 03:17:28.640
attention in the decoder and why we don't

03:17:28.640 --> 03:17:30.640
use it in the encoder and stuff like that

03:17:30.640 --> 03:17:32.640
so anyways

03:17:32.640 --> 03:17:34.640
we're going to go ahead and build this

03:17:34.640 --> 03:17:36.640
if you need to

03:17:36.640 --> 03:17:38.640
look back if something wasn't quite clear

03:17:38.640 --> 03:17:40.640
definitely skip back a few seconds

03:17:40.640 --> 03:17:42.640
or a few minutes through the video and just

03:17:42.640 --> 03:17:44.640
make sure you clarify everything up to this point

03:17:44.640 --> 03:17:46.640
but yeah

03:17:46.640 --> 03:17:48.640
I'm going to go over some more

03:17:48.640 --> 03:17:50.640
math on the side here and just some other

03:17:50.640 --> 03:17:52.640
little

03:17:52.640 --> 03:17:54.640
little widgets we're going to need

03:17:54.640 --> 03:17:56.640
for building the decoder

03:17:56.640 --> 03:17:58.640
GPT architecture

03:17:58.640 --> 03:18:00.640
so let's go ahead and do that

03:18:00.640 --> 03:18:02.640
we're going to jump into

03:18:02.640 --> 03:18:04.640
building the transformer rather than

03:18:04.640 --> 03:18:06.640
building the GPT from scratch

03:18:06.640 --> 03:18:08.640
what I want to do is linger on

03:18:08.640 --> 03:18:10.640
self-attention for a little bit

03:18:10.640 --> 03:18:12.640
or rather just the attention mechanism

03:18:12.640 --> 03:18:14.640
and the matrix multiplication behind it

03:18:14.640 --> 03:18:16.640
and why it works

03:18:16.640 --> 03:18:18.640
so I'm going to use

03:18:18.640 --> 03:18:20.640
whiteboard to illustrate this

03:18:20.640 --> 03:18:22.640
so we're going to go ahead and draw out

03:18:22.640 --> 03:18:24.640
a

03:18:24.640 --> 03:18:26.640
we'll just use maybe a four token

03:18:26.640 --> 03:18:28.640
sequence here of words

03:18:28.640 --> 03:18:30.640
okay

03:18:30.640 --> 03:18:32.640
so

03:18:32.640 --> 03:18:34.640
we're going to highlight which words

03:18:34.640 --> 03:18:36.640
are probably going to end up

03:18:36.640 --> 03:18:38.640
correlating together

03:18:38.640 --> 03:18:40.640
or the attention mechanism

03:18:40.640 --> 03:18:42.640
is going to multiply them together

03:18:42.640 --> 03:18:44.640
to a high amount based on what it learns

03:18:44.640 --> 03:18:46.640
about those tokens this is what this is

03:18:46.640 --> 03:18:48.640
so I'm going to help us illustrate that

03:18:48.640 --> 03:18:50.640
and what the

03:18:50.640 --> 03:18:52.640
GPT is going to see

03:18:52.640 --> 03:18:54.640
sort of from the inside what it looks like from the inside

03:18:54.640 --> 03:18:56.640
so

03:18:56.640 --> 03:18:58.640
I'm going to go ahead and draw this out here

03:19:02.640 --> 03:19:04.640
just make a table here

03:19:04.640 --> 03:19:06.640
we'll give it

03:19:10.640 --> 03:19:12.640
four of these

03:19:14.640 --> 03:19:16.640
and draw a little line through the middle

03:19:16.640 --> 03:19:18.640
my drawing might not be

03:19:18.640 --> 03:19:20.640
perfect but it's definitely better

03:19:20.640 --> 03:19:22.640
than on paper

03:19:22.640 --> 03:19:24.640
so cool we have this

03:19:24.640 --> 03:19:26.640
we have

03:19:26.640 --> 03:19:28.640
my

03:19:30.640 --> 03:19:32.640
I'm going to go here

03:19:32.640 --> 03:19:34.640
dog

03:19:38.640 --> 03:19:40.640
has

03:19:40.640 --> 03:19:42.640
please

03:19:42.640 --> 03:19:44.640
and then my

03:19:46.640 --> 03:19:48.640
my dog

03:19:50.640 --> 03:19:52.640
so I delete that

03:19:54.640 --> 03:19:56.640
my dog has

03:19:56.640 --> 03:19:58.640
please

03:19:58.640 --> 03:20:00.640
cool

03:20:00.640 --> 03:20:02.640
so to what degree

03:20:02.640 --> 03:20:04.640
are these going to interact well my and my

03:20:04.640 --> 03:20:06.640
I mean it doesn't really

03:20:06.640 --> 03:20:08.640
give away that much it's only just the start

03:20:08.640 --> 03:20:10.640
so maybe this will interact to

03:20:10.640 --> 03:20:12.640
a low amount

03:20:12.640 --> 03:20:14.640
and then you have my and dog

03:20:14.640 --> 03:20:16.640
these might interact to a medium

03:20:16.640 --> 03:20:18.640
amount because it's like your dog

03:20:18.640 --> 03:20:20.640
so we might go

03:20:20.640 --> 03:20:22.640
we might go medium

03:20:22.640 --> 03:20:24.640
like that

03:20:24.640 --> 03:20:26.640
and then my and has well that doesn't give away too much

03:20:26.640 --> 03:20:28.640
so maybe that'll be low

03:20:28.640 --> 03:20:30.640
and then my and please it's like oh

03:20:30.640 --> 03:20:32.640
that doesn't really mean much my please that doesn't

03:20:32.640 --> 03:20:34.640
really make sense maybe we'll

03:20:34.640 --> 03:20:36.640
have it interact to a low amount

03:20:36.640 --> 03:20:38.640
and then

03:20:38.640 --> 03:20:40.640
these would be the same

03:20:40.640 --> 03:20:42.640
thing so

03:20:42.640 --> 03:20:44.640
my and dog so be medium

03:20:44.640 --> 03:20:46.640
and then has and has

03:20:46.640 --> 03:20:48.640
would be low

03:20:48.640 --> 03:20:50.640
and then my and please would also be low

03:20:50.640 --> 03:20:52.640
and then you have dog and dog

03:20:52.640 --> 03:20:54.640
so these might interact to a low amount they're the same word

03:20:54.640 --> 03:20:56.640
so we'll just

03:20:56.640 --> 03:20:58.640
forget about that and then we have

03:20:58.640 --> 03:21:00.640
a dog has

03:21:00.640 --> 03:21:02.640
so these might interact to a medium amount

03:21:02.640 --> 03:21:04.640
dog has the dog has

03:21:04.640 --> 03:21:06.640
something

03:21:06.640 --> 03:21:08.640
and then dog and please

03:21:08.640 --> 03:21:10.640
these might interact to a high amount

03:21:10.640 --> 03:21:12.640
because they're associating the dog

03:21:12.640 --> 03:21:14.640
with something else meaning please

03:21:14.640 --> 03:21:16.640
we have has

03:21:16.640 --> 03:21:18.640
and dog these would interact to the same amount so

03:21:18.640 --> 03:21:20.640
medium and then has and has

03:21:20.640 --> 03:21:22.640
be

03:21:22.640 --> 03:21:24.640
probably

03:21:24.640 --> 03:21:26.640
to a low amount

03:21:26.640 --> 03:21:28.640
and then

03:21:28.640 --> 03:21:30.640
we could do low for

03:21:30.640 --> 03:21:32.640
we could do what was it high

03:21:32.640 --> 03:21:34.640
for this one as well please and dog

03:21:34.640 --> 03:21:36.640
so these will interact

03:21:36.640 --> 03:21:38.640
to a high amount

03:21:38.640 --> 03:21:40.640
and then we have has and please

03:21:40.640 --> 03:21:42.640
so

03:21:42.640 --> 03:21:44.640
these could interact maybe a medium

03:21:44.640 --> 03:21:46.640
amount

03:21:46.640 --> 03:21:48.640
medium and then please and please which would be low

03:21:48.640 --> 03:21:50.640
so what you get

03:21:50.640 --> 03:21:52.640
I'll just highlight this in

03:21:52.640 --> 03:21:54.640
I'll just highlight this in green here

03:21:54.640 --> 03:21:56.640
so you get

03:21:56.640 --> 03:21:58.640
all the medium

03:21:58.640 --> 03:22:00.640
and high attention scores

03:22:00.640 --> 03:22:02.640
you'd have your medium here

03:22:02.640 --> 03:22:04.640
medium here

03:22:04.640 --> 03:22:06.640
high medium

03:22:06.640 --> 03:22:08.640
medium high

03:22:08.640 --> 03:22:10.640
medium and medium

03:22:10.640 --> 03:22:12.640
so you can see that these are sort of symmetrical

03:22:12.640 --> 03:22:14.640
and this is what the attention map

03:22:14.640 --> 03:22:16.640
will look like of course there's going to be some

03:22:16.640 --> 03:22:18.640
scaling going on here based on the amount

03:22:18.640 --> 03:22:20.640
of actual attention's

03:22:20.640 --> 03:22:22.640
heads we have running in parallel

03:22:22.640 --> 03:22:24.640
but that's besides the point

03:22:24.640 --> 03:22:26.640
really what's going on here

03:22:26.640 --> 03:22:28.640
is the network

03:22:28.640 --> 03:22:30.640
is going to learn how to place

03:22:30.640 --> 03:22:32.640
the right

03:22:32.640 --> 03:22:34.640
attention scores because attention is simply

03:22:34.640 --> 03:22:36.640
being used to generate tokens

03:22:36.640 --> 03:22:38.640
that's that's how the

03:22:38.640 --> 03:22:40.640
that's how the GPT works it's using attention

03:22:40.640 --> 03:22:42.640
to generate tokens

03:22:42.640 --> 03:22:44.640
so we can make

03:22:44.640 --> 03:22:46.640
those sort of attention

03:22:46.640 --> 03:22:48.640
scores how they're placed

03:22:48.640 --> 03:22:50.640
we can make those learnable

03:22:50.640 --> 03:22:52.640
through all of the like embeddings

03:22:52.640 --> 03:22:54.640
like everything we have in the entire

03:22:54.640 --> 03:22:56.640
network can make sure

03:22:56.640 --> 03:22:58.640
that we place effective attention scores

03:22:58.640 --> 03:23:00.640
and to make sure that they're measured properly

03:23:00.640 --> 03:23:02.640
so

03:23:02.640 --> 03:23:04.640
obviously I didn't quantify these very well

03:23:04.640 --> 03:23:06.640
like not with floating point numbers

03:23:06.640 --> 03:23:08.640
but this is sort of the premise

03:23:08.640 --> 03:23:10.640
of how it works and how we want

03:23:10.640 --> 03:23:12.640
the model to look at different tokens

03:23:12.640 --> 03:23:14.640
and how they relate to one another

03:23:14.640 --> 03:23:16.640
so that's what the

03:23:16.640 --> 03:23:18.640
attention mechanism looks like under the hood

03:23:18.640 --> 03:23:20.640
so this is what the actual

03:23:20.640 --> 03:23:22.640
GPT or decoder only

03:23:22.640 --> 03:23:24.640
transformer architecture looks like

03:23:24.640 --> 03:23:26.640
and

03:23:26.640 --> 03:23:28.640
so I'm just going to go through this step by step here

03:23:28.640 --> 03:23:30.640
and then we can hopefully jump into some of the math

03:23:30.640 --> 03:23:32.640
and code behind how this works

03:23:32.640 --> 03:23:34.640
so we have

03:23:34.640 --> 03:23:36.640
our inputs embeddings and positional

03:23:36.640 --> 03:23:38.640
encodings we have only decoder

03:23:38.640 --> 03:23:40.640
blocks and then some

03:23:40.640 --> 03:23:42.640
linear transformation

03:23:42.640 --> 03:23:44.640
and then pretty much just

03:23:44.640 --> 03:23:46.640
we do some softmax

03:23:46.640 --> 03:23:48.640
probability distribution

03:23:48.640 --> 03:23:50.640
we sample from those and then we

03:23:50.640 --> 03:23:52.640
start just generating some output

03:23:52.640 --> 03:23:54.640
and then we compare those to our inputs

03:23:54.640 --> 03:23:56.640
and see how off they were, optimized from that

03:23:56.640 --> 03:23:58.640
in each of these

03:23:58.640 --> 03:24:00.640
decoder blocks we have our all data

03:24:00.640 --> 03:24:02.640
attention, res connections

03:24:02.640 --> 03:24:04.640
feedforward network consists

03:24:04.640 --> 03:24:06.640
of a linear, real linear

03:24:06.640 --> 03:24:08.640
border and then

03:24:08.640 --> 03:24:10.640
another res connection

03:24:10.640 --> 03:24:12.640
in each of these multi-attentions

03:24:12.640 --> 03:24:14.640
we have

03:24:14.640 --> 03:24:16.640
multiple heads running in parallel

03:24:16.640 --> 03:24:18.640
and each of these heads is going to take a

03:24:18.640 --> 03:24:20.640
key, query and value

03:24:20.640 --> 03:24:22.640
these are all learnable

03:24:22.640 --> 03:24:24.640
linear transformations

03:24:24.640 --> 03:24:26.640
and

03:24:26.640 --> 03:24:28.640
we're going to basically dot product the key and query together

03:24:28.640 --> 03:24:30.640
concatenate these results

03:24:30.640 --> 03:24:32.640
and

03:24:32.640 --> 03:24:34.640
do a little transformation to sort of

03:24:34.640 --> 03:24:36.640
summarize it afterwards

03:24:36.640 --> 03:24:38.640
and then what actually goes on in the

03:24:38.640 --> 03:24:40.640
dot product attention is just the dot

03:24:40.640 --> 03:24:42.640
product meaning of the key and query

03:24:42.640 --> 03:24:44.640
the scaling to prevent

03:24:44.640 --> 03:24:46.640
these values from exploding

03:24:46.640 --> 03:24:48.640
to prevent the vanishing gradient problem

03:24:48.640 --> 03:24:50.640
and then we have our

03:24:50.640 --> 03:24:52.640
masking to make sure that

03:24:52.640 --> 03:24:54.640
these, to make sure the model

03:24:54.640 --> 03:24:56.640
isn't looking ahead and cheating

03:24:56.640 --> 03:24:58.640
and then softmax matrix multiply

03:24:58.640 --> 03:25:00.640
we output that and then

03:25:00.640 --> 03:25:02.640
kind of fill in the blank there, so cool

03:25:02.640 --> 03:25:04.640
this is a little bit

03:25:04.640 --> 03:25:06.640
pretty much the

03:25:06.640 --> 03:25:08.640
transform architecture a little bit dumb

03:25:08.640 --> 03:25:10.640
down a little smaller

03:25:10.640 --> 03:25:12.640
in complexity to actually understand but

03:25:12.640 --> 03:25:14.640
that's kind of the premise of what's going on here

03:25:14.640 --> 03:25:16.640
so still

03:25:16.640 --> 03:25:18.640
implements a self-attention mechanism

03:25:20.640 --> 03:25:22.640
so as you can see now

03:25:22.640 --> 03:25:24.640
I am currently

03:25:24.640 --> 03:25:26.640
on my macbook

03:25:26.640 --> 03:25:28.640
M2 chip, I'm not going to

03:25:28.640 --> 03:25:30.640
go into the specs of why it's important

03:25:30.640 --> 03:25:32.640
but really quick, I'm just going to show you

03:25:32.640 --> 03:25:34.640
how I SSH onto my other PC

03:25:34.640 --> 03:25:36.640
so I go

03:25:36.640 --> 03:25:38.640
SSH

03:25:38.640 --> 03:25:40.640
just like that and then I type in my

03:25:40.640 --> 03:25:42.640
ipv4 address

03:25:42.640 --> 03:25:44.640
and then

03:25:44.640 --> 03:25:46.640
I just

03:25:46.640 --> 03:25:48.640
get a simple password

03:25:48.640 --> 03:25:50.640
here, password that I've never had

03:25:50.640 --> 03:25:52.640
is cool

03:25:52.640 --> 03:25:54.640
so now I'm on my desktop computer

03:25:54.640 --> 03:25:56.640
and this is the command prompt that I use for it

03:25:56.640 --> 03:25:58.640
so awesome

03:25:58.640 --> 03:26:00.640
I'm going to go ahead and go into the

03:26:00.640 --> 03:26:02.640
free code camp

03:26:02.640 --> 03:26:04.640
little directory I have

03:26:04.640 --> 03:26:06.640
so cd desktop

03:26:06.640 --> 03:26:08.640
cd python testing

03:26:08.640 --> 03:26:10.640
and then here I'm actually going to activate

03:26:10.640 --> 03:26:12.640
my CUDA virtual

03:26:12.640 --> 03:26:14.640
environment

03:26:14.640 --> 03:26:16.640
oops, not accelerate

03:26:16.640 --> 03:26:18.640
I'm going to go CUDA

03:26:18.640 --> 03:26:20.640
activate

03:26:20.640 --> 03:26:22.640
cool and then I'm going to go

03:26:22.640 --> 03:26:24.640
cd into free code camp

03:26:24.640 --> 03:26:26.640
gbt course, awesome

03:26:26.640 --> 03:26:28.640
so now, if I actually do

03:26:28.640 --> 03:26:30.640
code on here like this to open up my

03:26:30.640 --> 03:26:32.640
VS code, it doesn't do that

03:26:32.640 --> 03:26:34.640
so there's another little way I have to do this

03:26:34.640 --> 03:26:36.640
and you have to go into

03:26:36.640 --> 03:26:38.640
VS code

03:26:38.640 --> 03:26:40.640
go into a little remote explorer here

03:26:40.640 --> 03:26:42.640
and then you can simply connect

03:26:42.640 --> 03:26:44.640
so I'm just going to connect

03:26:44.640 --> 03:26:46.640
to the current window

03:26:46.640 --> 03:26:48.640
itself

03:26:48.640 --> 03:26:50.640
there's an extension you need for this

03:26:50.640 --> 03:26:52.640
called open SSH server, I think it's what it's called

03:26:52.640 --> 03:26:54.640
and

03:26:54.640 --> 03:26:56.640
it's simply the same password I used in the command prompt

03:26:56.640 --> 03:26:58.640
I can type it correctly

03:27:04.640 --> 03:27:06.640
awesome

03:27:06.640 --> 03:27:08.640
so now it's SSH into my computer

03:27:08.640 --> 03:27:10.640
upstairs

03:27:10.640 --> 03:27:12.640
and I'm just going to open the little editor in here

03:27:14.640 --> 03:27:16.640
nice, so you can see

03:27:16.640 --> 03:27:18.640
that it looks just like that, that's wonderful

03:27:18.640 --> 03:27:20.640
so now

03:27:20.640 --> 03:27:22.640
I'm going to open this in a Jupyter notebook

03:27:24.640 --> 03:27:26.640
actually

03:27:26.640 --> 03:27:28.640
cd into desktop here

03:27:28.640 --> 03:27:30.640
cd python

03:27:30.640 --> 03:27:32.640
cd python testing

03:27:32.640 --> 03:27:34.640
CUDA scripts

03:27:34.640 --> 03:27:36.640
activate

03:27:36.640 --> 03:27:38.640
cd free code camp

03:27:38.640 --> 03:27:40.640
gbt course and then code

03:27:40.640 --> 03:27:42.640
like that and it will open

03:27:42.640 --> 03:27:44.640
perfect

03:27:44.640 --> 03:27:46.640
how wonderful is that and I've already done

03:27:46.640 --> 03:27:48.640
a little bit of this here but

03:27:48.640 --> 03:27:50.640
we're going to

03:27:50.640 --> 03:27:52.640
jump into exactly

03:27:52.640 --> 03:27:54.640
how we can build up this transformer

03:27:54.640 --> 03:27:56.640
or gbt architecture

03:27:56.640 --> 03:27:58.640
in the code itself

03:27:58.640 --> 03:28:00.640
so I'm going to

03:28:00.640 --> 03:28:02.640
pop over to my Jupyter notebook in here

03:28:06.640 --> 03:28:08.640
cool and now this little address

03:28:08.640 --> 03:28:10.640
I'm going to paste that

03:28:10.640 --> 03:28:12.640
into my

03:28:12.640 --> 03:28:14.640
browser

03:28:14.640 --> 03:28:16.640
awesome

03:28:16.640 --> 03:28:18.640
so we have this gbt v1

03:28:18.640 --> 03:28:20.640
Jupyter notebook

03:28:22.640 --> 03:28:24.640
so what I've actually done is

03:28:24.640 --> 03:28:26.640
I've done some importations here

03:28:26.640 --> 03:28:28.640
so I've

03:28:28.640 --> 03:28:30.640
imported all of these

03:28:30.640 --> 03:28:32.640
python importations

03:28:32.640 --> 03:28:34.640
all the hyper parameters that we used from before

03:28:36.640 --> 03:28:38.640
I've imported the data loader

03:28:38.640 --> 03:28:40.640
I've imported the tokenizer

03:28:40.640 --> 03:28:42.640
the train and bell splits

03:28:42.640 --> 03:28:44.640
they get batch function

03:28:44.640 --> 03:28:46.640
estimate loss, just everything

03:28:46.640 --> 03:28:48.640
that we're going to need and it's all in

03:28:48.640 --> 03:28:50.640
neatly organized little code blocks

03:28:50.640 --> 03:28:52.640
so awesome

03:28:52.640 --> 03:28:54.640
now what?

03:28:54.640 --> 03:28:56.640
well let's go ahead and continue here

03:28:56.640 --> 03:28:58.640
with the actual

03:28:58.640 --> 03:29:00.640
upgrading

03:29:00.640 --> 03:29:02.640
from the very

03:29:02.640 --> 03:29:04.640
top level so I remember

03:29:04.640 --> 03:29:06.640
I actually showed

03:29:06.640 --> 03:29:08.640
and you can skip back to this

03:29:08.640 --> 03:29:10.640
I actually showed

03:29:10.640 --> 03:29:12.640
the architecture of the gbt

03:29:12.640 --> 03:29:14.640
sort of

03:29:14.640 --> 03:29:16.640
lined out in I guess a little sketch

03:29:16.640 --> 03:29:18.640
a little sketch that I did

03:29:18.640 --> 03:29:20.640
and all we're going to do

03:29:20.640 --> 03:29:22.640
is pretty much build up from the high level

03:29:22.640 --> 03:29:24.640
the high high level general

03:29:24.640 --> 03:29:26.640
architecture down to the technical stuff

03:29:26.640 --> 03:29:28.640
down to the very root

03:29:28.640 --> 03:29:30.640
dot product attention

03:29:30.640 --> 03:29:32.640
that we're going to be doing here

03:29:32.640 --> 03:29:34.640
so I'm going to go ahead and start off

03:29:34.640 --> 03:29:36.640
with this

03:29:36.640 --> 03:29:38.640
gbt language model which I just

03:29:38.640 --> 03:29:40.640
renamed I replaced

03:29:40.640 --> 03:29:42.640
bygram

03:29:42.640 --> 03:29:44.640
with gbt here

03:29:44.640 --> 03:29:46.640
so that's all we're doing and

03:29:46.640 --> 03:29:48.640
we're going to add some

03:29:48.640 --> 03:29:50.640
little code bits and

03:29:50.640 --> 03:29:52.640
just walk through step by step

03:29:52.640 --> 03:29:54.640
what we're doing so

03:29:54.640 --> 03:29:56.640
let's do that so great

03:29:56.640 --> 03:29:58.640
we're going to next we're going to talk about

03:29:58.640 --> 03:30:00.640
these positional encodings

03:30:00.640 --> 03:30:02.640
so I go back to the paper here

03:30:02.640 --> 03:30:04.640
rather this architecture

03:30:04.640 --> 03:30:06.640
we initially have our tokenize inputs

03:30:06.640 --> 03:30:08.640
and then we give

03:30:08.640 --> 03:30:10.640
we give them embedding

03:30:10.640 --> 03:30:12.640
so token embeddings and then a positional

03:30:12.640 --> 03:30:14.640
encoding so this positional

03:30:14.640 --> 03:30:16.640
encoding going back to the attention paper is right here

03:30:16.640 --> 03:30:18.640
so all it does

03:30:18.640 --> 03:30:20.640
is every

03:30:20.640 --> 03:30:22.640
even token index

03:30:22.640 --> 03:30:24.640
we apply this function

03:30:24.640 --> 03:30:26.640
and then every odd token index

03:30:26.640 --> 03:30:28.640
we apply this function you don't really need to know

03:30:28.640 --> 03:30:30.640
what it's doing other than

03:30:30.640 --> 03:30:32.640
the fact that these are the different sine

03:30:32.640 --> 03:30:34.640
and cosine functions that it uses

03:30:34.640 --> 03:30:36.640
to apply positional encodings

03:30:36.640 --> 03:30:38.640
to the tokenized inputs

03:30:38.640 --> 03:30:40.640
so every

03:30:40.640 --> 03:30:42.640
so on our first

03:30:42.640 --> 03:30:44.640
index or whatever let's say we have hello world

03:30:44.640 --> 03:30:46.640
okay there's five characters here

03:30:46.640 --> 03:30:48.640
h will be index zero

03:30:48.640 --> 03:30:50.640
so it'll get an even

03:30:50.640 --> 03:30:52.640
encoding function

03:30:52.640 --> 03:30:54.640
and then e will be odd

03:30:54.640 --> 03:30:56.640
since it's index one so it'll get this one

03:30:56.640 --> 03:30:58.640
and then l will get this the next l will get

03:30:58.640 --> 03:31:00.640
this and then

03:31:00.640 --> 03:31:02.640
or I don't know if I messed up that

03:31:02.640 --> 03:31:04.640
order but essentially it just iterates

03:31:04.640 --> 03:31:06.640
and it goes back and forth between

03:31:06.640 --> 03:31:08.640
those applying these fixed functions

03:31:08.640 --> 03:31:10.640
and the thing is with fixed functions

03:31:10.640 --> 03:31:12.640
is that they don't actually

03:31:12.640 --> 03:31:14.640
learn about the data at all

03:31:14.640 --> 03:31:16.640
because they're fixed so another way we could

03:31:16.640 --> 03:31:18.640
do this would be using

03:31:18.640 --> 03:31:20.640
nn.embedding which is what we use

03:31:20.640 --> 03:31:22.640
for the token

03:31:22.640 --> 03:31:24.640
embedding so I'm going to go ahead

03:31:24.640 --> 03:31:26.640
and implement this here in our

03:31:26.640 --> 03:31:28.640
gbtv one script so I'm going to go

03:31:28.640 --> 03:31:30.640
ahead and add on this line

03:31:30.640 --> 03:31:32.640
self dot positional

03:31:32.640 --> 03:31:34.640
self dot position embedding table

03:31:34.640 --> 03:31:36.640
nn.embedding block size

03:31:36.640 --> 03:31:38.640
so the block size is the length

03:31:38.640 --> 03:31:40.640
or the sequence length

03:31:40.640 --> 03:31:42.640
which in our case

03:31:42.640 --> 03:31:44.640
it's going to be 8 so there's going to be 8 tokens

03:31:44.640 --> 03:31:46.640
and

03:31:46.640 --> 03:31:48.640
this means

03:31:48.640 --> 03:31:50.640
we're going to have 8 different indices

03:31:50.640 --> 03:31:52.640
and each one is going to be

03:31:52.640 --> 03:31:54.640
of size nn.embed

03:31:54.640 --> 03:31:56.640
and this is a new parameter I actually want to add here

03:31:56.640 --> 03:31:58.640
so

03:31:58.640 --> 03:32:00.640
nn.embed will not only be used

03:32:00.640 --> 03:32:02.640
in positional embedding

03:32:02.640 --> 03:32:04.640
but it will also be used in our

03:32:04.640 --> 03:32:06.640
token embedding because when we actually

03:32:06.640 --> 03:32:08.640
store

03:32:08.640 --> 03:32:10.640
information about the tokens

03:32:10.640 --> 03:32:12.640
we want that to be in a very large

03:32:12.640 --> 03:32:14.640
vector so not necessarily

03:32:14.640 --> 03:32:16.640
a probability distribution

03:32:16.640 --> 03:32:18.640
or what we were using before in the

03:32:18.640 --> 03:32:20.640
bi-gram language model but rather

03:32:20.640 --> 03:32:22.640
a really large vector

03:32:22.640 --> 03:32:24.640
or a list you could think about it

03:32:24.640 --> 03:32:26.640
as a bunch of different

03:32:26.640 --> 03:32:28.640
attributes that

03:32:28.640 --> 03:32:30.640
are about a character so maybe

03:32:30.640 --> 03:32:32.640
you know

03:32:32.640 --> 03:32:34.640
A and E would be pretty close

03:32:34.640 --> 03:32:36.640
but both vowels versus like

03:32:36.640 --> 03:32:38.640
E and Z

03:32:38.640 --> 03:32:40.640
would be very different because Z is not

03:32:40.640 --> 03:32:42.640
a very common letter and E is the most common letter

03:32:42.640 --> 03:32:44.640
in the alphabet so

03:32:44.640 --> 03:32:46.640
we pretty much just want to have

03:32:46.640 --> 03:32:48.640
vectors to differentiate

03:32:48.640 --> 03:32:50.640
these tokens to place some

03:32:50.640 --> 03:32:52.640
semantic meaning on them

03:32:52.640 --> 03:32:54.640
and anyways

03:32:54.640 --> 03:32:56.640
that's a little talk about what token embedding table

03:32:56.640 --> 03:32:58.640
is going to do when we add n.embed

03:32:58.640 --> 03:33:00.640
and then positional embedding table

03:33:00.640 --> 03:33:02.640
is just the same thing

03:33:02.640 --> 03:33:04.640
but instead of each character

03:33:04.640 --> 03:33:06.640
having its own thing

03:33:06.640 --> 03:33:08.640
each letter

03:33:08.640 --> 03:33:10.640
index in the input is going to have its own embedding

03:33:10.640 --> 03:33:12.640
so I can go and add this

03:33:12.640 --> 03:33:14.640
up here

03:33:14.640 --> 03:33:16.640
the n.embed

03:33:16.640 --> 03:33:18.640
and we can just make this

03:33:18.640 --> 03:33:20.640
maybe 384

03:33:20.640 --> 03:33:22.640
so 384 is quite huge

03:33:22.640 --> 03:33:24.640
and it's maybe a little too big

03:33:24.640 --> 03:33:26.640
for your PC but we'll see in a second

03:33:26.640 --> 03:33:28.640
so

03:33:28.640 --> 03:33:30.640
what this is going to do is it's going to have a giant vector

03:33:30.640 --> 03:33:32.640
it's going to be like

03:33:32.640 --> 03:33:34.640
we could say like

03:33:34.640 --> 03:33:36.640
embedding

03:33:36.640 --> 03:33:38.640
embedding vector

03:33:38.640 --> 03:33:40.640
and then it would be like this

03:33:40.640 --> 03:33:42.640
and you would have

03:33:42.640 --> 03:33:44.640
a bunch of different attributes so like 0.1

03:33:44.640 --> 03:33:46.640
0.2

03:33:46.640 --> 03:33:48.640
0.8

03:33:48.640 --> 03:33:50.640
1.1

03:33:50.640 --> 03:33:52.640
right? except

03:33:52.640 --> 03:33:54.640
instead of 4 this is

03:33:54.640 --> 03:33:56.640
384 elements long

03:33:56.640 --> 03:33:58.640
and each of these

03:33:58.640 --> 03:34:00.640
is just going to store a tiny little attribute

03:34:00.640 --> 03:34:02.640
about that token

03:34:02.640 --> 03:34:04.640
so

03:34:04.640 --> 03:34:06.640
let's say we maybe had like a

03:34:06.640 --> 03:34:08.640
two dimensional and we were using a word

03:34:08.640 --> 03:34:10.640
so if we had

03:34:10.640 --> 03:34:12.640
sad versus

03:34:12.640 --> 03:34:14.640
happy

03:34:14.640 --> 03:34:16.640
sad might be

03:34:16.640 --> 03:34:18.640
sad might be

03:34:18.640 --> 03:34:20.640
0.1

03:34:20.640 --> 03:34:22.640
and then

03:34:22.640 --> 03:34:24.640
0.8

03:34:24.640 --> 03:34:26.640
or 0.8

03:34:26.640 --> 03:34:28.640
whereas happy

03:34:28.640 --> 03:34:30.640
sad would be

03:34:30.640 --> 03:34:32.640
maybe the positivity

03:34:32.640 --> 03:34:34.640
of what it's saying and then 0.8 would be

03:34:34.640 --> 03:34:36.640
is it showing some sort of emotion

03:34:36.640 --> 03:34:38.640
which is a lot right?

03:34:38.640 --> 03:34:40.640
it's 80% emotion

03:34:40.640 --> 03:34:42.640
and 0.1

03:34:42.640 --> 03:34:44.640
of maybe positive sentiment

03:34:44.640 --> 03:34:46.640
and then if we had

03:34:46.640 --> 03:34:48.640
0.9

03:34:48.640 --> 03:34:50.640
would be happy because it's happy

03:34:50.640 --> 03:34:52.640
it's very good and then 0.8

03:34:52.640 --> 03:34:54.640
is emotional because they're sort of the same

03:34:54.640 --> 03:34:56.640
emotional level

03:34:56.640 --> 03:34:58.640
but yeah so this is what our embedding vectors

03:34:58.640 --> 03:35:00.640
are pretty much describing and

03:35:00.640 --> 03:35:02.640
all this hyperparameter

03:35:02.640 --> 03:35:04.640
is concerned with is how long

03:35:04.640 --> 03:35:06.640
that vector actually is

03:35:06.640 --> 03:35:08.640
so anyways

03:35:08.640 --> 03:35:10.640
let's continue with the GPT

03:35:10.640 --> 03:35:12.640
language model class so the next bit I like

03:35:12.640 --> 03:35:14.640
to talk about is how many decoder

03:35:14.640 --> 03:35:16.640
layers we have

03:35:16.640 --> 03:35:18.640
so in here let's just say we have

03:35:18.640 --> 03:35:20.640
four decoder layers

03:35:20.640 --> 03:35:22.640
so we have four of these it's going to go through this one

03:35:22.640 --> 03:35:24.640
and then this one and then this one

03:35:24.640 --> 03:35:26.640
then this one this is all happening

03:35:26.640 --> 03:35:28.640
sequentially so we could

03:35:28.640 --> 03:35:30.640
actually make a little

03:35:30.640 --> 03:35:32.640
sequential neural network with

03:35:32.640 --> 03:35:34.640
four decoder layers

03:35:34.640 --> 03:35:36.640
so I'm actually going to add this in

03:35:36.640 --> 03:35:38.640
and then a little bit of extra code which I'll explain

03:35:38.640 --> 03:35:40.640
in a second here so this

03:35:40.640 --> 03:35:42.640
self

03:35:42.640 --> 03:35:44.640
dot blocks is how many

03:35:44.640 --> 03:35:46.640
decoder blocks we have running

03:35:46.640 --> 03:35:48.640
sequentially or layers

03:35:48.640 --> 03:35:50.640
blocks and layers can be used interchangeably in this

03:35:50.640 --> 03:35:52.640
context

03:35:52.640 --> 03:35:54.640
but yeah we have an end dot sequential

03:35:54.640 --> 03:35:56.640
and this asterisk is pretty much saying

03:35:56.640 --> 03:35:58.640
we're going to repeat

03:35:58.640 --> 03:36:00.640
this right here

03:36:00.640 --> 03:36:02.640
for how many

03:36:02.640 --> 03:36:04.640
end layer is and end layer is another hyperparameter

03:36:04.640 --> 03:36:06.640
we're going to add

03:36:06.640 --> 03:36:08.640
we go end underscore layer

03:36:08.640 --> 03:36:10.640
we go equals four

03:36:12.640 --> 03:36:14.640
so end underscore layer equals four

03:36:14.640 --> 03:36:16.640
that means it's going to make four of these

03:36:16.640 --> 03:36:18.640
I guess blocks

03:36:18.640 --> 03:36:20.640
or layers sequentially

03:36:20.640 --> 03:36:22.640
it's going to make four of them

03:36:22.640 --> 03:36:24.640
and this little block thing

03:36:24.640 --> 03:36:26.640
we're going to build on top of this in a second here

03:36:26.640 --> 03:36:28.640
we're going to make an actual block

03:36:28.640 --> 03:36:30.640
class and I'm going to explain what that does

03:36:30.640 --> 03:36:32.640
but for now

03:36:32.640 --> 03:36:34.640
this is going to be some temporary code

03:36:34.640 --> 03:36:36.640
as long as you understand that this is what

03:36:36.640 --> 03:36:38.640
this is how we create our four layers

03:36:38.640 --> 03:36:40.640
our four decoder layers

03:36:40.640 --> 03:36:42.640
that's all you need to know for now

03:36:42.640 --> 03:36:44.640
I'm going to move more into this block later

03:36:44.640 --> 03:36:46.640
as for this

03:36:46.640 --> 03:36:48.640
self dot layer norm final

03:36:48.640 --> 03:36:50.640
this is the final layer norm

03:36:50.640 --> 03:36:52.640
all this is going to do

03:36:52.640 --> 03:36:54.640
is we're just simply going to add this

03:36:54.640 --> 03:36:56.640
to the end of our network here

03:37:00.640 --> 03:37:02.640
just simply at the end here

03:37:02.640 --> 03:37:04.640
and all this is going to do

03:37:04.640 --> 03:37:06.640
is just going to help the model converge better

03:37:06.640 --> 03:37:08.640
layer norms are super useful

03:37:08.640 --> 03:37:10.640
and yeah

03:37:10.640 --> 03:37:12.640
so you'll see more how that works

03:37:12.640 --> 03:37:14.640
I'll actually remove it later on

03:37:14.640 --> 03:37:16.640
and we'll actually

03:37:16.640 --> 03:37:18.640
compare and see

03:37:18.640 --> 03:37:20.640
how good it actually does

03:37:20.640 --> 03:37:22.640
and you can totally go out of your way

03:37:22.640 --> 03:37:24.640
to experiment

03:37:24.640 --> 03:37:26.640
with different normalizations

03:37:26.640 --> 03:37:28.640
and see how well the layer norm

03:37:28.640 --> 03:37:30.640
helps the model perform

03:37:30.640 --> 03:37:32.640
or how well the loss

03:37:32.640 --> 03:37:34.640
sort of converges over time

03:37:34.640 --> 03:37:36.640
when you put the layer norm in different places

03:37:36.640 --> 03:37:38.640
so

03:37:38.640 --> 03:37:40.640
let's go back here

03:37:40.640 --> 03:37:42.640
and now we have this

03:37:42.640 --> 03:37:44.640
end here

03:37:44.640 --> 03:37:46.640
which is the language

03:37:46.640 --> 03:37:48.640
I believe this is the language modeling

03:37:48.640 --> 03:37:50.640
head or something

03:37:50.640 --> 03:37:52.640
again this is what Andrey Karpathy used

03:37:52.640 --> 03:37:54.640
I'm assuming that means language modeling head

03:37:54.640 --> 03:37:56.640
but pretty much

03:37:56.640 --> 03:37:58.640
all we're doing is we're just

03:37:58.640 --> 03:38:00.640
projecting

03:38:00.640 --> 03:38:02.640
we're doing this final

03:38:02.640 --> 03:38:04.640
transformation here

03:38:04.640 --> 03:38:06.640
this final little linear layer here

03:38:06.640 --> 03:38:08.640
from all of these sequential

03:38:08.640 --> 03:38:10.640
decoder outputs

03:38:10.640 --> 03:38:12.640
and we're just going to transform that

03:38:12.640 --> 03:38:14.640
to

03:38:14.640 --> 03:38:16.640
something that the softmax can work with

03:38:16.640 --> 03:38:18.640
so we have our layer norm afterwards

03:38:18.640 --> 03:38:20.640
to sort of normalize help the model converge

03:38:20.640 --> 03:38:22.640
after all these

03:38:22.640 --> 03:38:24.640
after all this computation

03:38:24.640 --> 03:38:26.640
we're going to feed that into a linear layer

03:38:26.640 --> 03:38:28.640
to make it I guess

03:38:28.640 --> 03:38:30.640
softmax

03:38:30.640 --> 03:38:32.640
workable so the softmax can work with it

03:38:32.640 --> 03:38:34.640
and

03:38:34.640 --> 03:38:36.640
yeah so we're just

03:38:36.640 --> 03:38:38.640
simply projecting it from

03:38:38.640 --> 03:38:40.640
an embed which is the vector length that we get

03:38:40.640 --> 03:38:42.640
from our decoder

03:38:42.640 --> 03:38:44.640
and

03:38:46.640 --> 03:38:48.640
and this vocab size

03:38:48.640 --> 03:38:50.640
so the vocab size is going to

03:38:50.640 --> 03:38:52.640
essentially give up a little

03:38:52.640 --> 03:38:54.640
probability distribution on each token that we have

03:38:54.640 --> 03:38:56.640
or the vocabulary

03:38:56.640 --> 03:38:58.640
so anyways

03:38:58.640 --> 03:39:00.640
I'm going to make this back to normal

03:39:00.640 --> 03:39:02.640
here and we're going to just

03:39:02.640 --> 03:39:04.640
apply this

03:39:04.640 --> 03:39:06.640
to the forward pass

03:39:06.640 --> 03:39:08.640
so a little thing I wanted to add on

03:39:08.640 --> 03:39:10.640
to

03:39:10.640 --> 03:39:12.640
this positional embedding

03:39:12.640 --> 03:39:14.640
or rather just the idea of

03:39:14.640 --> 03:39:16.640
embeddings versus

03:39:16.640 --> 03:39:18.640
the fixed definite function

03:39:18.640 --> 03:39:20.640
of the

03:39:20.640 --> 03:39:22.640
sinusoidal functions

03:39:22.640 --> 03:39:24.640
and the cosine functions that we used here

03:39:24.640 --> 03:39:26.640
these are both actually

03:39:26.640 --> 03:39:28.640
used in practice

03:39:28.640 --> 03:39:30.640
the reason I said we're going to use embeddings

03:39:30.640 --> 03:39:32.640
is because we just want it to be more oriented

03:39:32.640 --> 03:39:34.640
around our data

03:39:34.640 --> 03:39:36.640
however in practice

03:39:36.640 --> 03:39:38.640
sinusoidal encodings are used

03:39:38.640 --> 03:39:40.640
in base transformer models

03:39:40.640 --> 03:39:42.640
whereas learned embeddings what we're using

03:39:42.640 --> 03:39:44.640
are used in variants like

03:39:44.640 --> 03:39:46.640
GBT and we are building a

03:39:46.640 --> 03:39:48.640
GBT so we're probably

03:39:48.640 --> 03:39:50.640
going to find out a performance from learning about embeddings

03:39:50.640 --> 03:39:52.640
and this is just

03:39:52.640 --> 03:39:54.640
summing up the experts do

03:39:54.640 --> 03:39:56.640
it's a little practice that experts do

03:39:56.640 --> 03:39:58.640
when they're building transformer models

03:39:58.640 --> 03:40:00.640
versus variants like GBTs

03:40:00.640 --> 03:40:02.640
so that's just a little background on

03:40:02.640 --> 03:40:04.640
why we're using

03:40:04.640 --> 03:40:06.640
learnable embeddings

03:40:06.640 --> 03:40:08.640
so now let's continue

03:40:08.640 --> 03:40:10.640
with the forward pass here

03:40:10.640 --> 03:40:12.640
so I'm going to paste in some more code

03:40:12.640 --> 03:40:14.640
and

03:40:14.640 --> 03:40:16.640
let me just make sure this is

03:40:16.640 --> 03:40:18.640
formatted properly cool

03:40:20.640 --> 03:40:22.640
so we have this

03:40:22.640 --> 03:40:24.640
token embedding which is our token embedding

03:40:24.640 --> 03:40:26.640
table

03:40:26.640 --> 03:40:28.640
we take an IDX

03:40:28.640 --> 03:40:30.640
token embedding here

03:40:30.640 --> 03:40:32.640
then what we do with this positional embedding table

03:40:32.640 --> 03:40:34.640
so we have this torch.arrange

03:40:34.640 --> 03:40:36.640
we make sure this is on the CUDA device

03:40:36.640 --> 03:40:38.640
the GPU device

03:40:38.640 --> 03:40:40.640
so it's in parallel

03:40:40.640 --> 03:40:42.640
and all this is going to do

03:40:42.640 --> 03:40:44.640
is it's going to look at how long is T

03:40:44.640 --> 03:40:46.640
and

03:40:46.640 --> 03:40:48.640
let's say T is our block size

03:40:48.640 --> 03:40:50.640
so T is going to be 8

03:40:50.640 --> 03:40:52.640
so all it's going to do is give us 8 indices

03:40:52.640 --> 03:40:54.640
it's going to be like 0, 1, 2, 3,

03:40:54.640 --> 03:40:56.640
4, 5, 6, 7

03:40:56.640 --> 03:40:58.640
8 of those

03:40:58.640 --> 03:41:00.640
and we're essentially just going to give each of those

03:41:00.640 --> 03:41:02.640
each of those indices

03:41:02.640 --> 03:41:04.640
a different

03:41:06.640 --> 03:41:08.640
a different

03:41:08.640 --> 03:41:10.640
end embedding vector

03:41:10.640 --> 03:41:12.640
for each of those indices

03:41:12.640 --> 03:41:14.640
just a little lookup table

03:41:14.640 --> 03:41:16.640
and that's what that is

03:41:16.640 --> 03:41:18.640
so all we do now

03:41:18.640 --> 03:41:20.640
is it's actually quite simple

03:41:20.640 --> 03:41:22.640
and this is a very efficient way to do it

03:41:22.640 --> 03:41:24.640
is you just add these two together

03:41:24.640 --> 03:41:26.640
broadcasting rules

03:41:26.640 --> 03:41:28.640
which you might want to look into

03:41:28.640 --> 03:41:30.640
I'll actually search that up right now

03:41:30.640 --> 03:41:32.640
torch

03:41:32.640 --> 03:41:34.640
broadcasting semantics

03:41:36.640 --> 03:41:38.640
pie torch

03:41:38.640 --> 03:41:40.640
broadcasting

03:41:40.640 --> 03:41:42.640
I cannot spell

03:41:42.640 --> 03:41:44.640
broadcasting semantics

03:41:44.640 --> 03:41:46.640
so

03:41:46.640 --> 03:41:48.640
these are a little bit funky

03:41:48.640 --> 03:41:50.640
when you look at them the first time

03:41:50.640 --> 03:41:52.640
but pretty much these are just rules

03:41:52.640 --> 03:41:54.640
about how you can do

03:41:54.640 --> 03:41:56.640
arithmetic operations

03:41:56.640 --> 03:41:58.640
and just operations in general

03:41:58.640 --> 03:42:00.640
to tensors

03:42:00.640 --> 03:42:02.640
so tensors are like you think of matrices

03:42:02.640 --> 03:42:04.640
where it's like a 2x2

03:42:04.640 --> 03:42:06.640
tensors can be the same thing

03:42:06.640 --> 03:42:08.640
but they could be like a 2x2x2

03:42:08.640 --> 03:42:10.640
or a 2x2x2x2x2

03:42:10.640 --> 03:42:12.640
whatever dimension you want to have

03:42:12.640 --> 03:42:14.640
there

03:42:14.640 --> 03:42:16.640
and pretty much it's just rules about how you can

03:42:18.640 --> 03:42:20.640
have two of those

03:42:20.640 --> 03:42:22.640
weirdly

03:42:22.640 --> 03:42:24.640
shaped tensors and do things

03:42:24.640 --> 03:42:26.640
to them

03:42:26.640 --> 03:42:28.640
so just some rules here

03:42:28.640 --> 03:42:30.640
I would advise you familiarize yourself with these

03:42:30.640 --> 03:42:32.640
even play around with it if you want

03:42:32.640 --> 03:42:34.640
just for a few minutes

03:42:34.640 --> 03:42:36.640
and just get an idea for

03:42:36.640 --> 03:42:38.640
which, like just try to multiply

03:42:38.640 --> 03:42:40.640
tensors together

03:42:40.640 --> 03:42:42.640
and see which ones throw errors and which ones don't

03:42:42.640 --> 03:42:44.640
so it's a good idea to understand how broadcasting

03:42:44.640 --> 03:42:46.640
rules work

03:42:46.640 --> 03:42:48.640
obviously this term

03:42:48.640 --> 03:42:50.640
is a little fancy and it's like

03:42:50.640 --> 03:42:52.640
that's like a crazy advanced term

03:42:52.640 --> 03:42:54.640
not really

03:42:54.640 --> 03:42:56.640
it's pretty much just

03:42:56.640 --> 03:42:58.640
some rules about how you're

03:42:58.640 --> 03:43:00.640
multiplying these really weirdly shaped tensors

03:43:00.640 --> 03:43:02.640
so yeah

03:43:02.640 --> 03:43:04.640
anyways

03:43:04.640 --> 03:43:06.640
if we go back to here

03:43:08.640 --> 03:43:10.640
we are allowed to broadcast these

03:43:10.640 --> 03:43:12.640
we're allowed to actually add them together

03:43:12.640 --> 03:43:14.640
so the positional embedding and the token embedding

03:43:14.640 --> 03:43:16.640
we get X from this

03:43:16.640 --> 03:43:18.640
B by T by C shape

03:43:18.640 --> 03:43:20.640
so now

03:43:20.640 --> 03:43:22.640
what we can do

03:43:22.640 --> 03:43:24.640
with these is we can actually feed it

03:43:24.640 --> 03:43:26.640
into the

03:43:26.640 --> 03:43:28.640
GPT or I guess

03:43:28.640 --> 03:43:30.640
sort of a transformer network if you want to say that

03:43:30.640 --> 03:43:32.640
so we have these embeddings

03:43:32.640 --> 03:43:34.640
and positional encodings

03:43:34.640 --> 03:43:36.640
we add these together and then we feed them

03:43:36.640 --> 03:43:38.640
into our sequential network

03:43:38.640 --> 03:43:40.640
so how are we doing this

03:43:40.640 --> 03:43:42.640
well we go self dot blocks which is up here

03:43:42.640 --> 03:43:44.640
and we essentially just feed

03:43:44.640 --> 03:43:46.640
an X which is literally

03:43:46.640 --> 03:43:48.640
exactly what happens here

03:43:48.640 --> 03:43:50.640
we have our tokenized inputs

03:43:50.640 --> 03:43:52.640
we got our embeddings and our positional encodings

03:43:52.640 --> 03:43:54.640
through learnable embeddings we add them together

03:43:54.640 --> 03:43:56.640
and then we feed them into the network directly

03:43:56.640 --> 03:43:58.640
so

03:43:58.640 --> 03:44:00.640
that's all that's happening here

03:44:00.640 --> 03:44:02.640
and that's how we're feeding an X

03:44:02.640 --> 03:44:04.640
which is the output of these

03:44:04.640 --> 03:44:06.640
then after

03:44:06.640 --> 03:44:08.640
this is like way after

03:44:08.640 --> 03:44:10.640
we've gotten through all of these

03:44:10.640 --> 03:44:12.640
GPT layers or blocks

03:44:12.640 --> 03:44:14.640
we do this final layer norm

03:44:14.640 --> 03:44:16.640
and then this linear transformation

03:44:16.640 --> 03:44:18.640
to get it to a

03:44:18.640 --> 03:44:20.640
softmax

03:44:20.640 --> 03:44:22.640
to get it to essentially probabilities

03:44:22.640 --> 03:44:24.640
that we can feed into our softmax function

03:44:24.640 --> 03:44:26.640
and then other than that

03:44:26.640 --> 03:44:28.640
this forward pass is exactly the same

03:44:28.640 --> 03:44:30.640
other than this little block of code here

03:44:30.640 --> 03:44:32.640
so if this makes sense so far

03:44:32.640 --> 03:44:34.640
that is absolutely amazing

03:44:34.640 --> 03:44:36.640
let's continue I'm actually going to add

03:44:36.640 --> 03:44:38.640
a little bit of

03:44:38.640 --> 03:44:40.640
in practice

03:44:40.640 --> 03:44:42.640
some little

03:44:42.640 --> 03:44:44.640
weight initializations

03:44:44.640 --> 03:44:46.640
that we should be using

03:44:46.640 --> 03:44:48.640
in our language model

03:44:48.640 --> 03:44:50.640
and in module subclass

03:44:50.640 --> 03:44:52.640
so

03:44:52.640 --> 03:44:54.640
I'm going to go over a little bit of math here

03:44:54.640 --> 03:44:56.640
but this is just really important for practice

03:44:56.640 --> 03:44:58.640
and to make sure that your model

03:44:58.640 --> 03:45:00.640
does not fail in the training process

03:45:00.640 --> 03:45:02.640
this is very important

03:45:02.640 --> 03:45:04.640
it's going to be a little funky

03:45:04.640 --> 03:45:06.640
on the conceptualizing

03:45:06.640 --> 03:45:08.640
but bring out some pen and paper

03:45:08.640 --> 03:45:10.640
and do some math with me

03:45:10.640 --> 03:45:12.640
we've built up some of these

03:45:12.640 --> 03:45:14.640
initial GPT language model architecture

03:45:14.640 --> 03:45:16.640
and before we continue building

03:45:16.640 --> 03:45:18.640
more of it and the other functions

03:45:18.640 --> 03:45:20.640
some of the math stuff that's going on

03:45:20.640 --> 03:45:22.640
the parallelization that's going on in the script

03:45:22.640 --> 03:45:24.640
I want to show you some of the math

03:45:24.640 --> 03:45:26.640
that we're going to use to initialize the weights

03:45:26.640 --> 03:45:28.640
of the model to help it train

03:45:28.640 --> 03:45:30.640
and converge better

03:45:30.640 --> 03:45:32.640
so there's this new thing

03:45:32.640 --> 03:45:34.640
that I want to introduce called standard deviation

03:45:34.640 --> 03:45:36.640
and this is used in intermediate level mathematics

03:45:36.640 --> 03:45:38.640
the symbol essentially looks like this

03:45:38.640 --> 03:45:40.640
population standard deviation

03:45:40.640 --> 03:45:42.640
so

03:45:42.640 --> 03:45:44.640
n

03:45:44.640 --> 03:45:46.640
the size

03:45:46.640 --> 03:45:48.640
so it's just going to be an array

03:45:48.640 --> 03:45:50.640
the length of the array

03:45:50.640 --> 03:45:52.640
and then xi

03:45:52.640 --> 03:45:54.640
we iterate over each value

03:45:54.640 --> 03:45:56.640
so xf position 0

03:45:56.640 --> 03:45:58.640
xf position 1

03:45:58.640 --> 03:46:00.640
xf position 2

03:46:00.640 --> 03:46:02.640
and then this u here is the mean

03:46:02.640 --> 03:46:04.640
so

03:46:04.640 --> 03:46:06.640
we iterate over each element

03:46:06.640 --> 03:46:08.640
we're going to

03:46:08.640 --> 03:46:10.640
subtract it by the mean

03:46:10.640 --> 03:46:12.640
we're going to square that and then keep adding

03:46:12.640 --> 03:46:14.640
all these squared results together

03:46:14.640 --> 03:46:16.640
and then once we get the sum of that

03:46:16.640 --> 03:46:18.640
we're going to

03:46:18.640 --> 03:46:20.640
subtract or we're going to divide

03:46:20.640 --> 03:46:22.640
this by the number of elements there are

03:46:22.640 --> 03:46:24.640
and then once we get this result

03:46:24.640 --> 03:46:26.640
we're going to square root that

03:46:26.640 --> 03:46:28.640
so this symbol here

03:46:28.640 --> 03:46:30.640
might also look a little bit unfamiliar

03:46:30.640 --> 03:46:32.640
and

03:46:32.640 --> 03:46:34.640
I'll illustrate this out for you

03:46:34.640 --> 03:46:36.640
so we go to our whiteboard

03:46:36.640 --> 03:46:38.640
and this e

03:46:38.640 --> 03:46:40.640
looks like

03:46:40.640 --> 03:46:42.640
looks like that

03:46:42.640 --> 03:46:44.640
let's just say we were to put in

03:46:44.640 --> 03:46:46.640
x

03:46:46.640 --> 03:46:48.640
i like that

03:46:48.640 --> 03:46:50.640
and our array

03:46:50.640 --> 03:46:52.640
let's just say for instance

03:46:52.640 --> 03:46:54.640
our array

03:46:54.640 --> 03:46:56.640
is 0.1

03:46:56.640 --> 03:46:58.640
0.2, 0.3

03:46:58.640 --> 03:47:00.640
so what would the result of this be

03:47:00.640 --> 03:47:02.640
well if we look at each element

03:47:02.640 --> 03:47:04.640
iteratively add them together

03:47:04.640 --> 03:47:06.640
so 0.1

03:47:06.640 --> 03:47:08.640
plus 0.2 plus 0.3

03:47:08.640 --> 03:47:10.640
well we get 0.6 from that

03:47:10.640 --> 03:47:12.640
so this would essentially

03:47:12.640 --> 03:47:14.640
be equal to

03:47:14.640 --> 03:47:16.640
0.6

03:47:16.640 --> 03:47:18.640
that's what that equals

03:47:18.640 --> 03:47:20.640
we just add each of these up together

03:47:20.640 --> 03:47:22.640
or we do whatever this is iteratively

03:47:22.640 --> 03:47:24.640
whatever this element is

03:47:24.640 --> 03:47:26.640
we iterate over

03:47:26.640 --> 03:47:28.640
the number of elements we have in

03:47:28.640 --> 03:47:30.640
the arbitrary array

03:47:30.640 --> 03:47:32.640
or

03:47:32.640 --> 03:47:34.640
vector or list or whatever you want to call it

03:47:34.640 --> 03:47:36.640
and then we just

03:47:36.640 --> 03:47:38.640
sort of look at what's going on here

03:47:38.640 --> 03:47:40.640
and we can do some basic arithmetic stuff

03:47:40.640 --> 03:47:42.640
so

03:47:42.640 --> 03:47:44.640
let's walk through a few examples

03:47:44.640 --> 03:47:46.640
just to illustrate to you

03:47:46.640 --> 03:47:48.640
what the results look like

03:47:48.640 --> 03:47:50.640
based on the inputs here

03:47:50.640 --> 03:47:52.640
so I'm going to go back to my whiteboard

03:47:52.640 --> 03:47:54.640
we're going to draw a little line here

03:47:54.640 --> 03:47:56.640
just to separate this

03:47:56.640 --> 03:47:58.640
so

03:47:58.640 --> 03:48:00.640
I want to calculate the standard deviation

03:48:00.640 --> 03:48:02.640
do standard deviation

03:48:02.640 --> 03:48:04.640
of

03:48:06.640 --> 03:48:08.640
and then we'll just make some random array

03:48:08.640 --> 03:48:10.640
negative

03:48:10.640 --> 03:48:12.640
0.38

03:48:12.640 --> 03:48:14.640
negative 0.38

03:48:14.640 --> 03:48:16.640
0.52

03:48:18.640 --> 03:48:20.640
and then 2.48

03:48:20.640 --> 03:48:22.640
cool

03:48:22.640 --> 03:48:24.640
so we have this array this is three elements

03:48:24.640 --> 03:48:26.640
so that means n

03:48:26.640 --> 03:48:28.640
is going to be equal to three

03:48:28.640 --> 03:48:30.640
let me drag this over here

03:48:30.640 --> 03:48:32.640
so n is the number of elements

03:48:32.640 --> 03:48:34.640
so n is going to be equal to three

03:48:34.640 --> 03:48:36.640
our mean

03:48:36.640 --> 03:48:38.640
well

03:48:38.640 --> 03:48:40.640
our mean is just

03:48:40.640 --> 03:48:42.640
we add all these up together and then we average them

03:48:42.640 --> 03:48:44.640
so our mean

03:48:44.640 --> 03:48:46.640
is going to be equal to

03:48:46.640 --> 03:48:48.640
let's just say

03:48:48.640 --> 03:48:50.640
0.38

03:48:50.640 --> 03:48:52.640
plus 0.52

03:48:52.640 --> 03:48:54.640
plus

03:48:54.640 --> 03:48:56.640
2.48

03:48:56.640 --> 03:48:58.640
and then divided by three

03:48:58.640 --> 03:49:00.640
and the answer to this

03:49:00.640 --> 03:49:02.640
I did the math ahead of time

03:49:02.640 --> 03:49:04.640
is literally 0.873

03:49:04.640 --> 03:49:06.640
repeated but we're just going to put 0.87

03:49:06.640 --> 03:49:08.640
for simplicity's sake

03:49:08.640 --> 03:49:10.640
cool so the mean of this

03:49:10.640 --> 03:49:12.640
is 0.87 and n is equal to three

03:49:12.640 --> 03:49:14.640
now we can start doing

03:49:14.640 --> 03:49:16.640
some of the other math

03:49:16.640 --> 03:49:18.640
so

03:49:18.640 --> 03:49:20.640
we have this

03:49:20.640 --> 03:49:22.640
O has a cool line

03:49:24.640 --> 03:49:26.640
and we do

03:49:26.640 --> 03:49:28.640
square root

03:49:28.640 --> 03:49:30.640
one over

03:49:30.640 --> 03:49:32.640
n which is equal to three

03:49:32.640 --> 03:49:34.640
and then we

03:49:34.640 --> 03:49:36.640
multiply this

03:49:36.640 --> 03:49:38.640
by sigma

03:49:38.640 --> 03:49:40.640
that's what this symbol is

03:49:40.640 --> 03:49:42.640
that's sigma that's the name for it

03:49:42.640 --> 03:49:44.640
and then we go

03:49:44.640 --> 03:49:46.640
X

03:49:46.640 --> 03:49:48.640
I

03:49:48.640 --> 03:49:50.640
minus

03:49:50.640 --> 03:49:52.640
and then our mean of

03:49:52.640 --> 03:49:54.640
0.87

03:49:58.640 --> 03:50:00.640
apologies for the sloppy writing

03:50:02.640 --> 03:50:04.640
and then we square that

03:50:04.640 --> 03:50:06.640
so let me drag this out

03:50:06.640 --> 03:50:08.640
awesome

03:50:08.640 --> 03:50:10.640
so let's just do this

03:50:10.640 --> 03:50:12.640
step by step here

03:50:12.640 --> 03:50:14.640
so the first one is going to be

03:50:14.640 --> 03:50:16.640
0.38

03:50:16.640 --> 03:50:18.640
0.

03:50:18.640 --> 03:50:20.640
negative

03:50:20.640 --> 03:50:22.640
0.38

03:50:22.640 --> 03:50:24.640
and we're going to do minus the mean here

03:50:24.640 --> 03:50:26.640
so minus 0.87

03:50:26.640 --> 03:50:28.640
and I'm just going to wrap all this

03:50:28.640 --> 03:50:30.640
in brackets so that we don't miss anything

03:50:30.640 --> 03:50:32.640
wrap it in brackets

03:50:32.640 --> 03:50:34.640
and then just square it and see what we get after

03:50:34.640 --> 03:50:36.640
so I'm just going to write all these out

03:50:36.640 --> 03:50:38.640
then we can do the calculations

03:50:38.640 --> 03:50:40.640
so next up we have 0.52

03:50:40.640 --> 03:50:42.640
minus 0.87

03:50:42.640 --> 03:50:44.640
we'll square that

03:50:46.640 --> 03:50:48.640
and then next up we have

03:50:48.640 --> 03:50:50.640
2.48

03:50:50.640 --> 03:50:52.640
minus 0.87

03:50:52.640 --> 03:50:54.640
and then we square that as well

03:50:54.640 --> 03:50:56.640
so awesome

03:50:56.640 --> 03:50:58.640
what is the result of this

03:50:58.640 --> 03:51:00.640
the result of

03:51:00.640 --> 03:51:02.640
negative 0.38 minus

03:51:02.640 --> 03:51:04.640
0.87

03:51:04.640 --> 03:51:06.640
squared is

03:51:06.640 --> 03:51:08.640
1.57

03:51:08.640 --> 03:51:10.640
the result of

03:51:10.640 --> 03:51:12.640
this line

03:51:12.640 --> 03:51:14.640
is 0.12

03:51:14.640 --> 03:51:16.640
again these are all approximations

03:51:16.640 --> 03:51:18.640
they're not super spot on

03:51:18.640 --> 03:51:20.640
we're just doing this to understand

03:51:20.640 --> 03:51:22.640
what's going on here

03:51:22.640 --> 03:51:24.640
just to overview the function not for precision

03:51:24.640 --> 03:51:26.640
then the next one is going to be

03:51:26.640 --> 03:51:28.640
2.59

03:51:28.640 --> 03:51:30.640
and you can double check all these

03:51:30.640 --> 03:51:32.640
calculations if you'd like

03:51:32.640 --> 03:51:34.640
I have done these preemptively so

03:51:34.640 --> 03:51:36.640
that is that

03:51:36.640 --> 03:51:38.640
and now from here

03:51:38.640 --> 03:51:40.640
what we have to do is add each of these together

03:51:40.640 --> 03:51:42.640
so

03:51:42.640 --> 03:51:44.640
1.57

03:51:44.640 --> 03:51:46.640
plus 0.12

03:51:46.640 --> 03:51:48.640
plus 2.59

03:51:48.640 --> 03:51:50.640
divided by 3

03:51:50.640 --> 03:51:52.640
is

03:51:56.640 --> 03:51:58.640
1.57

03:51:58.640 --> 03:52:00.640
plus 0.12

03:52:00.640 --> 03:52:02.640
plus 2.59

03:52:02.640 --> 03:52:04.640
all that divided by 3

03:52:04.640 --> 03:52:06.640
is going to be equal to 1.42

03:52:06.640 --> 03:52:08.640
keep in mind we also have to

03:52:08.640 --> 03:52:10.640
square root this

03:52:10.640 --> 03:52:12.640
so the square root of that

03:52:12.640 --> 03:52:14.640
is going to be

03:52:14.640 --> 03:52:16.640
1.19

03:52:16.640 --> 03:52:18.640
approximately

03:52:18.640 --> 03:52:20.640
we'll just add

03:52:20.640 --> 03:52:22.640
this guy ahead of it

03:52:22.640 --> 03:52:24.640
so that's what the

03:52:24.640 --> 03:52:26.640
standard deviation of

03:52:26.640 --> 03:52:28.640
this array is

03:52:28.640 --> 03:52:30.640
negative 0.38

03:52:30.640 --> 03:52:32.640
0.52, 2.48

03:52:32.640 --> 03:52:34.640
standard deviation is

03:52:34.640 --> 03:52:36.640
1.19

03:52:36.640 --> 03:52:38.640
let's do another example

03:52:42.640 --> 03:52:44.640
so let's say

03:52:44.640 --> 03:52:46.640
we want to do the standard deviation

03:52:46.640 --> 03:52:48.640
of

03:52:48.640 --> 03:52:50.640
0.48

03:52:52.640 --> 03:52:54.640
0.5

03:52:56.640 --> 03:52:58.640
0.50

03:52:58.640 --> 03:53:00.640
I guess 0.52

03:53:04.640 --> 03:53:06.640
so there's a little pattern here

03:53:06.640 --> 03:53:08.640
just goes up by 0.02 each time

03:53:08.640 --> 03:53:10.640
and

03:53:10.640 --> 03:53:12.640
you're going to see why this is

03:53:12.640 --> 03:53:14.640
vastly different than the other example

03:53:14.640 --> 03:53:16.640
so let's walk through this

03:53:16.640 --> 03:53:18.640
so first of all we have N

03:53:20.640 --> 03:53:22.640
N is equal to 3

03:53:22.640 --> 03:53:24.640
cool

03:53:24.640 --> 03:53:26.640
what does our mean

03:53:26.640 --> 03:53:28.640
our mean

03:53:28.640 --> 03:53:30.640
well if you do our mean our mean is 0.5

03:53:30.640 --> 03:53:32.640
0.48 plus this

03:53:32.640 --> 03:53:34.640
plus that

03:53:34.640 --> 03:53:36.640
that's going to be 0.5

03:53:36.640 --> 03:53:38.640
and

03:53:38.640 --> 03:53:40.640
if you're good with numbers

03:53:40.640 --> 03:53:42.640
you'll probably already be able to do this in your head

03:53:42.640 --> 03:53:44.640
but that's okay if not

03:53:44.640 --> 03:53:46.640
next up

03:53:46.640 --> 03:53:48.640
we're going to do this in the formula

03:53:48.640 --> 03:53:50.640
so

03:53:50.640 --> 03:53:52.640
what do these iterations look like

03:53:52.640 --> 03:53:54.640
so

03:53:54.640 --> 03:53:56.640
0.

03:53:56.640 --> 03:53:58.640
let's just do these in brackets

03:53:58.640 --> 03:54:00.640
the old way

03:54:00.640 --> 03:54:02.640
0.5

03:54:02.640 --> 03:54:04.640
squared

03:54:04.640 --> 03:54:06.640
the next one is

03:54:06.640 --> 03:54:08.640
0.5

03:54:08.640 --> 03:54:10.640
minus 0.5

03:54:10.640 --> 03:54:12.640
squared which we already know is 0

03:54:14.640 --> 03:54:16.640
and this one is 0.52

03:54:16.640 --> 03:54:18.640
minus

03:54:18.640 --> 03:54:20.640
0.5

03:54:20.640 --> 03:54:22.640
squared so the result of 0.48

03:54:22.640 --> 03:54:24.640
minus 0.5 squared

03:54:24.640 --> 03:54:26.640
and what's right equals here

03:54:26.640 --> 03:54:28.640
is going to be

03:54:28.640 --> 03:54:30.640
approximately 0.02

03:54:30.640 --> 03:54:32.640
squared

03:54:32.640 --> 03:54:34.640
so that would be 0.004

03:54:34.640 --> 03:54:36.640
like that

03:54:36.640 --> 03:54:38.640
so I'll make this not actually overlap

03:54:38.640 --> 03:54:40.640
0.004

03:54:40.640 --> 03:54:42.640
and then this one

03:54:42.640 --> 03:54:44.640
we obviously know would be 0

03:54:44.640 --> 03:54:46.640
because 0.5 minus 0.5

03:54:46.640 --> 03:54:48.640
that's 0 then you square 0

03:54:48.640 --> 03:54:50.640
still the same thing

03:54:50.640 --> 03:54:52.640
and then this one is

03:54:52.640 --> 03:54:54.640
0.0004 as well

03:54:54.640 --> 03:54:56.640
so

03:54:58.640 --> 03:55:00.640
when we add these two together

03:55:00.640 --> 03:55:02.640
we're going to get

03:55:02.640 --> 03:55:04.640
0.0008

03:55:04.640 --> 03:55:06.640
just like that

03:55:06.640 --> 03:55:08.640
and then if we divide them by 3 or whatever

03:55:08.640 --> 03:55:10.640
n is

03:55:10.640 --> 03:55:12.640
then we end up getting

03:55:12.640 --> 03:55:14.640
0.00026

03:55:14.640 --> 03:55:16.640
repeating so I'll just write

03:55:16.640 --> 03:55:18.640
266 like that

03:55:18.640 --> 03:55:20.640
and so

03:55:20.640 --> 03:55:22.640
all we have to do at this point

03:55:22.640 --> 03:55:24.640
is do the

03:55:24.640 --> 03:55:26.640
square root of this

03:55:26.640 --> 03:55:28.640
and

03:55:28.640 --> 03:55:30.640
we'll do

03:55:30.640 --> 03:55:32.640
square root of 0.00026

03:55:32.640 --> 03:55:34.640
approximately

03:55:34.640 --> 03:55:36.640
and

03:55:36.640 --> 03:55:38.640
that's going to be equal to about

03:55:38.640 --> 03:55:40.640
0.0163

03:55:40.640 --> 03:55:42.640
so

03:55:42.640 --> 03:55:44.640
that is our

03:55:44.640 --> 03:55:46.640
standard deviation

03:55:46.640 --> 03:55:48.640
of both of these arrays here

03:55:48.640 --> 03:55:50.640
so 0.048

03:55:50.640 --> 03:55:52.640
and then 0.52

03:55:52.640 --> 03:55:54.640
our standard deviation is

03:55:54.640 --> 03:55:56.640
0.0163

03:55:56.640 --> 03:55:58.640
so very small

03:55:58.640 --> 03:56:00.640
and then we have

03:56:00.640 --> 03:56:02.640
negative 0.38, 0.52

03:56:02.640 --> 03:56:04.640
and 2.48

03:56:04.640 --> 03:56:06.640
we get a standard deviation of 1.19

03:56:06.640 --> 03:56:08.640
so you can see that these numbers are vastly different

03:56:08.640 --> 03:56:10.640
one is like

03:56:10.640 --> 03:56:12.640
one is literally

03:56:12.640 --> 03:56:14.640
100 times greater than the other

03:56:14.640 --> 03:56:16.640
so

03:56:16.640 --> 03:56:18.640
the reason for this is because these

03:56:18.640 --> 03:56:20.640
numbers are super

03:56:20.640 --> 03:56:22.640
diverse

03:56:22.640 --> 03:56:24.640
I guess another way

03:56:24.640 --> 03:56:26.640
you could think of them is that

03:56:26.640 --> 03:56:28.640
they stretch out very far from the

03:56:28.640 --> 03:56:30.640
mean

03:56:30.640 --> 03:56:32.640
this essentially means when you're initializing

03:56:32.640 --> 03:56:34.640
your parameters

03:56:34.640 --> 03:56:36.640
that if you have some outliers

03:56:36.640 --> 03:56:38.640
then your network

03:56:38.640 --> 03:56:40.640
is going to be funky

03:56:40.640 --> 03:56:42.640
because it's

03:56:42.640 --> 03:56:44.640
the learning process just messed up because you have outliers

03:56:44.640 --> 03:56:46.640
and it's not just learning the right way

03:56:46.640 --> 03:56:48.640
it's supposed to

03:56:48.640 --> 03:56:50.640
whereas if you had

03:56:50.640 --> 03:56:52.640
way too small of a standard deviation

03:56:52.640 --> 03:56:54.640
from your initial parameters

03:56:54.640 --> 03:56:56.640
like in here but maybe even smaller

03:56:56.640 --> 03:56:58.640
so let's say they were all

03:56:58.640 --> 03:57:00.640
0.5

03:57:00.640 --> 03:57:02.640
then all of your neurons

03:57:02.640 --> 03:57:04.640
would effectively be the same

03:57:04.640 --> 03:57:06.640
and they would all learn the same pattern

03:57:06.640 --> 03:57:08.640
so then you would have no learning done

03:57:08.640 --> 03:57:10.640
so one would either be

03:57:10.640 --> 03:57:12.640
you're learning a super super unstable

03:57:12.640 --> 03:57:14.640
and you have outliers that are

03:57:14.640 --> 03:57:16.640
just learning

03:57:16.640 --> 03:57:18.640
very distinct things and not really

03:57:18.640 --> 03:57:20.640
not really

03:57:20.640 --> 03:57:22.640
not really letting other neurons

03:57:22.640 --> 03:57:24.640
get opportunities to learn

03:57:24.640 --> 03:57:26.640
or rather other parameters to learn

03:57:28.640 --> 03:57:30.640
if you have a lot of diversity

03:57:30.640 --> 03:57:32.640
you just have outliers and then if you have

03:57:32.640 --> 03:57:34.640
no

03:57:34.640 --> 03:57:36.640
diversity at all then

03:57:36.640 --> 03:57:38.640
essentially nothing is learned and your network

03:57:38.640 --> 03:57:40.640
is useless so all we want to do

03:57:40.640 --> 03:57:42.640
is make sure that our standard deviation

03:57:42.640 --> 03:57:44.640
is balanced and stable

03:57:44.640 --> 03:57:46.640
so that the training process

03:57:46.640 --> 03:57:48.640
can learn effective things

03:57:48.640 --> 03:57:50.640
so each neuron can learn a little bit

03:57:50.640 --> 03:57:52.640
so you can see here

03:57:52.640 --> 03:57:54.640
this would probably be an okay standard deviation

03:57:54.640 --> 03:57:56.640
if these were some parameters because

03:57:56.640 --> 03:57:58.640
they're a little bit different than each other

03:57:58.640 --> 03:58:00.640
they're not all like super super

03:58:00.640 --> 03:58:02.640
close to the same

03:58:02.640 --> 03:58:04.640
and yeah

03:58:04.640 --> 03:58:06.640
so essentially what

03:58:06.640 --> 03:58:08.640
this looks like in

03:58:08.640 --> 03:58:10.640
code here is the following

03:58:10.640 --> 03:58:12.640
so you don't actually need to

03:58:12.640 --> 03:58:14.640
memorize what this does as it's

03:58:14.640 --> 03:58:16.640
just used in practice

03:58:16.640 --> 03:58:18.640
by professionals

03:58:18.640 --> 03:58:20.640
but essentially what this does

03:58:20.640 --> 03:58:22.640
is it initializes our weights

03:58:22.640 --> 03:58:24.640
around certain standard deviations

03:58:24.640 --> 03:58:26.640
so here we set it to 0.02

03:58:26.640 --> 03:58:28.640
which is pretty much the same

03:58:28.640 --> 03:58:30.640
as what we had in here

03:58:30.640 --> 03:58:32.640
so

03:58:32.640 --> 03:58:34.640
point

03:58:34.640 --> 03:58:36.640
point

03:58:36.640 --> 03:58:38.640
this one's a little bit off in the standard deviation

03:58:38.640 --> 03:58:40.640
set here

03:58:40.640 --> 03:58:42.640
but essentially

03:58:42.640 --> 03:58:44.640
we're just making sure that our weights

03:58:44.640 --> 03:58:46.640
are initialized properly

03:58:46.640 --> 03:58:48.640
and you don't have to memorize this at all

03:58:48.640 --> 03:58:50.640
it's just used in practice and it's going to help our training

03:58:50.640 --> 03:58:52.640
converge better

03:58:52.640 --> 03:58:54.640
so as long as you understand

03:58:54.640 --> 03:58:56.640
that we can apply some initializations

03:58:56.640 --> 03:58:58.640
on our weights

03:58:58.640 --> 03:59:00.640
that's all that really matters, so cool

03:59:00.640 --> 03:59:02.640
let's move on to the next part

03:59:02.640 --> 03:59:04.640
of our GBT architecture

03:59:04.640 --> 03:59:06.640
so awesome, we finished this GBT language

03:59:06.640 --> 03:59:08.640
class, everything's pretty much done here

03:59:08.640 --> 03:59:10.640
we did our knit

03:59:10.640 --> 03:59:12.640
we did some weight initializations

03:59:12.640 --> 03:59:14.640
and we did our forward pass, so awesome

03:59:14.640 --> 03:59:16.640
that's all done, now let's move on to the next

03:59:16.640 --> 03:59:18.640
which is the

03:59:18.640 --> 03:59:20.640
block class

03:59:20.640 --> 03:59:22.640
so what is block?

03:59:22.640 --> 03:59:24.640
well, if we go back to this diagram

03:59:24.640 --> 03:59:26.640
each of these decoder blocks is a block

03:59:26.640 --> 03:59:28.640
so

03:59:28.640 --> 03:59:30.640
we're pretty much just going to fill in this gap here

03:59:30.640 --> 03:59:32.640
our GBT language model has these two

03:59:32.640 --> 03:59:34.640
where we get our tokenized inputs

03:59:34.640 --> 03:59:36.640
and then we do some transformations

03:59:36.640 --> 03:59:38.640
and the softmax after

03:59:38.640 --> 03:59:40.640
and essentially we're just filling

03:59:40.640 --> 03:59:42.640
in this gap here and then we're going to build out

03:59:42.640 --> 03:59:44.640
and just sort of branch out until it's

03:59:44.640 --> 03:59:46.640
completely built

03:59:46.640 --> 03:59:48.640
so let's go ahead and build these blocks here

03:59:48.640 --> 03:59:50.640
what does this look like?

03:59:50.640 --> 03:59:52.640
that's what this does

03:59:52.640 --> 03:59:54.640
so we have our knit, we have a forward pass

03:59:54.640 --> 03:59:56.640
as per usual

03:59:56.640 --> 03:59:58.640
and knit

03:59:58.640 --> 04:00:00.640
and a forward pass as seen

04:00:00.640 --> 04:00:02.640
in the GBT language model class

04:00:02.640 --> 04:00:04.640
which is going to look like this

04:00:04.640 --> 04:00:06.640
forward and an init

04:00:06.640 --> 04:00:08.640
so the init

04:00:08.640 --> 04:00:10.640
is going to just initialize some things

04:00:10.640 --> 04:00:12.640
it's going to initialize some transformations

04:00:12.640 --> 04:00:14.640
and some things that we're going to do in the forward pass

04:00:14.640 --> 04:00:16.640
that's all it's doing

04:00:16.640 --> 04:00:18.640
so what do we do first?

04:00:18.640 --> 04:00:20.640
well we have this new head size

04:00:20.640 --> 04:00:22.640
parameter introduced

04:00:22.640 --> 04:00:24.640
so head size is the number of features

04:00:24.640 --> 04:00:26.640
that each head will be capturing

04:00:26.640 --> 04:00:28.640
in our multi-head attention

04:00:28.640 --> 04:00:30.640
so all the heads in parallel

04:00:30.640 --> 04:00:32.640
features are each of them capturing

04:00:32.640 --> 04:00:34.640
so we do that by dividing

04:00:34.640 --> 04:00:36.640
n embed by n head

04:00:36.640 --> 04:00:38.640
so n head is the number

04:00:38.640 --> 04:00:40.640
of heads we have

04:00:40.640 --> 04:00:42.640
and n embed is the number of features we have

04:00:42.640 --> 04:00:44.640
where we're capturing

04:00:44.640 --> 04:00:46.640
so 384 features divided by 4 heads

04:00:46.640 --> 04:00:48.640
so each head is going to be capturing

04:00:48.640 --> 04:00:50.640
96 features

04:00:50.640 --> 04:00:52.640
hence head size

04:00:52.640 --> 04:00:54.640
so

04:00:54.640 --> 04:00:56.640
next up we have self.sa

04:00:56.640 --> 04:00:58.640
which is just short for self-attention

04:00:58.640 --> 04:01:00.640
we do a multi-head attention

04:01:00.640 --> 04:01:02.640
we pass in our n head

04:01:02.640 --> 04:01:04.640
and our head size and you'll see how these

04:01:04.640 --> 04:01:06.640
parameters fit in later

04:01:06.640 --> 04:01:08.640
once we build up this multi-head attention

04:01:08.640 --> 04:01:10.640
class so cool

04:01:10.640 --> 04:01:12.640
now we have a feed forward

04:01:12.640 --> 04:01:14.640
which is as explained

04:01:14.640 --> 04:01:16.640
just in the diagram here

04:01:16.640 --> 04:01:18.640
our feed forward is just this

04:01:18.640 --> 04:01:20.640
which we're actually going to build out next

04:01:22.640 --> 04:01:24.640
and we have two layer norms

04:01:24.640 --> 04:01:26.640
and these are just for the

04:01:26.640 --> 04:01:28.640
post norm

04:01:28.640 --> 04:01:30.640
pre norm architecture that we could implement here

04:01:30.640 --> 04:01:32.640
in this case it's going to be

04:01:32.640 --> 04:01:34.640
post norm just because

04:01:34.640 --> 04:01:36.640
I found that it converges better for this

04:01:36.640 --> 04:01:38.640
for this course and the data that we're using

04:01:38.640 --> 04:01:40.640
and just the model parameters

04:01:40.640 --> 04:01:42.640
and what not it just works better

04:01:42.640 --> 04:01:44.640
so

04:01:44.640 --> 04:01:46.640
also that is the original

04:01:46.640 --> 04:01:48.640
architecture that we use in the

04:01:48.640 --> 04:01:50.640
attention paper

04:01:50.640 --> 04:01:52.640
so you might have seen that they do an add a norm

04:01:52.640 --> 04:01:54.640
rather than a norm and add

04:01:54.640 --> 04:01:56.640
anyways

04:01:56.640 --> 04:01:58.640
we've initialized all of these

04:01:58.640 --> 04:02:00.640
so we have head size, self attention

04:02:00.640 --> 04:02:02.640
feed forward and then two layer norms

04:02:02.640 --> 04:02:04.640
so in our forward pass

04:02:04.640 --> 04:02:06.640
we do our self attention first

04:02:06.640 --> 04:02:08.640
let's actually go back to here

04:02:08.640 --> 04:02:10.640
so we do our self attention

04:02:10.640 --> 04:02:12.640
then add a norm

04:02:12.640 --> 04:02:14.640
then a feed forward and then add a norm again

04:02:16.640 --> 04:02:18.640
so what does this look like

04:02:18.640 --> 04:02:20.640
self attention, add a norm

04:02:20.640 --> 04:02:22.640
feed forward, add a norm

04:02:22.640 --> 04:02:24.640
cool

04:02:24.640 --> 04:02:26.640
so we're doing an add so we're going

04:02:26.640 --> 04:02:28.640
x plus the previous

04:02:28.640 --> 04:02:30.640
answer which is adding them together

04:02:30.640 --> 04:02:32.640
and then we're just applying a layer norm to this

04:02:32.640 --> 04:02:34.640
so cool

04:02:34.640 --> 04:02:36.640
if you want to look up more into what layer norm does

04:02:36.640 --> 04:02:38.640
and everything and why it's so useful

04:02:38.640 --> 04:02:40.640
you can totally go out of your way to do that

04:02:40.640 --> 04:02:42.640
but

04:02:42.640 --> 04:02:44.640
layer norm is essentially just going to

04:02:44.640 --> 04:02:46.640
help smoothen out our features

04:02:46.640 --> 04:02:48.640
here

04:02:48.640 --> 04:02:50.640
so

04:02:50.640 --> 04:02:52.640
and honestly there's not much else to that

04:02:52.640 --> 04:02:54.640
we just return this final value here

04:02:54.640 --> 04:02:56.640
and that's pretty much the output of our blocks

04:02:56.640 --> 04:02:58.640
so

04:02:58.640 --> 04:03:00.640
next up I'm going to add

04:03:00.640 --> 04:03:02.640
a new little code block here

04:03:02.640 --> 04:03:04.640
which is going to be

04:03:04.640 --> 04:03:06.640
our feed forward

04:03:06.640 --> 04:03:08.640
so let's go ahead and do that

04:03:08.640 --> 04:03:10.640
so feed forward, it's just going to look exactly like this

04:03:10.640 --> 04:03:12.640
it's actually quite simple

04:03:12.640 --> 04:03:14.640
so all we do is we make an nn dot sequential

04:03:14.640 --> 04:03:16.640
torch dot nn

04:03:16.640 --> 04:03:18.640
we make this a sequential network of linear

04:03:18.640 --> 04:03:20.640
linear, relu, and then linear

04:03:20.640 --> 04:03:22.640
so

04:03:22.640 --> 04:03:24.640
in our linear

04:03:24.640 --> 04:03:26.640
we have to pay attention to the shapes here

04:03:26.640 --> 04:03:28.640
so we have n embed

04:03:28.640 --> 04:03:30.640
and then n embed times 4

04:03:30.640 --> 04:03:32.640
and then the relu will just

04:03:32.640 --> 04:03:34.640
essentially

04:03:34.640 --> 04:03:36.640
what the relu will do is it looks like this

04:03:36.640 --> 04:03:38.640
let me illustrate this for you guys

04:03:38.640 --> 04:03:40.640
so

04:03:40.640 --> 04:03:42.640
essentially you have this graph here

04:03:44.640 --> 04:03:46.640
and

04:03:46.640 --> 04:03:48.640
let's just make this a whole plane actually

04:03:50.640 --> 04:03:52.640
so

04:03:52.640 --> 04:03:54.640
all of these values

04:03:54.640 --> 04:03:56.640
that are below 0

04:03:56.640 --> 04:03:58.640
all these values that are below 0 on the x axis

04:03:58.640 --> 04:04:00.640
and

04:04:00.640 --> 04:04:02.640
equal to 0 will be changed

04:04:02.640 --> 04:04:04.640
just to 0 like that so you have all these values

04:04:04.640 --> 04:04:06.640
that look like this

04:04:06.640 --> 04:04:08.640
and then everything that is above 0 just stays the same

04:04:08.640 --> 04:04:10.640
so you essentially just have this

04:04:10.640 --> 04:04:12.640
funny looking shape it's like straight

04:04:12.640 --> 04:04:14.640
and then diagonal that's what the relu function does

04:04:14.640 --> 04:04:16.640
it looks at a number

04:04:16.640 --> 04:04:18.640
sees if it's equal to or less than 0

04:04:18.640 --> 04:04:20.640
if that's true we give that number 0

04:04:20.640 --> 04:04:22.640
and if it's not

04:04:22.640 --> 04:04:24.640
then we just leave the number alone

04:04:24.640 --> 04:04:26.640
so cool very cool

04:04:26.640 --> 04:04:28.640
non-linearity function

04:04:28.640 --> 04:04:30.640
you can read papers on that if you like

04:04:30.640 --> 04:04:32.640
but

04:04:32.640 --> 04:04:34.640
essentially the shape of this

04:04:34.640 --> 04:04:36.640
just doesn't matter all we're doing is

04:04:36.640 --> 04:04:38.640
we're just making sure that we're just converting

04:04:38.640 --> 04:04:40.640
some values if they're equal to

04:04:40.640 --> 04:04:42.640
or below 0 that's all this is doing

04:04:42.640 --> 04:04:44.640
and then

04:04:44.640 --> 04:04:46.640
we essentially are multiplying

04:04:46.640 --> 04:04:48.640
this we're doing this

04:04:48.640 --> 04:04:50.640
linear transformation times this one

04:04:50.640 --> 04:04:52.640
so we have to make sure that these inner

04:04:52.640 --> 04:04:54.640
we have to make sure that these

04:04:54.640 --> 04:04:56.640
inner dimensions line up so 4 times

04:04:56.640 --> 04:04:58.640
N embed and 4 times N embed

04:04:58.640 --> 04:05:00.640
those are equal to each other so our output shape

04:05:00.640 --> 04:05:02.640
should be N embed

04:05:02.640 --> 04:05:04.640
by N embed cool

04:05:04.640 --> 04:05:06.640
so now we have our dropout

04:05:06.640 --> 04:05:08.640
and in case you don't know what dropout is

04:05:08.640 --> 04:05:10.640
it pretty much just

04:05:10.640 --> 04:05:12.640
makes a certain percentage

04:05:12.640 --> 04:05:14.640
of our neurons just

04:05:14.640 --> 04:05:16.640
dropout and become 0

04:05:16.640 --> 04:05:18.640
this is used to prevent overfitting

04:05:18.640 --> 04:05:20.640
and some other little details

04:05:20.640 --> 04:05:22.640
that I'm sure you could

04:05:22.640 --> 04:05:24.640
you could figure out through experimenting

04:05:24.640 --> 04:05:26.640
so

04:05:26.640 --> 04:05:28.640
all this actually looks like in a parameter form

04:05:28.640 --> 04:05:30.640
is just

04:05:30.640 --> 04:05:32.640
dropout

04:05:34.640 --> 04:05:36.640
dropout equals

04:05:36.640 --> 04:05:38.640
we'll just say 0.2 for the same

04:05:38.640 --> 04:05:40.640
so 0.2 means

04:05:40.640 --> 04:05:42.640
20%

04:05:42.640 --> 04:05:44.640
or 0.2 is going to

04:05:44.640 --> 04:05:46.640
yeah so 0.2

04:05:46.640 --> 04:05:48.640
in percentage form is just going to dropout

04:05:48.640 --> 04:05:50.640
20% of our

04:05:50.640 --> 04:05:52.640
neurons turn them to 0 to prevent overfitting

04:05:52.640 --> 04:05:54.640
that's what that's doing

04:05:54.640 --> 04:05:56.640
so cool

04:05:56.640 --> 04:05:58.640
we have our feedforward network we dropout after

04:05:58.640 --> 04:06:00.640
to prevent overfitting and then we just

04:06:00.640 --> 04:06:02.640
call it forward on this sequential network

04:06:02.640 --> 04:06:04.640
so cool

04:06:04.640 --> 04:06:06.640
feedforward pretty self-explanatory

04:06:06.640 --> 04:06:08.640
we're going to add the

04:06:08.640 --> 04:06:10.640
multi-head attention class

04:06:10.640 --> 04:06:12.640
so we've built all these decoder blocks

04:06:12.640 --> 04:06:14.640
we've built

04:06:14.640 --> 04:06:16.640
inside of the decoder blocks we've built the feedforward

04:06:16.640 --> 04:06:18.640
and our res connections

04:06:18.640 --> 04:06:20.640
and now

04:06:20.640 --> 04:06:22.640
all we have to do left in this block

04:06:22.640 --> 04:06:24.640
is the multi-head attention

04:06:24.640 --> 04:06:26.640
so it's going to look exactly like this here

04:06:26.640 --> 04:06:28.640
we're going to ignore the keys and queers for now

04:06:28.640 --> 04:06:30.640
and save this for dot product attention

04:06:30.640 --> 04:06:32.640
so we're going to

04:06:32.640 --> 04:06:34.640
essentially just make a bunch of these

04:06:34.640 --> 04:06:36.640
multiple

04:06:36.640 --> 04:06:38.640
heads

04:06:38.640 --> 04:06:40.640
and we're going to concatenate results and do a linear

04:06:40.640 --> 04:06:42.640
transformation so what does this look like in code

04:06:42.640 --> 04:06:44.640
well let's go ahead

04:06:44.640 --> 04:06:46.640
and add this here

04:06:46.640 --> 04:06:48.640
all that attention cool

04:06:48.640 --> 04:06:50.640
so multiple heads of attention in parallel

04:06:50.640 --> 04:06:52.640
I explained this earlier so I'm not going to jump into

04:06:52.640 --> 04:06:54.640
too much detail on that

04:06:54.640 --> 04:06:56.640
but we have our knit

04:06:56.640 --> 04:06:58.640
we have our forward

04:06:58.640 --> 04:07:00.640
and what are we doing in here

04:07:00.640 --> 04:07:02.640
so our self dot heads is just a module list

04:07:02.640 --> 04:07:04.640
and

04:07:04.640 --> 04:07:06.640
module list is kind of funky I'll dive into it

04:07:06.640 --> 04:07:08.640
a little bit later

04:07:08.640 --> 04:07:10.640
but essentially what we're doing is we're having

04:07:10.640 --> 04:07:12.640
a bunch of these heads

04:07:12.640 --> 04:07:14.640
essentially in parallel

04:07:14.640 --> 04:07:16.640
for each head

04:07:16.640 --> 04:07:18.640
so num heads let's say our num heads is

04:07:18.640 --> 04:07:20.640
set to

04:07:20.640 --> 04:07:22.640
our num heads

04:07:22.640 --> 04:07:24.640
is set to

04:07:24.640 --> 04:07:26.640
maybe four in this

04:07:26.640 --> 04:07:28.640
block we do multi-head attention

04:07:28.640 --> 04:07:30.640
we do n heads and then head size

04:07:30.640 --> 04:07:32.640
so

04:07:32.640 --> 04:07:34.640
and heads and then head size so num heads

04:07:34.640 --> 04:07:36.640
essentially what it is so for the number of

04:07:36.640 --> 04:07:38.640
heads that we have which is four

04:07:38.640 --> 04:07:40.640
we're going to pretty much make one head

04:07:40.640 --> 04:07:42.640
running in parallel

04:07:42.640 --> 04:07:44.640
so four heads running in parallel is what this

04:07:44.640 --> 04:07:46.640
does here

04:07:46.640 --> 04:07:48.640
then we have this projection

04:07:48.640 --> 04:07:50.640
which is essentially just going to

04:07:50.640 --> 04:07:52.640
project the

04:07:52.640 --> 04:07:54.640
head size

04:07:54.640 --> 04:07:56.640
times the number of

04:07:56.640 --> 04:07:58.640
heads to an embed

04:07:58.640 --> 04:08:00.640
and you might ask well that's

04:08:00.640 --> 04:08:02.640
weird because

04:08:02.640 --> 04:08:04.640
num heads times this is

04:08:04.640 --> 04:08:06.640
literally equal to an embedding

04:08:06.640 --> 04:08:08.640
if you go back to the math

04:08:08.640 --> 04:08:10.640
we did here

04:08:10.640 --> 04:08:12.640
and the purpose of this is just to be

04:08:12.640 --> 04:08:14.640
super hackable so that if you actually do want to

04:08:14.640 --> 04:08:16.640
change these around it won't be throwing you dimensionality

04:08:16.640 --> 04:08:18.640
errors so that's what we're doing

04:08:18.640 --> 04:08:20.640
just a little projection

04:08:20.640 --> 04:08:22.640
from our

04:08:22.640 --> 04:08:24.640
whatever these values are

04:08:24.640 --> 04:08:26.640
up to this

04:08:26.640 --> 04:08:28.640
constant feature

04:08:28.640 --> 04:08:30.640
length of an embed

04:08:30.640 --> 04:08:32.640
so then we just follow that with a drop out

04:08:32.640 --> 04:08:34.640
dropping out 20% of the

04:08:34.640 --> 04:08:36.640
networks neurons

04:08:36.640 --> 04:08:38.640
now let's go into this forward here

04:08:38.640 --> 04:08:40.640
so forward

04:08:40.640 --> 04:08:42.640
torch dot concatenate or torch dot cat

04:08:42.640 --> 04:08:44.640
we do four h and self dot heads

04:08:44.640 --> 04:08:46.640
so we're going to concatenate

04:08:46.640 --> 04:08:48.640
each head together

04:08:48.640 --> 04:08:50.640
along the last dimension

04:08:50.640 --> 04:08:52.640
and the last dimension in this case

04:08:52.640 --> 04:08:54.640
is the

04:08:54.640 --> 04:08:56.640
b batch

04:08:56.640 --> 04:08:58.640
by time

04:08:58.640 --> 04:09:00.640
by we just say feature dimension or channel dimension

04:09:02.640 --> 04:09:04.640
the channel dimension here is the

04:09:04.640 --> 04:09:06.640
last one so we're going to

04:09:06.640 --> 04:09:08.640
concatenate along this feature dimension

04:09:10.640 --> 04:09:12.640
and let me just help you illustrate

04:09:12.640 --> 04:09:14.640
what exactly this looks like

04:09:14.640 --> 04:09:16.640
so

04:09:16.640 --> 04:09:18.640
when we concatenate along these

04:09:18.640 --> 04:09:20.640
we have this b by t

04:09:20.640 --> 04:09:22.640
and then we'll just say

04:09:24.640 --> 04:09:26.640
our features are going to be

04:09:26.640 --> 04:09:28.640
h1 like

04:09:28.640 --> 04:09:30.640
each of our heads here

04:09:30.640 --> 04:09:32.640
another h1

04:09:32.640 --> 04:09:34.640
h1 h1 and these are all just features

04:09:34.640 --> 04:09:36.640
of head one and then our next

04:09:36.640 --> 04:09:38.640
would be h2

04:09:38.640 --> 04:09:40.640
h2 h2 h2

04:09:40.640 --> 04:09:42.640
and then let's just say we have

04:09:42.640 --> 04:09:44.640
a third head go h3

04:09:44.640 --> 04:09:46.640
h3

04:09:46.640 --> 04:09:48.640
h3 h3

04:09:48.640 --> 04:09:50.640
h3 like that

04:09:50.640 --> 04:09:52.640
so we have

04:09:52.640 --> 04:09:54.640
maybe four features per head

04:09:54.640 --> 04:09:56.640
and there's three heads

04:09:56.640 --> 04:09:58.640
so essentially all we're doing

04:09:58.640 --> 04:10:00.640
when we do this concatenate

04:10:00.640 --> 04:10:02.640
is we're just concatenating these along the last

04:10:02.640 --> 04:10:04.640
dimension so to convert

04:10:04.640 --> 04:10:06.640
this like ugly list format

04:10:06.640 --> 04:10:08.640
of just each head

04:10:08.640 --> 04:10:10.640
features sequentially in order

04:10:10.640 --> 04:10:12.640
which is like really hard

04:10:12.640 --> 04:10:14.640
to process we're just concatenating these

04:10:14.640 --> 04:10:16.640
so they're easier to process

04:10:16.640 --> 04:10:18.640
so that's what that does

04:10:18.640 --> 04:10:20.640
and then we just follow this with a dropout

04:10:20.640 --> 04:10:22.640
self dot projection

04:10:22.640 --> 04:10:24.640
and then just follow that with a

04:10:24.640 --> 04:10:26.640
dropout so cool

04:10:26.640 --> 04:10:28.640
if that didn't totally make

04:10:28.640 --> 04:10:30.640
sense you can totally just plug this code into chat

04:10:30.640 --> 04:10:32.640
gbt and

04:10:32.640 --> 04:10:34.640
get a detailed explanation on how it works

04:10:34.640 --> 04:10:36.640
if something wasn't particularly clear

04:10:36.640 --> 04:10:38.640
but essentially that's the premise

04:10:38.640 --> 04:10:40.640
you have your batch by time

04:10:40.640 --> 04:10:42.640
batch by

04:10:42.640 --> 04:10:44.640
sequence length

04:10:44.640 --> 04:10:46.640
or time use interchangeably

04:10:46.640 --> 04:10:48.640
and then you have your features which are all

04:10:48.640 --> 04:10:50.640
just in this weird list format

04:10:50.640 --> 04:10:52.640
of each feature just listed

04:10:52.640 --> 04:10:54.640
after another

04:10:54.640 --> 04:10:56.640
so cool

04:10:56.640 --> 04:10:58.640
that's what multi head attention looks like

04:10:58.640 --> 04:11:00.640
let's go ahead and implement dot product

04:11:00.640 --> 04:11:02.640
attention or scale dot product attention

04:11:02.640 --> 04:11:04.640
so a little something I'd like to cover before

04:11:04.640 --> 04:11:06.640
we go into our next scaled

04:11:06.640 --> 04:11:08.640
dot product attention was just this linear

04:11:08.640 --> 04:11:10.640
transformation here

04:11:10.640 --> 04:11:12.640
and you might think well what's the point if we're just

04:11:12.640 --> 04:11:14.640
transforming

04:11:14.640 --> 04:11:16.640
an embed to an embed right

04:11:16.640 --> 04:11:18.640
we're to have the match like that

04:11:18.640 --> 04:11:20.640
and

04:11:20.640 --> 04:11:22.640
essentially what this does is it just adds in another

04:11:22.640 --> 04:11:24.640
learnable parameter

04:11:24.640 --> 04:11:26.640
for us so it has a weight

04:11:26.640 --> 04:11:28.640
and a bias if we set bias

04:11:28.640 --> 04:11:30.640
to false

04:11:30.640 --> 04:11:32.640
like that then it wouldn't have a bias

04:11:32.640 --> 04:11:34.640
but it does have

04:11:34.640 --> 04:11:36.640
a bias so another just wx

04:11:36.640 --> 04:11:38.640
plus b if you will a weight times x

04:11:38.640 --> 04:11:40.640
plus a bias so it just adds

04:11:40.640 --> 04:11:42.640
more learnable parameters to help our

04:11:42.640 --> 04:11:44.640
network

04:11:44.640 --> 04:11:46.640
learn more about this text

04:11:46.640 --> 04:11:48.640
so cool I'm going to go ahead and add

04:11:48.640 --> 04:11:50.640
in this last but not least

04:11:52.640 --> 04:11:54.640
scale dot product attention

04:11:54.640 --> 04:11:56.640
or head class so there's going to be

04:11:56.640 --> 04:11:58.640
a bunch of these

04:11:58.640 --> 04:12:00.640
heads hence

04:12:00.640 --> 04:12:02.640
class head running in parallel

04:12:02.640 --> 04:12:04.640
and inside of here we're going to do some

04:12:04.640 --> 04:12:06.640
scale dot product attention

04:12:06.640 --> 04:12:08.640
so there's a lot of code in here don't get

04:12:08.640 --> 04:12:10.640
too overwhelmed by this but I'm going to walk

04:12:10.640 --> 04:12:12.640
through this step by step so we have our

04:12:12.640 --> 04:12:14.640
in it we have our forward

04:12:14.640 --> 04:12:16.640
awesome

04:12:16.640 --> 04:12:18.640
so what do we do in our

04:12:18.640 --> 04:12:20.640
architecture here

04:12:20.640 --> 04:12:22.640
so we have a key

04:12:22.640 --> 04:12:24.640
a query and a value

04:12:24.640 --> 04:12:26.640
the keys and the queries dot

04:12:26.640 --> 04:12:28.640
product together they get scaled

04:12:28.640 --> 04:12:30.640
by

04:12:30.640 --> 04:12:32.640
one over the square root of

04:12:32.640 --> 04:12:34.640
length of a row in the keys or queries

04:12:34.640 --> 04:12:36.640
matrix so we'll just say maybe keys

04:12:36.640 --> 04:12:38.640
for example

04:12:38.640 --> 04:12:40.640
the row of keys

04:12:40.640 --> 04:12:42.640
the length of a row in keys

04:12:42.640 --> 04:12:44.640
and then we just do our

04:12:44.640 --> 04:12:46.640
masking to make sure the network

04:12:46.640 --> 04:12:48.640
does not look ahead and cheat

04:12:48.640 --> 04:12:50.640
and then we do a softmax

04:12:50.640 --> 04:12:52.640
and a matrix

04:12:52.640 --> 04:12:54.640
multiply to

04:12:54.640 --> 04:12:56.640
essentially add this

04:12:56.640 --> 04:12:58.640
value weight on top of it

04:12:58.640 --> 04:13:00.640
so cool

04:13:00.640 --> 04:13:02.640
we do this

04:13:02.640 --> 04:13:04.640
keep in mind this initialization

04:13:04.640 --> 04:13:06.640
is not actually doing any calculations

04:13:06.640 --> 04:13:08.640
but just rather initializing

04:13:08.640 --> 04:13:10.640
linear transformations that we will do

04:13:10.640 --> 04:13:12.640
in the forward pass

04:13:12.640 --> 04:13:14.640
so this self dot key

04:13:14.640 --> 04:13:16.640
is just going to

04:13:16.640 --> 04:13:18.640
transform and embed to head size

04:13:18.640 --> 04:13:20.640
bias false and then

04:13:20.640 --> 04:13:22.640
I mean the rest of these are just the same

04:13:22.640 --> 04:13:24.640
and embed to head size because each head

04:13:24.640 --> 04:13:26.640
will have 96 features

04:13:26.640 --> 04:13:28.640
rather than 384

04:13:28.640 --> 04:13:30.640
so we kind of already went over that

04:13:30.640 --> 04:13:32.640
but that's just what that's doing

04:13:32.640 --> 04:13:34.640
cool that's just a linear transformation

04:13:34.640 --> 04:13:36.640
that's happening to convert

04:13:36.640 --> 04:13:38.640
from 384 to 96 features

04:13:38.640 --> 04:13:40.640
then we have this

04:13:40.640 --> 04:13:42.640
self dot register buffer

04:13:42.640 --> 04:13:44.640
well what does this do you might ask

04:13:44.640 --> 04:13:46.640
register buffer is essentially just going

04:13:46.640 --> 04:13:48.640
to register

04:13:48.640 --> 04:13:50.640
this no look ahead

04:13:50.640 --> 04:13:52.640
masking in the model state

04:13:52.640 --> 04:13:54.640
so instead of having to re-initialize

04:13:54.640 --> 04:13:56.640
this every single head for every

04:13:56.640 --> 04:13:58.640
single forward and backward pass

04:13:58.640 --> 04:14:00.640
we're just going to add this to the model

04:14:00.640 --> 04:14:02.640
state so it's going to save us a lot of

04:14:02.640 --> 04:14:04.640
computation that way on our training

04:14:04.640 --> 04:14:06.640
so our training times can be reduced just because

04:14:06.640 --> 04:14:08.640
we're registering this

04:14:08.640 --> 04:14:10.640
yeah

04:14:10.640 --> 04:14:12.640
so it's just going to prevent some of that

04:14:12.640 --> 04:14:14.640
overhead computation of having to redo

04:14:14.640 --> 04:14:16.640
this over and over again

04:14:16.640 --> 04:14:18.640
you could still do training without

04:14:18.640 --> 04:14:20.640
this it would just take longer

04:14:20.640 --> 04:14:22.640
so that's what that's doing

04:14:22.640 --> 04:14:24.640
yeah

04:14:24.640 --> 04:14:26.640
so now we have this drop-out

04:14:26.640 --> 04:14:28.640
of course and then in our forward pass

04:14:28.640 --> 04:14:30.640
let's

04:14:30.640 --> 04:14:32.640
break this down step by step here

04:14:32.640 --> 04:14:34.640
so we have a b by t by c

04:14:34.640 --> 04:14:36.640
so batch by time

04:14:36.640 --> 04:14:38.640
by channel is our shape

04:14:38.640 --> 04:14:40.640
we just unpack those numbers

04:14:40.640 --> 04:14:42.640
and then we have a key

04:14:42.640 --> 04:14:44.640
which is just calling this

04:14:44.640 --> 04:14:46.640
linear transformation here on an input

04:14:46.640 --> 04:14:48.640
x

04:14:48.640 --> 04:14:50.640
and then a query which is also

04:14:50.640 --> 04:14:52.640
calling the same transformation but a different

04:14:52.640 --> 04:14:54.640
learnable transformation on x as well

04:14:54.640 --> 04:14:56.640
so what we get

04:14:56.640 --> 04:14:58.640
is this instead of b by t by c

04:14:58.640 --> 04:15:00.640
we get b by t by head size

04:15:00.640 --> 04:15:02.640
hence this transformation

04:15:02.640 --> 04:15:04.640
from 384 to 96

04:15:04.640 --> 04:15:06.640
so that's what that is

04:15:06.640 --> 04:15:08.640
that's how these turn out here

04:15:08.640 --> 04:15:10.640
so now we can actually compute

04:15:10.640 --> 04:15:12.640
the attention scores

04:15:12.640 --> 04:15:14.640
so what do we do

04:15:14.640 --> 04:15:16.640
we'll just say weights is our attention

04:15:16.640 --> 04:15:18.640
weights are

04:15:18.640 --> 04:15:20.640
I guess you could say that

04:15:20.640 --> 04:15:22.640
we have our queries

04:15:22.640 --> 04:15:24.640
dot product matrix multiply

04:15:24.640 --> 04:15:26.640
with the

04:15:26.640 --> 04:15:28.640
keys transposed

04:15:28.640 --> 04:15:30.640
so

04:15:30.640 --> 04:15:32.640
what does this actually look like

04:15:32.640 --> 04:15:34.640
and I want to help you guys

04:15:34.640 --> 04:15:36.640
sort of understand what transposing does here

04:15:36.640 --> 04:15:38.640
so

04:15:38.640 --> 04:15:40.640
let's go back to here

04:15:40.640 --> 04:15:42.640
and draw out what this is going to look like

04:15:42.640 --> 04:15:44.640
so

04:15:44.640 --> 04:15:46.640
essentially what transposing is going to do

04:15:46.640 --> 04:15:48.640
is

04:15:48.640 --> 04:15:50.640
it is just going to make sure

04:15:50.640 --> 04:15:52.640
let me draw this out

04:15:52.640 --> 04:15:54.640
first

04:15:54.640 --> 04:15:56.640
so let's say you had

04:15:56.640 --> 04:15:58.640
I don't know

04:15:58.640 --> 04:16:00.640
maybe

04:16:00.640 --> 04:16:02.640
a

04:16:06.640 --> 04:16:08.640
b

04:16:08.640 --> 04:16:10.640
c

04:16:10.640 --> 04:16:12.640
d

04:16:12.640 --> 04:16:14.640
and you have a

04:16:14.640 --> 04:16:16.640
b

04:16:16.640 --> 04:16:18.640
c

04:16:18.640 --> 04:16:20.640
and d cool let's draw some lines

04:16:20.640 --> 04:16:22.640
to separate these

04:16:24.640 --> 04:16:26.640
so

04:16:34.640 --> 04:16:36.640
awesome so essentially what this does

04:16:36.640 --> 04:16:38.640
is the transposing

04:16:38.640 --> 04:16:40.640
puts it into this form

04:16:40.640 --> 04:16:42.640
so if we didn't have

04:16:42.640 --> 04:16:44.640
transposed then this would be in a different order

04:16:44.640 --> 04:16:46.640
it wouldn't be a b c d

04:16:46.640 --> 04:16:48.640
in both

04:16:48.640 --> 04:16:50.640
from like top to bottom left to right type of thing

04:16:50.640 --> 04:16:52.640
it would be in a different order

04:16:52.640 --> 04:16:54.640
but essentially not allow us

04:16:54.640 --> 04:16:56.640
to multiply them the same way

04:16:56.640 --> 04:16:58.640
so when we do a by a

04:16:58.640 --> 04:17:00.640
a times b

04:17:00.640 --> 04:17:02.640
it's like sort of a direct

04:17:02.640 --> 04:17:04.640
multiply if you will

04:17:04.640 --> 04:17:06.640
I don't know if you remember times tables at all

04:17:06.640 --> 04:17:08.640
from elementary school

04:17:08.640 --> 04:17:10.640
but that's pretty much what it is

04:17:10.640 --> 04:17:12.640
we're just setting up in a times table form

04:17:12.640 --> 04:17:14.640
and we're computing attention scores that way

04:17:14.640 --> 04:17:16.640
so

04:17:16.640 --> 04:17:18.640
that's what that is

04:17:18.640 --> 04:17:20.640
that's what this transposing is doing

04:17:20.640 --> 04:17:22.640
all this does is it just flips

04:17:22.640 --> 04:17:24.640
the second last dimension

04:17:24.640 --> 04:17:26.640
with the last dimension

04:17:26.640 --> 04:17:28.640
so

04:17:28.640 --> 04:17:30.640
in our case our second last

04:17:30.640 --> 04:17:32.640
is t and our last is head size

04:17:32.640 --> 04:17:34.640
so it just swaps these two

04:17:34.640 --> 04:17:36.640
so we get b by t by head size

04:17:36.640 --> 04:17:38.640
and then b by head size by t

04:17:38.640 --> 04:17:40.640
we dot product these together

04:17:40.640 --> 04:17:42.640
also keeping in mind our scaling

04:17:42.640 --> 04:17:44.640
here

04:17:44.640 --> 04:17:46.640
which is taking this

04:17:46.640 --> 04:17:48.640
we're just taking this scaling

04:17:48.640 --> 04:17:50.640
one

04:17:50.640 --> 04:17:52.640
over the square root of length

04:17:52.640 --> 04:17:54.640
of a row in the keys

04:17:54.640 --> 04:17:56.640
if we look at this here

04:17:56.640 --> 04:17:58.640
now there's little analogy

04:17:58.640 --> 04:18:00.640
I'd like to provide for this scaling

04:18:00.640 --> 04:18:02.640
right here

04:18:02.640 --> 04:18:04.640
so imagine in a room

04:18:04.640 --> 04:18:06.640
with a group of people and you're trying to understand

04:18:06.640 --> 04:18:08.640
the overall conversation

04:18:08.640 --> 04:18:10.640
if everyone is talking at once

04:18:10.640 --> 04:18:12.640
it might be challenging to keep track

04:18:12.640 --> 04:18:14.640
of what's being said

04:18:14.640 --> 04:18:16.640
it would be more manageable if you could focus on

04:18:16.640 --> 04:18:18.640
time right?

04:18:18.640 --> 04:18:20.640
so that's similar to how a multi head attention

04:18:20.640 --> 04:18:22.640
in a transformer works

04:18:22.640 --> 04:18:24.640
so each of these heads

04:18:24.640 --> 04:18:26.640
divides the original problem

04:18:26.640 --> 04:18:28.640
of understanding the entire conversation

04:18:28.640 --> 04:18:30.640
i.e. the entire input sequence

04:18:30.640 --> 04:18:32.640
into smaller more manageable

04:18:32.640 --> 04:18:34.640
sub problems

04:18:34.640 --> 04:18:36.640
each of these sub problems is a head

04:18:36.640 --> 04:18:38.640
so the head size

04:18:38.640 --> 04:18:40.640
is the number of these sub problems

04:18:40.640 --> 04:18:42.640
now consider what happens when each person

04:18:42.640 --> 04:18:44.640
talks louder or quieter

04:18:44.640 --> 04:18:46.640
if someone speaks too loudly

04:18:46.640 --> 04:18:48.640
or

04:18:48.640 --> 04:18:50.640
the values and the vectors are very large

04:18:50.640 --> 04:18:52.640
it might drown out the others

04:18:52.640 --> 04:18:54.640
this could make it difficult to understand the conversation

04:18:54.640 --> 04:18:56.640
because you're only hearing one voice

04:18:56.640 --> 04:18:58.640
or most of one voice

04:18:58.640 --> 04:19:00.640
to prevent this

04:19:00.640 --> 04:19:02.640
we want to control how loud

04:19:02.640 --> 04:19:04.640
or how quiet each person is talking

04:19:04.640 --> 04:19:06.640
so we can hear everyone evenly

04:19:06.640 --> 04:19:08.640
the dot product of the

04:19:08.640 --> 04:19:10.640
query and key vectors

04:19:10.640 --> 04:19:12.640
in the attention mechanism

04:19:12.640 --> 04:19:14.640
we want to check how loud each of voices

04:19:14.640 --> 04:19:16.640
if the vectors are very large

04:19:16.640 --> 04:19:18.640
or high dimensional

04:19:18.640 --> 04:19:20.640
or many people are talking

04:19:20.640 --> 04:19:22.640
the dot product can be very large

04:19:22.640 --> 04:19:24.640
to control this volume

04:19:24.640 --> 04:19:26.640
by scaling down the dot product

04:19:26.640 --> 04:19:28.640
using the square root of the head size

04:19:30.640 --> 04:19:32.640
this scaling helps ensure that no single

04:19:32.640 --> 04:19:34.640
voice is too dominant

04:19:34.640 --> 04:19:36.640
allowing us to hear all the voices evenly

04:19:36.640 --> 04:19:38.640
this is why we don't scale

04:19:38.640 --> 04:19:40.640
by the number of heads

04:19:40.640 --> 04:19:42.640
time steps

04:19:42.640 --> 04:19:44.640
they don't directly affect how loud each voice is

04:19:44.640 --> 04:19:46.640
so in sum

04:19:46.640 --> 04:19:48.640
multi head attention allows us to focus on

04:19:48.640 --> 04:19:50.640
different parts of the conversation

04:19:50.640 --> 04:19:52.640
and scaling helps us to hear

04:19:52.640 --> 04:19:54.640
all parts of the conversation evenly

04:19:54.640 --> 04:19:56.640
allowing us to understand

04:19:56.640 --> 04:19:58.640
the overall conversation better

04:19:58.640 --> 04:20:00.640
so hopefully that helps you understand exactly

04:20:00.640 --> 04:20:02.640
what this scaling is doing

04:20:02.640 --> 04:20:04.640
so now let's go into the rest of this here

04:20:06.640 --> 04:20:08.640
so we have this scaling applied

04:20:08.640 --> 04:20:10.640
for our head size

04:20:10.640 --> 04:20:12.640
our head size dimension

04:20:12.640 --> 04:20:14.640
we're doing this

04:20:14.640 --> 04:20:16.640
dot product matrix multiplication

04:20:16.640 --> 04:20:18.640
here we get our B by T by T

04:20:18.640 --> 04:20:20.640
and then what is this

04:20:20.640 --> 04:20:22.640
masked fill doing

04:20:22.640 --> 04:20:24.640
so let me help you illustrate this here

04:20:24.640 --> 04:20:26.640
so mask fill

04:20:26.640 --> 04:20:28.640
is essentially

04:20:28.640 --> 04:20:30.640
we'll say block size

04:20:30.640 --> 04:20:32.640
is 3 here alright

04:20:32.640 --> 04:20:34.640
so we have

04:20:34.640 --> 04:20:36.640
initially

04:20:36.640 --> 04:20:38.640
like a 1

04:20:38.640 --> 04:20:40.640
a 0.6

04:20:40.640 --> 04:20:42.640
and then like a 0.4

04:20:42.640 --> 04:20:44.640
then our next one is

04:20:46.640 --> 04:20:48.640
yeah we'll just say all of these are the same

04:20:50.640 --> 04:20:52.640
so essentially

04:20:52.640 --> 04:20:54.640
in our first one

04:20:54.640 --> 04:20:56.640
we want to mask out everything

04:20:56.640 --> 04:20:58.640
except for the first time step

04:20:58.640 --> 04:21:00.640
and then when we advance one

04:21:00.640 --> 04:21:02.640
so let's just change this here back to 0

04:21:06.640 --> 04:21:08.640
when we go on to the next time step

04:21:08.640 --> 04:21:10.640
we want to expose the next piece

04:21:10.640 --> 04:21:12.640
so 0.6 I believe it was

04:21:12.640 --> 04:21:14.640
and then a 0 again

04:21:14.640 --> 04:21:16.640
and then when we expose the next time step after that

04:21:16.640 --> 04:21:18.640
we want to expose all of them

04:21:18.640 --> 04:21:20.640
so just kind of what this means is

04:21:20.640 --> 04:21:22.640
as we

04:21:22.640 --> 04:21:24.640
as the time step advances

04:21:24.640 --> 04:21:26.640
in this sort of I guess vertical

04:21:26.640 --> 04:21:28.640
part

04:21:28.640 --> 04:21:30.640
is every time this steps 1

04:21:30.640 --> 04:21:32.640
we just want to expose one more token

04:21:32.640 --> 04:21:34.640
or one more

04:21:34.640 --> 04:21:36.640
and then we'll use sort of in like a staircase format

04:21:36.640 --> 04:21:38.640
so

04:21:38.640 --> 04:21:40.640
essentially what this mask fill is doing

04:21:40.640 --> 04:21:42.640
is it's making this

04:21:42.640 --> 04:21:44.640
T by T so block size by block size

04:21:44.640 --> 04:21:46.640
and

04:21:46.640 --> 04:21:48.640
for each of these values we're going to set them

04:21:48.640 --> 04:21:50.640
to negative infinity

04:21:50.640 --> 04:21:52.640
so for each value that's 0

04:21:52.640 --> 04:21:54.640
we're going to make that the float value negative infinity

04:21:54.640 --> 04:21:56.640
so it's going to look like this

04:21:56.640 --> 04:21:58.640
negative infinity

04:22:00.640 --> 04:22:02.640
negative infinity

04:22:04.640 --> 04:22:06.640
just like that

04:22:06.640 --> 04:22:08.640
so essentially what happens after this

04:22:08.640 --> 04:22:10.640
is our softmax

04:22:10.640 --> 04:22:12.640
is going to take these values

04:22:12.640 --> 04:22:14.640
and it's going to exponentiate normalize them

04:22:14.640 --> 04:22:16.640
we already went over the soft

04:22:16.640 --> 04:22:18.640
softmax previously

04:22:18.640 --> 04:22:20.640
but

04:22:20.640 --> 04:22:22.640
essentially what this is going to do this

04:22:22.640 --> 04:22:24.640
this last dimension here

04:22:24.640 --> 04:22:26.640
concatenate

04:22:26.640 --> 04:22:28.640
or not concatenate

04:22:28.640 --> 04:22:30.640
rather apply the softmax along the last dimension

04:22:30.640 --> 04:22:32.640
is it's going to do that

04:22:32.640 --> 04:22:34.640
in this sort of horizontal here

04:22:34.640 --> 04:22:36.640
so this last

04:22:36.640 --> 04:22:38.640
this last T

04:22:38.640 --> 04:22:40.640
it's like blocks

04:22:40.640 --> 04:22:42.640
it's like block size by block size

04:22:42.640 --> 04:22:44.640
so it's like we'll say

04:22:44.640 --> 04:22:46.640
T1 and T2

04:22:46.640 --> 04:22:48.640
each of these being like the block size

04:22:48.640 --> 04:22:50.640
we're just going to do it to this last T2 here

04:22:50.640 --> 04:22:52.640
and this horizontal is T2

04:22:52.640 --> 04:22:54.640
so

04:22:54.640 --> 04:22:56.640
hopefully that makes sense

04:22:56.640 --> 04:22:58.640
and essentially

04:22:58.640 --> 04:23:00.640
what this exponentiation is going to do

04:23:00.640 --> 04:23:02.640
is it's going to turn these values to 0

04:23:02.640 --> 04:23:04.640
and

04:23:04.640 --> 04:23:06.640
this one is obviously going to remain a 1

04:23:06.640 --> 04:23:08.640
and then

04:23:08.640 --> 04:23:10.640
it's going to turn these

04:23:10.640 --> 04:23:12.640
into 0

04:23:12.640 --> 04:23:14.640
and it's going to probably sharpen this 1 here

04:23:14.640 --> 04:23:16.640
so this 1 is going to be more significant

04:23:16.640 --> 04:23:18.640
it's going to grow more than the 0.6

04:23:18.640 --> 04:23:20.640
because we're exponentiating

04:23:20.640 --> 04:23:22.640
and then same here so this 1 is going to be

04:23:22.640 --> 04:23:24.640
very, very sharp

04:23:24.640 --> 04:23:26.640
compared to 0.6

04:23:26.640 --> 04:23:28.640
or 0.4

04:23:28.640 --> 04:23:30.640
that's what the softmax does

04:23:30.640 --> 04:23:32.640
essentially the point of the softmax function

04:23:32.640 --> 04:23:34.640
is to

04:23:34.640 --> 04:23:36.640
make the values stand out more

04:23:36.640 --> 04:23:38.640
it's to make the model more confident

04:23:38.640 --> 04:23:40.640
in highlighting attention scores

04:23:40.640 --> 04:23:42.640
so when you have one value that's like very big

04:23:42.640 --> 04:23:44.640
but not too big, not exploding

04:23:44.640 --> 04:23:46.640
because of our scaling, right?

04:23:46.640 --> 04:23:48.640
we want to keep a minor scaling

04:23:48.640 --> 04:23:50.640
but when a value is big, when a score

04:23:50.640 --> 04:23:52.640
or attention score is very big

04:23:52.640 --> 04:23:54.640
we want the model to put a lot of focus on that

04:23:54.640 --> 04:23:56.640
and to say this

04:23:56.640 --> 04:23:58.640
the entire sentence or the entire thing of tokens

04:23:58.640 --> 04:24:00.640
and we just want it to learn the most

04:24:00.640 --> 04:24:02.640
from that

04:24:02.640 --> 04:24:04.640
so essentially that's what softmax is doing

04:24:04.640 --> 04:24:06.640
instead of just a normal

04:24:06.640 --> 04:24:08.640
normalizing mechanism

04:24:08.640 --> 04:24:10.640
it's just doing some exponentiation

04:24:10.640 --> 04:24:12.640
to that to make the model more confident

04:24:12.640 --> 04:24:14.640
in its predictions

04:24:14.640 --> 04:24:16.640
so this will help us score better

04:24:16.640 --> 04:24:18.640
in the long run if we just

04:24:18.640 --> 04:24:20.640
highlight what tokens

04:24:20.640 --> 04:24:22.640
and what attention scores are more important

04:24:22.640 --> 04:24:24.640
in the sequence

04:24:24.640 --> 04:24:26.640
and then after this

04:24:26.640 --> 04:24:28.640
softmax here

04:24:28.640 --> 04:24:30.640
we just apply a simple dropout

04:24:30.640 --> 04:24:32.640
on this way variable

04:24:32.640 --> 04:24:34.640
this new

04:24:34.640 --> 04:24:36.640
calculated way

04:24:36.640 --> 04:24:38.640
scale.product.attention

04:24:38.640 --> 04:24:40.640
masked

04:24:40.640 --> 04:24:42.640
and then softmaxed

04:24:42.640 --> 04:24:44.640
we apply a dropout on that

04:24:44.640 --> 04:24:46.640
and then we perform our final

04:24:46.640 --> 04:24:48.640
weighted aggregation

04:24:48.640 --> 04:24:50.640
so this v

04:24:50.640 --> 04:24:52.640
multiplied by the output of the softmax

04:24:52.640 --> 04:24:54.640
cool

04:24:54.640 --> 04:24:56.640
so we get this v

04:24:56.640 --> 04:24:58.640
self.value of x so we just multiply that

04:24:58.640 --> 04:25:00.640
a little pointer

04:25:00.640 --> 04:25:02.640
I wanted to add

04:25:02.640 --> 04:25:04.640
to this

04:25:04.640 --> 04:25:06.640
module list

04:25:06.640 --> 04:25:08.640
module list here

04:25:08.640 --> 04:25:10.640
and then our

04:25:10.640 --> 04:25:12.640
go

04:25:14.640 --> 04:25:16.640
yes our sequential network here

04:25:16.640 --> 04:25:18.640
so we have this

04:25:18.640 --> 04:25:20.640
sequential number of blocks here

04:25:20.640 --> 04:25:22.640
for n layers

04:25:22.640 --> 04:25:24.640
and we have our module

04:25:24.640 --> 04:25:26.640
list so what really is the difference here

04:25:26.640 --> 04:25:28.640
well

04:25:28.640 --> 04:25:30.640
module list is not the same as n and not sequential

04:25:30.640 --> 04:25:32.640
in terms of the

04:25:32.640 --> 04:25:34.640
asterisk usage that we see

04:25:36.640 --> 04:25:38.640
in the language model class

04:25:38.640 --> 04:25:40.640
module list doesn't run one layer

04:25:40.640 --> 04:25:42.640
or head after another

04:25:42.640 --> 04:25:44.640
but rather each is

04:25:44.640 --> 04:25:46.640
isolated and gets its own unique perspective

04:25:46.640 --> 04:25:48.640
sequential processing

04:25:48.640 --> 04:25:50.640
is where one block depends on another

04:25:50.640 --> 04:25:52.640
to synchronously complete

04:25:52.640 --> 04:25:54.640
so that means we're waiting on one

04:25:54.640 --> 04:25:56.640
to finish before we move on to the next

04:25:56.640 --> 04:25:58.640
so they're not completing asynchronously

04:25:58.640 --> 04:26:00.640
or in parallel

04:26:00.640 --> 04:26:02.640
so the multiple heads in a transformer model

04:26:02.640 --> 04:26:04.640
operate independently

04:26:04.640 --> 04:26:06.640
and their computations can be processed

04:26:06.640 --> 04:26:08.640
in parallel however this parallel

04:26:08.640 --> 04:26:10.640
parallelism isn't due

04:26:10.640 --> 04:26:12.640
to the module list that stores

04:26:12.640 --> 04:26:14.640
the heads

04:26:14.640 --> 04:26:16.640
instead

04:26:16.640 --> 04:26:18.640
it's because of how

04:26:18.640 --> 04:26:20.640
the computation are structured

04:26:20.640 --> 04:26:22.640
to take advantage of the GPU's capabilities

04:26:22.640 --> 04:26:24.640
for simultaneous

04:26:24.640 --> 04:26:26.640
computation

04:26:26.640 --> 04:26:28.640
and this is also how the deep learning

04:26:28.640 --> 04:26:30.640
framework PyTorch

04:26:30.640 --> 04:26:32.640
interfaces with the GPU

04:26:32.640 --> 04:26:34.640
so this isn't particularly something we have to worry

04:26:34.640 --> 04:26:36.640
about too much but

04:26:36.640 --> 04:26:38.640
you could supposedly think

04:26:38.640 --> 04:26:40.640
that these are sort of running in parallel

04:26:40.640 --> 04:26:42.640
yeah

04:26:42.640 --> 04:26:44.640
so if you want to get into hardware

04:26:44.640 --> 04:26:46.640
then that's like your whole realm there

04:26:46.640 --> 04:26:48.640
but this is PyTorch, this is software

04:26:48.640 --> 04:26:50.640
not hardware at all

04:26:50.640 --> 04:26:52.640
I don't expect you have to have any hardware

04:26:52.640 --> 04:26:54.640
knowledge about GPU, CPU

04:26:54.640 --> 04:26:56.640
anything like that

04:26:56.640 --> 04:26:58.640
anyways that's just kind of a background

04:26:58.640 --> 04:27:00.640
of what's going on there

04:27:00.640 --> 04:27:02.640
so cool

04:27:02.640 --> 04:27:04.640
so let's actually go over what is going on

04:27:04.640 --> 04:27:06.640
from the ground up here

04:27:06.640 --> 04:27:08.640
so we have this

04:27:08.640 --> 04:27:10.640
GPT language model

04:27:10.640 --> 04:27:12.640
we got our token embeddings, positional embeddings

04:27:12.640 --> 04:27:14.640
we have these sequential blocks

04:27:14.640 --> 04:27:16.640
initialize our weights

04:27:16.640 --> 04:27:18.640
for each of these blocks

04:27:18.640 --> 04:27:20.640
we have a

04:27:20.640 --> 04:27:22.640
this class block

04:27:22.640 --> 04:27:24.640
so we get a head size parameter

04:27:24.640 --> 04:27:26.640
which is n embedded of 384

04:27:26.640 --> 04:27:28.640
divided by n heads which is 4

04:27:28.640 --> 04:27:30.640
so we get 96 from that

04:27:30.640 --> 04:27:32.640
that's the number of features we're capturing

04:27:32.640 --> 04:27:34.640
self-attention

04:27:34.640 --> 04:27:36.640
we do a feed forward to layer norms

04:27:36.640 --> 04:27:38.640
self-attention, layer norm

04:27:38.640 --> 04:27:40.640
feed forward

04:27:40.640 --> 04:27:42.640
layer norm

04:27:42.640 --> 04:27:44.640
in the post norm architecture

04:27:44.640 --> 04:27:46.640
then we do a feed forward

04:27:46.640 --> 04:27:48.640
just a linear

04:27:48.640 --> 04:27:50.640
followed by a relu followed by a linear

04:27:50.640 --> 04:27:52.640
and then dropping that out

04:27:52.640 --> 04:27:54.640
and then we have our multi-head attention

04:27:54.640 --> 04:27:56.640
which just sort of structured

04:27:56.640 --> 04:27:58.640
these attention heads

04:27:58.640 --> 04:28:00.640
running in parallel and then concatenates the results

04:28:00.640 --> 04:28:02.640
and then for each of these heads

04:28:02.640 --> 04:28:04.640
we have our keys, queries

04:28:04.640 --> 04:28:06.640
and values

04:28:06.640 --> 04:28:08.640
we register a model state

04:28:08.640 --> 04:28:10.640
to prevent overhead computation

04:28:10.640 --> 04:28:12.640
excessively

04:28:12.640 --> 04:28:14.640
then we just

04:28:14.640 --> 04:28:16.640
do our scale dot product attention

04:28:16.640 --> 04:28:18.640
in this line, we do our mast field

04:28:18.640 --> 04:28:20.640
to prevent look ahead

04:28:20.640 --> 04:28:22.640
we do our softmax to make our values

04:28:22.640 --> 04:28:24.640
sharper and to make some of them stand out

04:28:24.640 --> 04:28:26.640
and then

04:28:26.640 --> 04:28:28.640
we do a drop out finally

04:28:28.640 --> 04:28:30.640
on that and just some weighted aggregation

04:28:30.640 --> 04:28:32.640
we do our weights

04:28:32.640 --> 04:28:34.640
this final

04:28:34.640 --> 04:28:36.640
weight variable

04:28:36.640 --> 04:28:38.640
multiplied by our

04:28:38.640 --> 04:28:40.640
weighted value

04:28:40.640 --> 04:28:42.640
from this

04:28:42.640 --> 04:28:44.640
initially this linear transformation

04:28:44.640 --> 04:28:46.640
so cool, that's what's happening

04:28:46.640 --> 04:28:48.640
step by step

04:28:48.640 --> 04:28:50.640
in this GBT architecture

04:28:50.640 --> 04:28:52.640
amazing

04:28:52.640 --> 04:28:54.640
give yourself a good pat on the back

04:28:54.640 --> 04:28:56.640
go grab some coffee, do whatever you need to do

04:28:56.640 --> 04:28:58.640
even get some sleep

04:28:58.640 --> 04:29:00.640
and get ready for the next section

04:29:00.640 --> 04:29:02.640
so there's actually another hyper parameter

04:29:02.640 --> 04:29:04.640
I forgot to add

04:29:04.640 --> 04:29:06.640
which is n layer

04:29:06.640 --> 04:29:08.640
and n layer

04:29:08.640 --> 04:29:10.640
is essentially equal to 4

04:29:10.640 --> 04:29:12.640
n layer is essentially equal to

04:29:12.640 --> 04:29:14.640
the number

04:29:14.640 --> 04:29:16.640
of decoder blocks

04:29:16.640 --> 04:29:18.640
we have

04:29:18.640 --> 04:29:20.640
so instead of n block we just say n layers

04:29:20.640 --> 04:29:22.640
doesn't really matter what it's called

04:29:22.640 --> 04:29:24.640
but that's what it means

04:29:24.640 --> 04:29:26.640
and then number of heads is how many heads

04:29:26.640 --> 04:29:28.640
we have running theoretically in parallel

04:29:28.640 --> 04:29:30.640
and then n embed

04:29:30.640 --> 04:29:32.640
is the number of total

04:29:32.640 --> 04:29:34.640
dimensions we want to capture

04:29:34.640 --> 04:29:36.640
from all the heads concatenated together

04:29:36.640 --> 04:29:38.640
type of thing, we already went over that, so cool

04:29:38.640 --> 04:29:40.640
hyper parameters

04:29:40.640 --> 04:29:42.640
block size, sequence length

04:29:42.640 --> 04:29:44.640
batch sizes, how many

04:29:44.640 --> 04:29:46.640
of these do we want at the same time

04:29:46.640 --> 04:29:48.640
max itters is just training

04:29:48.640 --> 04:29:50.640
how many iterations we want to do

04:29:50.640 --> 04:29:52.640
learning rate is

04:29:52.640 --> 04:29:54.640
what we covered that in

04:29:54.640 --> 04:29:56.640
actually the Desmos calculator that I showed

04:29:56.640 --> 04:29:58.640
a little while back

04:29:58.640 --> 04:30:00.640
just showing how

04:30:00.640 --> 04:30:02.640
we update the model weights based on the derivative

04:30:02.640 --> 04:30:04.640
of the loss function

04:30:04.640 --> 04:30:06.640
and then

04:30:06.640 --> 04:30:08.640
validators

04:30:08.640 --> 04:30:10.640
which was just reporting the loss

04:30:10.640 --> 04:30:12.640
and then lastly the

04:30:12.640 --> 04:30:14.640
dropout which is dropping out

04:30:14.640 --> 04:30:16.640
0.2 or 20%

04:30:16.640 --> 04:30:18.640
of the total neurons

04:30:18.640 --> 04:30:20.640
so awesome

04:30:20.640 --> 04:30:22.640
that's pretty cool, let's go ahead and jump into

04:30:22.640 --> 04:30:24.640
some data stuff

04:30:24.640 --> 04:30:26.640
I'm going to pull out a paper here

04:30:26.640 --> 04:30:28.640
so let's just make sure everything works here

04:30:28.640 --> 04:30:30.640
and then we're actually going to download our data

04:30:30.640 --> 04:30:32.640
so I want to try to run some iterations

04:30:32.640 --> 04:30:34.640
and

04:30:34.640 --> 04:30:36.640
just make sure that our, actually I made some changes

04:30:38.640 --> 04:30:40.640
pretty much this was

04:30:40.640 --> 04:30:42.640
weird and didn't work so I just changed

04:30:42.640 --> 04:30:44.640
this around to

04:30:44.640 --> 04:30:46.640
making our characters empty

04:30:46.640 --> 04:30:48.640
opening this text file

04:30:48.640 --> 04:30:50.640
opening it

04:30:50.640 --> 04:30:52.640
storing it in a variable

04:30:52.640 --> 04:30:54.640
format

04:30:54.640 --> 04:30:56.640
and then just making our vocab

04:30:56.640 --> 04:30:58.640
this sorted list

04:30:58.640 --> 04:31:00.640
set of our text

04:31:00.640 --> 04:31:02.640
and then just making the vocab

04:31:02.640 --> 04:31:04.640
size the length of that

04:31:04.640 --> 04:31:06.640
so let's go ahead and actually run

04:31:06.640 --> 04:31:08.640
this through, I did change the block size

04:31:08.640 --> 04:31:10.640
to 64 batch size 128

04:31:10.640 --> 04:31:12.640
some other hype parameters here

04:31:12.640 --> 04:31:14.640
so

04:31:14.640 --> 04:31:16.640
honestly the block size and batch size will depend

04:31:16.640 --> 04:31:18.640
on your

04:31:18.640 --> 04:31:20.640
computational resources

04:31:20.640 --> 04:31:22.640
so

04:31:22.640 --> 04:31:24.640
just experiment with these

04:31:24.640 --> 04:31:26.640
I'm just going to try these out first

04:31:26.640 --> 04:31:28.640
just to show you guys what this looks like

04:31:32.640 --> 04:31:34.640
okay

04:31:34.640 --> 04:31:36.640
so it looks like we're getting idx is not defined

04:31:36.640 --> 04:31:38.640
where could that be

04:31:38.640 --> 04:31:40.640
okay yep

04:31:40.640 --> 04:31:42.640
so this is

04:31:42.640 --> 04:31:44.640
we could just change that

04:31:44.640 --> 04:31:46.640
it's just saying idx is not defined

04:31:46.640 --> 04:31:48.640
we're using index here idx there so that should work now

04:31:50.640 --> 04:31:52.640
and we're getting local variable

04:31:52.640 --> 04:31:54.640
t reference before assignment

04:31:54.640 --> 04:31:56.640
okay so

04:31:56.640 --> 04:31:58.640
we have some

04:31:58.640 --> 04:32:00.640
we have t here and then we initialize

04:32:00.640 --> 04:32:02.640
t there so let's just bring up

04:32:04.640 --> 04:32:06.640
up to there cool

04:32:06.640 --> 04:32:08.640
now let's try and run this

04:32:08.640 --> 04:32:10.640
oh shape is invalid

04:32:10.640 --> 04:32:12.640
for input size of

04:32:12.640 --> 04:32:14.640
okay let's see what we got

04:32:14.640 --> 04:32:16.640
it turns out we don't actually need

04:32:16.640 --> 04:32:18.640
two token embedding tables a little bit of a selling

04:32:18.640 --> 04:32:20.640
mistake but we don't need two of those

04:32:20.640 --> 04:32:22.640
so I'll just delete that

04:32:22.640 --> 04:32:24.640
and then

04:32:24.640 --> 04:32:26.640
what I'm going to do is go ahead and run this

04:32:26.640 --> 04:32:28.640
again let's see a new error

04:32:28.640 --> 04:32:30.640
local variable t reference before assignment

04:32:30.640 --> 04:32:32.640
okay so our

04:32:32.640 --> 04:32:34.640
t is referenced here

04:32:34.640 --> 04:32:36.640
and well how can we initialize this

04:32:36.640 --> 04:32:38.640
what we can do is we could take this index

04:32:38.640 --> 04:32:40.640
here of shape

04:32:40.640 --> 04:32:42.640
b by t because it goes b by t

04:32:42.640 --> 04:32:44.640
plus 1 etc and just keeps growing

04:32:44.640 --> 04:32:46.640
so we could actually unpack that

04:32:46.640 --> 04:32:48.640
so we could go b

04:32:48.640 --> 04:32:50.640
b and

04:32:50.640 --> 04:32:52.640
t is going to be index

04:32:52.640 --> 04:32:54.640
that shape just unpack that

04:32:54.640 --> 04:32:56.640
so cool

04:32:56.640 --> 04:32:58.640
so now we're going to run this training

04:32:58.640 --> 04:33:00.640
loop and

04:33:00.640 --> 04:33:02.640
it looks like it's working so far

04:33:02.640 --> 04:33:04.640
so that's amazing

04:33:04.640 --> 04:33:06.640
super cool

04:33:06.640 --> 04:33:08.640
step 0 train last

04:33:08.640 --> 04:33:10.640
4.4 that's actually a pretty good training

04:33:10.640 --> 04:33:12.640
loss overall so

04:33:12.640 --> 04:33:14.640
we'll come back after this is done

04:33:14.640 --> 04:33:16.640
I've set it to train

04:33:16.640 --> 04:33:18.640
for

04:33:18.640 --> 04:33:20.640
3,000 iterations printing every 500 iterations

04:33:20.640 --> 04:33:22.640
so we'll just see

04:33:22.640 --> 04:33:24.640
the loss six times over this entire

04:33:24.640 --> 04:33:26.640
training process

04:33:26.640 --> 04:33:28.640
or we should

04:33:28.640 --> 04:33:30.640
I don't know why it's going to 100

04:33:30.640 --> 04:33:32.640
eval itters

04:33:34.640 --> 04:33:36.640
eval itters

04:33:36.640 --> 04:33:38.640
estimate loss is

04:33:40.640 --> 04:33:42.640
okay so we don't actually need

04:33:42.640 --> 04:33:44.640
eval interval

04:33:46.640 --> 04:33:48.640
we'll just make this

04:33:48.640 --> 04:33:50.640
sure why not 100

04:33:50.640 --> 04:33:52.640
we'll keep that

04:33:52.640 --> 04:33:54.640
and it's just going to keep going here

04:33:54.640 --> 04:33:56.640
we'll see our loss over time

04:33:56.640 --> 04:33:58.640
it's going to get smaller so

04:33:58.640 --> 04:34:00.640
I'll come back when that's done

04:34:00.640 --> 04:34:02.640
as for the data we're going to be using

04:34:02.640 --> 04:34:04.640
the open web text corpus

04:34:04.640 --> 04:34:06.640
and

04:34:06.640 --> 04:34:08.640
let's just go down here

04:34:08.640 --> 04:34:10.640
so this is a paper called

04:34:10.640 --> 04:34:12.640
survey

04:34:12.640 --> 04:34:14.640
survey of large language models

04:34:14.640 --> 04:34:16.640
so I'll just go back to open

04:34:16.640 --> 04:34:18.640
web

04:34:18.640 --> 04:34:20.640
text

04:34:20.640 --> 04:34:22.640
where that is

04:34:22.640 --> 04:34:24.640
up

04:34:24.640 --> 04:34:26.640
it's just fine

04:34:26.640 --> 04:34:28.640
so open web text

04:34:28.640 --> 04:34:30.640
this is consisted of a bunch of reddit links

04:34:30.640 --> 04:34:32.640
or just reddit upvotes

04:34:32.640 --> 04:34:34.640
so if you go and reddit and you see

04:34:34.640 --> 04:34:36.640
a bunch of those

04:34:36.640 --> 04:34:38.640
posts that are highly upvoted

04:34:38.640 --> 04:34:40.640
or downvoted

04:34:40.640 --> 04:34:42.640
they're pretty much those

04:34:42.640 --> 04:34:44.640
pieces of text are valuable

04:34:44.640 --> 04:34:46.640
and they contain things that we can train them

04:34:46.640 --> 04:34:48.640
so

04:34:48.640 --> 04:34:50.640
pretty much web text is

04:34:50.640 --> 04:34:52.640
just a corpus of all these upvoted links

04:34:52.640 --> 04:34:54.640
but it's not publicly available

04:34:54.640 --> 04:34:56.640
so somebody created an open source version

04:34:56.640 --> 04:34:58.640
called

04:34:58.640 --> 04:35:00.640
open web text

04:35:00.640 --> 04:35:02.640
hence open

04:35:02.640 --> 04:35:04.640
and it's pretty much as an open version of this

04:35:04.640 --> 04:35:06.640
so we're going to download that

04:35:06.640 --> 04:35:08.640
for a here like common crawl which is

04:35:08.640 --> 04:35:10.640
really really big so like

04:35:10.640 --> 04:35:12.640
petabyte scale data volume

04:35:12.640 --> 04:35:14.640
you have a bunch of books

04:35:14.640 --> 04:35:16.640
so this is a good paper to read over

04:35:16.640 --> 04:35:18.640
it's just called

04:35:18.640 --> 04:35:20.640
a survey

04:35:20.640 --> 04:35:22.640
of large language models you can search this up

04:35:22.640 --> 04:35:24.640
and it'll come up you can just download the pdf for

04:35:24.640 --> 04:35:26.640
so this is a really nice paper

04:35:26.640 --> 04:35:28.640
read over that if you'd like

04:35:28.640 --> 04:35:30.640
but anyways

04:35:30.640 --> 04:35:32.640
this is a download link for this open web text corpus

04:35:32.640 --> 04:35:34.640
so just go to this link

04:35:34.640 --> 04:35:36.640
I have it in the github repo

04:35:36.640 --> 04:35:38.640
and you just go to download

04:35:38.640 --> 04:35:40.640
and it'll bring you to this drive

04:35:40.640 --> 04:35:42.640
so you can go in and right click this

04:35:42.640 --> 04:35:44.640
and just hit download

04:35:44.640 --> 04:35:46.640
it'll say 12 gigabytes exceeds

04:35:46.640 --> 04:35:48.640
maximum file size that it can scan so it's like

04:35:48.640 --> 04:35:50.640
this might have a virus

04:35:50.640 --> 04:35:52.640
don't worry it doesn't have a virus this is actually

04:35:52.640 --> 04:35:54.640
created by a researcher so

04:35:54.640 --> 04:35:56.640
not really bad people are in charge of

04:35:56.640 --> 04:35:58.640
creating text corpora

04:35:58.640 --> 04:36:00.640
so go in and download anyway

04:36:00.640 --> 04:36:02.640
I've actually already downloaded this

04:36:02.640 --> 04:36:04.640
so

04:36:04.640 --> 04:36:06.640
yeah I'll come back

04:36:06.640 --> 04:36:08.640
when our training

04:36:08.640 --> 04:36:10.640
is actually done here

04:36:10.640 --> 04:36:12.640
so I'm actually going to stop here iteration

04:36:12.640 --> 04:36:14.640
2000 because

04:36:14.640 --> 04:36:16.640
we're not actually getting that much amazing progress

04:36:16.640 --> 04:36:18.640
and the reason for this is because

04:36:18.640 --> 04:36:20.640
our hyper parameters

04:36:20.640 --> 04:36:22.640
so batch size and block size

04:36:22.640 --> 04:36:24.640
I mean these are okay

04:36:24.640 --> 04:36:26.640
but we might want to change up as our learning rate

04:36:26.640 --> 04:36:28.640
so some combinations of learning rates that are really useful

04:36:28.640 --> 04:36:30.640
is like

04:36:30.640 --> 04:36:32.640
3e to the negative 3

04:36:32.640 --> 04:36:34.640
you go 3e to the

04:36:34.640 --> 04:36:36.640
negative 4

04:36:36.640 --> 04:36:38.640
you go 1e to the negative 3

04:36:38.640 --> 04:36:40.640
1e

04:36:40.640 --> 04:36:42.640
1e to the negative 4

04:36:42.640 --> 04:36:44.640
so these are all learning rates that I like to play around with

04:36:44.640 --> 04:36:46.640
these are just sort of common ones

04:36:46.640 --> 04:36:48.640
it's up to you if you want to use them or not but

04:36:48.640 --> 04:36:50.640
what I might do actually

04:36:50.640 --> 04:36:52.640
is just downgrade to 3e to the negative 4

04:36:52.640 --> 04:36:54.640
and we'll retest it

04:36:54.640 --> 04:36:56.640
as well I'm going to bump up the

04:36:56.640 --> 04:36:58.640
the number of heads

04:36:58.640 --> 04:37:00.640
and the number of layers

04:37:00.640 --> 04:37:02.640
so that we can capture more

04:37:02.640 --> 04:37:04.640
complex relationships in the text

04:37:04.640 --> 04:37:06.640
thus having it learn more

04:37:06.640 --> 04:37:08.640
so I'm going to change each of these

04:37:08.640 --> 04:37:10.640
to 8

04:37:10.640 --> 04:37:12.640
go 8

04:37:12.640 --> 04:37:14.640
actually

04:37:14.640 --> 04:37:16.640
kernel will go

04:37:16.640 --> 04:37:18.640
restart

04:37:20.640 --> 04:37:22.640
now we'll just run this from the top

04:37:28.640 --> 04:37:30.640
and

04:37:32.640 --> 04:37:34.640
and we'll run that

04:37:34.640 --> 04:37:36.640
cool

04:37:36.640 --> 04:37:38.640
so let's see

04:37:38.640 --> 04:37:40.640
what we actually start off with and what our loss looks like over time

04:37:50.640 --> 04:37:52.640
cool

04:37:52.640 --> 04:37:54.640
so we got step 1 4.5 about the same as last time

04:37:54.640 --> 04:37:56.640
it's like 0.2 off

04:37:56.640 --> 04:37:58.640
or something so it's pretty close

04:37:58.640 --> 04:38:00.640
let's see the next iteration here

04:38:26.640 --> 04:38:28.640
that's wonderful

04:38:28.640 --> 04:38:30.640
so before we were getting like 3.1 ish

04:38:30.640 --> 04:38:32.640
or something around that range 3.15

04:38:32.640 --> 04:38:34.640
now we're getting 2.2

04:38:34.640 --> 04:38:36.640
so you can see that

04:38:36.640 --> 04:38:38.640
as we change hyper parameters

04:38:38.640 --> 04:38:40.640
we can actually see a significant change

04:38:40.640 --> 04:38:42.640
in our loss

04:38:42.640 --> 04:38:44.640
this is amazing

04:38:44.640 --> 04:38:46.640
this is just to sort of prove how cool hyper

04:38:46.640 --> 04:38:48.640
parameters are and what they do for you

04:38:48.640 --> 04:38:50.640
so

04:38:50.640 --> 04:38:52.640
let's start

04:38:52.640 --> 04:38:54.640
changing around some data stuff

04:38:54.640 --> 04:38:56.640
this right here is the Wizard of Oz text

04:38:56.640 --> 04:38:58.640
just a simple text file

04:38:58.640 --> 04:39:00.640
it's the size isn't

04:39:00.640 --> 04:39:02.640
super large

04:39:02.640 --> 04:39:04.640
so we can actually open it all into ram at once

04:39:04.640 --> 04:39:06.640
but

04:39:06.640 --> 04:39:08.640
if we were to use the open web text

04:39:08.640 --> 04:39:10.640
we cannot actually read

04:39:10.640 --> 04:39:12.640
you know 45 gigabytes of

04:39:12.640 --> 04:39:14.640
utfa text in ram at once

04:39:14.640 --> 04:39:16.640
just can't do that unless you have like maybe

04:39:16.640 --> 04:39:18.640
64 or 128 gigabytes

04:39:18.640 --> 04:39:20.640
of ram this is really just

04:39:20.640 --> 04:39:22.640
not feasible at all

04:39:22.640 --> 04:39:24.640
so

04:39:24.640 --> 04:39:26.640
we're going to do some data pre-processing

04:39:26.640 --> 04:39:28.640
here some data cleaning

04:39:28.640 --> 04:39:30.640
and then just a way to simply load

04:39:30.640 --> 04:39:32.640
data into the

04:39:32.640 --> 04:39:34.640
GPT so let's go ahead and do that

04:39:34.640 --> 04:39:36.640
so the model has actually gotten really good at predicting

04:39:36.640 --> 04:39:38.640
the next token as you can see

04:39:38.640 --> 04:39:40.640
the train loss here is 1.01

04:39:40.640 --> 04:39:42.640
so let's actually

04:39:42.640 --> 04:39:44.640
find

04:39:44.640 --> 04:39:46.640
what the prediction accuracy of that is

04:39:46.640 --> 04:39:48.640
so I might just go into GPT-4

04:39:48.640 --> 04:39:50.640
here

04:39:50.640 --> 04:39:52.640
and

04:39:52.640 --> 04:39:54.640
just ask it

04:39:54.640 --> 04:39:56.640
what is

04:39:56.640 --> 04:39:58.640
the prediction

04:39:58.640 --> 04:40:00.640
accuracy

04:40:00.640 --> 04:40:02.640
of

04:40:02.640 --> 04:40:04.640
loss 1.01

04:40:06.640 --> 04:40:08.640
the loss value

04:40:08.640 --> 04:40:10.640
comes with a loss function during the pre-process

04:40:10.640 --> 04:40:12.640
okay so let's

04:40:12.640 --> 04:40:14.640
see

04:40:16.640 --> 04:40:18.640
cross entropy loss

04:40:18.640 --> 04:40:20.640
doesn't mean the model is 99% accurate

04:40:22.640 --> 04:40:24.640
okay

04:40:24.640 --> 04:40:26.640
so

04:40:26.640 --> 04:40:28.640
that pretty much means that the model is really accurate

04:40:28.640 --> 04:40:30.640
but I want to find a value here

04:40:30.640 --> 04:40:32.640
so

04:40:32.640 --> 04:40:34.640
if the

04:40:34.640 --> 04:40:36.640
we'll go to Wolfram alpha

04:40:38.640 --> 04:40:40.640
and just we'll just guess some values here

04:40:40.640 --> 04:40:42.640
so negative ln

04:40:42.640 --> 04:40:44.640
of let's say

04:40:44.640 --> 04:40:46.640
0.9

04:40:46.640 --> 04:40:48.640
okay so probably not that

04:40:50.640 --> 04:40:52.640
0.3

04:40:52.640 --> 04:40:54.640
0.2

04:40:54.640 --> 04:40:56.640
0.4

04:40:56.640 --> 04:40:58.640
0.35

04:40:58.640 --> 04:41:00.640
yep so the model

04:41:00.640 --> 04:41:02.640
has about a 35% chance

04:41:02.640 --> 04:41:04.640
of guessing the next token as of right now

04:41:04.640 --> 04:41:06.640
so that's actually pretty good

04:41:06.640 --> 04:41:08.640
so 1 in every 3 tokens

04:41:08.640 --> 04:41:10.640
are spot on

04:41:10.640 --> 04:41:12.640
so that is wonderful

04:41:12.640 --> 04:41:14.640
this is converging even more

04:41:14.640 --> 04:41:16.640
we're getting 0.89 so now it's getting like

04:41:16.640 --> 04:41:18.640
every

04:41:18.640 --> 04:41:20.640
40% are being guessed properly

04:41:20.640 --> 04:41:22.640
our validation is not doing

04:41:22.640 --> 04:41:24.640
amazing though

04:41:24.640 --> 04:41:26.640
but we'll linger on that a little bit here

04:41:26.640 --> 04:41:28.640
and you'll see sort of how this changes

04:41:28.640 --> 04:41:30.640
as we scale our data

04:41:30.640 --> 04:41:32.640
but

04:41:32.640 --> 04:41:34.640
so I've installed this

04:41:34.640 --> 04:41:36.640
webtext.tar file

04:41:36.640 --> 04:41:38.640
tar file is interesting

04:41:38.640 --> 04:41:40.640
so in order to actually extract these

04:41:40.640 --> 04:41:42.640
you simply just

04:41:42.640 --> 04:41:44.640
right click on them

04:41:44.640 --> 04:41:46.640
you go extract to

04:41:46.640 --> 04:41:48.640
and then it'll just make a new file here

04:41:48.640 --> 04:41:50.640
so it'll process this

04:41:50.640 --> 04:41:52.640
you have to make sure you have WinRAR or else this might not work

04:41:52.640 --> 04:41:54.640
to the fullest extent

04:41:54.640 --> 04:41:56.640
and yeah

04:41:56.640 --> 04:41:58.640
so we'll just wait for this to finish up here

04:41:58.640 --> 04:42:00.640
we should end up with something that looks like this

04:42:00.640 --> 04:42:02.640
so open webtext

04:42:02.640 --> 04:42:04.640
and inside of here

04:42:04.640 --> 04:42:06.640
you have a bunch of xz files

04:42:06.640 --> 04:42:08.640
cool so there's actually 20,000

04:42:08.640 --> 04:42:10.640
of these so we're gonna have to do a lot of

04:42:10.640 --> 04:42:12.640
there's definitely gonna be some for loops in here for sure

04:42:12.640 --> 04:42:14.640
so

04:42:14.640 --> 04:42:16.640
let's just handle this

04:42:16.640 --> 04:42:18.640
step by step in this data

04:42:18.640 --> 04:42:20.640
extract file

04:42:20.640 --> 04:42:22.640
so first off

04:42:22.640 --> 04:42:24.640
we're gonna need to import some python modules

04:42:24.640 --> 04:42:26.640
we're gonna use OS for interacting with the operating system

04:42:26.640 --> 04:42:28.640
LZMA

04:42:28.640 --> 04:42:30.640
for handling

04:42:30.640 --> 04:42:32.640
xz files which are a type of compressed file

04:42:32.640 --> 04:42:34.640
like 7zip for example

04:42:34.640 --> 04:42:36.640
and then

04:42:36.640 --> 04:42:38.640
TQDM for displaying a progress bar

04:42:38.640 --> 04:42:40.640
so you see a progress bar left to right

04:42:40.640 --> 04:42:42.640
in the terminal

04:42:42.640 --> 04:42:44.640
and that's pretty much gonna show us how quick we are

04:42:44.640 --> 04:42:46.640
at executing the script

04:42:46.640 --> 04:42:48.640
so next up

04:42:48.640 --> 04:42:50.640
we're gonna define a function

04:42:50.640 --> 04:42:52.640
called xz files in dir

04:42:52.640 --> 04:42:54.640
it takes a directory as an input

04:42:54.640 --> 04:42:56.640
returns a list of all of the xz file names

04:42:56.640 --> 04:42:58.640
in that directory

04:42:58.640 --> 04:43:00.640
it's gonna use os.listdir

04:43:00.640 --> 04:43:02.640
to get all the file names

04:43:02.640 --> 04:43:04.640
and os

04:43:04.640 --> 04:43:06.640
path as file

04:43:06.640 --> 04:43:08.640
to check if each one is a file

04:43:08.640 --> 04:43:10.640
and not a directory or

04:43:10.640 --> 04:43:12.640
symbolic link

04:43:12.640 --> 04:43:14.640
if a file name ends with .xz

04:43:14.640 --> 04:43:16.640
and it's a file

04:43:16.640 --> 04:43:18.640
it'll be added to the list

04:43:18.640 --> 04:43:20.640
so we just have a bunch of these files

04:43:20.640 --> 04:43:22.640
each element

04:43:22.640 --> 04:43:24.640
is just the title of each file in there

04:43:24.640 --> 04:43:26.640
so that's pretty much what that does

04:43:26.640 --> 04:43:28.640
and next up here

04:43:28.640 --> 04:43:30.640
we'll set up some variables

04:43:30.640 --> 04:43:32.640
folder path

04:43:32.640 --> 04:43:34.640
it's just gonna be where our xz files are located

04:43:34.640 --> 04:43:36.640
so I'm actually gonna change this here

04:43:36.640 --> 04:43:38.640
because that's an incorrect file path

04:43:38.640 --> 04:43:40.640
but

04:43:42.640 --> 04:43:44.640
yes

04:43:44.640 --> 04:43:46.640
just like that

04:43:48.640 --> 04:43:50.640
you have to make sure that these

04:43:50.640 --> 04:43:52.640
slashes are actually forward slashes

04:43:52.640 --> 04:43:54.640
or else you might get bytecode errors

04:43:54.640 --> 04:43:56.640
so when it actually tries to read the string

04:43:56.640 --> 04:43:58.640
it doesn't think that

04:43:58.640 --> 04:44:00.640
these are separated

04:44:00.640 --> 04:44:02.640
the backward slashes do weird things

04:44:02.640 --> 04:44:04.640
so you could either do

04:44:04.640 --> 04:44:06.640
a one forward slash

04:44:06.640 --> 04:44:08.640
or two backward slashes

04:44:08.640 --> 04:44:10.640
that should work

04:44:10.640 --> 04:44:12.640
just make sure you get forward slashes

04:44:12.640 --> 04:44:14.640
and you should be good

04:44:14.640 --> 04:44:16.640
so folder path is where all these files are located

04:44:16.640 --> 04:44:18.640
all these xz files are located as you saw

04:44:18.640 --> 04:44:20.640
output file

04:44:20.640 --> 04:44:22.640
is the pattern for output file names

04:44:22.640 --> 04:44:24.640
in case we want to have more than one of them

04:44:24.640 --> 04:44:26.640
so if you want to have 200 output files

04:44:26.640 --> 04:44:28.640
instead of one then it'll just be like

04:44:28.640 --> 04:44:30.640
output 0, output 1, output 2 etc

04:44:30.640 --> 04:44:32.640
and then vocab file is where we want to save

04:44:32.640 --> 04:44:34.640
our vocabulary

04:44:34.640 --> 04:44:36.640
keep in mind in this giant corpus

04:44:36.640 --> 04:44:38.640
you can't push it on to ram at once

04:44:38.640 --> 04:44:40.640
so what we're gonna do is as we're

04:44:40.640 --> 04:44:42.640
reading these little compressed files

04:44:42.640 --> 04:44:44.640
20,000 of them

04:44:44.640 --> 04:44:46.640
we're gonna take all of the new characters

04:44:46.640 --> 04:44:48.640
from them and just push them into some vocab file

04:44:48.640 --> 04:44:50.640
containing all of the different

04:44:50.640 --> 04:44:52.640
characters that we have

04:44:52.640 --> 04:44:54.640
so that way we can handle this later

04:44:54.640 --> 04:44:56.640
and just pretty much sort it into some

04:44:56.640 --> 04:44:58.640
list containing all of our vocabulary

04:44:58.640 --> 04:45:00.640
split files

04:45:00.640 --> 04:45:02.640
how many files do we want to split this into

04:45:02.640 --> 04:45:04.640
so pretty much this

04:45:04.640 --> 04:45:06.640
it ties back to output file

04:45:06.640 --> 04:45:08.640
and just these curly braces here

04:45:08.640 --> 04:45:10.640
how many do we want to have

04:45:10.640 --> 04:45:12.640
if we want to have more than one then we would

04:45:12.640 --> 04:45:14.640
this would take effect

04:45:14.640 --> 04:45:16.640
so cool

04:45:16.640 --> 04:45:18.640
now we'll use our

04:45:18.640 --> 04:45:20.640
x files in dir

04:45:20.640 --> 04:45:22.640
to get a list of file names and store them in this variable

04:45:22.640 --> 04:45:24.640
we'll count

04:45:24.640 --> 04:45:26.640
the number of

04:45:26.640 --> 04:45:28.640
total xd files

04:45:28.640 --> 04:45:30.640
simply the length of our file names

04:45:30.640 --> 04:45:32.640
now in here

04:45:32.640 --> 04:45:34.640
we'll calculate the number of files

04:45:34.640 --> 04:45:36.640
to process for each output file

04:45:36.640 --> 04:45:38.640
if the user is requested

04:45:38.640 --> 04:45:40.640
more than one output file

04:45:40.640 --> 04:45:42.640
request more than one output file

04:45:42.640 --> 04:45:44.640
this is the total number of

04:45:44.640 --> 04:45:46.640
files divided by the number

04:45:46.640 --> 04:45:48.640
output files rounded down

04:45:48.640 --> 04:45:50.640
so

04:45:50.640 --> 04:45:52.640
if the user only wants one

04:45:52.640 --> 04:45:54.640
output file max count is the same as total files

04:45:54.640 --> 04:45:56.640
and

04:45:56.640 --> 04:45:58.640
that's how that works

04:45:58.640 --> 04:46:00.640
so

04:46:00.640 --> 04:46:02.640
next up we'll just create a

04:46:02.640 --> 04:46:04.640
set to store a vocabulary when we

04:46:04.640 --> 04:46:06.640
start appending these new characters into it

04:46:06.640 --> 04:46:08.640
a set is a

04:46:08.640 --> 04:46:10.640
collection of unique items in case you did not know

04:46:10.640 --> 04:46:12.640
entirely what a set was

04:46:14.640 --> 04:46:16.640
now

04:46:16.640 --> 04:46:18.640
this is where it gets interesting

04:46:18.640 --> 04:46:20.640
we're ready to process our

04:46:20.640 --> 04:46:22.640
.xz files

04:46:22.640 --> 04:46:24.640
for each output file we'll process

04:46:24.640 --> 04:46:26.640
max count files

04:46:26.640 --> 04:46:28.640
for each file we'll open it

04:46:28.640 --> 04:46:30.640
read its contents

04:46:30.640 --> 04:46:32.640
and write the contents to the current output file

04:46:32.640 --> 04:46:34.640
and then add any unique characters to our vocabulary

04:46:34.640 --> 04:46:36.640
set

04:46:36.640 --> 04:46:38.640
after processing max count files

04:46:38.640 --> 04:46:40.640
remove them from our list of files

04:46:40.640 --> 04:46:42.640
and then finally

04:46:48.640 --> 04:46:50.640
we'll write all of our vocabulary to this file

04:46:50.640 --> 04:46:52.640
so

04:46:52.640 --> 04:46:54.640
we pretty much just open

04:46:54.640 --> 04:46:56.640
we just

04:46:56.640 --> 04:46:58.640
write all of these characters in the vocab

04:46:58.640 --> 04:47:00.640
to this

04:47:00.640 --> 04:47:02.640
vocab file which is here vocab.txt

04:47:02.640 --> 04:47:04.640
so awesome

04:47:04.640 --> 04:47:06.640
now

04:47:06.640 --> 04:47:08.640
honestly we could just go ahead

04:47:08.640 --> 04:47:10.640
and run this

04:47:10.640 --> 04:47:12.640
so let's go ahead and go in here

04:47:12.640 --> 04:47:14.640
I'm going to go cls to clear that

04:47:14.640 --> 04:47:16.640
we'll go python

04:47:16.640 --> 04:47:18.640
data extract

04:47:18.640 --> 04:47:20.640
.py

04:47:20.640 --> 04:47:22.640
let's see this work it's magic

04:47:24.640 --> 04:47:26.640
how many files would you like to split this into

04:47:26.640 --> 04:47:28.640
we'll go one

04:47:28.640 --> 04:47:30.640
then we get a progress bar

04:47:30.640 --> 04:47:32.640
20,000 files and we'll just let that load

04:47:32.640 --> 04:47:34.640
I'll come back to you in

04:47:34.640 --> 04:47:36.640
about 30 minutes to check up on this

04:47:36.640 --> 04:47:38.640
okay so there's not a little

04:47:38.640 --> 04:47:40.640
one thing we want to consider for

04:47:40.640 --> 04:47:42.640
and

04:47:42.640 --> 04:47:44.640
it's actually quite important is our splits

04:47:44.640 --> 04:47:46.640
for train and file splits

04:47:46.640 --> 04:47:48.640
it would be really inefficient

04:47:48.640 --> 04:47:50.640
to just get blocks and then creating

04:47:50.640 --> 04:47:52.640
train and file splits as we go

04:47:52.640 --> 04:47:54.640
every new batch we get

04:47:54.640 --> 04:47:56.640
so in turn

04:47:56.640 --> 04:47:58.640
what we might be better off doing

04:47:58.640 --> 04:48:00.640
is just creating an

04:48:00.640 --> 04:48:02.640
output train file and an output file file

04:48:02.640 --> 04:48:04.640
so just two of them instead of one

04:48:04.640 --> 04:48:06.640
train is 90% of our data

04:48:06.640 --> 04:48:08.640
file is 10% of our data

04:48:08.640 --> 04:48:10.640
if that makes sense

04:48:10.640 --> 04:48:12.640
so pretty much what I did

04:48:12.640 --> 04:48:14.640
is I got the output line for how many

04:48:14.640 --> 04:48:16.640
files do you want

04:48:16.640 --> 04:48:18.640
so you can see I got quite a bit of files

04:48:18.640 --> 04:48:20.640
produced here

04:48:20.640 --> 04:48:22.640
by not doing that correctly

04:48:22.640 --> 04:48:24.640
so don't do that

04:48:24.640 --> 04:48:26.640
and

04:48:26.640 --> 04:48:28.640
yeah

04:48:28.640 --> 04:48:30.640
essentially we're just

04:48:30.640 --> 04:48:32.640
we're pretty much just doing that

04:48:32.640 --> 04:48:34.640
so we're processing some training files

04:48:34.640 --> 04:48:36.640
we're separating 90%

04:48:36.640 --> 04:48:38.640
of the names on the left side

04:48:38.640 --> 04:48:40.640
and then 10% of the names

04:48:40.640 --> 04:48:42.640
we're just separating those in the two different

04:48:42.640 --> 04:48:44.640
arrays, file names

04:48:44.640 --> 04:48:46.640
and then we're just processing each of those

04:48:46.640 --> 04:48:48.640
arrays based on the file names

04:48:48.640 --> 04:48:50.640
so I took away that little bit

04:48:50.640 --> 04:48:52.640
that was asking

04:48:52.640 --> 04:48:54.640
how many files per

04:48:54.640 --> 04:48:56.640
split do you want

04:48:56.640 --> 04:48:58.640
so I took that away

04:48:58.640 --> 04:49:00.640
and this is effectively the same code

04:49:00.640 --> 04:49:02.640
just a little bit of tweaks

04:49:02.640 --> 04:49:04.640
and yeah

04:49:04.640 --> 04:49:06.640
so I'm going to go ahead and run this

04:49:06.640 --> 04:49:08.640
data extract

04:49:08.640 --> 04:49:10.640
cool

04:49:10.640 --> 04:49:12.640
so we got an output train

04:49:12.640 --> 04:49:14.640
and then after this it's going to do

04:49:14.640 --> 04:49:16.640
the output validation set

04:49:16.640 --> 04:49:18.640
so I'll come back after this is done

04:49:18.640 --> 04:49:20.640
so awesome I have just downloaded

04:49:20.640 --> 04:49:22.640
both or I've both

04:49:22.640 --> 04:49:24.640
got both these splits

04:49:24.640 --> 04:49:26.640
output train and val train so just to

04:49:26.640 --> 04:49:28.640
confirm that they're

04:49:28.640 --> 04:49:30.640
actually the right size got 38.9

04:49:30.640 --> 04:49:32.640
and then 4.27

04:49:32.640 --> 04:49:34.640
so if we do this divided by

04:49:34.640 --> 04:49:36.640
9 so about 30

04:49:36.640 --> 04:49:38.640
8.9 divided by 9

04:49:38.640 --> 04:49:40.640
4.32 and it's

04:49:40.640 --> 04:49:42.640
pretty close to 4.27 so

04:49:42.640 --> 04:49:44.640
we can confirm that these are pretty much

04:49:44.640 --> 04:49:46.640
the

04:49:46.640 --> 04:49:48.640
length that we expect them to be

04:49:48.640 --> 04:49:50.640
so awesome we have this vocab.txt

04:49:50.640 --> 04:49:52.640
file wonderful so now

04:49:52.640 --> 04:49:54.640
we have to focus on is

04:49:54.640 --> 04:49:56.640
getting this into

04:49:56.640 --> 04:49:58.640
our batches so when we call

04:49:58.640 --> 04:50:00.640
our get batch function actually

04:50:00.640 --> 04:50:02.640
cd out of this open this in

04:50:02.640 --> 04:50:04.640
a Jupyter notebook

04:50:06.640 --> 04:50:08.640
copy my desktop

04:50:10.640 --> 04:50:12.640
paste it over here

04:50:12.640 --> 04:50:14.640
and perfect

04:50:14.640 --> 04:50:16.640
so

04:50:16.640 --> 04:50:18.640
this open when web text folder with

04:50:18.640 --> 04:50:20.640
these files awesome

04:50:20.640 --> 04:50:22.640
and our GPTV

04:50:22.640 --> 04:50:24.640
one

04:50:24.640 --> 04:50:26.640
so

04:50:26.640 --> 04:50:28.640
this get batch function

04:50:28.640 --> 04:50:30.640
is going to have to

04:50:30.640 --> 04:50:32.640
change also these

04:50:32.640 --> 04:50:34.640
are going to have to change as well

04:50:34.640 --> 04:50:36.640
and this one too these are probably

04:50:36.640 --> 04:50:38.640
not going to be here

04:50:38.640 --> 04:50:40.640
but pretty much

04:50:40.640 --> 04:50:42.640
let's go ahead and first of all

04:50:42.640 --> 04:50:44.640
get this vocab.txt

04:50:44.640 --> 04:50:46.640
in so what I'm going to do

04:50:46.640 --> 04:50:48.640
I'm just going to go

04:50:48.640 --> 04:50:50.640
we're going to go

04:50:50.640 --> 04:50:52.640
open web text slash

04:50:52.640 --> 04:50:54.640
vocab.txt

04:50:54.640 --> 04:50:56.640
cool so that's our vocab

04:50:56.640 --> 04:50:58.640
right there text read

04:50:58.640 --> 04:51:00.640
vocab size the length of that nice

04:51:00.640 --> 04:51:02.640
so that's what our vocab is

04:51:02.640 --> 04:51:04.640
and then

04:51:04.640 --> 04:51:06.640
what we're going to do next

04:51:06.640 --> 04:51:08.640
is change this get batch function

04:51:08.640 --> 04:51:10.640
around

04:51:10.640 --> 04:51:12.640
so first of all I'm going to go ahead

04:51:12.640 --> 04:51:14.640
and get rid of this here

04:51:14.640 --> 04:51:16.640
and then

04:51:16.640 --> 04:51:18.640
I've actually produced

04:51:18.640 --> 04:51:20.640
some code specifically for

04:51:20.640 --> 04:51:22.640
this so I'm just going to go back

04:51:22.640 --> 04:51:24.640
to my

04:51:24.640 --> 04:51:26.640
I'm just going to find

04:51:26.640 --> 04:51:28.640
this folder

04:51:28.640 --> 04:51:30.640
okay so I've

04:51:30.640 --> 04:51:32.640
actually produced some

04:51:32.640 --> 04:51:34.640
code here

04:51:34.640 --> 04:51:36.640
I produced this off camera

04:51:36.640 --> 04:51:38.640
but

04:51:38.640 --> 04:51:40.640
pretty much what this is going to do

04:51:40.640 --> 04:51:42.640
it's going to let us call a split

04:51:42.640 --> 04:51:44.640
okay so we have our get batch

04:51:44.640 --> 04:51:46.640
function all of this down here is the

04:51:46.640 --> 04:51:48.640
same as our GPTV

04:51:48.640 --> 04:51:50.640
one file and then

04:51:50.640 --> 04:51:52.640
this data is

04:51:52.640 --> 04:51:54.640
just going to get a random chunk of text

04:51:54.640 --> 04:51:56.640
with giant block of text

04:51:56.640 --> 04:51:58.640
and the way

04:51:58.640 --> 04:52:00.640
that we get it is actually pretty interesting

04:52:00.640 --> 04:52:02.640
so the way that we get this text is

04:52:02.640 --> 04:52:04.640
something called memory mapping

04:52:04.640 --> 04:52:06.640
so memory mapping is a way

04:52:06.640 --> 04:52:08.640
to look at disk files

04:52:08.640 --> 04:52:10.640
or to open them and look at pieces of them

04:52:10.640 --> 04:52:12.640
without opening the entire thing at once

04:52:12.640 --> 04:52:14.640
so memory mapping

04:52:14.640 --> 04:52:16.640
I'm not a hardware guy so I can't

04:52:16.640 --> 04:52:18.640
really talk about that

04:52:18.640 --> 04:52:20.640
memory mapping is pretty

04:52:20.640 --> 04:52:22.640
cool and allows us to look at little

04:52:22.640 --> 04:52:24.640
chunks at a time in very large text files

04:52:24.640 --> 04:52:26.640
so that's essentially what we're doing here

04:52:26.640 --> 04:52:28.640
we're passing this split

04:52:28.640 --> 04:52:30.640
split

04:52:30.640 --> 04:52:32.640
file name is equal to train split

04:52:32.640 --> 04:52:34.640
this is just an example text file

04:52:36.640 --> 04:52:38.640
if the split is equal to train then this

04:52:38.640 --> 04:52:40.640
is our file name else

04:52:40.640 --> 04:52:42.640
file split and then we're going to

04:52:42.640 --> 04:52:44.640
open this file name in binary mode

04:52:44.640 --> 04:52:46.640
this has to be in binary mode

04:52:46.640 --> 04:52:48.640
it's also a lot more efficient in binary

04:52:48.640 --> 04:52:50.640
mode and then

04:52:50.640 --> 04:52:52.640
we're going to open this with a mem map

04:52:52.640 --> 04:52:54.640
so I don't expect you to memorize all the mem map syntax

04:52:54.640 --> 04:52:56.640
you can look at the docs if you would like

04:52:56.640 --> 04:52:58.640
but I'm just going to explain

04:52:58.640 --> 04:53:00.640
logically what's happening

04:53:00.640 --> 04:53:02.640
so we're going to open this

04:53:02.640 --> 04:53:04.640
with the mem map library

04:53:04.640 --> 04:53:06.640
and we're going to open this as

04:53:06.640 --> 04:53:08.640
mm so

04:53:08.640 --> 04:53:10.640
the file size is literally

04:53:10.640 --> 04:53:12.640
just the length of it so determining

04:53:12.640 --> 04:53:14.640
the file size and

04:53:14.640 --> 04:53:16.640
all we're doing from this point is we're just finding

04:53:16.640 --> 04:53:18.640
a position so we're using the random library

04:53:18.640 --> 04:53:20.640
and we're finding

04:53:20.640 --> 04:53:22.640
a position between

04:53:22.640 --> 04:53:24.640
0

04:53:24.640 --> 04:53:26.640
and the file size

04:53:26.640 --> 04:53:28.640
minus block size times batch size

04:53:28.640 --> 04:53:30.640
so pretty much we have this

04:53:30.640 --> 04:53:32.640
giant text

04:53:32.640 --> 04:53:34.640
file we could either

04:53:34.640 --> 04:53:36.640
what we want to do is we want to start

04:53:36.640 --> 04:53:38.640
from 0 and go up to like

04:53:38.640 --> 04:53:40.640
just before the end because if we

04:53:40.640 --> 04:53:42.640
actually sample

04:53:42.640 --> 04:53:44.640
that last piece then it's still

04:53:44.640 --> 04:53:46.640
going to have some wiggle room to

04:53:46.640 --> 04:53:48.640
reach further into the file

04:53:48.640 --> 04:53:50.640
if we just made it from like

04:53:50.640 --> 04:53:52.640
the first

04:53:52.640 --> 04:53:54.640
the very start of the file to the very end

04:53:54.640 --> 04:53:56.640
then it would want to do

04:53:56.640 --> 04:53:58.640
is it would want to look past the end

04:53:58.640 --> 04:54:00.640
because it would want to look at more tokens from that

04:54:00.640 --> 04:54:02.640
and then we would just get errors

04:54:02.640 --> 04:54:04.640
because you can't read more than

04:54:04.640 --> 04:54:06.640
the file size if that makes sense

04:54:06.640 --> 04:54:08.640
so that's why I'm just making this little threshold here

04:54:08.640 --> 04:54:10.640
and

04:54:10.640 --> 04:54:12.640
yeah so that's what that does

04:54:12.640 --> 04:54:14.640
that's the starting position could be a random

04:54:14.640 --> 04:54:16.640
number between the start and

04:54:16.640 --> 04:54:18.640
a little bit a little margin from the end

04:54:18.640 --> 04:54:20.640
here so

04:54:20.640 --> 04:54:22.640
next up we have

04:54:22.640 --> 04:54:24.640
this seek function so seek is going to

04:54:24.640 --> 04:54:26.640
go to the start position and then

04:54:26.640 --> 04:54:28.640
block is going to

04:54:28.640 --> 04:54:30.640
read we're going to

04:54:30.640 --> 04:54:32.640
go up to the start position it's going to

04:54:32.640 --> 04:54:34.640
seek up to there that's where it's going to start

04:54:34.640 --> 04:54:36.640
it's going to go up to it and then the read

04:54:36.640 --> 04:54:38.640
function is going to

04:54:38.640 --> 04:54:40.640
find a block of text that is

04:54:40.640 --> 04:54:42.640
block size times batch size so it's

04:54:42.640 --> 04:54:44.640
going to find a little snippet

04:54:44.640 --> 04:54:46.640
of text in there at the starting

04:54:46.640 --> 04:54:48.640
position and it's going to be of size

04:54:48.640 --> 04:54:50.640
it's going to have this the same amount of

04:54:50.640 --> 04:54:52.640
I guess bytes as

04:54:52.640 --> 04:54:54.640
block size time times batch size

04:54:54.640 --> 04:54:56.640
then all that minus one

04:54:56.640 --> 04:54:58.640
just so that it fits into this start position

04:54:58.640 --> 04:55:00.640
we don't get errors here that's why I put the minus one

04:55:00.640 --> 04:55:02.640
but

04:55:02.640 --> 04:55:04.640
yeah so we'll get a pretty

04:55:04.640 --> 04:55:06.640
we'll get a pretty decent

04:55:06.640 --> 04:55:08.640
text amount I guess you could say

04:55:08.640 --> 04:55:10.640
it's going to be enough to work with you could

04:55:10.640 --> 04:55:12.640
you could of course increases if you

04:55:12.640 --> 04:55:14.640
wanted to you could do like

04:55:14.640 --> 04:55:16.640
times eight if you wanted

04:55:16.640 --> 04:55:18.640
times eight and then times eight up here but

04:55:18.640 --> 04:55:20.640
we're not going to do that

04:55:20.640 --> 04:55:22.640
based on my experience this is performed pretty well

04:55:22.640 --> 04:55:24.640
so we're going to stick with this method here

04:55:24.640 --> 04:55:26.640
and then

04:55:26.640 --> 04:55:28.640
we just decode this

04:55:28.640 --> 04:55:30.640
bit of text the reason we decode it is it's

04:55:30.640 --> 04:55:32.640
it's because it's

04:55:32.640 --> 04:55:34.640
we read it in binary form

04:55:34.640 --> 04:55:36.640
so once we have this block of

04:55:36.640 --> 04:55:38.640
text we actually have to decode this to

04:55:38.640 --> 04:55:40.640
UFA format or UTF

04:55:40.640 --> 04:55:42.640
format and then any like

04:55:42.640 --> 04:55:44.640
bytecode errors we get we're just going to ignore

04:55:44.640 --> 04:55:46.640
that this is something you learn

04:55:46.640 --> 04:55:48.640
through practice is when you start dealing

04:55:48.640 --> 04:55:50.640
with like really weird data or if it has

04:55:50.640 --> 04:55:52.640
like corruptions in it you'll get errors

04:55:52.640 --> 04:55:54.640
so all you want to do is all

04:55:54.640 --> 04:55:56.640
this does is it pretty much says

04:55:56.640 --> 04:55:58.640
okay we're just going to ignore this

04:55:58.640 --> 04:56:00.640
bit of text and we're just going to sample

04:56:00.640 --> 04:56:02.640
everything around it and not include that

04:56:02.640 --> 04:56:04.640
part and plus since we're doing so many

04:56:04.640 --> 04:56:06.640
iterations it won't actually interfere

04:56:06.640 --> 04:56:08.640
that much so we should

04:56:08.640 --> 04:56:10.640
be all right and then for this replace

04:56:10.640 --> 04:56:12.640
go function here I was noticing

04:56:12.640 --> 04:56:14.640
I got errors about this slash R

04:56:14.640 --> 04:56:16.640
so all this does is it just replaces that

04:56:16.640 --> 04:56:18.640
with an empty string and then finally

04:56:18.640 --> 04:56:20.640
we have all this

04:56:20.640 --> 04:56:22.640
we have all this decoded data

04:56:22.640 --> 04:56:24.640
so all we're going to do is just encode

04:56:24.640 --> 04:56:26.640
this into the

04:56:26.640 --> 04:56:28.640
tokenized form so it's all in

04:56:28.640 --> 04:56:30.640
it's all in the tokenized form

04:56:30.640 --> 04:56:32.640
integers or torch.longs

04:56:32.640 --> 04:56:34.640
data type

04:56:34.640 --> 04:56:36.640
and we just that's what our data is

04:56:36.640 --> 04:56:38.640
instead of a bunch of characters it's just a bunch

04:56:38.640 --> 04:56:40.640
of numbers and then we

04:56:40.640 --> 04:56:42.640
return that into our get batch

04:56:42.640 --> 04:56:44.640
and this is what our data is

04:56:44.640 --> 04:56:46.640
so that's pretty cool

04:56:46.640 --> 04:56:48.640
we can get either train or a valve

04:56:48.640 --> 04:56:50.640
split and

04:56:50.640 --> 04:56:52.640
that's sort of what it looks like in practice

04:56:52.640 --> 04:56:54.640
that's how we sample from

04:56:54.640 --> 04:56:56.640
very large text files at a smaller

04:56:56.640 --> 04:56:58.640
scale bit by bit so

04:56:58.640 --> 04:57:00.640
let's go ahead and implement this here

04:57:00.640 --> 04:57:02.640
and go grab this entire

04:57:02.640 --> 04:57:04.640
thing

04:57:04.640 --> 04:57:06.640
and pop over to here

04:57:06.640 --> 04:57:08.640
we're just going to replace that

04:57:08.640 --> 04:57:10.640
so

04:57:10.640 --> 04:57:12.640
get random chunk, get batch

04:57:12.640 --> 04:57:14.640
cool

04:57:14.640 --> 04:57:16.640
so now we can actually go ahead and

04:57:16.640 --> 04:57:18.640
perhaps run this

04:57:18.640 --> 04:57:20.640
actually before we run this there's a little something we need to

04:57:20.640 --> 04:57:22.640
add in here

04:57:22.640 --> 04:57:24.640
so I have this

04:57:24.640 --> 04:57:26.640
train split.txt and a valve split.txt

04:57:26.640 --> 04:57:28.640
so I actually need to

04:57:28.640 --> 04:57:30.640
change these

04:57:30.640 --> 04:57:32.640
so let's go rename we'll go

04:57:32.640 --> 04:57:34.640
train split.txt

04:57:34.640 --> 04:57:36.640
and then

04:57:36.640 --> 04:57:38.640
a valve split.txt

04:57:38.640 --> 04:57:40.640
cool

04:57:40.640 --> 04:57:42.640
and then we could just go

04:57:42.640 --> 04:57:44.640
open web text

04:57:44.640 --> 04:57:46.640
forward slash

04:57:46.640 --> 04:57:48.640
and then same thing for here

04:57:48.640 --> 04:57:50.640
cool let's go ahead

04:57:50.640 --> 04:57:52.640
and run this now

04:57:56.640 --> 04:57:58.640
and we're getting errors

04:57:58.640 --> 04:58:00.640
mem map is not defined

04:58:00.640 --> 04:58:02.640
so that's another thing we need to probably

04:58:02.640 --> 04:58:04.640
add in then

04:58:04.640 --> 04:58:06.640
so I'm actually just going to

04:58:06.640 --> 04:58:08.640
stop this process from running here

04:58:08.640 --> 04:58:10.640
we're going to go pip

04:58:10.640 --> 04:58:12.640
install

04:58:12.640 --> 04:58:14.640
mem map

04:58:16.640 --> 04:58:18.640
mem map is not defined

04:58:18.640 --> 04:58:20.640
we don't actually need to install this

04:58:20.640 --> 04:58:22.640
by default comes with the operating system

04:58:22.640 --> 04:58:24.640
so

04:58:24.640 --> 04:58:26.640
what we actually need to do

04:58:26.640 --> 04:58:28.640
is

04:58:28.640 --> 04:58:30.640
just close this

04:58:30.640 --> 04:58:32.640
gptv1

04:58:32.640 --> 04:58:34.640
awesome

04:58:34.640 --> 04:58:36.640
everything is good

04:58:36.640 --> 04:58:38.640
nothing is broken

04:58:38.640 --> 04:58:40.640
so what I actually need to do up here

04:58:40.640 --> 04:58:42.640
is import this

04:58:42.640 --> 04:58:44.640
so I need to go

04:58:44.640 --> 04:58:46.640
import mem map

04:58:46.640 --> 04:58:48.640
just like that

04:58:48.640 --> 04:58:50.640
and

04:58:50.640 --> 04:58:52.640
should be good to start running this script

04:58:52.640 --> 04:58:54.640
name random is not defined

04:58:54.640 --> 04:58:56.640
again another importation we have to make

04:58:56.640 --> 04:58:58.640
import

04:58:58.640 --> 04:59:00.640
import

04:59:00.640 --> 04:59:02.640
random

04:59:06.640 --> 04:59:08.640
and we should start seeing some

04:59:08.640 --> 04:59:10.640
progress going here so once we see the first iteration

04:59:10.640 --> 04:59:12.640
I'm going to stop it come back

04:59:12.640 --> 04:59:14.640
at the last iteration and

04:59:14.640 --> 04:59:16.640
then we'll start adding some little bits and pieces

04:59:16.640 --> 04:59:18.640
onto our script here to make it better

04:59:18.640 --> 04:59:20.640
so we're already about 600 iterations

04:59:20.640 --> 04:59:22.640
in and you can see how the training loss

04:59:22.640 --> 04:59:24.640
is actually done really well so far

04:59:24.640 --> 04:59:26.640
it's gone from 10.5 drop all the way to

04:59:26.640 --> 04:59:28.640
2.38

04:59:28.640 --> 04:59:30.640
and

04:59:30.640 --> 04:59:32.640
we can actually see that

04:59:32.640 --> 04:59:34.640
we might be able to actually get a

04:59:34.640 --> 04:59:36.640
val loss that is lower than the

04:59:36.640 --> 04:59:38.640
train because keep in mind

04:59:38.640 --> 04:59:40.640
in train mode

04:59:40.640 --> 04:59:42.640
the dropout takes effect but in val

04:59:42.640 --> 04:59:44.640
in eval mode

04:59:44.640 --> 04:59:46.640
let me just scroll up to this here

04:59:46.640 --> 04:59:48.640
yes

04:59:48.640 --> 04:59:50.640
so model about eval what this does

04:59:50.640 --> 04:59:52.640
is it turns off the dropout

04:59:52.640 --> 04:59:54.640
so

04:59:54.640 --> 04:59:56.640
we don't lose any of the neurons

04:59:56.640 --> 04:59:58.640
and they're all sort of showing the same

04:59:58.640 --> 05:00:00.640
features and giving all the information that they're supposed

05:00:00.640 --> 05:00:02.640
to because they're all active but in train mode

05:00:02.640 --> 05:00:04.640
20% of them are off so

05:00:04.640 --> 05:00:06.640
once you actually see

05:00:06.640 --> 05:00:08.640
in eval mode it does better

05:00:08.640 --> 05:00:10.640
that means

05:00:10.640 --> 05:00:12.640
that the network has started to

05:00:12.640 --> 05:00:14.640
form a sense

05:00:14.640 --> 05:00:16.640
of completeness in its learning

05:00:16.640 --> 05:00:18.640
so it's just adjusting things a little bit

05:00:18.640 --> 05:00:20.640
once it hits that point

05:00:20.640 --> 05:00:22.640
and we might see this happen

05:00:22.640 --> 05:00:24.640
momentarily but this is

05:00:24.640 --> 05:00:26.640
really good progress so far a loss of

05:00:26.640 --> 05:00:28.640
1.8 is amazing

05:00:28.640 --> 05:00:30.640
so

05:00:30.640 --> 05:00:32.640
in the meantime

05:00:32.640 --> 05:00:34.640
I'm just going to add some little tweaks

05:00:34.640 --> 05:00:36.640
here and there to improve this script

05:00:36.640 --> 05:00:38.640
so I've actually stopped the iteration process

05:00:38.640 --> 05:00:40.640
but we've gotten to 700 steps and we can already

05:00:40.640 --> 05:00:42.640
see that val loss

05:00:42.640 --> 05:00:44.640
is becoming a less than train loss

05:00:44.640 --> 05:00:46.640
which is showing that the model is actually converging

05:00:46.640 --> 05:00:48.640
and doing very well

05:00:48.640 --> 05:00:50.640
so this architecture is amazing

05:00:50.640 --> 05:00:52.640
we've pretty much covered

05:00:52.640 --> 05:00:54.640
every

05:00:54.640 --> 05:00:56.640
architectural, math, pie torch part

05:00:56.640 --> 05:00:58.640
that this script has to offer

05:00:58.640 --> 05:01:00.640
the only thing I want to add

05:01:00.640 --> 05:01:02.640
actually a few things I want to add

05:01:02.640 --> 05:01:04.640
one of them being torch.load

05:01:04.640 --> 05:01:06.640
and torch.save

05:01:06.640 --> 05:01:08.640
so one thing that's going to be really important

05:01:08.640 --> 05:01:10.640
when you start to scale up

05:01:10.640 --> 05:01:12.640
your iterations

05:01:12.640 --> 05:01:14.640
is you don't just want to run a script

05:01:14.640 --> 05:01:16.640
that executes a training loop

05:01:16.640 --> 05:01:18.640
with an architecture and

05:01:18.640 --> 05:01:20.640
that's it. You won't have some way to

05:01:20.640 --> 05:01:22.640
store those learning parameters

05:01:22.640 --> 05:01:24.640
so that's what torch.load and torch.save does

05:01:26.640 --> 05:01:28.640
save some file

05:01:28.640 --> 05:01:30.640
right and

05:01:30.640 --> 05:01:32.640
you can pretty much

05:01:32.640 --> 05:01:34.640
you could put it into like a serialized

05:01:34.640 --> 05:01:36.640
format when you

05:01:36.640 --> 05:01:38.640
save it you take your initial

05:01:38.640 --> 05:01:40.640
architecture in our case it would actually

05:01:40.640 --> 05:01:42.640
be the GPT language model so you would

05:01:42.640 --> 05:01:44.640
save this because it contains

05:01:44.640 --> 05:01:46.640
everything all these other classes

05:01:46.640 --> 05:01:48.640
as well they're all inside of GPT

05:01:48.640 --> 05:01:50.640
language model would save that architecture

05:01:50.640 --> 05:01:52.640
and you essentially

05:01:52.640 --> 05:01:54.640
serialize it into some pickled file

05:01:54.640 --> 05:01:56.640
that would have

05:01:56.640 --> 05:01:58.640
the file extension .pkl

05:01:58.640 --> 05:02:00.640
so

05:02:00.640 --> 05:02:02.640
essentially

05:02:02.640 --> 05:02:04.640
instead of using torch we're just going to use

05:02:04.640 --> 05:02:06.640
a library called pickle because

05:02:06.640 --> 05:02:08.640
they're essentially the same thing

05:02:08.640 --> 05:02:10.640
pickle is a little bit easier

05:02:10.640 --> 05:02:12.640
to use or at least a little bit easier to understand

05:02:12.640 --> 05:02:14.640
there's less to it

05:02:14.640 --> 05:02:16.640
pickle will only work

05:02:16.640 --> 05:02:18.640
on one GPU

05:02:18.640 --> 05:02:20.640
so if you have like 8 GPUs at the same time

05:02:20.640 --> 05:02:22.640
you're going to want to learn a little bit more

05:02:22.640 --> 05:02:24.640
about hardware stuff and

05:02:24.640 --> 05:02:26.640
some PyTorch docs but

05:02:26.640 --> 05:02:28.640
pretty much

05:02:28.640 --> 05:02:30.640
if we want to

05:02:30.640 --> 05:02:32.640
save this after training

05:02:32.640 --> 05:02:34.640
what we're going to do is we're going to use

05:02:34.640 --> 05:02:36.640
a little library called pickle and this comes

05:02:36.640 --> 05:02:38.640
pre-installed with windows

05:02:40.640 --> 05:02:42.640
import pickle

05:02:42.640 --> 05:02:44.640
okay so what we want to do is

05:02:44.640 --> 05:02:46.640
implement this after the training loop

05:02:46.640 --> 05:02:48.640
after all these parameters have been updated

05:02:48.640 --> 05:02:50.640
and learned to the fullest extent

05:02:50.640 --> 05:02:52.640
so after this training loop

05:02:52.640 --> 05:02:54.640
we're simply going to open

05:02:54.640 --> 05:02:56.640
we're going to do with open

05:02:56.640 --> 05:02:58.640
and we could just go

05:02:58.640 --> 05:03:00.640
model 01 like that

05:03:00.640 --> 05:03:02.640
and then

05:03:02.640 --> 05:03:04.640
just that .pkl is the file extension

05:03:04.640 --> 05:03:06.640
for it

05:03:06.640 --> 05:03:08.640
and then since we're writing to it we're going to go

05:03:08.640 --> 05:03:10.640
write binary

05:03:10.640 --> 05:03:12.640
F

05:03:12.640 --> 05:03:14.640
and then in order to actually save this

05:03:14.640 --> 05:03:16.640
we just go pickle.dump

05:03:16.640 --> 05:03:18.640
and then we can use

05:03:18.640 --> 05:03:20.640
model and then

05:03:20.640 --> 05:03:22.640
just F like that

05:03:22.640 --> 05:03:24.640
so

05:03:24.640 --> 05:03:26.640
if I start recording this

05:03:26.640 --> 05:03:28.640
it's going to make

05:03:28.640 --> 05:03:30.640
if I start recording this training process

05:03:30.640 --> 05:03:32.640
it's going to make my clip

05:03:32.640 --> 05:03:34.640
like so

05:03:34.640 --> 05:03:36.640
I'm going to come back to this after we've done

05:03:36.640 --> 05:03:38.640
let's just say about

05:03:38.640 --> 05:03:40.640
100 iterations

05:03:40.640 --> 05:03:42.640
we're going to do 100 editors

05:03:42.640 --> 05:03:44.640
and I'm going to come back and

05:03:44.640 --> 05:03:46.640
show you guys

05:03:46.640 --> 05:03:48.640
what the model file looks like

05:03:48.640 --> 05:03:50.640
what I actually did is I changed some of the model

05:03:50.640 --> 05:03:52.640
hyper parameters because

05:03:52.640 --> 05:03:54.640
it was taking way too long

05:03:54.640 --> 05:03:56.640
to perform what we wanted it to so I changed

05:03:56.640 --> 05:03:58.640
and head to one and layer to one

05:03:58.640 --> 05:04:00.640
and I had half batch size

05:04:00.640 --> 05:04:02.640
all the way down from 64 to 32

05:04:02.640 --> 05:04:04.640
so what I'm actually going to add here is just

05:04:04.640 --> 05:04:06.640
to make sure I like to print this out at the beginning

05:04:06.640 --> 05:04:08.640
of this

05:04:08.640 --> 05:04:10.640
make sure that the device is CUDA

05:04:10.640 --> 05:04:12.640
let's go back down

05:04:12.640 --> 05:04:14.640
so it did in fact train the model

05:04:14.640 --> 05:04:16.640
so we got all this done

05:04:16.640 --> 05:04:18.640
and yeah

05:04:18.640 --> 05:04:20.640
so I don't know why I did 2.54

05:04:20.640 --> 05:04:22.640
whatever that

05:04:22.640 --> 05:04:24.640
that was just the entire loss

05:04:24.640 --> 05:04:26.640
so

05:04:26.640 --> 05:04:28.640
model saved awesome

05:04:28.640 --> 05:04:30.640
what does this actually look like here

05:04:30.640 --> 05:04:32.640
so this model.pkl

05:04:32.640 --> 05:04:34.640
106 megabytes isn't that wonderful

05:04:34.640 --> 05:04:36.640
so this is our model file this is what they look like

05:04:36.640 --> 05:04:38.640
it's just a serialized

05:04:38.640 --> 05:04:40.640
pretty much the entire architecture

05:04:40.640 --> 05:04:42.640
all the parameters of the model the state

05:04:42.640 --> 05:04:44.640
everything that it contains

05:04:44.640 --> 05:04:46.640
and we just compress that

05:04:46.640 --> 05:04:48.640
into a little pkl file take that out

05:04:48.640 --> 05:04:50.640
decompress it and then just use it again

05:04:50.640 --> 05:04:52.640
with all those same parameters so

05:04:52.640 --> 05:04:54.640
awesome

05:04:54.640 --> 05:04:56.640
and all this really took was

05:04:56.640 --> 05:04:58.640
we just open

05:04:58.640 --> 05:05:00.640
as this

05:05:00.640 --> 05:05:02.640
we do a pickle.dump

05:05:02.640 --> 05:05:04.640
to make sure that actually save I just like to add

05:05:04.640 --> 05:05:06.640
a little print statement there cool

05:05:06.640 --> 05:05:08.640
so next

05:05:08.640 --> 05:05:10.640
what I'd like to add is a little

05:05:10.640 --> 05:05:12.640
wait for us to

05:05:12.640 --> 05:05:14.640
instead of just doing all of our training at once

05:05:14.640 --> 05:05:16.640
and then saving the model being able to

05:05:16.640 --> 05:05:18.640
train multiple times

05:05:18.640 --> 05:05:20.640
so I'm gonna go up here

05:05:20.640 --> 05:05:22.640
to our

05:05:22.640 --> 05:05:24.640
GPT language model here

05:05:24.640 --> 05:05:26.640
and

05:05:26.640 --> 05:05:28.640
let's just see

05:05:28.640 --> 05:05:30.640
what I'm gonna do

05:05:30.640 --> 05:05:32.640
with open

05:05:32.640 --> 05:05:34.640
and we're gonna go

05:05:34.640 --> 05:05:36.640
model 01

05:05:36.640 --> 05:05:38.640
pkl

05:05:38.640 --> 05:05:40.640
and we're gonna go read binary

05:05:40.640 --> 05:05:42.640
so actually gonna read it we're gonna

05:05:42.640 --> 05:05:44.640
load this into

05:05:44.640 --> 05:05:46.640
our script here

05:05:46.640 --> 05:05:48.640
so

05:05:48.640 --> 05:05:50.640
we're gonna go as f

05:05:50.640 --> 05:05:52.640
and then

05:05:52.640 --> 05:05:54.640
I believe it's pickle.load

05:05:56.640 --> 05:05:58.640
you just go yeah

05:05:58.640 --> 05:06:00.640
model equals

05:06:00.640 --> 05:06:02.640
pickle.load and then we'll just

05:06:02.640 --> 05:06:04.640
essentially dump that

05:06:04.640 --> 05:06:06.640
right in there

05:06:06.640 --> 05:06:08.640
go print

05:06:08.640 --> 05:06:10.640
loading

05:06:10.640 --> 05:06:12.640
model

05:06:12.640 --> 05:06:14.640
parameters

05:06:14.640 --> 05:06:16.640
dot dot dot

05:06:16.640 --> 05:06:18.640
and then

05:06:18.640 --> 05:06:20.640
just put f in there

05:06:20.640 --> 05:06:22.640
and then once it is loaded

05:06:22.640 --> 05:06:24.640
we'll do print

05:06:24.640 --> 05:06:26.640
loaded

05:06:26.640 --> 05:06:28.640
successfully

05:06:28.640 --> 05:06:30.640
cool

05:06:30.640 --> 05:06:32.640
so I'm actually gonna try this out now

05:06:32.640 --> 05:06:34.640
go

05:06:34.640 --> 05:06:36.640
do that

05:06:36.640 --> 05:06:38.640
boom

05:06:38.640 --> 05:06:40.640
and boom

05:06:40.640 --> 05:06:42.640
okay

05:06:42.640 --> 05:06:44.640
so

05:06:44.640 --> 05:06:46.640
loading model parameters loaded successfully

05:06:46.640 --> 05:06:48.640
and we'll actually see this

05:06:48.640 --> 05:06:50.640
start to work on its own now

05:06:50.640 --> 05:06:52.640
so

05:06:52.640 --> 05:06:54.640
is it going to begin or is it not going to begin

05:06:54.640 --> 05:06:56.640
let's run that

05:06:56.640 --> 05:06:58.640
okay perfect

05:06:58.640 --> 05:07:00.640
so now we should take the loss

05:07:00.640 --> 05:07:02.640
that we had before which was about

05:07:02.640 --> 05:07:04.640
2.54 I believe

05:07:04.640 --> 05:07:06.640
something around those, something along those lines

05:07:06.640 --> 05:07:08.640
you can see that our training process

05:07:08.640 --> 05:07:10.640
is greatly accelerated

05:07:12.640 --> 05:07:14.640
so we had 100

05:07:14.640 --> 05:07:16.640
now it's just gonna do an estimate loss

05:07:16.640 --> 05:07:18.640
cool

05:07:20.640 --> 05:07:22.640
and we're almost done

05:07:25.640 --> 05:07:27.640
1.96 awesome

05:07:27.640 --> 05:07:29.640
and the model saved

05:07:29.640 --> 05:07:31.640
so essentially what we can do with this

05:07:31.640 --> 05:07:33.640
is we can now

05:07:33.640 --> 05:07:35.640
save models

05:07:35.640 --> 05:07:37.640
and then we can load them and then iterate further

05:07:37.640 --> 05:07:39.640
so if you wanted to

05:07:39.640 --> 05:07:41.640
you could create a super cool

05:07:41.640 --> 05:07:43.640
GPT language model

05:07:43.640 --> 05:07:45.640
script here and

05:07:45.640 --> 05:07:47.640
you could essentially give it like 10,000 or 20,000

05:07:47.640 --> 05:07:49.640
iterations to run overnight

05:07:49.640 --> 05:07:51.640
you'd be able to save it

05:07:51.640 --> 05:07:53.640
and then import that into say a chat bot

05:07:53.640 --> 05:07:55.640
if you want

05:07:55.640 --> 05:07:57.640
so that's pretty cool and that's just kind of

05:07:57.640 --> 05:07:59.640
a good thing

05:07:59.640 --> 05:08:01.640
good little, it's kind of

05:08:01.640 --> 05:08:03.640
essential for language modeling because

05:08:03.640 --> 05:08:05.640
what's the point

05:08:05.640 --> 05:08:07.640
in having a machine learning model if you can't

05:08:07.640 --> 05:08:09.640
actually use it and deploy it

05:08:09.640 --> 05:08:11.640
so you need to save for this stuff to work

05:08:11.640 --> 05:08:13.640
alright

05:08:13.640 --> 05:08:15.640
now let's move on to

05:08:15.640 --> 05:08:17.640
a little something in this task manager

05:08:17.640 --> 05:08:19.640
here which I'd like to go over

05:08:19.640 --> 05:08:21.640
so this shared GPU memory here

05:08:21.640 --> 05:08:23.640
and this dedicated GPU memory

05:08:23.640 --> 05:08:25.640
so dedicated

05:08:25.640 --> 05:08:27.640
means how much

05:08:27.640 --> 05:08:29.640
VRAM, video RAM

05:08:29.640 --> 05:08:31.640
does your GPU actually have

05:08:31.640 --> 05:08:33.640
on the card

05:08:33.640 --> 05:08:35.640
so on the card it's going to be very quick memory

05:08:35.640 --> 05:08:37.640
because it doesn't have to

05:08:37.640 --> 05:08:39.640
the electrons don't have to travel as quickly

05:08:39.640 --> 05:08:41.640
that's kind of the logic of it

05:08:41.640 --> 05:08:43.640
the electrons don't have to travel

05:08:43.640 --> 05:08:45.640
they don't have to travel as far

05:08:45.640 --> 05:08:47.640
because

05:08:47.640 --> 05:08:49.640
the little RAM chip is right there

05:08:49.640 --> 05:08:51.640
so

05:08:51.640 --> 05:08:53.640
dedicated GPU memory is a lot faster

05:08:53.640 --> 05:08:55.640
shared GPU memory

05:08:55.640 --> 05:08:57.640
is essentially if this gets overloaded

05:08:57.640 --> 05:08:59.640
it'll use some of the RAM on your

05:08:59.640 --> 05:09:01.640
computer instead

05:09:01.640 --> 05:09:03.640
so this will typically be about half of your

05:09:03.640 --> 05:09:05.640
computer's RAM

05:09:05.640 --> 05:09:07.640
I have 32 gigabytes of RAM on my computer

05:09:07.640 --> 05:09:09.640
so 16.0 makes sense

05:09:09.640 --> 05:09:11.640
half 32

05:09:11.640 --> 05:09:13.640
and yeah

05:09:13.640 --> 05:09:15.640
so you want to make sure you're only using dedicated

05:09:15.640 --> 05:09:17.640
GPU memory

05:09:17.640 --> 05:09:19.640
having your shared GPU memory go up

05:09:19.640 --> 05:09:21.640
is not usually a good thing

05:09:21.640 --> 05:09:23.640
a little bit is fine

05:09:23.640 --> 05:09:25.640
but

05:09:25.640 --> 05:09:27.640
dedicated GPU memory is the fastest

05:09:27.640 --> 05:09:29.640
and you want everything to stick on there

05:09:29.640 --> 05:09:31.640
just try to make sure all of your parameters

05:09:31.640 --> 05:09:33.640
sort of fit around this

05:09:33.640 --> 05:09:35.640
whatever your max capacity is

05:09:35.640 --> 05:09:37.640
maybe it's 4, maybe it's 8

05:09:37.640 --> 05:09:39.640
maybe it's 48

05:09:39.640 --> 05:09:41.640
who knows

05:09:41.640 --> 05:09:43.640
and a good way to figure out

05:09:43.640 --> 05:09:45.640
what you can use on your GPU

05:09:45.640 --> 05:09:47.640
without it getting memory errors

05:09:47.640 --> 05:09:49.640
or using shared memory

05:09:49.640 --> 05:09:51.640
is to actually play around

05:09:51.640 --> 05:09:53.640
with

05:09:53.640 --> 05:09:55.640
these parameters up here

05:09:55.640 --> 05:09:57.640
so

05:09:57.640 --> 05:09:59.640
block size and batch size

05:09:59.640 --> 05:10:01.640
actually let me switch those around

05:10:01.640 --> 05:10:03.640
these are not supposed to be in that order

05:10:03.640 --> 05:10:05.640
but

05:10:05.640 --> 05:10:07.640
all good

05:10:07.640 --> 05:10:09.640
we'll make our batch size

05:10:09.640 --> 05:10:11.640
64

05:10:11.640 --> 05:10:13.640
that's 128

05:10:13.640 --> 05:10:15.640
okay

05:10:15.640 --> 05:10:17.640
so

05:10:17.640 --> 05:10:19.640
batch size and block size

05:10:19.640 --> 05:10:21.640
are very big contributors to how much memory you're going to use

05:10:21.640 --> 05:10:23.640
learning rate is not

05:10:23.640 --> 05:10:25.640
max iterations is not

05:10:25.640 --> 05:10:27.640
evaluators is not

05:10:27.640 --> 05:10:29.640
but these three will

05:10:29.640 --> 05:10:31.640
the amount of features that you store

05:10:31.640 --> 05:10:33.640
the amount of heads you have running in parallel

05:10:33.640 --> 05:10:35.640
and then also

05:10:35.640 --> 05:10:37.640
layers so

05:10:37.640 --> 05:10:39.640
some of these will not

05:10:39.640 --> 05:10:41.640
affect you as much because they're more

05:10:41.640 --> 05:10:43.640
sort of restrained to computation

05:10:43.640 --> 05:10:45.640
how quickly you can do operations if something is sequential

05:10:47.640 --> 05:10:49.640
so N layer won't strain you

05:10:49.640 --> 05:10:51.640
as much as something like batch and block size

05:10:51.640 --> 05:10:53.640
but

05:10:53.640 --> 05:10:55.640
those are just good little things to

05:10:55.640 --> 05:10:57.640
sort of tweak and play around with

05:10:57.640 --> 05:10:59.640
so I found the optimal

05:10:59.640 --> 05:11:01.640
sort of set of

05:11:01.640 --> 05:11:03.640
hyper parameters for my PC

05:11:03.640 --> 05:11:05.640
that happens to be

05:11:05.640 --> 05:11:07.640
8, 8, 3, 8, 4

05:11:07.640 --> 05:11:09.640
learning rates is the same

05:11:09.640 --> 05:11:11.640
and then 64, 128 for this

05:11:11.640 --> 05:11:13.640
so that happened to be the optimal

05:11:13.640 --> 05:11:15.640
hyper parameters for my computer

05:11:15.640 --> 05:11:17.640
it'll probably be different for yours

05:11:17.640 --> 05:11:19.640
if you don't have 8 gigabytes of RAM on your GPU

05:11:21.640 --> 05:11:23.640
so anyways

05:11:23.640 --> 05:11:25.640
that's a little something you have to pay attention to

05:11:25.640 --> 05:11:27.640
to make sure you don't run out of errors

05:11:27.640 --> 05:11:29.640
and a technique you can use

05:11:29.640 --> 05:11:31.640
which I'm not actually going to show you in this course

05:11:31.640 --> 05:11:33.640
but it's quite useful is something called auto tuning

05:11:33.640 --> 05:11:35.640
and what auto tuning does

05:11:35.640 --> 05:11:37.640
is it pretty much runs

05:11:37.640 --> 05:11:39.640
a bunch of these

05:11:39.640 --> 05:11:41.640
a bunch of models with different

05:11:41.640 --> 05:11:43.640
sets of hyper parameters

05:11:43.640 --> 05:11:45.640
so to run like batch size 64

05:11:45.640 --> 05:11:47.640
batch size 32, batch size 16

05:11:47.640 --> 05:11:49.640
batch size maybe 256

05:11:49.640 --> 05:11:51.640
we'll be like okay which ones are throwing errors and which ones aren't

05:11:51.640 --> 05:11:53.640
so what it'll do

05:11:53.640 --> 05:11:55.640
if you properly

05:11:55.640 --> 05:11:57.640
if you properly set up an auto tuning script

05:11:57.640 --> 05:11:59.640
is

05:11:59.640 --> 05:12:01.640
you will be able to find

05:12:01.640 --> 05:12:03.640
the most optimal

05:12:03.640 --> 05:12:05.640
set of parameters for your computer

05:12:05.640 --> 05:12:07.640
most optimal set of hyper parameters

05:12:07.640 --> 05:12:09.640
that is possible

05:12:09.640 --> 05:12:11.640
so auto tuning is cool

05:12:11.640 --> 05:12:13.640
you can definitely look more into that

05:12:13.640 --> 05:12:15.640
there's tons of research on it

05:12:15.640 --> 05:12:17.640
and yeah so

05:12:17.640 --> 05:12:19.640
auto tuning is cool let's dig into the next part

05:12:19.640 --> 05:12:21.640
the next little trick we use in practice

05:12:21.640 --> 05:12:23.640
especially by machine learning engineers

05:12:23.640 --> 05:12:25.640
it's a little something called arguments

05:12:25.640 --> 05:12:27.640
so you pass an argument into

05:12:27.640 --> 05:12:29.640
not necessarily a function but into the command line

05:12:29.640 --> 05:12:31.640
so this is what it'll look like

05:12:31.640 --> 05:12:33.640
this is just a basic example

05:12:33.640 --> 05:12:35.640
of what arg parsing will look like

05:12:35.640 --> 05:12:37.640
so just go

05:12:37.640 --> 05:12:39.640
python, arg parsing

05:12:39.640 --> 05:12:41.640
because that's a script's name

05:12:41.640 --> 05:12:43.640
I go dash

05:12:43.640 --> 05:12:45.640
llms because that's what it says

05:12:45.640 --> 05:12:47.640
right here this is what the argument is

05:12:47.640 --> 05:12:49.640
and then we can just pass in a string

05:12:49.640 --> 05:12:51.640
say hello

05:12:51.640 --> 05:12:53.640
the provided

05:12:53.640 --> 05:12:55.640
whatever is hello

05:12:55.640 --> 05:12:57.640
cool you can add little arguments to this

05:12:57.640 --> 05:12:59.640
and I'm even going to change this around

05:12:59.640 --> 05:13:01.640
I could say

05:13:05.640 --> 05:13:07.640
batch size

05:13:07.640 --> 05:13:09.640
and then

05:13:09.640 --> 05:13:11.640
let's go like that

05:13:11.640 --> 05:13:13.640
batch

05:13:13.640 --> 05:13:15.640
batch size

05:13:19.640 --> 05:13:21.640
please

05:13:21.640 --> 05:13:23.640
provide

05:13:23.640 --> 05:13:25.640
a batch size

05:13:27.640 --> 05:13:29.640
I can do the same thing again

05:13:31.640 --> 05:13:33.640
and see it says

05:13:33.640 --> 05:13:35.640
following arguments required are batch size

05:13:35.640 --> 05:13:37.640
so that obviously didn't work

05:13:37.640 --> 05:13:39.640
and if we actually tried the correct way

05:13:39.640 --> 05:13:41.640
our parsing.py then we go

05:13:41.640 --> 05:13:43.640
dash, batch size

05:13:43.640 --> 05:13:45.640
we can make it 32

05:13:45.640 --> 05:13:47.640
oops

05:13:51.640 --> 05:13:53.640
that's because it's not a string

05:13:53.640 --> 05:13:55.640
so

05:13:55.640 --> 05:13:57.640
what we need to actually do

05:13:57.640 --> 05:13:59.640
is it's bs somewhere

05:13:59.640 --> 05:14:01.640
okay

05:14:01.640 --> 05:14:03.640
so

05:14:03.640 --> 05:14:05.640
args

05:14:05.640 --> 05:14:07.640
parse args

05:14:07.640 --> 05:14:09.640
so we need to change this

05:14:09.640 --> 05:14:11.640
to bs like that

05:14:11.640 --> 05:14:13.640
let me go batch size

05:14:13.640 --> 05:14:15.640
batch size is 32

05:14:15.640 --> 05:14:17.640
okay

05:14:17.640 --> 05:14:19.640
so even I'm a little bit new to arguments as well

05:14:19.640 --> 05:14:21.640
but

05:14:21.640 --> 05:14:23.640
this is something that comes in very handy

05:14:23.640 --> 05:14:25.640
when you're trying to know each time

05:14:25.640 --> 05:14:27.640
you're trying to change some parameters

05:14:27.640 --> 05:14:29.640
if you add

05:14:29.640 --> 05:14:31.640
new gpu or whatever and you're like oh I want to double my batch size

05:14:31.640 --> 05:14:33.640
it's like sure you can easily do that

05:14:33.640 --> 05:14:35.640
so a lot of the times

05:14:35.640 --> 05:14:37.640
it won't just have one but you'll have like

05:14:37.640 --> 05:14:39.640
many meaning like maybe a dozen

05:14:39.640 --> 05:14:41.640
or so of these

05:14:41.640 --> 05:14:43.640
of these little arguments

05:14:43.640 --> 05:14:45.640
so that is what this looks like

05:14:45.640 --> 05:14:47.640
and

05:14:47.640 --> 05:14:49.640
we're going to go ahead and implement this

05:14:49.640 --> 05:14:51.640
into our little script here

05:14:51.640 --> 05:14:53.640
so

05:14:53.640 --> 05:14:55.640
I'm just going to

05:14:55.640 --> 05:14:57.640
pop over to gpt1

05:14:57.640 --> 05:14:59.640
I'm going to pull this up on my

05:14:59.640 --> 05:15:01.640
second monitor here

05:15:03.640 --> 05:15:05.640
and

05:15:05.640 --> 05:15:07.640
in terms of these

05:15:07.640 --> 05:15:09.640
I'm just going to start off

05:15:09.640 --> 05:15:11.640
making a

05:15:11.640 --> 05:15:13.640
importation

05:15:13.640 --> 05:15:15.640
arg

05:15:15.640 --> 05:15:17.640
arg parser

05:15:17.640 --> 05:15:19.640
or arg parse rather

05:15:19.640 --> 05:15:21.640
that's what it's called

05:15:21.640 --> 05:15:23.640
and then we go

05:15:23.640 --> 05:15:25.640
parser is equal to

05:15:25.640 --> 05:15:27.640
I'll just

05:15:27.640 --> 05:15:29.640
copy and paste this entire thing

05:15:29.640 --> 05:15:31.640
and why not

05:15:31.640 --> 05:15:33.640
cool

05:15:35.640 --> 05:15:37.640
okay

05:15:37.640 --> 05:15:39.640
so

05:15:39.640 --> 05:15:41.640
we get a batch size

05:15:41.640 --> 05:15:43.640
or something

05:15:43.640 --> 05:15:45.640
and then

05:15:45.640 --> 05:15:47.640
we'll add in the second part here

05:15:47.640 --> 05:15:49.640
so

05:15:55.640 --> 05:15:57.640
args parse the arguments

05:15:57.640 --> 05:15:59.640
here

05:16:01.640 --> 05:16:03.640
and the little scope

05:16:03.640 --> 05:16:05.640
of

05:16:05.640 --> 05:16:07.640
batch size like that

05:16:07.640 --> 05:16:09.640
our batch size is equal to

05:16:09.640 --> 05:16:11.640
whatever that was

05:16:11.640 --> 05:16:13.640
and we'll just go args

05:16:13.640 --> 05:16:15.640
dot

05:16:15.640 --> 05:16:17.640
args dot batch size so cool

05:16:19.640 --> 05:16:21.640
we're going to run this

05:16:21.640 --> 05:16:23.640
and

05:16:23.640 --> 05:16:25.640
not defined

05:16:25.640 --> 05:16:27.640
so I got a little not defined thing here

05:16:27.640 --> 05:16:29.640
and pretty much

05:16:29.640 --> 05:16:31.640
all I missed was that

05:16:31.640 --> 05:16:33.640
we're doing this so essentially

05:16:33.640 --> 05:16:35.640
this

05:16:35.640 --> 05:16:37.640
should be equal to this right here

05:16:37.640 --> 05:16:39.640
so I'm just going to go ahead and copy that

05:16:39.640 --> 05:16:41.640
and

05:16:43.640 --> 05:16:45.640
boot parse args

05:16:45.640 --> 05:16:47.640
except

05:16:47.640 --> 05:16:49.640
we don't have a parse args function

05:16:49.640 --> 05:16:51.640
so

05:16:51.640 --> 05:16:53.640
what do we need to do instead

05:16:53.640 --> 05:16:55.640
well it

05:16:55.640 --> 05:16:57.640
actually that might just work on it so let's try it out

05:17:01.640 --> 05:17:03.640
okay so it looks like

05:17:03.640 --> 05:17:05.640
it's actually expecting some input here

05:17:05.640 --> 05:17:07.640
in code so

05:17:07.640 --> 05:17:09.640
that's probably working

05:17:09.640 --> 05:17:11.640
and if we

05:17:11.640 --> 05:17:13.640
ported this into a script

05:17:13.640 --> 05:17:15.640
then it would simply ask us for some input

05:17:15.640 --> 05:17:17.640
so I believe we're doing this correctly

05:17:17.640 --> 05:17:19.640
let's go ahead

05:17:19.640 --> 05:17:21.640
and actually switch over

05:17:21.640 --> 05:17:23.640
and pour all of this into some code

05:17:23.640 --> 05:17:25.640
so I'm going to make

05:17:25.640 --> 05:17:27.640
a training file

05:17:27.640 --> 05:17:29.640
and a chat file

05:17:29.640 --> 05:17:31.640
the training file is going to be all of our parameters

05:17:31.640 --> 05:17:33.640
whatever all of our architecture

05:17:33.640 --> 05:17:35.640
and then the actual training loop itself

05:17:35.640 --> 05:17:37.640
we're going to have some arguments in there

05:17:37.640 --> 05:17:39.640
and then the chat bot is going to be

05:17:39.640 --> 05:17:41.640
pretty much just a question-answer

05:17:41.640 --> 05:17:43.640
thing that just reproduces text

05:17:43.640 --> 05:17:45.640
so it'll just be like prompt, completion

05:17:45.640 --> 05:17:47.640
type of thing and

05:17:47.640 --> 05:17:49.640
yeah so let's go ahead and implement that here

05:17:49.640 --> 05:17:51.640
so in our

05:17:51.640 --> 05:17:53.640
GPT course

05:17:53.640 --> 05:17:55.640
here I'm going to go

05:17:55.640 --> 05:17:57.640
training.py

05:17:57.640 --> 05:17:59.640
and we're going to go

05:17:59.640 --> 05:18:01.640
chatbot.py

05:18:01.640 --> 05:18:03.640
just like that

05:18:03.640 --> 05:18:05.640
so in training

05:18:05.640 --> 05:18:07.640
let's go ahead and drag everything in here

05:18:09.640 --> 05:18:11.640
I'm just going to

05:18:11.640 --> 05:18:13.640
move this over to the second screen

05:18:13.640 --> 05:18:15.640
and just copy and paste

05:18:15.640 --> 05:18:17.640
everything in order here

05:18:17.640 --> 05:18:19.640
so next up we have our

05:18:19.640 --> 05:18:21.640
characters

05:18:21.640 --> 05:18:23.640
and then we have our

05:18:23.640 --> 05:18:25.640
tokenizer

05:18:25.640 --> 05:18:27.640
and then our

05:18:27.640 --> 05:18:29.640
getRandomChunk and getBatches

05:18:33.640 --> 05:18:35.640
suite

05:18:35.640 --> 05:18:37.640
our estimateLoss function

05:18:41.640 --> 05:18:43.640
and then this giant piece

05:18:43.640 --> 05:18:45.640
of code

05:18:45.640 --> 05:18:47.640
containing

05:18:47.640 --> 05:18:49.640
most of the architecture we built up

05:18:51.640 --> 05:18:53.640
we're just going to add that in there

05:18:53.640 --> 05:18:55.640
we're not getting any warnings

05:18:57.640 --> 05:18:59.640
and then the training loop

05:19:01.640 --> 05:19:03.640
and the optimizer

05:19:03.640 --> 05:19:05.640
awesome

05:19:05.640 --> 05:19:07.640
then after this

05:19:07.640 --> 05:19:09.640
we would simply have this context

05:19:09.640 --> 05:19:11.640
but the point of this is that we want to have this in our

05:19:11.640 --> 05:19:13.640
chatbot script

05:19:13.640 --> 05:19:15.640
so what I'm going to do

05:19:15.640 --> 05:19:17.640
is in this training.py

05:19:17.640 --> 05:19:19.640
I'm going to keep

05:19:19.640 --> 05:19:21.640
all of these the same I'm going to keep this entire thing

05:19:21.640 --> 05:19:23.640
the same

05:19:23.640 --> 05:19:25.640
get rid of this little block of code

05:19:25.640 --> 05:19:27.640
and we're going to go into

05:19:27.640 --> 05:19:29.640
the chatbot

05:19:29.640 --> 05:19:31.640
here so loadingMile

05:19:31.640 --> 05:19:33.640
parameters good we want to load some in

05:19:33.640 --> 05:19:35.640
train some more and then dump it

05:19:35.640 --> 05:19:37.640
chatbot is not going to dump anything

05:19:37.640 --> 05:19:39.640
it's just going to save so I'm going to take

05:19:39.640 --> 05:19:41.640
all of our training here

05:19:43.640 --> 05:19:45.640
and instead of dumping

05:19:45.640 --> 05:19:47.640
take that away we'll also take

05:19:47.640 --> 05:19:49.640
away the training

05:19:49.640 --> 05:19:51.640
loop as well

05:19:55.640 --> 05:19:57.640
I don't believe we have anything

05:19:57.640 --> 05:19:59.640
else to actually bring in

05:19:59.640 --> 05:20:01.640
we don't need our getBatch

05:20:01.640 --> 05:20:03.640
we do not need our getRandomChunks

05:20:03.640 --> 05:20:05.640
so awesome

05:20:05.640 --> 05:20:07.640
we're just importing these parameters

05:20:07.640 --> 05:20:09.640
by default like that

05:20:09.640 --> 05:20:11.640
awesome

05:20:11.640 --> 05:20:13.640
so from this point

05:20:13.640 --> 05:20:15.640
we have imported

05:20:15.640 --> 05:20:17.640
we've imported our model

05:20:17.640 --> 05:20:19.640
cool so let's go ahead

05:20:19.640 --> 05:20:21.640
and port in our little

05:20:21.640 --> 05:20:23.640
chatbot here

05:20:23.640 --> 05:20:25.640
this little end piece

05:20:25.640 --> 05:20:27.640
which is going to allow us to

05:20:27.640 --> 05:20:29.640
essentially chat with the model

05:20:29.640 --> 05:20:31.640
this is what it looks like a little wild loop

05:20:31.640 --> 05:20:33.640
we have a prompt we just input

05:20:33.640 --> 05:20:35.640
something

05:20:35.640 --> 05:20:37.640
prompt next line that should be fairly self explanatory

05:20:37.640 --> 05:20:39.640
and we have this tensor

05:20:39.640 --> 05:20:41.640
we're going to encode this prompt into a bunch

05:20:41.640 --> 05:20:43.640
of integers or torch.long data types

05:20:43.640 --> 05:20:45.640
on the GPU

05:20:45.640 --> 05:20:47.640
devices CUDA

05:20:47.640 --> 05:20:49.640
and then after

05:20:49.640 --> 05:20:51.640
after we've actually generated these

05:20:51.640 --> 05:20:53.640
so model.generate

05:20:53.640 --> 05:20:55.640
we're going to unsqueeze these

05:20:55.640 --> 05:20:57.640
remember it's a torch.tensor

05:20:57.640 --> 05:20:59.640
so it's going to be in the matrices form

05:20:59.640 --> 05:21:01.640
so it's going to look like this

05:21:01.640 --> 05:21:03.640
it's going to look like this or whatever

05:21:03.640 --> 05:21:05.640
that's essentially what the shape is

05:21:05.640 --> 05:21:07.640
so all we're doing when we unsqueeze it

05:21:07.640 --> 05:21:09.640
is we're just taking away this wrapping

05:21:09.640 --> 05:21:11.640
around it

05:21:11.640 --> 05:21:13.640
so awesome

05:21:13.640 --> 05:21:15.640
we're just going to do some

05:21:15.640 --> 05:21:17.640
tokens for example 150 here

05:21:17.640 --> 05:21:19.640
and then to a list format

05:21:19.640 --> 05:21:21.640
and then we can just print these out

05:21:21.640 --> 05:21:23.640
as January characters

05:21:23.640 --> 05:21:25.640
awesome so we're just going to ask this prompt

05:21:25.640 --> 05:21:27.640
and then do some compute give us a completion

05:21:27.640 --> 05:21:29.640
so on and so forth

05:21:29.640 --> 05:21:31.640
so that's what this is doing here

05:21:31.640 --> 05:21:33.640
and another thing I wanted to point out

05:21:33.640 --> 05:21:35.640
is actually when we load these

05:21:35.640 --> 05:21:37.640
parameters in

05:21:37.640 --> 05:21:39.640
at least on training

05:21:39.640 --> 05:21:41.640
it's going to initially give us errors

05:21:41.640 --> 05:21:43.640
from we're going to get errors from that

05:21:43.640 --> 05:21:45.640
because the model will just not be

05:21:45.640 --> 05:21:47.640
anything and we won't be able to import stuff

05:21:47.640 --> 05:21:49.640
so that's going to give you errors first of all

05:21:49.640 --> 05:21:51.640
another thing you want to pay attention to

05:21:51.640 --> 05:21:53.640
is to make sure that when you've actually trained

05:21:53.640 --> 05:21:55.640
this initial model that it matches

05:21:55.640 --> 05:21:57.640
all of the architectural

05:21:57.640 --> 05:21:59.640
stuff and the hyper parameters

05:21:59.640 --> 05:22:01.640
that you used

05:22:01.640 --> 05:22:03.640
that when you're using to load up again

05:22:03.640 --> 05:22:05.640
so

05:22:05.640 --> 05:22:07.640
when you're running your forward pass and whatnot

05:22:07.640 --> 05:22:09.640
you just want to make sure that this architecture

05:22:09.640 --> 05:22:11.640
sort of lines up with it

05:22:11.640 --> 05:22:13.640
just so that you don't get any architectural errors

05:22:13.640 --> 05:22:15.640
those can be really confusing to debug

05:22:15.640 --> 05:22:17.640
so yeah

05:22:17.640 --> 05:22:19.640
and the way we can do this is actually just

05:22:19.640 --> 05:22:21.640
commenting it out here

05:22:21.640 --> 05:22:23.640
awesome, we're able to save load models

05:22:23.640 --> 05:22:25.640
and

05:22:25.640 --> 05:22:27.640
we're able to use a little loop

05:22:27.640 --> 05:22:29.640
to create a sort of

05:22:29.640 --> 05:22:31.640
chat-up that's not really helpful

05:22:31.640 --> 05:22:33.640
because we haven't trained it

05:22:33.640 --> 05:22:35.640
an insane amount on

05:22:35.640 --> 05:22:37.640
data that actually is useful

05:22:37.640 --> 05:22:39.640
so another little detail that's very important

05:22:39.640 --> 05:22:41.640
is to actually

05:22:41.640 --> 05:22:43.640
make sure that you have nn-module in all

05:22:43.640 --> 05:22:45.640
of these classes and subclasses

05:22:45.640 --> 05:22:47.640
nn.module basically works

05:22:47.640 --> 05:22:49.640
as a tracker for all of your

05:22:49.640 --> 05:22:51.640
parameters it makes

05:22:51.640 --> 05:22:53.640
make sure that all of your

05:22:53.640 --> 05:22:55.640
nn extensions run correctly

05:22:55.640 --> 05:22:57.640
and just overall a cornerstone

05:22:57.640 --> 05:22:59.640
for PyTorch like you need it

05:22:59.640 --> 05:23:01.640
so make sure you have nn-module in all of these classes

05:23:01.640 --> 05:23:03.640
I know that

05:23:03.640 --> 05:23:05.640
block sort of comes out of GPT

05:23:05.640 --> 05:23:07.640
language model and so on and so forth

05:23:07.640 --> 05:23:09.640
but just all of these

05:23:09.640 --> 05:23:11.640
classes with nn

05:23:11.640 --> 05:23:13.640
or any learnable parameters

05:23:13.640 --> 05:23:15.640
you will need it in it's overall just

05:23:15.640 --> 05:23:17.640
a good practice to have nn-module in all

05:23:17.640 --> 05:23:19.640
of your classes overall

05:23:19.640 --> 05:23:21.640
just to sort of avoid those errors

05:23:21.640 --> 05:23:23.640
so cool

05:23:23.640 --> 05:23:25.640
I didn't explicitly go over that

05:23:25.640 --> 05:23:27.640
at the beginning but that's just a heads up

05:23:27.640 --> 05:23:29.640
you always want to make sure nn-module is inside of these

05:23:29.640 --> 05:23:31.640
so cool

05:23:31.640 --> 05:23:33.640
now

05:23:33.640 --> 05:23:35.640
something I'd like to highlight

05:23:35.640 --> 05:23:37.640
is a little error that we get

05:23:37.640 --> 05:23:39.640
we try to generate when we have max new

05:23:39.640 --> 05:23:41.640
tokens above block size so let me show you

05:23:41.640 --> 05:23:43.640
that right now

05:23:43.640 --> 05:23:45.640
you just go python, chat bot

05:23:45.640 --> 05:23:47.640
and then batch size 32

05:23:47.640 --> 05:23:49.640
so we could say

05:23:49.640 --> 05:23:51.640
we could say hello

05:23:51.640 --> 05:23:53.640
for example

05:23:55.640 --> 05:23:57.640
okay so it's going to give us

05:23:57.640 --> 05:23:59.640
some errors here and what exactly

05:23:59.640 --> 05:24:01.640
does this error mean

05:24:01.640 --> 05:24:03.640
well when we try to

05:24:03.640 --> 05:24:05.640
generate 150 new

05:24:05.640 --> 05:24:07.640
tokens what it's doing

05:24:07.640 --> 05:24:09.640
is it's taking the previous

05:24:09.640 --> 05:24:11.640
you know

05:24:11.640 --> 05:24:13.640
H-E-L-L-O

05:24:13.640 --> 05:24:15.640
exclamation mark 6 tokens

05:24:15.640 --> 05:24:17.640
and it's pretty much adding up 150

05:24:17.640 --> 05:24:19.640
on top of that so we have

05:24:19.640 --> 05:24:21.640
156 tokens

05:24:21.640 --> 05:24:23.640
that we're now trying to fit inside of block size

05:24:23.640 --> 05:24:25.640
which in our case is

05:24:25.640 --> 05:24:27.640
128

05:24:27.640 --> 05:24:29.640
so of course

05:24:29.640 --> 05:24:31.640
156 does not fit

05:24:31.640 --> 05:24:33.640
into 128 and that's

05:24:33.640 --> 05:24:35.640
why we get some errors here

05:24:35.640 --> 05:24:37.640
so

05:24:37.640 --> 05:24:39.640
all we have to do is make sure

05:24:39.640 --> 05:24:41.640
that

05:24:41.640 --> 05:24:43.640
we essentially

05:24:43.640 --> 05:24:45.640
what we could do is make sure that max new tokens

05:24:45.640 --> 05:24:47.640
is small enough and then be sort of

05:24:47.640 --> 05:24:49.640
paying attention when we make prompts

05:24:49.640 --> 05:24:51.640
or

05:24:51.640 --> 05:24:53.640
we could actually make a little

05:24:53.640 --> 05:24:55.640
cropping

05:24:55.640 --> 05:24:57.640
cropping tool here so what this will do

05:24:57.640 --> 05:24:59.640
is it will pretty much crop

05:24:59.640 --> 05:25:01.640
through the last block size tokens

05:25:01.640 --> 05:25:03.640
and

05:25:03.640 --> 05:25:05.640
this is super useful because it

05:25:05.640 --> 05:25:07.640
pretty much doesn't make us have to pay

05:25:07.640 --> 05:25:09.640
attention to max new tokens all the time

05:25:09.640 --> 05:25:11.640
and it just essentially

05:25:11.640 --> 05:25:13.640
crops it around that 128 limit

05:25:13.640 --> 05:25:15.640
so

05:25:15.640 --> 05:25:17.640
I'm going to go ahead and replace index here

05:25:17.640 --> 05:25:19.640
with index con or index condition

05:25:19.640 --> 05:25:21.640
and

05:25:21.640 --> 05:25:23.640
we go ahead and run this again

05:25:27.640 --> 05:25:29.640
so I could say hello

05:25:31.640 --> 05:25:33.640
and we get a successful

05:25:33.640 --> 05:25:35.640
completion awesome

05:25:35.640 --> 05:25:37.640
we can keep asking new prompts like this

05:25:37.640 --> 05:25:39.640
right

05:25:43.640 --> 05:25:45.640
and awesome so

05:25:45.640 --> 05:25:47.640
yeah we're not really getting any of these

05:25:47.640 --> 05:25:49.640
dimensionality like

05:25:49.640 --> 05:25:51.640
architecture fitting type errors if you want to call them

05:25:51.640 --> 05:25:53.640
if you want to make it super fancy that way

05:25:53.640 --> 05:25:55.640
but yeah

05:25:55.640 --> 05:25:57.640
not really that much else to do

05:25:57.640 --> 05:25:59.640
yeah there's a few points I want to go over

05:25:59.640 --> 05:26:01.640
including fine tuning

05:26:01.640 --> 05:26:03.640
so I'm going to go over a little

05:26:03.640 --> 05:26:05.640
illustrative example as to what

05:26:05.640 --> 05:26:07.640
fine tuning actually looks like in practice

05:26:07.640 --> 05:26:09.640
so in pre-training

05:26:09.640 --> 05:26:11.640
which is what this course is based off of

05:26:11.640 --> 05:26:13.640
in pre-training you have this

05:26:13.640 --> 05:26:15.640
giant text corpus right you have this

05:26:15.640 --> 05:26:17.640
giant corpus here

05:26:19.640 --> 05:26:21.640
some text in it

05:26:21.640 --> 05:26:23.640
and essentially

05:26:23.640 --> 05:26:25.640
what you do is you take out little snippets

05:26:25.640 --> 05:26:27.640
these are called

05:26:27.640 --> 05:26:29.640
blocks or batches

05:26:29.640 --> 05:26:31.640
or chunks you could say you take out little batches

05:26:31.640 --> 05:26:33.640
of these you sample

05:26:33.640 --> 05:26:35.640
random little blocks and you take multiple batches

05:26:35.640 --> 05:26:37.640
of them and

05:26:37.640 --> 05:26:39.640
you essentially have this

05:26:39.640 --> 05:26:41.640
let's just say

05:26:41.640 --> 05:26:43.640
H E L L O

05:26:43.640 --> 05:26:45.640
and maybe the next

05:26:45.640 --> 05:26:47.640
predict maybe the outputs

05:26:47.640 --> 05:26:49.640
or the targets rather

05:26:49.640 --> 05:26:51.640
or

05:26:51.640 --> 05:26:53.640
the L L O

05:26:53.640 --> 05:26:55.640
exclamation mark

05:26:55.640 --> 05:26:57.640
so it's just shifted over by one

05:26:57.640 --> 05:26:59.640
and so given this

05:26:59.640 --> 05:27:01.640
sequence of characters

05:27:01.640 --> 05:27:03.640
you want to predict this which is just

05:27:03.640 --> 05:27:05.640
the input shifted by one

05:27:05.640 --> 05:27:07.640
that's what pre-training is

05:27:07.640 --> 05:27:09.640
and keep in mind that these are the same size

05:27:09.640 --> 05:27:11.640
this is one, two,

05:27:11.640 --> 05:27:13.640
three, four, and five

05:27:13.640 --> 05:27:15.640
same thing here these are both

05:27:15.640 --> 05:27:17.640
five characters long

05:27:17.640 --> 05:27:19.640
fine tuning however is not completely the same

05:27:19.640 --> 05:27:21.640
so I could have

05:27:21.640 --> 05:27:23.640
hello

05:27:23.640 --> 05:27:25.640
and then maybe like a question mark

05:27:25.640 --> 05:27:27.640
and it would respond

05:27:27.640 --> 05:27:29.640
you know

05:27:31.640 --> 05:27:33.640
the model might respond

05:27:33.640 --> 05:27:35.640
L R U

05:27:35.640 --> 05:27:37.640
maybe that's just a

05:27:37.640 --> 05:27:39.640
a response that it gives us

05:27:39.640 --> 05:27:41.640
we can obviously see that hello does not have the same amount of characters

05:27:41.640 --> 05:27:43.640
with the same amount of indices

05:27:43.640 --> 05:27:45.640
as how are you

05:27:45.640 --> 05:27:47.640
so

05:27:47.640 --> 05:27:49.640
this is essentially the difference between

05:27:49.640 --> 05:27:51.640
fine tuning and pre-training

05:27:51.640 --> 05:27:53.640
with fine tuning you just have to add a little bit of

05:27:53.640 --> 05:27:55.640
different things in your generate function

05:27:55.640 --> 05:27:57.640
to compensate for not having

05:27:57.640 --> 05:27:59.640
the same

05:27:59.640 --> 05:28:01.640
amount of indices in your inputs

05:28:01.640 --> 05:28:03.640
and targets and rather just

05:28:03.640 --> 05:28:05.640
generate until you receive an end token

05:28:05.640 --> 05:28:07.640
so

05:28:07.640 --> 05:28:09.640
what they don't explicitly say here is at the

05:28:09.640 --> 05:28:11.640
end of this question

05:28:11.640 --> 05:28:13.640
there's actually a little end token which we usually

05:28:13.640 --> 05:28:15.640
do

05:28:15.640 --> 05:28:17.640
looks like this

05:28:17.640 --> 05:28:19.640
like that

05:28:19.640 --> 05:28:21.640
or

05:28:21.640 --> 05:28:23.640
like this

05:28:23.640 --> 05:28:25.640
these are end tokens and then you typically

05:28:25.640 --> 05:28:27.640
have the same for start tokens like an s

05:28:27.640 --> 05:28:29.640
or

05:28:29.640 --> 05:28:31.640
start

05:28:31.640 --> 05:28:33.640
like that, pretty simple

05:28:33.640 --> 05:28:35.640
and essentially you would just append them

05:28:37.640 --> 05:28:39.640
and

05:28:39.640 --> 05:28:41.640
a start token

05:28:41.640 --> 05:28:43.640
the start token doesn't matter as much

05:28:43.640 --> 05:28:45.640
as we essentially just are looking at

05:28:45.640 --> 05:28:47.640
what this does and then

05:28:47.640 --> 05:28:49.640
we start generating the start doesn't really

05:28:49.640 --> 05:28:51.640
matter because

05:28:51.640 --> 05:28:53.640
we don't really need to know when to start generating

05:28:53.640 --> 05:28:55.640
it just happens but the end token is

05:28:55.640 --> 05:28:57.640
important because we don't want to just generate

05:28:57.640 --> 05:28:59.640
an infinite number of tokens

05:28:59.640 --> 05:29:01.640
because these aren't the same size

05:29:01.640 --> 05:29:03.640
it could theoretically generate a really

05:29:03.640 --> 05:29:05.640
really long completion

05:29:05.640 --> 05:29:07.640
so all we want to make sure

05:29:07.640 --> 05:29:09.640
is that it's not generating an infinite amount of tokens

05:29:09.640 --> 05:29:11.640
consuming an infinite amount of computation

05:29:11.640 --> 05:29:13.640
and just to prevent that loop

05:29:13.640 --> 05:29:15.640
so that's why we append this end token

05:29:15.640 --> 05:29:17.640
to the end here

05:29:19.640 --> 05:29:21.640
we have this little end bit

05:29:21.640 --> 05:29:23.640
and

05:29:23.640 --> 05:29:25.640
essentially once this end token is sampled

05:29:25.640 --> 05:29:27.640
you would end the generation

05:29:27.640 --> 05:29:29.640
simple as that

05:29:29.640 --> 05:29:31.640
and we don't actually

05:29:31.640 --> 05:29:33.640
sample from the token itself

05:29:33.640 --> 05:29:35.640
but rather the actual

05:29:35.640 --> 05:29:37.640
the

05:29:37.640 --> 05:29:39.640
I guess you could say index

05:29:39.640 --> 05:29:41.640
or the miracle value

05:29:41.640 --> 05:29:43.640
the encoded version of end

05:29:43.640 --> 05:29:45.640
which

05:29:45.640 --> 05:29:47.640
is usually just going to be the length of your vocab

05:29:47.640 --> 05:29:49.640
size

05:29:49.640 --> 05:29:51.640
plus one

05:29:51.640 --> 05:29:53.640
so if your vocab size in our case

05:29:53.640 --> 05:29:55.640
is like maybe 32,000

05:29:55.640 --> 05:29:57.640
your end token would be at index

05:29:57.640 --> 05:29:59.640
32,001

05:29:59.640 --> 05:30:01.640
so that way when you sample

05:30:01.640 --> 05:30:03.640
when you sample an end token

05:30:03.640 --> 05:30:05.640
when you sample that

05:30:05.640 --> 05:30:07.640
32,001 token

05:30:09.640 --> 05:30:11.640
you actually just end the sequence

05:30:11.640 --> 05:30:13.640
and of course when you train

05:30:13.640 --> 05:30:15.640
your model you're always

05:30:15.640 --> 05:30:17.640
appending this end token to the end

05:30:17.640 --> 05:30:19.640
so you get your initial inputs

05:30:19.640 --> 05:30:21.640
and then inside of either your

05:30:21.640 --> 05:30:23.640
training data

05:30:23.640 --> 05:30:25.640
or when you actually are processing it

05:30:25.640 --> 05:30:27.640
and feeding it into that transformer

05:30:27.640 --> 05:30:29.640
you have some sort of function that's just appending

05:30:29.640 --> 05:30:31.640
that little

05:30:31.640 --> 05:30:33.640
32,001 token index

05:30:33.640 --> 05:30:35.640
to it

05:30:35.640 --> 05:30:37.640
so that's pretty much what fine tuning is

05:30:37.640 --> 05:30:39.640
it comes up fine tuning

05:30:39.640 --> 05:30:41.640
and the whole process of creating

05:30:41.640 --> 05:30:43.640
these giant language models

05:30:43.640 --> 05:30:45.640
is to of course help people

05:30:45.640 --> 05:30:47.640
and there's no better way to do that

05:30:47.640 --> 05:30:49.640
than to

05:30:49.640 --> 05:30:51.640
literally have all the information

05:30:51.640 --> 05:30:53.640
that humans have ever known meaning like common crawl

05:30:53.640 --> 05:30:55.640
open web text or Wikipedia

05:30:55.640 --> 05:30:57.640
and even research papers

05:30:57.640 --> 05:30:59.640
pre-training on all of that

05:30:59.640 --> 05:31:01.640
so just doing again the same size

05:31:01.640 --> 05:31:03.640
and then shift over for targets

05:31:03.640 --> 05:31:05.640
and then after you've iterated on that

05:31:05.640 --> 05:31:07.640
many many times you switch over to fine tuning

05:31:07.640 --> 05:31:09.640
where you have these

05:31:09.640 --> 05:31:11.640
specifically picked out

05:31:11.640 --> 05:31:13.640
prompt and completion pairs

05:31:13.640 --> 05:31:15.640
and you just train on those for a really long time

05:31:15.640 --> 05:31:17.640
until you are satisfied

05:31:17.640 --> 05:31:19.640
with your result

05:31:19.640 --> 05:31:21.640
and yeah that's what language modeling is

05:31:21.640 --> 05:31:23.640
there are a few key pointers I want to leave you with

05:31:23.640 --> 05:31:25.640
before you head on your way to

05:31:25.640 --> 05:31:27.640
research and development and machine learning

05:31:27.640 --> 05:31:29.640
so first things first

05:31:29.640 --> 05:31:31.640
there's a little something called

05:31:31.640 --> 05:31:33.640
efficiency testing

05:31:33.640 --> 05:31:35.640
or just finding out how quickly

05:31:35.640 --> 05:31:37.640
certain operations takes

05:31:37.640 --> 05:31:39.640
we'll just call this

05:31:39.640 --> 05:31:41.640
efficiency testing and I'll show you

05:31:41.640 --> 05:31:43.640
exactly how to do this right here

05:31:43.640 --> 05:31:45.640
efficiency

05:31:45.640 --> 05:31:47.640
yeah

05:31:47.640 --> 05:31:49.640
I don't know if I spelled that correctly

05:31:49.640 --> 05:31:51.640
I don't know what it's doing now

05:31:51.640 --> 05:31:53.640
anyways

05:31:53.640 --> 05:31:55.640
we'll just pop into code here

05:31:55.640 --> 05:31:57.640
and

05:31:57.640 --> 05:31:59.640
essentially

05:31:59.640 --> 05:32:01.640
we'll just do

05:32:01.640 --> 05:32:03.640
I don't know

05:32:03.640 --> 05:32:05.640
I'm testing

05:32:05.640 --> 05:32:07.640
import time

05:32:07.640 --> 05:32:09.640
and

05:32:09.640 --> 05:32:11.640
essentially

05:32:11.640 --> 05:32:13.640
all we're going to do is just

05:32:13.640 --> 05:32:15.640
time how long operations take

05:32:15.640 --> 05:32:17.640
so

05:32:17.640 --> 05:32:19.640
in here you can go

05:32:19.640 --> 05:32:21.640
you can go start time

05:32:21.640 --> 05:32:23.640
equals time dot time

05:32:23.640 --> 05:32:25.640
and essentially what this function does

05:32:25.640 --> 05:32:27.640
is it just takes a look at the current time right now

05:32:27.640 --> 05:32:29.640
the current like millisecond

05:32:29.640 --> 05:32:31.640
very precise

05:32:31.640 --> 05:32:33.640
and we can do some little

05:32:33.640 --> 05:32:35.640
operation like

05:32:35.640 --> 05:32:37.640
I don't know 4

05:32:37.640 --> 05:32:39.640
I in range

05:32:41.640 --> 05:32:43.640
we'll just go

05:32:43.640 --> 05:32:45.640
10,000

05:32:45.640 --> 05:32:47.640
go

05:32:47.640 --> 05:32:49.640
print

05:32:49.640 --> 05:32:51.640
I

05:32:51.640 --> 05:32:53.640
print I times 2

05:32:53.640 --> 05:32:55.640
and then we can just end the time here

05:32:55.640 --> 05:32:57.640
so go end time

05:32:57.640 --> 05:32:59.640
equals time dot time again

05:32:59.640 --> 05:33:01.640
calling the current time so we're doing

05:33:01.640 --> 05:33:03.640
right now versus back then

05:33:03.640 --> 05:33:05.640
and that little difference is how long it took to execute

05:33:05.640 --> 05:33:07.640
so all we can do

05:33:07.640 --> 05:33:09.640
is just do we can say total time

05:33:09.640 --> 05:33:11.640
we can say total time equals

05:33:11.640 --> 05:33:13.640
end time

05:33:13.640 --> 05:33:15.640
minus start time

05:33:15.640 --> 05:33:17.640
and we'll just go print

05:33:17.640 --> 05:33:19.640
end time

05:33:19.640 --> 05:33:21.640
or

05:33:23.640 --> 05:33:25.640
I'm taking

05:33:25.640 --> 05:33:27.640
let's go total

05:33:29.640 --> 05:33:31.640
total time like that

05:33:31.640 --> 05:33:33.640
just execute this

05:33:35.640 --> 05:33:37.640
Python

05:33:37.640 --> 05:33:39.640
time testing

05:33:39.640 --> 05:33:41.640
cool

05:33:41.640 --> 05:33:43.640
time taken 1.32 seconds

05:33:43.640 --> 05:33:45.640
so you can essentially time every single operation

05:33:45.640 --> 05:33:47.640
you do with this method

05:33:47.640 --> 05:33:49.640
and you can see even in your

05:33:49.640 --> 05:33:51.640
I encourage you to actually try this out

05:33:51.640 --> 05:33:53.640
I'm not going to but I encourage you to try out

05:33:53.640 --> 05:33:55.640
how long the model actually takes

05:33:55.640 --> 05:33:57.640
to do certain things like how long does it take

05:33:57.640 --> 05:33:59.640
to load a model how does it take to save a model

05:33:59.640 --> 05:34:01.640
how long does it take to estimate the loss

05:34:01.640 --> 05:34:03.640
right

05:34:03.640 --> 05:34:05.640
play around with hyperparameters see how long things take

05:34:05.640 --> 05:34:07.640
and maybe you'll figure out something new who knows

05:34:07.640 --> 05:34:09.640
but this is a little something we use

05:34:09.640 --> 05:34:11.640
to pretty much test how long something

05:34:11.640 --> 05:34:13.640
takes how efficient it is

05:34:13.640 --> 05:34:15.640
and then to also see if

05:34:15.640 --> 05:34:17.640
it's worth investigating a new way of approaching

05:34:17.640 --> 05:34:19.640
something in case it takes

05:34:19.640 --> 05:34:21.640
ridiculous amount of time

05:34:21.640 --> 05:34:23.640
so that's time testing

05:34:23.640 --> 05:34:25.640
and efficiency testing for you

05:34:25.640 --> 05:34:27.640
the next little bit I want to cover

05:34:27.640 --> 05:34:29.640
is the history

05:34:29.640 --> 05:34:31.640
I'm not going to go over the entire history

05:34:31.640 --> 05:34:33.640
of AI and LLMs

05:34:33.640 --> 05:34:35.640
but essentially

05:34:35.640 --> 05:34:37.640
we originated with something called RNNs

05:34:37.640 --> 05:34:39.640
okay RNNs are called

05:34:39.640 --> 05:34:41.640
recurrent neural networks

05:34:41.640 --> 05:34:43.640
and they're really inefficient

05:34:43.640 --> 05:34:45.640
at least for scaled

05:34:45.640 --> 05:34:47.640
AI systems so RNNs

05:34:47.640 --> 05:34:49.640
are a little essentially think of it as a little loop

05:34:49.640 --> 05:34:51.640
keeps learning and learning

05:34:51.640 --> 05:34:53.640
and this is sequential right

05:34:53.640 --> 05:34:55.640
it does this and then this and then this

05:34:55.640 --> 05:34:57.640
has to wait for each completion

05:34:57.640 --> 05:34:59.640
synchronous you can't have multiple of them at once

05:34:59.640 --> 05:35:01.640
because they're complex

05:35:01.640 --> 05:35:03.640
GPUs cannot run complex things

05:35:03.640 --> 05:35:05.640
they're only designed for just

05:35:05.640 --> 05:35:07.640
matrix multiplication and very simple

05:35:07.640 --> 05:35:09.640
math like that

05:35:09.640 --> 05:35:11.640
so RNNs are essentially

05:35:11.640 --> 05:35:13.640
a little bit dumber than transformers

05:35:13.640 --> 05:35:15.640
and they

05:35:15.640 --> 05:35:17.640
are run on the CPU

05:35:17.640 --> 05:35:19.640
so RNNs was where we last sort of stopped at

05:35:19.640 --> 05:35:21.640
and what I encourage you to do

05:35:21.640 --> 05:35:23.640
is look into more of the language

05:35:23.640 --> 05:35:25.640
modeling and AI

05:35:25.640 --> 05:35:27.640
history and research that has led up to this

05:35:27.640 --> 05:35:29.640
point so you can have an idea

05:35:29.640 --> 05:35:31.640
as to how researchers

05:35:31.640 --> 05:35:33.640
have been able to quickly innovate

05:35:33.640 --> 05:35:35.640
given

05:35:35.640 --> 05:35:37.640
all these historical innovations

05:35:37.640 --> 05:35:39.640
so you have like all these things leading up to the transformer

05:35:39.640 --> 05:35:41.640
well how did they all

05:35:41.640 --> 05:35:43.640
philosophize

05:35:43.640 --> 05:35:45.640
up to that point

05:35:45.640 --> 05:35:47.640
and yeah it's just

05:35:47.640 --> 05:35:49.640
something good to sort of be confident

05:35:49.640 --> 05:35:51.640
in is innovating

05:35:51.640 --> 05:35:53.640
as both a researcher

05:35:53.640 --> 05:35:55.640
and engineer and a

05:35:55.640 --> 05:35:57.640
business person

05:35:57.640 --> 05:35:59.640
so cool

05:35:59.640 --> 05:36:01.640
RNNs were where we sort of

05:36:01.640 --> 05:36:03.640
finished off and now it's transformers and GPTs

05:36:03.640 --> 05:36:05.640
that's the current state of AI

05:36:05.640 --> 05:36:07.640
next up I

05:36:07.640 --> 05:36:09.640
would like to go over something called

05:36:09.640 --> 05:36:11.640
quantization

05:36:11.640 --> 05:36:13.640
so quantization is essentially

05:36:13.640 --> 05:36:15.640
a way to reduce the memory

05:36:15.640 --> 05:36:17.640
usage by your parameters

05:36:17.640 --> 05:36:19.640
so there's actually a paper here

05:36:19.640 --> 05:36:21.640
called QLaura Efficient Fine

05:36:21.640 --> 05:36:23.640
Tuning of Quantized

05:36:23.640 --> 05:36:25.640
LLMs so

05:36:25.640 --> 05:36:27.640
all this does in simple

05:36:27.640 --> 05:36:29.640
form is pretty much instead of

05:36:29.640 --> 05:36:31.640
using 32 bit floating

05:36:31.640 --> 05:36:33.640
point numbers it goes not only

05:36:33.640 --> 05:36:35.640
to 16 bit of half precision

05:36:35.640 --> 05:36:37.640
but all the way down to 4

05:36:37.640 --> 05:36:39.640
so what this actually

05:36:39.640 --> 05:36:41.640
looks like is in binary code

05:36:41.640 --> 05:36:43.640
or in bytecode

05:36:43.640 --> 05:36:45.640
it will look

05:36:45.640 --> 05:36:47.640
here there's some array

05:36:47.640 --> 05:36:49.640
of numbers

05:36:49.640 --> 05:36:51.640
that it uses

05:36:57.640 --> 05:36:59.640
okay I can't find it

05:36:59.640 --> 05:37:01.640
but pretty much what it is

05:37:01.640 --> 05:37:03.640
it is a bunch of

05:37:03.640 --> 05:37:05.640
it's a bunch of floating point numbers

05:37:05.640 --> 05:37:07.640
and they're all between

05:37:07.640 --> 05:37:09.640
negative one and one

05:37:09.640 --> 05:37:11.640
and there are 16 of them

05:37:11.640 --> 05:37:13.640
if you have a 4 bit number

05:37:13.640 --> 05:37:15.640
that means it can hold 16 different

05:37:15.640 --> 05:37:17.640
values 0 through 15

05:37:17.640 --> 05:37:19.640
which is 16 values

05:37:19.640 --> 05:37:21.640
and all you pretty much do is you have this

05:37:21.640 --> 05:37:23.640
array of floating point numbers

05:37:23.640 --> 05:37:25.640
you use the bytecode of

05:37:25.640 --> 05:37:27.640
that 4 bit

05:37:27.640 --> 05:37:29.640
number to look up the index

05:37:29.640 --> 05:37:31.640
in that array and that is your weight

05:37:31.640 --> 05:37:33.640
that is the weight they use

05:37:33.640 --> 05:37:35.640
in your model

05:37:35.640 --> 05:37:37.640
so this way instead of using 32 bit

05:37:37.640 --> 05:37:39.640
having these super long numbers

05:37:39.640 --> 05:37:41.640
that are super precise

05:37:41.640 --> 05:37:43.640
you can have super precise numbers

05:37:43.640 --> 05:37:45.640
that are just generally good parameters

05:37:45.640 --> 05:37:47.640
to have that just perform

05:37:47.640 --> 05:37:49.640
decently

05:37:49.640 --> 05:37:51.640
they're just sort of well spread out

05:37:51.640 --> 05:37:53.640
and experimented on and they just

05:37:53.640 --> 05:37:55.640
happen to work and you have 16 of them

05:37:55.640 --> 05:37:57.640
instead of a lot

05:37:57.640 --> 05:37:59.640
so that's

05:37:59.640 --> 05:38:01.640
another cool little thing that's going on right now

05:38:01.640 --> 05:38:03.640
is 4 bit quantizations

05:38:03.640 --> 05:38:05.640
it's a little bit harder

05:38:05.640 --> 05:38:07.640
to implement

05:38:07.640 --> 05:38:09.640
I would encourage you to experiment with half precision

05:38:09.640 --> 05:38:11.640
meaning 16 bit

05:38:11.640 --> 05:38:13.640
floating point numbers

05:38:13.640 --> 05:38:15.640
so that means it occupies

05:38:15.640 --> 05:38:17.640
16 on and off switches

05:38:17.640 --> 05:38:19.640
or capacitors on your GPU

05:38:19.640 --> 05:38:21.640
and

05:38:21.640 --> 05:38:23.640
so quantization is cool to

05:38:23.640 --> 05:38:25.640
sort of scale down the memory

05:38:25.640 --> 05:38:27.640
so that way you can scale up all of your hyper parameters

05:38:27.640 --> 05:38:29.640
and have a more complex model

05:38:29.640 --> 05:38:31.640
with these

05:38:31.640 --> 05:38:33.640
yeah just essentially to have bigger models

05:38:33.640 --> 05:38:35.640
with less space

05:38:35.640 --> 05:38:37.640
take it up

05:38:37.640 --> 05:38:39.640
so that is

05:38:39.640 --> 05:38:41.640
quantization

05:38:41.640 --> 05:38:43.640
and this is the paper for it

05:38:43.640 --> 05:38:45.640
it's a little link you can search out if you want to get

05:38:45.640 --> 05:38:47.640
more familiar with this see

05:38:47.640 --> 05:38:49.640
sort of performance standards and what not

05:38:49.640 --> 05:38:51.640
the next thing I'd like to cover

05:38:51.640 --> 05:38:53.640
is gradient accumulation

05:38:53.640 --> 05:38:55.640
so you might have heard of this you might not have heard of this

05:38:55.640 --> 05:38:57.640
gradient accumulation

05:38:57.640 --> 05:38:59.640
will

05:38:59.640 --> 05:39:01.640
what gradient accumulation does

05:39:01.640 --> 05:39:03.640
is it will accumulate

05:39:03.640 --> 05:39:05.640
gradients

05:39:05.640 --> 05:39:07.640
over say we just set a variable

05:39:07.640 --> 05:39:09.640
x so every x iterations

05:39:09.640 --> 05:39:11.640
it'll just accumulate those

05:39:11.640 --> 05:39:13.640
iterations, average them

05:39:13.640 --> 05:39:15.640
and what this allows you to do

05:39:15.640 --> 05:39:17.640
is instead of

05:39:17.640 --> 05:39:19.640
updating each iteration

05:39:19.640 --> 05:39:21.640
you're updating every x iterations

05:39:21.640 --> 05:39:23.640
so that allows you to fit

05:39:23.640 --> 05:39:25.640
more parameters and more info

05:39:25.640 --> 05:39:27.640
or generalization into this one piece

05:39:27.640 --> 05:39:29.640
so that way when you

05:39:29.640 --> 05:39:31.640
update your parameters

05:39:31.640 --> 05:39:33.640
it's able to generalize more

05:39:33.640 --> 05:39:35.640
over maybe a higher batch size

05:39:35.640 --> 05:39:37.640
or a higher block size

05:39:37.640 --> 05:39:39.640
so when you distribute this

05:39:39.640 --> 05:39:41.640
over many

05:39:41.640 --> 05:39:43.640
iterations and average them

05:39:43.640 --> 05:39:45.640
you can fit more into each iteration

05:39:45.640 --> 05:39:47.640
because it's sort of calculating

05:39:47.640 --> 05:39:49.640
all of them combined

05:39:49.640 --> 05:39:51.640
so yeah that's a cool little trick

05:39:51.640 --> 05:39:53.640
you can use if

05:39:53.640 --> 05:39:55.640
your GPU maybe isn't

05:39:55.640 --> 05:39:57.640
as big if it doesn't have as much

05:39:57.640 --> 05:39:59.640
VRAM on it

05:39:59.640 --> 05:40:01.640
so gradient accumulation is wonderful

05:40:01.640 --> 05:40:03.640
and it's used lots in practice

05:40:03.640 --> 05:40:05.640
the final thing I'd like to leave

05:40:05.640 --> 05:40:07.640
you guys off with is something called

05:40:07.640 --> 05:40:09.640
hugging face and you've probably

05:40:09.640 --> 05:40:11.640
heard a lot about this so far

05:40:11.640 --> 05:40:13.640
but let me just guide you through

05:40:13.640 --> 05:40:15.640
and show you how absolutely explosive

05:40:15.640 --> 05:40:17.640
hugging face is

05:40:17.640 --> 05:40:19.640
for machine learning so you have

05:40:19.640 --> 05:40:21.640
a bunch of models, data sets

05:40:21.640 --> 05:40:23.640
spaces, docs, etc

05:40:23.640 --> 05:40:25.640
and

05:40:25.640 --> 05:40:27.640
let's go to models for example

05:40:27.640 --> 05:40:29.640
so let's just showcase how cool this is

05:40:29.640 --> 05:40:31.640
you have multimodal AIs which could be

05:40:31.640 --> 05:40:33.640
like

05:40:33.640 --> 05:40:35.640
image and text or video

05:40:35.640 --> 05:40:37.640
etc you have multiple different modes

05:40:37.640 --> 05:40:39.640
so it's not just text or not just video

05:40:39.640 --> 05:40:41.640
it's many different ones at the same time

05:40:41.640 --> 05:40:43.640
so you have multimodal models

05:40:43.640 --> 05:40:45.640
you have computer vision

05:40:45.640 --> 05:40:47.640
you have natural language processing

05:40:47.640 --> 05:40:49.640
and we're actually doing natural language

05:40:49.640 --> 05:40:51.640
processing in this course

05:40:51.640 --> 05:40:53.640
we have audio, a tabular

05:40:53.640 --> 05:40:55.640
and reinforcement learning

05:40:55.640 --> 05:40:57.640
so this is really cool

05:40:57.640 --> 05:40:59.640
and you can actually just download these models

05:40:59.640 --> 05:41:01.640
and host them on your own computer

05:41:01.640 --> 05:41:03.640
that is really cool

05:41:03.640 --> 05:41:05.640
you also have data sets which are even cooler

05:41:05.640 --> 05:41:07.640
and these are pretty much

05:41:07.640 --> 05:41:09.640
just really high quality data sets

05:41:09.640 --> 05:41:11.640
of prompt and answer completions

05:41:11.640 --> 05:41:13.640
at least for our purpose

05:41:13.640 --> 05:41:15.640
if you want to use those

05:41:15.640 --> 05:41:17.640
so you have

05:41:17.640 --> 05:41:19.640
question answering

05:41:19.640 --> 05:41:21.640
or conversational

05:41:21.640 --> 05:41:23.640
work data set for example

05:41:23.640 --> 05:41:25.640
as 9000 downloads

05:41:25.640 --> 05:41:27.640
500 likes

05:41:27.640 --> 05:41:29.640
it has a bunch of

05:41:29.640 --> 05:41:31.640
IDs, system prompts

05:41:31.640 --> 05:41:33.640
so you're an AI assistant or whatever

05:41:33.640 --> 05:41:35.640
and then you have the cool stuff which is

05:41:35.640 --> 05:41:37.640
you'll be given a definition of a task first

05:41:37.640 --> 05:41:39.640
and some input of the task etc

05:41:39.640 --> 05:41:41.640
and then the response it's like oh

05:41:41.640 --> 05:41:43.640
we just gave it an input and asked it to answer

05:41:43.640 --> 05:41:45.640
in a format and actually did that

05:41:45.640 --> 05:41:47.640
correctly so

05:41:47.640 --> 05:41:49.640
you could pretty much train these

05:41:49.640 --> 05:41:51.640
on a bunch of

05:41:51.640 --> 05:41:53.640
prompts that you would be able to feed into GPT-4

05:41:53.640 --> 05:41:55.640
and try to make your model perform that way

05:41:55.640 --> 05:41:57.640
and this actually has

05:41:57.640 --> 05:41:59.640
4.23 million rows

05:41:59.640 --> 05:42:01.640
in the training split which is amazing

05:42:01.640 --> 05:42:03.640
so

05:42:03.640 --> 05:42:05.640
data sets are wonderful

05:42:05.640 --> 05:42:07.640
and you can find the best ones

05:42:07.640 --> 05:42:09.640
at least the best fine tuning data sets on OpenORCA

05:42:09.640 --> 05:42:11.640
really good

05:42:11.640 --> 05:42:13.640
as for pre-training

05:42:13.640 --> 05:42:15.640
I believe I mentioned this earlier

05:42:15.640 --> 05:42:17.640
in this survey of large language models

05:42:17.640 --> 05:42:19.640
that we just

05:42:19.640 --> 05:42:21.640
put down through Reddit links

05:42:25.640 --> 05:42:27.640
yep so you could use like OpenWebText

05:42:27.640 --> 05:42:29.640
you could use CommonCrawl

05:42:29.640 --> 05:42:31.640
you could use Books

05:42:31.640 --> 05:42:33.640
you could use Wikipedia

05:42:33.640 --> 05:42:35.640
these are all pre-training data sources

05:42:35.640 --> 05:42:37.640
so yeah

05:42:37.640 --> 05:42:39.640
hopefully that leaves you with a better understanding

05:42:39.640 --> 05:42:41.640
on how to create GPTs, transformers

05:42:41.640 --> 05:42:43.640
and

05:42:43.640 --> 05:42:45.640
pretty good large language models from scratch

05:42:45.640 --> 05:42:47.640
with your own data that you scraped

05:42:47.640 --> 05:42:49.640
or that you downloaded

05:42:49.640 --> 05:42:51.640
and yeah

05:42:51.640 --> 05:42:53.640
that's it, thanks for watching

05:42:53.640 --> 05:42:55.640
so you've learned a ton in this course

05:42:55.640 --> 05:42:57.640
about language modeling

05:42:57.640 --> 05:42:59.640
how to use data, how to create architecture

05:42:59.640 --> 05:43:01.640
from scratch

05:43:01.640 --> 05:43:03.640
maybe even how to look at research papers

05:43:03.640 --> 05:43:05.640
so if you really enjoy this content

05:43:05.640 --> 05:43:07.640
I would encourage you to maybe subscribe

05:43:07.640 --> 05:43:09.640
and like on my YouTube channel

05:43:09.640 --> 05:43:11.640
which is in the description

05:43:11.640 --> 05:43:13.640
I make many videos about AI

05:43:13.640 --> 05:43:15.640
and computer science in general

05:43:15.640 --> 05:43:17.640
so

05:43:17.640 --> 05:43:19.640
you could totally feel free to subscribe there

05:43:19.640 --> 05:43:21.640
if you don't want to subscribe, that's fine

05:43:21.640 --> 05:43:23.640
you could always unsubscribe later if you want to

05:43:23.640 --> 05:43:25.640
it's completely free

05:43:25.640 --> 05:43:27.640
but yeah, also have a GitHub repo in the description

05:43:27.640 --> 05:43:29.640
for all the code that we used

05:43:29.640 --> 05:43:31.640
not the data because it's way too big

05:43:31.640 --> 05:43:33.640
but

05:43:33.640 --> 05:43:35.640
all of the code and the Wizard of Oz

05:43:35.640 --> 05:43:37.640
Text file

05:43:37.640 --> 05:43:39.640
so that's all in the GitHub repo in the description

05:43:39.640 --> 05:43:41.640
thanks for watching

