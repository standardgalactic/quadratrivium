1
00:00:00,000 --> 00:00:04,560
In this course, you will learn all about natural language processing and how to apply it to

2
00:00:04,560 --> 00:00:07,920
real-world problems using the Spacey Library.

3
00:00:07,920 --> 00:00:12,880
Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.

4
00:00:12,880 --> 00:00:14,960
Hi, and welcome to this video.

5
00:00:14,960 --> 00:00:21,080
My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.

6
00:00:21,080 --> 00:00:24,520
I come to NLP from a humanities perspective.

7
00:00:24,520 --> 00:00:27,520
I have my PhD in medieval history.

8
00:00:27,520 --> 00:00:32,040
But I use Spacey on a regular basis to do all of my NLP needs.

9
00:00:32,040 --> 00:00:36,120
So what you're going to get out of this video over the next few hours is a basic understanding

10
00:00:36,120 --> 00:00:43,600
of what natural language processing is, or NLP, and also how to apply it to domain-specific

11
00:00:43,600 --> 00:00:48,560
problems, or problems that exist within your own area of expertise.

12
00:00:48,560 --> 00:00:54,280
I happen to use this all the time to analyze historical documents or financial documents

13
00:00:54,280 --> 00:00:56,880
for my own personal investments.

14
00:00:56,880 --> 00:01:01,920
Over the next few hours, you're going to learn a lot about NLP, language as a whole,

15
00:01:01,920 --> 00:01:05,120
and most importantly, the Spacey Library.

16
00:01:05,120 --> 00:01:10,320
I like the Spacey Library because it's easy to use and easy to also implement really kind

17
00:01:10,320 --> 00:01:14,840
of general solutions to general problems with the off-the-shelf models that are already

18
00:01:14,840 --> 00:01:16,160
available to you.

19
00:01:16,160 --> 00:01:19,960
I'm going to walk you through, in part one of this video series, how to get the most

20
00:01:19,960 --> 00:01:23,040
out of Spacey with these off-the-shelf features.

21
00:01:23,040 --> 00:01:26,480
In part two, we're going to start tackling some of the features that don't exist in

22
00:01:26,480 --> 00:01:31,760
off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components

23
00:01:31,760 --> 00:01:37,960
in Spacey, to actually solve domain-specific problems in your own area, from the entity

24
00:01:37,960 --> 00:01:44,360
ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects

25
00:01:44,360 --> 00:01:50,440
patterns in a custom Spacey component that doesn't actually exist at the moment.

26
00:01:50,440 --> 00:01:54,560
I'm going to be showing you all that in part two, so that in part three, we can take the

27
00:01:54,560 --> 00:01:59,640
lessons that we learned in part one and part two, and actually apply them to solve a very

28
00:01:59,640 --> 00:02:06,560
kind of common problem that exists in an LP, and that is information extraction from financial

29
00:02:06,560 --> 00:02:07,560
documents.

30
00:02:07,560 --> 00:02:14,160
So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.

31
00:02:14,160 --> 00:02:19,200
If you join me over the next few hours, you will leave this lesson with a good understanding

32
00:02:19,200 --> 00:02:23,680
of Spacey, and also a good understanding of kind of the off-the-shelf components that

33
00:02:23,680 --> 00:02:30,920
are there, and a way to take the off-the-shelf components and apply them to your own domain.

34
00:02:30,920 --> 00:02:34,360
If you also join me in this video and you like it, please let me know in the comments

35
00:02:34,360 --> 00:02:38,960
down below, because I am interested in making a second part to this video that will explore

36
00:02:38,960 --> 00:02:44,200
not only the rules-based aspects of Spacey, but the machine learning-based aspects of

37
00:02:44,200 --> 00:02:45,200
Spacey.

38
00:02:45,200 --> 00:02:49,320
So teaching you how to train your own models to do your own things, such as training a

39
00:02:49,320 --> 00:02:54,320
dependency parser, training a named entity recognizer, things like this, which are not

40
00:02:54,320 --> 00:02:56,400
covered in this video.

41
00:02:56,400 --> 00:03:00,400
Nevertheless, if you join me for this one and you like it, you will find part two much

42
00:03:00,400 --> 00:03:02,480
easier to understand.

43
00:03:02,480 --> 00:03:09,400
So sit back, relax, and let's jump into what NLP is, what kind of things you can do with

44
00:03:09,400 --> 00:03:14,240
NLP, such as information extraction, and what the Spacey library is, and how this course

45
00:03:14,240 --> 00:03:15,680
will be laid out.

46
00:03:15,680 --> 00:03:20,800
If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital

47
00:03:20,800 --> 00:03:24,380
Humanities, which is linked in the description down below.

48
00:03:24,380 --> 00:03:29,440
Even if you're not a digital humanist like me, you will find these Python tutorials useful

49
00:03:29,440 --> 00:03:36,120
because they take Python and make it accessible to students of all levels, specifically those

50
00:03:36,120 --> 00:03:37,320
who are beginners.

51
00:03:37,320 --> 00:03:41,960
I walk you through not only the basics of Python, but also I walk you through step-by-step

52
00:03:41,960 --> 00:03:45,240
some of the more common libraries that you need.

53
00:03:45,240 --> 00:03:50,600
A lot of the channel deals with texts or text-based problems, but other content deals with things

54
00:03:50,600 --> 00:03:56,920
like machine learning and image classification and OCR, all in Python.

55
00:03:56,920 --> 00:04:00,840
So before we begin with Spacey, I think we should spend a little bit of time talking

56
00:04:00,840 --> 00:04:06,440
about what NLP or natural language processing actually is.

57
00:04:06,440 --> 00:04:11,600
Natural language processing is the process by which we try to get a computer system to

58
00:04:11,600 --> 00:04:18,480
understand and parse and extract human language, often times with raw text.

59
00:04:18,480 --> 00:04:21,760
There are a couple different areas of natural language processing.

60
00:04:21,760 --> 00:04:28,080
There's named entity recognition, part of speech tagging, syntactic parsing, text categorization,

61
00:04:28,080 --> 00:04:33,120
also known as text classification, co-reference resolution, machine translation.

62
00:04:33,120 --> 00:04:38,720
Adjacent to NLP is another kind of computational linguistics field called natural language

63
00:04:38,720 --> 00:04:40,980
understanding, or NLU.

64
00:04:40,980 --> 00:04:45,340
This is where we train computer systems to do things like relation extraction, semantic

65
00:04:45,340 --> 00:04:53,540
parsing, question and answering, summarization, sentiment analysis, and paraphrasing.

66
00:04:53,540 --> 00:04:59,400
NLP and NLU are used by a wide array of industries, from finance industry all the way through

67
00:04:59,400 --> 00:05:06,740
to law and academia, with researchers trying to do information extraction from texts.

68
00:05:06,740 --> 00:05:09,660
Within NLP, there's a couple different applications.

69
00:05:09,660 --> 00:05:14,620
The first and probably the most important is information extraction.

70
00:05:14,620 --> 00:05:19,880
This is the process by which we try to get a computer system to extract information that

71
00:05:19,880 --> 00:05:23,880
we find relevant to our own research or needs.

72
00:05:23,880 --> 00:05:28,420
So for example, as we're going to see in part three of this video, when we apply spacey

73
00:05:28,420 --> 00:05:35,280
to the financial sector, a person interested in finances might need NLP to go through and

74
00:05:35,280 --> 00:05:39,180
extract things like company names, stocks, indexes.

75
00:05:39,180 --> 00:05:43,340
Things that are referenced within maybe news articles, from Reuters to New York Times to

76
00:05:43,340 --> 00:05:45,240
Wall Street Journal.

77
00:05:45,240 --> 00:05:49,340
This is an example of using NLP to extract information.

78
00:05:49,340 --> 00:05:55,060
A good way to think about NLP's application in this area is it takes in some unstructured

79
00:05:55,060 --> 00:06:01,940
data, in this case raw text, and extracts structured data from it, or metadata.

80
00:06:01,940 --> 00:06:05,780
So it finds the things that you want it to find and extracts them for you.

81
00:06:05,780 --> 00:06:11,260
Now while there's ways to do this with gazetteers and list matching, using an NLP framework

82
00:06:11,260 --> 00:06:15,620
like spacey, which I'll talk about in just a second, has certain advantages.

83
00:06:15,620 --> 00:06:20,380
The main one being that you can use and leverage things that have been parsed syntactically

84
00:06:20,380 --> 00:06:21,660
or semantically.

85
00:06:21,660 --> 00:06:25,460
So things like the part of speech of a word, things like its dependencies, things like

86
00:06:25,460 --> 00:06:26,940
its co-reference.

87
00:06:26,940 --> 00:06:31,180
These are things that the spacey framework allow for you to do off the shelf and also

88
00:06:31,180 --> 00:06:36,700
train into machine learning models and work into pipelines with rules.

89
00:06:36,700 --> 00:06:40,720
So that's kind of one aspect of NLP and one way it's used.

90
00:06:40,720 --> 00:06:44,380
Another way it's used is to read in data and classify it.

91
00:06:44,380 --> 00:06:49,780
This is known as text categorization and we see that on the left hand side of this image.

92
00:06:49,780 --> 00:06:53,740
Text categorization or text classification, and we conclude in this sentiment analysis

93
00:06:53,740 --> 00:06:59,740
for the most part as well, is a way we take information into a computer system, again unstructured

94
00:06:59,780 --> 00:07:04,300
data, a raw text, and we classify it in some way.

95
00:07:04,300 --> 00:07:10,140
You've actually seen this at work for many decades now with spam detection.

96
00:07:10,140 --> 00:07:12,180
Spam detection is nearly perfect.

97
00:07:12,180 --> 00:07:16,380
It needs to be continually updated, but for the most part it is a solved problem.

98
00:07:16,380 --> 00:07:20,380
The reason why you have emails that automatically go to your spam folder is because there's

99
00:07:20,380 --> 00:07:24,380
a machine learning model that sits on the background of your, on the back end of your

100
00:07:24,380 --> 00:07:25,700
email server.

101
00:07:25,700 --> 00:07:29,780
And what it does is it actually looks at the emails, it sees if it fits the pattern for

102
00:07:29,780 --> 00:07:34,420
what it's seen as spam before, and it assigns it a spam label.

103
00:07:34,420 --> 00:07:37,140
This is known as classification.

104
00:07:37,140 --> 00:07:41,500
This is also used by researchers, especially in the legal industry.

105
00:07:41,500 --> 00:07:46,580
Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.

106
00:07:46,580 --> 00:07:51,780
They don't necessarily have the human time to go through and analyze every single document

107
00:07:51,780 --> 00:07:52,900
verbatim.

108
00:07:52,900 --> 00:07:57,820
It is important to kind of get a quick umbrella sense of the documents without actually having

109
00:07:57,820 --> 00:08:00,700
to go through and read them page by page.

110
00:08:00,700 --> 00:08:06,100
And so what lawyers will oftentimes do is use NLP to do classification and information

111
00:08:06,100 --> 00:08:07,280
extraction.

112
00:08:07,280 --> 00:08:12,020
They will find keywords that are relevant to their case, or they will find documents

113
00:08:12,020 --> 00:08:15,980
that are classified according to the relevant fields of their case.

114
00:08:15,980 --> 00:08:20,860
And that way they can take a million documents and reduce it down to maybe only a handful,

115
00:08:20,860 --> 00:08:24,180
maybe a thousand that they have to read verbatim.

116
00:08:24,180 --> 00:08:29,420
This is a real world application of NLP or natural language processing, and both of these

117
00:08:29,420 --> 00:08:33,540
tasks can be achieved through the SPACI framework.

118
00:08:33,540 --> 00:08:36,460
SPACI is a framework for doing NLP.

119
00:08:36,460 --> 00:08:41,500
Right now, as of 2021, it's only available, I believe, in Python.

120
00:08:41,500 --> 00:08:45,180
I think there is a community that's working on an application with R, but I don't know

121
00:08:45,180 --> 00:08:46,620
that for certain.

122
00:08:46,620 --> 00:08:51,740
But SPACI is one of many NLP frameworks that Python has available.

123
00:08:51,740 --> 00:08:55,340
If you're interested in looking at all of them, you can explore things like NLDK, the

124
00:08:55,340 --> 00:08:59,740
Natural Language Toolkit, Stanza, which I believe is coming out of the same program

125
00:08:59,740 --> 00:09:01,540
at Stanford.

126
00:09:01,540 --> 00:09:05,700
There's many out there, but I find SPACI to be the best of all of them for a couple

127
00:09:05,700 --> 00:09:07,180
different reasons.

128
00:09:07,180 --> 00:09:11,860
Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they

129
00:09:11,860 --> 00:09:16,700
perform very quickly, and they also have very good accuracy metrics, such as precision,

130
00:09:16,700 --> 00:09:17,700
recall, and f-score.

131
00:09:17,700 --> 00:09:21,500
And I'm not going to talk too much about the way we measure machine learning accuracy

132
00:09:21,500 --> 00:09:24,500
right now, but know that they are quite good.

133
00:09:24,500 --> 00:09:29,740
Second, SPACI has the ability to leverage current natural language processing methods,

134
00:09:29,740 --> 00:09:35,100
specifically, transformer models, also known usually kind of collectively as BERT models,

135
00:09:35,100 --> 00:09:37,820
even though that's not entirely accurate.

136
00:09:37,820 --> 00:09:41,780
And it allows for you to use an off-the-shelf transformer model.

137
00:09:41,780 --> 00:09:47,820
And third, it provides the framework for doing custom training relatively easily compared

138
00:09:47,820 --> 00:09:50,820
to these other NLP frameworks that are out there.

139
00:09:50,820 --> 00:09:56,340
Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales

140
00:09:56,340 --> 00:09:57,340
well.

141
00:09:57,340 --> 00:10:02,860
SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale.

142
00:10:02,860 --> 00:10:09,100
By at scale, we mean working with large quantities of documents efficiently, effectively, and

143
00:10:09,100 --> 00:10:10,620
accurately.

144
00:10:10,620 --> 00:10:15,100
SPACI scales well because it can process hundreds of thousands of documents with relative ease

145
00:10:15,100 --> 00:10:20,220
in a relative short period of time, especially if you stick with more rules-based pipes,

146
00:10:20,220 --> 00:10:24,100
which we're going to talk about in part two of this video.

147
00:10:24,100 --> 00:10:29,020
So those are the two things you really need to know about NLP and SPACI in general.

148
00:10:29,020 --> 00:10:33,580
We're going to talk about SPACI in-depth as we explore it both through this video and

149
00:10:33,580 --> 00:10:41,460
in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,

150
00:10:41,460 --> 00:10:44,900
and it should be linked in the description down below.

151
00:10:44,900 --> 00:10:48,460
This video and the textbook are meant to work in tandem.

152
00:10:48,460 --> 00:10:52,100
Some stuff that I cover in the video might not necessarily be in the textbook because

153
00:10:52,100 --> 00:10:57,140
it doesn't lend itself well to text representation, and the same goes for the opposite.

154
00:10:57,140 --> 00:11:01,580
Some stuff that I don't have the time to cover verbatim in this video I cover in a

155
00:11:01,580 --> 00:11:04,980
little bit more depth in the book.

156
00:11:04,980 --> 00:11:07,140
I think that you should try to use both of these.

157
00:11:07,140 --> 00:11:11,580
What I would recommend is doing one pass through this whole video, watch it in its entirety,

158
00:11:11,580 --> 00:11:15,420
and get an umbrella sense of everything that SPACI can do and everything that we're going

159
00:11:15,420 --> 00:11:16,580
to cover.

160
00:11:16,580 --> 00:11:22,100
I would then go back and try to replicate each stage of this process on a separate window

161
00:11:22,100 --> 00:11:26,380
or on a separate screen and try to kind of follow along in code, and then I would go

162
00:11:26,380 --> 00:11:30,660
back through a third time and try to watch the first part where I talk about what we're

163
00:11:30,660 --> 00:11:34,540
going to be doing and try to do it on your own without looking at the textbook or the

164
00:11:34,540 --> 00:11:35,540
video.

165
00:11:35,540 --> 00:11:39,820
If you can do that by your third pass, you'll be in very good shape to start using SPACI

166
00:11:39,820 --> 00:11:42,980
to solve your own domain specific problems.

167
00:11:42,980 --> 00:11:49,700
NLP is a complex field, and applying NLP is really complex, but fortunately frameworks

168
00:11:49,700 --> 00:11:54,300
like SPACI make this project and this process a lot easier.

169
00:11:54,300 --> 00:11:58,420
I encourage you to spend a few hours in this video, get to know SPACI, and I think you're

170
00:11:58,420 --> 00:12:02,740
going to find that you can do things that you didn't think possible in relative short

171
00:12:02,740 --> 00:12:03,900
order.

172
00:12:03,900 --> 00:12:08,180
So sit back, relax, and enjoy this video series on SPACI.

173
00:12:08,180 --> 00:12:12,820
In order to use SPACI, you're first going to have to install SPACI.

174
00:12:12,820 --> 00:12:17,340
Now there's a few different ways to do this depending on your environment and your operating

175
00:12:17,340 --> 00:12:18,340
system.

176
00:12:18,340 --> 00:12:25,700
I recommend going to SPACI.io backslash usage and kind of enter in the correct framework

177
00:12:25,700 --> 00:12:26,700
that you're working with.

178
00:12:26,700 --> 00:12:31,620
So if you're using Mac OS versus Windows versus Linux, you can go through and in this very

179
00:12:31,620 --> 00:12:36,360
handy kind of user interface, you can go through and select the different features that matter

180
00:12:36,360 --> 00:12:37,980
most to you.

181
00:12:37,980 --> 00:12:42,980
I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be

182
00:12:42,980 --> 00:12:47,340
doing everything on the CPU and I'm going to be working with English.

183
00:12:47,340 --> 00:12:50,940
So I've established all of those different parameters, and it goes through and it tells

184
00:12:50,940 --> 00:12:56,780
me exactly how to go through and install it using PIP in the terminal.

185
00:12:56,780 --> 00:13:02,020
So I encourage you to go through pause the video right now, go ahead and install Windows

186
00:13:02,020 --> 00:13:06,620
however you want to, I'm going to be walking through how to install it within the Jupyter

187
00:13:06,620 --> 00:13:09,700
notebook that we're going to be moving to in just a second.

188
00:13:09,700 --> 00:13:13,660
I want you to not work with the GPU at all.

189
00:13:13,660 --> 00:13:17,900
Working with Spacey on the GPU requires a lot more understanding about what the GPU

190
00:13:17,900 --> 00:13:21,660
is used for, specifically in training machine learning models.

191
00:13:21,660 --> 00:13:24,420
It requires you to have CUDA installed correctly.

192
00:13:24,420 --> 00:13:28,860
It requires a couple other things that I don't really have the time to get into in this video,

193
00:13:28,860 --> 00:13:33,900
but we'll be addressing in a more advanced Spacey tutorial video.

194
00:13:33,900 --> 00:13:39,540
So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,

195
00:13:39,540 --> 00:13:43,300
and then selecting CPU and since you're going to be working through this video with English

196
00:13:43,340 --> 00:13:49,980
texts, I encourage you to select English right now and go ahead and just install or download

197
00:13:49,980 --> 00:13:52,700
the Ncore Web SM model.

198
00:13:52,700 --> 00:13:53,700
This is the small model.

199
00:13:53,700 --> 00:13:56,020
I'll talk about that in just a second.

200
00:13:56,020 --> 00:14:02,660
So the first thing we're going to do in our Jupyter notebook is we are going to be using

201
00:14:02,660 --> 00:14:07,100
the exclamation mark to delineate in the cell that this is a terminal command.

202
00:14:07,100 --> 00:14:10,180
We're going to say PIP install Spacey.

203
00:14:10,180 --> 00:14:14,140
Your output when you execute this cell is going to look a little different than mine.

204
00:14:14,140 --> 00:14:17,260
I already have Spacey installed in this environment.

205
00:14:17,260 --> 00:14:20,940
And so mine kind of goes through and looks like this yours will actually go through.

206
00:14:20,940 --> 00:14:25,820
And instead of saying requirement already satisfied, it'll be actually passing out the the different

207
00:14:25,820 --> 00:14:31,420
things that it's actually installing to install Spacey and all of its dependencies.

208
00:14:31,420 --> 00:14:36,060
The next thing that you're going to do is you're going to again, you follow the instructions

209
00:14:36,140 --> 00:14:42,540
and you're going to be doing Python dash M space Spacey space download and then the model

210
00:14:42,540 --> 00:14:44,180
that you want to download.

211
00:14:44,180 --> 00:14:46,220
So let's go ahead and do that right now.

212
00:14:46,220 --> 00:14:52,060
So let's go ahead and say Python M Spacey download.

213
00:14:52,060 --> 00:14:54,540
So this is a Spacey terminal command.

214
00:14:54,540 --> 00:14:57,420
And we're going to download the Ncore Web SM.

215
00:14:57,420 --> 00:15:00,060
And again, I already have this model downloaded.

216
00:15:00,060 --> 00:15:05,460
So on my end, Spacey is going to look a little differently than as it's going to look on your

217
00:15:05,460 --> 00:15:09,100
end as it prints off on the Jupyter notebook.

218
00:15:09,100 --> 00:15:13,340
And if we give it a just a second, everything will go through and it says that it's collected

219
00:15:13,340 --> 00:15:17,900
it, it's downloading it and we are all very happy now.

220
00:15:17,900 --> 00:15:23,060
And so now that we've got Spacey installed correctly, and that we've got the small model

221
00:15:23,060 --> 00:15:28,100
downloaded correctly, we can go ahead and start actually using Spacey and make sure

222
00:15:28,100 --> 00:15:29,380
everything's correct.

223
00:15:29,380 --> 00:15:33,700
The first thing we're going to do is we're going to import the Spacey library as you

224
00:15:33,820 --> 00:15:35,820
would with any other Python library.

225
00:15:35,820 --> 00:15:41,500
If you're not familiar with this, a library is simply a set of classes and functions that

226
00:15:41,500 --> 00:15:46,260
you can import into a Python script so that you don't have to write a whole bunch of extra

227
00:15:46,260 --> 00:15:47,460
code.

228
00:15:47,460 --> 00:15:52,220
Libraries are massive collections of classes and functions that you can call.

229
00:15:52,220 --> 00:15:57,380
So when we import Spacey, we're importing the whole library of Spacey.

230
00:15:57,380 --> 00:16:01,860
And now that we've seen something like this, we know that Spacey has imported correctly,

231
00:16:01,860 --> 00:16:07,540
as long as you're not getting an error message, everything was imported fine.

232
00:16:07,540 --> 00:16:13,460
The next thing that we need to do is we want to make sure that our English Core Web SM,

233
00:16:13,460 --> 00:16:16,740
our small English model, was downloaded correctly.

234
00:16:16,740 --> 00:16:20,580
So the next thing that we need to do is we need to create an NLP object.

235
00:16:20,580 --> 00:16:24,940
I'm going to be talking a lot more about this as we move forward right now.

236
00:16:24,940 --> 00:16:29,420
This is just troubleshooting to make sure that we've installed Spacey correctly and

237
00:16:29,420 --> 00:16:32,140
we've downloaded our model correctly.

238
00:16:32,140 --> 00:16:35,540
So we're going to use the spacey.load command.

239
00:16:35,540 --> 00:16:37,460
This is going to take one argument.

240
00:16:37,460 --> 00:16:42,700
It's going to be a string that is going to correspond to the model that you've installed.

241
00:16:42,700 --> 00:16:47,820
In this case, N Core Web SM.

242
00:16:47,820 --> 00:16:54,100
And if you execute this cell and you have no errors, you have successfully installed

243
00:16:54,100 --> 00:16:59,980
Spacey correctly and you've downloaded the English Core Web SM model correctly.

244
00:16:59,980 --> 00:17:05,340
So go ahead, take time and get all this stuff set up, pause the video if you need to and

245
00:17:05,340 --> 00:17:11,180
then pop back and we're going to start actually working through the basics of Spacey.

246
00:17:11,180 --> 00:17:16,460
I'm now going to move into kind of an overview of kind of what's within Spacey, why it's

247
00:17:16,460 --> 00:17:20,940
useful and kind of some of the basic features of it that you need to be familiar with.

248
00:17:20,940 --> 00:17:25,420
And I'm going to be working from the Jupyter Notebook that I talked about in the introduction

249
00:17:25,420 --> 00:17:26,420
to this video.

250
00:17:26,420 --> 00:17:31,820
If we scroll down to the bottom of chapter one, the basics of Spacey and you get past

251
00:17:31,820 --> 00:17:35,500
the install section, you get to this section on containers.

252
00:17:35,500 --> 00:17:36,780
So what are containers?

253
00:17:36,780 --> 00:17:42,780
Well, containers within Spacey are objects that contain a large quantity of data about

254
00:17:42,780 --> 00:17:44,300
a text.

255
00:17:44,300 --> 00:17:47,780
There are several different containers that you can work with in Spacey.

256
00:17:47,780 --> 00:17:53,620
There's the doc, the doc bin, example, language, lexeme, span, span group and token.

257
00:17:53,620 --> 00:17:57,420
We're going to be dealing with the lexeme a little bit in this video series and we're

258
00:17:57,420 --> 00:18:01,580
going to be dealing with the language container a little bit in this video series, but really

259
00:18:01,580 --> 00:18:05,500
the three big things that we're going to be talking about again and again is the doc,

260
00:18:05,500 --> 00:18:08,420
the span and the token.

261
00:18:08,420 --> 00:18:12,580
And I think when you first come to Spacey, there's a little bit of a learning curve about

262
00:18:12,580 --> 00:18:17,260
what these things are, what they do, how they are structured hierarchically.

263
00:18:17,420 --> 00:18:23,140
And for that reason, I've created this, in my opinion, kind of easy to understand image

264
00:18:23,140 --> 00:18:25,420
of what different containers are.

265
00:18:25,420 --> 00:18:30,580
So if you think about what Spacey is as a pyramid, so a hierarchical system, we've

266
00:18:30,580 --> 00:18:36,340
got all these different containers structured around really the doc object.

267
00:18:36,340 --> 00:18:42,300
Your doc container or your doc object contains a whole bunch of metadata about the text

268
00:18:42,300 --> 00:18:46,980
that you pass to the Spacey pipeline, which we're going to see in practice.

269
00:18:46,980 --> 00:18:51,500
In just a few minutes, the doc object contains a bunch of different things.

270
00:18:51,500 --> 00:18:53,660
It contains attributes.

271
00:18:53,660 --> 00:18:57,100
These attributes can be things like sentences.

272
00:18:57,100 --> 00:19:02,140
So if you iterate over doc.sense, you can actually access all the different sentences

273
00:19:02,140 --> 00:19:05,100
found within that doc object.

274
00:19:05,100 --> 00:19:10,940
If you iterate over each individual item or index in your doc object, you can get individual

275
00:19:10,940 --> 00:19:12,380
tokens.

276
00:19:12,380 --> 00:19:16,540
Tokens are going to be things like words or punctuation marks.

277
00:19:16,580 --> 00:19:23,020
Anything within your sentence or text that has a self contained important value, either

278
00:19:23,020 --> 00:19:25,660
syntactically or semantically.

279
00:19:25,660 --> 00:19:30,340
So this is going to be things like words, a comma, a period, a semi colon, a quotation

280
00:19:30,340 --> 00:19:34,140
mark, things like this, these are all going to be your tokens.

281
00:19:34,140 --> 00:19:39,900
And we're going to see how tokens are a little different than just splitting words up with

282
00:19:39,900 --> 00:19:43,900
traditional string methods and Python.

283
00:19:43,900 --> 00:19:47,580
The next thing that you should be kind of familiar with are spans.

284
00:19:47,580 --> 00:19:54,380
So spans are important because they kind of exist within and without of the doc object.

285
00:19:54,380 --> 00:20:01,220
So unlike the token, which is an index of the doc object, a span can be a token itself,

286
00:20:01,220 --> 00:20:04,740
but it can also be a sequence of multiple tokens.

287
00:20:04,740 --> 00:20:06,260
We're going to see that at play.

288
00:20:06,260 --> 00:20:15,260
So imagine if you had a span in its category, maybe group one are our places.

289
00:20:15,260 --> 00:20:18,740
So a single token might be like a city like Berlin.

290
00:20:18,740 --> 00:20:23,220
But span group two, this could be something like full proper names.

291
00:20:23,220 --> 00:20:27,500
So of a people, for example, so this could be like as we're going to see Martin Luther

292
00:20:27,500 --> 00:20:28,500
King.

293
00:20:28,500 --> 00:20:33,260
This would be a sequence of tokens, a sequence of three different items in the sentence that

294
00:20:33,260 --> 00:20:37,180
make up one span or one self contained item.

295
00:20:37,180 --> 00:20:45,420
So Martin Luther King would be a person who's a collection of a sequence of individual tokens.

296
00:20:45,420 --> 00:20:50,620
If that doesn't make sense right now, this image will be reinforced as we go through

297
00:20:50,620 --> 00:20:54,740
and learn more about spacey in practice.

298
00:20:54,740 --> 00:21:01,500
For right now, I want you to be just understanding that the doc object is the thing around which

299
00:21:01,500 --> 00:21:03,420
all of spacey sits.

300
00:21:03,420 --> 00:21:06,280
This is going to be the object that you create.

301
00:21:06,280 --> 00:21:10,740
This is going to be the object that contains all the metadata that you need to access.

302
00:21:10,740 --> 00:21:16,740
And this is going to be the object that you try to essentially improve with different

303
00:21:16,740 --> 00:21:22,260
custom components, factories and pipelines as you go through and do more advanced things

304
00:21:22,260 --> 00:21:23,260
with spacey.

305
00:21:23,260 --> 00:21:29,260
We're going to now see in just a few seconds how that doc object is kind of similar to the

306
00:21:29,260 --> 00:21:35,860
text itself, but how it's very, very different and much more powerful.

307
00:21:35,860 --> 00:21:39,420
We're now going to be moving on to chapter two of this textbook, which is going to deal

308
00:21:39,420 --> 00:21:44,380
with kind of getting used to the in depth features of spacey.

309
00:21:44,380 --> 00:21:49,220
If you want to pause the video or keep this notebook or this book open up kind of separate

310
00:21:49,220 --> 00:21:53,980
from this video and follow along as we go through and explore it in live coding.

311
00:21:53,980 --> 00:21:57,420
We're going to be talking about a few different things as we explore chapter two.

312
00:21:57,780 --> 00:22:00,180
This will be a lot longer than chapter one.

313
00:22:00,180 --> 00:22:04,380
We're going to be not only importing spacey, but actually going through and loading up

314
00:22:04,380 --> 00:22:09,300
a model, creating a doc object around that model so that we're going to work with the

315
00:22:09,300 --> 00:22:11,180
doc container and practice.

316
00:22:11,180 --> 00:22:15,500
And then we're going to see how that doc container stores a lot of different features

317
00:22:15,500 --> 00:22:18,660
or metadata or attributes about the text.

318
00:22:18,660 --> 00:22:23,020
And while they look the same on the surface, they're actually quite different.

319
00:22:23,020 --> 00:22:28,700
So let's go ahead and work within our same Jupiter notebook where we've imported spacey

320
00:22:28,700 --> 00:22:31,940
and we have already created the NLP object.

321
00:22:31,940 --> 00:22:36,980
The first thing that I want to do is I want to open up a text to start working with within

322
00:22:36,980 --> 00:22:39,220
this repo.

323
00:22:39,220 --> 00:22:42,580
We've got a data folder within this data sub folder.

324
00:22:42,580 --> 00:22:45,260
I've got a couple of different Wikipedia openings.

325
00:22:45,260 --> 00:22:49,420
I've got one on MLK that we're going to be using a little later in this video and I have

326
00:22:49,420 --> 00:22:51,500
one on the United States.

327
00:22:51,500 --> 00:22:54,180
This is wiki underscore us.

328
00:22:54,180 --> 00:22:56,220
That's going to be what we work with right now.

329
00:22:56,220 --> 00:23:04,260
So let's use our with operator and open up data backslash wiki underscore us dot txt.

330
00:23:04,260 --> 00:23:09,300
We're going to just read that in as F and then we're going to create this text object,

331
00:23:09,300 --> 00:23:12,220
which is going to be equal to F dot read.

332
00:23:12,220 --> 00:23:16,260
And now that we've got our text object created, let's go ahead and see what this looks like.

333
00:23:16,260 --> 00:23:18,780
So let's print text.

334
00:23:18,780 --> 00:23:22,980
Then we see that it's a standard Wikipedia article kind of follows that same introductory

335
00:23:22,980 --> 00:23:28,420
format and it's about four or five paragraphs long with a lot of the features left in such

336
00:23:28,420 --> 00:23:31,900
as the brackets that delineate some kind of a footnote.

337
00:23:31,900 --> 00:23:35,300
We're not going to worry too much about cleaning this up right now because we're interested

338
00:23:35,300 --> 00:23:41,460
not with cleaning our data so much as just starting to work with the doc object in spacey.

339
00:23:41,460 --> 00:23:46,220
So the first thing that you want to do is you're going to want to create a doc object.

340
00:23:46,220 --> 00:23:50,260
It is oftentimes good practice if you're only ever working with one doc object in your

341
00:23:50,260 --> 00:23:54,540
script to just call your only object doc.

342
00:23:54,540 --> 00:23:59,620
If you're working with multiple objects, sometimes you'll say doc one doc two doc three

343
00:23:59,620 --> 00:24:05,100
or give it some kind of specific name so that your variables can be unique and easily identifiable

344
00:24:05,100 --> 00:24:06,600
later in your script.

345
00:24:06,600 --> 00:24:10,820
Since we're just working with one doc object right now, we're going to say doc is equal

346
00:24:10,820 --> 00:24:12,900
to NLP.

347
00:24:12,900 --> 00:24:18,660
So this is going to call our NLP model that we imported earlier in this case the English

348
00:24:18,660 --> 00:24:21,460
Core Web SM model.

349
00:24:21,460 --> 00:24:26,100
And that's going to for right now just take one argument and that's going to be the text

350
00:24:26,100 --> 00:24:27,100
itself.

351
00:24:27,100 --> 00:24:33,960
So the text object, if you execute that cell, you should have a doc object now created.

352
00:24:33,960 --> 00:24:38,000
Let's print off that doc object and see what it looks like.

353
00:24:38,000 --> 00:24:44,000
And if you scroll down, you might be thinking to yourself, this looks very, very similar

354
00:24:44,000 --> 00:24:47,760
if not identical to what I just saw a second ago.

355
00:24:47,760 --> 00:24:53,760
And in fact, on the surface, it is very similar to that text object that we gave to the NLP

356
00:24:53,760 --> 00:24:57,400
model or pipeline, but let's see how they're different.

357
00:24:57,400 --> 00:25:01,040
Let's print off the length of text.

358
00:25:01,040 --> 00:25:05,520
And let's print off the length of the doc object.

359
00:25:05,520 --> 00:25:08,160
And what we have here are two different numbers.

360
00:25:08,160 --> 00:25:15,240
Our text is 3525 and our doc object is 152.

361
00:25:15,240 --> 00:25:16,600
What is going on here?

362
00:25:16,600 --> 00:25:22,440
Well, let's get a sense by trying to iterate over the text object and iterating over the

363
00:25:22,440 --> 00:25:24,920
doc object with a simple for loop.

364
00:25:24,920 --> 00:25:28,800
So we're going to say for token and text, so we're going to iterate first over that

365
00:25:28,800 --> 00:25:32,640
text object, we're going to print off the token.

366
00:25:32,640 --> 00:25:37,280
So the first 10 indices, and we get individual letters as one might expect.

367
00:25:37,280 --> 00:25:41,480
But when we do something the same thing with the doc object, let's go ahead and start writing

368
00:25:41,480 --> 00:25:42,480
this out.

369
00:25:42,480 --> 00:25:49,000
We're going to say for token and doc, we're going to iterate over the first 10.

370
00:25:49,000 --> 00:25:51,120
We're going to print off the token.

371
00:25:51,120 --> 00:25:53,120
We see something very different.

372
00:25:53,120 --> 00:25:56,160
What we see here are tokens.

373
00:25:56,160 --> 00:26:01,880
This is why the doc object is so much more valuable and this is why the doc object has

374
00:26:01,880 --> 00:26:04,600
a different length than the text object.

375
00:26:04,600 --> 00:26:09,560
The text object is just basically counting up every instance of a character, a white

376
00:26:09,560 --> 00:26:11,700
space, a punctuation, etc.

377
00:26:11,700 --> 00:26:18,280
The doc object is counting individual tokens, so any word, any punctuation, etc.

378
00:26:18,280 --> 00:26:21,160
That's why they're of different length and that's why when we print them off, we see

379
00:26:21,160 --> 00:26:22,160
something different.

380
00:26:22,160 --> 00:26:25,920
So you might now already be seeing the power of spacey.

381
00:26:25,920 --> 00:26:30,040
It allows for you to easily on the surface with nothing else being done, easily split

382
00:26:30,040 --> 00:26:33,720
up your text into individual tokens without any effort at all.

383
00:26:33,720 --> 00:26:38,480
Now, those of you familiar with Python and different string methods might be thinking

384
00:26:38,480 --> 00:26:40,840
to yourself, but I've got the split method.

385
00:26:40,840 --> 00:26:43,560
I can just use this to split up the text.

386
00:26:43,560 --> 00:26:45,960
I don't need anything fancy from spacey.

387
00:26:45,960 --> 00:26:47,560
Well, you'd be wrong.

388
00:26:47,560 --> 00:26:49,320
Let me demonstrate this right now.

389
00:26:49,320 --> 00:26:56,880
So if I were to say for token and text.split, so I'm splitting up that text into individual

390
00:26:56,880 --> 00:27:01,720
and theory individual words, essentially, it's just a split method where it's splitting

391
00:27:01,720 --> 00:27:03,360
by individual white spaces.

392
00:27:03,360 --> 00:27:08,160
If I were to do that and iterate over the first 10 again.

393
00:27:08,160 --> 00:27:13,560
And I would just say print token, it looks good until you get down here.

394
00:27:13,560 --> 00:27:18,160
So until you get to USA, well, why is it a problem?

395
00:27:18,160 --> 00:27:20,560
The problem is quite simple.

396
00:27:20,560 --> 00:27:23,760
There is a parentheses mark right here.

397
00:27:23,760 --> 00:27:30,120
And this is where we have a huge advantage with spacey.

398
00:27:30,120 --> 00:27:35,920
Spacey automatically separates out these these kind of punctuation marks and removes them

399
00:27:35,920 --> 00:27:40,200
from individual tokens when they're not relevant to the token itself.

400
00:27:40,200 --> 00:27:45,160
Notice that USA has got a period within the middle of it.

401
00:27:45,160 --> 00:27:49,520
It's not looking at that and thinking that that is some kind of unique token, a you a

402
00:27:49,520 --> 00:27:53,200
period and s a period and an a in a period.

403
00:27:53,200 --> 00:27:57,400
It's not seeing these as four individual tokens rather it's automatically identifying them

404
00:27:57,400 --> 00:28:04,640
as one thing one tied together single token that's a string of characters and punctuation.

405
00:28:04,640 --> 00:28:09,640
This is where the power of spacey really lies just on the surface level and go ahead spend

406
00:28:09,640 --> 00:28:11,760
a few minutes and play around with this.

407
00:28:11,760 --> 00:28:16,360
And then we're going to kind of jump back here and start talking about how the doc object

408
00:28:16,360 --> 00:28:19,720
has a lot more than just tokens within it.

409
00:28:19,720 --> 00:28:22,600
It's got sentences each token has attributes.

410
00:28:22,600 --> 00:28:30,480
We're going to start exploring these when you pop back.

411
00:28:30,480 --> 00:28:34,560
If you're following along with the textbook, we're now going to be moving on to the next

412
00:28:34,560 --> 00:28:37,880
section, which is sentence boundary detection.

413
00:28:37,880 --> 00:28:45,360
An NLP sentence boundary detection is the identification of sentences within a text on the surface.

414
00:28:45,360 --> 00:28:46,600
This might look simple.

415
00:28:46,600 --> 00:28:51,520
You might be thinking to yourself, I could simply use the split function and split up

416
00:28:51,520 --> 00:28:53,800
a text with a simple period.

417
00:28:53,800 --> 00:28:55,920
And that's going to give me all my sentences.

418
00:28:55,920 --> 00:29:00,200
Those of you who have tried to do this might already be shaking your heads and saying no.

419
00:29:00,200 --> 00:29:04,560
If you do think about it, there's a really easy explanation for why this doesn't work.

420
00:29:04,560 --> 00:29:10,560
Were you to try to split up a text by period and make a presumption that anything that occurs

421
00:29:10,560 --> 00:29:14,920
with between periods is going to be an individual sentence, you would have a serious mistake

422
00:29:14,960 --> 00:29:20,560
when you get to things like USA, especially in Western languages, where the punctuation

423
00:29:20,560 --> 00:29:26,840
of a period mark is used not only to delineate the change of its sentence, rather it's used

424
00:29:26,840 --> 00:29:29,920
to also delineate abbreviations.

425
00:29:29,920 --> 00:29:36,000
So United States of America, each period represents an abbreviated word.

426
00:29:36,000 --> 00:29:40,880
So you could write in rules to kind of account for this, you could write in rules that could

427
00:29:40,880 --> 00:29:45,240
also include in other ways that sentences are created, such as question marks, such

428
00:29:45,240 --> 00:29:46,920
as exclamation marks.

429
00:29:46,920 --> 00:29:48,200
But why do that?

430
00:29:48,200 --> 00:29:50,200
That's a lot of effort.

431
00:29:50,200 --> 00:29:56,820
When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly

432
00:29:56,820 --> 00:29:58,320
how that works.

433
00:29:58,320 --> 00:30:06,800
So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing

434
00:30:06,800 --> 00:30:09,880
the sentence attribute of the doc object.

435
00:30:09,880 --> 00:30:13,080
Let's print off scent.

436
00:30:13,080 --> 00:30:17,520
And if you do that, you are now able to print off every individual sentence.

437
00:30:17,520 --> 00:30:21,880
So the entire text has been tokenized at the sentence level.

438
00:30:21,880 --> 00:30:27,760
In other words, spacey has used its sentence boundary detection and done all that for you

439
00:30:27,760 --> 00:30:29,920
and giving you all the sentences.

440
00:30:29,920 --> 00:30:33,600
If you work with different models of different sizes, you're going to notice that certain

441
00:30:33,600 --> 00:30:37,840
models the larger they get tend to do better at sentence detection.

442
00:30:37,840 --> 00:30:41,400
And that's because machine learning models tend to do a little bit better than heuristic

443
00:30:41,400 --> 00:30:42,600
approaches.

444
00:30:42,600 --> 00:30:47,960
The English core web SM model, while having some machine learning components in it, does

445
00:30:47,960 --> 00:30:50,360
not save word vectors.

446
00:30:50,360 --> 00:30:55,200
And so the larger you go with the models, typically the better you're going to have with regards

447
00:30:55,200 --> 00:30:57,440
to sentence detection.

448
00:30:57,440 --> 00:31:00,680
Let's go ahead and try to access one of these sentences.

449
00:31:00,680 --> 00:31:06,200
So let's create an object called sentence one, we're going to make that equal to doc.sense

450
00:31:06,200 --> 00:31:07,200
zero.

451
00:31:07,400 --> 00:31:14,520
We're going to try to grab that zero index and let's print off sentence one, we do this,

452
00:31:14,520 --> 00:31:15,920
we get an error.

453
00:31:15,920 --> 00:31:17,600
Why have we gotten an error?

454
00:31:17,600 --> 00:31:20,120
Well, it tells you why right here, it's a type air.

455
00:31:20,120 --> 00:31:26,720
And this means that this is not a type that can be kind of iterated over, it's not subscriptable.

456
00:31:26,720 --> 00:31:28,880
And it's because it is a generator.

457
00:31:28,880 --> 00:31:32,960
Now in Python, if you're familiar with generators, you might be thinking to yourself, there's

458
00:31:32,960 --> 00:31:33,960
a solution for this.

459
00:31:33,960 --> 00:31:35,400
And in fact, there is.

460
00:31:35,440 --> 00:31:40,040
If you want to work with generator objects, you need to convert them into a list.

461
00:31:40,040 --> 00:31:43,360
So let's say sentence one is equal to list.

462
00:31:43,360 --> 00:31:48,680
So using the list function to convert doc.sense into a list.

463
00:31:48,680 --> 00:31:53,080
And then with outside of that, we're going to grab zero, the zero index, and then we're

464
00:31:53,080 --> 00:31:55,960
going to print off sentence one.

465
00:31:55,960 --> 00:32:00,320
And we grab the first sentence of that text.

466
00:32:00,400 --> 00:32:06,080
This as we go deeper and deeper in spacey one by one, you're going to see the immense

467
00:32:06,080 --> 00:32:10,880
power that you can do with Pacea, all the immense incredible things you can use spacey

468
00:32:10,880 --> 00:32:14,240
for with very, very minimal code.

469
00:32:14,240 --> 00:32:19,160
The doc object does a lot of things for you that would take hours to actually write out

470
00:32:19,160 --> 00:32:22,040
and code to do with heuristic approaches.

471
00:32:22,040 --> 00:32:27,280
This is now a great way to segment an entire text up by sentence.

472
00:32:27,280 --> 00:32:33,520
And if you work with text a lot, you will already know that this has a lot of applications.

473
00:32:33,520 --> 00:32:37,440
As we move forward, we're going to not just talk about sentences, we're also going to

474
00:32:37,440 --> 00:32:43,160
be talking about token attributes, because within the doc object are individual tokens.

475
00:32:43,160 --> 00:32:47,920
I encourage you to pause here and go ahead and play around with the doc.sense a little

476
00:32:47,920 --> 00:32:52,680
bit and get familiar with how it works, what it contains, and try to convert it into a

477
00:32:52,680 --> 00:32:53,680
list.

478
00:32:54,680 --> 00:32:58,920
And we'll continue talking about tokens.

479
00:32:58,920 --> 00:33:02,800
This is where I really encourage you to spend a little bit of time with the textbook.

480
00:33:02,800 --> 00:33:08,600
Under token attributes in chapter two, I have all the different kind of major things that

481
00:33:08,600 --> 00:33:11,280
you're going to be using with regards to token attributes.

482
00:33:11,280 --> 00:33:14,680
We're going to look and see how to access them in just a second.

483
00:33:14,680 --> 00:33:18,640
I've provided for you kind of the most important ones that you should probably be familiar

484
00:33:18,640 --> 00:33:19,640
with.

485
00:33:19,640 --> 00:33:22,880
We're going to see this in code in just a second, and I'm going to explain with a little

486
00:33:22,880 --> 00:33:27,400
bit more detail than what's in the spacey documentation about what these different things

487
00:33:27,400 --> 00:33:30,360
are, why they're useful, and how they're used.

488
00:33:30,360 --> 00:33:36,480
So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.

489
00:33:36,480 --> 00:33:40,120
If you remember, the doc object had a sequence of tokens.

490
00:33:40,120 --> 00:33:43,400
So for token and doc, you could print off token.

491
00:33:43,400 --> 00:33:46,480
And let's just do this with the first 10.

492
00:33:46,480 --> 00:33:49,440
And we've got each individual token.

493
00:33:49,440 --> 00:33:55,880
What you don't see here is that each individual token has a bunch of metadata buried within

494
00:33:55,880 --> 00:33:56,880
it.

495
00:33:56,880 --> 00:34:02,840
These metadata are things that we call attributes or different things about that token that

496
00:34:02,840 --> 00:34:07,160
you can access through the spacey framework.

497
00:34:07,160 --> 00:34:09,680
So let's go ahead and try to do that right now.

498
00:34:09,680 --> 00:34:15,920
Let's just work with for right now token number two, which we're going to call sentence one,

499
00:34:15,920 --> 00:34:18,920
and we're going to grab from sentence one, the second index.

500
00:34:19,440 --> 00:34:21,960
Let's print off that word.

501
00:34:21,960 --> 00:34:23,360
And it should be states.

502
00:34:23,360 --> 00:34:26,000
And in fact, it is fantastic.

503
00:34:26,000 --> 00:34:30,280
So now that we've got the word states accessed, we can start kind of going through and playing

504
00:34:30,280 --> 00:34:33,920
around with some of the attributes that that word actually has.

505
00:34:33,920 --> 00:34:37,240
Now when you print it off, it looks like a regular piece of text, it looks like just

506
00:34:37,240 --> 00:34:42,400
a string, but it's got so much more buried within it now because it's been passed through

507
00:34:42,400 --> 00:34:46,440
our NLP model or pipeline from spacey.

508
00:34:46,480 --> 00:34:50,040
So let's go ahead and say token to dot text.

509
00:34:50,040 --> 00:34:52,000
And I'm going to be saying token to dot text.

510
00:34:52,000 --> 00:34:56,440
If you're working within an IDE like Adam, you're going to need to say print token to

511
00:34:56,440 --> 00:34:57,440
dot text.

512
00:34:57,440 --> 00:35:02,600
When we do this, we see we get a string that just is states.

513
00:35:02,600 --> 00:35:08,960
This is telling us that the dot text of the object, the pure text corresponds to the word

514
00:35:08,960 --> 00:35:09,960
states.

515
00:35:09,960 --> 00:35:14,200
This is really important if you need to extract the text itself from the token and not work

516
00:35:14,240 --> 00:35:18,160
with the token object, which has behind it a whole bunch of different metadata that we're

517
00:35:18,160 --> 00:35:21,000
going to go through now and start accessing.

518
00:35:21,000 --> 00:35:23,480
Let's use the token left edge.

519
00:35:23,480 --> 00:35:27,760
So we can say token to dot left underscore edge.

520
00:35:27,760 --> 00:35:28,760
And we can print that off.

521
00:35:28,760 --> 00:35:30,440
Well, what's that telling us?

522
00:35:30,440 --> 00:35:35,640
It's telling us that this is part of a multi word token or a token that is multiple has

523
00:35:35,640 --> 00:35:39,720
multiple components to make up a larger span.

524
00:35:39,720 --> 00:35:43,800
And that this is the leftmost token that corresponds to it.

525
00:35:43,800 --> 00:35:48,480
So this is going to be the word the as in the United States.

526
00:35:48,480 --> 00:35:51,360
Let's take a look at the right edge.

527
00:35:51,360 --> 00:35:57,080
We can say token to dot right underscore edge, print that off, and we get the word America.

528
00:35:57,080 --> 00:36:02,720
So we're able to see where this token fits within a larger span in this case a noun chunk,

529
00:36:02,720 --> 00:36:04,760
which we're going to explore in just a few minutes.

530
00:36:04,760 --> 00:36:08,600
But we also learn a lot about it, kind of the different components, so we know where

531
00:36:08,600 --> 00:36:12,280
to grab it from the beginning and from the very end.

532
00:36:12,280 --> 00:36:15,720
So that's how the left edge and the right edge work.

533
00:36:15,720 --> 00:36:21,120
We also have within this token to dot int type.

534
00:36:21,120 --> 00:36:23,360
This is going to be the type of entity.

535
00:36:23,360 --> 00:36:25,900
Now what you're seeing here is a integer.

536
00:36:25,900 --> 00:36:27,820
So this is 384.

537
00:36:27,820 --> 00:36:33,160
In order to actually know what 384 means, I encourage you to not really use that so much

538
00:36:33,160 --> 00:36:37,280
as and type with an underscore after it.

539
00:36:37,280 --> 00:36:41,960
This is going to give you the string corresponding to number 384.

540
00:36:41,960 --> 00:36:47,280
In this case, it is GPE or geopolitical entity.

541
00:36:47,280 --> 00:36:50,640
We're going to be working with named entity a little bit in this video, but I have a whole

542
00:36:50,640 --> 00:36:53,560
other book on named entity recognition.

543
00:36:53,560 --> 00:36:59,360
It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning

544
00:36:59,360 --> 00:37:02,160
and rules based in a lot more depth.

545
00:37:02,160 --> 00:37:06,560
Let's go ahead and keep on moving on though and looking at different entity types here

546
00:37:06,560 --> 00:37:07,560
as well.

547
00:37:07,560 --> 00:37:09,840
Not entity types, attribute types.

548
00:37:09,840 --> 00:37:17,920
So we're going to say token to dot int IOB, all lowercase and again underscore at the

549
00:37:17,920 --> 00:37:22,120
end and we get the string here, I.

550
00:37:22,120 --> 00:37:27,040
Now IOB is a specific kind of named entity code.

551
00:37:27,040 --> 00:37:31,400
B would mean that it's the beginning of an entity and I means that it's inside of an

552
00:37:31,400 --> 00:37:36,320
entity and O means that it's outside of an entity.

553
00:37:36,320 --> 00:37:43,040
The fact that we're seeing I here tells us that this word states is inside of a larger

554
00:37:43,040 --> 00:37:44,040
entity.

555
00:37:44,040 --> 00:37:47,480
In fact, we know that because we've seen the left edge and we've seen the right edge.

556
00:37:47,480 --> 00:37:51,280
It's inside of the United States of America.

557
00:37:51,280 --> 00:37:54,600
So it's part of a larger entity at hand.

558
00:37:54,600 --> 00:38:02,080
We can also say token to dot lima and under case again after that and we get the word

559
00:38:02,080 --> 00:38:03,080
states.

560
00:38:03,080 --> 00:38:06,720
Lima form or the root form of the word.

561
00:38:06,720 --> 00:38:09,760
This means that this is what the word looks like with no inflection.

562
00:38:09,760 --> 00:38:14,560
If we were working with a verb, in fact, let's go ahead and do that right now.

563
00:38:14,560 --> 00:38:16,560
Let's grab sentence.

564
00:38:16,560 --> 00:38:22,000
We're going to grab sentence one index 12, which should be the word no and we're going

565
00:38:22,000 --> 00:38:29,840
to print off the lima for the word or sorry, it's a verb and we see the verb lima as no.

566
00:38:29,840 --> 00:38:39,600
So if we were to print off sentence one specifically index 12, we see that its original form is

567
00:38:39,600 --> 00:38:40,780
known.

568
00:38:40,780 --> 00:38:47,640
So the lima form uninflected is the verb no K N O W.

569
00:38:47,640 --> 00:38:51,240
Another thing that we can access and we're going to see that have the power of this later

570
00:38:51,240 --> 00:38:52,240
on.

571
00:38:52,240 --> 00:38:55,680
This might not seem important right now, but I promise you it will be.

572
00:38:55,680 --> 00:39:01,400
Let's print off token that I call this again token to we're going to print that off, but

573
00:39:01,400 --> 00:39:04,320
we're going to print off specifically the morph.

574
00:39:04,320 --> 00:39:05,720
No underscore here.

575
00:39:05,720 --> 00:39:06,720
Just morph.

576
00:39:06,720 --> 00:39:12,320
What you get is what looks like a really weird output a string called noun type equal

577
00:39:12,320 --> 00:39:13,320
to prop.

578
00:39:13,320 --> 00:39:18,600
In fact, this means proper noun, a number which corresponds to sing.

579
00:39:18,600 --> 00:39:24,480
We're going to talk a lot more about morphological analysis later on when we try to find an extract

580
00:39:24,480 --> 00:39:27,320
information from our texts.

581
00:39:27,320 --> 00:39:32,520
But for right now, understand that what you're looking at is the output of kind of what that

582
00:39:32,520 --> 00:39:34,600
word is morphologically.

583
00:39:34,600 --> 00:39:37,640
So in this case, it's a proper noun and it's singular.

584
00:39:37,640 --> 00:39:45,360
If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb

585
00:39:45,360 --> 00:39:46,360
it is.

586
00:39:46,360 --> 00:39:53,040
So it's a perfect past participle known perfect past participle.

587
00:39:53,040 --> 00:39:56,960
For being good at NLP is also being good with language.

588
00:39:56,960 --> 00:40:00,240
So I encourage you to spend time and start getting familiar with those things that you

589
00:40:00,240 --> 00:40:04,540
might have forgotten about from like fifth grade grammar, such as perfect participles

590
00:40:04,540 --> 00:40:06,240
and things like that.

591
00:40:06,240 --> 00:40:10,620
Because when you need to start creating rules to extract information, you're going to find

592
00:40:10,620 --> 00:40:14,920
those pieces of information very important for writing rules.

593
00:40:14,920 --> 00:40:16,920
We'll talk about that in a little bit though.

594
00:40:16,920 --> 00:40:20,240
Let's go back to our other attributes from the token.

595
00:40:20,240 --> 00:40:25,680
So again, let's go to token two, and we're going to grab the POS part of speech, not

596
00:40:25,680 --> 00:40:27,120
what you might be thinking.

597
00:40:27,120 --> 00:40:33,020
So part of speech underscore POS underscore, and we output PROPN.

598
00:40:33,020 --> 00:40:35,760
This means that it is a proper noun.

599
00:40:35,760 --> 00:40:42,640
It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological

600
00:40:42,640 --> 00:40:49,320
detailed extraction, what kind of noun it might be with regards to in this case, singular.

601
00:40:49,320 --> 00:40:52,040
So that's going to be how you extract the part of speech.

602
00:40:52,040 --> 00:40:57,580
And the thing that you can do is you can extract the dependency relation.

603
00:40:57,580 --> 00:41:02,400
So in this case, we can figure out what role it plays in the sentence.

604
00:41:02,400 --> 00:41:05,600
In this case, the noun subject.

605
00:41:05,600 --> 00:41:09,800
And then finally, the last thing I really want to talk about before we move into a more

606
00:41:09,800 --> 00:41:17,800
detailed analysis of part of speech is going to be the token two dot lane.

607
00:41:17,800 --> 00:41:23,280
And what this grabs for you is the language of the doc object in this case, we're working

608
00:41:23,280 --> 00:41:28,680
with something from the English language, so in every language is going to have two

609
00:41:28,680 --> 00:41:30,320
letters that correspond to it.

610
00:41:30,320 --> 00:41:32,540
These are universally recognized.

611
00:41:32,540 --> 00:41:38,880
So that's going to be how you access different kinds of attributes that each token has.

612
00:41:38,880 --> 00:41:42,240
And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that

613
00:41:42,240 --> 00:41:43,680
I haven't covered.

614
00:41:43,680 --> 00:41:48,880
I gave you the ones that are the most important that I find to be used on a regular basis

615
00:41:48,880 --> 00:41:55,400
to solve different problems with regards to information extraction from the text.

616
00:41:55,400 --> 00:41:59,580
So that's going to be where we stop here with token attributes, and we're going to be moving

617
00:41:59,580 --> 00:42:06,200
on to part 2.5 of the book, which is part of speech tagging.

618
00:42:06,200 --> 00:42:12,360
I now want to move into kind of a more detailed analysis of part of speech within spacey and

619
00:42:12,360 --> 00:42:18,640
the dependency parser and how to actually analyze it really nicely either in a notebook

620
00:42:18,640 --> 00:42:20,280
or outside of a notebook.

621
00:42:20,280 --> 00:42:23,120
So let's work with a different text for just a few minutes.

622
00:42:23,120 --> 00:42:24,880
We're going to see why this is important.

623
00:42:24,880 --> 00:42:28,760
It's because I'm working on a zoomed in screen, and to make this sentence a little easier

624
00:42:28,760 --> 00:42:36,120
to understand, we're going to just use Mike and Joy's plain football, a very simple sentence.

625
00:42:36,120 --> 00:42:39,960
And we're going to create a new doc object, and we're going to call this doc two.

626
00:42:39,960 --> 00:42:42,840
That's going to be equal to NLP text.

627
00:42:42,840 --> 00:42:47,440
Let's print off doc two just to make sure that it was created, and in fact that we see

628
00:42:47,440 --> 00:42:48,440
that it was.

629
00:42:48,440 --> 00:42:54,200
Now that we've got it created, let's iterate over the tokens within this and say for token

630
00:42:54,200 --> 00:42:59,480
in text, we want to print off token dot text.

631
00:42:59,480 --> 00:43:01,600
We want to see what the text actually is.

632
00:43:01,600 --> 00:43:13,240
We want to see the token dot POS, and the token dot DEP helps if you actually iterate

633
00:43:13,240 --> 00:43:16,120
over the correct object over the doc to object.

634
00:43:16,120 --> 00:43:20,640
And we see that we've got Mike, proper noun, noun, subject, and Joy's verb.

635
00:43:20,640 --> 00:43:22,640
It's the root plane.

636
00:43:22,640 --> 00:43:24,440
In this case, it's a verb.

637
00:43:24,440 --> 00:43:28,280
And then we've got football, the noun, the direct object, and a period, which is the

638
00:43:28,280 --> 00:43:29,280
punctuation.

639
00:43:29,400 --> 00:43:33,640
So we can see the basic semantics of the sentence at play.

640
00:43:33,640 --> 00:43:37,200
What's really nice from spacey is we have a way to really visualize this information

641
00:43:37,200 --> 00:43:39,360
and how these words relate to one another.

642
00:43:39,360 --> 00:43:47,400
So we can say from spacey, import, displacey, and we're going to do displacey, displacey

643
00:43:47,400 --> 00:43:51,400
dot render.

644
00:43:51,400 --> 00:43:55,440
And this is going to take two arguments, it's going to be the text, and then it's going

645
00:43:55,440 --> 00:44:01,320
to be the, actually, it's going to be doc two, and then it's going to be style.

646
00:44:01,320 --> 00:44:05,440
In this case, we're going to be working with DEP, and we're going to print that off.

647
00:44:05,440 --> 00:44:10,440
And we actually see how that sentence is structured.

648
00:44:10,440 --> 00:44:13,720
Now in the textbook, I use a more complicated sentence.

649
00:44:13,720 --> 00:44:17,480
But for the reasons of this video, I've kept it a little shorter, just because I think

650
00:44:17,480 --> 00:44:22,320
it displays better on this screen, because you can see that this becomes a little bit

651
00:44:22,320 --> 00:44:25,600
more difficult to understand when you're zoomed in.

652
00:44:25,600 --> 00:44:29,440
But this is one sentence from that Wikipedia article.

653
00:44:29,440 --> 00:44:32,640
So go ahead and look at the textbook and see how elaborate this is.

654
00:44:32,640 --> 00:44:36,400
You can see how it's part of a compound, how it's preposition.

655
00:44:36,400 --> 00:44:42,680
You can see the more fine-grained aspects of the dependency parser and the part of speech

656
00:44:42,680 --> 00:44:46,800
tagger really at play with more complicated sentences.

657
00:44:46,800 --> 00:44:50,960
So that's going to be how you really access part of speech and how you can start to visualize

658
00:44:50,960 --> 00:44:57,240
how words in a sentence are connected to other words in a sentence with regards to their

659
00:44:57,240 --> 00:45:00,480
part of speech and their dependencies.

660
00:45:00,480 --> 00:45:01,960
That's going to be where we stop with that.

661
00:45:01,960 --> 00:45:07,200
In the next section, we're going to be talking about named entity recognition and how to visualize

662
00:45:07,200 --> 00:45:09,680
that information.

663
00:45:09,680 --> 00:45:13,920
So named entity recognition is a very common NLP task.

664
00:45:13,920 --> 00:45:19,080
It's part of kind of data extraction or information extraction from texts.

665
00:45:19,080 --> 00:45:23,480
It's oftentimes just called NER, named entity recognition.

666
00:45:23,480 --> 00:45:28,160
I have a whole book on how to do NER with Python and with Spacey, but we're not going

667
00:45:28,160 --> 00:45:30,120
to be talking about all the ins and outs right now.

668
00:45:30,120 --> 00:45:35,600
We're just going to be talking about how to access the pieces of information throughout

669
00:45:35,600 --> 00:45:37,080
kind of our text.

670
00:45:37,080 --> 00:45:42,560
And then we're going to be dealing with a lot of NER as we try to create elaborate systems

671
00:45:42,560 --> 00:45:47,880
to do named entity extraction for things like financial analysis.

672
00:45:47,880 --> 00:45:52,080
Let's go ahead and figure out how to iterate over a doc object.

673
00:45:52,080 --> 00:45:57,520
So we're going to say for int and doc.n, so we're going to go back to that original doc,

674
00:45:57,520 --> 00:46:03,800
the one that's got the text from Wikipedia on the United States.

675
00:46:03,800 --> 00:46:11,880
We're going to say print off int.text, so the text from it, and int.label, label underscore

676
00:46:11,880 --> 00:46:12,880
here.

677
00:46:12,880 --> 00:46:16,200
That's going to tell us what label corresponds to that text.

678
00:46:16,320 --> 00:46:17,320
Then we print this off.

679
00:46:17,320 --> 00:46:21,800
We've got a lot of GPEs, which are geopolitical entities, North America.

680
00:46:21,800 --> 00:46:23,280
This isn't a geopolitical entity.

681
00:46:23,280 --> 00:46:30,080
It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in

682
00:46:30,080 --> 00:46:36,920
this case, which is a national or religious political entity, quantity, the number of

683
00:46:36,920 --> 00:46:44,440
miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke.

684
00:46:44,480 --> 00:46:48,400
Then we have date being extracted, so at least 12,000 years ago.

685
00:46:48,400 --> 00:46:53,360
This is a small model, and it's extracting for us a lot of very important structured

686
00:46:53,360 --> 00:46:54,520
data.

687
00:46:54,520 --> 00:46:58,160
But we can see that the small model makes mistakes.

688
00:46:58,160 --> 00:47:02,120
So the Revolutionary War is being considered an organization.

689
00:47:02,120 --> 00:47:06,880
Were I to use a large model right now, which I can download separately from Spacey, and

690
00:47:06,880 --> 00:47:12,440
we're going to be seeing this later in this video, or were I to use the much larger transformer

691
00:47:12,440 --> 00:47:13,520
model.

692
00:47:13,520 --> 00:47:19,720
This would be correctly identified most likely as an event, not as an organization, but because

693
00:47:19,720 --> 00:47:24,360
this is a small model that doesn't contain word vectors, which we're going to talk about

694
00:47:24,360 --> 00:47:30,520
in just a little bit, it does not generalize or make predictions well on this particular

695
00:47:30,520 --> 00:47:31,520
data.

696
00:47:31,520 --> 00:47:35,680
Nevertheless, we do see really good extraction here.

697
00:47:35,680 --> 00:47:39,060
We have the American Civil War being extracted as an event.

698
00:47:39,060 --> 00:47:43,940
We have the Spanish American War, even with this encoding typographical error here.

699
00:47:43,940 --> 00:47:48,500
And World War being extracted as an event, World War II event, Cold War event.

700
00:47:48,500 --> 00:47:50,580
All of this is looking good.

701
00:47:50,580 --> 00:47:56,020
And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd

702
00:47:56,020 --> 00:47:57,020
expect to see.

703
00:47:57,020 --> 00:48:00,740
We even see percentages extracted correctly here.

704
00:48:00,740 --> 00:48:07,180
So this is how you access really vital information about your tokens, but more importantly about

705
00:48:07,180 --> 00:48:10,660
the entities found within your text.

706
00:48:10,660 --> 00:48:17,100
And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook.

707
00:48:17,100 --> 00:48:24,540
We can say displacie.render, we can say doc, style, we can say int.

708
00:48:24,540 --> 00:48:30,060
And we get this really nice visualization where each entity has its own particular color.

709
00:48:30,060 --> 00:48:34,140
So you can see where these entities appear within the text, as you kind of just naturally

710
00:48:34,140 --> 00:48:35,140
read it.

711
00:48:35,140 --> 00:48:39,100
And you can do this with the text as long as you want, you can even change the max length

712
00:48:39,100 --> 00:48:41,780
to be more than a million characters long.

713
00:48:41,780 --> 00:48:46,780
And again, we can see right here, org is incorrectly identified as the American Revolutionary War

714
00:48:46,780 --> 00:48:51,220
incorrectly identified as org, but nevertheless, we see really, really good results with a

715
00:48:51,220 --> 00:48:55,300
small English model without a lot of custom fine tune training.

716
00:48:55,300 --> 00:48:56,460
And there's a reason for this.

717
00:48:56,460 --> 00:49:00,860
A lot of Wikipedia data gets included into machine learning models.

718
00:49:00,860 --> 00:49:05,900
The machine learning models on text typically make good predictions on Wikipedia data, because

719
00:49:05,900 --> 00:49:08,060
it was included in their training process.

720
00:49:08,060 --> 00:49:10,620
Nevertheless, these are still good results.

721
00:49:10,620 --> 00:49:14,260
If I'm right or wrong on that, I'm not entirely certain.

722
00:49:14,260 --> 00:49:18,700
But that's going to be how you kind of extract important entities from your text, and most

723
00:49:18,700 --> 00:49:20,580
importantly visualize it.

724
00:49:20,580 --> 00:49:24,300
This is where chapter two of my book kind of ends.

725
00:49:24,300 --> 00:49:28,860
After this chapter, you have a good understanding, hopefully, of kind of what the dot container

726
00:49:28,860 --> 00:49:35,980
is, what tokens are, and how the doc object contains the attributes such as since and

727
00:49:35,980 --> 00:49:41,100
ends, which allows for you to find sentences and entities within a text.

728
00:49:41,100 --> 00:49:47,180
Hopefully you also have a good understanding of how to access the linguistic features of

729
00:49:47,180 --> 00:49:50,060
each token through token attributes.

730
00:49:50,060 --> 00:49:56,740
I encourage you to spend a lot of time becoming familiar with these basics, as these basics

731
00:49:56,740 --> 00:50:01,180
are the building block for really robust things that we're going to be getting into

732
00:50:01,180 --> 00:50:05,700
in the next few lessons.

733
00:50:05,700 --> 00:50:11,860
We're now moving into chapter three of our textbook on Spacey and Python.

734
00:50:11,860 --> 00:50:16,940
Now in chapter three, we're going to be continuing our theme of part one, where we're trying

735
00:50:16,940 --> 00:50:20,900
to understand the larger building blocks of Spacey.

736
00:50:20,900 --> 00:50:25,420
Even though this video is not going to deal with Spacey machine learning approaches, our

737
00:50:25,460 --> 00:50:30,260
custom ones, that is, it's still important to be familiar with what machine learning is

738
00:50:30,260 --> 00:50:36,620
and how it works, specifically with regards to language, because a lot of the Spacey models

739
00:50:36,620 --> 00:50:44,060
such as the medium, large and transformer models, all are machine learning models that

740
00:50:44,060 --> 00:50:47,620
have word vectors stored within them.

741
00:50:47,620 --> 00:50:54,420
This means that they're going to be larger, more accurate, and do the things a bit more

742
00:50:54,420 --> 00:50:58,260
slowly, depending upon its size.

743
00:50:58,260 --> 00:51:03,900
We're going to be working through not only what kind of machine learning is generally,

744
00:51:03,900 --> 00:51:08,540
but specifically how it works with regards to text.

745
00:51:08,540 --> 00:51:13,380
I think that this is where you're going to find this textbook to be somewhat helpful.

746
00:51:13,380 --> 00:51:18,820
What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did

747
00:51:18,820 --> 00:51:23,860
before, but this time we're going to be installing a new model.

748
00:51:23,900 --> 00:51:32,900
We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then

749
00:51:32,900 --> 00:51:37,500
we're going to download the Ncore Web MD model.

750
00:51:37,500 --> 00:51:39,700
This is the medium English model.

751
00:51:39,700 --> 00:51:43,580
This is going to take a little longer to download, and the reason why I'm having you download

752
00:51:43,580 --> 00:51:48,660
the medium model, and the reason why we're going to be using the medium model, is because

753
00:51:48,660 --> 00:51:53,620
the medium model has stored within it word vectors.

754
00:51:54,380 --> 00:52:01,340
Let's go ahead and talk a little bit about what word vectors are and how they're useful.

755
00:52:01,340 --> 00:52:04,700
So word vectors are word embeddings.

756
00:52:04,700 --> 00:52:12,820
So these are numerical representations of words in multi-dimensional space through matrices.

757
00:52:12,820 --> 00:52:15,620
That's a very compacted sentence.

758
00:52:15,620 --> 00:52:17,380
So let's break it down.

759
00:52:17,380 --> 00:52:19,260
What are word vectors used for?

760
00:52:19,260 --> 00:52:26,220
Well, they're used for a computer system to understand what a word actually means.

761
00:52:26,220 --> 00:52:29,740
So computers can't really parse text all that efficiently.

762
00:52:29,740 --> 00:52:31,660
They can't parse it at all.

763
00:52:31,660 --> 00:52:35,660
Every word needs to be converted into some kind of a number.

764
00:52:35,660 --> 00:52:39,540
Now for some old approaches, you would use something like a bag of words approach where

765
00:52:39,540 --> 00:52:43,460
each individual word would have a corresponding number to it.

766
00:52:43,460 --> 00:52:47,540
This would be a unique number that corresponds just to that word.

767
00:52:47,540 --> 00:52:53,660
There are a lot of tasks that can work, but for something like text understanding or trying

768
00:52:53,660 --> 00:52:59,780
to get a computer system to be able to understand how a word functions within a sentence in general,

769
00:52:59,780 --> 00:53:04,700
in other words, how it works in the language, how it relates to all other words, that doesn't

770
00:53:04,700 --> 00:53:06,860
really work for us.

771
00:53:06,860 --> 00:53:11,060
So what a word vector is, is it's a multi-dimensional representation.

772
00:53:11,060 --> 00:53:15,780
So instead of a number having just a single integer that corresponds to it, it instead

773
00:53:15,820 --> 00:53:22,100
has what looks like to an unsuspecting eye, essentially.

774
00:53:22,100 --> 00:53:28,300
It has a very complex sequence of floating numbers that are stored as an array, which

775
00:53:28,300 --> 00:53:34,580
is a computationally less expensive form of a list in Python or just computing in general.

776
00:53:34,580 --> 00:53:36,460
And this is what it looks like, a long sequence.

777
00:53:36,460 --> 00:53:42,700
In this case, I believe it's a 300-dimensional word that corresponds to a specific word.

778
00:53:42,700 --> 00:53:47,860
So this is what an array or a word vector or a word embedding looks like.

779
00:53:47,860 --> 00:53:53,540
What this means to a computer system is it means syntactical and semantical meaning.

780
00:53:53,540 --> 00:53:57,780
So the way word vectors are typically trained is, oh, there's a few different approaches,

781
00:53:57,780 --> 00:54:03,220
but kind of the old-school word-to-vec approach is you give a computer system a whole bunch

782
00:54:03,220 --> 00:54:09,260
of texts and different smaller, larger collections of texts, and what it does is it reads through

783
00:54:09,300 --> 00:54:15,820
all of them and figures out how words are used in relation to other words.

784
00:54:15,820 --> 00:54:21,060
And so what it's able to essentially do through this training process is figure out meaning.

785
00:54:21,060 --> 00:54:25,940
And what that meaning allows for a computer system to do is understand how a word might

786
00:54:25,940 --> 00:54:31,860
relate to other words within a sentence or within a language as a whole.

787
00:54:31,860 --> 00:54:36,060
And in order to understand this, I think it's best if we move away from this textbook and

788
00:54:36,060 --> 00:54:40,620
actually try to explore what word vectors look like in spacey.

789
00:54:40,620 --> 00:54:45,300
So you can have a better sense of specifically what they do, why they're useful, and how

790
00:54:45,300 --> 00:54:51,580
you, as a NLP practitioner, can go ahead and start leveraging them.

791
00:54:51,580 --> 00:54:54,780
So just like before, we're going to create an NLP object.

792
00:54:54,780 --> 00:54:59,820
This time, however, instead of loading in our Encore Web SM, we're going to load in

793
00:54:59,820 --> 00:55:02,780
our Encore Web MD.

794
00:55:02,780 --> 00:55:07,820
So the one that actually has these word vectors stored, the static vectors saved, and it's

795
00:55:07,820 --> 00:55:09,060
going to be a larger model.

796
00:55:09,060 --> 00:55:11,300
Let's go ahead and execute that cell.

797
00:55:11,300 --> 00:55:14,940
And while that's executing, we're going to start opening up our text.

798
00:55:14,940 --> 00:55:23,500
So we're going to say with open data wiki underscore us.txt, r as f, and we're going

799
00:55:23,500 --> 00:55:28,460
to say text is equal to f.read, so we're going to successfully load in that text file and

800
00:55:28,460 --> 00:55:29,460
open it up.

801
00:55:29,540 --> 00:55:33,980
Then we're going to create our doc object, which will be equal to NLP text.

802
00:55:33,980 --> 00:55:37,300
All the syntax is staying the exact same.

803
00:55:37,300 --> 00:55:40,620
And just like before, let's grab the first sentence.

804
00:55:40,620 --> 00:55:47,540
So we're going to convert our doc.sense generator into a list, and we're going to grab index

805
00:55:47,540 --> 00:55:48,540
zero.

806
00:55:48,540 --> 00:55:53,060
And let's go ahead and print off sentence one, just so you can kind of see it.

807
00:55:53,060 --> 00:55:54,720
And there it is.

808
00:55:54,720 --> 00:56:00,000
So now that we've got that kind of in memory, we can start kind of working with it a little

809
00:56:00,000 --> 00:56:01,000
bit.

810
00:56:01,000 --> 00:56:06,400
So let's go ahead and just start tackling how we can actually use word vectors with

811
00:56:06,400 --> 00:56:08,080
spacey.

812
00:56:08,080 --> 00:56:12,720
So let's kind of think about a general question right now.

813
00:56:12,720 --> 00:56:22,800
Let's say I wanted to know how the word, let's say country is similar to other words within

814
00:56:22,800 --> 00:56:25,080
our model's word embeddings.

815
00:56:25,080 --> 00:56:27,840
So let's create a little way we can do this.

816
00:56:27,840 --> 00:56:35,040
We're going to say your word, and this is going to be equal to the word country, country.

817
00:56:35,040 --> 00:56:36,040
There we go.

818
00:56:36,040 --> 00:56:40,280
And what we can do is we can say MS is equal to NLP.

819
00:56:40,280 --> 00:56:42,600
So we're going to go into that NLP object.

820
00:56:42,600 --> 00:56:50,440
We're going to grab the vocab.vectors, and we're going to say most similar.

821
00:56:50,440 --> 00:56:52,440
And this is a little complicated way of doing it.

822
00:56:52,440 --> 00:56:55,520
In fact, I'm going to go ahead and just kind of copy and paste this in.

823
00:56:55,520 --> 00:57:01,520
You have the code already in your textbook that you can follow along with.

824
00:57:01,520 --> 00:57:10,960
And I'm going to go ahead and just copy and paste it in right here and print off this.

825
00:57:10,960 --> 00:57:15,880
And what this is going to do is it is going to go ahead and just do this entirely.

826
00:57:15,880 --> 00:57:18,680
There we go.

827
00:57:19,680 --> 00:57:25,520
And we have to import numpy as MP.

828
00:57:25,520 --> 00:57:28,760
This lets us actually work with the data as a numpy array.

829
00:57:28,760 --> 00:57:34,040
And when we execute this cell, what we get is an output that tells us all the words

830
00:57:34,040 --> 00:57:37,200
that are most similar to the word country.

831
00:57:37,200 --> 00:57:41,520
So in this scenario, the word country, it has these kind of all these different similar

832
00:57:41,520 --> 00:57:47,440
words to it from the word country to the word country, capitalized nation, nation.

833
00:57:47,440 --> 00:57:49,960
Now it's important to understand what you're seeing here.

834
00:57:49,960 --> 00:57:56,440
What you're seeing is not necessarily a synonym for the word country, rather what you're seeing

835
00:57:56,440 --> 00:58:00,160
is are the words that are the most similar.

836
00:58:00,160 --> 00:58:06,560
Now this can be anything from a synonym to a variant spelling of that word to something

837
00:58:06,560 --> 00:58:09,720
that occurs frequently alongside of it.

838
00:58:09,720 --> 00:58:14,640
So for example, world, while this isn't the same, we would never consider world to be

839
00:58:14,640 --> 00:58:16,720
the synonym of country.

840
00:58:16,720 --> 00:58:21,640
But what happens is, is syntactically they're used in very similar situations.

841
00:58:21,640 --> 00:58:25,440
So the way you describe a country is sometimes the way you would describe your world, or

842
00:58:25,440 --> 00:58:30,120
maybe it's something to do with the hierarchy, so a country is found within the world.

843
00:58:30,120 --> 00:58:31,520
This is a good way to understand it.

844
00:58:31,520 --> 00:58:39,560
So it's always good to use this word as most similar, not to be something like synonym.

845
00:58:39,560 --> 00:58:43,520
So when you're talking about word vectors similarity, you're not talking about synonym

846
00:58:43,520 --> 00:58:44,520
similarity.

847
00:58:45,400 --> 00:58:47,200
But this is a way you can kind of quickly get a sense.

848
00:58:47,200 --> 00:58:49,440
So what does this do for you?

849
00:58:49,440 --> 00:58:51,960
Why did I go through and explain all these things about word vectors?

850
00:58:51,960 --> 00:58:56,240
If I'm not going to be talking about machine learning a whole bunch throughout this video.

851
00:58:56,240 --> 00:58:59,560
Well, I did it so that you can do one thing that's really important.

852
00:58:59,560 --> 00:59:03,040
And that's calculate document similarity in the spacey.

853
00:59:03,040 --> 00:59:05,680
So we've already got our NLP model loaded up.

854
00:59:05,680 --> 00:59:07,560
Let's create one object.

855
00:59:07,560 --> 00:59:11,400
So we're going to make doc one, we're going to make that equal to NLP.

856
00:59:11,400 --> 00:59:14,080
And we're going to create the text right here in this object.

857
00:59:14,080 --> 00:59:18,120
So let's say this is coming straight from the spacey documentation.

858
00:59:18,120 --> 00:59:23,600
I like salty fries and hamburgers.

859
00:59:23,600 --> 00:59:26,760
And we're going to say doc two is equal to NLP.

860
00:59:26,760 --> 00:59:33,040
And this is going to be the text fast food tastes very good.

861
00:59:33,040 --> 00:59:36,320
And now what we can do is let's go ahead and load those into memory.

862
00:59:36,320 --> 00:59:40,920
What we can do is we can actually make a calculation using spacey to find out how similar they

863
00:59:40,920 --> 00:59:43,600
actually are these two different sentences.

864
00:59:43,640 --> 00:59:49,000
We can say print off doc one, and we're going to say this again, this is coming straight

865
00:59:49,000 --> 00:59:53,240
from the spacey documentation doc two, so you're going to be able to see what both documents

866
00:59:53,240 --> 00:59:54,240
are.

867
00:59:54,240 --> 00:59:57,080
And then we're going to do doc one dot similarity.

868
00:59:57,080 --> 01:00:02,120
So we can go into the doc one dot similarity method and we can compare it to doc two.

869
01:00:02,120 --> 01:00:03,920
We can print that off.

870
01:00:03,920 --> 01:00:09,120
So what we're seeing here on the left is document one, this little divider thing that we printed

871
01:00:09,120 --> 01:00:10,240
off here.

872
01:00:10,240 --> 01:00:15,400
On the right, we have document two, and then we can see the degree of similarity between

873
01:00:15,400 --> 01:00:17,840
document one and document two.

874
01:00:17,840 --> 01:00:18,840
Let's create another doc object.

875
01:00:18,840 --> 01:00:22,920
We're going to call this NLP doc three, and we're going to make this NLP.

876
01:00:22,920 --> 01:00:25,240
Let's come up with a sentence that's completely different.

877
01:00:25,240 --> 01:00:32,840
The Empire State Building is in New York.

878
01:00:32,840 --> 01:00:36,640
So this is when I'm just making up off the top of my head right now.

879
01:00:36,640 --> 01:00:42,240
I'm going to copy and paste this down, and we're going to compare this to doc one.

880
01:00:42,240 --> 01:00:46,160
We're going to compare it to doc three, and we get a score of point five one.

881
01:00:46,160 --> 01:00:50,400
So this is less similar to than these two.

882
01:00:50,400 --> 01:00:52,880
So this is a way that you can take a whole bunch of documents.

883
01:00:52,880 --> 01:00:56,760
You can create a simple for loop, and you can find and start clustering the documents

884
01:00:56,760 --> 01:00:59,880
that have a lot of overlap or similarity.

885
01:00:59,880 --> 01:01:01,760
How is this similarity being calculated?

886
01:01:01,760 --> 01:01:06,080
Well, it's being calculated because what spacey is doing is it's going into its word

887
01:01:06,080 --> 01:01:11,360
embeddings, and even though in these two situations, we're not using the word fast

888
01:01:11,360 --> 01:01:14,000
food ever in this document.

889
01:01:14,000 --> 01:01:19,600
It's going in and it knows that salty fries and hamburgers are probably in a close cluster

890
01:01:19,600 --> 01:01:26,240
with the biogram or a token that's made up of two words, a biogram of fast food.

891
01:01:26,240 --> 01:01:31,240
So what it's doing is it's assigning a prediction that these two are still somewhat similar,

892
01:01:31,240 --> 01:01:36,600
more similar than these two, because of these overlapping in words.

893
01:01:36,600 --> 01:01:41,140
So let's try one more example, see if we can get something that's really, really close.

894
01:01:41,140 --> 01:01:49,840
So let's take doc four, and this is going to be equal to NLP, I enjoy oranges.

895
01:01:49,840 --> 01:01:55,480
And then we're going to have doc five is going to be equal to NLP, I enjoy apples.

896
01:01:55,480 --> 01:02:00,400
So two, I would agree, I would argue very, very syntactically similar sentences.

897
01:02:00,400 --> 01:02:05,200
And we're going to do doc four here, doc five here, and we're going to look and see

898
01:02:05,200 --> 01:02:08,000
a similarity between doc four and doc five.

899
01:02:08,000 --> 01:02:13,000
And if we execute this, we get a similarity of 0.96.

900
01:02:13,000 --> 01:02:14,600
So this is really high.

901
01:02:14,600 --> 01:02:18,600
This is telling me that these two sentences are very similar, and it's not just that they're

902
01:02:18,600 --> 01:02:25,160
similar because of the similar syntax here, that's definitely pushing the number up.

903
01:02:25,160 --> 01:02:29,200
It's that what the individual is liking in the scenario between these two texts, they're

904
01:02:29,200 --> 01:02:30,720
both fruits.

905
01:02:30,720 --> 01:02:31,720
Let's try something different.

906
01:02:31,720 --> 01:02:33,920
Let's make doc five.

907
01:02:33,920 --> 01:02:42,240
Let's just make doc six here, and do something like this NLP, I enjoy, what's another word

908
01:02:42,240 --> 01:02:44,800
we could say.

909
01:02:44,800 --> 01:02:49,920
Something that's different, let's say burgers, something different from a fruit.

910
01:02:49,920 --> 01:02:54,080
So we're going to make doc six like that, and we're going to again copy and paste this

911
01:02:54,080 --> 01:03:01,400
down, copy and paste this down, we're going to put doc six here.

912
01:03:01,400 --> 01:03:03,000
And we see this drop.

913
01:03:03,000 --> 01:03:08,000
So what this demonstrates, and I'm really glad this worked because I improvised this,

914
01:03:08,000 --> 01:03:14,840
what this demonstrates is that the similarity, the number that's given is not dependent on

915
01:03:14,840 --> 01:03:23,080
the contextual words, rather it's dependent upon the semantic similarity of the words.

916
01:03:23,080 --> 01:03:31,160
So apples and oranges are in a similar cluster around fruit because of their word embeddings.

917
01:03:31,160 --> 01:03:37,720
The word burgers while still being food and still being plural is different from apples

918
01:03:37,720 --> 01:03:38,960
and oranges.

919
01:03:38,960 --> 01:03:42,960
So in other words, this similarity is being calculated based on something that we humans

920
01:03:42,960 --> 01:03:49,000
would calculate difference in meaning based on a large understanding of a language as

921
01:03:49,000 --> 01:03:50,160
a whole.

922
01:03:50,160 --> 01:03:54,520
That's where word vectors really come into play.

923
01:03:54,520 --> 01:03:57,240
This allows for you to calculate other things as well.

924
01:03:57,240 --> 01:04:02,720
So you could even calculate the difference between salty fries and hamburgers, for example,

925
01:04:02,720 --> 01:04:07,840
I've got this example ready to go in the textbook, let's go ahead and try this as well.

926
01:04:07,840 --> 01:04:13,880
So we're going to grab doc one, and print off these few things right here.

927
01:04:13,880 --> 01:04:18,400
So we're going to try to calculate the similarity between french fries and burgers and what

928
01:04:18,400 --> 01:04:23,360
we get is a similarity of 0.73.

929
01:04:23,360 --> 01:04:28,880
So if we were to maybe change this up a little bit and try to calculate the similarity between

930
01:04:28,880 --> 01:04:37,560
maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity.

931
01:04:37,560 --> 01:04:42,000
So my point is, is play around with the similarity calculator, play around with the structure,

932
01:04:42,000 --> 01:04:47,920
the code I provided here, and get familiar with how spacey can help you kind of find

933
01:04:47,920 --> 01:04:52,280
a similarity, not just between documents, but between words as well.

934
01:04:52,280 --> 01:04:55,960
And we're going to be seeing how this is useful later on.

935
01:04:55,960 --> 01:04:59,840
But again, it's good to be familiar with kind of generally how machine learning kind of

936
01:04:59,840 --> 01:05:05,200
functions here in this context, and why these medium and large models are so much bigger.

937
01:05:05,200 --> 01:05:10,000
They're so much bigger because they have more word vectors that are much deeper.

938
01:05:10,000 --> 01:05:15,080
And the transformer model is much larger because it was trained in a completely different method

939
01:05:15,080 --> 01:05:17,680
than the way the medium and large models were trained.

940
01:05:17,680 --> 01:05:22,360
But again, that's out of the scope for this video.

941
01:05:22,360 --> 01:05:28,120
I now want to turn to really the last subject of this introduction to spacey part one, which

942
01:05:28,120 --> 01:05:32,120
is when we're taking this large umbrella view of the spacey.

943
01:05:32,120 --> 01:05:35,480
And in the textbook, it's going to correspond to chapter four.

944
01:05:35,480 --> 01:05:41,320
So what we go over in this textbook is kind of a large view of not just the dot container

945
01:05:41,320 --> 01:05:46,760
and the word vectors and the linguistic annotations, but really kind of the structure of the spacey

946
01:05:46,880 --> 01:05:50,240
framework, which comes around the pipeline.

947
01:05:50,240 --> 01:05:56,280
So a pipeline is a very common expression in computer science and in data science.

948
01:05:56,280 --> 01:05:59,680
Think of it as a traditional pipeline that you would see in a house.

949
01:05:59,680 --> 01:06:04,760
Now think of a pipeline being a sequence of different pipes.

950
01:06:04,760 --> 01:06:11,200
Each pipe in a computer system is going to perform some kind of permutation or some action

951
01:06:11,200 --> 01:06:16,280
on a piece of data as it goes through the pipeline.

952
01:06:16,280 --> 01:06:21,920
And as each pipe has a chance to act and make changes to and additions to that data,

953
01:06:21,920 --> 01:06:25,440
the later pipes get to benefit from those changes.

954
01:06:25,440 --> 01:06:28,320
So this is very common when you're thinking about logic of code.

955
01:06:28,320 --> 01:06:32,240
I provide it like a little image here that I think maybe might help you.

956
01:06:32,240 --> 01:06:38,600
So if we imagine some input sentence, right, so some input text is entering a spacey pipeline,

957
01:06:38,600 --> 01:06:41,440
it's going to go through a bunch of things if you're working with the medium model or

958
01:06:41,480 --> 01:06:47,280
the small model, that'll tokenize it and give it a word and vector for different words.

959
01:06:47,280 --> 01:06:53,000
It'll also find the POS, the part of speech, the dependency parser will act on it.

960
01:06:53,000 --> 01:07:00,040
But it might eventually get to an entity ruler, which we're going to see in just a few minutes.

961
01:07:00,040 --> 01:07:05,400
The entity ruler will be a series of rules-based NER named entity recognition.

962
01:07:05,400 --> 01:07:09,880
So it'll maybe assign a token to an entity.

963
01:07:09,880 --> 01:07:13,320
Might be the beginning of an entity, might be the end of an entity,

964
01:07:13,320 --> 01:07:15,960
might just be an individual token entity.

965
01:07:15,960 --> 01:07:21,960
And then what will happen is that doc object, as it kind of goes through this pipeline,

966
01:07:21,960 --> 01:07:25,360
will now receive a bunch of doc.ins.

967
01:07:25,360 --> 01:07:31,600
So it'll be, this pipe will actually add to the doc object as it goes through the pipeline,

968
01:07:31,600 --> 01:07:33,240
the entity component.

969
01:07:33,240 --> 01:07:38,120
And then the next pipeline, the entity linker, might take all those entities and try to find out

970
01:07:38,120 --> 01:07:39,400
which ones they are.

971
01:07:39,400 --> 01:07:45,120
So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number

972
01:07:45,120 --> 01:07:47,320
that corresponds to a specific person.

973
01:07:47,320 --> 01:07:52,960
So for example, if you were seeing a bunch of things like Paul something, Paul something,

974
01:07:52,960 --> 01:07:56,920
maybe that one Paul something might be Paul Hollywood from the Great British Bake Off,

975
01:07:56,920 --> 01:08:00,520
and it might have to make a connection to a specific person.

976
01:08:00,520 --> 01:08:05,920
So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood,

977
01:08:05,920 --> 01:08:07,720
depending on the context.

978
01:08:07,840 --> 01:08:13,120
That's out of the scope of this video series, but keep in mind that that pipe would do something

979
01:08:13,120 --> 01:08:17,680
else that would modify the ints that would give them greater specificity.

980
01:08:17,680 --> 01:08:22,200
And then what you'd be left with is the doc object on the output that not only has entities

981
01:08:22,200 --> 01:08:27,200
annotated, but it's also got entities linked to some generic specific data.

982
01:08:27,200 --> 01:08:29,360
So that's going to be how a pipeline works.

983
01:08:29,360 --> 01:08:31,520
And this is really what spacey is.

984
01:08:31,520 --> 01:08:35,640
It's a sequence of pipes that act on your data.

985
01:08:35,640 --> 01:08:39,640
And that's important to understand, because it means that as you add things to a spacey

986
01:08:39,640 --> 01:08:45,160
pipeline, you need to be very conscientious about where they're outed and in what order.

987
01:08:45,160 --> 01:08:48,720
As we're going to see as we move over to kind of rules based spacey, when we start talking

988
01:08:48,720 --> 01:08:54,560
about these different pipes, the entity ruler, the matcher custom components, regex components,

989
01:08:54,560 --> 01:08:56,760
you're going to need to know which order to put them in.

990
01:08:56,760 --> 01:08:58,800
It's going to be very important.

991
01:08:58,800 --> 01:09:01,000
So do please keep that in mind.

992
01:09:01,000 --> 01:09:05,400
Now spacey has a bunch of different attribute rulers or different pipes that you can kind

993
01:09:05,400 --> 01:09:06,960
of add into it.

994
01:09:06,960 --> 01:09:11,000
You've got dependency parsers that are going to come standard with all of your models.

995
01:09:11,000 --> 01:09:15,040
You've got the entity linker entity, recognizer entity ruler, you're going to have to make

996
01:09:15,040 --> 01:09:17,840
these yourself and add them in oftentimes.

997
01:09:17,840 --> 01:09:19,000
You've got a limitizer.

998
01:09:19,000 --> 01:09:22,440
This is going to be on most of your standard models, your morphologue, that's going to

999
01:09:22,440 --> 01:09:25,960
be on on there as well, sentence recognizer, synthesizer.

1000
01:09:25,960 --> 01:09:31,520
This is what allow for you to have the doc.sense right here span categorizer.

1001
01:09:31,520 --> 01:09:37,320
This will help categorize different spans, be them single token spans or sequence of

1002
01:09:37,320 --> 01:09:42,560
token spans, your tagger, this will tag the different things in your text, which will

1003
01:09:42,560 --> 01:09:45,160
help with part of speech, your text categorizer.

1004
01:09:45,160 --> 01:09:49,200
This is when you train a machine learning model to recognize different categories of

1005
01:09:49,200 --> 01:09:50,200
a text.

1006
01:09:50,200 --> 01:09:55,760
So text classification, which is a very important machine learning task, tote to VEC.

1007
01:09:55,760 --> 01:10:01,440
This is going to be what assigns word embeddings to the different words in your doc object.

1008
01:10:01,440 --> 01:10:06,760
Organizer is what breaks that thing up and all your text into individual tokens.

1009
01:10:06,760 --> 01:10:09,800
And you've got things like transformer and trainable pipes.

1010
01:10:09,800 --> 01:10:13,160
Then within this, you've also got some other things called matchers.

1011
01:10:13,160 --> 01:10:14,840
So you can do some dependency matching.

1012
01:10:14,840 --> 01:10:17,000
We're not going to get into that in this video.

1013
01:10:17,000 --> 01:10:20,120
You've also got the ability to use matcher and phrase matcher.

1014
01:10:20,120 --> 01:10:24,720
These are a lot of the times can do some similar things, but they're executed a little differently

1015
01:10:24,720 --> 01:10:25,960
to make things less confusing.

1016
01:10:25,960 --> 01:10:29,680
I'm really only talking about the matcher of these two.

1017
01:10:29,800 --> 01:10:33,880
If there's a need for it, I'll add into the textbook the phrase matcher at a later date,

1018
01:10:33,880 --> 01:10:35,760
but I'm not going to cover it in this video.

1019
01:10:35,760 --> 01:10:39,880
And if I do add in the phrase matcher, it's going to be after this matcher section here.

1020
01:10:39,880 --> 01:10:41,440
I have it in the GitHub repo.

1021
01:10:41,440 --> 01:10:45,200
I just haven't included in the textbook to keep things a little bit simpler, at least

1022
01:10:45,200 --> 01:10:46,880
if you're just starting out.

1023
01:10:46,880 --> 01:10:52,280
So a big good question is, well, how do you add pipes to a spacey pipeline?

1024
01:10:52,280 --> 01:10:53,680
So let's go ahead and do that.

1025
01:10:53,680 --> 01:10:57,440
We're going to make a blank spacey pipeline right now.

1026
01:10:57,440 --> 01:11:02,800
Let's go ahead and just make, we'll just work with the same live coding notebook that we

1027
01:11:02,800 --> 01:11:04,080
have open right now.

1028
01:11:04,080 --> 01:11:07,520
So what we're going to do is we're going to make a blank model, and we're going to actually

1029
01:11:07,520 --> 01:11:14,120
add in our own sentenizer to our, to our text.

1030
01:11:14,120 --> 01:11:16,240
So let's go ahead and do that.

1031
01:11:16,240 --> 01:11:20,680
So I'm going to say NLP is equal to a spacey dot blank.

1032
01:11:20,680 --> 01:11:24,440
This is going to allow for me to make a blank spacey pipeline.

1033
01:11:24,440 --> 01:11:28,640
And I'm going to say Ian so that it knows that the tokenizer that I need to use is the

1034
01:11:28,640 --> 01:11:30,640
English tokenizer.

1035
01:11:30,640 --> 01:11:36,040
And now if I want to add a pipe to that, I can use one of the built-in spacey features.

1036
01:11:36,040 --> 01:11:40,880
So I can say add underscore pipe, and I can say sentenizer.

1037
01:11:40,880 --> 01:11:43,400
So I can add in a sentenizer.

1038
01:11:43,400 --> 01:11:49,160
This is going to allow for me to create a pipeline now that has a sequence of two different

1039
01:11:49,160 --> 01:11:50,160
pipes.

1040
01:11:50,160 --> 01:11:53,560
And I demonstrate in the textbook why this is important.

1041
01:11:53,560 --> 01:11:58,960
Sometimes what you need to do is you need to just only break down a text into individual

1042
01:11:58,960 --> 01:11:59,960
sentences.

1043
01:11:59,960 --> 01:12:08,080
So I grabbed a massive, massive corpus from the internet, which is on MIT.edu.

1044
01:12:08,080 --> 01:12:10,560
And it's the entire Shakespeare corpus.

1045
01:12:10,560 --> 01:12:15,320
And I just try to calculate the, the quantity of sentences found within it.

1046
01:12:15,320 --> 01:12:22,760
There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and

1047
01:12:22,760 --> 01:12:26,040
count those sentences with the spacey model.

1048
01:12:26,040 --> 01:12:32,080
Using the small model, however, it took a total amount of time of 47 minutes to actually

1049
01:12:32,080 --> 01:12:35,520
break down all those sentences and extract them.

1050
01:12:35,520 --> 01:12:39,600
Why is there a difference in time between 7 seconds and 47 minutes?

1051
01:12:39,600 --> 01:12:44,320
It's because that this spacey small model has a bunch of other pipes in it that are

1052
01:12:44,320 --> 01:12:46,520
trying to do a bunch of other things.

1053
01:12:46,520 --> 01:12:54,600
If you just need to do one task, it's always a good idea to just activate one pipe or maybe

1054
01:12:54,600 --> 01:12:59,240
make a blank model and just add that single pipe or the only pipes that you need to it.

1055
01:12:59,240 --> 01:13:04,120
A great example of this is needing to tokenize a whole bunch of sentences in relatively short

1056
01:13:04,120 --> 01:13:05,120
time.

1057
01:13:05,120 --> 01:13:11,280
So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.

1058
01:13:11,280 --> 01:13:12,840
However comes at a trade-off.

1059
01:13:12,840 --> 01:13:18,640
The small model is going to be more accurate in how it finds sentence boundaries.

1060
01:13:18,640 --> 01:13:21,080
So we have a difference in quantity here.

1061
01:13:21,080 --> 01:13:26,240
This difference in quantity indicates that this one messed up and made some mistakes because

1062
01:13:26,240 --> 01:13:27,960
it was just the sentenceizer.

1063
01:13:27,960 --> 01:13:31,760
The sentenceizer didn't have extra data being fed to it.

1064
01:13:31,760 --> 01:13:36,560
In fact, if I probably used larger models, I might even have better results.

1065
01:13:36,560 --> 01:13:37,760
But always think about that.

1066
01:13:37,760 --> 01:13:42,400
If time is of the essence and you don't care so much about accuracy, a great way to get

1067
01:13:42,400 --> 01:13:46,800
the quantity of sentences or at least a ballpark is to use this method where you simply add

1068
01:13:46,800 --> 01:13:50,120
in a sentenceizer to a blank model.

1069
01:13:50,120 --> 01:13:56,240
So that's how you actually add in different pipes to a spacey pipeline and we're going

1070
01:13:56,240 --> 01:14:00,760
to be reinforcing that skill as we go through, especially in part two, where we really kind

1071
01:14:00,760 --> 01:14:03,200
of work with this in a lot of detail.

1072
01:14:03,200 --> 01:14:08,800
Right now I'm just interested in giving you the general understanding of how this might

1073
01:14:08,800 --> 01:14:09,800
work.

1074
01:14:09,800 --> 01:14:16,240
Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes.

1075
01:14:16,240 --> 01:14:20,160
We can analyze what our analyze, there we go.

1076
01:14:20,160 --> 01:14:22,460
We can actually analyze our pipeline.

1077
01:14:22,460 --> 01:14:27,600
If we look at the NLP object, which is our blank model with the sentenceizer, we see

1078
01:14:27,600 --> 01:14:35,760
that our NLP pipeline ignore summary, ignore this bit here.

1079
01:14:35,800 --> 01:14:39,840
But what you're actually able to kind of go through and see right away is that we've really

1080
01:14:39,840 --> 01:14:42,640
just got the sentenceizer sitting in it.

1081
01:14:42,640 --> 01:14:48,520
If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey

1082
01:14:48,520 --> 01:14:55,920
dot load and core web SM, we're going to create that NLP two object around the small spacey

1083
01:14:55,920 --> 01:14:59,760
English model.

1084
01:14:59,760 --> 01:15:06,760
We can analyze the pipes again, and we see a much more elaborate pipeline.

1085
01:15:06,760 --> 01:15:07,760
So what are we looking at?

1086
01:15:07,760 --> 01:15:12,400
Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger

1087
01:15:12,400 --> 01:15:18,880
after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got

1088
01:15:18,880 --> 01:15:23,520
an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed

1089
01:15:23,520 --> 01:15:26,400
the doc dot ends, and we keep on going down.

1090
01:15:26,400 --> 01:15:31,240
We can see the limitizer, but we can see also a whole bunch of other things.

1091
01:15:31,240 --> 01:15:34,840
We can see what these different things actually assign.

1092
01:15:34,840 --> 01:15:42,000
So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually

1093
01:15:42,000 --> 01:15:43,120
require.

1094
01:15:43,120 --> 01:15:48,040
So if we look up here, we see that the NER pipe, so the name to the recognition pipe

1095
01:15:48,040 --> 01:15:51,280
is responsible for assigning the doc dot ends.

1096
01:15:51,280 --> 01:15:56,080
So that attribute of the doc object, and it's also responsible at the token level for

1097
01:15:56,080 --> 01:16:02,480
assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes

1098
01:16:02,480 --> 01:16:10,080
ago when we talked about the IOB being the opening beginning or out beginning inside

1099
01:16:10,080 --> 01:16:16,720
for a different entity, it also assigns the end dot end underscore type for each token

1100
01:16:16,720 --> 01:16:17,720
attribute.

1101
01:16:17,720 --> 01:16:23,240
So you can see a lot of different things about your pipeline by using NLP dot analyze underscore

1102
01:16:23,240 --> 01:16:24,880
pipes.

1103
01:16:24,880 --> 01:16:29,800
If you've gotten to this point in the video, then I think you should by now have a good

1104
01:16:29,800 --> 01:16:36,040
really umbrella view of what spacey is, how it works, why it's useful.

1105
01:16:36,040 --> 01:16:39,560
And some of the basic features that it can do and how it can solve some pretty complex

1106
01:16:39,560 --> 01:16:43,320
problems with some pretty simple lines of code.

1107
01:16:43,320 --> 01:16:48,280
What we're going to see now moving forward is how you as a practitioner of NLP cannot

1108
01:16:48,280 --> 01:16:52,920
just take what's given to you with spacey, but start working with it and start leveraging

1109
01:16:52,920 --> 01:16:55,200
it for your own uses.

1110
01:16:55,200 --> 01:16:57,280
So taking what is already available.

1111
01:16:57,280 --> 01:17:02,120
So like these models like the English model and adding to them contributing to them.

1112
01:17:02,120 --> 01:17:07,000
Maybe you want to make an entity ruler where you can find more entities in a text based

1113
01:17:07,000 --> 01:17:10,120
on some cousin tier or list that you have.

1114
01:17:10,120 --> 01:17:14,280
Maybe you want to make a matcher so you can find specific sequences within a text.

1115
01:17:14,280 --> 01:17:16,880
Maybe that's important for information extraction.

1116
01:17:16,880 --> 01:17:20,600
Maybe you need to add custom functions or components into your spacey pipeline.

1117
01:17:20,600 --> 01:17:24,920
I'm going to be going through in part two rules based spacey and giving you all the

1118
01:17:24,920 --> 01:17:31,160
basics of how to do some really robust custom things relatively quickly with within the

1119
01:17:31,160 --> 01:17:32,920
spacey framework.

1120
01:17:32,920 --> 01:17:36,720
All of that's going to lay the groundwork so that in part three, we can start applying

1121
01:17:36,720 --> 01:17:40,760
all these skills and start solving some real world problems.

1122
01:17:40,760 --> 01:17:43,680
In this case, we're going to look at financial analysis.

1123
01:17:43,680 --> 01:17:48,440
So that's going to be where we move to next is part two.

1124
01:17:48,440 --> 01:17:53,280
We are now moving into part two of this Jupiter book on spacey and we're going to be working

1125
01:17:53,280 --> 01:17:55,520
with rules based spacey.

1126
01:17:55,520 --> 01:17:58,440
Now this is really kind of the bread and butter of this video.

1127
01:17:58,440 --> 01:18:02,800
You've gotten a sense of the umbrella structure of spacey as a framework.

1128
01:18:02,800 --> 01:18:05,840
You've gotten a sense of what the container can contain.

1129
01:18:05,840 --> 01:18:10,920
You've gotten a sense of the token attributes and the linguistic annotations from part one

1130
01:18:10,920 --> 01:18:13,680
of this book and the earlier part of this video.

1131
01:18:13,680 --> 01:18:18,880
Now we're going to move into taking those skills and really developing them into custom

1132
01:18:18,880 --> 01:18:23,520
components and modified pipes that exist within spacey.

1133
01:18:23,520 --> 01:18:27,960
In other words, I'm going to show you how to take what we've learned now and start really

1134
01:18:27,960 --> 01:18:32,720
doing more robust and sophisticated things with that knowledge.

1135
01:18:32,720 --> 01:18:36,320
So we're going to be working first with the entity ruler, then with the matcher in the

1136
01:18:36,320 --> 01:18:39,080
next chapter, then in the components in spacey.

1137
01:18:39,080 --> 01:18:43,160
So a custom component is a custom function that you can put into a pipeline.

1138
01:18:43,160 --> 01:18:46,280
Then we're going to talk about regex or regular expressions.

1139
01:18:46,280 --> 01:18:48,880
And then we're going to talk about some advanced regex with spacey.

1140
01:18:48,880 --> 01:18:53,720
If you don't know what regex is, I'm going to cover this in chapter eight.

1141
01:18:53,720 --> 01:18:59,000
So let's go over to our Jupiter notebook that we're going to be using for our entity ruler

1142
01:18:59,000 --> 01:19:00,000
lesson.

1143
01:19:00,000 --> 01:19:02,680
So let's go ahead and execute some of these cells.

1144
01:19:02,680 --> 01:19:05,600
And then I'm going to be talking about it in just a second.

1145
01:19:05,600 --> 01:19:10,960
First I want to take some time to explain what the entity ruler is as a pipe in spacey,

1146
01:19:10,960 --> 01:19:15,200
what it's used for, why you'd find it useful and when to actually implement it.

1147
01:19:15,200 --> 01:19:19,840
So there are two different ways in which you can kind of add in custom features to a spacey

1148
01:19:19,840 --> 01:19:21,320
language pipeline.

1149
01:19:21,320 --> 01:19:25,600
There is a rules based approach and a machine learning based approach.

1150
01:19:25,600 --> 01:19:29,320
Rules based approaches should be used when you can think about how to generate a set

1151
01:19:29,320 --> 01:19:35,680
of rules based on either a list of known things or a set of rules that can be generated through

1152
01:19:35,680 --> 01:19:39,680
regex, code or linguistic features.

1153
01:19:39,680 --> 01:19:44,160
Machine learning is when you don't know how to actually write out the rules or the rules

1154
01:19:44,160 --> 01:19:47,800
that you would need to write out would be exceptionally complicated.

1155
01:19:47,800 --> 01:19:51,920
A great example of a rules based approach versus a machine learning based approach and when

1156
01:19:51,920 --> 01:19:57,080
to use them is with entity types for named entity recognition.

1157
01:19:57,080 --> 01:20:01,760
Imagine if you wanted to extract dates from a text.

1158
01:20:01,760 --> 01:20:06,200
There are a finite, very finite number of ways that a date can appear in a text.

1159
01:20:06,200 --> 01:20:11,960
You could have something like January 1, 2005, you could have one January 2005, you could

1160
01:20:11,960 --> 01:20:18,840
have one Jan 2005, you could have one slash five slash 2005, there's there's different

1161
01:20:18,840 --> 01:20:21,320
ways that you can do this and there's a lot of them.

1162
01:20:21,320 --> 01:20:25,400
But there really is a finite number that you could easily write a regex expression for

1163
01:20:25,400 --> 01:20:29,600
a regular expression for to capture all of those.

1164
01:20:29,600 --> 01:20:33,240
And in fact, those regex expressions already exist.

1165
01:20:33,240 --> 01:20:37,360
That's why spacey is already really good at identifying dates.

1166
01:20:37,360 --> 01:20:43,040
So dates are something that you would probably use a rules based approach for something that's

1167
01:20:43,040 --> 01:20:47,880
a good machine learning approach for or something like names.

1168
01:20:47,880 --> 01:20:53,880
If you wanted to capture the names of people, you would have to generate an entity ruler

1169
01:20:53,880 --> 01:20:57,680
with a whole bunch of robust features.

1170
01:20:57,680 --> 01:21:02,840
So you would have to have a list of all known possible first names, all known possible last

1171
01:21:02,920 --> 01:21:09,920
names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc.

1172
01:21:10,320 --> 01:21:12,600
And you'd have to have a list of all known suffixes.

1173
01:21:12,600 --> 01:21:17,320
So junior, senior, the third, the fourth, etc. on down the list.

1174
01:21:17,320 --> 01:21:22,520
This would be very, very difficult to write because first of all, the quantity of names

1175
01:21:22,520 --> 01:21:25,440
that exist in the world are massive.

1176
01:21:25,440 --> 01:21:29,440
The quantity of last names that exist in the world is massive.

1177
01:21:29,440 --> 01:21:34,040
There's not a set gazetteer or set list out there of these anywhere.

1178
01:21:34,040 --> 01:21:39,040
So for this reason, oftentimes things like people names will be worked into machine learning

1179
01:21:39,040 --> 01:21:40,040
components.

1180
01:21:40,040 --> 01:21:44,040
I'm going to address machine learning in another video at a later date, but right now we're

1181
01:21:44,040 --> 01:21:47,320
going to focus on a rules based approach.

1182
01:21:47,320 --> 01:21:54,320
So using the rules based features that spacey offers, a good NLP practitioner will be excellent

1183
01:21:55,000 --> 01:22:00,120
at both rules based approaches and machine learning based approaches and knowing when

1184
01:22:00,120 --> 01:22:06,920
to use which approach and when maybe maybe a task is not appropriate for machine learning

1185
01:22:06,920 --> 01:22:10,800
when it can be worked in with rules relatively well.

1186
01:22:10,800 --> 01:22:15,560
If you're taking a rules based approach, the approach that you take should have a high

1187
01:22:15,560 --> 01:22:20,640
degree of confidence that the rules will always return true positives.

1188
01:22:20,640 --> 01:22:22,400
And you need to think about that.

1189
01:22:22,400 --> 01:22:27,640
If you are okay with your rules, maybe catching a few false positives or missing a few true

1190
01:22:27,640 --> 01:22:32,800
positives, then maybe think about how you write the rules and allowing for those and

1191
01:22:32,800 --> 01:22:35,040
making it known in your documentation.

1192
01:22:35,040 --> 01:22:39,760
So that's generally what a rules based approach is in an entity ruler is a way that we can

1193
01:22:39,760 --> 01:22:46,760
use a list or a series of features, language features to add tokens into the entity, the

1194
01:22:47,320 --> 01:22:51,060
dot ints container within the dot container.

1195
01:22:51,060 --> 01:22:53,940
So let's go ahead and try to do this right now.

1196
01:22:53,940 --> 01:22:57,180
The text we're going to be working with is a kind of fun one, I think.

1197
01:22:57,180 --> 01:23:01,820
So if you've already gotten the reference, congratulations, it's kind of obscure.

1198
01:23:01,820 --> 01:23:04,540
But we're going to have a sentence right here that I just wrote out.

1199
01:23:04,540 --> 01:23:08,180
West Chesterton Fieldville was referenced in Mr. Deeds.

1200
01:23:08,180 --> 01:23:11,180
So in this context, we are going to have a few different entities.

1201
01:23:11,180 --> 01:23:16,980
We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.

1202
01:23:16,980 --> 01:23:19,020
It's a fake place that doesn't really exist.

1203
01:23:19,020 --> 01:23:21,380
It was made up in the movie Mr. Deeds.

1204
01:23:21,380 --> 01:23:25,180
And what we want is for Mr. Deeds to be grabbed as an entity as well.

1205
01:23:25,180 --> 01:23:28,100
And this would ideally be labeled as a film.

1206
01:23:28,100 --> 01:23:30,780
But in this case, that's probably not going to happen.

1207
01:23:30,780 --> 01:23:32,740
Let's go ahead and see what does happen.

1208
01:23:32,740 --> 01:23:39,580
So we're going to say for int and doc dot ints, print off int dot text, int dot label,

1209
01:23:39,580 --> 01:23:44,020
like we learned from our NER lesson a few moments ago.

1210
01:23:44,020 --> 01:23:46,100
And we see that the output looks like this.

1211
01:23:46,100 --> 01:23:48,700
It's gotten almost all the entities that we wanted.

1212
01:23:48,700 --> 01:23:50,940
Mr. was left off of Deeds.

1213
01:23:50,940 --> 01:23:54,620
And it's grabbed the West Chesterton Fieldville and labeled it as a person.

1214
01:23:54,620 --> 01:23:56,020
So what's gone wrong here?

1215
01:23:56,020 --> 01:23:58,180
Well, there's a few different things that have gone wrong.

1216
01:23:58,180 --> 01:24:02,980
The NCORE Web SM model is a machine learning model for NER.

1217
01:24:02,980 --> 01:24:05,140
The word vectors are not saved.

1218
01:24:05,140 --> 01:24:07,860
So the static vectors are not in it.

1219
01:24:07,860 --> 01:24:10,020
So it's making the best prediction that it can.

1220
01:24:10,020 --> 01:24:14,780
But even with a very robust machine learning model, unless it has seen West Chesterton

1221
01:24:14,780 --> 01:24:21,180
Fieldville, there is not really a good way for the model to actually know that that's

1222
01:24:21,180 --> 01:24:22,700
a place.

1223
01:24:22,700 --> 01:24:28,740
Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer

1224
01:24:28,740 --> 01:24:31,500
model might actually get this right.

1225
01:24:31,500 --> 01:24:33,540
But for the most part, this is a very challenging thing.

1226
01:24:33,540 --> 01:24:34,900
This would be challenging for a human.

1227
01:24:34,900 --> 01:24:39,620
There's not a lot of context here to tell you what this kind of entity is, unless you

1228
01:24:39,620 --> 01:24:48,540
knew a lot about how maybe northeastern villages and towns in North America would be called.

1229
01:24:48,540 --> 01:24:54,020
Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.

1230
01:24:54,020 --> 01:24:59,420
Now ideally, we would have an NER model that would label West Chesterton Fieldville as

1231
01:24:59,420 --> 01:25:01,940
a GPE and Mr. Deeds as a film.

1232
01:25:01,940 --> 01:25:03,660
But we've got two problems.

1233
01:25:03,660 --> 01:25:08,460
One, the machine learning model doesn't have film as an entity type.

1234
01:25:08,460 --> 01:25:13,980
And on top of that, West Chesterton Fieldville is not coming out correct as GPE.

1235
01:25:13,980 --> 01:25:18,820
So our goal right now is to fix both of these problems with an entity ruler.

1236
01:25:18,820 --> 01:25:24,300
This would be useful if I were maybe doing some text analysis on fictional places referenced

1237
01:25:24,300 --> 01:25:25,460
in films.

1238
01:25:25,460 --> 01:25:29,700
So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all

1239
01:25:29,700 --> 01:25:31,820
be classified as kind of fictional places.

1240
01:25:31,820 --> 01:25:36,660
So let's go ahead and make a ruler to correct this problem.

1241
01:25:36,660 --> 01:25:42,100
So what we're going to do is first we're going to make a ruler by saying ruler is equal

1242
01:25:42,100 --> 01:25:46,920
to NLP dot add pipe.

1243
01:25:46,920 --> 01:25:50,620
And this is going to take one argument here, you're going to find out when we start working

1244
01:25:50,620 --> 01:25:54,300
with custom components that you can have a few different arguments here, especially

1245
01:25:54,300 --> 01:25:56,300
if you create your own custom components.

1246
01:25:56,300 --> 01:26:00,300
But for right now, we're working with the components that come standard with spacey.

1247
01:26:00,300 --> 01:26:01,860
There's about 18 of them.

1248
01:26:01,860 --> 01:26:07,220
One of them is the entity underscore ruler, all lowercase.

1249
01:26:07,220 --> 01:26:10,980
We're going to add that ruler into our NLP model.

1250
01:26:10,980 --> 01:26:17,980
And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER

1251
01:26:17,980 --> 01:26:25,780
model and see as we go down that the NER pipe is here and the entity ruler is now the exit,

1252
01:26:25,780 --> 01:26:27,740
the final pipe in our pipeline.

1253
01:26:27,740 --> 01:26:30,660
So we see that it has been successfully added.

1254
01:26:30,660 --> 01:26:35,540
Let's go ahead now and try to add patterns into that pipeline.

1255
01:26:35,540 --> 01:26:39,780
Patterns are the things that the spacey model is going to look for in the label that it's

1256
01:26:39,780 --> 01:26:43,300
going to assign when it finds something that meets that pattern.

1257
01:26:43,300 --> 01:26:46,260
This will always be a list of lists.

1258
01:26:46,260 --> 01:26:48,140
So let's go ahead and do this right now.

1259
01:26:48,140 --> 01:26:50,100
Sorry, a list of dictionaries.

1260
01:26:50,100 --> 01:26:57,380
So the first pattern that we're really looking for here is going to be a dictionary.

1261
01:26:57,380 --> 01:27:04,340
It's going to have one key of label, which is going to be equal to GPE and another label

1262
01:27:04,340 --> 01:27:10,060
of pattern, which is going to be equal to, in this case, we want to find West Chesterton

1263
01:27:10,060 --> 01:27:11,060
Fieldville.

1264
01:27:11,060 --> 01:27:19,020
Let me go ahead and just copy and paste it so I don't make a mistake here.

1265
01:27:19,020 --> 01:27:23,620
And what we want to do is we want our entity ruler to see West Chesterton Fieldville.

1266
01:27:23,620 --> 01:27:26,380
And when it sees it, assign the label of GPE.

1267
01:27:26,380 --> 01:27:27,940
So it's a geopolitical entity.

1268
01:27:27,940 --> 01:27:29,740
So it's a place.

1269
01:27:29,740 --> 01:27:31,340
So let's go ahead and execute that.

1270
01:27:31,340 --> 01:27:32,340
Great.

1271
01:27:32,340 --> 01:27:33,340
We've got the patterns.

1272
01:27:33,340 --> 01:27:35,740
Now comes time to load them into the ruler.

1273
01:27:35,740 --> 01:27:40,020
So we can say ruler.add underscore patterns.

1274
01:27:40,020 --> 01:27:41,340
This is going to take one argument.

1275
01:27:41,340 --> 01:27:46,380
It's going to be our list of patterns added in.

1276
01:27:46,380 --> 01:27:47,540
Cool.

1277
01:27:47,540 --> 01:27:49,020
Now let's create a new doc object.

1278
01:27:49,020 --> 01:27:52,540
We're going to call this doc to that's going to be equal to NLP.

1279
01:27:52,540 --> 01:27:55,700
We're going to pass in that same text.

1280
01:27:55,700 --> 01:28:03,780
We're going to say for int n doc to dot ints print off int dot text and end dot label.

1281
01:28:03,780 --> 01:28:08,940
And you're going to notice that nothing has changed.

1282
01:28:08,940 --> 01:28:10,700
So why has nothing changed?

1283
01:28:10,700 --> 01:28:12,660
We're still getting the same results.

1284
01:28:12,660 --> 01:28:14,780
And we've added the correct pattern in.

1285
01:28:14,780 --> 01:28:17,460
The answer lies into one key thing.

1286
01:28:17,460 --> 01:28:23,340
If we look back up here, we see that our entity ruler comes after our NER.

1287
01:28:23,340 --> 01:28:24,340
What does that mean?

1288
01:28:24,340 --> 01:28:28,060
Well, imagine how the pipeline works that I talked about a little while ago in this

1289
01:28:28,060 --> 01:28:29,060
video.

1290
01:28:29,060 --> 01:28:34,420
A pipeline works by different components, adding things to an object and making changes

1291
01:28:34,420 --> 01:28:41,460
to it, in this case, adding ints to it, and then making those things isolated from later

1292
01:28:41,460 --> 01:28:44,940
pipes from being able to overwrite them unless specified.

1293
01:28:44,940 --> 01:28:49,860
What this means is that when West Chesterton field bill goes through and is identified

1294
01:28:49,860 --> 01:28:57,020
by the NER pipe as a person, it can no longer be identified as anything else.

1295
01:28:57,020 --> 01:29:01,520
What this means is that you need to do one of two things give your ruler the ability

1296
01:29:01,520 --> 01:29:09,160
to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline.

1297
01:29:09,160 --> 01:29:13,220
So let's go through and solve this common problem right now.

1298
01:29:13,220 --> 01:29:18,460
We're going to create a new NLP object called NLP to, which is going to be equal to spacey

1299
01:29:18,460 --> 01:29:19,460
dot load.

1300
01:29:19,460 --> 01:29:26,220
And again, we're going to load in the English core web SM's model and core web SM.

1301
01:29:26,220 --> 01:29:28,060
Great.

1302
01:29:28,060 --> 01:29:41,540
And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going

1303
01:29:41,540 --> 01:29:45,420
to make that an object too.

1304
01:29:45,420 --> 01:29:49,980
Now what we can do is we can say ruler dot add patterns, again, we're going to go through

1305
01:29:49,980 --> 01:29:53,060
all of these steps that we just went through, we're going to add in those patterns that

1306
01:29:53,060 --> 01:29:54,940
we created up above.

1307
01:29:54,940 --> 01:29:58,940
And now what we're going to do is we're going to actually do one thing a little different

1308
01:29:58,940 --> 01:30:00,380
than what we did.

1309
01:30:00,380 --> 01:30:04,440
What we're going to do is we're going to load this up again, and we're going to do an extra

1310
01:30:04,440 --> 01:30:05,820
keyword argument.

1311
01:30:05,820 --> 01:30:11,620
Now we can say either after or before here, we're going to say before NER, what this is

1312
01:30:11,620 --> 01:30:18,540
going to do is it's going to place our NER before our entity will ever for the NER component.

1313
01:30:18,540 --> 01:30:24,820
And now when we add our patterns in, we can now create a new doc object.

1314
01:30:24,820 --> 01:30:32,300
Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print

1315
01:30:32,300 --> 01:30:37,340
off int dot text, and dot label.

1316
01:30:37,460 --> 01:30:41,780
Now we notice that it is correctly labeled as a GPE.

1317
01:30:41,780 --> 01:30:42,780
Why is this?

1318
01:30:42,780 --> 01:30:50,540
Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will

1319
01:30:50,540 --> 01:30:55,220
notice that our entity ruler now in the pipeline sits before the NER model.

1320
01:30:55,220 --> 01:31:00,100
In other words, we've given primacy to our custom entity ruler, so that it's going to

1321
01:31:00,100 --> 01:31:04,580
have the first shot at actually correctly identifying these things, but we've got another

1322
01:31:04,580 --> 01:31:06,060
problem here.

1323
01:31:06,060 --> 01:31:14,260
This is coming out as a person, it should be Mr. Deeds as the entire collective multi

1324
01:31:14,260 --> 01:31:16,660
word token, and that should be a new entity.

1325
01:31:16,660 --> 01:31:21,660
We can use the entity ruler to add in custom types of labels here.

1326
01:31:21,660 --> 01:31:24,900
So let's go ahead and do this same thing.

1327
01:31:24,900 --> 01:31:31,220
Let's go ahead and just copy and paste our patterns, and we're going to create one more

1328
01:31:31,380 --> 01:31:39,300
NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM.

1329
01:31:39,300 --> 01:31:42,020
Great, we've got that loaded up.

1330
01:31:42,020 --> 01:31:48,260
We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to

1331
01:31:48,260 --> 01:31:54,860
NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it

1332
01:31:54,860 --> 01:32:01,620
before the NER pipe, NLP three, there we go.

1333
01:32:01,620 --> 01:32:04,820
And what we need to do now is we need to copy in these patterns, and we're going to add

1334
01:32:04,820 --> 01:32:06,620
in one more pattern.

1335
01:32:06,620 --> 01:32:08,580
Remember this can be a list here.

1336
01:32:08,580 --> 01:32:15,020
So this pattern, we're going to have a new label called film, and we're going to look

1337
01:32:15,020 --> 01:32:20,180
for the sequence Mr. Deeds.

1338
01:32:20,180 --> 01:32:23,940
And that's going to be our pattern that we want to add in to our ruler.

1339
01:32:24,020 --> 01:32:30,060
So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember

1340
01:32:30,060 --> 01:32:34,980
that one keyword argument, or one argument is going to be the list itself.

1341
01:32:34,980 --> 01:32:39,380
And now we can create a new doc object, which is going to be equal to NLP three, I think

1342
01:32:39,380 --> 01:32:48,260
I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text

1343
01:32:48,260 --> 01:32:51,060
and and dot label.

1344
01:32:51,060 --> 01:32:55,860
And if we execute this, we see now that not only have you gotten the entity ruler to correctly

1345
01:32:55,860 --> 01:33:02,940
identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly,

1346
01:33:02,940 --> 01:33:04,980
Mr. Deeds as a film.

1347
01:33:04,980 --> 01:33:08,580
Now some of you might be realizing the problem here, this is actually a problem for machine

1348
01:33:08,580 --> 01:33:09,580
learning models.

1349
01:33:09,580 --> 01:33:14,180
And the reason for this is because Mr. Deeds in some instances could be the person and

1350
01:33:14,180 --> 01:33:17,980
Mr. Deeds in other instances could be the movie itself.

1351
01:33:17,980 --> 01:33:20,220
This is what we would call a toponym.

1352
01:33:20,220 --> 01:33:24,020
So spelled like this, and this is a common problem in natural language processing.

1353
01:33:24,020 --> 01:33:28,020
And it's actually one of the few problems or one of many problems really, that remain

1354
01:33:28,020 --> 01:33:35,420
a little bit unsolved toponym resolution, spelled like this, or TR is the resolution

1355
01:33:35,420 --> 01:33:36,420
of toponym.

1356
01:33:36,420 --> 01:33:41,260
So things that can have multiple labels that are dependent upon context.

1357
01:33:41,260 --> 01:33:46,860
Another example of toponym resolution is something like this, if you were to look at this word,

1358
01:33:46,860 --> 01:33:51,460
let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology.

1359
01:33:51,460 --> 01:33:53,940
Let's say it's only going to ever be a GPE.

1360
01:33:53,940 --> 01:33:59,700
The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas.

1361
01:33:59,700 --> 01:34:05,460
Toponym resolution is also the ability to resolve problems like this, when in context

1362
01:34:05,460 --> 01:34:11,540
is Paris was kind of talking about Paris, France, when in context is it talking about

1363
01:34:11,540 --> 01:34:15,580
Kentucky, and when in context is it talking about Texas.

1364
01:34:15,580 --> 01:34:19,500
So that's something that you really want to think about when you're generating your

1365
01:34:19,500 --> 01:34:25,540
rules for an entity ruler, is this ever going to be a false positive?

1366
01:34:25,540 --> 01:34:30,180
And if the answer is that it's going to be a false positive half the time, or it's a

1367
01:34:30,180 --> 01:34:36,100
50-50 shot, then really consider incorporating that kind of an entity into a machine learning

1368
01:34:36,100 --> 01:34:41,740
model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as

1369
01:34:41,740 --> 01:34:42,740
a person.

1370
01:34:42,740 --> 01:34:48,460
And learn with word embeddings when that context means it's a film and when that context means

1371
01:34:48,460 --> 01:34:49,460
it's a person.

1372
01:34:49,460 --> 01:34:52,020
That's just a little toy example.

1373
01:34:52,020 --> 01:34:55,300
What we're going to see moving forward, though, and we're going to do this with a matcher,

1374
01:34:55,300 --> 01:34:59,620
not with the entity ruler, is that spacey can do a lot of things.

1375
01:34:59,620 --> 01:35:05,060
You might be thinking to yourself, now I could easily just come up with a list and just check

1376
01:35:05,060 --> 01:35:09,820
and see whenever Mr. Deeds pops up and just inject that into the doc.ins.

1377
01:35:09,820 --> 01:35:12,180
I could do the same thing with West Chesterton Field Bill.

1378
01:35:12,180 --> 01:35:15,500
Why do I need an NLP framework to do this?

1379
01:35:15,500 --> 01:35:19,780
And the answer is going to come up in just a few minutes when we start realizing that

1380
01:35:19,780 --> 01:35:25,740
spacey can do a lot more than things like regex or things like just a basic gazetteer

1381
01:35:25,740 --> 01:35:27,820
check or a list check.

1382
01:35:27,820 --> 01:35:32,380
What you can do with spacey is you can have the pattern not just take a sequence of characters

1383
01:35:32,380 --> 01:35:38,180
and look for a match, but a sequence of linguistic features as well, that earlier pipes have

1384
01:35:38,180 --> 01:35:39,180
identified.

1385
01:35:39,180 --> 01:35:43,420
And I think it's best if we save that for just a second when we start talking about

1386
01:35:43,420 --> 01:35:49,140
the matcher, which is, in my opinion, one of the more robust things that you can do

1387
01:35:49,140 --> 01:35:54,860
with spacey and what sets spacey apart from things like regex or other fancier string

1388
01:35:54,860 --> 01:35:56,860
matching approaches.

1389
01:35:56,860 --> 01:36:03,180
Okay, we're now moving into chapter six of this book, and this is really kind of, in

1390
01:36:03,180 --> 01:36:07,300
my opinion, one of the most important areas in this entire video.

1391
01:36:07,300 --> 01:36:11,620
If you can master the techniques I'm going to show you for the next maybe 20 minutes

1392
01:36:11,620 --> 01:36:16,300
or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really

1393
01:36:16,300 --> 01:36:20,060
going to see really kind of its true power.

1394
01:36:20,060 --> 01:36:24,380
A lot of the stuff that we talk about here in the matcher can also be implemented in

1395
01:36:24,380 --> 01:36:28,500
the entity ruler as well with a pattern.

1396
01:36:28,500 --> 01:36:33,660
The key difference between the entity ruler and the matcher is in how data the data is

1397
01:36:33,660 --> 01:36:35,080
kind of extracted.

1398
01:36:35,080 --> 01:36:39,280
So the matcher is going to store information a little differently.

1399
01:36:39,280 --> 01:36:43,120
It's going to store it as within the vocab of the NLP model.

1400
01:36:43,120 --> 01:36:48,920
It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk

1401
01:36:48,920 --> 01:36:50,920
about that more in just a second.

1402
01:36:50,920 --> 01:36:53,360
And it's not going to store it in the doc ends.

1403
01:36:53,360 --> 01:36:57,360
So matchers don't put things in your doc.ends.

1404
01:36:57,360 --> 01:37:01,540
So when do you want to use a matcher over an entity ruler?

1405
01:37:01,540 --> 01:37:06,620
You want to use the entity ruler when the thing that you're trying to extract is something

1406
01:37:06,620 --> 01:37:11,340
that is important to have a label that corresponds to it within the entities that are coming

1407
01:37:11,340 --> 01:37:12,620
out.

1408
01:37:12,620 --> 01:37:17,300
So in my research, I use this for anything from like, let's say stocks, if I'm working

1409
01:37:17,300 --> 01:37:23,660
with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm

1410
01:37:23,660 --> 01:37:31,500
a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside

1411
01:37:31,620 --> 01:37:32,620
other entities.

1412
01:37:32,620 --> 01:37:38,260
I'll also work in things like ships, so the names of ships, streets, things like that.

1413
01:37:38,260 --> 01:37:43,460
When I use the the matcher, it's when I'm looking for something that is not necessarily

1414
01:37:43,460 --> 01:37:51,060
an entity type, but something that is a structure within the text that will help me extract

1415
01:37:51,060 --> 01:37:52,060
information.

1416
01:37:52,060 --> 01:37:55,500
And I think that'll make more sense as we go through and I show you kind of how to improve

1417
01:37:55,500 --> 01:38:01,060
examples going through it, we're kind of using the matcher as you would in the real world.

1418
01:38:01,100 --> 01:38:05,980
But remember, all the patterns that I show you can also be implemented in the entity

1419
01:38:05,980 --> 01:38:06,980
ruler.

1420
01:38:06,980 --> 01:38:11,140
And I'm also going to talk about when we get to chapter eight, how rejects can actually

1421
01:38:11,140 --> 01:38:14,460
be used to do similar things, but in a different way.

1422
01:38:14,460 --> 01:38:20,180
Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic

1423
01:38:20,180 --> 01:38:27,260
components, so the lemma of a word or the identifying if the word is a specific type

1424
01:38:27,340 --> 01:38:32,300
of an entity, that's when you're going to want to use the matcher over rejects.

1425
01:38:32,300 --> 01:38:36,380
And when you're going to use rejects is when you really have a complicated pattern that

1426
01:38:36,380 --> 01:38:38,540
you need to extract.

1427
01:38:38,540 --> 01:38:43,380
And that pattern is not dependent upon specific parts of speech, you're going to see with

1428
01:38:43,380 --> 01:38:47,820
that how that works as we kind of go through the rest of part two, but keep that in the

1429
01:38:47,820 --> 01:38:49,220
back of your mind.

1430
01:38:49,220 --> 01:38:54,100
So let's go ahead and take our work over to our blank Jupiter notebook again.

1431
01:38:54,100 --> 01:38:56,780
So what we're going to do is we're going to just set up with a basic example.

1432
01:38:56,780 --> 01:38:58,780
We need to import spacey.

1433
01:38:58,780 --> 01:39:04,780
And since we're working with the matcher, we also need to say from spacey dot matcher,

1434
01:39:04,780 --> 01:39:10,020
import matcher with a capital M, very important capital M.

1435
01:39:10,020 --> 01:39:13,900
Once we have this loaded up, we can start actually working with the matcher.

1436
01:39:13,900 --> 01:39:20,260
And we're going to be putting the matcher in a just the small English model.

1437
01:39:20,260 --> 01:39:24,100
And we're going to say NLP is equal to spacey dot load.

1438
01:39:24,100 --> 01:39:29,580
And you should be getting familiar with this in core web SM, the small English model.

1439
01:39:29,580 --> 01:39:34,780
Once we've got that loaded, and we do now, we can start actually working with the matcher.

1440
01:39:34,780 --> 01:39:36,380
So how do you create the matcher?

1441
01:39:36,380 --> 01:39:39,900
Well, the Pythonic way to do this and the weights in the documentation is to call the

1442
01:39:39,900 --> 01:39:44,740
object a matcher, that's going to be equal to matcher with a capital M. So we're calling

1443
01:39:44,740 --> 01:39:47,580
this class right here.

1444
01:39:47,580 --> 01:39:50,980
And now what we need to do is we need to pass in one argument.

1445
01:39:50,980 --> 01:39:54,100
This is going to be NLP dot vocab.

1446
01:39:54,100 --> 01:39:57,500
We're going to see that we can add in some extra features here in just a little bit.

1447
01:39:57,500 --> 01:40:00,740
I'm going to show you why you want to add in extra features at this stage, but we're

1448
01:40:00,740 --> 01:40:02,580
going to ignore that for right now.

1449
01:40:02,580 --> 01:40:07,220
What we're going to try to do is we're going to try to find email addresses within a text,

1450
01:40:07,220 --> 01:40:11,300
a very simple task that's really not that difficult to do.

1451
01:40:11,300 --> 01:40:15,380
We can do it with a very simple pattern because spacey has given us that ability.

1452
01:40:15,380 --> 01:40:17,820
So let's create a pattern.

1453
01:40:17,820 --> 01:40:25,980
And that's going to be equal to a list, which is going to contain a dictionary.

1454
01:40:25,980 --> 01:40:33,860
The first item in the dictionary, or the first key, is going to be the thing that you're

1455
01:40:33,860 --> 01:40:35,060
looking for.

1456
01:40:35,060 --> 01:40:38,580
So in this case, we have a bunch of different things that the matcher can look for.

1457
01:40:38,580 --> 01:40:40,540
And I'm going to be talking about all those in just a second.

1458
01:40:40,540 --> 01:40:45,540
But one of them is very handily, this label of like email.

1459
01:40:45,540 --> 01:40:51,700
So if the if the string or the sequence of tokens or the token is looking like an email,

1460
01:40:51,700 --> 01:40:58,580
and that's true, then that is what we want to extract, we want to extract everything that

1461
01:40:58,580 --> 01:40:59,900
looks like an email.

1462
01:40:59,900 --> 01:41:04,540
And to make sure that this occurs, we're going to say matcher dot add.

1463
01:41:04,540 --> 01:41:09,180
And then here, we're going to pass in two arguments, argument one is going to be the

1464
01:41:09,180 --> 01:41:13,840
think of it as a label that we want to assign to it.

1465
01:41:13,840 --> 01:41:18,480
And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll

1466
01:41:18,480 --> 01:41:20,000
see in just a second.

1467
01:41:20,000 --> 01:41:23,520
And the next thing is a pattern.

1468
01:41:23,520 --> 01:41:27,200
And it's important here to note that this is a list.

1469
01:41:27,200 --> 01:41:30,880
The argument here takes a list of lists.

1470
01:41:30,880 --> 01:41:35,400
And because this is just one list right now, I'm making it into a list.

1471
01:41:35,400 --> 01:41:41,680
So each one of these different patterns would be a list within a list, essentially the let's

1472
01:41:41,680 --> 01:41:43,920
go ahead and execute that.

1473
01:41:43,920 --> 01:41:49,160
And now we're going to say doc is equal to nlp.

1474
01:41:49,160 --> 01:41:57,320
And I'm going to add in a text that I have in the textbook.

1475
01:41:57,320 --> 01:42:01,600
And this is my email address w mattingly at aol.com.

1476
01:42:01,600 --> 01:42:02,840
That might be a real email address.

1477
01:42:02,840 --> 01:42:04,920
I don't believe it is, it's definitely not mine.

1478
01:42:04,920 --> 01:42:07,360
So don't try and email it.

1479
01:42:07,360 --> 01:42:11,200
And then we're going to say matches is equal to matcher doc.

1480
01:42:11,240 --> 01:42:13,760
This is going to be how we find our matches.

1481
01:42:13,760 --> 01:42:20,040
We pass that doc object into our matcher class.

1482
01:42:20,040 --> 01:42:23,520
And now what we have is the ability to print off our matches.

1483
01:42:23,520 --> 01:42:25,760
And what we get is a list.

1484
01:42:25,760 --> 01:42:30,400
And this list is a set of tuples that will always have three indices.

1485
01:42:30,400 --> 01:42:34,440
So index zero is going to be this very long number.

1486
01:42:34,440 --> 01:42:40,600
What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook.

1487
01:42:40,600 --> 01:42:44,880
And the next thing is the start token and the end token.

1488
01:42:44,880 --> 01:42:47,600
So you might be seeing the importance here already.

1489
01:42:47,600 --> 01:42:53,840
What we can do with this is we can actually go into the nlp vocab where this integer lies

1490
01:42:53,840 --> 01:42:56,080
and find what it corresponds to.

1491
01:42:56,080 --> 01:42:58,000
So this is where this is pretty cool.

1492
01:42:58,000 --> 01:42:59,280
Check this out.

1493
01:42:59,280 --> 01:43:02,000
So you print off nlp dot vocab.

1494
01:43:02,000 --> 01:43:04,720
So we're going into that vocab object.

1495
01:43:04,720 --> 01:43:08,600
We're going to index it matches zero.

1496
01:43:08,600 --> 01:43:13,800
So this is going to be the first index, so this tuple at this point.

1497
01:43:13,800 --> 01:43:15,960
And then we're going to grab index zero.

1498
01:43:15,960 --> 01:43:18,240
So now we've gone into this list.

1499
01:43:18,240 --> 01:43:24,480
We've gone to index zero, this first tuple, and now we're grabbing that first item there.

1500
01:43:24,480 --> 01:43:30,800
Now what we need to do is we need to say dot text, you need to do it right here.

1501
01:43:30,800 --> 01:43:37,040
And if you print this off, we get this email address, that label that we gave it up there

1502
01:43:37,040 --> 01:43:42,440
was added into the nlp vocab with this unique lexeme that allows for us to understand what

1503
01:43:42,440 --> 01:43:47,400
that number corresponds to within the nlp framework.

1504
01:43:47,400 --> 01:43:54,120
So this is a very simple example of how a matcher works and how you can use it to do

1505
01:43:54,120 --> 01:43:56,040
some pretty cool things.

1506
01:43:56,040 --> 01:44:01,120
But let's take a moment, let's pause and let's see what we can do with this matcher.

1507
01:44:01,120 --> 01:44:06,360
So if we go up into spacey's documentation on the matcher, we'll see that you got a couple

1508
01:44:06,360 --> 01:44:08,000
different attributes you can work with.

1509
01:44:08,000 --> 01:44:10,760
Now we've, we're going to be seeing this a little bit.

1510
01:44:10,760 --> 01:44:14,560
The orth, this is the exact verbatim of a token.

1511
01:44:14,560 --> 01:44:20,160
And we're also going to see text, the exact verbatim, text of a token.

1512
01:44:20,160 --> 01:44:21,920
What we also have is lower.

1513
01:44:21,920 --> 01:44:27,800
So what you can do here is you can use lower to say when the item is lowercase and it looks

1514
01:44:27,800 --> 01:44:31,040
like and then give some lowercase pattern.

1515
01:44:31,040 --> 01:44:35,720
This is going to be very useful for capturing things that might be at the start of a sentence.

1516
01:44:35,720 --> 01:44:42,440
For example, if you were to look for the penguin in the text, anywhere you saw the penguin.

1517
01:44:42,440 --> 01:44:47,760
If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the

1518
01:44:47,760 --> 01:44:49,040
start of a sentence.

1519
01:44:49,040 --> 01:44:51,560
It would miss it because the T would be capitalized.

1520
01:44:51,560 --> 01:44:55,640
By using lower, you can ensure that your pattern that you're giving it is going to be looking

1521
01:44:55,640 --> 01:45:00,360
for any pattern that matches that when the text is lowercase.

1522
01:45:00,360 --> 01:45:07,760
If is going to be the, the length of your token text is alpha is ASCII is digit.

1523
01:45:07,760 --> 01:45:11,560
This is when your characters are either going to be alphabetical ASCII characters.

1524
01:45:11,560 --> 01:45:16,640
So the American standard coding initiative, I can't remember what it stands for, but it's

1525
01:45:16,640 --> 01:45:22,920
that, I think it's 128 bit thing that America came up with when they started in coding text.

1526
01:45:22,920 --> 01:45:27,200
It's now replaced with UTF eight and is digit is going to look for something if it is a

1527
01:45:27,200 --> 01:45:28,200
digit.

1528
01:45:28,200 --> 01:45:29,400
So think of each of these as a token.

1529
01:45:29,400 --> 01:45:34,800
So if the token is a digit, then that counts in the pattern is lower is upper is title.

1530
01:45:34,800 --> 01:45:36,360
These should be all self explanatory.

1531
01:45:36,360 --> 01:45:40,800
If it's lowercase, if it's uppercase, if it's a title, so capitalized.

1532
01:45:40,800 --> 01:45:44,000
And if you don't understand what all of these do right now, I'm going to be going through

1533
01:45:44,000 --> 01:45:47,640
and showing you in just a second, just giving you an overview of different things that can

1534
01:45:47,640 --> 01:45:52,640
be included within the, the matcher or the entity ruler here.

1535
01:45:52,640 --> 01:45:59,240
So what we can also do is find something that if the token is actually the start of a sentence,

1536
01:45:59,240 --> 01:46:03,400
if it's like a number, like a URL, like an email, you can extract it.

1537
01:46:03,400 --> 01:46:07,000
And here is the main part I want to talk about because this is where you're really going

1538
01:46:07,000 --> 01:46:12,520
to find spacey out shines any other string matching system out there.

1539
01:46:12,520 --> 01:46:17,800
So what you can do is you can use the tokens, part of speech tag, morphological analysis,

1540
01:46:17,800 --> 01:46:22,360
dependency label, lima and shape to actually make matches.

1541
01:46:22,360 --> 01:46:27,380
So not just matching a sequence of characters, but matching a sequence of linguistic features.

1542
01:46:27,380 --> 01:46:28,660
So think about this.

1543
01:46:28,660 --> 01:46:33,900
If you wanted to capture all instances of a proper noun followed by a verb, you would

1544
01:46:33,900 --> 01:46:36,620
not be able to do that with regex.

1545
01:46:36,620 --> 01:46:37,720
There's not a way to do it.

1546
01:46:37,720 --> 01:46:40,100
You can't give regex if this is a verb.

1547
01:46:40,100 --> 01:46:42,320
Regex is just a string matching framework.

1548
01:46:42,320 --> 01:46:46,500
It's not a framework for actually identifying linguistic features, using them and extracting

1549
01:46:46,500 --> 01:46:47,500
them.

1550
01:46:47,500 --> 01:46:52,220
So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological

1551
01:46:52,220 --> 01:46:56,900
analysis, the depth, the lemma, et cetera.

1552
01:46:57,540 --> 01:47:01,940
We can actually use all those things that have gone through the pipeline and the matcher

1553
01:47:01,940 --> 01:47:07,460
can leverage those linguistic features and make some really cool, allow us to make really

1554
01:47:07,460 --> 01:47:11,380
cool patterns that can match really robust and complicated things.

1555
01:47:11,380 --> 01:47:14,700
And the final thing I'm going to talk about is right here, the OP.

1556
01:47:14,700 --> 01:47:19,340
This is the operator or quantifier and determines how often to match a token.

1557
01:47:19,340 --> 01:47:20,980
So there's a few different things you can use here.

1558
01:47:20,980 --> 01:47:26,020
There's the exclamation mark, negate the pattern, requiring it to match zero times.

1559
01:47:26,060 --> 01:47:29,260
So in this scenario, the sequence would never occur.

1560
01:47:29,260 --> 01:47:33,980
There's the question mark, make the pattern optional, allowing it to match zero or one

1561
01:47:33,980 --> 01:47:39,580
times require the pattern to match one or more times with the plus and the asterisk,

1562
01:47:39,580 --> 01:47:43,500
the thing on the shift eight, allow the pattern to match zero or more times.

1563
01:47:43,500 --> 01:47:47,620
There's other things as well that you can do to make this match or a bit more robust.

1564
01:47:47,620 --> 01:47:51,700
But for right now, let's jump into the basics and see how we can really kind of take these

1565
01:47:51,700 --> 01:47:55,580
and apply them in a real world question.

1566
01:47:55,580 --> 01:48:00,580
So what I'm going to do is I'm going to work with another data set or another piece of

1567
01:48:00,580 --> 01:48:02,980
data that I've grabbed off of Wikipedia.

1568
01:48:02,980 --> 01:48:07,100
And this is the Wikipedia article entry on Martin Luther King, Jr.

1569
01:48:07,100 --> 01:48:12,820
It's the opening opening few paragraphs, let's print it off and just take a quick look.

1570
01:48:12,820 --> 01:48:13,820
And this is what it looks like.

1571
01:48:13,820 --> 01:48:15,020
You can go through and read it.

1572
01:48:15,020 --> 01:48:17,820
We're not too concerned about what it says right now.

1573
01:48:17,820 --> 01:48:21,660
We're concerned about trying to extract a very specific set of patterns.

1574
01:48:21,660 --> 01:48:25,060
What we're interested in grabbing are all proper nouns.

1575
01:48:25,060 --> 01:48:26,540
That's the task ahead of us.

1576
01:48:26,540 --> 01:48:32,060
Somebody has asked us to take this text in, extract all the proper nouns for me, but we're

1577
01:48:32,060 --> 01:48:36,740
going to do a lot more and not just the proper nouns, but we want to get multi word tokens.

1578
01:48:36,740 --> 01:48:43,300
So we want to have Martin Luther King, Jr. extracted as one token, so one export.

1579
01:48:43,300 --> 01:48:49,020
So the other things that we want to have are these kind of structured in sequential order.

1580
01:48:49,020 --> 01:48:54,260
So find out where they appear and extract them based on their start token.

1581
01:48:54,260 --> 01:48:57,660
So let's go ahead and start trying to do some of these things right now.

1582
01:48:57,660 --> 01:48:59,180
Let's scroll down here.

1583
01:48:59,180 --> 01:49:00,220
Great.

1584
01:49:00,220 --> 01:49:04,420
So we need to create really a new NLP object now at this point.

1585
01:49:04,420 --> 01:49:05,420
So let's create a new one.

1586
01:49:05,420 --> 01:49:10,760
We're going to start working with the Ncore Web SM model.

1587
01:49:10,760 --> 01:49:15,300
If you're working with a different model, like the large or the transformer, you're

1588
01:49:15,300 --> 01:49:17,420
going to have more accurate results.

1589
01:49:17,420 --> 01:49:21,460
But for right now, we're just trying to do this quickly for demonstration purposes.

1590
01:49:21,460 --> 01:49:26,780
So again, just like before, we're creating that with NLP dot vocab.

1591
01:49:26,780 --> 01:49:29,060
And then we're going to create a pattern.

1592
01:49:29,060 --> 01:49:31,380
So this is the pattern that we're going to work with.

1593
01:49:31,380 --> 01:49:38,980
We want to find any occurrence of a POS part of speech that corresponds to proper noun.

1594
01:49:38,980 --> 01:49:45,220
That's the way in which POS labels proper nouns is prop in.

1595
01:49:45,220 --> 01:49:48,340
And we should be able to with that extract all proper nouns.

1596
01:49:48,340 --> 01:49:54,540
So we can say matcher dot add, and we're going to say proper noun.

1597
01:49:54,540 --> 01:49:58,060
And that's going to be our pattern.

1598
01:49:58,060 --> 01:50:00,820
And then what we can do just like before, we're going to create the doc object.

1599
01:50:00,820 --> 01:50:04,220
This is going to be NLP text.

1600
01:50:04,220 --> 01:50:08,300
And then we're going to say matches is equal to matcher doc.

1601
01:50:08,300 --> 01:50:14,420
So we're going to create the matches by passing that doc object into our matcher class.

1602
01:50:14,420 --> 01:50:17,860
And then we're going to print off the length of the matches.

1603
01:50:17,860 --> 01:50:22,540
So how many matches were found, and then we're going to say for match in matches.

1604
01:50:22,540 --> 01:50:25,420
And we're just going to grab the first 10 because I've done this and there's a lot

1605
01:50:25,420 --> 01:50:30,940
and you'll see why let's print off.

1606
01:50:30,940 --> 01:50:33,380
Let's print off in this case, match.

1607
01:50:33,380 --> 01:50:36,260
And then we're going to print off specifically what that text is.

1608
01:50:36,260 --> 01:50:41,900
Remember, the output is the lexine followed by the start token and the end token, which

1609
01:50:41,900 --> 01:50:44,420
means we can go into the doc object.

1610
01:50:44,420 --> 01:50:46,260
And we can set up something like this.

1611
01:50:46,260 --> 01:50:53,820
We can say match one, so index one, which is the start token and match two, which is

1612
01:50:53,820 --> 01:50:55,020
the end token.

1613
01:50:55,020 --> 01:50:58,620
And that'll allow us to actually index what these words are.

1614
01:50:58,620 --> 01:51:00,900
And when we do this, we can see all these printed out.

1615
01:51:00,900 --> 01:51:07,140
So this is the match, the lexine here, which is going to be proper down all the way down.

1616
01:51:07,140 --> 01:51:13,140
We've got the zero here, which corresponds to the start token, the end token.

1617
01:51:13,140 --> 01:51:15,060
And this is the the token that we extracted.

1618
01:51:15,060 --> 01:51:21,020
Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right?

1619
01:51:21,020 --> 01:51:23,780
So the problem should be pretty obvious right now.

1620
01:51:23,780 --> 01:51:31,900
And the problem is that we have grabbed all proper nouns, but these proper nouns are just

1621
01:51:31,900 --> 01:51:33,540
individual tokens.

1622
01:51:33,540 --> 01:51:36,220
We haven't grabbed the multi word tokens.

1623
01:51:36,220 --> 01:51:37,780
So how do we go about doing that?

1624
01:51:37,780 --> 01:51:42,180
Well, we can solve this problem by let's go ahead and just copy and paste all this from

1625
01:51:42,180 --> 01:51:43,180
here.

1626
01:51:43,180 --> 01:51:47,540
And we're going to make one small adjustment here.

1627
01:51:47,540 --> 01:51:53,940
We're going to change this to OP with a plus.

1628
01:51:53,940 --> 01:51:55,180
So what does that mean?

1629
01:51:55,180 --> 01:51:59,740
Well, let's pop back into our matcher under spacey and check it out.

1630
01:51:59,740 --> 01:52:05,800
So OP members, the operator or quantifier, we're going to use the plus symbol.

1631
01:52:05,800 --> 01:52:12,560
So it's going to look for a proper noun that occurs one or more times.

1632
01:52:12,560 --> 01:52:16,560
So in theory, right, this should allow us to grab multi word tokens.

1633
01:52:16,560 --> 01:52:19,600
It's going to look for a proper noun and grab as many as there are.

1634
01:52:19,600 --> 01:52:24,440
So anything that occurs one or more times, if we run this, though, we see a problem.

1635
01:52:24,440 --> 01:52:28,560
We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther

1636
01:52:28,560 --> 01:52:33,000
King, Luther King, King Martin Luther King, Junior, what what is going on here?

1637
01:52:33,000 --> 01:52:35,560
Well, you might already have figured it out.

1638
01:52:35,560 --> 01:52:38,320
It has done exactly what we told it to do.

1639
01:52:38,320 --> 01:52:42,800
It's grabbed all sequence of tokens that were proper nouns that occurred one or more

1640
01:52:42,800 --> 01:52:44,440
times.

1641
01:52:44,440 --> 01:52:46,280
Just so happens some of these overlap.

1642
01:52:46,280 --> 01:52:50,960
So token that's doc zero to one, zero to two.

1643
01:52:50,960 --> 01:52:56,360
So you can see the problem here is it's grabbing all of these and any combination of them.

1644
01:52:56,360 --> 01:53:00,400
What we can do, though, is we can add an extra layer to this.

1645
01:53:00,400 --> 01:53:04,560
So let's again, copy what we've just done because it was, it was almost there.

1646
01:53:04,560 --> 01:53:06,720
It was good, but it wasn't great.

1647
01:53:06,720 --> 01:53:11,080
We're going to do one new thing here when we add in the patterns, we're going to pass

1648
01:53:11,080 --> 01:53:17,480
in the keyword argument, greedy, we're going to say longest capital, all capital letters

1649
01:53:17,480 --> 01:53:18,480
here.

1650
01:53:18,480 --> 01:53:22,560
And if we execute that, it's going to look for the longest token out of that mix, and

1651
01:53:22,560 --> 01:53:26,720
it's going to give that one, make that one the only token that it extracts.

1652
01:53:26,720 --> 01:53:33,560
We noticed that our length has changed from what was it up here, 175 to 61.

1653
01:53:33,560 --> 01:53:34,880
So this is much better.

1654
01:53:34,880 --> 01:53:39,720
However, we should have recognized right now, another problem.

1655
01:53:39,720 --> 01:53:40,960
What have we done wrong?

1656
01:53:40,960 --> 01:53:45,240
Well, what we've done wrong is these are all out of order.

1657
01:53:45,240 --> 01:53:48,880
In fact, what happens is when you do this, I don't have evidence to support this, but

1658
01:53:48,880 --> 01:53:51,360
I believe it's right.

1659
01:53:51,360 --> 01:53:56,200
What will always happen is the, the greedy longest will result in all of your tokens

1660
01:53:56,200 --> 01:54:01,360
being organized or all your matches being organized from longest to shortest.

1661
01:54:01,360 --> 01:54:06,320
So if we were to scroll down the list and look at maybe negative one, negative, let's

1662
01:54:06,320 --> 01:54:10,800
do negative 10 on, you'll see single word tokens.

1663
01:54:10,800 --> 01:54:14,200
And again, this is me just guessing, but I think based on what you've just seen, that's

1664
01:54:14,200 --> 01:54:16,080
a fairly good guess.

1665
01:54:16,080 --> 01:54:19,080
So let's go ahead and just kind of so we can see what the output is here.

1666
01:54:19,080 --> 01:54:22,720
So how would you go about organizing these sequentially?

1667
01:54:22,720 --> 01:54:29,840
Well, this is where really kind of a sort comes in handy when you can pass a lambda to

1668
01:54:29,840 --> 01:54:30,840
it.

1669
01:54:30,840 --> 01:54:35,240
I can copy all this again, because again, we almost had this right.

1670
01:54:35,240 --> 01:54:41,160
Here we're going to sort our matches though, we can say matches.sort, and this is going

1671
01:54:41,160 --> 01:54:46,600
to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb

1672
01:54:46,600 --> 01:54:53,600
is going to allow us to actually iterate over all this and find any instance where X occurs.

1673
01:54:53,600 --> 01:54:56,320
And we're going to say to sort by X one.

1674
01:54:56,320 --> 01:54:58,520
So what this is, it's a list of tuples.

1675
01:54:58,520 --> 01:55:02,760
And what we're using lambda for is we're going to say sort this whole list of tuples out,

1676
01:55:02,760 --> 01:55:07,520
but sort it by the first index, in other words, sort it by the start token.

1677
01:55:07,520 --> 01:55:12,520
And when we execute that, we've got everything now coming out as we would expect and nor

1678
01:55:12,520 --> 01:55:14,600
these typos that exist.

1679
01:55:14,600 --> 01:55:17,880
We've got zero to four, six to nine.

1680
01:55:17,880 --> 01:55:23,040
So we actually are extracting these things in sequential order as they appear in our

1681
01:55:23,040 --> 01:55:24,040
text.

1682
01:55:24,040 --> 01:55:30,000
You can actually go through and sort the appearance of the, of the matcher.

1683
01:55:30,000 --> 01:55:35,600
But what if our, the person who kind of gave us this job, they were happy with this, but

1684
01:55:35,600 --> 01:55:37,160
they came back and said, okay, that's cool.

1685
01:55:37,160 --> 01:55:41,440
But what we're really interested in what we really want to know is every instance where

1686
01:55:41,440 --> 01:55:48,520
a proper noun of any length, grab the multi word token still, but we want to know anytime

1687
01:55:48,520 --> 01:55:50,560
that occurs after a verb.

1688
01:55:50,560 --> 01:55:53,600
So anytime this proper noun is followed by a verb.

1689
01:55:53,600 --> 01:55:56,480
So what we can do is we can add in, okay, okay, we can do this.

1690
01:55:56,480 --> 01:55:57,800
We're going to have a comma here.

1691
01:55:57,800 --> 01:56:00,520
So the same pattern is going to be a sequence now.

1692
01:56:00,520 --> 01:56:02,760
It's not just going to be one thing.

1693
01:56:02,760 --> 01:56:08,040
We're going to say token one needs to be a proper noun and grab as many of those tokens

1694
01:56:08,040 --> 01:56:11,720
as you can zero or one to more times.

1695
01:56:11,720 --> 01:56:17,360
And then after those are done comma, this is where the next thing has to occur POS.

1696
01:56:17,360 --> 01:56:20,440
So the part of speech needs to be a verb.

1697
01:56:20,440 --> 01:56:24,000
So the next thing that comes out needs to be a verb.

1698
01:56:24,000 --> 01:56:25,680
And we want that to be the case.

1699
01:56:25,680 --> 01:56:29,840
Well, when we do this, we can kind of go through and see the results of the first instance

1700
01:56:29,840 --> 01:56:38,040
of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced

1701
01:56:38,040 --> 01:56:42,280
258 director J Edgar Hoover considered.

1702
01:56:42,280 --> 01:56:49,000
Now we're able to use those linguistic features that make Spacey amazing and actually extract

1703
01:56:49,000 --> 01:56:51,160
some vital information.

1704
01:56:51,160 --> 01:56:57,840
So we've been able to figure out where in this text a a proper noun is proceeded by

1705
01:56:57,840 --> 01:56:58,840
a verb.

1706
01:56:58,840 --> 01:57:02,480
So you can already start to probably see the implications here.

1707
01:57:02,480 --> 01:57:06,120
And we can we can create very elaborate things with this.

1708
01:57:06,120 --> 01:57:09,880
We can use any of these as long of a sequence as you can imagine.

1709
01:57:09,880 --> 01:57:13,640
We're going to work with a different text and kind of demonstrate that it's a fun toy

1710
01:57:13,640 --> 01:57:14,640
example.

1711
01:57:14,640 --> 01:57:20,240
We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file.

1712
01:57:20,240 --> 01:57:23,160
I'm going to load it in right now.

1713
01:57:23,160 --> 01:57:29,400
And then I'm going to just grab the first sentence from the first chapter.

1714
01:57:29,400 --> 01:57:32,000
And what we have here is the first sentence.

1715
01:57:32,000 --> 01:57:33,800
So here's our scenario.

1716
01:57:33,800 --> 01:57:41,320
Somebody has asked us to grab all the quotation marks and try to identify the person described

1717
01:57:41,320 --> 01:57:45,320
or the person described the person who's doing the speaking or the thinking.

1718
01:57:45,320 --> 01:57:48,560
In other words, we want to be able to grab Alice thought.

1719
01:57:48,560 --> 01:57:54,120
Now I picked Alice in Wonderland because of the complexity of the text, not complexity

1720
01:57:54,120 --> 01:57:59,680
in the sense of the language used children's book, but complexity and the syntax.

1721
01:57:59,680 --> 01:58:05,720
These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly

1722
01:58:05,720 --> 01:58:10,040
inconsistent in how we structured these kind of sequences of quotes.

1723
01:58:10,040 --> 01:58:14,680
And the other thing I chose to do as I left in one mistake here, and that is this non

1724
01:58:14,680 --> 01:58:16,680
standardized quotation mark.

1725
01:58:16,680 --> 01:58:19,820
So remember, when you need to do this, things need to match perfectly.

1726
01:58:19,820 --> 01:58:24,360
So we're going to replace this first things first is to create a cleaner text, or we do

1727
01:58:24,360 --> 01:58:29,160
text equals text dot replace, and we're going to replace the instance of I believe it's

1728
01:58:29,160 --> 01:58:33,400
that mark, but let's just copy and paste it in to make sure we're going to replace that

1729
01:58:33,400 --> 01:58:39,240
with a, with a single quotation mark, and we can print off text just to make sure that

1730
01:58:39,400 --> 01:58:40,400
was done correctly.

1731
01:58:40,400 --> 01:58:41,400
Cool.

1732
01:58:41,400 --> 01:58:42,400
Great.

1733
01:58:42,400 --> 01:58:43,400
It was it's now looking good.

1734
01:58:43,400 --> 01:58:47,880
Remember, whenever you're doing information extraction, standardize the texts as much as

1735
01:58:47,880 --> 01:58:49,080
possible.

1736
01:58:49,080 --> 01:58:53,720
Things like quotation marks will always throw off your data.

1737
01:58:53,720 --> 01:59:00,480
Now that we've got that, let's go ahead and start trying to create a fairly robust pattern

1738
01:59:00,480 --> 01:59:07,640
to try to grab all instances where there is a quotation mark, thought, something like

1739
01:59:07,720 --> 01:59:11,480
this, and then followed by another quotation mark.

1740
01:59:11,480 --> 01:59:15,760
So the first thing I'm going to try and do is I'm going to try to just capture all quotation

1741
01:59:15,760 --> 01:59:17,440
marks and a text.

1742
01:59:17,440 --> 01:59:21,520
So let's go through and try to figure out how to do that right now.

1743
01:59:21,520 --> 01:59:25,480
So we're going to copy in a lot of the same things that we used up above, but we're going

1744
01:59:25,480 --> 01:59:27,040
to make some modifications to it.

1745
01:59:27,040 --> 01:59:32,640
Let's go ahead and copy and paste all that we're going to completely change our pattern.

1746
01:59:32,640 --> 01:59:34,040
So let's get rid of this.

1747
01:59:34,040 --> 01:59:35,400
So what are we looking for?

1748
01:59:35,400 --> 01:59:41,240
Well, first of all, the first thing that's going to occur in this pattern is this quotation

1749
01:59:41,240 --> 01:59:42,080
mark.

1750
01:59:42,080 --> 01:59:47,320
So that's going to be a full text match, which is an or if you remember, and we're going

1751
01:59:47,320 --> 01:59:51,600
to have to use double quotation marks to add in that single quotation mark.

1752
01:59:51,600 --> 01:59:52,720
So that's what we grabbed first.

1753
01:59:52,720 --> 01:59:57,160
We're going to look for anything that is an or and the next thing that's going to occur

1754
01:59:57,160 --> 02:00:02,520
after that, I think this is good to probably do this now on a line by line basis.

1755
02:00:02,520 --> 02:00:05,120
So we can keep this straight.

1756
02:00:05,120 --> 02:00:08,400
So the next thing that's going to occur is we're looking for anything in between.

1757
02:00:08,400 --> 02:00:14,240
So anything that is an alpha character, we're going to just grab it all.

1758
02:00:14,240 --> 02:00:22,160
So is alpha and then we need to say true.

1759
02:00:22,160 --> 02:00:26,960
But within this, we need to specify how many times that occurs because if we say is true,

1760
02:00:26,960 --> 02:00:32,600
it's just going to look at the next token in this case and and then say that's the end.

1761
02:00:32,600 --> 02:00:33,600
That's it.

1762
02:00:33,600 --> 02:00:34,600
That's the pattern.

1763
02:00:35,080 --> 02:00:40,880
But we want to grab not just and but and what is the use of a everything.

1764
02:00:40,880 --> 02:00:47,280
So we need to grab not only that, but when you say OP, so our operator again.

1765
02:00:47,280 --> 02:00:50,280
And if you said plus, you would be right here.

1766
02:00:50,280 --> 02:00:53,960
We need to make sure that it's a plus sign, so it's grabbing everything.

1767
02:00:53,960 --> 02:00:59,720
Now in this scenario, this is a common construct is when you have a injection here in the middle

1768
02:00:59,720 --> 02:01:00,720
of the sentence.

1769
02:01:00,720 --> 02:01:03,180
So thought or said, and it's the character doing it.

1770
02:01:03,180 --> 02:01:06,020
This oftentimes got a comma right here.

1771
02:01:06,020 --> 02:01:08,580
So we need to add in that kind of a feature.

1772
02:01:08,580 --> 02:01:12,380
So there could be is punked.

1773
02:01:12,380 --> 02:01:15,220
There could be a punked here.

1774
02:01:15,220 --> 02:01:18,760
And we're going to say that that is equal to true.

1775
02:01:18,760 --> 02:01:21,300
But that might not always be the case.

1776
02:01:21,300 --> 02:01:23,180
There might not always be one there.

1777
02:01:23,180 --> 02:01:28,300
So we're going to say OP is equal to a star.

1778
02:01:28,300 --> 02:01:29,300
We go back.

1779
02:01:29,300 --> 02:01:30,300
We'll see why.

1780
02:01:30,300 --> 02:01:35,300
To our OP, the star allow the pattern to match zero or more time.

1781
02:01:35,300 --> 02:01:40,120
So in this scenario, the punctuation may or may not be there.

1782
02:01:40,120 --> 02:01:41,860
So that's the next thing that occurs.

1783
02:01:41,860 --> 02:01:46,820
Once we've got that, the last thing that we need to match is the exact same thing that

1784
02:01:46,820 --> 02:01:51,980
we had at the start is this or appear.

1785
02:01:51,980 --> 02:01:53,420
And that's our sequence.

1786
02:01:53,420 --> 02:01:57,020
So this is going to look for anything that starts with a quotation mark has a series

1787
02:01:57,020 --> 02:02:03,700
of alpha characters has a punctuation like a comma possibly, and then closes the quotation

1788
02:02:03,700 --> 02:02:04,700
marks.

1789
02:02:04,700 --> 02:02:07,580
If we execute this, we succeeded.

1790
02:02:07,580 --> 02:02:08,580
We got it.

1791
02:02:08,580 --> 02:02:11,580
We extracted both matches from that first sentence.

1792
02:02:11,580 --> 02:02:13,820
There are no other quotation marks in there.

1793
02:02:13,820 --> 02:02:17,300
But our task was not just to extract this information.

1794
02:02:17,300 --> 02:02:22,340
Our task was also to match who is the speaker.

1795
02:02:22,340 --> 02:02:25,580
Now we can do this in a few different ways and you're going to see why this is such a

1796
02:02:25,580 --> 02:02:28,420
complicated problem in just a second.

1797
02:02:28,420 --> 02:02:30,260
So let's go ahead and do this.

1798
02:02:30,260 --> 02:02:32,060
How can we make this better?

1799
02:02:32,060 --> 02:02:36,180
Well, we're going to have this occur twice.

1800
02:02:36,180 --> 02:02:40,380
But in the middle, we need to figure out when somebody is speaking.

1801
02:02:40,380 --> 02:02:43,180
So one of the things that we can do is we can make a list.

1802
02:02:43,180 --> 02:02:49,240
So let's make a list of limitized forms of our verbs.

1803
02:02:49,240 --> 02:02:53,980
So we're going to say, let's call this speak underscore limits.

1804
02:02:53,980 --> 02:02:56,060
This can be equal to a list.

1805
02:02:56,060 --> 02:02:59,500
And the first thing we're going to say is think, because we know that think is in there

1806
02:02:59,500 --> 02:03:04,720
and say this is the limitized form of thought and said.

1807
02:03:04,720 --> 02:03:08,440
So what we can do now is after that occurs, it's adding a new thing.

1808
02:03:08,440 --> 02:03:12,060
We're going to be able to now add in a new pattern that we're looking for.

1809
02:03:12,060 --> 02:03:17,700
And so not just the start of a quotation mark, not just the end of a quotation mark, but

1810
02:03:17,700 --> 02:03:20,980
also a sequence that'll be something like this.

1811
02:03:20,980 --> 02:03:22,980
So it's going to be a part of speech.

1812
02:03:22,980 --> 02:03:26,380
So it's going to be a verb that occurs first, right?

1813
02:03:26,380 --> 02:03:28,620
And that's going to be a verb.

1814
02:03:28,620 --> 02:03:41,180
But more importantly, it's going to be a lemma that is in what did I call you speak lemmas?

1815
02:03:41,180 --> 02:03:43,740
So let's break this down.

1816
02:03:43,740 --> 02:03:48,180
The next token needs to be a verb.

1817
02:03:48,180 --> 02:03:57,460
And it needs to have a limitized form that is contained within the speak lemmas list.

1818
02:03:57,460 --> 02:04:01,620
So if it's got that fantastic, let's execute this and see what happens.

1819
02:04:01,620 --> 02:04:03,620
We should only have one hit.

1820
02:04:03,620 --> 02:04:04,620
Cool.

1821
02:04:04,620 --> 02:04:05,620
We do.

1822
02:04:05,620 --> 02:04:06,620
So we've got that first hit.

1823
02:04:06,620 --> 02:04:10,020
And the second one hasn't appeared anymore because that second quotation mark wasn't

1824
02:04:10,020 --> 02:04:12,980
proceeded by a verb.

1825
02:04:12,980 --> 02:04:16,660
Let's go ahead and make some modifications that we can improve this a little bit.

1826
02:04:16,660 --> 02:04:20,860
Because we want to know not just what that person's doing.

1827
02:04:20,860 --> 02:04:22,980
We also need to know who the speaker is.

1828
02:04:22,980 --> 02:04:24,940
So let's grab it.

1829
02:04:24,940 --> 02:04:26,780
Let's figure out who that speaker is.

1830
02:04:26,780 --> 02:04:28,820
So we can use part of speech.

1831
02:04:28,820 --> 02:04:31,020
Again, another feature here.

1832
02:04:31,020 --> 02:04:34,780
We know that it's going to be a proper noun because oftentimes proper nouns are doing

1833
02:04:34,780 --> 02:04:36,300
the speaking.

1834
02:04:36,300 --> 02:04:37,660
Sometimes it might not be.

1835
02:04:37,660 --> 02:04:41,800
Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those

1836
02:04:41,800 --> 02:04:44,360
situations for just right now.

1837
02:04:44,360 --> 02:04:46,000
So we're looking for a proper noun.

1838
02:04:46,000 --> 02:04:51,480
But remember proper nouns, as we saw just a second ago, could be multiple tokens.

1839
02:04:51,480 --> 02:04:53,840
So we're going to say OP plus.

1840
02:04:53,840 --> 02:04:55,920
So it could be a sequence of tokens.

1841
02:04:55,920 --> 02:04:57,680
Let's execute this.

1842
02:04:57,680 --> 02:04:59,960
Now we've captured Alice here as well.

1843
02:04:59,960 --> 02:05:04,000
So and is the use and what is the use of a book thought Alice.

1844
02:05:04,000 --> 02:05:08,120
Now we know who the speaker is, but this is a partial quotation.

1845
02:05:08,120 --> 02:05:09,240
This is not the whole thing.

1846
02:05:09,240 --> 02:05:11,160
We need to grab the other quote.

1847
02:05:11,160 --> 02:05:12,960
Oh, how will we ever do that?

1848
02:05:12,960 --> 02:05:15,000
Well, we've already solved that.

1849
02:05:15,000 --> 02:05:21,120
We can copy and paste all of this that we already have done right down here.

1850
02:05:21,120 --> 02:05:26,400
And now we've successfully extracted that entire quote.

1851
02:05:26,400 --> 02:05:28,640
So you might be thinking to yourself, yeah, we did it.

1852
02:05:28,640 --> 02:05:36,600
We can now extract quotation marks and we can even extract, extract, you know, any instance

1853
02:05:36,600 --> 02:05:39,800
where there's a quote and somebody speaking.

1854
02:05:39,800 --> 02:05:40,800
Not so fast.

1855
02:05:40,800 --> 02:05:42,360
We're going to try to iterate over this data.

1856
02:05:42,360 --> 02:05:50,040
So we're going to say for text in data, zero twos, we're going to iterate over the first

1857
02:05:50,040 --> 02:05:52,400
chapter.

1858
02:05:52,400 --> 02:05:59,920
And we're going to go ahead and let's let's do all of this.

1859
02:05:59,920 --> 02:06:07,600
Doc is going to be equal to that sort that out.

1860
02:06:07,600 --> 02:06:13,040
And then again, we're going to be printing out this information, the same stuff I did

1861
02:06:13,040 --> 02:06:16,360
before, just now it's going to be iterating over the whole chapter.

1862
02:06:16,360 --> 02:06:23,080
And if we let this run, we've got a serious, serious problem.

1863
02:06:23,080 --> 02:06:26,680
And it doesn't actually grab us anything.

1864
02:06:26,680 --> 02:06:29,200
Nothing has been grabbed successfully.

1865
02:06:29,200 --> 02:06:34,200
What is going on?

1866
02:06:34,200 --> 02:06:35,840
We've got a problem.

1867
02:06:35,840 --> 02:06:44,520
And that problem stems from the fact that our patterns and the problem is that we don't

1868
02:06:44,520 --> 02:06:51,280
have our our text correctly, we're being removing the quotation mark that was the problem up

1869
02:06:51,280 --> 02:06:52,280
above.

1870
02:06:52,280 --> 02:06:55,640
So we're going to add this bit of code in.

1871
02:06:55,640 --> 02:06:57,640
And we're going to be able to fix it.

1872
02:06:57,640 --> 02:07:00,880
So now when we execute this, we see that we've only grabbed one match.

1873
02:07:00,880 --> 02:07:03,720
Now you might be thinking to yourself, there's an issue here and there there is, let's go

1874
02:07:03,720 --> 02:07:07,000
ahead and print off the length of matches.

1875
02:07:07,000 --> 02:07:09,720
And we see that we've only grabbed one match.

1876
02:07:09,720 --> 02:07:11,000
And then we haven't grabbed anything else.

1877
02:07:11,000 --> 02:07:12,400
Well, what's the problem here?

1878
02:07:12,400 --> 02:07:16,760
Are there are there no other instances of quotation marks in the rest of the first chapter?

1879
02:07:16,760 --> 02:07:18,480
And the answer is no, there are.

1880
02:07:18,480 --> 02:07:23,320
There absolutely are other quotation marks and other paragraphs from the first chapter.

1881
02:07:23,320 --> 02:07:27,840
The problem is, is that our pattern is singular.

1882
02:07:27,840 --> 02:07:29,280
It's not multivariate.

1883
02:07:29,280 --> 02:07:35,560
We need to add in additional ways in which a text might be structured.

1884
02:07:35,560 --> 02:07:40,800
So let's go ahead and try and do this with some more patterns.

1885
02:07:40,800 --> 02:07:44,840
I'm going to go ahead and copy and paste these in from the textbook.

1886
02:07:44,840 --> 02:07:50,000
So you'll be able to actually see them at work.

1887
02:07:50,000 --> 02:07:54,320
And so what I've did, I've done is I've added in more patterns, pattern two and pattern

1888
02:07:54,320 --> 02:07:58,920
three allow for instances like this, well thought Alice.

1889
02:07:58,920 --> 02:08:03,160
So an instance where there's a punctuation, but there's no proceeding quotation after

1890
02:08:03,160 --> 02:08:07,760
this, and then which certainly said before an instance where there's a comma followed

1891
02:08:07,760 --> 02:08:08,760
by that.

1892
02:08:08,760 --> 02:08:12,640
So we've been able to capture more variants and more ways in which quotation marks might

1893
02:08:12,640 --> 02:08:14,920
exist followed by the speaker.

1894
02:08:14,920 --> 02:08:18,320
Now this is where being a domain expert comes into play.

1895
02:08:18,320 --> 02:08:21,440
You'd have to kind of look through and see the different ways that Louis C. Carroll

1896
02:08:21,440 --> 02:08:25,640
structures quotation marks and write out patterns for capturing them.

1897
02:08:25,640 --> 02:08:29,280
I'm not going to go through and try to capture everything from Alice in Wonderland because

1898
02:08:29,280 --> 02:08:31,600
that would take a good deal of time.

1899
02:08:31,600 --> 02:08:35,760
And it's not really in the best interest because it doesn't matter to me at all.

1900
02:08:35,760 --> 02:08:39,400
What I encourage you to do, if this is something interesting to you is try to apply it to your

1901
02:08:39,400 --> 02:08:44,640
own texts, different authors, structure quotation marks a little differently than what patterns

1902
02:08:44,640 --> 02:08:47,560
that I've gotten written here are a good starting point.

1903
02:08:47,560 --> 02:08:50,480
But I would encourage you to start playing around with them a little bit more.

1904
02:08:50,480 --> 02:08:55,520
And what you can do is when you actually have this match extracted, you know that the

1905
02:08:55,880 --> 02:09:01,960
instance of a proper noun that occurs between these quotation marks or after one is probably

1906
02:09:01,960 --> 02:09:08,040
going to be the person or thing that is doing the speaking or the thinking.

1907
02:09:08,040 --> 02:09:10,080
So that's kind of how the matcher works.

1908
02:09:10,080 --> 02:09:15,680
It allows for you to do these things, these robust type data extractions without relying

1909
02:09:15,680 --> 02:09:16,920
on entity ruler.

1910
02:09:16,920 --> 02:09:21,000
And remember, you can use a lot of these same things with an entity ruler as well.

1911
02:09:21,000 --> 02:09:25,400
But we don't want this in this case, we don't want things like this to be labeled as entities.

1912
02:09:25,440 --> 02:09:29,320
We want them to just be separate things that we can extract outside of the of the

1913
02:09:29,320 --> 02:09:30,960
ints dot doc dot ints.

1914
02:09:31,320 --> 02:09:36,560
That's going to be where we conclude our chapter on on the on the matcher.

1915
02:09:36,920 --> 02:09:40,920
In the next section of this video, we're going to be talking about custom components in

1916
02:09:40,920 --> 02:09:46,880
spacey, which allow for us to do some pretty cool things such as add in special functions

1917
02:09:46,880 --> 02:09:53,840
that allow for us to kind of do different custom shapes, permutations on our data with

1918
02:09:54,520 --> 02:09:58,680
components that don't exist like an entity ruler would be a component components that

1919
02:09:58,680 --> 02:10:00,880
don't exist within the spacey framework.

1920
02:10:00,880 --> 02:10:06,760
So add in custom things like an entity ruler that do very specific things to your data.

1921
02:10:09,080 --> 02:10:14,160
Hello, we're now moving into a more advanced aspect of the textbook specifically chapter

1922
02:10:14,160 --> 02:10:14,760
seven.

1923
02:10:14,760 --> 02:10:17,000
And that's working with custom components.

1924
02:10:17,320 --> 02:10:21,240
A good way to think about a custom component is something that you need to do to the doc

1925
02:10:21,240 --> 02:10:25,080
object or the doc container that spacey can't do off the shelf.

1926
02:10:25,080 --> 02:10:27,880
You want to modify it at some point in the pipeline.

1927
02:10:28,200 --> 02:10:32,560
So I'm going to use a basic toy example that demonstrates the power of this.

1928
02:10:32,840 --> 02:10:35,720
Let's look at this basic example that I've already loaded into memory.

1929
02:10:35,920 --> 02:10:38,760
It's two sentences that are in the doc object now.

1930
02:10:39,440 --> 02:10:40,640
And that's Britain is a place.

1931
02:10:40,840 --> 02:10:42,000
Mary is a doctor.

1932
02:10:42,360 --> 02:10:49,080
So let's do for int and doc dot ints print off int dot text and dot label.

1933
02:10:50,080 --> 02:10:51,520
And we see what we'd expect.

1934
02:10:51,520 --> 02:10:54,520
Britain is GPE a geopolitical entity.

1935
02:10:54,760 --> 02:10:56,400
Mary is a person.

1936
02:10:57,120 --> 02:10:58,000
That's fantastic.

1937
02:10:58,280 --> 02:11:04,560
But I've just been told by somebody higher up that they want the model to never ever

1938
02:11:04,560 --> 02:11:11,480
give anything as GPE or maybe they want any instance of GPE to be flagged as LOC.

1939
02:11:12,280 --> 02:11:17,640
So all the different locations all have LOC as a label or we just want to remove them

1940
02:11:17,640 --> 02:11:18,240
entirely.

1941
02:11:18,920 --> 02:11:20,680
So I'm going to work with that latter example.

1942
02:11:21,000 --> 02:11:27,760
We need to create a custom pipe that removes all instances of GPE from the doc dot

1943
02:11:27,800 --> 02:11:29,040
ints container.

1944
02:11:29,320 --> 02:11:30,080
So how do we do that?

1945
02:11:30,120 --> 02:11:32,880
Well, we need to use a custom component.

1946
02:11:33,200 --> 02:11:38,720
We can do this very easily in spacey by saying from spacey dot language import

1947
02:11:39,080 --> 02:11:41,240
language capital L very important there.

1948
02:11:41,240 --> 02:11:44,600
Capital L now that we've got that class loaded up.

1949
02:11:45,080 --> 02:11:46,320
Let's start working with this.

1950
02:11:46,320 --> 02:11:49,280
What we need to do first is we need to use a flag.

1951
02:11:49,280 --> 02:11:53,520
So the symbol and we need to say at language dot component.

1952
02:11:54,480 --> 02:11:56,280
And we need to give that component a name.

1953
02:11:56,720 --> 02:12:00,160
We're going to say in this case, let's say remove GPE.

1954
02:12:01,480 --> 02:12:03,560
And now we need to create a function to do this.

1955
02:12:03,920 --> 02:12:07,200
So we're going to call this remove GPE.

1956
02:12:07,400 --> 02:12:09,240
I always kind of keep these as the same.

1957
02:12:09,680 --> 02:12:11,280
That's my personal preference.

1958
02:12:11,760 --> 02:12:15,040
And this is going to take one, one, one thing.

1959
02:12:15,080 --> 02:12:16,400
That's going to be the doc object.

1960
02:12:16,680 --> 02:12:19,600
So the doc object, think about how it moves through the pipeline.

1961
02:12:19,840 --> 02:12:22,640
This component is another pipe and that pipeline.

1962
02:12:22,640 --> 02:12:27,400
It needs to receive the doc object and send off the doc object.

1963
02:12:27,640 --> 02:12:28,760
You could do a lot of other things.

1964
02:12:29,080 --> 02:12:30,600
It could print off entity found.

1965
02:12:30,600 --> 02:12:32,800
It could do really any number of things.

1966
02:12:32,800 --> 02:12:36,440
It could add stuff to the data coming out of the pipeline.

1967
02:12:37,040 --> 02:12:40,240
All we're concerned with right now is modifying the doc dot ints.

1968
02:12:41,080 --> 02:12:42,400
So we can do something like this.

1969
02:12:42,880 --> 02:12:48,120
We can say original ends is equal to a list of the doc dot ends.

1970
02:12:48,240 --> 02:12:52,800
So remember, we have to convert the ends from a generator into a list.

1971
02:12:53,040 --> 02:12:58,000
Now what we can do is we can say for int and doc dot ends, if the end not label.

1972
02:12:58,000 --> 02:13:05,040
So if that label is equal to GPE, then what we want to do is we want to just

1973
02:13:05,160 --> 02:13:06,240
we just want to remove it.

1974
02:13:06,360 --> 02:13:11,360
So let's say original ints.remove and we're going to remove the int.

1975
02:13:11,640 --> 02:13:12,720
Remember, it's now a list.

1976
02:13:13,000 --> 02:13:14,560
Sorry, I executed that too soon.

1977
02:13:14,840 --> 02:13:16,040
Remember, it's now a list.

1978
02:13:16,200 --> 02:13:20,960
So what we can do is we can go ahead now and convert those original

1979
02:13:20,960 --> 02:13:26,320
ends back into doc dot ends by saying doc dot ends equals original ends.

1980
02:13:26,720 --> 02:13:30,800
And if we've done things correctly, we can return the doc object and it will

1981
02:13:30,800 --> 02:13:32,840
have all of those things removed.

1982
02:13:33,040 --> 02:13:35,280
So this is what we would call a custom component.

1983
02:13:35,280 --> 02:13:39,760
Something that changes the doc object along the way in the pipeline, but

1984
02:13:39,760 --> 02:13:41,440
we need to add it to NLP.

1985
02:13:41,880 --> 02:13:44,120
So we can do NLP dot add pipe.

1986
02:13:45,040 --> 02:13:47,240
We want to make sure that it comes after the NER.

1987
02:13:47,480 --> 02:13:52,880
So we're just going to say, uh, add the pipe or move GPE corresponds

1988
02:13:52,880 --> 02:13:54,160
to the component name.

1989
02:13:55,880 --> 02:13:59,400
And now let's go ahead and NLP dot analyze pipes.

1990
02:14:00,400 --> 02:14:04,880
And you'll be able to see that it sits at the end of our pipeline right there.

1991
02:14:04,880 --> 02:14:05,880
Remove GPE.

1992
02:14:06,360 --> 02:14:08,400
Now comes time to see if it actually works.

1993
02:14:08,720 --> 02:14:11,800
So we're going to copy and paste our code from earlier up here.

1994
02:14:17,040 --> 02:14:21,480
Let's go ahead and copy this.

1995
02:14:23,680 --> 02:14:28,600
And now we're going to say for int and doc dot ends print off int dot text.

1996
02:14:29,400 --> 02:14:30,480
And dot label.

1997
02:14:30,920 --> 02:14:35,200
And we should see, as we would expect, just marry coming out.

1998
02:14:35,640 --> 02:14:38,400
Our pipeline has successfully worked.

1999
02:14:38,840 --> 02:14:42,560
Now, as we're going to see when we move into red checks, you can do a lot

2000
02:14:42,560 --> 02:14:46,280
of really, really cool things with custom components.

2001
02:14:46,640 --> 02:14:50,480
I'm going to kind of save the, the advanced features for, I think I've

2002
02:14:50,480 --> 02:14:54,400
got it scheduled for chapter here, chapter nine in our textbook.

2003
02:14:54,680 --> 02:14:59,320
This is just a very, very basic example of how you can introduce a custom

2004
02:14:59,320 --> 02:15:01,560
component to your spacey pipeline.

2005
02:15:01,920 --> 02:15:04,080
If you can do this, you can do a lot more.

2006
02:15:04,400 --> 02:15:06,440
You can maybe change a different entity.

2007
02:15:06,440 --> 02:15:07,640
So they have different labels.

2008
02:15:07,800 --> 02:15:10,680
You can make it where GPEs and locks all agree.

2009
02:15:10,920 --> 02:15:12,200
You can remove certain things.

2010
02:15:12,200 --> 02:15:15,280
You can have it print off place found person found.

2011
02:15:15,560 --> 02:15:16,400
You can do a lot.

2012
02:15:17,240 --> 02:15:20,800
So really the sky's the limit here, but a lot of the times you're going

2013
02:15:20,800 --> 02:15:22,600
to need to modify that doc object.

2014
02:15:22,920 --> 02:15:26,240
And this is how you do it with a custom pipe so that you don't have to write

2015
02:15:26,240 --> 02:15:31,880
a bunch of code for a user outside of that NLP object, the NLP object.

2016
02:15:31,880 --> 02:15:38,560
Once you save it to disk by doing something like NLP dot to disk data,

2017
02:15:39,200 --> 02:15:45,640
new and core web SM, it's going to actually be able to go to the disk

2018
02:15:45,960 --> 02:15:47,640
and be saved with everything.

2019
02:15:48,080 --> 02:15:52,440
But one thing that you should note is that the component that you have

2020
02:15:52,440 --> 02:15:56,520
here is not automatically saved with your data.

2021
02:15:56,920 --> 02:16:00,320
So in order for your component to actually be saved with your data,

2022
02:16:00,640 --> 02:16:06,000
you need to store that outside of this entire script.

2023
02:16:06,280 --> 02:16:11,360
You need to save it as a library that can be given to the model

2024
02:16:11,400 --> 02:16:12,760
when you go to package it.

2025
02:16:12,960 --> 02:16:15,280
That's beyond the scope of this video for right now.

2026
02:16:15,640 --> 02:16:19,880
In order for this to work in a different Jupyter notebook, if you were to try

2027
02:16:19,880 --> 02:16:25,320
to use this, this container, this component has to actually be in the script.

2028
02:16:25,560 --> 02:16:29,400
When it comes time to package your model, your pipeline and distribute it,

2029
02:16:29,680 --> 02:16:30,800
that's a different scenario.

2030
02:16:30,800 --> 02:16:34,480
And that scenario, you're going to make sure that you've got a special my

2031
02:16:34,480 --> 02:16:39,560
component dot pie file with this bit of code in there so that, so that spacing

2032
02:16:39,560 --> 02:16:42,560
knows how to handle your particular data.

2033
02:16:43,560 --> 02:16:46,800
It's now time to move on to chapter eight of this textbook.

2034
02:16:46,800 --> 02:16:48,680
And this is where a spacey gets really interesting.

2035
02:16:48,680 --> 02:16:53,280
You can start applying regular expressions into a spacey component

2036
02:16:53,280 --> 02:16:57,360
like an entity ruler or a custom component, as we're going to see in just

2037
02:16:57,360 --> 02:16:58,840
a moment with chapter nine.

2038
02:16:59,200 --> 02:17:02,680
I'm not going to spend a good deal of time talking about regular expressions.

2039
02:17:02,680 --> 02:17:07,000
I could spend five hours talking about regex and what all it can do.

2040
02:17:07,120 --> 02:17:10,880
In the textbook, I go over what you really need to know, which is what regular

2041
02:17:10,920 --> 02:17:15,920
expressions is, which is as a way to do a really robust string pattern matching.

2042
02:17:16,280 --> 02:17:19,440
I talk about the strengths of it, the weaknesses of it, its drawbacks,

2043
02:17:19,800 --> 02:17:23,080
how to implement it in Python and how to really work with regex.

2044
02:17:23,240 --> 02:17:25,640
But this is a video series on spacey.

2045
02:17:25,880 --> 02:17:29,320
What I want to talk about is how to use regex with spacey.

2046
02:17:29,640 --> 02:17:32,400
And so let's move over to a Jupiter notebook where we actually have this

2047
02:17:32,400 --> 02:17:34,240
code to execute and play around with.

2048
02:17:35,000 --> 02:17:37,840
If we look here, we have the same example that we saw before.

2049
02:17:38,160 --> 02:17:40,800
What my goal is is not to extract the whole phone number,

2050
02:17:40,800 --> 02:17:43,200
rather try to grab this sequence here.

2051
02:17:43,520 --> 02:17:45,760
And we do this with a regular expression pattern.

2052
02:17:46,080 --> 02:17:49,680
What this says is it tells it to look for a sequence of tokens or sequence

2053
02:17:49,680 --> 02:17:51,400
of characters like this.

2054
02:17:51,680 --> 02:17:56,840
It's going to be three digits followed by a dash followed by four digits.

2055
02:17:57,160 --> 02:18:00,440
And if I were to execute this whole code, nothing is printed out.

2056
02:18:00,840 --> 02:18:03,400
Does that mean that I failed to write good regex?

2057
02:18:03,400 --> 02:18:04,760
No, it does not at all.

2058
02:18:05,120 --> 02:18:07,360
It's failed for one very important reason.

2059
02:18:07,440 --> 02:18:11,720
And this is the whole reason why I have this chapter in here is that regex,

2060
02:18:12,000 --> 02:18:15,320
when it comes to pattern matching, pattern matching only really works

2061
02:18:16,160 --> 02:18:19,840
when it comes to regex for single tokens.

2062
02:18:20,040 --> 02:18:25,480
You can't use regex across multi-word tokens, at least as of spacey 3.1.

2063
02:18:25,920 --> 02:18:27,200
So what does that mean?

2064
02:18:27,200 --> 02:18:30,920
Well, it means that that dash right there in our phone number is causing

2065
02:18:30,920 --> 02:18:32,200
all kinds of problems.

2066
02:18:32,520 --> 02:18:35,960
If we move down to our second example, it's going to be the exact same pattern.

2067
02:18:36,280 --> 02:18:37,080
A little different.

2068
02:18:37,080 --> 02:18:39,640
Let me go ahead and move this over so you can see it a bit better.

2069
02:18:40,840 --> 02:18:44,920
It's going to be regex that looks like this, where we just look for a sequence

2070
02:18:44,920 --> 02:18:49,040
of five digits, we execute that, we find it just fine.

2071
02:18:49,080 --> 02:18:52,160
And the reason for that is because this does not have a dash.

2072
02:18:52,520 --> 02:18:56,800
So regex, if you're familiar with it, if you've worked with it, it's very powerful.

2073
02:18:57,000 --> 02:18:58,880
You can do a lot of cool things.

2074
02:18:59,320 --> 02:19:04,440
When you're going to use this in Python, if you're using just the standard

2075
02:19:04,440 --> 02:19:05,960
off the shelf components.

2076
02:19:06,200 --> 02:19:09,680
So the entity ruler, the matcher, you're going to be using this when

2077
02:19:09,680 --> 02:19:14,160
you want to match regex to a single token.

2078
02:19:14,440 --> 02:19:19,480
So think about this, if you're looking for a word that starts off with a capital

2079
02:19:19,480 --> 02:19:24,320
D, and you want to just grab all words that start with a capital D, that would

2080
02:19:24,320 --> 02:19:28,360
be an example of when you would want to use it in a standard off the shelf component.

2081
02:19:28,800 --> 02:19:31,520
But that's not all you can do in spacey.

2082
02:19:31,880 --> 02:19:35,800
You can use regex to actually capture multi word tokens.

2083
02:19:36,080 --> 02:19:38,320
So capture things like Mr.

2084
02:19:38,400 --> 02:19:39,280
Deeds.

2085
02:19:39,480 --> 02:19:41,200
So any instance of Mr.

2086
02:19:41,200 --> 02:19:44,800
Period Space Name, a sequence of proper nouns.

2087
02:19:45,680 --> 02:19:50,880
You can also use it to, but yet in order to do that, you have to actually

2088
02:19:50,880 --> 02:19:54,520
understand how to add in a custom component for it.

2089
02:19:54,800 --> 02:19:59,320
And we're going to be seeing that in just a second as we move on to chapter nine,

2090
02:19:59,560 --> 02:20:01,120
which is advanced regex.

2091
02:20:01,320 --> 02:20:05,360
If you're not familiar with regex at all, take a few minutes, read chapter eight.

2092
02:20:05,360 --> 02:20:10,800
I encourage you to do so because I go over in detail and I talk about how to

2093
02:20:10,800 --> 02:20:15,080
actually engage in regex and Python and its strengths and weaknesses.

2094
02:20:15,440 --> 02:20:18,680
What I want you to really focus on though, and get away from, get from all this

2095
02:20:18,920 --> 02:20:24,160
is how to do some really complex multi word token matching with regex.

2096
02:20:24,160 --> 02:20:27,440
Remember, you're going to want to use regular expressions when the pattern

2097
02:20:27,440 --> 02:20:33,840
matching that you want to do is unindependent of the, the lima, the POS,

2098
02:20:33,840 --> 02:20:36,360
or any of the linguistic features that space is going to use.

2099
02:20:36,680 --> 02:20:40,040
If you're working with linguistic features, you have to use the

2100
02:20:40,040 --> 02:20:45,040
spacey pattern, pattern matching things like the morph, the orth, the lima,

2101
02:20:45,040 --> 02:20:45,840
things like that.

2102
02:20:45,840 --> 02:20:50,400
But if your sequence of strings is not dependent on that, so you're looking

2103
02:20:50,400 --> 02:20:53,840
for any instance of, in this case, we're going to talk about in just a second,

2104
02:20:54,240 --> 02:20:59,880
a, a case where Paul is followed by a capitalized letter and then a word break.

2105
02:21:00,680 --> 02:21:03,640
Then you're going to want to use regular expressions because in this case,

2106
02:21:03,960 --> 02:21:08,840
this is independent of any linguistic features and regular expressions

2107
02:21:08,840 --> 02:21:12,040
allows for you to write much more robust patterns, much more quickly.

2108
02:21:12,040 --> 02:21:14,960
If you know how to use it well, and it allows for you to do much more

2109
02:21:14,960 --> 02:21:18,440
quick robust things within a custom component.

2110
02:21:18,680 --> 02:21:21,000
And that's going to be where we move to now.

2111
02:21:22,000 --> 02:21:25,400
Now that we know a little bit about regex and how it can be implemented in

2112
02:21:25,400 --> 02:21:30,000
Python, let's go ahead and also in spacey, let's go ahead and try and see

2113
02:21:30,200 --> 02:21:37,400
how we can get regex to actually find multi word tokens for us within spacey

2114
02:21:37,600 --> 02:21:39,400
using everything in the spacey framework.

2115
02:21:39,600 --> 02:21:43,000
So the first thing I'm going to do to kind of demonstrate all this is I'm going

2116
02:21:43,000 --> 02:21:45,000
to import regex.

2117
02:21:45,000 --> 02:21:49,400
This comes standard with Python and you can import it as RE just that way.

2118
02:21:49,400 --> 02:21:52,400
Import RE and that's going to import regex.

2119
02:21:52,800 --> 02:21:56,800
I'm going to work from the textbook and work with this sample text.

2120
02:21:57,000 --> 02:22:01,800
So this is Paul Newman was an American actor, but Paul Hollywood is a British TV

2121
02:22:02,000 --> 02:22:02,800
TV host.

2122
02:22:03,000 --> 02:22:05,000
The name Paul is quite common.

2123
02:22:05,200 --> 02:22:09,000
So it's going to be the text that we work with throughout this entire chapter.

2124
02:22:09,400 --> 02:22:13,600
Now a regex pattern that I could write to capture all instances of things like

2125
02:22:13,600 --> 02:22:17,800
Paul Newman and Paul Hollywood, which is what my goal is, could look something

2126
02:22:18,400 --> 02:22:24,600
like this, I could say or make an R string here and say Paul, and then I'm going

2127
02:22:24,600 --> 02:22:28,200
to grab everything that starts with a capital letter and then my grab

2128
02:22:28,400 --> 02:22:30,200
everything until a word break.

2129
02:22:30,400 --> 02:22:34,200
And that's going to be a pattern that I can use in regex with this formula

2130
02:22:34,200 --> 02:22:39,100
means is find any instance of Paul proceeded by a in this case, a capital

2131
02:22:39,100 --> 02:22:41,900
letter until the actual word break.

2132
02:22:41,900 --> 02:22:45,300
So grab the first name Paul and then what we can make a presumption is going

2133
02:22:45,300 --> 02:22:49,700
to be that individual's last name in the text, a simple example, but one

2134
02:22:49,700 --> 02:22:52,400
that will demonstrate our kind of purpose right now.

2135
02:22:52,700 --> 02:22:57,100
So how we can do this is we can create an object called matches and use regex

2136
02:22:57,100 --> 02:23:03,200
dot find it or we can pass in the pattern and we can pass in the text.

2137
02:23:03,400 --> 02:23:07,100
So what this is going to do is it's going to use regex to try to find this

2138
02:23:07,100 --> 02:23:09,300
pattern within this text.

2139
02:23:09,500 --> 02:23:11,900
And then what we can do is we can iterate over those matches.

2140
02:23:11,900 --> 02:23:19,200
So for match and matches, we can grab and print off the match and we have

2141
02:23:19,200 --> 02:23:21,500
something that looks like this.

2142
02:23:23,100 --> 02:23:26,900
What we're looking at here is what we would call it a regex match object.

2143
02:23:27,100 --> 02:23:28,800
It's got a couple of different components here.

2144
02:23:29,000 --> 02:23:34,100
It's got a span, which tells us the start character and the end character.

2145
02:23:35,200 --> 02:23:39,500
And then it has a match and what this match means is the actual text itself.

2146
02:23:39,700 --> 02:23:43,300
So the match here is Paul Newman and the match here is Paul Hollywood.

2147
02:23:43,500 --> 02:23:47,900
So we've been able to extract the two entities in the text that begin with

2148
02:23:47,900 --> 02:23:52,500
Paul and have a proper last name structured with a capital letter.

2149
02:23:52,700 --> 02:23:54,500
We grabbed everything up until the word break.

2150
02:23:54,900 --> 02:23:55,500
That's great.

2151
02:23:55,700 --> 02:23:58,500
That's going to be what you need to know kind of going forward because what

2152
02:23:58,500 --> 02:24:03,200
we're going to do now is we're going to implement this in a custom spacey pipe.

2153
02:24:03,400 --> 02:24:07,100
But first let's go through and write the code so that we can then easily kind

2154
02:24:07,100 --> 02:24:08,600
of create the pipe afterwards.

2155
02:24:09,400 --> 02:24:14,000
So what we need to do is we need to import spacey and we also need to say

2156
02:24:14,000 --> 02:24:19,500
from spacey dot tokens import span and we're going to be importing a couple

2157
02:24:19,500 --> 02:24:22,500
of different things as we move forward because we're going to see that we're

2158
02:24:22,500 --> 02:24:24,300
going to make a couple of mistakes intentionally.

2159
02:24:24,300 --> 02:24:27,400
I'm going to show you how to kind of address these common mistakes that might

2160
02:24:27,400 --> 02:24:29,900
surface in trying to do something like this.

2161
02:24:30,400 --> 02:24:33,900
So once we've imported those two things, we can start actually writing out our

2162
02:24:33,900 --> 02:24:34,300
code.

2163
02:24:34,600 --> 02:24:37,300
Again, we're going to stick with the exact same text and again, we're going

2164
02:24:37,300 --> 02:24:41,900
to stick with the exact same pattern that we've got stored in memory up above.

2165
02:24:42,600 --> 02:24:46,500
So what we need to do now is we need to create a blank spacey object or sorry,

2166
02:24:46,500 --> 02:24:50,700
a blank spacey pipeline that we can kind of put all this information into.

2167
02:24:52,200 --> 02:24:57,500
And for right now what we're going to do is we're just going to kind of go

2168
02:24:57,500 --> 02:25:00,300
through and look at these individual entities.

2169
02:25:08,300 --> 02:25:16,100
So again, we're going to create the doc object, which is going to be equal to

2170
02:25:16,100 --> 02:25:21,500
nlp text and this is not going to be necessary for right now, but I'm

2171
02:25:21,500 --> 02:25:25,300
establishing a kind of a consistent workflow for us and you're going to see

2172
02:25:25,300 --> 02:25:28,400
how we kind of take all this and implement it inside of a pipeline.

2173
02:25:28,700 --> 02:25:32,200
So we're going to say original ends is equal to list doc dot ends.

2174
02:25:32,200 --> 02:25:35,500
Now in this scenario, there's not going to be any entities because we don't

2175
02:25:35,500 --> 02:25:40,200
have an any R or an entity ruler in our blank spacey pipeline.

2176
02:25:41,100 --> 02:25:43,800
What we're going to do next is we're going to create something called an

2177
02:25:43,800 --> 02:25:49,000
nwt int and that's going to stand for multi word token entity.

2178
02:25:50,000 --> 02:25:51,400
You can name this whatever you like.

2179
02:25:51,400 --> 02:25:54,800
This is just what I kind of stick to and then we're going to do and this

2180
02:25:54,800 --> 02:25:56,600
is straight from the spacey documentation.

2181
02:25:56,900 --> 02:26:00,100
We're going to say for match an RE dot find it or the same thing

2182
02:26:00,100 --> 02:26:04,200
that we saw above pattern doc dot text.

2183
02:26:04,500 --> 02:26:07,300
So what this is going to do is it's going to take that doc object.

2184
02:26:08,200 --> 02:26:12,000
Look at it as raw text because remember the doc object is a container

2185
02:26:12,300 --> 02:26:16,200
that doesn't actually have raw text in it until you actually call the dot

2186
02:26:16,200 --> 02:26:19,900
text attribute and then our goal is for each of these things.

2187
02:26:19,900 --> 02:26:23,200
We're going to look and call in this span.

2188
02:26:23,500 --> 02:26:29,300
So we're going to say is start and the end is equal to match dot span.

2189
02:26:29,600 --> 02:26:33,200
So what we're doing here is we're going in and grabbing the span attribute

2190
02:26:33,700 --> 02:26:37,000
and we're grabbing these two components the start and the end.

2191
02:26:37,200 --> 02:26:38,100
But we have a problem.

2192
02:26:38,400 --> 02:26:40,400
These are character spans.

2193
02:26:40,400 --> 02:26:43,600
Remember the doc object works on a token level.

2194
02:26:43,800 --> 02:26:47,300
So we've got to kind of figure out a way to reverse engineer this almost

2195
02:26:47,500 --> 02:26:50,200
to actually get this into a spacey form.

2196
02:26:50,400 --> 02:26:55,300
Fortunately the doc object also has an attribute called character span.

2197
02:26:56,000 --> 02:27:01,500
So what we can do is we can say the span is equal to doc dot char span

2198
02:27:02,500 --> 02:27:03,700
start and end.

2199
02:27:03,700 --> 02:27:07,100
So what this is going to do is it's going to print off essentially for us.

2200
02:27:07,300 --> 02:27:08,800
Let's go ahead and do that.

2201
02:27:09,100 --> 02:27:12,000
It would print off for us where we worry to actually have an entity here.

2202
02:27:12,600 --> 02:27:16,400
It would print off for us as we can see Paul Newman and Paul Hollywood.

2203
02:27:16,700 --> 02:27:22,700
So what we need to do now is we need to get this span into our entities.

2204
02:27:23,400 --> 02:27:29,100
So what we can do is instead of printing things off we can say if span is not

2205
02:27:29,100 --> 02:27:32,100
none because in some instance instances this will be the case.

2206
02:27:32,500 --> 02:27:35,900
You're going to say NWT ends dot append.

2207
02:27:36,700 --> 02:27:43,600
You're going to append a tuple here span dot start span dot end span dot

2208
02:27:43,600 --> 02:27:44,300
text.

2209
02:27:44,400 --> 02:27:48,600
So this is going to be the start the end and the text itself.

2210
02:27:49,100 --> 02:27:53,900
And once we've done that we've managed to get our multi word tokens into

2211
02:27:55,100 --> 02:27:57,100
a list that looks like this.

2212
02:27:58,100 --> 02:28:06,600
Start and Paul Newman Paul Hollywood and notice that our span dot start is

2213
02:28:06,600 --> 02:28:09,900
aligning not with a character span.

2214
02:28:09,900 --> 02:28:14,200
Now it's rather aligning with a token span.

2215
02:28:14,400 --> 02:28:17,800
So what we've done is we've taken this character span here and been able to

2216
02:28:17,800 --> 02:28:23,600
find out where they start and end within the the token sequence.

2217
02:28:23,600 --> 02:28:24,900
So we have zero and two.

2218
02:28:25,300 --> 02:28:28,300
So Paul Newman one this was the zero index.

2219
02:28:28,300 --> 02:28:30,200
It goes up until the second index.

2220
02:28:30,400 --> 02:28:34,300
So it grabs index token zero and token one and we've done the same thing

2221
02:28:34,300 --> 02:28:35,300
with Paul Hollywood.

2222
02:28:35,700 --> 02:28:37,000
Now we've got that data.

2223
02:28:37,200 --> 02:28:44,900
We can actually start to inject these entities into our original entities.

2224
02:28:44,900 --> 02:28:46,600
So let's go through and do that right now.

2225
02:28:47,100 --> 02:28:50,200
So we can do once we've got these things appended to this list.

2226
02:28:50,200 --> 02:28:53,200
We can start injecting them into our original entities.

2227
02:28:53,200 --> 02:28:57,300
So we can say for end in MWT ends.

2228
02:28:57,800 --> 02:29:01,500
What we want to do is we want to say the start the end and the name is equal

2229
02:29:01,500 --> 02:29:05,900
to end because this is going to correspond to the tuple the start the

2230
02:29:05,900 --> 02:29:08,700
end and the entity text.

2231
02:29:10,000 --> 02:29:12,000
Now what we can do is we can say per end.

2232
02:29:12,000 --> 02:29:13,600
So this is going to be the individual end.

2233
02:29:13,600 --> 02:29:16,800
We're going to create a span object in spacey.

2234
02:29:17,900 --> 02:29:19,000
It's going to look like this.

2235
02:29:19,300 --> 02:29:20,700
So a capital S here.

2236
02:29:20,700 --> 02:29:22,800
Remember we imported it right up here.

2237
02:29:23,000 --> 02:29:27,300
This is where we're going to be working with the span class and this is

2238
02:29:27,300 --> 02:29:31,300
going to create for us a span object that we can now safely inject into

2239
02:29:31,300 --> 02:29:33,800
the spacey doc.ins list.

2240
02:29:34,300 --> 02:29:39,800
So we can say doc start and label and this is going to be the label that

2241
02:29:39,800 --> 02:29:43,300
we want to actually assign it and this is going to be person in this

2242
02:29:43,300 --> 02:29:48,100
case because these are all people we can do now as we can go through and

2243
02:29:48,100 --> 02:29:52,900
say doc we can inject this into the original ends.

2244
02:29:56,100 --> 02:30:02,500
Original ins dot append and we're going to append the per end which is

2245
02:30:02,500 --> 02:30:08,900
going to be this span object and finally what we can say is doc.ins is

2246
02:30:08,900 --> 02:30:14,500
equal to original ends kind of like what we saw just a few moments ago

2247
02:30:15,300 --> 02:30:17,200
and let's go ahead and print off.

2248
02:30:23,100 --> 02:30:26,600
We've got our entities right there or we to do this up here when we first

2249
02:30:26,600 --> 02:30:30,900
kind of create the doc object you'll see nothing an empty list but now

2250
02:30:30,900 --> 02:30:36,300
what we've been able to do is inject these into the doc object the doc.ins

2251
02:30:36,300 --> 02:30:40,300
attribute and we can say for end and doc.ins just like everything else

2252
02:30:40,300 --> 02:30:45,200
and dot text and dot label and because we converted it into a span we

2253
02:30:45,200 --> 02:30:50,900
were able to inject it into the entity attribute from the doc object kind

2254
02:30:50,900 --> 02:30:53,400
of natively so that spacey can actually understand it.

2255
02:30:53,800 --> 02:30:57,100
So what can we do with this well one of the things that we could do is

2256
02:30:57,100 --> 02:31:00,200
we can use the knowledge that we just acquired about custom components

2257
02:31:00,500 --> 02:31:04,000
and build a custom component around all of this.

2258
02:31:04,100 --> 02:31:07,400
So how might we do that well let's go through and try it out.

2259
02:31:08,400 --> 02:31:14,600
The first thing that we need to do is we need to import our language class so

2260
02:31:14,600 --> 02:31:17,600
if you remember from a few moments ago whenever you need to work with a

2261
02:31:17,600 --> 02:31:24,600
custom component you need to say from spacey dot language import language

2262
02:31:24,600 --> 02:31:28,100
with a capital L what we're going to do now is we're going to take the code

2263
02:31:28,100 --> 02:31:31,500
that we just wrote and we're going to try to convert that into an actual

2264
02:31:31,500 --> 02:31:36,700
custom pipe that can fit inside of our pipeline as kind of our own custom

2265
02:31:36,700 --> 02:31:37,900
entity ruler if you will.

2266
02:31:38,700 --> 02:31:41,700
So what we're going to do now is we're going to call this language dot

2267
02:31:41,700 --> 02:31:47,500
component and we're going to call this let's call this Paul NER something

2268
02:31:47,600 --> 02:31:51,400
not too not too clever but kind of very descriptive we're going to call

2269
02:31:51,400 --> 02:31:56,000
this Paul NER and this is going to take that single doc object because

2270
02:31:56,000 --> 02:31:59,800
remember this pipe needs to receive the doc object and do stuff to it.

2271
02:32:00,200 --> 02:32:03,100
So what we can do is we can take all this code that we just wrote.

2272
02:32:06,700 --> 02:32:14,300
From here down and paste it into our function and what we have is the

2273
02:32:14,300 --> 02:32:17,100
ability now to implement this as a custom pipe.

2274
02:32:18,200 --> 02:32:21,500
We don't need to do this because we don't want to print things off but

2275
02:32:21,500 --> 02:32:23,900
here we're going to return the doc object.

2276
02:32:23,900 --> 02:32:28,500
So we have now is a custom kind of entity ruler that uses regex across

2277
02:32:28,500 --> 02:32:29,900
multiple tokens.

2278
02:32:30,100 --> 02:32:34,700
If you want to use regex in spacey across multiple tokens as of spacey

2279
02:32:34,700 --> 02:32:37,500
3.1 this is the only way to implement this.

2280
02:32:38,100 --> 02:32:45,200
So now we can take this pipe and we can actually add it to a blank custom

2281
02:32:45,300 --> 02:32:45,800
model.

2282
02:32:46,100 --> 02:32:52,600
So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're

2283
02:32:52,600 --> 02:32:57,900
going to create a blank English model nlp2 dot add pipe.

2284
02:32:59,300 --> 02:33:01,500
We're going to add in Paul NER.

2285
02:33:05,100 --> 02:33:07,700
And now we see that we've actually created that successfully.

2286
02:33:07,700 --> 02:33:10,400
So we have one pipe kind of sitting in all of this.

2287
02:33:10,800 --> 02:33:14,500
Now what we can do is we can go through and we need to probably add in our

2288
02:33:14,500 --> 02:33:19,300
pattern as well here just for good practice because this should be

2289
02:33:19,300 --> 02:33:21,100
stored somewhat adjacent.

2290
02:33:21,100 --> 02:33:24,100
I like to sometimes to keep it up here when I'm doing this but you can

2291
02:33:24,100 --> 02:33:27,000
also keep it kind of inside of the function itself.

2292
02:33:28,200 --> 02:33:31,500
Let's go ahead and just kind of save that and we're going to rerun this

2293
02:33:32,300 --> 02:33:32,700
cool.

2294
02:33:33,400 --> 02:33:37,200
Now what we can do is we can say doc to is equal to nlp2 we're going

2295
02:33:37,200 --> 02:33:41,200
to go over that exact same text and we're going to print off our doc

2296
02:33:41,300 --> 02:33:46,300
to dot ints and we've now managed to implement that as a custom

2297
02:33:46,400 --> 02:33:49,500
spacey pipe but we've got one big problem.

2298
02:33:49,900 --> 02:33:55,900
Let's say just hypothetically we wanted to also kind of work in really

2299
02:33:55,900 --> 02:34:01,900
a another kind of something into our actual pipeline.

2300
02:34:01,900 --> 02:34:06,900
We wanted this pipeline to sit on top of maybe an existing spacey model

2301
02:34:07,300 --> 02:34:11,800
and for whatever reason we don't want Paul Hollywood to have that title.

2302
02:34:11,800 --> 02:34:13,200
We wanted to have the title.

2303
02:34:13,700 --> 02:34:17,000
Maybe we want to just kind of keep Paul Hollywood as a person but we

2304
02:34:17,000 --> 02:34:21,600
also want to find maybe other cinema style entities.

2305
02:34:21,600 --> 02:34:25,400
So we're going to create another entity here instead of all this that's

2306
02:34:25,400 --> 02:34:30,100
going to be something like let's go ahead and make a new a new container

2307
02:34:30,100 --> 02:34:32,200
down here a new component down here.

2308
02:34:33,100 --> 02:34:36,600
We're going to just look for any instance of Hollywood and we're going

2309
02:34:36,600 --> 02:34:39,300
to call that the word the label of cinema.

2310
02:34:39,800 --> 02:34:42,000
So I want to demonstrate this because this is going to show you

2311
02:34:42,000 --> 02:34:44,800
something that you are going to encounter when you try to implement

2312
02:34:44,800 --> 02:34:47,200
this in the real world and I'm going to show you how to kind of

2313
02:34:47,500 --> 02:34:49,500
address the problem that you're going to encounter.

2314
02:34:49,900 --> 02:34:53,100
So if we had a component that looked like this now it's going to look

2315
02:34:53,100 --> 02:34:57,200
for just instance instances of Hollywood and let's call this Holly

2316
02:34:58,900 --> 02:35:02,900
Cinema NER and change this here as well.

2317
02:35:03,400 --> 02:35:05,700
What we can do now is go ahead and load that up into memories.

2318
02:35:05,700 --> 02:35:09,500
We've got this new component called Cinema NER and just like before

2319
02:35:09,500 --> 02:35:12,200
we're going to create an LP three now this is going to be spacey dot

2320
02:35:12,200 --> 02:35:14,400
load in core web.

2321
02:35:15,500 --> 02:35:20,500
SM and so what this is going to do is it's going to load up the spacey

2322
02:35:20,500 --> 02:35:26,700
small model and LP three dot add pipe and it's going to be the what did

2323
02:35:26,700 --> 02:35:32,300
I call this again the cinema NER and if we were to go through

2324
02:35:32,300 --> 02:35:36,900
and add that and create a new object called doc three make that

2325
02:35:36,900 --> 02:35:39,900
equal to an LP three text.

2326
02:35:41,700 --> 02:35:45,600
We're going to get this air and this is a common air and if you

2327
02:35:45,600 --> 02:35:47,700
Google it you'll eventually find the right answer.

2328
02:35:47,700 --> 02:35:49,500
I'm just going to give it to you right now.

2329
02:35:49,800 --> 02:35:53,800
So what this is telling you is that there are spans that overlap

2330
02:35:54,800 --> 02:36:01,200
that don't actually work because one of the spans for cinema is Hollywood

2331
02:36:01,500 --> 02:36:07,500
and the small model is extracting not only that Hollywood as a cinema

2332
02:36:07,500 --> 02:36:11,600
but it's also extracting Paul Hollywood as part of a longer token.

2333
02:36:11,900 --> 02:36:17,100
So what's happened here is we're trying to assign a span to two of

2334
02:36:17,100 --> 02:36:19,400
the same tokens and that doesn't work in spacey.

2335
02:36:19,400 --> 02:36:21,500
It'll break so what can you do?

2336
02:36:21,700 --> 02:36:26,300
Well a common method of solving this issue is to work with the filter

2337
02:36:26,300 --> 02:36:28,900
spans from the spacey dot util.

2338
02:36:29,800 --> 02:36:33,900
Let's go ahead and do this right now so you can say from spacey dot

2339
02:36:33,900 --> 02:36:37,200
util import filter spans.

2340
02:36:37,400 --> 02:36:42,200
What filter spans allows for you to do is to actually filter out all

2341
02:36:42,200 --> 02:36:44,600
of the the spans that are being identified.

2342
02:36:44,900 --> 02:36:47,700
So what we can do is we can say at this stage.

2343
02:36:50,400 --> 02:36:55,400
Before you get to the dock dot ends you can say filtered is equal

2344
02:36:55,400 --> 02:36:59,400
to filter spans original ends.

2345
02:36:59,500 --> 02:37:00,500
So what does this do?

2346
02:37:00,500 --> 02:37:03,200
Well what this does is it goes through and looks at all of the

2347
02:37:03,200 --> 02:37:07,800
different start and end sections from all of your entities.

2348
02:37:08,200 --> 02:37:12,400
And if there is an ever an instance where there is a an overlap

2349
02:37:12,400 --> 02:37:16,700
of tokens so 8 to 10 and 9 to 10.

2350
02:37:17,300 --> 02:37:22,000
Primacy and priority is going to be given to the longer token.

2351
02:37:22,400 --> 02:37:25,800
So what we can do is we can set this now to filtered and it helps

2352
02:37:25,800 --> 02:37:28,200
if you call it correctly filtered.

2353
02:37:28,200 --> 02:37:28,800
There we go.

2354
02:37:29,800 --> 02:37:32,800
We can set that to filtered instead of the original entities.

2355
02:37:33,100 --> 02:37:34,200
Go ahead and save that.

2356
02:37:34,500 --> 02:37:38,700
We're going to add this again and we're going to do doc 3 and

2357
02:37:38,700 --> 02:37:43,200
we're going to say for int and doc 3 dot ends print int dot

2358
02:37:43,200 --> 02:37:45,100
text and int dot label.

2359
02:37:46,100 --> 02:37:50,400
And if we've done this correctly we're not going to see the

2360
02:37:50,600 --> 02:37:54,900
cinema label come out at all because Paul Hollywood is a

2361
02:37:54,900 --> 02:37:59,000
longer token than just Hollywood.

2362
02:37:59,000 --> 02:38:03,400
So what we've done is we've set told spacey give the primacy

2363
02:38:03,400 --> 02:38:07,600
to the longer tokens and assign that label by filtering out

2364
02:38:07,600 --> 02:38:10,800
the tokens you can prevent that air from ever surfacing.

2365
02:38:11,100 --> 02:38:14,100
But this is a very common thing that you're going to have to

2366
02:38:14,100 --> 02:38:18,300
implement sometimes rejects really is the easiest way to

2367
02:38:18,300 --> 02:38:20,700
inject and do pattern matching in the entity.

2368
02:38:21,300 --> 02:38:21,600
Okay.

2369
02:38:21,600 --> 02:38:24,300
So here's the scenario that we have before us in order to make

2370
02:38:24,300 --> 02:38:27,500
this live this kind of live coding and applied spacey a

2371
02:38:27,500 --> 02:38:30,300
little bit more interesting imagine in this scenario we

2372
02:38:30,300 --> 02:38:34,100
have a client and the client is a stockbroker or somebody

2373
02:38:34,100 --> 02:38:36,900
who's interested in investing and what they want to be able

2374
02:38:36,900 --> 02:38:39,800
to do is look at news articles like those coming out of Reuters

2375
02:38:40,200 --> 02:38:42,200
and they want to find the news articles that are the most

2376
02:38:42,200 --> 02:38:46,400
relevant to what they need to actually search for and read

2377
02:38:46,400 --> 02:38:46,900
for the day.

2378
02:38:46,900 --> 02:38:49,000
So they want to find the ones that deal with their their

2379
02:38:49,000 --> 02:38:52,600
personal stocks their holdings or maybe their the specific

2380
02:38:52,600 --> 02:38:54,300
index that they're actually interested in.

2381
02:38:54,600 --> 02:39:00,600
So what this client wants is a way to use spacey to automatically

2382
02:39:00,600 --> 02:39:05,500
find all companies referenced within a text all stocks

2383
02:39:05,500 --> 02:39:09,500
referenced within a text and all indexes referenced within

2384
02:39:09,500 --> 02:39:12,600
the next text and maybe even some stock exchanges as well.

2385
02:39:12,800 --> 02:39:15,800
Now on the actual textbook if you go through to this chapter

2386
02:39:15,800 --> 02:39:18,900
which is number 10 you're going to find all the kind of

2387
02:39:18,900 --> 02:39:22,200
solutions laid out for you what I'm going to do throughout

2388
02:39:22,200 --> 02:39:26,500
the next 30 or 40 minutes is kind of walk through how I might

2389
02:39:26,500 --> 02:39:28,800
solve this problem at least on the surface.

2390
02:39:28,800 --> 02:39:32,300
This is going to be a rudimentary solution that demonstrates

2391
02:39:32,300 --> 02:39:35,700
the power of spacey and how you can apply it in a very short

2392
02:39:35,700 --> 02:39:40,100
period of time to do some pretty custom tasks such as financial

2393
02:39:40,100 --> 02:39:44,000
analysis with that structured data that you've extracted you

2394
02:39:44,000 --> 02:39:47,700
can then do any number of things what we're going to start off

2395
02:39:47,700 --> 02:39:55,000
with though is importing spacey and importing pandas as PD if

2396
02:39:55,000 --> 02:39:57,900
you're not familiar with pandas I've got a whole tutorial

2397
02:39:57,900 --> 02:40:00,900
series on that on my channel Python tutorials for digital

2398
02:40:00,900 --> 02:40:03,700
humanities even though it has digital humanities in the title

2399
02:40:03,700 --> 02:40:06,900
it's for kind of everyone but go through if you're not familiar

2400
02:40:06,900 --> 02:40:09,500
with pandas and check that out you're not really going to need

2401
02:40:09,500 --> 02:40:13,000
it for for this video here you're going to just need to

2402
02:40:13,000 --> 02:40:17,000
understand that I'm using pandas to access and grab the data

2403
02:40:17,000 --> 02:40:21,200
that I need from a couple CSV files or comma separated value

2404
02:40:21,200 --> 02:40:22,200
files that I have.

2405
02:40:23,400 --> 02:40:25,400
So the first thing that we need to do is we need to create

2406
02:40:25,400 --> 02:40:28,400
what's known as a pandas data frame and this is going to be

2407
02:40:28,400 --> 02:40:32,700
equal to PD dot read CSV and I actually have these stored in

2408
02:40:32,700 --> 02:40:36,700
the data sub folder in the repo you have free access to these

2409
02:40:36,700 --> 02:40:39,500
they're a little tiny data sets that I cultivated pretty

2410
02:40:39,500 --> 02:40:42,300
quickly they're not perfect but they're good enough for our

2411
02:40:42,300 --> 02:40:45,900
purposes and we're going to use the separator keyword argument

2412
02:40:45,900 --> 02:40:49,500
which is going to say to separate everything out by tab

2413
02:40:49,500 --> 02:40:53,400
because these are CSV files tab separated value files and we

2414
02:40:53,400 --> 02:40:56,100
have something that looks like this so what this stocks dot

2415
02:40:56,100 --> 02:41:00,100
CSV file is is it's all the symbols company names industry

2416
02:41:00,100 --> 02:41:03,300
and market caps for I think it's around five thousand seven

2417
02:41:03,300 --> 02:41:05,700
hundred different stocks five thousand eight hundred and seventy

2418
02:41:05,700 --> 02:41:08,900
nine and so what we're going to use this for is as a way to

2419
02:41:08,900 --> 02:41:13,300
start working into an entity ruler all these different symbols

2420
02:41:13,300 --> 02:41:16,400
and company names what we want to do is we want to use these

2421
02:41:16,400 --> 02:41:19,900
symbols to work into a model as a way to grab stocks that might

2422
02:41:19,900 --> 02:41:22,400
be referenced and you can already probably start to see a

2423
02:41:22,400 --> 02:41:25,400
problem with this capital a here we're going to get to that

2424
02:41:25,400 --> 02:41:27,900
in a little bit and we want to grab all the company names

2425
02:41:27,900 --> 02:41:30,300
so we can maybe create two different entity types from

2426
02:41:30,300 --> 02:41:35,900
this data set stock and company so let's go through and make

2427
02:41:35,900 --> 02:41:39,100
these into lists so they're a little bit more so let's go

2428
02:41:39,100 --> 02:41:41,500
through and make these into lists so they're a little bit

2429
02:41:41,500 --> 02:41:46,100
more manageable what we need to do is we need to create a list

2430
02:41:46,100 --> 02:41:50,500
of symbols and that's going to be equal to DF dot symbol dot

2431
02:41:50,500 --> 02:41:53,900
two list this is a great way to do it and pandas so you can

2432
02:41:53,900 --> 02:41:57,300
kind of easily convert all these different columns into

2433
02:41:58,500 --> 02:42:01,400
different lists that you can work with in Python so companies

2434
02:42:01,400 --> 02:42:05,100
is going to be equal to DF dot company and name I believe

2435
02:42:05,100 --> 02:42:08,800
the name was two list and just to demonstrate how this works

2436
02:42:08,800 --> 02:42:12,700
let's print off symbols we're going to print up to 10 and

2437
02:42:12,700 --> 02:42:15,400
you can kind of see we've managed to take these columns now

2438
02:42:15,400 --> 02:42:19,100
and kind of build them into a simple Python list so what can

2439
02:42:19,100 --> 02:42:22,100
we do with that well one of the things that we can do is we

2440
02:42:22,100 --> 02:42:25,900
can use that information to start cultivating an entity

2441
02:42:25,900 --> 02:42:29,700
ruler but remember we want more things than just one or two

2442
02:42:29,700 --> 02:42:32,700
kind of in our entity ruler we don't just want stocks and we

2443
02:42:32,700 --> 02:42:35,900
don't just want companies we also want things like indexes

2444
02:42:35,900 --> 02:42:37,800
we're going to get to that in just a second though for right

2445
02:42:37,800 --> 02:42:41,600
now let's try to work these two things into an entity ruler

2446
02:42:41,600 --> 02:42:45,300
how might we go about doing that well as you might expect

2447
02:42:45,300 --> 02:42:49,000
we're going to create a fairly simple entity ruler so we're

2448
02:42:49,000 --> 02:42:51,800
going to say is nlp is going to be equal to spacey dot blank

2449
02:42:51,800 --> 02:42:54,500
we don't need a lot of fancy features here we're just going

2450
02:42:54,500 --> 02:42:58,200
to have a blank model that's just going to hose host an

2451
02:42:58,700 --> 02:43:02,900
single entity ruler that's going to be equal to nlp dot add

2452
02:43:03,500 --> 02:43:08,100
underscore pipe and this is going to be entity ruler and now

2453
02:43:08,100 --> 02:43:10,700
what we need to do is we need to come up with a way to go

2454
02:43:10,700 --> 02:43:14,800
through all of these different symbols and add them in so we

2455
02:43:14,800 --> 02:43:20,200
can say for symbol and symbols we want to say patterns dot

2456
02:43:20,200 --> 02:43:23,700
append and we're going to make a an empty list of patterns

2457
02:43:23,700 --> 02:43:29,100
up here and what we're going to append is that dictionary that

2458
02:43:29,100 --> 02:43:32,100
you met when we talked about the entity ruler and I believe

2459
02:43:32,100 --> 02:43:36,100
it was chapter five yeah and what this is going to have

2460
02:43:36,100 --> 02:43:39,000
there are two things label which is going to correspond to

2461
02:43:39,100 --> 02:43:44,600
stock in this case and it's going to have a pattern and

2462
02:43:44,600 --> 02:43:49,000
that's going to correspond to the pattern of the symbol so

2463
02:43:49,000 --> 02:43:52,500
we're going to say symbol and what that lets us do is kind of

2464
02:43:52,500 --> 02:43:56,100
go through and easily create and add these patterns and and

2465
02:43:56,100 --> 02:43:59,500
we can do the same thing for company remember it's never a

2466
02:43:59,500 --> 02:44:03,200
good idea to copy and paste in your code I am simply doing

2467
02:44:03,200 --> 02:44:06,200
it for demonstration purposes right now this is not polished

2468
02:44:06,200 --> 02:44:09,600
code by any stretch of the imagination and what we can do

2469
02:44:09,600 --> 02:44:12,000
here now is we can do the same thing loop over the different

2470
02:44:12,000 --> 02:44:15,600
companies and add each company and so what this is doing is

2471
02:44:15,600 --> 02:44:18,500
it's creating a large list of different patterns that the

2472
02:44:18,500 --> 02:44:22,500
entity ruler will use to then go through and as we create the

2473
02:44:22,500 --> 02:44:27,100
a doc object over that sample Reuters text I just showed you

2474
02:44:27,100 --> 02:44:30,200
a second ago which we should probably just go ahead and

2475
02:44:30,200 --> 02:44:33,100
pull up right now I'm going to copy and paste it straight

2476
02:44:33,100 --> 02:44:35,100
from the textbook.

2477
02:44:38,600 --> 02:44:41,600
Let's go ahead and execute that cell and we're going to add

2478
02:44:41,600 --> 02:44:45,000
in this text here it is a little lengthy but it'll be all

2479
02:44:45,000 --> 02:44:47,800
right and what we're going to do now is we're going to iterate

2480
02:44:47,800 --> 02:44:51,000
over create a doc object to iterate over all of that.

2481
02:44:54,000 --> 02:44:57,800
And our goal here is going to be able to say for int and doc

2482
02:44:57,800 --> 02:45:01,800
dot ends we want to have extracted all of these different

2483
02:45:02,100 --> 02:45:08,100
entities so we can say print off and dot text and dot label

2484
02:45:10,000 --> 02:45:11,300
and let's see if we succeeded.

2485
02:45:15,400 --> 02:45:19,000
And we have to add in our patterns to our entity ruler so

2486
02:45:19,000 --> 02:45:22,600
remember we can do this by saying ruler dot add patterns.

2487
02:45:24,900 --> 02:45:26,000
Patterns there we go.

2488
02:45:27,000 --> 02:45:32,000
That's what this error actually means and now when we do it

2489
02:45:32,000 --> 02:45:36,500
we see that we've been able to extract Apple as a company

2490
02:45:36,500 --> 02:45:40,700
Apple as a company Nasdaq everything's looking pretty good

2491
02:45:40,700 --> 02:45:43,900
but I notice really quickly that I wasn't actually able to

2492
02:45:43,900 --> 02:45:48,800
extract Apple as a stock and I've also got another problem

2493
02:45:49,100 --> 02:45:54,700
I've extracted to the lowercase TWO as a stock as well why

2494
02:45:54,700 --> 02:45:57,900
have these two things are as a company. Well it turns out in

2495
02:45:57,900 --> 02:46:03,500
our data set we've got to TWO that is a company name that's

2496
02:46:03,500 --> 02:46:07,200
almost always going to be a false positive and we know that

2497
02:46:07,200 --> 02:46:10,100
that kind of thing might be better off worked into a machine

2498
02:46:10,100 --> 02:46:13,100
learning model for right now though we're going to work

2499
02:46:13,100 --> 02:46:15,700
under the presumption that anytime we encounter this kind

2500
02:46:15,700 --> 02:46:19,500
of obscure company TWO as a lowercase it's going to be a

2501
02:46:19,500 --> 02:46:22,900
false positive. I also have another problem I know for a

2502
02:46:22,900 --> 02:46:28,500
fact that Apple the stock is referenced within this text to

2503
02:46:28,500 --> 02:46:31,400
make it a little easier. Let's see it right here and notice

2504
02:46:31,400 --> 02:46:33,900
that it didn't find it to make this a little easier to

2505
02:46:33,900 --> 02:46:37,300
display. Let's go ahead and display what we're looking at

2506
02:46:37,500 --> 02:46:41,700
as the splacy render so what we can do is we can use that the

2507
02:46:41,700 --> 02:46:45,200
splacy render that we met a little bit ago in this video.

2508
02:46:46,700 --> 02:46:49,700
So in order to import this if you remember we need to say

2509
02:46:50,700 --> 02:46:56,800
from spacey import display see and that's going to allow us

2510
02:46:56,800 --> 02:47:00,200
to actually display our entities. Let's go ahead and put

2511
02:47:00,200 --> 02:47:03,400
this however on a different cell just so we don't have to

2512
02:47:03,400 --> 02:47:09,400
execute that every time and we're going to say at splacy render

2513
02:47:09,600 --> 02:47:13,500
and we're going to render the doc object with a style that's

2514
02:47:13,500 --> 02:47:17,800
equal to ENT and we can see that we've got our text now

2515
02:47:17,800 --> 02:47:21,300
popping out with our things labeled and you can see pretty

2516
02:47:21,300 --> 02:47:24,100
quickly where we've made some mistakes where we need to

2517
02:47:24,100 --> 02:47:28,300
incorporate some things into our entity ruler. So for example

2518
02:47:28,300 --> 02:47:31,500
if I'm scrolling through this is gray little ugly we can change

2519
02:47:31,500 --> 02:47:34,500
the colors that's beyond the scope of this video though but

2520
02:47:34,500 --> 02:47:37,900
let's keep on going down we notice that we have Apple dot

2521
02:47:37,900 --> 02:47:41,600
IO and yet this has been missed by our entity ruler. Why has

2522
02:47:41,600 --> 02:47:46,400
this been missed well. Spacey as a tokenizer is seeing this

2523
02:47:46,400 --> 02:47:52,200
as a single token so Apple dot Oh the letter Oh capital letter

2524
02:47:52,200 --> 02:47:55,900
Oh why is that well I didn't know about this but apparently

2525
02:47:55,900 --> 02:47:59,800
it does has to deal with kind of the way in which stock indices

2526
02:47:59,800 --> 02:48:03,500
are I think it's on the NASDAQ kind of structure things so

2527
02:48:03,500 --> 02:48:06,200
what can we do well we've got a couple different options here

2528
02:48:06,600 --> 02:48:09,400
I know that these go through all different letters from A to

2529
02:48:09,400 --> 02:48:13,000
Z so we can either work with the string library or we can do

2530
02:48:13,000 --> 02:48:16,500
is we can import a quick list that I've already written out

2531
02:48:16,800 --> 02:48:20,000
of all the different letters of the alphabet and iterate

2532
02:48:20,000 --> 02:48:23,800
through those with our ruler up here.

2533
02:48:26,000 --> 02:48:29,100
Let's go ahead and add these letters right there and we can

2534
02:48:29,100 --> 02:48:31,100
kind of iterate through those and whenever a stock kind of

2535
02:48:31,100 --> 02:48:35,800
pops out with that kind of symbol plus any occurrence where

2536
02:48:35,800 --> 02:48:40,400
it's got a period followed by a letter in those scenarios we

2537
02:48:40,400 --> 02:48:44,600
want that to be flagged as a stock as well so what we can do

2538
02:48:44,600 --> 02:48:47,400
is we can add in another thing right here add in another

2539
02:48:47,400 --> 02:48:51,800
pattern and this is now going to be symbol plus we're going

2540
02:48:51,800 --> 02:48:55,900
to add in F string right here a formatted string any occurrence

2541
02:48:55,900 --> 02:49:05,300
of L we can set up a loop to say for L and letters do this

2542
02:49:05,700 --> 02:49:09,400
and what this is going to allow us to do is to look for any

2543
02:49:09,400 --> 02:49:14,900
instance where there is a symbol followed by a period

2544
02:49:14,900 --> 02:49:18,400
followed by one of these capitalized letters that I just

2545
02:49:18,400 --> 02:49:22,400
copied and pasted in so if we do that we can execute that cell

2546
02:49:24,400 --> 02:49:27,600
and we can scroll down and we can now do the exact same thing

2547
02:49:27,600 --> 02:49:31,800
that we just did a second ago and actually display this

2548
02:49:32,800 --> 02:49:40,000
and now we're finding these stocks highlighted as stock so

2549
02:49:40,000 --> 02:49:42,800
we're successfully getting these stocks and extracting them

2550
02:49:42,800 --> 02:49:45,800
we've got a few different things that our client wants to

2551
02:49:45,800 --> 02:49:49,000
also extract though they don't want to just extract companies

2552
02:49:49,000 --> 02:49:53,200
and they don't want to just extract stock and they want to

2553
02:49:53,200 --> 02:49:57,200
also extract stock exchanges and indexes but we have one

2554
02:49:57,200 --> 02:50:00,500
other problem and go ahead and get rid of this as the display

2555
02:50:00,500 --> 02:50:03,600
mode and switch back to just our set of entities because

2556
02:50:03,600 --> 02:50:06,400
it's a little easier to read for this example we've got

2557
02:50:06,400 --> 02:50:08,400
another problem and we see we have a couple other stocks

2558
02:50:08,400 --> 02:50:12,600
popping out we now know that Kroger stock is here the n i

2559
02:50:12,600 --> 02:50:16,000
o dot n stock is in this text as well now we're starting to

2560
02:50:16,000 --> 02:50:19,700
see a greater degree of specificity for right now I'm

2561
02:50:19,700 --> 02:50:24,000
going to include two as a set of a stop technical term would

2562
02:50:24,000 --> 02:50:26,500
be like a stop or something that I don't want to be included

2563
02:50:26,500 --> 02:50:29,700
into the model so I'm going to make a list of stops and

2564
02:50:29,700 --> 02:50:32,400
we're just going to include two in that and we're going to

2565
02:50:32,400 --> 02:50:40,200
say for company and companies do all this if company not in

2566
02:50:40,200 --> 02:50:44,900
stops we want this to occur what this means now is that our

2567
02:50:44,900 --> 02:50:47,900
our pipeline while going through and having all of these

2568
02:50:47,900 --> 02:50:50,900
different things all these different rules it's also going

2569
02:50:50,900 --> 02:50:54,200
to have another rule that looks to see if there's a stop or

2570
02:50:54,200 --> 02:50:58,400
if this company name is this stop and if it is then we want

2571
02:50:58,400 --> 02:51:03,000
it to just kind of skip over and ignore it and if we go

2572
02:51:03,000 --> 02:51:05,800
through we notice that now we've successfully eliminated

2573
02:51:05,800 --> 02:51:10,300
this what we would presume to be a consistent false positive

2574
02:51:10,300 --> 02:51:13,100
something that's going to come up again and again as a false

2575
02:51:13,100 --> 02:51:17,100
positive great so we've been able to get this where it works

2576
02:51:17,100 --> 02:51:20,300
now pretty well what I also want to work into this model if

2577
02:51:20,300 --> 02:51:23,600
you remember though are things like indexes fortunately

2578
02:51:23,600 --> 02:51:28,200
I've also provided for us a list of all different indexes that

2579
02:51:28,200 --> 02:51:31,500
are available from I believe it's like everything like the

2580
02:51:31,500 --> 02:51:35,700
Dow Jones is about 13 or 14 of them let's go ahead and import

2581
02:51:35,700 --> 02:51:41,300
those up above and let's do that right here in this cell so

2582
02:51:41,300 --> 02:51:43,800
it kind of goes in sequential order that follows better with

2583
02:51:43,800 --> 02:51:46,900
the textbook to so it's a new data frame object this is going

2584
02:51:46,900 --> 02:51:50,800
to be equal to P a PD dot read CSV we're going to read in that

2585
02:51:50,800 --> 02:51:53,700
data file that I've given us and that's going to be the indexes

2586
02:51:53,700 --> 02:51:59,900
dot T SV with a separator that's equal to a tab let's see what

2587
02:51:59,900 --> 02:52:04,000
that looks like and this is what it looks like so all these

2588
02:52:04,000 --> 02:52:06,900
different indices now I know I'm going to have a problem right

2589
02:52:06,900 --> 02:52:11,100
out of the gate and that's going to be that sometimes you're

2590
02:52:11,100 --> 02:52:14,600
going to see things referenced as SNP 500 I don't know a lot

2591
02:52:14,600 --> 02:52:17,100
about finances but I know that you don't always see it as

2592
02:52:17,100 --> 02:52:22,200
SNP 500 index but I do think that these index symbols are

2593
02:52:22,200 --> 02:52:25,700
also going to be useful so like I did before I'm going to convert

2594
02:52:25,700 --> 02:52:28,500
these things into a list so it's a little easier for me to work

2595
02:52:28,500 --> 02:52:33,900
with in a for loop and I'm going to say indexes is equal to

2596
02:52:33,900 --> 02:52:41,800
DF2 dot index name so grabbing that column to list and index

2597
02:52:41,800 --> 02:52:48,400
symbols is equal to DF2 dot index symbol dot to list and

2598
02:52:48,400 --> 02:52:51,500
both of these are going to be different and they're both going

2599
02:52:51,500 --> 02:52:56,200
to have the same exact entity label which is going to be an

2600
02:52:56,200 --> 02:53:00,200
index and so let's go ahead and iterate over these and add them

2601
02:53:00,200 --> 02:53:03,300
in as well so I'm going to go ahead and do that right now

2602
02:53:04,000 --> 02:53:13,700
for indexes and indexes we want this label to be index we

2603
02:53:13,700 --> 02:53:17,700
want this to be index here so it's going to allow us to kind

2604
02:53:17,700 --> 02:53:20,200
of go through and grab all those and we want to do the same

2605
02:53:20,200 --> 02:53:26,000
thing with index symbols keep these a little separated here

2606
02:53:26,000 --> 02:53:32,300
index symbols and that allows for us to do that and let's go

2607
02:53:32,300 --> 02:53:35,200
ahead and without making any adjustments let's see let's see

2608
02:53:35,200 --> 02:53:37,900
how this does with these new patterns that we've added in

2609
02:53:38,400 --> 02:53:41,100
and because we've already got this text loaded into memory

2610
02:53:41,100 --> 02:53:44,500
I'm going to go ahead and put this right here doc is going to

2611
02:53:44,500 --> 02:53:52,900
be equal to nlp text for int and doc and print off and dot

2612
02:53:52,900 --> 02:53:58,400
text and dot label and we can kind of go through and we're

2613
02:53:58,400 --> 02:54:02,200
actually now able to extract some indexes and I believe when

2614
02:54:02,200 --> 02:54:05,200
I was looking at this text really quickly though I noticed

2615
02:54:05,200 --> 02:54:09,600
that there was one instance at least where we had not only

2616
02:54:09,600 --> 02:54:15,300
the index referenced but also a name like S&P 500 right here

2617
02:54:15,300 --> 02:54:18,700
S&P 500 notice that it isn't found because it doesn't have

2618
02:54:18,700 --> 02:54:21,800
the name index after it and notice also that none of our

2619
02:54:22,300 --> 02:54:25,000
our symbols are being found because they all seem to be

2620
02:54:25,000 --> 02:54:31,500
preceded by a dot so in this case a dot J a DJI and so that's

2621
02:54:31,500 --> 02:54:33,600
something else that I have to work into this model and the

2622
02:54:33,600 --> 02:54:37,000
list I gave the data set that's not there so I need to collect

2623
02:54:37,100 --> 02:54:40,600
a list of these different names and work those into an entity

2624
02:54:40,600 --> 02:54:43,500
ruler as well but for right now let's ignore that and focus

2625
02:54:43,500 --> 02:54:48,500
on including this S&P 500 so how can I get the S&P 500 in

2626
02:54:48,500 --> 02:54:51,800
there from the list I already gave it well what I can do is

2627
02:54:51,800 --> 02:54:56,800
I can say okay so under these indices not only do I want to

2628
02:54:56,800 --> 02:55:00,300
add that specific pattern let's go ahead and break these things

2629
02:55:00,300 --> 02:55:03,700
up into different words and so I'm going to have the words is

2630
02:55:03,700 --> 02:55:07,400
equal to index dot split and then I'm going to make a

2631
02:55:07,400 --> 02:55:13,600
presumption that the the first two words so the S&P 500 the

2632
02:55:13,600 --> 02:55:18,800
S&P 400 are sometimes going to be referenced by themselves so

2633
02:55:18,800 --> 02:55:21,400
what I want to do is I want to work that into the model as

2634
02:55:21,400 --> 02:55:26,600
well and I want to say we're going to say patterns dot

2635
02:55:26,600 --> 02:55:34,700
append copy this as well we can say something like dot join

2636
02:55:36,300 --> 02:55:41,800
words up until the second index and let's go ahead and work

2637
02:55:41,800 --> 02:55:45,700
that into our model in our patterns or pipeline and print

2638
02:55:45,700 --> 02:55:50,300
off our NLP again and you'll find that we've now been able to

2639
02:55:50,300 --> 02:55:56,300
capture things like S&P 500 that aren't proceeded by the word

2640
02:55:56,300 --> 02:56:00,700
index and we see that we in fact have S&P 500 is now popping

2641
02:56:00,700 --> 02:56:04,100
out time and again that's fantastic I'm pretty happy with

2642
02:56:04,100 --> 02:56:06,800
that now we're we're getting a deeper sense of what this

2643
02:56:06,800 --> 02:56:09,700
text is about without actually having to read it we know that

2644
02:56:09,700 --> 02:56:12,700
it's going to deal heavily with Apple and we know that it's

2645
02:56:12,700 --> 02:56:15,000
also going to tangentially deal with some of these other

2646
02:56:15,000 --> 02:56:19,200
things as well but I also want to include into this into this

2647
02:56:19,200 --> 02:56:23,200
pipeline the ability for the entity ruler to not just find

2648
02:56:23,200 --> 02:56:26,200
these things but I also wanted to be able to find different

2649
02:56:26,200 --> 02:56:30,000
stock exchanges so I've got a list I cultivated for different

2650
02:56:30,000 --> 02:56:33,800
stock exchanges which are things like NYSE things like that

2651
02:56:33,800 --> 02:56:40,500
so I can say DS3 is going to be equal to PD dot read CSV data

2652
02:56:40,500 --> 02:56:47,800
backslash stock exchanges dot TSV and then the separator is

2653
02:56:47,800 --> 02:56:51,300
going to be again a tab and let's take a look at what this

2654
02:56:51,300 --> 02:56:52,000
looks like.

2655
02:56:54,700 --> 02:56:56,000
Stanges there we go.

2656
02:57:00,600 --> 02:57:03,300
There we are and we have something that looks like this

2657
02:57:03,300 --> 02:57:09,600
a pretty a pretty large CSV file CSV file sorry that's got a

2658
02:57:09,600 --> 02:57:13,100
bunch of different rows the ones I'm most interested in well

2659
02:57:13,100 --> 02:57:16,700
there's a couple actually I'm interested in specifically the

2660
02:57:16,700 --> 02:57:20,900
Google Prefix and this description the description has

2661
02:57:20,900 --> 02:57:24,400
the actual name and the Prefix has this really nice abbreviation

2662
02:57:24,400 --> 02:57:27,900
that I've seen pop out a few different times such as Nasdaq

2663
02:57:27,900 --> 02:57:31,100
here if we keep on going down we would see different things

2664
02:57:31,100 --> 02:57:35,200
as well NYSE these are kind of different stock exchanges.

2665
02:57:36,000 --> 02:57:40,600
So let's pop back down here and let's go ahead and convert

2666
02:57:40,600 --> 02:57:44,600
those two things into individual lists as well so we're going

2667
02:57:44,600 --> 02:57:49,200
to say exchanges it's going to be equal to DF3 dot ISO Mike

2668
02:57:49,900 --> 02:57:56,800
dot to list and then I'm also going to grab the F3 dot sorry

2669
02:57:56,800 --> 02:58:00,200
Google I have to do this as a dictionary because it's the

2670
02:58:00,200 --> 02:58:03,700
way the data sets cultivated it's got a space in the middle

2671
02:58:03,700 --> 02:58:07,900
this is a common problem that you run into and then I also

2672
02:58:07,900 --> 02:58:12,100
want to know grab all of these exchanges as well so I'm going

2673
02:58:12,100 --> 02:58:20,400
to say also on top of that DF3 dot description to list so I'm

2674
02:58:20,400 --> 02:58:26,700
making a large list exchanges and I get this here because it

2675
02:58:26,700 --> 02:58:30,400
says Google Prefix isn't an actual thing and in fact it's

2676
02:58:30,400 --> 02:58:34,500
prefix with an I and now we actually are able to get all

2677
02:58:34,500 --> 02:58:39,400
these things extracted so what I want to do now is I want to

2678
02:58:39,400 --> 02:58:43,300
work all these different symbols and descriptions into into

2679
02:58:43,300 --> 02:58:46,500
the model as well or into the pipeline as well so I can say

2680
02:58:46,500 --> 02:58:54,500
for for E and exchanges I want to say patterns dot append

2681
02:58:57,700 --> 02:59:01,700
and I want to do a label that's going to be let's do stock

2682
02:59:02,400 --> 02:59:07,300
exchange and then the next thing I want to do is a pattern

2683
02:59:07,600 --> 02:59:11,100
and that's going to be equal to in this case E as we're going

2684
02:59:11,100 --> 02:59:14,600
to see this is not adequate enough we need to do a few

2685
02:59:14,600 --> 02:59:18,100
different things to really kind of work this out but it's going

2686
02:59:18,100 --> 02:59:20,200
to be a good enough to at least get started

2687
02:59:25,200 --> 02:59:26,600
and it's going to take it just a second

2688
02:59:32,000 --> 02:59:34,200
and the main thing that's happening right now are these

2689
02:59:34,200 --> 02:59:38,200
different for loops so if we keep on going down we now see

2690
02:59:38,200 --> 02:59:41,800
that we were able to extract the NYSE stock exchange so we've

2691
02:59:41,800 --> 02:59:44,600
not only been able to work into a pipeline in a very short

2692
02:59:44,600 --> 02:59:47,200
order maybe about 20 30 minutes we've been able to work

2693
02:59:47,200 --> 02:59:50,600
into a pipeline all of these different things that are coming

2694
02:59:50,600 --> 02:59:54,000
out we do however see a couple problems and this is where I'm

2695
02:59:54,000 --> 02:59:56,000
going to leave it though because you've got the basic

2696
02:59:56,000 --> 02:59:59,600
mechanics down now comes time for you being a domain expert

2697
02:59:59,800 --> 03:00:02,100
to work out and come up with rules to solve some of these

2698
03:00:02,100 --> 03:00:06,300
problems Nasdaq is not a company so there's a problem with

2699
03:00:06,300 --> 03:00:10,100
the data set or Nasdaq is listed as a company name and one of

2700
03:00:10,100 --> 03:00:13,500
the data sets we need to work that out where Nasdaq is never

2701
03:00:13,500 --> 03:00:17,100
referenced as a company we have the S&P and is now being

2702
03:00:17,100 --> 03:00:20,200
coming out correctly as S&P 500 there might be instances

2703
03:00:20,200 --> 03:00:23,600
where just S&P is referenced which I think in that context

2704
03:00:23,600 --> 03:00:27,100
would probably be the S&P 500 but nevertheless we've been

2705
03:00:27,100 --> 03:00:33,200
able to actually extract these things sometimes the Dow Jones

2706
03:00:33,200 --> 03:00:37,800
Industrial Average might just be referenced to Dow Jones so

2707
03:00:37,800 --> 03:00:40,400
this index might just be these first two words I know that's

2708
03:00:40,400 --> 03:00:42,900
a common occurrence we've also seen that we weren't able to

2709
03:00:42,900 --> 03:00:45,700
extract some of those things that were a period followed by

2710
03:00:45,700 --> 03:00:50,100
a symbol that referenced the actual index itself nevertheless

2711
03:00:50,100 --> 03:00:52,600
this is a really good starting point and you can see how just

2712
03:00:52,600 --> 03:00:55,200
in a few minutes you're able to generate this thing that can

2713
03:00:55,200 --> 03:00:59,700
extract information from unstructured text at the end of

2714
03:00:59,700 --> 03:01:02,100
the day like I said in the introduction to this entire

2715
03:01:02,100 --> 03:01:07,100
video that's one of the essential tasks of NLP designing

2716
03:01:07,100 --> 03:01:10,600
this and implementing it is pretty quick and easy perfecting

2717
03:01:10,600 --> 03:01:14,200
it is where the time really is to get this financial analysis

2718
03:01:14,800 --> 03:01:19,000
entity ruler working really well where it has almost no false

2719
03:01:19,000 --> 03:01:24,600
positives and almost never misses a true a true positive it

2720
03:01:24,600 --> 03:01:27,100
would take maybe a few more hours of just some kind of working

2721
03:01:27,100 --> 03:01:29,300
and eventually there are certain things you might find that

2722
03:01:29,300 --> 03:01:32,600
would work better in a machine learning model nevertheless

2723
03:01:32,600 --> 03:01:36,000
you can see the degree to which rules based approaches in

2724
03:01:36,000 --> 03:01:39,600
Spacey can really accomplish some pretty robust tasks with

2725
03:01:39,600 --> 03:01:43,400
minimal minimal amount of code so long as you have access to

2726
03:01:43,400 --> 03:01:46,400
or have already cultivated the data sets required.

2727
03:01:49,100 --> 03:01:52,900
Thank you so much for watching this video series on Spacey

2728
03:01:52,900 --> 03:01:55,700
an introduction to basic concepts of natural language

2729
03:01:55,700 --> 03:02:00,700
processing linguistic annotations in Spacey vectors pipelines

2730
03:02:00,700 --> 03:02:03,800
and kind of rules based Spacey you've enjoyed this video

2731
03:02:03,800 --> 03:02:07,200
please like and subscribe down below and if you've also found

2732
03:02:07,200 --> 03:02:10,600
this video useful consider joining me on my channel Python

2733
03:02:10,600 --> 03:02:13,700
tutorials for digital humanities if you have like this and

2734
03:02:13,700 --> 03:02:17,300
found this video useful I'm envisioning a second part to

2735
03:02:17,300 --> 03:02:21,500
this video where I go with the machine learning aspects of

2736
03:02:21,500 --> 03:02:24,000
Spacey if you're interested in that let me know in the

2737
03:02:24,000 --> 03:02:26,700
comments down below and I'll make a second video that

2738
03:02:26,700 --> 03:02:28,000
corresponds to this one.

2739
03:02:28,700 --> 03:02:30,600
Thank you for watching and have a great day.

