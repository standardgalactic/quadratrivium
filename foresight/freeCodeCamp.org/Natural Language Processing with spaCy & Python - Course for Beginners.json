{"text": " In this course, you will learn all about natural language processing and how to apply it to real-world problems using the Spacey Library. Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher. Hi, and welcome to this video. My name is Dr. William Mattingly, and I specialize in multilingual natural language processing. I come to NLP from a humanities perspective. I have my PhD in medieval history. But I use Spacey on a regular basis to do all of my NLP needs. So what you're going to get out of this video over the next few hours is a basic understanding of what natural language processing is, or NLP, and also how to apply it to domain-specific problems, or problems that exist within your own area of expertise. I happen to use this all the time to analyze historical documents or financial documents for my own personal investments. Over the next few hours, you're going to learn a lot about NLP, language as a whole, and most importantly, the Spacey Library. I like the Spacey Library because it's easy to use and easy to also implement really kind of general solutions to general problems with the off-the-shelf models that are already available to you. I'm going to walk you through, in part one of this video series, how to get the most out of Spacey with these off-the-shelf features. In part two, we're going to start tackling some of the features that don't exist in off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components in Spacey, to actually solve domain-specific problems in your own area, from the entity ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects patterns in a custom Spacey component that doesn't actually exist at the moment. I'm going to be showing you all that in part two, so that in part three, we can take the lessons that we learned in part one and part two, and actually apply them to solve a very kind of common problem that exists in an LP, and that is information extraction from financial documents. So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges. If you join me over the next few hours, you will leave this lesson with a good understanding of Spacey, and also a good understanding of kind of the off-the-shelf components that are there, and a way to take the off-the-shelf components and apply them to your own domain. If you also join me in this video and you like it, please let me know in the comments down below, because I am interested in making a second part to this video that will explore not only the rules-based aspects of Spacey, but the machine learning-based aspects of Spacey. So teaching you how to train your own models to do your own things, such as training a dependency parser, training a named entity recognizer, things like this, which are not covered in this video. Nevertheless, if you join me for this one and you like it, you will find part two much easier to understand. So sit back, relax, and let's jump into what NLP is, what kind of things you can do with NLP, such as information extraction, and what the Spacey library is, and how this course will be laid out. If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital Humanities, which is linked in the description down below. Even if you're not a digital humanist like me, you will find these Python tutorials useful because they take Python and make it accessible to students of all levels, specifically those who are beginners. I walk you through not only the basics of Python, but also I walk you through step-by-step some of the more common libraries that you need. A lot of the channel deals with texts or text-based problems, but other content deals with things like machine learning and image classification and OCR, all in Python. So before we begin with Spacey, I think we should spend a little bit of time talking about what NLP or natural language processing actually is. Natural language processing is the process by which we try to get a computer system to understand and parse and extract human language, often times with raw text. There are a couple different areas of natural language processing. There's named entity recognition, part of speech tagging, syntactic parsing, text categorization, also known as text classification, co-reference resolution, machine translation. Adjacent to NLP is another kind of computational linguistics field called natural language understanding, or NLU. This is where we train computer systems to do things like relation extraction, semantic parsing, question and answering, summarization, sentiment analysis, and paraphrasing. NLP and NLU are used by a wide array of industries, from finance industry all the way through to law and academia, with researchers trying to do information extraction from texts. Within NLP, there's a couple different applications. The first and probably the most important is information extraction. This is the process by which we try to get a computer system to extract information that we find relevant to our own research or needs. So for example, as we're going to see in part three of this video, when we apply spacey to the financial sector, a person interested in finances might need NLP to go through and extract things like company names, stocks, indexes. Things that are referenced within maybe news articles, from Reuters to New York Times to Wall Street Journal. This is an example of using NLP to extract information. A good way to think about NLP's application in this area is it takes in some unstructured data, in this case raw text, and extracts structured data from it, or metadata. So it finds the things that you want it to find and extracts them for you. Now while there's ways to do this with gazetteers and list matching, using an NLP framework like spacey, which I'll talk about in just a second, has certain advantages. The main one being that you can use and leverage things that have been parsed syntactically or semantically. So things like the part of speech of a word, things like its dependencies, things like its co-reference. These are things that the spacey framework allow for you to do off the shelf and also train into machine learning models and work into pipelines with rules. So that's kind of one aspect of NLP and one way it's used. Another way it's used is to read in data and classify it. This is known as text categorization and we see that on the left hand side of this image. Text categorization or text classification, and we conclude in this sentiment analysis for the most part as well, is a way we take information into a computer system, again unstructured data, a raw text, and we classify it in some way. You've actually seen this at work for many decades now with spam detection. Spam detection is nearly perfect. It needs to be continually updated, but for the most part it is a solved problem. The reason why you have emails that automatically go to your spam folder is because there's a machine learning model that sits on the background of your, on the back end of your email server. And what it does is it actually looks at the emails, it sees if it fits the pattern for what it's seen as spam before, and it assigns it a spam label. This is known as classification. This is also used by researchers, especially in the legal industry. Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents. They don't necessarily have the human time to go through and analyze every single document verbatim. It is important to kind of get a quick umbrella sense of the documents without actually having to go through and read them page by page. And so what lawyers will oftentimes do is use NLP to do classification and information extraction. They will find keywords that are relevant to their case, or they will find documents that are classified according to the relevant fields of their case. And that way they can take a million documents and reduce it down to maybe only a handful, maybe a thousand that they have to read verbatim. This is a real world application of NLP or natural language processing, and both of these tasks can be achieved through the SPACI framework. SPACI is a framework for doing NLP. Right now, as of 2021, it's only available, I believe, in Python. I think there is a community that's working on an application with R, but I don't know that for certain. But SPACI is one of many NLP frameworks that Python has available. If you're interested in looking at all of them, you can explore things like NLDK, the Natural Language Toolkit, Stanza, which I believe is coming out of the same program at Stanford. There's many out there, but I find SPACI to be the best of all of them for a couple different reasons. Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they perform very quickly, and they also have very good accuracy metrics, such as precision, recall, and f-score. And I'm not going to talk too much about the way we measure machine learning accuracy right now, but know that they are quite good. Second, SPACI has the ability to leverage current natural language processing methods, specifically, transformer models, also known usually kind of collectively as BERT models, even though that's not entirely accurate. And it allows for you to use an off-the-shelf transformer model. And third, it provides the framework for doing custom training relatively easily compared to these other NLP frameworks that are out there. Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales well. SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale. By at scale, we mean working with large quantities of documents efficiently, effectively, and accurately. SPACI scales well because it can process hundreds of thousands of documents with relative ease in a relative short period of time, especially if you stick with more rules-based pipes, which we're going to talk about in part two of this video. So those are the two things you really need to know about NLP and SPACI in general. We're going to talk about SPACI in-depth as we explore it both through this video and in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com, and it should be linked in the description down below. This video and the textbook are meant to work in tandem. Some stuff that I cover in the video might not necessarily be in the textbook because it doesn't lend itself well to text representation, and the same goes for the opposite. Some stuff that I don't have the time to cover verbatim in this video I cover in a little bit more depth in the book. I think that you should try to use both of these. What I would recommend is doing one pass through this whole video, watch it in its entirety, and get an umbrella sense of everything that SPACI can do and everything that we're going to cover. I would then go back and try to replicate each stage of this process on a separate window or on a separate screen and try to kind of follow along in code, and then I would go back through a third time and try to watch the first part where I talk about what we're going to be doing and try to do it on your own without looking at the textbook or the video. If you can do that by your third pass, you'll be in very good shape to start using SPACI to solve your own domain specific problems. NLP is a complex field, and applying NLP is really complex, but fortunately frameworks like SPACI make this project and this process a lot easier. I encourage you to spend a few hours in this video, get to know SPACI, and I think you're going to find that you can do things that you didn't think possible in relative short order. So sit back, relax, and enjoy this video series on SPACI. In order to use SPACI, you're first going to have to install SPACI. Now there's a few different ways to do this depending on your environment and your operating system. I recommend going to SPACI.io backslash usage and kind of enter in the correct framework that you're working with. So if you're using Mac OS versus Windows versus Linux, you can go through and in this very handy kind of user interface, you can go through and select the different features that matter most to you. I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be doing everything on the CPU and I'm going to be working with English. So I've established all of those different parameters, and it goes through and it tells me exactly how to go through and install it using PIP in the terminal. So I encourage you to go through pause the video right now, go ahead and install Windows however you want to, I'm going to be walking through how to install it within the Jupyter notebook that we're going to be moving to in just a second. I want you to not work with the GPU at all. Working with Spacey on the GPU requires a lot more understanding about what the GPU is used for, specifically in training machine learning models. It requires you to have CUDA installed correctly. It requires a couple other things that I don't really have the time to get into in this video, but we'll be addressing in a more advanced Spacey tutorial video. So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda, and then selecting CPU and since you're going to be working through this video with English texts, I encourage you to select English right now and go ahead and just install or download the Ncore Web SM model. This is the small model. I'll talk about that in just a second. So the first thing we're going to do in our Jupyter notebook is we are going to be using the exclamation mark to delineate in the cell that this is a terminal command. We're going to say PIP install Spacey. Your output when you execute this cell is going to look a little different than mine. I already have Spacey installed in this environment. And so mine kind of goes through and looks like this yours will actually go through. And instead of saying requirement already satisfied, it'll be actually passing out the the different things that it's actually installing to install Spacey and all of its dependencies. The next thing that you're going to do is you're going to again, you follow the instructions and you're going to be doing Python dash M space Spacey space download and then the model that you want to download. So let's go ahead and do that right now. So let's go ahead and say Python M Spacey download. So this is a Spacey terminal command. And we're going to download the Ncore Web SM. And again, I already have this model downloaded. So on my end, Spacey is going to look a little differently than as it's going to look on your end as it prints off on the Jupyter notebook. And if we give it a just a second, everything will go through and it says that it's collected it, it's downloading it and we are all very happy now. And so now that we've got Spacey installed correctly, and that we've got the small model downloaded correctly, we can go ahead and start actually using Spacey and make sure everything's correct. The first thing we're going to do is we're going to import the Spacey library as you would with any other Python library. If you're not familiar with this, a library is simply a set of classes and functions that you can import into a Python script so that you don't have to write a whole bunch of extra code. Libraries are massive collections of classes and functions that you can call. So when we import Spacey, we're importing the whole library of Spacey. And now that we've seen something like this, we know that Spacey has imported correctly, as long as you're not getting an error message, everything was imported fine. The next thing that we need to do is we want to make sure that our English Core Web SM, our small English model, was downloaded correctly. So the next thing that we need to do is we need to create an NLP object. I'm going to be talking a lot more about this as we move forward right now. This is just troubleshooting to make sure that we've installed Spacey correctly and we've downloaded our model correctly. So we're going to use the spacey.load command. This is going to take one argument. It's going to be a string that is going to correspond to the model that you've installed. In this case, N Core Web SM. And if you execute this cell and you have no errors, you have successfully installed Spacey correctly and you've downloaded the English Core Web SM model correctly. So go ahead, take time and get all this stuff set up, pause the video if you need to and then pop back and we're going to start actually working through the basics of Spacey. I'm now going to move into kind of an overview of kind of what's within Spacey, why it's useful and kind of some of the basic features of it that you need to be familiar with. And I'm going to be working from the Jupyter Notebook that I talked about in the introduction to this video. If we scroll down to the bottom of chapter one, the basics of Spacey and you get past the install section, you get to this section on containers. So what are containers? Well, containers within Spacey are objects that contain a large quantity of data about a text. There are several different containers that you can work with in Spacey. There's the doc, the doc bin, example, language, lexeme, span, span group and token. We're going to be dealing with the lexeme a little bit in this video series and we're going to be dealing with the language container a little bit in this video series, but really the three big things that we're going to be talking about again and again is the doc, the span and the token. And I think when you first come to Spacey, there's a little bit of a learning curve about what these things are, what they do, how they are structured hierarchically. And for that reason, I've created this, in my opinion, kind of easy to understand image of what different containers are. So if you think about what Spacey is as a pyramid, so a hierarchical system, we've got all these different containers structured around really the doc object. Your doc container or your doc object contains a whole bunch of metadata about the text that you pass to the Spacey pipeline, which we're going to see in practice. In just a few minutes, the doc object contains a bunch of different things. It contains attributes. These attributes can be things like sentences. So if you iterate over doc.sense, you can actually access all the different sentences found within that doc object. If you iterate over each individual item or index in your doc object, you can get individual tokens. Tokens are going to be things like words or punctuation marks. Anything within your sentence or text that has a self contained important value, either syntactically or semantically. So this is going to be things like words, a comma, a period, a semi colon, a quotation mark, things like this, these are all going to be your tokens. And we're going to see how tokens are a little different than just splitting words up with traditional string methods and Python. The next thing that you should be kind of familiar with are spans. So spans are important because they kind of exist within and without of the doc object. So unlike the token, which is an index of the doc object, a span can be a token itself, but it can also be a sequence of multiple tokens. We're going to see that at play. So imagine if you had a span in its category, maybe group one are our places. So a single token might be like a city like Berlin. But span group two, this could be something like full proper names. So of a people, for example, so this could be like as we're going to see Martin Luther King. This would be a sequence of tokens, a sequence of three different items in the sentence that make up one span or one self contained item. So Martin Luther King would be a person who's a collection of a sequence of individual tokens. If that doesn't make sense right now, this image will be reinforced as we go through and learn more about spacey in practice. For right now, I want you to be just understanding that the doc object is the thing around which all of spacey sits. This is going to be the object that you create. This is going to be the object that contains all the metadata that you need to access. And this is going to be the object that you try to essentially improve with different custom components, factories and pipelines as you go through and do more advanced things with spacey. We're going to now see in just a few seconds how that doc object is kind of similar to the text itself, but how it's very, very different and much more powerful. We're now going to be moving on to chapter two of this textbook, which is going to deal with kind of getting used to the in depth features of spacey. If you want to pause the video or keep this notebook or this book open up kind of separate from this video and follow along as we go through and explore it in live coding. We're going to be talking about a few different things as we explore chapter two. This will be a lot longer than chapter one. We're going to be not only importing spacey, but actually going through and loading up a model, creating a doc object around that model so that we're going to work with the doc container and practice. And then we're going to see how that doc container stores a lot of different features or metadata or attributes about the text. And while they look the same on the surface, they're actually quite different. So let's go ahead and work within our same Jupiter notebook where we've imported spacey and we have already created the NLP object. The first thing that I want to do is I want to open up a text to start working with within this repo. We've got a data folder within this data sub folder. I've got a couple of different Wikipedia openings. I've got one on MLK that we're going to be using a little later in this video and I have one on the United States. This is wiki underscore us. That's going to be what we work with right now. So let's use our with operator and open up data backslash wiki underscore us dot txt. We're going to just read that in as F and then we're going to create this text object, which is going to be equal to F dot read. And now that we've got our text object created, let's go ahead and see what this looks like. So let's print text. Then we see that it's a standard Wikipedia article kind of follows that same introductory format and it's about four or five paragraphs long with a lot of the features left in such as the brackets that delineate some kind of a footnote. We're not going to worry too much about cleaning this up right now because we're interested not with cleaning our data so much as just starting to work with the doc object in spacey. So the first thing that you want to do is you're going to want to create a doc object. It is oftentimes good practice if you're only ever working with one doc object in your script to just call your only object doc. If you're working with multiple objects, sometimes you'll say doc one doc two doc three or give it some kind of specific name so that your variables can be unique and easily identifiable later in your script. Since we're just working with one doc object right now, we're going to say doc is equal to NLP. So this is going to call our NLP model that we imported earlier in this case the English Core Web SM model. And that's going to for right now just take one argument and that's going to be the text itself. So the text object, if you execute that cell, you should have a doc object now created. Let's print off that doc object and see what it looks like. And if you scroll down, you might be thinking to yourself, this looks very, very similar if not identical to what I just saw a second ago. And in fact, on the surface, it is very similar to that text object that we gave to the NLP model or pipeline, but let's see how they're different. Let's print off the length of text. And let's print off the length of the doc object. And what we have here are two different numbers. Our text is 3525 and our doc object is 152. What is going on here? Well, let's get a sense by trying to iterate over the text object and iterating over the doc object with a simple for loop. So we're going to say for token and text, so we're going to iterate first over that text object, we're going to print off the token. So the first 10 indices, and we get individual letters as one might expect. But when we do something the same thing with the doc object, let's go ahead and start writing this out. We're going to say for token and doc, we're going to iterate over the first 10. We're going to print off the token. We see something very different. What we see here are tokens. This is why the doc object is so much more valuable and this is why the doc object has a different length than the text object. The text object is just basically counting up every instance of a character, a white space, a punctuation, etc. The doc object is counting individual tokens, so any word, any punctuation, etc. That's why they're of different length and that's why when we print them off, we see something different. So you might now already be seeing the power of spacey. It allows for you to easily on the surface with nothing else being done, easily split up your text into individual tokens without any effort at all. Now, those of you familiar with Python and different string methods might be thinking to yourself, but I've got the split method. I can just use this to split up the text. I don't need anything fancy from spacey. Well, you'd be wrong. Let me demonstrate this right now. So if I were to say for token and text.split, so I'm splitting up that text into individual and theory individual words, essentially, it's just a split method where it's splitting by individual white spaces. If I were to do that and iterate over the first 10 again. And I would just say print token, it looks good until you get down here. So until you get to USA, well, why is it a problem? The problem is quite simple. There is a parentheses mark right here. And this is where we have a huge advantage with spacey. Spacey automatically separates out these these kind of punctuation marks and removes them from individual tokens when they're not relevant to the token itself. Notice that USA has got a period within the middle of it. It's not looking at that and thinking that that is some kind of unique token, a you a period and s a period and an a in a period. It's not seeing these as four individual tokens rather it's automatically identifying them as one thing one tied together single token that's a string of characters and punctuation. This is where the power of spacey really lies just on the surface level and go ahead spend a few minutes and play around with this. And then we're going to kind of jump back here and start talking about how the doc object has a lot more than just tokens within it. It's got sentences each token has attributes. We're going to start exploring these when you pop back. If you're following along with the textbook, we're now going to be moving on to the next section, which is sentence boundary detection. An NLP sentence boundary detection is the identification of sentences within a text on the surface. This might look simple. You might be thinking to yourself, I could simply use the split function and split up a text with a simple period. And that's going to give me all my sentences. Those of you who have tried to do this might already be shaking your heads and saying no. If you do think about it, there's a really easy explanation for why this doesn't work. Were you to try to split up a text by period and make a presumption that anything that occurs with between periods is going to be an individual sentence, you would have a serious mistake when you get to things like USA, especially in Western languages, where the punctuation of a period mark is used not only to delineate the change of its sentence, rather it's used to also delineate abbreviations. So United States of America, each period represents an abbreviated word. So you could write in rules to kind of account for this, you could write in rules that could also include in other ways that sentences are created, such as question marks, such as exclamation marks. But why do that? That's a lot of effort. When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly how that works. So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing the sentence attribute of the doc object. Let's print off scent. And if you do that, you are now able to print off every individual sentence. So the entire text has been tokenized at the sentence level. In other words, spacey has used its sentence boundary detection and done all that for you and giving you all the sentences. If you work with different models of different sizes, you're going to notice that certain models the larger they get tend to do better at sentence detection. And that's because machine learning models tend to do a little bit better than heuristic approaches. The English core web SM model, while having some machine learning components in it, does not save word vectors. And so the larger you go with the models, typically the better you're going to have with regards to sentence detection. Let's go ahead and try to access one of these sentences. So let's create an object called sentence one, we're going to make that equal to doc.sense zero. We're going to try to grab that zero index and let's print off sentence one, we do this, we get an error. Why have we gotten an error? Well, it tells you why right here, it's a type air. And this means that this is not a type that can be kind of iterated over, it's not subscriptable. And it's because it is a generator. Now in Python, if you're familiar with generators, you might be thinking to yourself, there's a solution for this. And in fact, there is. If you want to work with generator objects, you need to convert them into a list. So let's say sentence one is equal to list. So using the list function to convert doc.sense into a list. And then with outside of that, we're going to grab zero, the zero index, and then we're going to print off sentence one. And we grab the first sentence of that text. This as we go deeper and deeper in spacey one by one, you're going to see the immense power that you can do with Pacea, all the immense incredible things you can use spacey for with very, very minimal code. The doc object does a lot of things for you that would take hours to actually write out and code to do with heuristic approaches. This is now a great way to segment an entire text up by sentence. And if you work with text a lot, you will already know that this has a lot of applications. As we move forward, we're going to not just talk about sentences, we're also going to be talking about token attributes, because within the doc object are individual tokens. I encourage you to pause here and go ahead and play around with the doc.sense a little bit and get familiar with how it works, what it contains, and try to convert it into a list. And we'll continue talking about tokens. This is where I really encourage you to spend a little bit of time with the textbook. Under token attributes in chapter two, I have all the different kind of major things that you're going to be using with regards to token attributes. We're going to look and see how to access them in just a second. I've provided for you kind of the most important ones that you should probably be familiar with. We're going to see this in code in just a second, and I'm going to explain with a little bit more detail than what's in the spacey documentation about what these different things are, why they're useful, and how they're used. So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes. If you remember, the doc object had a sequence of tokens. So for token and doc, you could print off token. And let's just do this with the first 10. And we've got each individual token. What you don't see here is that each individual token has a bunch of metadata buried within it. These metadata are things that we call attributes or different things about that token that you can access through the spacey framework. So let's go ahead and try to do that right now. Let's just work with for right now token number two, which we're going to call sentence one, and we're going to grab from sentence one, the second index. Let's print off that word. And it should be states. And in fact, it is fantastic. So now that we've got the word states accessed, we can start kind of going through and playing around with some of the attributes that that word actually has. Now when you print it off, it looks like a regular piece of text, it looks like just a string, but it's got so much more buried within it now because it's been passed through our NLP model or pipeline from spacey. So let's go ahead and say token to dot text. And I'm going to be saying token to dot text. If you're working within an IDE like Adam, you're going to need to say print token to dot text. When we do this, we see we get a string that just is states. This is telling us that the dot text of the object, the pure text corresponds to the word states. This is really important if you need to extract the text itself from the token and not work with the token object, which has behind it a whole bunch of different metadata that we're going to go through now and start accessing. Let's use the token left edge. So we can say token to dot left underscore edge. And we can print that off. Well, what's that telling us? It's telling us that this is part of a multi word token or a token that is multiple has multiple components to make up a larger span. And that this is the leftmost token that corresponds to it. So this is going to be the word the as in the United States. Let's take a look at the right edge. We can say token to dot right underscore edge, print that off, and we get the word America. So we're able to see where this token fits within a larger span in this case a noun chunk, which we're going to explore in just a few minutes. But we also learn a lot about it, kind of the different components, so we know where to grab it from the beginning and from the very end. So that's how the left edge and the right edge work. We also have within this token to dot int type. This is going to be the type of entity. Now what you're seeing here is a integer. So this is 384. In order to actually know what 384 means, I encourage you to not really use that so much as and type with an underscore after it. This is going to give you the string corresponding to number 384. In this case, it is GPE or geopolitical entity. We're going to be working with named entity a little bit in this video, but I have a whole other book on named entity recognition. It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning and rules based in a lot more depth. Let's go ahead and keep on moving on though and looking at different entity types here as well. Not entity types, attribute types. So we're going to say token to dot int IOB, all lowercase and again underscore at the end and we get the string here, I. Now IOB is a specific kind of named entity code. B would mean that it's the beginning of an entity and I means that it's inside of an entity and O means that it's outside of an entity. The fact that we're seeing I here tells us that this word states is inside of a larger entity. In fact, we know that because we've seen the left edge and we've seen the right edge. It's inside of the United States of America. So it's part of a larger entity at hand. We can also say token to dot lima and under case again after that and we get the word states. Lima form or the root form of the word. This means that this is what the word looks like with no inflection. If we were working with a verb, in fact, let's go ahead and do that right now. Let's grab sentence. We're going to grab sentence one index 12, which should be the word no and we're going to print off the lima for the word or sorry, it's a verb and we see the verb lima as no. So if we were to print off sentence one specifically index 12, we see that its original form is known. So the lima form uninflected is the verb no K N O W. Another thing that we can access and we're going to see that have the power of this later on. This might not seem important right now, but I promise you it will be. Let's print off token that I call this again token to we're going to print that off, but we're going to print off specifically the morph. No underscore here. Just morph. What you get is what looks like a really weird output a string called noun type equal to prop. In fact, this means proper noun, a number which corresponds to sing. We're going to talk a lot more about morphological analysis later on when we try to find an extract information from our texts. But for right now, understand that what you're looking at is the output of kind of what that word is morphologically. So in this case, it's a proper noun and it's singular. If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb it is. So it's a perfect past participle known perfect past participle. For being good at NLP is also being good with language. So I encourage you to spend time and start getting familiar with those things that you might have forgotten about from like fifth grade grammar, such as perfect participles and things like that. Because when you need to start creating rules to extract information, you're going to find those pieces of information very important for writing rules. We'll talk about that in a little bit though. Let's go back to our other attributes from the token. So again, let's go to token two, and we're going to grab the POS part of speech, not what you might be thinking. So part of speech underscore POS underscore, and we output PROPN. This means that it is a proper noun. It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological detailed extraction, what kind of noun it might be with regards to in this case, singular. So that's going to be how you extract the part of speech. And the thing that you can do is you can extract the dependency relation. So in this case, we can figure out what role it plays in the sentence. In this case, the noun subject. And then finally, the last thing I really want to talk about before we move into a more detailed analysis of part of speech is going to be the token two dot lane. And what this grabs for you is the language of the doc object in this case, we're working with something from the English language, so in every language is going to have two letters that correspond to it. These are universally recognized. So that's going to be how you access different kinds of attributes that each token has. And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that I haven't covered. I gave you the ones that are the most important that I find to be used on a regular basis to solve different problems with regards to information extraction from the text. So that's going to be where we stop here with token attributes, and we're going to be moving on to part 2.5 of the book, which is part of speech tagging. I now want to move into kind of a more detailed analysis of part of speech within spacey and the dependency parser and how to actually analyze it really nicely either in a notebook or outside of a notebook. So let's work with a different text for just a few minutes. We're going to see why this is important. It's because I'm working on a zoomed in screen, and to make this sentence a little easier to understand, we're going to just use Mike and Joy's plain football, a very simple sentence. And we're going to create a new doc object, and we're going to call this doc two. That's going to be equal to NLP text. Let's print off doc two just to make sure that it was created, and in fact that we see that it was. Now that we've got it created, let's iterate over the tokens within this and say for token in text, we want to print off token dot text. We want to see what the text actually is. We want to see the token dot POS, and the token dot DEP helps if you actually iterate over the correct object over the doc to object. And we see that we've got Mike, proper noun, noun, subject, and Joy's verb. It's the root plane. In this case, it's a verb. And then we've got football, the noun, the direct object, and a period, which is the punctuation. So we can see the basic semantics of the sentence at play. What's really nice from spacey is we have a way to really visualize this information and how these words relate to one another. So we can say from spacey, import, displacey, and we're going to do displacey, displacey dot render. And this is going to take two arguments, it's going to be the text, and then it's going to be the, actually, it's going to be doc two, and then it's going to be style. In this case, we're going to be working with DEP, and we're going to print that off. And we actually see how that sentence is structured. Now in the textbook, I use a more complicated sentence. But for the reasons of this video, I've kept it a little shorter, just because I think it displays better on this screen, because you can see that this becomes a little bit more difficult to understand when you're zoomed in. But this is one sentence from that Wikipedia article. So go ahead and look at the textbook and see how elaborate this is. You can see how it's part of a compound, how it's preposition. You can see the more fine-grained aspects of the dependency parser and the part of speech tagger really at play with more complicated sentences. So that's going to be how you really access part of speech and how you can start to visualize how words in a sentence are connected to other words in a sentence with regards to their part of speech and their dependencies. That's going to be where we stop with that. In the next section, we're going to be talking about named entity recognition and how to visualize that information. So named entity recognition is a very common NLP task. It's part of kind of data extraction or information extraction from texts. It's oftentimes just called NER, named entity recognition. I have a whole book on how to do NER with Python and with Spacey, but we're not going to be talking about all the ins and outs right now. We're just going to be talking about how to access the pieces of information throughout kind of our text. And then we're going to be dealing with a lot of NER as we try to create elaborate systems to do named entity extraction for things like financial analysis. Let's go ahead and figure out how to iterate over a doc object. So we're going to say for int and doc.n, so we're going to go back to that original doc, the one that's got the text from Wikipedia on the United States. We're going to say print off int.text, so the text from it, and int.label, label underscore here. That's going to tell us what label corresponds to that text. Then we print this off. We've got a lot of GPEs, which are geopolitical entities, North America. This isn't a geopolitical entity. It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in this case, which is a national or religious political entity, quantity, the number of miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke. Then we have date being extracted, so at least 12,000 years ago. This is a small model, and it's extracting for us a lot of very important structured data. But we can see that the small model makes mistakes. So the Revolutionary War is being considered an organization. Were I to use a large model right now, which I can download separately from Spacey, and we're going to be seeing this later in this video, or were I to use the much larger transformer model. This would be correctly identified most likely as an event, not as an organization, but because this is a small model that doesn't contain word vectors, which we're going to talk about in just a little bit, it does not generalize or make predictions well on this particular data. Nevertheless, we do see really good extraction here. We have the American Civil War being extracted as an event. We have the Spanish American War, even with this encoding typographical error here. And World War being extracted as an event, World War II event, Cold War event. All of this is looking good. And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd expect to see. We even see percentages extracted correctly here. So this is how you access really vital information about your tokens, but more importantly about the entities found within your text. And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook. We can say displacie.render, we can say doc, style, we can say int. And we get this really nice visualization where each entity has its own particular color. So you can see where these entities appear within the text, as you kind of just naturally read it. And you can do this with the text as long as you want, you can even change the max length to be more than a million characters long. And again, we can see right here, org is incorrectly identified as the American Revolutionary War incorrectly identified as org, but nevertheless, we see really, really good results with a small English model without a lot of custom fine tune training. And there's a reason for this. A lot of Wikipedia data gets included into machine learning models. The machine learning models on text typically make good predictions on Wikipedia data, because it was included in their training process. Nevertheless, these are still good results. If I'm right or wrong on that, I'm not entirely certain. But that's going to be how you kind of extract important entities from your text, and most importantly visualize it. This is where chapter two of my book kind of ends. After this chapter, you have a good understanding, hopefully, of kind of what the dot container is, what tokens are, and how the doc object contains the attributes such as since and ends, which allows for you to find sentences and entities within a text. Hopefully you also have a good understanding of how to access the linguistic features of each token through token attributes. I encourage you to spend a lot of time becoming familiar with these basics, as these basics are the building block for really robust things that we're going to be getting into in the next few lessons. We're now moving into chapter three of our textbook on Spacey and Python. Now in chapter three, we're going to be continuing our theme of part one, where we're trying to understand the larger building blocks of Spacey. Even though this video is not going to deal with Spacey machine learning approaches, our custom ones, that is, it's still important to be familiar with what machine learning is and how it works, specifically with regards to language, because a lot of the Spacey models such as the medium, large and transformer models, all are machine learning models that have word vectors stored within them. This means that they're going to be larger, more accurate, and do the things a bit more slowly, depending upon its size. We're going to be working through not only what kind of machine learning is generally, but specifically how it works with regards to text. I think that this is where you're going to find this textbook to be somewhat helpful. What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did before, but this time we're going to be installing a new model. We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then we're going to download the Ncore Web MD model. This is the medium English model. This is going to take a little longer to download, and the reason why I'm having you download the medium model, and the reason why we're going to be using the medium model, is because the medium model has stored within it word vectors. Let's go ahead and talk a little bit about what word vectors are and how they're useful. So word vectors are word embeddings. So these are numerical representations of words in multi-dimensional space through matrices. That's a very compacted sentence. So let's break it down. What are word vectors used for? Well, they're used for a computer system to understand what a word actually means. So computers can't really parse text all that efficiently. They can't parse it at all. Every word needs to be converted into some kind of a number. Now for some old approaches, you would use something like a bag of words approach where each individual word would have a corresponding number to it. This would be a unique number that corresponds just to that word. There are a lot of tasks that can work, but for something like text understanding or trying to get a computer system to be able to understand how a word functions within a sentence in general, in other words, how it works in the language, how it relates to all other words, that doesn't really work for us. So what a word vector is, is it's a multi-dimensional representation. So instead of a number having just a single integer that corresponds to it, it instead has what looks like to an unsuspecting eye, essentially. It has a very complex sequence of floating numbers that are stored as an array, which is a computationally less expensive form of a list in Python or just computing in general. And this is what it looks like, a long sequence. In this case, I believe it's a 300-dimensional word that corresponds to a specific word. So this is what an array or a word vector or a word embedding looks like. What this means to a computer system is it means syntactical and semantical meaning. So the way word vectors are typically trained is, oh, there's a few different approaches, but kind of the old-school word-to-vec approach is you give a computer system a whole bunch of texts and different smaller, larger collections of texts, and what it does is it reads through all of them and figures out how words are used in relation to other words. And so what it's able to essentially do through this training process is figure out meaning. And what that meaning allows for a computer system to do is understand how a word might relate to other words within a sentence or within a language as a whole. And in order to understand this, I think it's best if we move away from this textbook and actually try to explore what word vectors look like in spacey. So you can have a better sense of specifically what they do, why they're useful, and how you, as a NLP practitioner, can go ahead and start leveraging them. So just like before, we're going to create an NLP object. This time, however, instead of loading in our Encore Web SM, we're going to load in our Encore Web MD. So the one that actually has these word vectors stored, the static vectors saved, and it's going to be a larger model. Let's go ahead and execute that cell. And while that's executing, we're going to start opening up our text. So we're going to say with open data wiki underscore us.txt, r as f, and we're going to say text is equal to f.read, so we're going to successfully load in that text file and open it up. Then we're going to create our doc object, which will be equal to NLP text. All the syntax is staying the exact same. And just like before, let's grab the first sentence. So we're going to convert our doc.sense generator into a list, and we're going to grab index zero. And let's go ahead and print off sentence one, just so you can kind of see it. And there it is. So now that we've got that kind of in memory, we can start kind of working with it a little bit. So let's go ahead and just start tackling how we can actually use word vectors with spacey. So let's kind of think about a general question right now. Let's say I wanted to know how the word, let's say country is similar to other words within our model's word embeddings. So let's create a little way we can do this. We're going to say your word, and this is going to be equal to the word country, country. There we go. And what we can do is we can say MS is equal to NLP. So we're going to go into that NLP object. We're going to grab the vocab.vectors, and we're going to say most similar. And this is a little complicated way of doing it. In fact, I'm going to go ahead and just kind of copy and paste this in. You have the code already in your textbook that you can follow along with. And I'm going to go ahead and just copy and paste it in right here and print off this. And what this is going to do is it is going to go ahead and just do this entirely. There we go. And we have to import numpy as MP. This lets us actually work with the data as a numpy array. And when we execute this cell, what we get is an output that tells us all the words that are most similar to the word country. So in this scenario, the word country, it has these kind of all these different similar words to it from the word country to the word country, capitalized nation, nation. Now it's important to understand what you're seeing here. What you're seeing is not necessarily a synonym for the word country, rather what you're seeing is are the words that are the most similar. Now this can be anything from a synonym to a variant spelling of that word to something that occurs frequently alongside of it. So for example, world, while this isn't the same, we would never consider world to be the synonym of country. But what happens is, is syntactically they're used in very similar situations. So the way you describe a country is sometimes the way you would describe your world, or maybe it's something to do with the hierarchy, so a country is found within the world. This is a good way to understand it. So it's always good to use this word as most similar, not to be something like synonym. So when you're talking about word vectors similarity, you're not talking about synonym similarity. But this is a way you can kind of quickly get a sense. So what does this do for you? Why did I go through and explain all these things about word vectors? If I'm not going to be talking about machine learning a whole bunch throughout this video. Well, I did it so that you can do one thing that's really important. And that's calculate document similarity in the spacey. So we've already got our NLP model loaded up. Let's create one object. So we're going to make doc one, we're going to make that equal to NLP. And we're going to create the text right here in this object. So let's say this is coming straight from the spacey documentation. I like salty fries and hamburgers. And we're going to say doc two is equal to NLP. And this is going to be the text fast food tastes very good. And now what we can do is let's go ahead and load those into memory. What we can do is we can actually make a calculation using spacey to find out how similar they actually are these two different sentences. We can say print off doc one, and we're going to say this again, this is coming straight from the spacey documentation doc two, so you're going to be able to see what both documents are. And then we're going to do doc one dot similarity. So we can go into the doc one dot similarity method and we can compare it to doc two. We can print that off. So what we're seeing here on the left is document one, this little divider thing that we printed off here. On the right, we have document two, and then we can see the degree of similarity between document one and document two. Let's create another doc object. We're going to call this NLP doc three, and we're going to make this NLP. Let's come up with a sentence that's completely different. The Empire State Building is in New York. So this is when I'm just making up off the top of my head right now. I'm going to copy and paste this down, and we're going to compare this to doc one. We're going to compare it to doc three, and we get a score of point five one. So this is less similar to than these two. So this is a way that you can take a whole bunch of documents. You can create a simple for loop, and you can find and start clustering the documents that have a lot of overlap or similarity. How is this similarity being calculated? Well, it's being calculated because what spacey is doing is it's going into its word embeddings, and even though in these two situations, we're not using the word fast food ever in this document. It's going in and it knows that salty fries and hamburgers are probably in a close cluster with the biogram or a token that's made up of two words, a biogram of fast food. So what it's doing is it's assigning a prediction that these two are still somewhat similar, more similar than these two, because of these overlapping in words. So let's try one more example, see if we can get something that's really, really close. So let's take doc four, and this is going to be equal to NLP, I enjoy oranges. And then we're going to have doc five is going to be equal to NLP, I enjoy apples. So two, I would agree, I would argue very, very syntactically similar sentences. And we're going to do doc four here, doc five here, and we're going to look and see a similarity between doc four and doc five. And if we execute this, we get a similarity of 0.96. So this is really high. This is telling me that these two sentences are very similar, and it's not just that they're similar because of the similar syntax here, that's definitely pushing the number up. It's that what the individual is liking in the scenario between these two texts, they're both fruits. Let's try something different. Let's make doc five. Let's just make doc six here, and do something like this NLP, I enjoy, what's another word we could say. Something that's different, let's say burgers, something different from a fruit. So we're going to make doc six like that, and we're going to again copy and paste this down, copy and paste this down, we're going to put doc six here. And we see this drop. So what this demonstrates, and I'm really glad this worked because I improvised this, what this demonstrates is that the similarity, the number that's given is not dependent on the contextual words, rather it's dependent upon the semantic similarity of the words. So apples and oranges are in a similar cluster around fruit because of their word embeddings. The word burgers while still being food and still being plural is different from apples and oranges. So in other words, this similarity is being calculated based on something that we humans would calculate difference in meaning based on a large understanding of a language as a whole. That's where word vectors really come into play. This allows for you to calculate other things as well. So you could even calculate the difference between salty fries and hamburgers, for example, I've got this example ready to go in the textbook, let's go ahead and try this as well. So we're going to grab doc one, and print off these few things right here. So we're going to try to calculate the similarity between french fries and burgers and what we get is a similarity of 0.73. So if we were to maybe change this up a little bit and try to calculate the similarity between maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity. So my point is, is play around with the similarity calculator, play around with the structure, the code I provided here, and get familiar with how spacey can help you kind of find a similarity, not just between documents, but between words as well. And we're going to be seeing how this is useful later on. But again, it's good to be familiar with kind of generally how machine learning kind of functions here in this context, and why these medium and large models are so much bigger. They're so much bigger because they have more word vectors that are much deeper. And the transformer model is much larger because it was trained in a completely different method than the way the medium and large models were trained. But again, that's out of the scope for this video. I now want to turn to really the last subject of this introduction to spacey part one, which is when we're taking this large umbrella view of the spacey. And in the textbook, it's going to correspond to chapter four. So what we go over in this textbook is kind of a large view of not just the dot container and the word vectors and the linguistic annotations, but really kind of the structure of the spacey framework, which comes around the pipeline. So a pipeline is a very common expression in computer science and in data science. Think of it as a traditional pipeline that you would see in a house. Now think of a pipeline being a sequence of different pipes. Each pipe in a computer system is going to perform some kind of permutation or some action on a piece of data as it goes through the pipeline. And as each pipe has a chance to act and make changes to and additions to that data, the later pipes get to benefit from those changes. So this is very common when you're thinking about logic of code. I provide it like a little image here that I think maybe might help you. So if we imagine some input sentence, right, so some input text is entering a spacey pipeline, it's going to go through a bunch of things if you're working with the medium model or the small model, that'll tokenize it and give it a word and vector for different words. It'll also find the POS, the part of speech, the dependency parser will act on it. But it might eventually get to an entity ruler, which we're going to see in just a few minutes. The entity ruler will be a series of rules-based NER named entity recognition. So it'll maybe assign a token to an entity. Might be the beginning of an entity, might be the end of an entity, might just be an individual token entity. And then what will happen is that doc object, as it kind of goes through this pipeline, will now receive a bunch of doc.ins. So it'll be, this pipe will actually add to the doc object as it goes through the pipeline, the entity component. And then the next pipeline, the entity linker, might take all those entities and try to find out which ones they are. So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number that corresponds to a specific person. So for example, if you were seeing a bunch of things like Paul something, Paul something, maybe that one Paul something might be Paul Hollywood from the Great British Bake Off, and it might have to make a connection to a specific person. So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood, depending on the context. That's out of the scope of this video series, but keep in mind that that pipe would do something else that would modify the ints that would give them greater specificity. And then what you'd be left with is the doc object on the output that not only has entities annotated, but it's also got entities linked to some generic specific data. So that's going to be how a pipeline works. And this is really what spacey is. It's a sequence of pipes that act on your data. And that's important to understand, because it means that as you add things to a spacey pipeline, you need to be very conscientious about where they're outed and in what order. As we're going to see as we move over to kind of rules based spacey, when we start talking about these different pipes, the entity ruler, the matcher custom components, regex components, you're going to need to know which order to put them in. It's going to be very important. So do please keep that in mind. Now spacey has a bunch of different attribute rulers or different pipes that you can kind of add into it. You've got dependency parsers that are going to come standard with all of your models. You've got the entity linker entity, recognizer entity ruler, you're going to have to make these yourself and add them in oftentimes. You've got a limitizer. This is going to be on most of your standard models, your morphologue, that's going to be on on there as well, sentence recognizer, synthesizer. This is what allow for you to have the doc.sense right here span categorizer. This will help categorize different spans, be them single token spans or sequence of token spans, your tagger, this will tag the different things in your text, which will help with part of speech, your text categorizer. This is when you train a machine learning model to recognize different categories of a text. So text classification, which is a very important machine learning task, tote to VEC. This is going to be what assigns word embeddings to the different words in your doc object. Organizer is what breaks that thing up and all your text into individual tokens. And you've got things like transformer and trainable pipes. Then within this, you've also got some other things called matchers. So you can do some dependency matching. We're not going to get into that in this video. You've also got the ability to use matcher and phrase matcher. These are a lot of the times can do some similar things, but they're executed a little differently to make things less confusing. I'm really only talking about the matcher of these two. If there's a need for it, I'll add into the textbook the phrase matcher at a later date, but I'm not going to cover it in this video. And if I do add in the phrase matcher, it's going to be after this matcher section here. I have it in the GitHub repo. I just haven't included in the textbook to keep things a little bit simpler, at least if you're just starting out. So a big good question is, well, how do you add pipes to a spacey pipeline? So let's go ahead and do that. We're going to make a blank spacey pipeline right now. Let's go ahead and just make, we'll just work with the same live coding notebook that we have open right now. So what we're going to do is we're going to make a blank model, and we're going to actually add in our own sentenizer to our, to our text. So let's go ahead and do that. So I'm going to say NLP is equal to a spacey dot blank. This is going to allow for me to make a blank spacey pipeline. And I'm going to say Ian so that it knows that the tokenizer that I need to use is the English tokenizer. And now if I want to add a pipe to that, I can use one of the built-in spacey features. So I can say add underscore pipe, and I can say sentenizer. So I can add in a sentenizer. This is going to allow for me to create a pipeline now that has a sequence of two different pipes. And I demonstrate in the textbook why this is important. Sometimes what you need to do is you need to just only break down a text into individual sentences. So I grabbed a massive, massive corpus from the internet, which is on MIT.edu. And it's the entire Shakespeare corpus. And I just try to calculate the, the quantity of sentences found within it. There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and count those sentences with the spacey model. Using the small model, however, it took a total amount of time of 47 minutes to actually break down all those sentences and extract them. Why is there a difference in time between 7 seconds and 47 minutes? It's because that this spacey small model has a bunch of other pipes in it that are trying to do a bunch of other things. If you just need to do one task, it's always a good idea to just activate one pipe or maybe make a blank model and just add that single pipe or the only pipes that you need to it. A great example of this is needing to tokenize a whole bunch of sentences in relatively short time. So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes. However comes at a trade-off. The small model is going to be more accurate in how it finds sentence boundaries. So we have a difference in quantity here. This difference in quantity indicates that this one messed up and made some mistakes because it was just the sentenceizer. The sentenceizer didn't have extra data being fed to it. In fact, if I probably used larger models, I might even have better results. But always think about that. If time is of the essence and you don't care so much about accuracy, a great way to get the quantity of sentences or at least a ballpark is to use this method where you simply add in a sentenceizer to a blank model. So that's how you actually add in different pipes to a spacey pipeline and we're going to be reinforcing that skill as we go through, especially in part two, where we really kind of work with this in a lot of detail. Right now I'm just interested in giving you the general understanding of how this might work. Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes. We can analyze what our analyze, there we go. We can actually analyze our pipeline. If we look at the NLP object, which is our blank model with the sentenceizer, we see that our NLP pipeline ignore summary, ignore this bit here. But what you're actually able to kind of go through and see right away is that we've really just got the sentenceizer sitting in it. If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey dot load and core web SM, we're going to create that NLP two object around the small spacey English model. We can analyze the pipes again, and we see a much more elaborate pipeline. So what are we looking at? Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed the doc dot ends, and we keep on going down. We can see the limitizer, but we can see also a whole bunch of other things. We can see what these different things actually assign. So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually require. So if we look up here, we see that the NER pipe, so the name to the recognition pipe is responsible for assigning the doc dot ends. So that attribute of the doc object, and it's also responsible at the token level for assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes ago when we talked about the IOB being the opening beginning or out beginning inside for a different entity, it also assigns the end dot end underscore type for each token attribute. So you can see a lot of different things about your pipeline by using NLP dot analyze underscore pipes. If you've gotten to this point in the video, then I think you should by now have a good really umbrella view of what spacey is, how it works, why it's useful. And some of the basic features that it can do and how it can solve some pretty complex problems with some pretty simple lines of code. What we're going to see now moving forward is how you as a practitioner of NLP cannot just take what's given to you with spacey, but start working with it and start leveraging it for your own uses. So taking what is already available. So like these models like the English model and adding to them contributing to them. Maybe you want to make an entity ruler where you can find more entities in a text based on some cousin tier or list that you have. Maybe you want to make a matcher so you can find specific sequences within a text. Maybe that's important for information extraction. Maybe you need to add custom functions or components into your spacey pipeline. I'm going to be going through in part two rules based spacey and giving you all the basics of how to do some really robust custom things relatively quickly with within the spacey framework. All of that's going to lay the groundwork so that in part three, we can start applying all these skills and start solving some real world problems. In this case, we're going to look at financial analysis. So that's going to be where we move to next is part two. We are now moving into part two of this Jupiter book on spacey and we're going to be working with rules based spacey. Now this is really kind of the bread and butter of this video. You've gotten a sense of the umbrella structure of spacey as a framework. You've gotten a sense of what the container can contain. You've gotten a sense of the token attributes and the linguistic annotations from part one of this book and the earlier part of this video. Now we're going to move into taking those skills and really developing them into custom components and modified pipes that exist within spacey. In other words, I'm going to show you how to take what we've learned now and start really doing more robust and sophisticated things with that knowledge. So we're going to be working first with the entity ruler, then with the matcher in the next chapter, then in the components in spacey. So a custom component is a custom function that you can put into a pipeline. Then we're going to talk about regex or regular expressions. And then we're going to talk about some advanced regex with spacey. If you don't know what regex is, I'm going to cover this in chapter eight. So let's go over to our Jupiter notebook that we're going to be using for our entity ruler lesson. So let's go ahead and execute some of these cells. And then I'm going to be talking about it in just a second. First I want to take some time to explain what the entity ruler is as a pipe in spacey, what it's used for, why you'd find it useful and when to actually implement it. So there are two different ways in which you can kind of add in custom features to a spacey language pipeline. There is a rules based approach and a machine learning based approach. Rules based approaches should be used when you can think about how to generate a set of rules based on either a list of known things or a set of rules that can be generated through regex, code or linguistic features. Machine learning is when you don't know how to actually write out the rules or the rules that you would need to write out would be exceptionally complicated. A great example of a rules based approach versus a machine learning based approach and when to use them is with entity types for named entity recognition. Imagine if you wanted to extract dates from a text. There are a finite, very finite number of ways that a date can appear in a text. You could have something like January 1, 2005, you could have one January 2005, you could have one Jan 2005, you could have one slash five slash 2005, there's there's different ways that you can do this and there's a lot of them. But there really is a finite number that you could easily write a regex expression for a regular expression for to capture all of those. And in fact, those regex expressions already exist. That's why spacey is already really good at identifying dates. So dates are something that you would probably use a rules based approach for something that's a good machine learning approach for or something like names. If you wanted to capture the names of people, you would have to generate an entity ruler with a whole bunch of robust features. So you would have to have a list of all known possible first names, all known possible last names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc. And you'd have to have a list of all known suffixes. So junior, senior, the third, the fourth, etc. on down the list. This would be very, very difficult to write because first of all, the quantity of names that exist in the world are massive. The quantity of last names that exist in the world is massive. There's not a set gazetteer or set list out there of these anywhere. So for this reason, oftentimes things like people names will be worked into machine learning components. I'm going to address machine learning in another video at a later date, but right now we're going to focus on a rules based approach. So using the rules based features that spacey offers, a good NLP practitioner will be excellent at both rules based approaches and machine learning based approaches and knowing when to use which approach and when maybe maybe a task is not appropriate for machine learning when it can be worked in with rules relatively well. If you're taking a rules based approach, the approach that you take should have a high degree of confidence that the rules will always return true positives. And you need to think about that. If you are okay with your rules, maybe catching a few false positives or missing a few true positives, then maybe think about how you write the rules and allowing for those and making it known in your documentation. So that's generally what a rules based approach is in an entity ruler is a way that we can use a list or a series of features, language features to add tokens into the entity, the dot ints container within the dot container. So let's go ahead and try to do this right now. The text we're going to be working with is a kind of fun one, I think. So if you've already gotten the reference, congratulations, it's kind of obscure. But we're going to have a sentence right here that I just wrote out. West Chesterton Fieldville was referenced in Mr. Deeds. So in this context, we are going to have a few different entities. We want our model or our pipeline to extract West Chesterton Fieldville as a GPE. It's a fake place that doesn't really exist. It was made up in the movie Mr. Deeds. And what we want is for Mr. Deeds to be grabbed as an entity as well. And this would ideally be labeled as a film. But in this case, that's probably not going to happen. Let's go ahead and see what does happen. So we're going to say for int and doc dot ints, print off int dot text, int dot label, like we learned from our NER lesson a few moments ago. And we see that the output looks like this. It's gotten almost all the entities that we wanted. Mr. was left off of Deeds. And it's grabbed the West Chesterton Fieldville and labeled it as a person. So what's gone wrong here? Well, there's a few different things that have gone wrong. The NCORE Web SM model is a machine learning model for NER. The word vectors are not saved. So the static vectors are not in it. So it's making the best prediction that it can. But even with a very robust machine learning model, unless it has seen West Chesterton Fieldville, there is not really a good way for the model to actually know that that's a place. Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer model might actually get this right. But for the most part, this is a very challenging thing. This would be challenging for a human. There's not a lot of context here to tell you what this kind of entity is, unless you knew a lot about how maybe northeastern villages and towns in North America would be called. Also, Mr. Deeds is not extracted as a whole entity, just Deeds is. Now ideally, we would have an NER model that would label West Chesterton Fieldville as a GPE and Mr. Deeds as a film. But we've got two problems. One, the machine learning model doesn't have film as an entity type. And on top of that, West Chesterton Fieldville is not coming out correct as GPE. So our goal right now is to fix both of these problems with an entity ruler. This would be useful if I were maybe doing some text analysis on fictional places referenced in films. So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all be classified as kind of fictional places. So let's go ahead and make a ruler to correct this problem. So what we're going to do is first we're going to make a ruler by saying ruler is equal to NLP dot add pipe. And this is going to take one argument here, you're going to find out when we start working with custom components that you can have a few different arguments here, especially if you create your own custom components. But for right now, we're working with the components that come standard with spacey. There's about 18 of them. One of them is the entity underscore ruler, all lowercase. We're going to add that ruler into our NLP model. And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER model and see as we go down that the NER pipe is here and the entity ruler is now the exit, the final pipe in our pipeline. So we see that it has been successfully added. Let's go ahead now and try to add patterns into that pipeline. Patterns are the things that the spacey model is going to look for in the label that it's going to assign when it finds something that meets that pattern. This will always be a list of lists. So let's go ahead and do this right now. Sorry, a list of dictionaries. So the first pattern that we're really looking for here is going to be a dictionary. It's going to have one key of label, which is going to be equal to GPE and another label of pattern, which is going to be equal to, in this case, we want to find West Chesterton Fieldville. Let me go ahead and just copy and paste it so I don't make a mistake here. And what we want to do is we want our entity ruler to see West Chesterton Fieldville. And when it sees it, assign the label of GPE. So it's a geopolitical entity. So it's a place. So let's go ahead and execute that. Great. We've got the patterns. Now comes time to load them into the ruler. So we can say ruler.add underscore patterns. This is going to take one argument. It's going to be our list of patterns added in. Cool. Now let's create a new doc object. We're going to call this doc to that's going to be equal to NLP. We're going to pass in that same text. We're going to say for int n doc to dot ints print off int dot text and end dot label. And you're going to notice that nothing has changed. So why has nothing changed? We're still getting the same results. And we've added the correct pattern in. The answer lies into one key thing. If we look back up here, we see that our entity ruler comes after our NER. What does that mean? Well, imagine how the pipeline works that I talked about a little while ago in this video. A pipeline works by different components, adding things to an object and making changes to it, in this case, adding ints to it, and then making those things isolated from later pipes from being able to overwrite them unless specified. What this means is that when West Chesterton field bill goes through and is identified by the NER pipe as a person, it can no longer be identified as anything else. What this means is that you need to do one of two things give your ruler the ability to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline. So let's go through and solve this common problem right now. We're going to create a new NLP object called NLP to, which is going to be equal to spacey dot load. And again, we're going to load in the English core web SM's model and core web SM. Great. And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going to make that an object too. Now what we can do is we can say ruler dot add patterns, again, we're going to go through all of these steps that we just went through, we're going to add in those patterns that we created up above. And now what we're going to do is we're going to actually do one thing a little different than what we did. What we're going to do is we're going to load this up again, and we're going to do an extra keyword argument. Now we can say either after or before here, we're going to say before NER, what this is going to do is it's going to place our NER before our entity will ever for the NER component. And now when we add our patterns in, we can now create a new doc object. Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print off int dot text, and dot label. Now we notice that it is correctly labeled as a GPE. Why is this? Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will notice that our entity ruler now in the pipeline sits before the NER model. In other words, we've given primacy to our custom entity ruler, so that it's going to have the first shot at actually correctly identifying these things, but we've got another problem here. This is coming out as a person, it should be Mr. Deeds as the entire collective multi word token, and that should be a new entity. We can use the entity ruler to add in custom types of labels here. So let's go ahead and do this same thing. Let's go ahead and just copy and paste our patterns, and we're going to create one more NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM. Great, we've got that loaded up. We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it before the NER pipe, NLP three, there we go. And what we need to do now is we need to copy in these patterns, and we're going to add in one more pattern. Remember this can be a list here. So this pattern, we're going to have a new label called film, and we're going to look for the sequence Mr. Deeds. And that's going to be our pattern that we want to add in to our ruler. So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember that one keyword argument, or one argument is going to be the list itself. And now we can create a new doc object, which is going to be equal to NLP three, I think I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text and and dot label. And if we execute this, we see now that not only have you gotten the entity ruler to correctly identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly, Mr. Deeds as a film. Now some of you might be realizing the problem here, this is actually a problem for machine learning models. And the reason for this is because Mr. Deeds in some instances could be the person and Mr. Deeds in other instances could be the movie itself. This is what we would call a toponym. So spelled like this, and this is a common problem in natural language processing. And it's actually one of the few problems or one of many problems really, that remain a little bit unsolved toponym resolution, spelled like this, or TR is the resolution of toponym. So things that can have multiple labels that are dependent upon context. Another example of toponym resolution is something like this, if you were to look at this word, let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology. Let's say it's only going to ever be a GPE. The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas. Toponym resolution is also the ability to resolve problems like this, when in context is Paris was kind of talking about Paris, France, when in context is it talking about Kentucky, and when in context is it talking about Texas. So that's something that you really want to think about when you're generating your rules for an entity ruler, is this ever going to be a false positive? And if the answer is that it's going to be a false positive half the time, or it's a 50-50 shot, then really consider incorporating that kind of an entity into a machine learning model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as a person. And learn with word embeddings when that context means it's a film and when that context means it's a person. That's just a little toy example. What we're going to see moving forward, though, and we're going to do this with a matcher, not with the entity ruler, is that spacey can do a lot of things. You might be thinking to yourself, now I could easily just come up with a list and just check and see whenever Mr. Deeds pops up and just inject that into the doc.ins. I could do the same thing with West Chesterton Field Bill. Why do I need an NLP framework to do this? And the answer is going to come up in just a few minutes when we start realizing that spacey can do a lot more than things like regex or things like just a basic gazetteer check or a list check. What you can do with spacey is you can have the pattern not just take a sequence of characters and look for a match, but a sequence of linguistic features as well, that earlier pipes have identified. And I think it's best if we save that for just a second when we start talking about the matcher, which is, in my opinion, one of the more robust things that you can do with spacey and what sets spacey apart from things like regex or other fancier string matching approaches. Okay, we're now moving into chapter six of this book, and this is really kind of, in my opinion, one of the most important areas in this entire video. If you can master the techniques I'm going to show you for the next maybe 20 minutes or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really going to see really kind of its true power. A lot of the stuff that we talk about here in the matcher can also be implemented in the entity ruler as well with a pattern. The key difference between the entity ruler and the matcher is in how data the data is kind of extracted. So the matcher is going to store information a little differently. It's going to store it as within the vocab of the NLP model. It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk about that more in just a second. And it's not going to store it in the doc ends. So matchers don't put things in your doc.ends. So when do you want to use a matcher over an entity ruler? You want to use the entity ruler when the thing that you're trying to extract is something that is important to have a label that corresponds to it within the entities that are coming out. So in my research, I use this for anything from like, let's say stocks, if I'm working with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside other entities. I'll also work in things like ships, so the names of ships, streets, things like that. When I use the the matcher, it's when I'm looking for something that is not necessarily an entity type, but something that is a structure within the text that will help me extract information. And I think that'll make more sense as we go through and I show you kind of how to improve examples going through it, we're kind of using the matcher as you would in the real world. But remember, all the patterns that I show you can also be implemented in the entity ruler. And I'm also going to talk about when we get to chapter eight, how rejects can actually be used to do similar things, but in a different way. Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic components, so the lemma of a word or the identifying if the word is a specific type of an entity, that's when you're going to want to use the matcher over rejects. And when you're going to use rejects is when you really have a complicated pattern that you need to extract. And that pattern is not dependent upon specific parts of speech, you're going to see with that how that works as we kind of go through the rest of part two, but keep that in the back of your mind. So let's go ahead and take our work over to our blank Jupiter notebook again. So what we're going to do is we're going to just set up with a basic example. We need to import spacey. And since we're working with the matcher, we also need to say from spacey dot matcher, import matcher with a capital M, very important capital M. Once we have this loaded up, we can start actually working with the matcher. And we're going to be putting the matcher in a just the small English model. And we're going to say NLP is equal to spacey dot load. And you should be getting familiar with this in core web SM, the small English model. Once we've got that loaded, and we do now, we can start actually working with the matcher. So how do you create the matcher? Well, the Pythonic way to do this and the weights in the documentation is to call the object a matcher, that's going to be equal to matcher with a capital M. So we're calling this class right here. And now what we need to do is we need to pass in one argument. This is going to be NLP dot vocab. We're going to see that we can add in some extra features here in just a little bit. I'm going to show you why you want to add in extra features at this stage, but we're going to ignore that for right now. What we're going to try to do is we're going to try to find email addresses within a text, a very simple task that's really not that difficult to do. We can do it with a very simple pattern because spacey has given us that ability. So let's create a pattern. And that's going to be equal to a list, which is going to contain a dictionary. The first item in the dictionary, or the first key, is going to be the thing that you're looking for. So in this case, we have a bunch of different things that the matcher can look for. And I'm going to be talking about all those in just a second. But one of them is very handily, this label of like email. So if the if the string or the sequence of tokens or the token is looking like an email, and that's true, then that is what we want to extract, we want to extract everything that looks like an email. And to make sure that this occurs, we're going to say matcher dot add. And then here, we're going to pass in two arguments, argument one is going to be the think of it as a label that we want to assign to it. And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll see in just a second. And the next thing is a pattern. And it's important here to note that this is a list. The argument here takes a list of lists. And because this is just one list right now, I'm making it into a list. So each one of these different patterns would be a list within a list, essentially the let's go ahead and execute that. And now we're going to say doc is equal to nlp. And I'm going to add in a text that I have in the textbook. And this is my email address w mattingly at aol.com. That might be a real email address. I don't believe it is, it's definitely not mine. So don't try and email it. And then we're going to say matches is equal to matcher doc. This is going to be how we find our matches. We pass that doc object into our matcher class. And now what we have is the ability to print off our matches. And what we get is a list. And this list is a set of tuples that will always have three indices. So index zero is going to be this very long number. What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook. And the next thing is the start token and the end token. So you might be seeing the importance here already. What we can do with this is we can actually go into the nlp vocab where this integer lies and find what it corresponds to. So this is where this is pretty cool. Check this out. So you print off nlp dot vocab. So we're going into that vocab object. We're going to index it matches zero. So this is going to be the first index, so this tuple at this point. And then we're going to grab index zero. So now we've gone into this list. We've gone to index zero, this first tuple, and now we're grabbing that first item there. Now what we need to do is we need to say dot text, you need to do it right here. And if you print this off, we get this email address, that label that we gave it up there was added into the nlp vocab with this unique lexeme that allows for us to understand what that number corresponds to within the nlp framework. So this is a very simple example of how a matcher works and how you can use it to do some pretty cool things. But let's take a moment, let's pause and let's see what we can do with this matcher. So if we go up into spacey's documentation on the matcher, we'll see that you got a couple different attributes you can work with. Now we've, we're going to be seeing this a little bit. The orth, this is the exact verbatim of a token. And we're also going to see text, the exact verbatim, text of a token. What we also have is lower. So what you can do here is you can use lower to say when the item is lowercase and it looks like and then give some lowercase pattern. This is going to be very useful for capturing things that might be at the start of a sentence. For example, if you were to look for the penguin in the text, anywhere you saw the penguin. If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the start of a sentence. It would miss it because the T would be capitalized. By using lower, you can ensure that your pattern that you're giving it is going to be looking for any pattern that matches that when the text is lowercase. If is going to be the, the length of your token text is alpha is ASCII is digit. This is when your characters are either going to be alphabetical ASCII characters. So the American standard coding initiative, I can't remember what it stands for, but it's that, I think it's 128 bit thing that America came up with when they started in coding text. It's now replaced with UTF eight and is digit is going to look for something if it is a digit. So think of each of these as a token. So if the token is a digit, then that counts in the pattern is lower is upper is title. These should be all self explanatory. If it's lowercase, if it's uppercase, if it's a title, so capitalized. And if you don't understand what all of these do right now, I'm going to be going through and showing you in just a second, just giving you an overview of different things that can be included within the, the matcher or the entity ruler here. So what we can also do is find something that if the token is actually the start of a sentence, if it's like a number, like a URL, like an email, you can extract it. And here is the main part I want to talk about because this is where you're really going to find spacey out shines any other string matching system out there. So what you can do is you can use the tokens, part of speech tag, morphological analysis, dependency label, lima and shape to actually make matches. So not just matching a sequence of characters, but matching a sequence of linguistic features. So think about this. If you wanted to capture all instances of a proper noun followed by a verb, you would not be able to do that with regex. There's not a way to do it. You can't give regex if this is a verb. Regex is just a string matching framework. It's not a framework for actually identifying linguistic features, using them and extracting them. So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological analysis, the depth, the lemma, et cetera. We can actually use all those things that have gone through the pipeline and the matcher can leverage those linguistic features and make some really cool, allow us to make really cool patterns that can match really robust and complicated things. And the final thing I'm going to talk about is right here, the OP. This is the operator or quantifier and determines how often to match a token. So there's a few different things you can use here. There's the exclamation mark, negate the pattern, requiring it to match zero times. So in this scenario, the sequence would never occur. There's the question mark, make the pattern optional, allowing it to match zero or one times require the pattern to match one or more times with the plus and the asterisk, the thing on the shift eight, allow the pattern to match zero or more times. There's other things as well that you can do to make this match or a bit more robust. But for right now, let's jump into the basics and see how we can really kind of take these and apply them in a real world question. So what I'm going to do is I'm going to work with another data set or another piece of data that I've grabbed off of Wikipedia. And this is the Wikipedia article entry on Martin Luther King, Jr. It's the opening opening few paragraphs, let's print it off and just take a quick look. And this is what it looks like. You can go through and read it. We're not too concerned about what it says right now. We're concerned about trying to extract a very specific set of patterns. What we're interested in grabbing are all proper nouns. That's the task ahead of us. Somebody has asked us to take this text in, extract all the proper nouns for me, but we're going to do a lot more and not just the proper nouns, but we want to get multi word tokens. So we want to have Martin Luther King, Jr. extracted as one token, so one export. So the other things that we want to have are these kind of structured in sequential order. So find out where they appear and extract them based on their start token. So let's go ahead and start trying to do some of these things right now. Let's scroll down here. Great. So we need to create really a new NLP object now at this point. So let's create a new one. We're going to start working with the Ncore Web SM model. If you're working with a different model, like the large or the transformer, you're going to have more accurate results. But for right now, we're just trying to do this quickly for demonstration purposes. So again, just like before, we're creating that with NLP dot vocab. And then we're going to create a pattern. So this is the pattern that we're going to work with. We want to find any occurrence of a POS part of speech that corresponds to proper noun. That's the way in which POS labels proper nouns is prop in. And we should be able to with that extract all proper nouns. So we can say matcher dot add, and we're going to say proper noun. And that's going to be our pattern. And then what we can do just like before, we're going to create the doc object. This is going to be NLP text. And then we're going to say matches is equal to matcher doc. So we're going to create the matches by passing that doc object into our matcher class. And then we're going to print off the length of the matches. So how many matches were found, and then we're going to say for match in matches. And we're just going to grab the first 10 because I've done this and there's a lot and you'll see why let's print off. Let's print off in this case, match. And then we're going to print off specifically what that text is. Remember, the output is the lexine followed by the start token and the end token, which means we can go into the doc object. And we can set up something like this. We can say match one, so index one, which is the start token and match two, which is the end token. And that'll allow us to actually index what these words are. And when we do this, we can see all these printed out. So this is the match, the lexine here, which is going to be proper down all the way down. We've got the zero here, which corresponds to the start token, the end token. And this is the the token that we extracted. Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right? So the problem should be pretty obvious right now. And the problem is that we have grabbed all proper nouns, but these proper nouns are just individual tokens. We haven't grabbed the multi word tokens. So how do we go about doing that? Well, we can solve this problem by let's go ahead and just copy and paste all this from here. And we're going to make one small adjustment here. We're going to change this to OP with a plus. So what does that mean? Well, let's pop back into our matcher under spacey and check it out. So OP members, the operator or quantifier, we're going to use the plus symbol. So it's going to look for a proper noun that occurs one or more times. So in theory, right, this should allow us to grab multi word tokens. It's going to look for a proper noun and grab as many as there are. So anything that occurs one or more times, if we run this, though, we see a problem. We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther King, Luther King, King Martin Luther King, Junior, what what is going on here? Well, you might already have figured it out. It has done exactly what we told it to do. It's grabbed all sequence of tokens that were proper nouns that occurred one or more times. Just so happens some of these overlap. So token that's doc zero to one, zero to two. So you can see the problem here is it's grabbing all of these and any combination of them. What we can do, though, is we can add an extra layer to this. So let's again, copy what we've just done because it was, it was almost there. It was good, but it wasn't great. We're going to do one new thing here when we add in the patterns, we're going to pass in the keyword argument, greedy, we're going to say longest capital, all capital letters here. And if we execute that, it's going to look for the longest token out of that mix, and it's going to give that one, make that one the only token that it extracts. We noticed that our length has changed from what was it up here, 175 to 61. So this is much better. However, we should have recognized right now, another problem. What have we done wrong? Well, what we've done wrong is these are all out of order. In fact, what happens is when you do this, I don't have evidence to support this, but I believe it's right. What will always happen is the, the greedy longest will result in all of your tokens being organized or all your matches being organized from longest to shortest. So if we were to scroll down the list and look at maybe negative one, negative, let's do negative 10 on, you'll see single word tokens. And again, this is me just guessing, but I think based on what you've just seen, that's a fairly good guess. So let's go ahead and just kind of so we can see what the output is here. So how would you go about organizing these sequentially? Well, this is where really kind of a sort comes in handy when you can pass a lambda to it. I can copy all this again, because again, we almost had this right. Here we're going to sort our matches though, we can say matches.sort, and this is going to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb is going to allow us to actually iterate over all this and find any instance where X occurs. And we're going to say to sort by X one. So what this is, it's a list of tuples. And what we're using lambda for is we're going to say sort this whole list of tuples out, but sort it by the first index, in other words, sort it by the start token. And when we execute that, we've got everything now coming out as we would expect and nor these typos that exist. We've got zero to four, six to nine. So we actually are extracting these things in sequential order as they appear in our text. You can actually go through and sort the appearance of the, of the matcher. But what if our, the person who kind of gave us this job, they were happy with this, but they came back and said, okay, that's cool. But what we're really interested in what we really want to know is every instance where a proper noun of any length, grab the multi word token still, but we want to know anytime that occurs after a verb. So anytime this proper noun is followed by a verb. So what we can do is we can add in, okay, okay, we can do this. We're going to have a comma here. So the same pattern is going to be a sequence now. It's not just going to be one thing. We're going to say token one needs to be a proper noun and grab as many of those tokens as you can zero or one to more times. And then after those are done comma, this is where the next thing has to occur POS. So the part of speech needs to be a verb. So the next thing that comes out needs to be a verb. And we want that to be the case. Well, when we do this, we can kind of go through and see the results of the first instance of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced 258 director J Edgar Hoover considered. Now we're able to use those linguistic features that make Spacey amazing and actually extract some vital information. So we've been able to figure out where in this text a a proper noun is proceeded by a verb. So you can already start to probably see the implications here. And we can we can create very elaborate things with this. We can use any of these as long of a sequence as you can imagine. We're going to work with a different text and kind of demonstrate that it's a fun toy example. We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file. I'm going to load it in right now. And then I'm going to just grab the first sentence from the first chapter. And what we have here is the first sentence. So here's our scenario. Somebody has asked us to grab all the quotation marks and try to identify the person described or the person described the person who's doing the speaking or the thinking. In other words, we want to be able to grab Alice thought. Now I picked Alice in Wonderland because of the complexity of the text, not complexity in the sense of the language used children's book, but complexity and the syntax. These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly inconsistent in how we structured these kind of sequences of quotes. And the other thing I chose to do as I left in one mistake here, and that is this non standardized quotation mark. So remember, when you need to do this, things need to match perfectly. So we're going to replace this first things first is to create a cleaner text, or we do text equals text dot replace, and we're going to replace the instance of I believe it's that mark, but let's just copy and paste it in to make sure we're going to replace that with a, with a single quotation mark, and we can print off text just to make sure that was done correctly. Cool. Great. It was it's now looking good. Remember, whenever you're doing information extraction, standardize the texts as much as possible. Things like quotation marks will always throw off your data. Now that we've got that, let's go ahead and start trying to create a fairly robust pattern to try to grab all instances where there is a quotation mark, thought, something like this, and then followed by another quotation mark. So the first thing I'm going to try and do is I'm going to try to just capture all quotation marks and a text. So let's go through and try to figure out how to do that right now. So we're going to copy in a lot of the same things that we used up above, but we're going to make some modifications to it. Let's go ahead and copy and paste all that we're going to completely change our pattern. So let's get rid of this. So what are we looking for? Well, first of all, the first thing that's going to occur in this pattern is this quotation mark. So that's going to be a full text match, which is an or if you remember, and we're going to have to use double quotation marks to add in that single quotation mark. So that's what we grabbed first. We're going to look for anything that is an or and the next thing that's going to occur after that, I think this is good to probably do this now on a line by line basis. So we can keep this straight. So the next thing that's going to occur is we're looking for anything in between. So anything that is an alpha character, we're going to just grab it all. So is alpha and then we need to say true. But within this, we need to specify how many times that occurs because if we say is true, it's just going to look at the next token in this case and and then say that's the end. That's it. That's the pattern. But we want to grab not just and but and what is the use of a everything. So we need to grab not only that, but when you say OP, so our operator again. And if you said plus, you would be right here. We need to make sure that it's a plus sign, so it's grabbing everything. Now in this scenario, this is a common construct is when you have a injection here in the middle of the sentence. So thought or said, and it's the character doing it. This oftentimes got a comma right here. So we need to add in that kind of a feature. So there could be is punked. There could be a punked here. And we're going to say that that is equal to true. But that might not always be the case. There might not always be one there. So we're going to say OP is equal to a star. We go back. We'll see why. To our OP, the star allow the pattern to match zero or more time. So in this scenario, the punctuation may or may not be there. So that's the next thing that occurs. Once we've got that, the last thing that we need to match is the exact same thing that we had at the start is this or appear. And that's our sequence. So this is going to look for anything that starts with a quotation mark has a series of alpha characters has a punctuation like a comma possibly, and then closes the quotation marks. If we execute this, we succeeded. We got it. We extracted both matches from that first sentence. There are no other quotation marks in there. But our task was not just to extract this information. Our task was also to match who is the speaker. Now we can do this in a few different ways and you're going to see why this is such a complicated problem in just a second. So let's go ahead and do this. How can we make this better? Well, we're going to have this occur twice. But in the middle, we need to figure out when somebody is speaking. So one of the things that we can do is we can make a list. So let's make a list of limitized forms of our verbs. So we're going to say, let's call this speak underscore limits. This can be equal to a list. And the first thing we're going to say is think, because we know that think is in there and say this is the limitized form of thought and said. So what we can do now is after that occurs, it's adding a new thing. We're going to be able to now add in a new pattern that we're looking for. And so not just the start of a quotation mark, not just the end of a quotation mark, but also a sequence that'll be something like this. So it's going to be a part of speech. So it's going to be a verb that occurs first, right? And that's going to be a verb. But more importantly, it's going to be a lemma that is in what did I call you speak lemmas? So let's break this down. The next token needs to be a verb. And it needs to have a limitized form that is contained within the speak lemmas list. So if it's got that fantastic, let's execute this and see what happens. We should only have one hit. Cool. We do. So we've got that first hit. And the second one hasn't appeared anymore because that second quotation mark wasn't proceeded by a verb. Let's go ahead and make some modifications that we can improve this a little bit. Because we want to know not just what that person's doing. We also need to know who the speaker is. So let's grab it. Let's figure out who that speaker is. So we can use part of speech. Again, another feature here. We know that it's going to be a proper noun because oftentimes proper nouns are doing the speaking. Sometimes it might not be. Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those situations for just right now. So we're looking for a proper noun. But remember proper nouns, as we saw just a second ago, could be multiple tokens. So we're going to say OP plus. So it could be a sequence of tokens. Let's execute this. Now we've captured Alice here as well. So and is the use and what is the use of a book thought Alice. Now we know who the speaker is, but this is a partial quotation. This is not the whole thing. We need to grab the other quote. Oh, how will we ever do that? Well, we've already solved that. We can copy and paste all of this that we already have done right down here. And now we've successfully extracted that entire quote. So you might be thinking to yourself, yeah, we did it. We can now extract quotation marks and we can even extract, extract, you know, any instance where there's a quote and somebody speaking. Not so fast. We're going to try to iterate over this data. So we're going to say for text in data, zero twos, we're going to iterate over the first chapter. And we're going to go ahead and let's let's do all of this. Doc is going to be equal to that sort that out. And then again, we're going to be printing out this information, the same stuff I did before, just now it's going to be iterating over the whole chapter. And if we let this run, we've got a serious, serious problem. And it doesn't actually grab us anything. Nothing has been grabbed successfully. What is going on? We've got a problem. And that problem stems from the fact that our patterns and the problem is that we don't have our our text correctly, we're being removing the quotation mark that was the problem up above. So we're going to add this bit of code in. And we're going to be able to fix it. So now when we execute this, we see that we've only grabbed one match. Now you might be thinking to yourself, there's an issue here and there there is, let's go ahead and print off the length of matches. And we see that we've only grabbed one match. And then we haven't grabbed anything else. Well, what's the problem here? Are there are there no other instances of quotation marks in the rest of the first chapter? And the answer is no, there are. There absolutely are other quotation marks and other paragraphs from the first chapter. The problem is, is that our pattern is singular. It's not multivariate. We need to add in additional ways in which a text might be structured. So let's go ahead and try and do this with some more patterns. I'm going to go ahead and copy and paste these in from the textbook. So you'll be able to actually see them at work. And so what I've did, I've done is I've added in more patterns, pattern two and pattern three allow for instances like this, well thought Alice. So an instance where there's a punctuation, but there's no proceeding quotation after this, and then which certainly said before an instance where there's a comma followed by that. So we've been able to capture more variants and more ways in which quotation marks might exist followed by the speaker. Now this is where being a domain expert comes into play. You'd have to kind of look through and see the different ways that Louis C. Carroll structures quotation marks and write out patterns for capturing them. I'm not going to go through and try to capture everything from Alice in Wonderland because that would take a good deal of time. And it's not really in the best interest because it doesn't matter to me at all. What I encourage you to do, if this is something interesting to you is try to apply it to your own texts, different authors, structure quotation marks a little differently than what patterns that I've gotten written here are a good starting point. But I would encourage you to start playing around with them a little bit more. And what you can do is when you actually have this match extracted, you know that the instance of a proper noun that occurs between these quotation marks or after one is probably going to be the person or thing that is doing the speaking or the thinking. So that's kind of how the matcher works. It allows for you to do these things, these robust type data extractions without relying on entity ruler. And remember, you can use a lot of these same things with an entity ruler as well. But we don't want this in this case, we don't want things like this to be labeled as entities. We want them to just be separate things that we can extract outside of the of the ints dot doc dot ints. That's going to be where we conclude our chapter on on the on the matcher. In the next section of this video, we're going to be talking about custom components in spacey, which allow for us to do some pretty cool things such as add in special functions that allow for us to kind of do different custom shapes, permutations on our data with components that don't exist like an entity ruler would be a component components that don't exist within the spacey framework. So add in custom things like an entity ruler that do very specific things to your data. Hello, we're now moving into a more advanced aspect of the textbook specifically chapter seven. And that's working with custom components. A good way to think about a custom component is something that you need to do to the doc object or the doc container that spacey can't do off the shelf. You want to modify it at some point in the pipeline. So I'm going to use a basic toy example that demonstrates the power of this. Let's look at this basic example that I've already loaded into memory. It's two sentences that are in the doc object now. And that's Britain is a place. Mary is a doctor. So let's do for int and doc dot ints print off int dot text and dot label. And we see what we'd expect. Britain is GPE a geopolitical entity. Mary is a person. That's fantastic. But I've just been told by somebody higher up that they want the model to never ever give anything as GPE or maybe they want any instance of GPE to be flagged as LOC. So all the different locations all have LOC as a label or we just want to remove them entirely. So I'm going to work with that latter example. We need to create a custom pipe that removes all instances of GPE from the doc dot ints container. So how do we do that? Well, we need to use a custom component. We can do this very easily in spacey by saying from spacey dot language import language capital L very important there. Capital L now that we've got that class loaded up. Let's start working with this. What we need to do first is we need to use a flag. So the symbol and we need to say at language dot component. And we need to give that component a name. We're going to say in this case, let's say remove GPE. And now we need to create a function to do this. So we're going to call this remove GPE. I always kind of keep these as the same. That's my personal preference. And this is going to take one, one, one thing. That's going to be the doc object. So the doc object, think about how it moves through the pipeline. This component is another pipe and that pipeline. It needs to receive the doc object and send off the doc object. You could do a lot of other things. It could print off entity found. It could do really any number of things. It could add stuff to the data coming out of the pipeline. All we're concerned with right now is modifying the doc dot ints. So we can do something like this. We can say original ends is equal to a list of the doc dot ends. So remember, we have to convert the ends from a generator into a list. Now what we can do is we can say for int and doc dot ends, if the end not label. So if that label is equal to GPE, then what we want to do is we want to just we just want to remove it. So let's say original ints.remove and we're going to remove the int. Remember, it's now a list. Sorry, I executed that too soon. Remember, it's now a list. So what we can do is we can go ahead now and convert those original ends back into doc dot ends by saying doc dot ends equals original ends. And if we've done things correctly, we can return the doc object and it will have all of those things removed. So this is what we would call a custom component. Something that changes the doc object along the way in the pipeline, but we need to add it to NLP. So we can do NLP dot add pipe. We want to make sure that it comes after the NER. So we're just going to say, uh, add the pipe or move GPE corresponds to the component name. And now let's go ahead and NLP dot analyze pipes. And you'll be able to see that it sits at the end of our pipeline right there. Remove GPE. Now comes time to see if it actually works. So we're going to copy and paste our code from earlier up here. Let's go ahead and copy this. And now we're going to say for int and doc dot ends print off int dot text. And dot label. And we should see, as we would expect, just marry coming out. Our pipeline has successfully worked. Now, as we're going to see when we move into red checks, you can do a lot of really, really cool things with custom components. I'm going to kind of save the, the advanced features for, I think I've got it scheduled for chapter here, chapter nine in our textbook. This is just a very, very basic example of how you can introduce a custom component to your spacey pipeline. If you can do this, you can do a lot more. You can maybe change a different entity. So they have different labels. You can make it where GPEs and locks all agree. You can remove certain things. You can have it print off place found person found. You can do a lot. So really the sky's the limit here, but a lot of the times you're going to need to modify that doc object. And this is how you do it with a custom pipe so that you don't have to write a bunch of code for a user outside of that NLP object, the NLP object. Once you save it to disk by doing something like NLP dot to disk data, new and core web SM, it's going to actually be able to go to the disk and be saved with everything. But one thing that you should note is that the component that you have here is not automatically saved with your data. So in order for your component to actually be saved with your data, you need to store that outside of this entire script. You need to save it as a library that can be given to the model when you go to package it. That's beyond the scope of this video for right now. In order for this to work in a different Jupyter notebook, if you were to try to use this, this container, this component has to actually be in the script. When it comes time to package your model, your pipeline and distribute it, that's a different scenario. And that scenario, you're going to make sure that you've got a special my component dot pie file with this bit of code in there so that, so that spacing knows how to handle your particular data. It's now time to move on to chapter eight of this textbook. And this is where a spacey gets really interesting. You can start applying regular expressions into a spacey component like an entity ruler or a custom component, as we're going to see in just a moment with chapter nine. I'm not going to spend a good deal of time talking about regular expressions. I could spend five hours talking about regex and what all it can do. In the textbook, I go over what you really need to know, which is what regular expressions is, which is as a way to do a really robust string pattern matching. I talk about the strengths of it, the weaknesses of it, its drawbacks, how to implement it in Python and how to really work with regex. But this is a video series on spacey. What I want to talk about is how to use regex with spacey. And so let's move over to a Jupiter notebook where we actually have this code to execute and play around with. If we look here, we have the same example that we saw before. What my goal is is not to extract the whole phone number, rather try to grab this sequence here. And we do this with a regular expression pattern. What this says is it tells it to look for a sequence of tokens or sequence of characters like this. It's going to be three digits followed by a dash followed by four digits. And if I were to execute this whole code, nothing is printed out. Does that mean that I failed to write good regex? No, it does not at all. It's failed for one very important reason. And this is the whole reason why I have this chapter in here is that regex, when it comes to pattern matching, pattern matching only really works when it comes to regex for single tokens. You can't use regex across multi-word tokens, at least as of spacey 3.1. So what does that mean? Well, it means that that dash right there in our phone number is causing all kinds of problems. If we move down to our second example, it's going to be the exact same pattern. A little different. Let me go ahead and move this over so you can see it a bit better. It's going to be regex that looks like this, where we just look for a sequence of five digits, we execute that, we find it just fine. And the reason for that is because this does not have a dash. So regex, if you're familiar with it, if you've worked with it, it's very powerful. You can do a lot of cool things. When you're going to use this in Python, if you're using just the standard off the shelf components. So the entity ruler, the matcher, you're going to be using this when you want to match regex to a single token. So think about this, if you're looking for a word that starts off with a capital D, and you want to just grab all words that start with a capital D, that would be an example of when you would want to use it in a standard off the shelf component. But that's not all you can do in spacey. You can use regex to actually capture multi word tokens. So capture things like Mr. Deeds. So any instance of Mr. Period Space Name, a sequence of proper nouns. You can also use it to, but yet in order to do that, you have to actually understand how to add in a custom component for it. And we're going to be seeing that in just a second as we move on to chapter nine, which is advanced regex. If you're not familiar with regex at all, take a few minutes, read chapter eight. I encourage you to do so because I go over in detail and I talk about how to actually engage in regex and Python and its strengths and weaknesses. What I want you to really focus on though, and get away from, get from all this is how to do some really complex multi word token matching with regex. Remember, you're going to want to use regular expressions when the pattern matching that you want to do is unindependent of the, the lima, the POS, or any of the linguistic features that space is going to use. If you're working with linguistic features, you have to use the spacey pattern, pattern matching things like the morph, the orth, the lima, things like that. But if your sequence of strings is not dependent on that, so you're looking for any instance of, in this case, we're going to talk about in just a second, a, a case where Paul is followed by a capitalized letter and then a word break. Then you're going to want to use regular expressions because in this case, this is independent of any linguistic features and regular expressions allows for you to write much more robust patterns, much more quickly. If you know how to use it well, and it allows for you to do much more quick robust things within a custom component. And that's going to be where we move to now. Now that we know a little bit about regex and how it can be implemented in Python, let's go ahead and also in spacey, let's go ahead and try and see how we can get regex to actually find multi word tokens for us within spacey using everything in the spacey framework. So the first thing I'm going to do to kind of demonstrate all this is I'm going to import regex. This comes standard with Python and you can import it as RE just that way. Import RE and that's going to import regex. I'm going to work from the textbook and work with this sample text. So this is Paul Newman was an American actor, but Paul Hollywood is a British TV TV host. The name Paul is quite common. So it's going to be the text that we work with throughout this entire chapter. Now a regex pattern that I could write to capture all instances of things like Paul Newman and Paul Hollywood, which is what my goal is, could look something like this, I could say or make an R string here and say Paul, and then I'm going to grab everything that starts with a capital letter and then my grab everything until a word break. And that's going to be a pattern that I can use in regex with this formula means is find any instance of Paul proceeded by a in this case, a capital letter until the actual word break. So grab the first name Paul and then what we can make a presumption is going to be that individual's last name in the text, a simple example, but one that will demonstrate our kind of purpose right now. So how we can do this is we can create an object called matches and use regex dot find it or we can pass in the pattern and we can pass in the text. So what this is going to do is it's going to use regex to try to find this pattern within this text. And then what we can do is we can iterate over those matches. So for match and matches, we can grab and print off the match and we have something that looks like this. What we're looking at here is what we would call it a regex match object. It's got a couple of different components here. It's got a span, which tells us the start character and the end character. And then it has a match and what this match means is the actual text itself. So the match here is Paul Newman and the match here is Paul Hollywood. So we've been able to extract the two entities in the text that begin with Paul and have a proper last name structured with a capital letter. We grabbed everything up until the word break. That's great. That's going to be what you need to know kind of going forward because what we're going to do now is we're going to implement this in a custom spacey pipe. But first let's go through and write the code so that we can then easily kind of create the pipe afterwards. So what we need to do is we need to import spacey and we also need to say from spacey dot tokens import span and we're going to be importing a couple of different things as we move forward because we're going to see that we're going to make a couple of mistakes intentionally. I'm going to show you how to kind of address these common mistakes that might surface in trying to do something like this. So once we've imported those two things, we can start actually writing out our code. Again, we're going to stick with the exact same text and again, we're going to stick with the exact same pattern that we've got stored in memory up above. So what we need to do now is we need to create a blank spacey object or sorry, a blank spacey pipeline that we can kind of put all this information into. And for right now what we're going to do is we're just going to kind of go through and look at these individual entities. So again, we're going to create the doc object, which is going to be equal to nlp text and this is not going to be necessary for right now, but I'm establishing a kind of a consistent workflow for us and you're going to see how we kind of take all this and implement it inside of a pipeline. So we're going to say original ends is equal to list doc dot ends. Now in this scenario, there's not going to be any entities because we don't have an any R or an entity ruler in our blank spacey pipeline. What we're going to do next is we're going to create something called an nwt int and that's going to stand for multi word token entity. You can name this whatever you like. This is just what I kind of stick to and then we're going to do and this is straight from the spacey documentation. We're going to say for match an RE dot find it or the same thing that we saw above pattern doc dot text. So what this is going to do is it's going to take that doc object. Look at it as raw text because remember the doc object is a container that doesn't actually have raw text in it until you actually call the dot text attribute and then our goal is for each of these things. We're going to look and call in this span. So we're going to say is start and the end is equal to match dot span. So what we're doing here is we're going in and grabbing the span attribute and we're grabbing these two components the start and the end. But we have a problem. These are character spans. Remember the doc object works on a token level. So we've got to kind of figure out a way to reverse engineer this almost to actually get this into a spacey form. Fortunately the doc object also has an attribute called character span. So what we can do is we can say the span is equal to doc dot char span start and end. So what this is going to do is it's going to print off essentially for us. Let's go ahead and do that. It would print off for us where we worry to actually have an entity here. It would print off for us as we can see Paul Newman and Paul Hollywood. So what we need to do now is we need to get this span into our entities. So what we can do is instead of printing things off we can say if span is not none because in some instance instances this will be the case. You're going to say NWT ends dot append. You're going to append a tuple here span dot start span dot end span dot text. So this is going to be the start the end and the text itself. And once we've done that we've managed to get our multi word tokens into a list that looks like this. Start and Paul Newman Paul Hollywood and notice that our span dot start is aligning not with a character span. Now it's rather aligning with a token span. So what we've done is we've taken this character span here and been able to find out where they start and end within the the token sequence. So we have zero and two. So Paul Newman one this was the zero index. It goes up until the second index. So it grabs index token zero and token one and we've done the same thing with Paul Hollywood. Now we've got that data. We can actually start to inject these entities into our original entities. So let's go through and do that right now. So we can do once we've got these things appended to this list. We can start injecting them into our original entities. So we can say for end in MWT ends. What we want to do is we want to say the start the end and the name is equal to end because this is going to correspond to the tuple the start the end and the entity text. Now what we can do is we can say per end. So this is going to be the individual end. We're going to create a span object in spacey. It's going to look like this. So a capital S here. Remember we imported it right up here. This is where we're going to be working with the span class and this is going to create for us a span object that we can now safely inject into the spacey doc.ins list. So we can say doc start and label and this is going to be the label that we want to actually assign it and this is going to be person in this case because these are all people we can do now as we can go through and say doc we can inject this into the original ends. Original ins dot append and we're going to append the per end which is going to be this span object and finally what we can say is doc.ins is equal to original ends kind of like what we saw just a few moments ago and let's go ahead and print off. We've got our entities right there or we to do this up here when we first kind of create the doc object you'll see nothing an empty list but now what we've been able to do is inject these into the doc object the doc.ins attribute and we can say for end and doc.ins just like everything else and dot text and dot label and because we converted it into a span we were able to inject it into the entity attribute from the doc object kind of natively so that spacey can actually understand it. So what can we do with this well one of the things that we could do is we can use the knowledge that we just acquired about custom components and build a custom component around all of this. So how might we do that well let's go through and try it out. The first thing that we need to do is we need to import our language class so if you remember from a few moments ago whenever you need to work with a custom component you need to say from spacey dot language import language with a capital L what we're going to do now is we're going to take the code that we just wrote and we're going to try to convert that into an actual custom pipe that can fit inside of our pipeline as kind of our own custom entity ruler if you will. So what we're going to do now is we're going to call this language dot component and we're going to call this let's call this Paul NER something not too not too clever but kind of very descriptive we're going to call this Paul NER and this is going to take that single doc object because remember this pipe needs to receive the doc object and do stuff to it. So what we can do is we can take all this code that we just wrote. From here down and paste it into our function and what we have is the ability now to implement this as a custom pipe. We don't need to do this because we don't want to print things off but here we're going to return the doc object. So we have now is a custom kind of entity ruler that uses regex across multiple tokens. If you want to use regex in spacey across multiple tokens as of spacey 3.1 this is the only way to implement this. So now we can take this pipe and we can actually add it to a blank custom model. So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're going to create a blank English model nlp2 dot add pipe. We're going to add in Paul NER. And now we see that we've actually created that successfully. So we have one pipe kind of sitting in all of this. Now what we can do is we can go through and we need to probably add in our pattern as well here just for good practice because this should be stored somewhat adjacent. I like to sometimes to keep it up here when I'm doing this but you can also keep it kind of inside of the function itself. Let's go ahead and just kind of save that and we're going to rerun this cool. Now what we can do is we can say doc to is equal to nlp2 we're going to go over that exact same text and we're going to print off our doc to dot ints and we've now managed to implement that as a custom spacey pipe but we've got one big problem. Let's say just hypothetically we wanted to also kind of work in really a another kind of something into our actual pipeline. We wanted this pipeline to sit on top of maybe an existing spacey model and for whatever reason we don't want Paul Hollywood to have that title. We wanted to have the title. Maybe we want to just kind of keep Paul Hollywood as a person but we also want to find maybe other cinema style entities. So we're going to create another entity here instead of all this that's going to be something like let's go ahead and make a new a new container down here a new component down here. We're going to just look for any instance of Hollywood and we're going to call that the word the label of cinema. So I want to demonstrate this because this is going to show you something that you are going to encounter when you try to implement this in the real world and I'm going to show you how to kind of address the problem that you're going to encounter. So if we had a component that looked like this now it's going to look for just instance instances of Hollywood and let's call this Holly Cinema NER and change this here as well. What we can do now is go ahead and load that up into memories. We've got this new component called Cinema NER and just like before we're going to create an LP three now this is going to be spacey dot load in core web. SM and so what this is going to do is it's going to load up the spacey small model and LP three dot add pipe and it's going to be the what did I call this again the cinema NER and if we were to go through and add that and create a new object called doc three make that equal to an LP three text. We're going to get this air and this is a common air and if you Google it you'll eventually find the right answer. I'm just going to give it to you right now. So what this is telling you is that there are spans that overlap that don't actually work because one of the spans for cinema is Hollywood and the small model is extracting not only that Hollywood as a cinema but it's also extracting Paul Hollywood as part of a longer token. So what's happened here is we're trying to assign a span to two of the same tokens and that doesn't work in spacey. It'll break so what can you do? Well a common method of solving this issue is to work with the filter spans from the spacey dot util. Let's go ahead and do this right now so you can say from spacey dot util import filter spans. What filter spans allows for you to do is to actually filter out all of the the spans that are being identified. So what we can do is we can say at this stage. Before you get to the dock dot ends you can say filtered is equal to filter spans original ends. So what does this do? Well what this does is it goes through and looks at all of the different start and end sections from all of your entities. And if there is an ever an instance where there is a an overlap of tokens so 8 to 10 and 9 to 10. Primacy and priority is going to be given to the longer token. So what we can do is we can set this now to filtered and it helps if you call it correctly filtered. There we go. We can set that to filtered instead of the original entities. Go ahead and save that. We're going to add this again and we're going to do doc 3 and we're going to say for int and doc 3 dot ends print int dot text and int dot label. And if we've done this correctly we're not going to see the cinema label come out at all because Paul Hollywood is a longer token than just Hollywood. So what we've done is we've set told spacey give the primacy to the longer tokens and assign that label by filtering out the tokens you can prevent that air from ever surfacing. But this is a very common thing that you're going to have to implement sometimes rejects really is the easiest way to inject and do pattern matching in the entity. Okay. So here's the scenario that we have before us in order to make this live this kind of live coding and applied spacey a little bit more interesting imagine in this scenario we have a client and the client is a stockbroker or somebody who's interested in investing and what they want to be able to do is look at news articles like those coming out of Reuters and they want to find the news articles that are the most relevant to what they need to actually search for and read for the day. So they want to find the ones that deal with their their personal stocks their holdings or maybe their the specific index that they're actually interested in. So what this client wants is a way to use spacey to automatically find all companies referenced within a text all stocks referenced within a text and all indexes referenced within the next text and maybe even some stock exchanges as well. Now on the actual textbook if you go through to this chapter which is number 10 you're going to find all the kind of solutions laid out for you what I'm going to do throughout the next 30 or 40 minutes is kind of walk through how I might solve this problem at least on the surface. This is going to be a rudimentary solution that demonstrates the power of spacey and how you can apply it in a very short period of time to do some pretty custom tasks such as financial analysis with that structured data that you've extracted you can then do any number of things what we're going to start off with though is importing spacey and importing pandas as PD if you're not familiar with pandas I've got a whole tutorial series on that on my channel Python tutorials for digital humanities even though it has digital humanities in the title it's for kind of everyone but go through if you're not familiar with pandas and check that out you're not really going to need it for for this video here you're going to just need to understand that I'm using pandas to access and grab the data that I need from a couple CSV files or comma separated value files that I have. So the first thing that we need to do is we need to create what's known as a pandas data frame and this is going to be equal to PD dot read CSV and I actually have these stored in the data sub folder in the repo you have free access to these they're a little tiny data sets that I cultivated pretty quickly they're not perfect but they're good enough for our purposes and we're going to use the separator keyword argument which is going to say to separate everything out by tab because these are CSV files tab separated value files and we have something that looks like this so what this stocks dot CSV file is is it's all the symbols company names industry and market caps for I think it's around five thousand seven hundred different stocks five thousand eight hundred and seventy nine and so what we're going to use this for is as a way to start working into an entity ruler all these different symbols and company names what we want to do is we want to use these symbols to work into a model as a way to grab stocks that might be referenced and you can already probably start to see a problem with this capital a here we're going to get to that in a little bit and we want to grab all the company names so we can maybe create two different entity types from this data set stock and company so let's go through and make these into lists so they're a little bit more so let's go through and make these into lists so they're a little bit more manageable what we need to do is we need to create a list of symbols and that's going to be equal to DF dot symbol dot two list this is a great way to do it and pandas so you can kind of easily convert all these different columns into different lists that you can work with in Python so companies is going to be equal to DF dot company and name I believe the name was two list and just to demonstrate how this works let's print off symbols we're going to print up to 10 and you can kind of see we've managed to take these columns now and kind of build them into a simple Python list so what can we do with that well one of the things that we can do is we can use that information to start cultivating an entity ruler but remember we want more things than just one or two kind of in our entity ruler we don't just want stocks and we don't just want companies we also want things like indexes we're going to get to that in just a second though for right now let's try to work these two things into an entity ruler how might we go about doing that well as you might expect we're going to create a fairly simple entity ruler so we're going to say is nlp is going to be equal to spacey dot blank we don't need a lot of fancy features here we're just going to have a blank model that's just going to hose host an single entity ruler that's going to be equal to nlp dot add underscore pipe and this is going to be entity ruler and now what we need to do is we need to come up with a way to go through all of these different symbols and add them in so we can say for symbol and symbols we want to say patterns dot append and we're going to make a an empty list of patterns up here and what we're going to append is that dictionary that you met when we talked about the entity ruler and I believe it was chapter five yeah and what this is going to have there are two things label which is going to correspond to stock in this case and it's going to have a pattern and that's going to correspond to the pattern of the symbol so we're going to say symbol and what that lets us do is kind of go through and easily create and add these patterns and and we can do the same thing for company remember it's never a good idea to copy and paste in your code I am simply doing it for demonstration purposes right now this is not polished code by any stretch of the imagination and what we can do here now is we can do the same thing loop over the different companies and add each company and so what this is doing is it's creating a large list of different patterns that the entity ruler will use to then go through and as we create the a doc object over that sample Reuters text I just showed you a second ago which we should probably just go ahead and pull up right now I'm going to copy and paste it straight from the textbook. Let's go ahead and execute that cell and we're going to add in this text here it is a little lengthy but it'll be all right and what we're going to do now is we're going to iterate over create a doc object to iterate over all of that. And our goal here is going to be able to say for int and doc dot ends we want to have extracted all of these different entities so we can say print off and dot text and dot label and let's see if we succeeded. And we have to add in our patterns to our entity ruler so remember we can do this by saying ruler dot add patterns. Patterns there we go. That's what this error actually means and now when we do it we see that we've been able to extract Apple as a company Apple as a company Nasdaq everything's looking pretty good but I notice really quickly that I wasn't actually able to extract Apple as a stock and I've also got another problem I've extracted to the lowercase TWO as a stock as well why have these two things are as a company. Well it turns out in our data set we've got to TWO that is a company name that's almost always going to be a false positive and we know that that kind of thing might be better off worked into a machine learning model for right now though we're going to work under the presumption that anytime we encounter this kind of obscure company TWO as a lowercase it's going to be a false positive. I also have another problem I know for a fact that Apple the stock is referenced within this text to make it a little easier. Let's see it right here and notice that it didn't find it to make this a little easier to display. Let's go ahead and display what we're looking at as the splacy render so what we can do is we can use that the splacy render that we met a little bit ago in this video. So in order to import this if you remember we need to say from spacey import display see and that's going to allow us to actually display our entities. Let's go ahead and put this however on a different cell just so we don't have to execute that every time and we're going to say at splacy render and we're going to render the doc object with a style that's equal to ENT and we can see that we've got our text now popping out with our things labeled and you can see pretty quickly where we've made some mistakes where we need to incorporate some things into our entity ruler. So for example if I'm scrolling through this is gray little ugly we can change the colors that's beyond the scope of this video though but let's keep on going down we notice that we have Apple dot IO and yet this has been missed by our entity ruler. Why has this been missed well. Spacey as a tokenizer is seeing this as a single token so Apple dot Oh the letter Oh capital letter Oh why is that well I didn't know about this but apparently it does has to deal with kind of the way in which stock indices are I think it's on the NASDAQ kind of structure things so what can we do well we've got a couple different options here I know that these go through all different letters from A to Z so we can either work with the string library or we can do is we can import a quick list that I've already written out of all the different letters of the alphabet and iterate through those with our ruler up here. Let's go ahead and add these letters right there and we can kind of iterate through those and whenever a stock kind of pops out with that kind of symbol plus any occurrence where it's got a period followed by a letter in those scenarios we want that to be flagged as a stock as well so what we can do is we can add in another thing right here add in another pattern and this is now going to be symbol plus we're going to add in F string right here a formatted string any occurrence of L we can set up a loop to say for L and letters do this and what this is going to allow us to do is to look for any instance where there is a symbol followed by a period followed by one of these capitalized letters that I just copied and pasted in so if we do that we can execute that cell and we can scroll down and we can now do the exact same thing that we just did a second ago and actually display this and now we're finding these stocks highlighted as stock so we're successfully getting these stocks and extracting them we've got a few different things that our client wants to also extract though they don't want to just extract companies and they don't want to just extract stock and they want to also extract stock exchanges and indexes but we have one other problem and go ahead and get rid of this as the display mode and switch back to just our set of entities because it's a little easier to read for this example we've got another problem and we see we have a couple other stocks popping out we now know that Kroger stock is here the n i o dot n stock is in this text as well now we're starting to see a greater degree of specificity for right now I'm going to include two as a set of a stop technical term would be like a stop or something that I don't want to be included into the model so I'm going to make a list of stops and we're just going to include two in that and we're going to say for company and companies do all this if company not in stops we want this to occur what this means now is that our our pipeline while going through and having all of these different things all these different rules it's also going to have another rule that looks to see if there's a stop or if this company name is this stop and if it is then we want it to just kind of skip over and ignore it and if we go through we notice that now we've successfully eliminated this what we would presume to be a consistent false positive something that's going to come up again and again as a false positive great so we've been able to get this where it works now pretty well what I also want to work into this model if you remember though are things like indexes fortunately I've also provided for us a list of all different indexes that are available from I believe it's like everything like the Dow Jones is about 13 or 14 of them let's go ahead and import those up above and let's do that right here in this cell so it kind of goes in sequential order that follows better with the textbook to so it's a new data frame object this is going to be equal to P a PD dot read CSV we're going to read in that data file that I've given us and that's going to be the indexes dot T SV with a separator that's equal to a tab let's see what that looks like and this is what it looks like so all these different indices now I know I'm going to have a problem right out of the gate and that's going to be that sometimes you're going to see things referenced as SNP 500 I don't know a lot about finances but I know that you don't always see it as SNP 500 index but I do think that these index symbols are also going to be useful so like I did before I'm going to convert these things into a list so it's a little easier for me to work with in a for loop and I'm going to say indexes is equal to DF2 dot index name so grabbing that column to list and index symbols is equal to DF2 dot index symbol dot to list and both of these are going to be different and they're both going to have the same exact entity label which is going to be an index and so let's go ahead and iterate over these and add them in as well so I'm going to go ahead and do that right now for indexes and indexes we want this label to be index we want this to be index here so it's going to allow us to kind of go through and grab all those and we want to do the same thing with index symbols keep these a little separated here index symbols and that allows for us to do that and let's go ahead and without making any adjustments let's see let's see how this does with these new patterns that we've added in and because we've already got this text loaded into memory I'm going to go ahead and put this right here doc is going to be equal to nlp text for int and doc and print off and dot text and dot label and we can kind of go through and we're actually now able to extract some indexes and I believe when I was looking at this text really quickly though I noticed that there was one instance at least where we had not only the index referenced but also a name like S&P 500 right here S&P 500 notice that it isn't found because it doesn't have the name index after it and notice also that none of our our symbols are being found because they all seem to be preceded by a dot so in this case a dot J a DJI and so that's something else that I have to work into this model and the list I gave the data set that's not there so I need to collect a list of these different names and work those into an entity ruler as well but for right now let's ignore that and focus on including this S&P 500 so how can I get the S&P 500 in there from the list I already gave it well what I can do is I can say okay so under these indices not only do I want to add that specific pattern let's go ahead and break these things up into different words and so I'm going to have the words is equal to index dot split and then I'm going to make a presumption that the the first two words so the S&P 500 the S&P 400 are sometimes going to be referenced by themselves so what I want to do is I want to work that into the model as well and I want to say we're going to say patterns dot append copy this as well we can say something like dot join words up until the second index and let's go ahead and work that into our model in our patterns or pipeline and print off our NLP again and you'll find that we've now been able to capture things like S&P 500 that aren't proceeded by the word index and we see that we in fact have S&P 500 is now popping out time and again that's fantastic I'm pretty happy with that now we're we're getting a deeper sense of what this text is about without actually having to read it we know that it's going to deal heavily with Apple and we know that it's also going to tangentially deal with some of these other things as well but I also want to include into this into this pipeline the ability for the entity ruler to not just find these things but I also wanted to be able to find different stock exchanges so I've got a list I cultivated for different stock exchanges which are things like NYSE things like that so I can say DS3 is going to be equal to PD dot read CSV data backslash stock exchanges dot TSV and then the separator is going to be again a tab and let's take a look at what this looks like. Stanges there we go. There we are and we have something that looks like this a pretty a pretty large CSV file CSV file sorry that's got a bunch of different rows the ones I'm most interested in well there's a couple actually I'm interested in specifically the Google Prefix and this description the description has the actual name and the Prefix has this really nice abbreviation that I've seen pop out a few different times such as Nasdaq here if we keep on going down we would see different things as well NYSE these are kind of different stock exchanges. So let's pop back down here and let's go ahead and convert those two things into individual lists as well so we're going to say exchanges it's going to be equal to DF3 dot ISO Mike dot to list and then I'm also going to grab the F3 dot sorry Google I have to do this as a dictionary because it's the way the data sets cultivated it's got a space in the middle this is a common problem that you run into and then I also want to know grab all of these exchanges as well so I'm going to say also on top of that DF3 dot description to list so I'm making a large list exchanges and I get this here because it says Google Prefix isn't an actual thing and in fact it's prefix with an I and now we actually are able to get all these things extracted so what I want to do now is I want to work all these different symbols and descriptions into into the model as well or into the pipeline as well so I can say for for E and exchanges I want to say patterns dot append and I want to do a label that's going to be let's do stock exchange and then the next thing I want to do is a pattern and that's going to be equal to in this case E as we're going to see this is not adequate enough we need to do a few different things to really kind of work this out but it's going to be a good enough to at least get started and it's going to take it just a second and the main thing that's happening right now are these different for loops so if we keep on going down we now see that we were able to extract the NYSE stock exchange so we've not only been able to work into a pipeline in a very short order maybe about 20 30 minutes we've been able to work into a pipeline all of these different things that are coming out we do however see a couple problems and this is where I'm going to leave it though because you've got the basic mechanics down now comes time for you being a domain expert to work out and come up with rules to solve some of these problems Nasdaq is not a company so there's a problem with the data set or Nasdaq is listed as a company name and one of the data sets we need to work that out where Nasdaq is never referenced as a company we have the S&P and is now being coming out correctly as S&P 500 there might be instances where just S&P is referenced which I think in that context would probably be the S&P 500 but nevertheless we've been able to actually extract these things sometimes the Dow Jones Industrial Average might just be referenced to Dow Jones so this index might just be these first two words I know that's a common occurrence we've also seen that we weren't able to extract some of those things that were a period followed by a symbol that referenced the actual index itself nevertheless this is a really good starting point and you can see how just in a few minutes you're able to generate this thing that can extract information from unstructured text at the end of the day like I said in the introduction to this entire video that's one of the essential tasks of NLP designing this and implementing it is pretty quick and easy perfecting it is where the time really is to get this financial analysis entity ruler working really well where it has almost no false positives and almost never misses a true a true positive it would take maybe a few more hours of just some kind of working and eventually there are certain things you might find that would work better in a machine learning model nevertheless you can see the degree to which rules based approaches in Spacey can really accomplish some pretty robust tasks with minimal minimal amount of code so long as you have access to or have already cultivated the data sets required. Thank you so much for watching this video series on Spacey an introduction to basic concepts of natural language processing linguistic annotations in Spacey vectors pipelines and kind of rules based Spacey you've enjoyed this video please like and subscribe down below and if you've also found this video useful consider joining me on my channel Python tutorials for digital humanities if you have like this and found this video useful I'm envisioning a second part to this video where I go with the machine learning aspects of Spacey if you're interested in that let me know in the comments down below and I'll make a second video that corresponds to this one. Thank you for watching and have a great day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.5600000000000005, "text": " In this course, you will learn all about natural language processing and how to apply it to", "tokens": [50364, 682, 341, 1164, 11, 291, 486, 1466, 439, 466, 3303, 2856, 9007, 293, 577, 281, 3079, 309, 281, 50592], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 1, "seek": 0, "start": 4.5600000000000005, "end": 7.92, "text": " real-world problems using the Spacey Library.", "tokens": [50592, 957, 12, 13217, 2740, 1228, 264, 8705, 88, 12806, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 2, "seek": 0, "start": 7.92, "end": 12.88, "text": " Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.", "tokens": [50760, 2491, 13, 6789, 783, 356, 307, 4664, 33800, 294, 341, 1859, 11, 293, 415, 311, 364, 7103, 5027, 13, 51008], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 3, "seek": 0, "start": 12.88, "end": 14.96, "text": " Hi, and welcome to this video.", "tokens": [51008, 2421, 11, 293, 2928, 281, 341, 960, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 4, "seek": 0, "start": 14.96, "end": 21.080000000000002, "text": " My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.", "tokens": [51112, 1222, 1315, 307, 2491, 13, 6740, 6789, 783, 356, 11, 293, 286, 37938, 294, 2120, 38219, 3303, 2856, 9007, 13, 51418], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 5, "seek": 0, "start": 21.080000000000002, "end": 24.52, "text": " I come to NLP from a humanities perspective.", "tokens": [51418, 286, 808, 281, 426, 45196, 490, 257, 36140, 4585, 13, 51590], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 6, "seek": 0, "start": 24.52, "end": 27.52, "text": " I have my PhD in medieval history.", "tokens": [51590, 286, 362, 452, 14476, 294, 24078, 2503, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2031397338307232, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.006189195904880762}, {"id": 7, "seek": 2752, "start": 27.52, "end": 32.04, "text": " But I use Spacey on a regular basis to do all of my NLP needs.", "tokens": [50364, 583, 286, 764, 8705, 88, 322, 257, 3890, 5143, 281, 360, 439, 295, 452, 426, 45196, 2203, 13, 50590], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 8, "seek": 2752, "start": 32.04, "end": 36.12, "text": " So what you're going to get out of this video over the next few hours is a basic understanding", "tokens": [50590, 407, 437, 291, 434, 516, 281, 483, 484, 295, 341, 960, 670, 264, 958, 1326, 2496, 307, 257, 3875, 3701, 50794], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 9, "seek": 2752, "start": 36.12, "end": 43.6, "text": " of what natural language processing is, or NLP, and also how to apply it to domain-specific", "tokens": [50794, 295, 437, 3303, 2856, 9007, 307, 11, 420, 426, 45196, 11, 293, 611, 577, 281, 3079, 309, 281, 9274, 12, 29258, 51168], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 10, "seek": 2752, "start": 43.6, "end": 48.56, "text": " problems, or problems that exist within your own area of expertise.", "tokens": [51168, 2740, 11, 420, 2740, 300, 2514, 1951, 428, 1065, 1859, 295, 11769, 13, 51416], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 11, "seek": 2752, "start": 48.56, "end": 54.28, "text": " I happen to use this all the time to analyze historical documents or financial documents", "tokens": [51416, 286, 1051, 281, 764, 341, 439, 264, 565, 281, 12477, 8584, 8512, 420, 4669, 8512, 51702], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 12, "seek": 2752, "start": 54.28, "end": 56.879999999999995, "text": " for my own personal investments.", "tokens": [51702, 337, 452, 1065, 2973, 13784, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11061093517553026, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.002471758285537362}, {"id": 13, "seek": 5688, "start": 56.88, "end": 61.92, "text": " Over the next few hours, you're going to learn a lot about NLP, language as a whole,", "tokens": [50364, 4886, 264, 958, 1326, 2496, 11, 291, 434, 516, 281, 1466, 257, 688, 466, 426, 45196, 11, 2856, 382, 257, 1379, 11, 50616], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 14, "seek": 5688, "start": 61.92, "end": 65.12, "text": " and most importantly, the Spacey Library.", "tokens": [50616, 293, 881, 8906, 11, 264, 8705, 88, 12806, 13, 50776], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 15, "seek": 5688, "start": 65.12, "end": 70.32000000000001, "text": " I like the Spacey Library because it's easy to use and easy to also implement really kind", "tokens": [50776, 286, 411, 264, 8705, 88, 12806, 570, 309, 311, 1858, 281, 764, 293, 1858, 281, 611, 4445, 534, 733, 51036], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 16, "seek": 5688, "start": 70.32000000000001, "end": 74.84, "text": " of general solutions to general problems with the off-the-shelf models that are already", "tokens": [51036, 295, 2674, 6547, 281, 2674, 2740, 365, 264, 766, 12, 3322, 12, 46626, 5245, 300, 366, 1217, 51262], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 17, "seek": 5688, "start": 74.84, "end": 76.16, "text": " available to you.", "tokens": [51262, 2435, 281, 291, 13, 51328], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 18, "seek": 5688, "start": 76.16, "end": 79.96000000000001, "text": " I'm going to walk you through, in part one of this video series, how to get the most", "tokens": [51328, 286, 478, 516, 281, 1792, 291, 807, 11, 294, 644, 472, 295, 341, 960, 2638, 11, 577, 281, 483, 264, 881, 51518], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 19, "seek": 5688, "start": 79.96000000000001, "end": 83.04, "text": " out of Spacey with these off-the-shelf features.", "tokens": [51518, 484, 295, 8705, 88, 365, 613, 766, 12, 3322, 12, 46626, 4122, 13, 51672], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 20, "seek": 5688, "start": 83.04, "end": 86.48, "text": " In part two, we're going to start tackling some of the features that don't exist in", "tokens": [51672, 682, 644, 732, 11, 321, 434, 516, 281, 722, 34415, 512, 295, 264, 4122, 300, 500, 380, 2514, 294, 51844], "temperature": 0.0, "avg_logprob": -0.10170314681362098, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.0013248194009065628}, {"id": 21, "seek": 8648, "start": 86.48, "end": 91.76, "text": " off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components", "tokens": [50364, 766, 12, 3322, 12, 46626, 5245, 11, 293, 286, 478, 516, 281, 855, 291, 577, 281, 764, 4474, 12, 6032, 21882, 11, 420, 6677, 50628], "temperature": 0.0, "avg_logprob": -0.13460910529421086, "compression_ratio": 1.736220472440945, "no_speech_prob": 0.014501476660370827}, {"id": 22, "seek": 8648, "start": 91.76, "end": 97.96000000000001, "text": " in Spacey, to actually solve domain-specific problems in your own area, from the entity", "tokens": [50628, 294, 8705, 88, 11, 281, 767, 5039, 9274, 12, 29258, 2740, 294, 428, 1065, 1859, 11, 490, 264, 13977, 50938], "temperature": 0.0, "avg_logprob": -0.13460910529421086, "compression_ratio": 1.736220472440945, "no_speech_prob": 0.014501476660370827}, {"id": 23, "seek": 8648, "start": 97.96000000000001, "end": 104.36, "text": " ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects", "tokens": [50938, 19661, 281, 264, 2995, 260, 11, 281, 767, 10711, 278, 13956, 11, 3997, 11, 3890, 6114, 11, 420, 8248, 82, 51258], "temperature": 0.0, "avg_logprob": -0.13460910529421086, "compression_ratio": 1.736220472440945, "no_speech_prob": 0.014501476660370827}, {"id": 24, "seek": 8648, "start": 104.36, "end": 110.44, "text": " patterns in a custom Spacey component that doesn't actually exist at the moment.", "tokens": [51258, 8294, 294, 257, 2375, 8705, 88, 6542, 300, 1177, 380, 767, 2514, 412, 264, 1623, 13, 51562], "temperature": 0.0, "avg_logprob": -0.13460910529421086, "compression_ratio": 1.736220472440945, "no_speech_prob": 0.014501476660370827}, {"id": 25, "seek": 8648, "start": 110.44, "end": 114.56, "text": " I'm going to be showing you all that in part two, so that in part three, we can take the", "tokens": [51562, 286, 478, 516, 281, 312, 4099, 291, 439, 300, 294, 644, 732, 11, 370, 300, 294, 644, 1045, 11, 321, 393, 747, 264, 51768], "temperature": 0.0, "avg_logprob": -0.13460910529421086, "compression_ratio": 1.736220472440945, "no_speech_prob": 0.014501476660370827}, {"id": 26, "seek": 11456, "start": 114.56, "end": 119.64, "text": " lessons that we learned in part one and part two, and actually apply them to solve a very", "tokens": [50364, 8820, 300, 321, 3264, 294, 644, 472, 293, 644, 732, 11, 293, 767, 3079, 552, 281, 5039, 257, 588, 50618], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 27, "seek": 11456, "start": 119.64, "end": 126.56, "text": " kind of common problem that exists in an LP, and that is information extraction from financial", "tokens": [50618, 733, 295, 2689, 1154, 300, 8198, 294, 364, 38095, 11, 293, 300, 307, 1589, 30197, 490, 4669, 50964], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 28, "seek": 11456, "start": 126.56, "end": 127.56, "text": " documents.", "tokens": [50964, 8512, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 29, "seek": 11456, "start": 127.56, "end": 134.16, "text": " So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.", "tokens": [51014, 407, 5006, 721, 300, 366, 295, 32684, 11, 1270, 382, 12966, 11, 8383, 11, 8186, 279, 11, 293, 4127, 27374, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 30, "seek": 11456, "start": 134.16, "end": 139.2, "text": " If you join me over the next few hours, you will leave this lesson with a good understanding", "tokens": [51344, 759, 291, 3917, 385, 670, 264, 958, 1326, 2496, 11, 291, 486, 1856, 341, 6898, 365, 257, 665, 3701, 51596], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 31, "seek": 11456, "start": 139.2, "end": 143.68, "text": " of Spacey, and also a good understanding of kind of the off-the-shelf components that", "tokens": [51596, 295, 8705, 88, 11, 293, 611, 257, 665, 3701, 295, 733, 295, 264, 766, 12, 3322, 12, 46626, 6677, 300, 51820], "temperature": 0.0, "avg_logprob": -0.1187789695603507, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.061862219125032425}, {"id": 32, "seek": 14368, "start": 143.68, "end": 150.92000000000002, "text": " are there, and a way to take the off-the-shelf components and apply them to your own domain.", "tokens": [50364, 366, 456, 11, 293, 257, 636, 281, 747, 264, 766, 12, 3322, 12, 46626, 6677, 293, 3079, 552, 281, 428, 1065, 9274, 13, 50726], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 33, "seek": 14368, "start": 150.92000000000002, "end": 154.36, "text": " If you also join me in this video and you like it, please let me know in the comments", "tokens": [50726, 759, 291, 611, 3917, 385, 294, 341, 960, 293, 291, 411, 309, 11, 1767, 718, 385, 458, 294, 264, 3053, 50898], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 34, "seek": 14368, "start": 154.36, "end": 158.96, "text": " down below, because I am interested in making a second part to this video that will explore", "tokens": [50898, 760, 2507, 11, 570, 286, 669, 3102, 294, 1455, 257, 1150, 644, 281, 341, 960, 300, 486, 6839, 51128], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 35, "seek": 14368, "start": 158.96, "end": 164.20000000000002, "text": " not only the rules-based aspects of Spacey, but the machine learning-based aspects of", "tokens": [51128, 406, 787, 264, 4474, 12, 6032, 7270, 295, 8705, 88, 11, 457, 264, 3479, 2539, 12, 6032, 7270, 295, 51390], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 36, "seek": 14368, "start": 164.20000000000002, "end": 165.20000000000002, "text": " Spacey.", "tokens": [51390, 8705, 88, 13, 51440], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 37, "seek": 14368, "start": 165.20000000000002, "end": 169.32, "text": " So teaching you how to train your own models to do your own things, such as training a", "tokens": [51440, 407, 4571, 291, 577, 281, 3847, 428, 1065, 5245, 281, 360, 428, 1065, 721, 11, 1270, 382, 3097, 257, 51646], "temperature": 0.0, "avg_logprob": -0.09790379425574994, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.14027372002601624}, {"id": 38, "seek": 16932, "start": 169.32, "end": 174.32, "text": " dependency parser, training a named entity recognizer, things like this, which are not", "tokens": [50364, 33621, 21156, 260, 11, 3097, 257, 4926, 13977, 3068, 6545, 11, 721, 411, 341, 11, 597, 366, 406, 50614], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 39, "seek": 16932, "start": 174.32, "end": 176.4, "text": " covered in this video.", "tokens": [50614, 5343, 294, 341, 960, 13, 50718], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 40, "seek": 16932, "start": 176.4, "end": 180.4, "text": " Nevertheless, if you join me for this one and you like it, you will find part two much", "tokens": [50718, 26554, 11, 498, 291, 3917, 385, 337, 341, 472, 293, 291, 411, 309, 11, 291, 486, 915, 644, 732, 709, 50918], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 41, "seek": 16932, "start": 180.4, "end": 182.48, "text": " easier to understand.", "tokens": [50918, 3571, 281, 1223, 13, 51022], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 42, "seek": 16932, "start": 182.48, "end": 189.4, "text": " So sit back, relax, and let's jump into what NLP is, what kind of things you can do with", "tokens": [51022, 407, 1394, 646, 11, 5789, 11, 293, 718, 311, 3012, 666, 437, 426, 45196, 307, 11, 437, 733, 295, 721, 291, 393, 360, 365, 51368], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 43, "seek": 16932, "start": 189.4, "end": 194.24, "text": " NLP, such as information extraction, and what the Spacey library is, and how this course", "tokens": [51368, 426, 45196, 11, 1270, 382, 1589, 30197, 11, 293, 437, 264, 8705, 88, 6405, 307, 11, 293, 577, 341, 1164, 51610], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 44, "seek": 16932, "start": 194.24, "end": 195.68, "text": " will be laid out.", "tokens": [51610, 486, 312, 9897, 484, 13, 51682], "temperature": 0.0, "avg_logprob": -0.14008044345038279, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.7820999026298523}, {"id": 45, "seek": 19568, "start": 195.68, "end": 200.8, "text": " If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital", "tokens": [50364, 759, 291, 4501, 341, 960, 11, 611, 1949, 19981, 281, 452, 2269, 11, 15329, 18392, 5181, 82, 337, 15522, 50620], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 46, "seek": 19568, "start": 200.8, "end": 204.38, "text": " Humanities, which is linked in the description down below.", "tokens": [50620, 10294, 1088, 11, 597, 307, 9408, 294, 264, 3855, 760, 2507, 13, 50799], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 47, "seek": 19568, "start": 204.38, "end": 209.44, "text": " Even if you're not a digital humanist like me, you will find these Python tutorials useful", "tokens": [50799, 2754, 498, 291, 434, 406, 257, 4562, 1952, 468, 411, 385, 11, 291, 486, 915, 613, 15329, 17616, 4420, 51052], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 48, "seek": 19568, "start": 209.44, "end": 216.12, "text": " because they take Python and make it accessible to students of all levels, specifically those", "tokens": [51052, 570, 436, 747, 15329, 293, 652, 309, 9515, 281, 1731, 295, 439, 4358, 11, 4682, 729, 51386], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 49, "seek": 19568, "start": 216.12, "end": 217.32, "text": " who are beginners.", "tokens": [51386, 567, 366, 26992, 13, 51446], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 50, "seek": 19568, "start": 217.32, "end": 221.96, "text": " I walk you through not only the basics of Python, but also I walk you through step-by-step", "tokens": [51446, 286, 1792, 291, 807, 406, 787, 264, 14688, 295, 15329, 11, 457, 611, 286, 1792, 291, 807, 1823, 12, 2322, 12, 16792, 51678], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 51, "seek": 19568, "start": 221.96, "end": 225.24, "text": " some of the more common libraries that you need.", "tokens": [51678, 512, 295, 264, 544, 2689, 15148, 300, 291, 643, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12146537586794061, "compression_ratio": 1.667785234899329, "no_speech_prob": 0.8000959753990173}, {"id": 52, "seek": 22524, "start": 225.24, "end": 230.60000000000002, "text": " A lot of the channel deals with texts or text-based problems, but other content deals with things", "tokens": [50364, 316, 688, 295, 264, 2269, 11215, 365, 15765, 420, 2487, 12, 6032, 2740, 11, 457, 661, 2701, 11215, 365, 721, 50632], "temperature": 0.0, "avg_logprob": -0.10912619466366975, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.031129926443099976}, {"id": 53, "seek": 22524, "start": 230.60000000000002, "end": 236.92000000000002, "text": " like machine learning and image classification and OCR, all in Python.", "tokens": [50632, 411, 3479, 2539, 293, 3256, 21538, 293, 422, 18547, 11, 439, 294, 15329, 13, 50948], "temperature": 0.0, "avg_logprob": -0.10912619466366975, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.031129926443099976}, {"id": 54, "seek": 22524, "start": 236.92000000000002, "end": 240.84, "text": " So before we begin with Spacey, I think we should spend a little bit of time talking", "tokens": [50948, 407, 949, 321, 1841, 365, 8705, 88, 11, 286, 519, 321, 820, 3496, 257, 707, 857, 295, 565, 1417, 51144], "temperature": 0.0, "avg_logprob": -0.10912619466366975, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.031129926443099976}, {"id": 55, "seek": 22524, "start": 240.84, "end": 246.44, "text": " about what NLP or natural language processing actually is.", "tokens": [51144, 466, 437, 426, 45196, 420, 3303, 2856, 9007, 767, 307, 13, 51424], "temperature": 0.0, "avg_logprob": -0.10912619466366975, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.031129926443099976}, {"id": 56, "seek": 22524, "start": 246.44, "end": 251.60000000000002, "text": " Natural language processing is the process by which we try to get a computer system to", "tokens": [51424, 20137, 2856, 9007, 307, 264, 1399, 538, 597, 321, 853, 281, 483, 257, 3820, 1185, 281, 51682], "temperature": 0.0, "avg_logprob": -0.10912619466366975, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.031129926443099976}, {"id": 57, "seek": 25160, "start": 251.6, "end": 258.48, "text": " understand and parse and extract human language, often times with raw text.", "tokens": [50364, 1223, 293, 48377, 293, 8947, 1952, 2856, 11, 2049, 1413, 365, 8936, 2487, 13, 50708], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 58, "seek": 25160, "start": 258.48, "end": 261.76, "text": " There are a couple different areas of natural language processing.", "tokens": [50708, 821, 366, 257, 1916, 819, 3179, 295, 3303, 2856, 9007, 13, 50872], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 59, "seek": 25160, "start": 261.76, "end": 268.08, "text": " There's named entity recognition, part of speech tagging, syntactic parsing, text categorization,", "tokens": [50872, 821, 311, 4926, 13977, 11150, 11, 644, 295, 6218, 6162, 3249, 11, 23980, 19892, 21156, 278, 11, 2487, 19250, 2144, 11, 51188], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 60, "seek": 25160, "start": 268.08, "end": 273.12, "text": " also known as text classification, co-reference resolution, machine translation.", "tokens": [51188, 611, 2570, 382, 2487, 21538, 11, 598, 12, 265, 5158, 8669, 11, 3479, 12853, 13, 51440], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 61, "seek": 25160, "start": 273.12, "end": 278.71999999999997, "text": " Adjacent to NLP is another kind of computational linguistics field called natural language", "tokens": [51440, 1999, 19586, 317, 281, 426, 45196, 307, 1071, 733, 295, 28270, 21766, 6006, 2519, 1219, 3303, 2856, 51720], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 62, "seek": 25160, "start": 278.71999999999997, "end": 280.98, "text": " understanding, or NLU.", "tokens": [51720, 3701, 11, 420, 426, 43, 52, 13, 51833], "temperature": 0.0, "avg_logprob": -0.13740526064477784, "compression_ratio": 1.6603053435114503, "no_speech_prob": 0.30387842655181885}, {"id": 63, "seek": 28098, "start": 280.98, "end": 285.34000000000003, "text": " This is where we train computer systems to do things like relation extraction, semantic", "tokens": [50364, 639, 307, 689, 321, 3847, 3820, 3652, 281, 360, 721, 411, 9721, 30197, 11, 47982, 50582], "temperature": 0.0, "avg_logprob": -0.1767444815686954, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0002868424926418811}, {"id": 64, "seek": 28098, "start": 285.34000000000003, "end": 293.54, "text": " parsing, question and answering, summarization, sentiment analysis, and paraphrasing.", "tokens": [50582, 21156, 278, 11, 1168, 293, 13430, 11, 14611, 2144, 11, 16149, 5215, 11, 293, 36992, 1703, 3349, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1767444815686954, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0002868424926418811}, {"id": 65, "seek": 28098, "start": 293.54, "end": 299.40000000000003, "text": " NLP and NLU are used by a wide array of industries, from finance industry all the way through", "tokens": [50992, 426, 45196, 293, 426, 43, 52, 366, 1143, 538, 257, 4874, 10225, 295, 13284, 11, 490, 10719, 3518, 439, 264, 636, 807, 51285], "temperature": 0.0, "avg_logprob": -0.1767444815686954, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0002868424926418811}, {"id": 66, "seek": 28098, "start": 299.40000000000003, "end": 306.74, "text": " to law and academia, with researchers trying to do information extraction from texts.", "tokens": [51285, 281, 2101, 293, 28937, 11, 365, 10309, 1382, 281, 360, 1589, 30197, 490, 15765, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1767444815686954, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0002868424926418811}, {"id": 67, "seek": 28098, "start": 306.74, "end": 309.66, "text": " Within NLP, there's a couple different applications.", "tokens": [51652, 15996, 426, 45196, 11, 456, 311, 257, 1916, 819, 5821, 13, 51798], "temperature": 0.0, "avg_logprob": -0.1767444815686954, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0002868424926418811}, {"id": 68, "seek": 30966, "start": 309.66, "end": 314.62, "text": " The first and probably the most important is information extraction.", "tokens": [50364, 440, 700, 293, 1391, 264, 881, 1021, 307, 1589, 30197, 13, 50612], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 69, "seek": 30966, "start": 314.62, "end": 319.88000000000005, "text": " This is the process by which we try to get a computer system to extract information that", "tokens": [50612, 639, 307, 264, 1399, 538, 597, 321, 853, 281, 483, 257, 3820, 1185, 281, 8947, 1589, 300, 50875], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 70, "seek": 30966, "start": 319.88000000000005, "end": 323.88000000000005, "text": " we find relevant to our own research or needs.", "tokens": [50875, 321, 915, 7340, 281, 527, 1065, 2132, 420, 2203, 13, 51075], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 71, "seek": 30966, "start": 323.88000000000005, "end": 328.42, "text": " So for example, as we're going to see in part three of this video, when we apply spacey", "tokens": [51075, 407, 337, 1365, 11, 382, 321, 434, 516, 281, 536, 294, 644, 1045, 295, 341, 960, 11, 562, 321, 3079, 1901, 88, 51302], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 72, "seek": 30966, "start": 328.42, "end": 335.28000000000003, "text": " to the financial sector, a person interested in finances might need NLP to go through and", "tokens": [51302, 281, 264, 4669, 6977, 11, 257, 954, 3102, 294, 25123, 1062, 643, 426, 45196, 281, 352, 807, 293, 51645], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 73, "seek": 30966, "start": 335.28000000000003, "end": 339.18, "text": " extract things like company names, stocks, indexes.", "tokens": [51645, 8947, 721, 411, 2237, 5288, 11, 12966, 11, 8186, 279, 13, 51840], "temperature": 0.0, "avg_logprob": -0.12997951322388882, "compression_ratio": 1.6254681647940075, "no_speech_prob": 0.060070931911468506}, {"id": 74, "seek": 33918, "start": 339.18, "end": 343.34000000000003, "text": " Things that are referenced within maybe news articles, from Reuters to New York Times to", "tokens": [50364, 9514, 300, 366, 32734, 1951, 1310, 2583, 11290, 11, 490, 1300, 48396, 281, 1873, 3609, 11366, 281, 50572], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 75, "seek": 33918, "start": 343.34000000000003, "end": 345.24, "text": " Wall Street Journal.", "tokens": [50572, 9551, 7638, 16936, 13, 50667], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 76, "seek": 33918, "start": 345.24, "end": 349.34000000000003, "text": " This is an example of using NLP to extract information.", "tokens": [50667, 639, 307, 364, 1365, 295, 1228, 426, 45196, 281, 8947, 1589, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 77, "seek": 33918, "start": 349.34000000000003, "end": 355.06, "text": " A good way to think about NLP's application in this area is it takes in some unstructured", "tokens": [50872, 316, 665, 636, 281, 519, 466, 426, 45196, 311, 3861, 294, 341, 1859, 307, 309, 2516, 294, 512, 18799, 46847, 51158], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 78, "seek": 33918, "start": 355.06, "end": 361.94, "text": " data, in this case raw text, and extracts structured data from it, or metadata.", "tokens": [51158, 1412, 11, 294, 341, 1389, 8936, 2487, 11, 293, 8947, 82, 18519, 1412, 490, 309, 11, 420, 26603, 13, 51502], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 79, "seek": 33918, "start": 361.94, "end": 365.78000000000003, "text": " So it finds the things that you want it to find and extracts them for you.", "tokens": [51502, 407, 309, 10704, 264, 721, 300, 291, 528, 309, 281, 915, 293, 8947, 82, 552, 337, 291, 13, 51694], "temperature": 0.0, "avg_logprob": -0.12887218365302452, "compression_ratio": 1.64, "no_speech_prob": 0.05831332877278328}, {"id": 80, "seek": 36578, "start": 365.78, "end": 371.26, "text": " Now while there's ways to do this with gazetteers and list matching, using an NLP framework", "tokens": [50364, 823, 1339, 456, 311, 2098, 281, 360, 341, 365, 26232, 3007, 433, 293, 1329, 14324, 11, 1228, 364, 426, 45196, 8388, 50638], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 81, "seek": 36578, "start": 371.26, "end": 375.61999999999995, "text": " like spacey, which I'll talk about in just a second, has certain advantages.", "tokens": [50638, 411, 1901, 88, 11, 597, 286, 603, 751, 466, 294, 445, 257, 1150, 11, 575, 1629, 14906, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 82, "seek": 36578, "start": 375.61999999999995, "end": 380.38, "text": " The main one being that you can use and leverage things that have been parsed syntactically", "tokens": [50856, 440, 2135, 472, 885, 300, 291, 393, 764, 293, 13982, 721, 300, 362, 668, 21156, 292, 23980, 578, 984, 51094], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 83, "seek": 36578, "start": 380.38, "end": 381.65999999999997, "text": " or semantically.", "tokens": [51094, 420, 4361, 49505, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 84, "seek": 36578, "start": 381.65999999999997, "end": 385.46, "text": " So things like the part of speech of a word, things like its dependencies, things like", "tokens": [51158, 407, 721, 411, 264, 644, 295, 6218, 295, 257, 1349, 11, 721, 411, 1080, 36606, 11, 721, 411, 51348], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 85, "seek": 36578, "start": 385.46, "end": 386.94, "text": " its co-reference.", "tokens": [51348, 1080, 598, 12, 265, 5158, 13, 51422], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 86, "seek": 36578, "start": 386.94, "end": 391.17999999999995, "text": " These are things that the spacey framework allow for you to do off the shelf and also", "tokens": [51422, 1981, 366, 721, 300, 264, 1901, 88, 8388, 2089, 337, 291, 281, 360, 766, 264, 15222, 293, 611, 51634], "temperature": 0.0, "avg_logprob": -0.10472157796223959, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0518377423286438}, {"id": 87, "seek": 39118, "start": 391.18, "end": 396.7, "text": " train into machine learning models and work into pipelines with rules.", "tokens": [50364, 3847, 666, 3479, 2539, 5245, 293, 589, 666, 40168, 365, 4474, 13, 50640], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 88, "seek": 39118, "start": 396.7, "end": 400.72, "text": " So that's kind of one aspect of NLP and one way it's used.", "tokens": [50640, 407, 300, 311, 733, 295, 472, 4171, 295, 426, 45196, 293, 472, 636, 309, 311, 1143, 13, 50841], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 89, "seek": 39118, "start": 400.72, "end": 404.38, "text": " Another way it's used is to read in data and classify it.", "tokens": [50841, 3996, 636, 309, 311, 1143, 307, 281, 1401, 294, 1412, 293, 33872, 309, 13, 51024], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 90, "seek": 39118, "start": 404.38, "end": 409.78000000000003, "text": " This is known as text categorization and we see that on the left hand side of this image.", "tokens": [51024, 639, 307, 2570, 382, 2487, 19250, 2144, 293, 321, 536, 300, 322, 264, 1411, 1011, 1252, 295, 341, 3256, 13, 51294], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 91, "seek": 39118, "start": 409.78000000000003, "end": 413.74, "text": " Text categorization or text classification, and we conclude in this sentiment analysis", "tokens": [51294, 18643, 19250, 2144, 420, 2487, 21538, 11, 293, 321, 16886, 294, 341, 16149, 5215, 51492], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 92, "seek": 39118, "start": 413.74, "end": 419.74, "text": " for the most part as well, is a way we take information into a computer system, again unstructured", "tokens": [51492, 337, 264, 881, 644, 382, 731, 11, 307, 257, 636, 321, 747, 1589, 666, 257, 3820, 1185, 11, 797, 18799, 46847, 51792], "temperature": 0.0, "avg_logprob": -0.11395710706710815, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.06559605151414871}, {"id": 93, "seek": 41974, "start": 419.78000000000003, "end": 424.3, "text": " data, a raw text, and we classify it in some way.", "tokens": [50366, 1412, 11, 257, 8936, 2487, 11, 293, 321, 33872, 309, 294, 512, 636, 13, 50592], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 94, "seek": 41974, "start": 424.3, "end": 430.14, "text": " You've actually seen this at work for many decades now with spam detection.", "tokens": [50592, 509, 600, 767, 1612, 341, 412, 589, 337, 867, 7878, 586, 365, 24028, 17784, 13, 50884], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 95, "seek": 41974, "start": 430.14, "end": 432.18, "text": " Spam detection is nearly perfect.", "tokens": [50884, 1738, 335, 17784, 307, 6217, 2176, 13, 50986], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 96, "seek": 41974, "start": 432.18, "end": 436.38, "text": " It needs to be continually updated, but for the most part it is a solved problem.", "tokens": [50986, 467, 2203, 281, 312, 22277, 10588, 11, 457, 337, 264, 881, 644, 309, 307, 257, 13041, 1154, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 97, "seek": 41974, "start": 436.38, "end": 440.38, "text": " The reason why you have emails that automatically go to your spam folder is because there's", "tokens": [51196, 440, 1778, 983, 291, 362, 12524, 300, 6772, 352, 281, 428, 24028, 10820, 307, 570, 456, 311, 51396], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 98, "seek": 41974, "start": 440.38, "end": 444.38, "text": " a machine learning model that sits on the background of your, on the back end of your", "tokens": [51396, 257, 3479, 2539, 2316, 300, 12696, 322, 264, 3678, 295, 428, 11, 322, 264, 646, 917, 295, 428, 51596], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 99, "seek": 41974, "start": 444.38, "end": 445.7, "text": " email server.", "tokens": [51596, 3796, 7154, 13, 51662], "temperature": 0.0, "avg_logprob": -0.13336181640625, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.004609288647770882}, {"id": 100, "seek": 44570, "start": 445.7, "end": 449.78, "text": " And what it does is it actually looks at the emails, it sees if it fits the pattern for", "tokens": [50364, 400, 437, 309, 775, 307, 309, 767, 1542, 412, 264, 12524, 11, 309, 8194, 498, 309, 9001, 264, 5102, 337, 50568], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 101, "seek": 44570, "start": 449.78, "end": 454.42, "text": " what it's seen as spam before, and it assigns it a spam label.", "tokens": [50568, 437, 309, 311, 1612, 382, 24028, 949, 11, 293, 309, 6269, 82, 309, 257, 24028, 7645, 13, 50800], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 102, "seek": 44570, "start": 454.42, "end": 457.14, "text": " This is known as classification.", "tokens": [50800, 639, 307, 2570, 382, 21538, 13, 50936], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 103, "seek": 44570, "start": 457.14, "end": 461.5, "text": " This is also used by researchers, especially in the legal industry.", "tokens": [50936, 639, 307, 611, 1143, 538, 10309, 11, 2318, 294, 264, 5089, 3518, 13, 51154], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 104, "seek": 44570, "start": 461.5, "end": 466.58, "text": " Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.", "tokens": [51154, 7744, 13917, 18349, 4774, 6779, 295, 5383, 295, 8512, 11, 498, 406, 6803, 295, 8512, 13, 51408], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 105, "seek": 44570, "start": 466.58, "end": 471.78, "text": " They don't necessarily have the human time to go through and analyze every single document", "tokens": [51408, 814, 500, 380, 4725, 362, 264, 1952, 565, 281, 352, 807, 293, 12477, 633, 2167, 4166, 51668], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 106, "seek": 44570, "start": 471.78, "end": 472.9, "text": " verbatim.", "tokens": [51668, 9595, 267, 332, 13, 51724], "temperature": 0.0, "avg_logprob": -0.12206873187312374, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.10666292905807495}, {"id": 107, "seek": 47290, "start": 472.9, "end": 477.82, "text": " It is important to kind of get a quick umbrella sense of the documents without actually having", "tokens": [50364, 467, 307, 1021, 281, 733, 295, 483, 257, 1702, 21925, 2020, 295, 264, 8512, 1553, 767, 1419, 50610], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 108, "seek": 47290, "start": 477.82, "end": 480.7, "text": " to go through and read them page by page.", "tokens": [50610, 281, 352, 807, 293, 1401, 552, 3028, 538, 3028, 13, 50754], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 109, "seek": 47290, "start": 480.7, "end": 486.09999999999997, "text": " And so what lawyers will oftentimes do is use NLP to do classification and information", "tokens": [50754, 400, 370, 437, 16219, 486, 18349, 360, 307, 764, 426, 45196, 281, 360, 21538, 293, 1589, 51024], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 110, "seek": 47290, "start": 486.09999999999997, "end": 487.28, "text": " extraction.", "tokens": [51024, 30197, 13, 51083], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 111, "seek": 47290, "start": 487.28, "end": 492.02, "text": " They will find keywords that are relevant to their case, or they will find documents", "tokens": [51083, 814, 486, 915, 21009, 300, 366, 7340, 281, 641, 1389, 11, 420, 436, 486, 915, 8512, 51320], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 112, "seek": 47290, "start": 492.02, "end": 495.97999999999996, "text": " that are classified according to the relevant fields of their case.", "tokens": [51320, 300, 366, 20627, 4650, 281, 264, 7340, 7909, 295, 641, 1389, 13, 51518], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 113, "seek": 47290, "start": 495.97999999999996, "end": 500.85999999999996, "text": " And that way they can take a million documents and reduce it down to maybe only a handful,", "tokens": [51518, 400, 300, 636, 436, 393, 747, 257, 2459, 8512, 293, 5407, 309, 760, 281, 1310, 787, 257, 16458, 11, 51762], "temperature": 0.0, "avg_logprob": -0.10774971820690014, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.019122282043099403}, {"id": 114, "seek": 50086, "start": 500.86, "end": 504.18, "text": " maybe a thousand that they have to read verbatim.", "tokens": [50364, 1310, 257, 4714, 300, 436, 362, 281, 1401, 9595, 267, 332, 13, 50530], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 115, "seek": 50086, "start": 504.18, "end": 509.42, "text": " This is a real world application of NLP or natural language processing, and both of these", "tokens": [50530, 639, 307, 257, 957, 1002, 3861, 295, 426, 45196, 420, 3303, 2856, 9007, 11, 293, 1293, 295, 613, 50792], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 116, "seek": 50086, "start": 509.42, "end": 513.54, "text": " tasks can be achieved through the SPACI framework.", "tokens": [50792, 9608, 393, 312, 11042, 807, 264, 8420, 4378, 40, 8388, 13, 50998], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 117, "seek": 50086, "start": 513.54, "end": 516.46, "text": " SPACI is a framework for doing NLP.", "tokens": [50998, 8420, 4378, 40, 307, 257, 8388, 337, 884, 426, 45196, 13, 51144], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 118, "seek": 50086, "start": 516.46, "end": 521.5, "text": " Right now, as of 2021, it's only available, I believe, in Python.", "tokens": [51144, 1779, 586, 11, 382, 295, 7201, 11, 309, 311, 787, 2435, 11, 286, 1697, 11, 294, 15329, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 119, "seek": 50086, "start": 521.5, "end": 525.1800000000001, "text": " I think there is a community that's working on an application with R, but I don't know", "tokens": [51396, 286, 519, 456, 307, 257, 1768, 300, 311, 1364, 322, 364, 3861, 365, 497, 11, 457, 286, 500, 380, 458, 51580], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 120, "seek": 50086, "start": 525.1800000000001, "end": 526.62, "text": " that for certain.", "tokens": [51580, 300, 337, 1629, 13, 51652], "temperature": 0.0, "avg_logprob": -0.14896550612016157, "compression_ratio": 1.5269230769230768, "no_speech_prob": 0.47625425457954407}, {"id": 121, "seek": 52662, "start": 526.62, "end": 531.74, "text": " But SPACI is one of many NLP frameworks that Python has available.", "tokens": [50364, 583, 8420, 4378, 40, 307, 472, 295, 867, 426, 45196, 29834, 300, 15329, 575, 2435, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 122, "seek": 52662, "start": 531.74, "end": 535.34, "text": " If you're interested in looking at all of them, you can explore things like NLDK, the", "tokens": [50620, 759, 291, 434, 3102, 294, 1237, 412, 439, 295, 552, 11, 291, 393, 6839, 721, 411, 426, 23704, 42, 11, 264, 50800], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 123, "seek": 52662, "start": 535.34, "end": 539.74, "text": " Natural Language Toolkit, Stanza, which I believe is coming out of the same program", "tokens": [50800, 20137, 24445, 15934, 22681, 11, 745, 20030, 11, 597, 286, 1697, 307, 1348, 484, 295, 264, 912, 1461, 51020], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 124, "seek": 52662, "start": 539.74, "end": 541.54, "text": " at Stanford.", "tokens": [51020, 412, 20374, 13, 51110], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 125, "seek": 52662, "start": 541.54, "end": 545.7, "text": " There's many out there, but I find SPACI to be the best of all of them for a couple", "tokens": [51110, 821, 311, 867, 484, 456, 11, 457, 286, 915, 8420, 4378, 40, 281, 312, 264, 1151, 295, 439, 295, 552, 337, 257, 1916, 51318], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 126, "seek": 52662, "start": 545.7, "end": 547.18, "text": " different reasons.", "tokens": [51318, 819, 4112, 13, 51392], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 127, "seek": 52662, "start": 547.18, "end": 551.86, "text": " Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they", "tokens": [51392, 39693, 472, 307, 300, 436, 2893, 337, 291, 766, 12, 3322, 12, 46626, 5245, 300, 18927, 588, 731, 11, 3620, 436, 51626], "temperature": 0.0, "avg_logprob": -0.11973395229371125, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.15199171006679535}, {"id": 128, "seek": 55186, "start": 551.86, "end": 556.7, "text": " perform very quickly, and they also have very good accuracy metrics, such as precision,", "tokens": [50364, 2042, 588, 2661, 11, 293, 436, 611, 362, 588, 665, 14170, 16367, 11, 1270, 382, 18356, 11, 50606], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 129, "seek": 55186, "start": 556.7, "end": 557.7, "text": " recall, and f-score.", "tokens": [50606, 9901, 11, 293, 283, 12, 4417, 418, 13, 50656], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 130, "seek": 55186, "start": 557.7, "end": 561.5, "text": " And I'm not going to talk too much about the way we measure machine learning accuracy", "tokens": [50656, 400, 286, 478, 406, 516, 281, 751, 886, 709, 466, 264, 636, 321, 3481, 3479, 2539, 14170, 50846], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 131, "seek": 55186, "start": 561.5, "end": 564.5, "text": " right now, but know that they are quite good.", "tokens": [50846, 558, 586, 11, 457, 458, 300, 436, 366, 1596, 665, 13, 50996], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 132, "seek": 55186, "start": 564.5, "end": 569.74, "text": " Second, SPACI has the ability to leverage current natural language processing methods,", "tokens": [50996, 5736, 11, 8420, 4378, 40, 575, 264, 3485, 281, 13982, 2190, 3303, 2856, 9007, 7150, 11, 51258], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 133, "seek": 55186, "start": 569.74, "end": 575.1, "text": " specifically, transformer models, also known usually kind of collectively as BERT models,", "tokens": [51258, 4682, 11, 31782, 5245, 11, 611, 2570, 2673, 733, 295, 24341, 382, 363, 31479, 5245, 11, 51526], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 134, "seek": 55186, "start": 575.1, "end": 577.82, "text": " even though that's not entirely accurate.", "tokens": [51526, 754, 1673, 300, 311, 406, 7696, 8559, 13, 51662], "temperature": 0.0, "avg_logprob": -0.19438535357834003, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.6257524490356445}, {"id": 135, "seek": 57782, "start": 577.82, "end": 581.7800000000001, "text": " And it allows for you to use an off-the-shelf transformer model.", "tokens": [50364, 400, 309, 4045, 337, 291, 281, 764, 364, 766, 12, 3322, 12, 46626, 31782, 2316, 13, 50562], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 136, "seek": 57782, "start": 581.7800000000001, "end": 587.82, "text": " And third, it provides the framework for doing custom training relatively easily compared", "tokens": [50562, 400, 2636, 11, 309, 6417, 264, 8388, 337, 884, 2375, 3097, 7226, 3612, 5347, 50864], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 137, "seek": 57782, "start": 587.82, "end": 590.82, "text": " to these other NLP frameworks that are out there.", "tokens": [50864, 281, 613, 661, 426, 45196, 29834, 300, 366, 484, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 138, "seek": 57782, "start": 590.82, "end": 596.34, "text": " Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales", "tokens": [51014, 6288, 11, 264, 6409, 1778, 983, 286, 6183, 8420, 4378, 40, 670, 661, 426, 45196, 29834, 307, 570, 309, 17408, 51290], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 139, "seek": 57782, "start": 596.34, "end": 597.34, "text": " well.", "tokens": [51290, 731, 13, 51340], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 140, "seek": 57782, "start": 597.34, "end": 602.86, "text": " SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale.", "tokens": [51340, 8420, 4378, 40, 390, 4761, 538, 12514, 19996, 48698, 11, 293, 264, 2302, 4334, 295, 8420, 4378, 40, 307, 281, 589, 412, 4373, 13, 51616], "temperature": 0.0, "avg_logprob": -0.15226141296991028, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.1520000398159027}, {"id": 141, "seek": 60286, "start": 602.86, "end": 609.1, "text": " By at scale, we mean working with large quantities of documents efficiently, effectively, and", "tokens": [50364, 3146, 412, 4373, 11, 321, 914, 1364, 365, 2416, 22927, 295, 8512, 19621, 11, 8659, 11, 293, 50676], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 142, "seek": 60286, "start": 609.1, "end": 610.62, "text": " accurately.", "tokens": [50676, 20095, 13, 50752], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 143, "seek": 60286, "start": 610.62, "end": 615.1, "text": " SPACI scales well because it can process hundreds of thousands of documents with relative ease", "tokens": [50752, 8420, 4378, 40, 17408, 731, 570, 309, 393, 1399, 6779, 295, 5383, 295, 8512, 365, 4972, 12708, 50976], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 144, "seek": 60286, "start": 615.1, "end": 620.22, "text": " in a relative short period of time, especially if you stick with more rules-based pipes,", "tokens": [50976, 294, 257, 4972, 2099, 2896, 295, 565, 11, 2318, 498, 291, 2897, 365, 544, 4474, 12, 6032, 21882, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 145, "seek": 60286, "start": 620.22, "end": 624.1, "text": " which we're going to talk about in part two of this video.", "tokens": [51232, 597, 321, 434, 516, 281, 751, 466, 294, 644, 732, 295, 341, 960, 13, 51426], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 146, "seek": 60286, "start": 624.1, "end": 629.02, "text": " So those are the two things you really need to know about NLP and SPACI in general.", "tokens": [51426, 407, 729, 366, 264, 732, 721, 291, 534, 643, 281, 458, 466, 426, 45196, 293, 8420, 4378, 40, 294, 2674, 13, 51672], "temperature": 0.0, "avg_logprob": -0.12525142156160796, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.36282581090927124}, {"id": 147, "seek": 62902, "start": 629.02, "end": 633.5799999999999, "text": " We're going to talk about SPACI in-depth as we explore it both through this video and", "tokens": [50364, 492, 434, 516, 281, 751, 466, 8420, 4378, 40, 294, 12, 25478, 382, 321, 6839, 309, 1293, 807, 341, 960, 293, 50592], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 148, "seek": 62902, "start": 633.5799999999999, "end": 641.46, "text": " in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,", "tokens": [50592, 294, 264, 1737, 25591, 286, 2893, 281, 352, 2051, 365, 341, 960, 11, 597, 307, 6870, 412, 637, 2551, 13, 8200, 11943, 18796, 1088, 13, 1112, 11, 50986], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 149, "seek": 62902, "start": 641.46, "end": 644.9, "text": " and it should be linked in the description down below.", "tokens": [50986, 293, 309, 820, 312, 9408, 294, 264, 3855, 760, 2507, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 150, "seek": 62902, "start": 644.9, "end": 648.46, "text": " This video and the textbook are meant to work in tandem.", "tokens": [51158, 639, 960, 293, 264, 25591, 366, 4140, 281, 589, 294, 48120, 13, 51336], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 151, "seek": 62902, "start": 648.46, "end": 652.1, "text": " Some stuff that I cover in the video might not necessarily be in the textbook because", "tokens": [51336, 2188, 1507, 300, 286, 2060, 294, 264, 960, 1062, 406, 4725, 312, 294, 264, 25591, 570, 51518], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 152, "seek": 62902, "start": 652.1, "end": 657.14, "text": " it doesn't lend itself well to text representation, and the same goes for the opposite.", "tokens": [51518, 309, 1177, 380, 21774, 2564, 731, 281, 2487, 10290, 11, 293, 264, 912, 1709, 337, 264, 6182, 13, 51770], "temperature": 0.0, "avg_logprob": -0.10617360347459297, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.6403557658195496}, {"id": 153, "seek": 65714, "start": 657.14, "end": 661.58, "text": " Some stuff that I don't have the time to cover verbatim in this video I cover in a", "tokens": [50364, 2188, 1507, 300, 286, 500, 380, 362, 264, 565, 281, 2060, 9595, 267, 332, 294, 341, 960, 286, 2060, 294, 257, 50586], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 154, "seek": 65714, "start": 661.58, "end": 664.98, "text": " little bit more depth in the book.", "tokens": [50586, 707, 857, 544, 7161, 294, 264, 1446, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 155, "seek": 65714, "start": 664.98, "end": 667.14, "text": " I think that you should try to use both of these.", "tokens": [50756, 286, 519, 300, 291, 820, 853, 281, 764, 1293, 295, 613, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 156, "seek": 65714, "start": 667.14, "end": 671.58, "text": " What I would recommend is doing one pass through this whole video, watch it in its entirety,", "tokens": [50864, 708, 286, 576, 2748, 307, 884, 472, 1320, 807, 341, 1379, 960, 11, 1159, 309, 294, 1080, 31557, 11, 51086], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 157, "seek": 65714, "start": 671.58, "end": 675.42, "text": " and get an umbrella sense of everything that SPACI can do and everything that we're going", "tokens": [51086, 293, 483, 364, 21925, 2020, 295, 1203, 300, 8420, 4378, 40, 393, 360, 293, 1203, 300, 321, 434, 516, 51278], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 158, "seek": 65714, "start": 675.42, "end": 676.58, "text": " to cover.", "tokens": [51278, 281, 2060, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 159, "seek": 65714, "start": 676.58, "end": 682.1, "text": " I would then go back and try to replicate each stage of this process on a separate window", "tokens": [51336, 286, 576, 550, 352, 646, 293, 853, 281, 25356, 1184, 3233, 295, 341, 1399, 322, 257, 4994, 4910, 51612], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 160, "seek": 65714, "start": 682.1, "end": 686.38, "text": " or on a separate screen and try to kind of follow along in code, and then I would go", "tokens": [51612, 420, 322, 257, 4994, 2568, 293, 853, 281, 733, 295, 1524, 2051, 294, 3089, 11, 293, 550, 286, 576, 352, 51826], "temperature": 0.0, "avg_logprob": -0.1046647196230681, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.2813185453414917}, {"id": 161, "seek": 68638, "start": 686.38, "end": 690.66, "text": " back through a third time and try to watch the first part where I talk about what we're", "tokens": [50364, 646, 807, 257, 2636, 565, 293, 853, 281, 1159, 264, 700, 644, 689, 286, 751, 466, 437, 321, 434, 50578], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 162, "seek": 68638, "start": 690.66, "end": 694.54, "text": " going to be doing and try to do it on your own without looking at the textbook or the", "tokens": [50578, 516, 281, 312, 884, 293, 853, 281, 360, 309, 322, 428, 1065, 1553, 1237, 412, 264, 25591, 420, 264, 50772], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 163, "seek": 68638, "start": 694.54, "end": 695.54, "text": " video.", "tokens": [50772, 960, 13, 50822], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 164, "seek": 68638, "start": 695.54, "end": 699.82, "text": " If you can do that by your third pass, you'll be in very good shape to start using SPACI", "tokens": [50822, 759, 291, 393, 360, 300, 538, 428, 2636, 1320, 11, 291, 603, 312, 294, 588, 665, 3909, 281, 722, 1228, 8420, 4378, 40, 51036], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 165, "seek": 68638, "start": 699.82, "end": 702.98, "text": " to solve your own domain specific problems.", "tokens": [51036, 281, 5039, 428, 1065, 9274, 2685, 2740, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 166, "seek": 68638, "start": 702.98, "end": 709.7, "text": " NLP is a complex field, and applying NLP is really complex, but fortunately frameworks", "tokens": [51194, 426, 45196, 307, 257, 3997, 2519, 11, 293, 9275, 426, 45196, 307, 534, 3997, 11, 457, 25511, 29834, 51530], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 167, "seek": 68638, "start": 709.7, "end": 714.3, "text": " like SPACI make this project and this process a lot easier.", "tokens": [51530, 411, 8420, 4378, 40, 652, 341, 1716, 293, 341, 1399, 257, 688, 3571, 13, 51760], "temperature": 0.0, "avg_logprob": -0.13396027508903952, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.01971595361828804}, {"id": 168, "seek": 71430, "start": 714.3, "end": 718.42, "text": " I encourage you to spend a few hours in this video, get to know SPACI, and I think you're", "tokens": [50364, 286, 5373, 291, 281, 3496, 257, 1326, 2496, 294, 341, 960, 11, 483, 281, 458, 8420, 4378, 40, 11, 293, 286, 519, 291, 434, 50570], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 169, "seek": 71430, "start": 718.42, "end": 722.74, "text": " going to find that you can do things that you didn't think possible in relative short", "tokens": [50570, 516, 281, 915, 300, 291, 393, 360, 721, 300, 291, 994, 380, 519, 1944, 294, 4972, 2099, 50786], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 170, "seek": 71430, "start": 722.74, "end": 723.9, "text": " order.", "tokens": [50786, 1668, 13, 50844], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 171, "seek": 71430, "start": 723.9, "end": 728.18, "text": " So sit back, relax, and enjoy this video series on SPACI.", "tokens": [50844, 407, 1394, 646, 11, 5789, 11, 293, 2103, 341, 960, 2638, 322, 8420, 4378, 40, 13, 51058], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 172, "seek": 71430, "start": 728.18, "end": 732.8199999999999, "text": " In order to use SPACI, you're first going to have to install SPACI.", "tokens": [51058, 682, 1668, 281, 764, 8420, 4378, 40, 11, 291, 434, 700, 516, 281, 362, 281, 3625, 8420, 4378, 40, 13, 51290], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 173, "seek": 71430, "start": 732.8199999999999, "end": 737.3399999999999, "text": " Now there's a few different ways to do this depending on your environment and your operating", "tokens": [51290, 823, 456, 311, 257, 1326, 819, 2098, 281, 360, 341, 5413, 322, 428, 2823, 293, 428, 7447, 51516], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 174, "seek": 71430, "start": 737.3399999999999, "end": 738.3399999999999, "text": " system.", "tokens": [51516, 1185, 13, 51566], "temperature": 0.0, "avg_logprob": -0.11865666038111637, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.4146250784397125}, {"id": 175, "seek": 73834, "start": 738.34, "end": 745.7, "text": " I recommend going to SPACI.io backslash usage and kind of enter in the correct framework", "tokens": [50364, 286, 2748, 516, 281, 8420, 4378, 40, 13, 1004, 646, 10418, 1299, 14924, 293, 733, 295, 3242, 294, 264, 3006, 8388, 50732], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 176, "seek": 73834, "start": 745.7, "end": 746.7, "text": " that you're working with.", "tokens": [50732, 300, 291, 434, 1364, 365, 13, 50782], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 177, "seek": 73834, "start": 746.7, "end": 751.62, "text": " So if you're using Mac OS versus Windows versus Linux, you can go through and in this very", "tokens": [50782, 407, 498, 291, 434, 1228, 5707, 12731, 5717, 8591, 5717, 18734, 11, 291, 393, 352, 807, 293, 294, 341, 588, 51028], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 178, "seek": 73834, "start": 751.62, "end": 756.36, "text": " handy kind of user interface, you can go through and select the different features that matter", "tokens": [51028, 13239, 733, 295, 4195, 9226, 11, 291, 393, 352, 807, 293, 3048, 264, 819, 4122, 300, 1871, 51265], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 179, "seek": 73834, "start": 756.36, "end": 757.98, "text": " most to you.", "tokens": [51265, 881, 281, 291, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 180, "seek": 73834, "start": 757.98, "end": 762.98, "text": " I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be", "tokens": [51346, 286, 478, 1364, 365, 8591, 11, 286, 478, 516, 281, 312, 1228, 430, 9139, 294, 341, 1389, 11, 293, 286, 478, 516, 281, 312, 51596], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 181, "seek": 73834, "start": 762.98, "end": 767.34, "text": " doing everything on the CPU and I'm going to be working with English.", "tokens": [51596, 884, 1203, 322, 264, 13199, 293, 286, 478, 516, 281, 312, 1364, 365, 3669, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13349404373789223, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.6294295787811279}, {"id": 182, "seek": 76734, "start": 767.34, "end": 770.94, "text": " So I've established all of those different parameters, and it goes through and it tells", "tokens": [50364, 407, 286, 600, 7545, 439, 295, 729, 819, 9834, 11, 293, 309, 1709, 807, 293, 309, 5112, 50544], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 183, "seek": 76734, "start": 770.94, "end": 776.7800000000001, "text": " me exactly how to go through and install it using PIP in the terminal.", "tokens": [50544, 385, 2293, 577, 281, 352, 807, 293, 3625, 309, 1228, 430, 9139, 294, 264, 14709, 13, 50836], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 184, "seek": 76734, "start": 776.7800000000001, "end": 782.02, "text": " So I encourage you to go through pause the video right now, go ahead and install Windows", "tokens": [50836, 407, 286, 5373, 291, 281, 352, 807, 10465, 264, 960, 558, 586, 11, 352, 2286, 293, 3625, 8591, 51098], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 185, "seek": 76734, "start": 782.02, "end": 786.62, "text": " however you want to, I'm going to be walking through how to install it within the Jupyter", "tokens": [51098, 4461, 291, 528, 281, 11, 286, 478, 516, 281, 312, 4494, 807, 577, 281, 3625, 309, 1951, 264, 22125, 88, 391, 51328], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 186, "seek": 76734, "start": 786.62, "end": 789.7, "text": " notebook that we're going to be moving to in just a second.", "tokens": [51328, 21060, 300, 321, 434, 516, 281, 312, 2684, 281, 294, 445, 257, 1150, 13, 51482], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 187, "seek": 76734, "start": 789.7, "end": 793.6600000000001, "text": " I want you to not work with the GPU at all.", "tokens": [51482, 286, 528, 291, 281, 406, 589, 365, 264, 18407, 412, 439, 13, 51680], "temperature": 0.0, "avg_logprob": -0.11364080224718366, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.21196560561656952}, {"id": 188, "seek": 79366, "start": 793.66, "end": 797.9, "text": " Working with Spacey on the GPU requires a lot more understanding about what the GPU", "tokens": [50364, 18337, 365, 8705, 88, 322, 264, 18407, 7029, 257, 688, 544, 3701, 466, 437, 264, 18407, 50576], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 189, "seek": 79366, "start": 797.9, "end": 801.66, "text": " is used for, specifically in training machine learning models.", "tokens": [50576, 307, 1143, 337, 11, 4682, 294, 3097, 3479, 2539, 5245, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 190, "seek": 79366, "start": 801.66, "end": 804.42, "text": " It requires you to have CUDA installed correctly.", "tokens": [50764, 467, 7029, 291, 281, 362, 29777, 7509, 8899, 8944, 13, 50902], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 191, "seek": 79366, "start": 804.42, "end": 808.86, "text": " It requires a couple other things that I don't really have the time to get into in this video,", "tokens": [50902, 467, 7029, 257, 1916, 661, 721, 300, 286, 500, 380, 534, 362, 264, 565, 281, 483, 666, 294, 341, 960, 11, 51124], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 192, "seek": 79366, "start": 808.86, "end": 813.9, "text": " but we'll be addressing in a more advanced Spacey tutorial video.", "tokens": [51124, 457, 321, 603, 312, 14329, 294, 257, 544, 7339, 8705, 88, 7073, 960, 13, 51376], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 193, "seek": 79366, "start": 813.9, "end": 819.54, "text": " So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,", "tokens": [51376, 407, 337, 558, 586, 11, 286, 2748, 18182, 428, 12731, 11, 18182, 2139, 516, 281, 764, 430, 9139, 420, 591, 5575, 11, 51658], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 194, "seek": 79366, "start": 819.54, "end": 823.3, "text": " and then selecting CPU and since you're going to be working through this video with English", "tokens": [51658, 293, 550, 18182, 13199, 293, 1670, 291, 434, 516, 281, 312, 1364, 807, 341, 960, 365, 3669, 51846], "temperature": 0.0, "avg_logprob": -0.15359834986408866, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.8145790696144104}, {"id": 195, "seek": 82330, "start": 823.3399999999999, "end": 829.9799999999999, "text": " texts, I encourage you to select English right now and go ahead and just install or download", "tokens": [50366, 15765, 11, 286, 5373, 291, 281, 3048, 3669, 558, 586, 293, 352, 2286, 293, 445, 3625, 420, 5484, 50698], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 196, "seek": 82330, "start": 829.9799999999999, "end": 832.6999999999999, "text": " the Ncore Web SM model.", "tokens": [50698, 264, 426, 12352, 9573, 13115, 2316, 13, 50834], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 197, "seek": 82330, "start": 832.6999999999999, "end": 833.6999999999999, "text": " This is the small model.", "tokens": [50834, 639, 307, 264, 1359, 2316, 13, 50884], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 198, "seek": 82330, "start": 833.6999999999999, "end": 836.02, "text": " I'll talk about that in just a second.", "tokens": [50884, 286, 603, 751, 466, 300, 294, 445, 257, 1150, 13, 51000], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 199, "seek": 82330, "start": 836.02, "end": 842.66, "text": " So the first thing we're going to do in our Jupyter notebook is we are going to be using", "tokens": [51000, 407, 264, 700, 551, 321, 434, 516, 281, 360, 294, 527, 22125, 88, 391, 21060, 307, 321, 366, 516, 281, 312, 1228, 51332], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 200, "seek": 82330, "start": 842.66, "end": 847.0999999999999, "text": " the exclamation mark to delineate in the cell that this is a terminal command.", "tokens": [51332, 264, 1624, 43233, 1491, 281, 1103, 533, 473, 294, 264, 2815, 300, 341, 307, 257, 14709, 5622, 13, 51554], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 201, "seek": 82330, "start": 847.0999999999999, "end": 850.18, "text": " We're going to say PIP install Spacey.", "tokens": [51554, 492, 434, 516, 281, 584, 430, 9139, 3625, 8705, 88, 13, 51708], "temperature": 0.0, "avg_logprob": -0.16616699430677626, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09396570920944214}, {"id": 202, "seek": 85018, "start": 850.18, "end": 854.14, "text": " Your output when you execute this cell is going to look a little different than mine.", "tokens": [50364, 2260, 5598, 562, 291, 14483, 341, 2815, 307, 516, 281, 574, 257, 707, 819, 813, 3892, 13, 50562], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 203, "seek": 85018, "start": 854.14, "end": 857.26, "text": " I already have Spacey installed in this environment.", "tokens": [50562, 286, 1217, 362, 8705, 88, 8899, 294, 341, 2823, 13, 50718], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 204, "seek": 85018, "start": 857.26, "end": 860.9399999999999, "text": " And so mine kind of goes through and looks like this yours will actually go through.", "tokens": [50718, 400, 370, 3892, 733, 295, 1709, 807, 293, 1542, 411, 341, 6342, 486, 767, 352, 807, 13, 50902], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 205, "seek": 85018, "start": 860.9399999999999, "end": 865.8199999999999, "text": " And instead of saying requirement already satisfied, it'll be actually passing out the the different", "tokens": [50902, 400, 2602, 295, 1566, 11695, 1217, 11239, 11, 309, 603, 312, 767, 8437, 484, 264, 264, 819, 51146], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 206, "seek": 85018, "start": 865.8199999999999, "end": 871.42, "text": " things that it's actually installing to install Spacey and all of its dependencies.", "tokens": [51146, 721, 300, 309, 311, 767, 20762, 281, 3625, 8705, 88, 293, 439, 295, 1080, 36606, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 207, "seek": 85018, "start": 871.42, "end": 876.06, "text": " The next thing that you're going to do is you're going to again, you follow the instructions", "tokens": [51426, 440, 958, 551, 300, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 797, 11, 291, 1524, 264, 9415, 51658], "temperature": 0.0, "avg_logprob": -0.14111729355545732, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.14029385149478912}, {"id": 208, "seek": 87606, "start": 876.14, "end": 882.54, "text": " and you're going to be doing Python dash M space Spacey space download and then the model", "tokens": [50368, 293, 291, 434, 516, 281, 312, 884, 15329, 8240, 376, 1901, 8705, 88, 1901, 5484, 293, 550, 264, 2316, 50688], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 209, "seek": 87606, "start": 882.54, "end": 884.18, "text": " that you want to download.", "tokens": [50688, 300, 291, 528, 281, 5484, 13, 50770], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 210, "seek": 87606, "start": 884.18, "end": 886.2199999999999, "text": " So let's go ahead and do that right now.", "tokens": [50770, 407, 718, 311, 352, 2286, 293, 360, 300, 558, 586, 13, 50872], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 211, "seek": 87606, "start": 886.2199999999999, "end": 892.06, "text": " So let's go ahead and say Python M Spacey download.", "tokens": [50872, 407, 718, 311, 352, 2286, 293, 584, 15329, 376, 8705, 88, 5484, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 212, "seek": 87606, "start": 892.06, "end": 894.54, "text": " So this is a Spacey terminal command.", "tokens": [51164, 407, 341, 307, 257, 8705, 88, 14709, 5622, 13, 51288], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 213, "seek": 87606, "start": 894.54, "end": 897.42, "text": " And we're going to download the Ncore Web SM.", "tokens": [51288, 400, 321, 434, 516, 281, 5484, 264, 426, 12352, 9573, 13115, 13, 51432], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 214, "seek": 87606, "start": 897.42, "end": 900.06, "text": " And again, I already have this model downloaded.", "tokens": [51432, 400, 797, 11, 286, 1217, 362, 341, 2316, 21748, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 215, "seek": 87606, "start": 900.06, "end": 905.4599999999999, "text": " So on my end, Spacey is going to look a little differently than as it's going to look on your", "tokens": [51564, 407, 322, 452, 917, 11, 8705, 88, 307, 516, 281, 574, 257, 707, 7614, 813, 382, 309, 311, 516, 281, 574, 322, 428, 51834], "temperature": 0.0, "avg_logprob": -0.15332500993712875, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.051835671067237854}, {"id": 216, "seek": 90546, "start": 905.46, "end": 909.1, "text": " end as it prints off on the Jupyter notebook.", "tokens": [50364, 917, 382, 309, 22305, 766, 322, 264, 22125, 88, 391, 21060, 13, 50546], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 217, "seek": 90546, "start": 909.1, "end": 913.34, "text": " And if we give it a just a second, everything will go through and it says that it's collected", "tokens": [50546, 400, 498, 321, 976, 309, 257, 445, 257, 1150, 11, 1203, 486, 352, 807, 293, 309, 1619, 300, 309, 311, 11087, 50758], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 218, "seek": 90546, "start": 913.34, "end": 917.9000000000001, "text": " it, it's downloading it and we are all very happy now.", "tokens": [50758, 309, 11, 309, 311, 32529, 309, 293, 321, 366, 439, 588, 2055, 586, 13, 50986], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 219, "seek": 90546, "start": 917.9000000000001, "end": 923.0600000000001, "text": " And so now that we've got Spacey installed correctly, and that we've got the small model", "tokens": [50986, 400, 370, 586, 300, 321, 600, 658, 8705, 88, 8899, 8944, 11, 293, 300, 321, 600, 658, 264, 1359, 2316, 51244], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 220, "seek": 90546, "start": 923.0600000000001, "end": 928.1, "text": " downloaded correctly, we can go ahead and start actually using Spacey and make sure", "tokens": [51244, 21748, 8944, 11, 321, 393, 352, 2286, 293, 722, 767, 1228, 8705, 88, 293, 652, 988, 51496], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 221, "seek": 90546, "start": 928.1, "end": 929.38, "text": " everything's correct.", "tokens": [51496, 1203, 311, 3006, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 222, "seek": 90546, "start": 929.38, "end": 933.7, "text": " The first thing we're going to do is we're going to import the Spacey library as you", "tokens": [51560, 440, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 974, 264, 8705, 88, 6405, 382, 291, 51776], "temperature": 0.0, "avg_logprob": -0.14017926968210112, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02096138894557953}, {"id": 223, "seek": 93370, "start": 933.82, "end": 935.82, "text": " would with any other Python library.", "tokens": [50370, 576, 365, 604, 661, 15329, 6405, 13, 50470], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 224, "seek": 93370, "start": 935.82, "end": 941.5, "text": " If you're not familiar with this, a library is simply a set of classes and functions that", "tokens": [50470, 759, 291, 434, 406, 4963, 365, 341, 11, 257, 6405, 307, 2935, 257, 992, 295, 5359, 293, 6828, 300, 50754], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 225, "seek": 93370, "start": 941.5, "end": 946.26, "text": " you can import into a Python script so that you don't have to write a whole bunch of extra", "tokens": [50754, 291, 393, 974, 666, 257, 15329, 5755, 370, 300, 291, 500, 380, 362, 281, 2464, 257, 1379, 3840, 295, 2857, 50992], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 226, "seek": 93370, "start": 946.26, "end": 947.46, "text": " code.", "tokens": [50992, 3089, 13, 51052], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 227, "seek": 93370, "start": 947.46, "end": 952.22, "text": " Libraries are massive collections of classes and functions that you can call.", "tokens": [51052, 12006, 4889, 366, 5994, 16641, 295, 5359, 293, 6828, 300, 291, 393, 818, 13, 51290], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 228, "seek": 93370, "start": 952.22, "end": 957.38, "text": " So when we import Spacey, we're importing the whole library of Spacey.", "tokens": [51290, 407, 562, 321, 974, 8705, 88, 11, 321, 434, 43866, 264, 1379, 6405, 295, 8705, 88, 13, 51548], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 229, "seek": 93370, "start": 957.38, "end": 961.86, "text": " And now that we've seen something like this, we know that Spacey has imported correctly,", "tokens": [51548, 400, 586, 300, 321, 600, 1612, 746, 411, 341, 11, 321, 458, 300, 8705, 88, 575, 25524, 8944, 11, 51772], "temperature": 0.0, "avg_logprob": -0.12132233067562706, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.24497990310192108}, {"id": 230, "seek": 96186, "start": 961.86, "end": 967.54, "text": " as long as you're not getting an error message, everything was imported fine.", "tokens": [50364, 382, 938, 382, 291, 434, 406, 1242, 364, 6713, 3636, 11, 1203, 390, 25524, 2489, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 231, "seek": 96186, "start": 967.54, "end": 973.46, "text": " The next thing that we need to do is we want to make sure that our English Core Web SM,", "tokens": [50648, 440, 958, 551, 300, 321, 643, 281, 360, 307, 321, 528, 281, 652, 988, 300, 527, 3669, 14798, 9573, 13115, 11, 50944], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 232, "seek": 96186, "start": 973.46, "end": 976.74, "text": " our small English model, was downloaded correctly.", "tokens": [50944, 527, 1359, 3669, 2316, 11, 390, 21748, 8944, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 233, "seek": 96186, "start": 976.74, "end": 980.58, "text": " So the next thing that we need to do is we need to create an NLP object.", "tokens": [51108, 407, 264, 958, 551, 300, 321, 643, 281, 360, 307, 321, 643, 281, 1884, 364, 426, 45196, 2657, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 234, "seek": 96186, "start": 980.58, "end": 984.94, "text": " I'm going to be talking a lot more about this as we move forward right now.", "tokens": [51300, 286, 478, 516, 281, 312, 1417, 257, 688, 544, 466, 341, 382, 321, 1286, 2128, 558, 586, 13, 51518], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 235, "seek": 96186, "start": 984.94, "end": 989.4200000000001, "text": " This is just troubleshooting to make sure that we've installed Spacey correctly and", "tokens": [51518, 639, 307, 445, 15379, 47011, 281, 652, 988, 300, 321, 600, 8899, 8705, 88, 8944, 293, 51742], "temperature": 0.0, "avg_logprob": -0.12180695491554462, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0008830001461319625}, {"id": 236, "seek": 98942, "start": 989.42, "end": 992.14, "text": " we've downloaded our model correctly.", "tokens": [50364, 321, 600, 21748, 527, 2316, 8944, 13, 50500], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 237, "seek": 98942, "start": 992.14, "end": 995.54, "text": " So we're going to use the spacey.load command.", "tokens": [50500, 407, 321, 434, 516, 281, 764, 264, 1901, 88, 13, 2907, 5622, 13, 50670], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 238, "seek": 98942, "start": 995.54, "end": 997.4599999999999, "text": " This is going to take one argument.", "tokens": [50670, 639, 307, 516, 281, 747, 472, 6770, 13, 50766], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 239, "seek": 98942, "start": 997.4599999999999, "end": 1002.6999999999999, "text": " It's going to be a string that is going to correspond to the model that you've installed.", "tokens": [50766, 467, 311, 516, 281, 312, 257, 6798, 300, 307, 516, 281, 6805, 281, 264, 2316, 300, 291, 600, 8899, 13, 51028], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 240, "seek": 98942, "start": 1002.6999999999999, "end": 1007.8199999999999, "text": " In this case, N Core Web SM.", "tokens": [51028, 682, 341, 1389, 11, 426, 14798, 9573, 13115, 13, 51284], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 241, "seek": 98942, "start": 1007.8199999999999, "end": 1014.0999999999999, "text": " And if you execute this cell and you have no errors, you have successfully installed", "tokens": [51284, 400, 498, 291, 14483, 341, 2815, 293, 291, 362, 572, 13603, 11, 291, 362, 10727, 8899, 51598], "temperature": 0.0, "avg_logprob": -0.13155562302161908, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.2280324548482895}, {"id": 242, "seek": 101410, "start": 1014.1, "end": 1019.98, "text": " Spacey correctly and you've downloaded the English Core Web SM model correctly.", "tokens": [50364, 8705, 88, 8944, 293, 291, 600, 21748, 264, 3669, 14798, 9573, 13115, 2316, 8944, 13, 50658], "temperature": 0.0, "avg_logprob": -0.10293069891973373, "compression_ratio": 1.6796875, "no_speech_prob": 0.29408320784568787}, {"id": 243, "seek": 101410, "start": 1019.98, "end": 1025.34, "text": " So go ahead, take time and get all this stuff set up, pause the video if you need to and", "tokens": [50658, 407, 352, 2286, 11, 747, 565, 293, 483, 439, 341, 1507, 992, 493, 11, 10465, 264, 960, 498, 291, 643, 281, 293, 50926], "temperature": 0.0, "avg_logprob": -0.10293069891973373, "compression_ratio": 1.6796875, "no_speech_prob": 0.29408320784568787}, {"id": 244, "seek": 101410, "start": 1025.34, "end": 1031.18, "text": " then pop back and we're going to start actually working through the basics of Spacey.", "tokens": [50926, 550, 1665, 646, 293, 321, 434, 516, 281, 722, 767, 1364, 807, 264, 14688, 295, 8705, 88, 13, 51218], "temperature": 0.0, "avg_logprob": -0.10293069891973373, "compression_ratio": 1.6796875, "no_speech_prob": 0.29408320784568787}, {"id": 245, "seek": 101410, "start": 1031.18, "end": 1036.46, "text": " I'm now going to move into kind of an overview of kind of what's within Spacey, why it's", "tokens": [51218, 286, 478, 586, 516, 281, 1286, 666, 733, 295, 364, 12492, 295, 733, 295, 437, 311, 1951, 8705, 88, 11, 983, 309, 311, 51482], "temperature": 0.0, "avg_logprob": -0.10293069891973373, "compression_ratio": 1.6796875, "no_speech_prob": 0.29408320784568787}, {"id": 246, "seek": 101410, "start": 1036.46, "end": 1040.94, "text": " useful and kind of some of the basic features of it that you need to be familiar with.", "tokens": [51482, 4420, 293, 733, 295, 512, 295, 264, 3875, 4122, 295, 309, 300, 291, 643, 281, 312, 4963, 365, 13, 51706], "temperature": 0.0, "avg_logprob": -0.10293069891973373, "compression_ratio": 1.6796875, "no_speech_prob": 0.29408320784568787}, {"id": 247, "seek": 104094, "start": 1040.94, "end": 1045.42, "text": " And I'm going to be working from the Jupyter Notebook that I talked about in the introduction", "tokens": [50364, 400, 286, 478, 516, 281, 312, 1364, 490, 264, 22125, 88, 391, 11633, 2939, 300, 286, 2825, 466, 294, 264, 9339, 50588], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 248, "seek": 104094, "start": 1045.42, "end": 1046.42, "text": " to this video.", "tokens": [50588, 281, 341, 960, 13, 50638], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 249, "seek": 104094, "start": 1046.42, "end": 1051.8200000000002, "text": " If we scroll down to the bottom of chapter one, the basics of Spacey and you get past", "tokens": [50638, 759, 321, 11369, 760, 281, 264, 2767, 295, 7187, 472, 11, 264, 14688, 295, 8705, 88, 293, 291, 483, 1791, 50908], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 250, "seek": 104094, "start": 1051.8200000000002, "end": 1055.5, "text": " the install section, you get to this section on containers.", "tokens": [50908, 264, 3625, 3541, 11, 291, 483, 281, 341, 3541, 322, 17089, 13, 51092], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 251, "seek": 104094, "start": 1055.5, "end": 1056.78, "text": " So what are containers?", "tokens": [51092, 407, 437, 366, 17089, 30, 51156], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 252, "seek": 104094, "start": 1056.78, "end": 1062.78, "text": " Well, containers within Spacey are objects that contain a large quantity of data about", "tokens": [51156, 1042, 11, 17089, 1951, 8705, 88, 366, 6565, 300, 5304, 257, 2416, 11275, 295, 1412, 466, 51456], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 253, "seek": 104094, "start": 1062.78, "end": 1064.3, "text": " a text.", "tokens": [51456, 257, 2487, 13, 51532], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 254, "seek": 104094, "start": 1064.3, "end": 1067.78, "text": " There are several different containers that you can work with in Spacey.", "tokens": [51532, 821, 366, 2940, 819, 17089, 300, 291, 393, 589, 365, 294, 8705, 88, 13, 51706], "temperature": 0.0, "avg_logprob": -0.11562848724095168, "compression_ratio": 1.7421875, "no_speech_prob": 0.06953222304582596}, {"id": 255, "seek": 106778, "start": 1067.78, "end": 1073.62, "text": " There's the doc, the doc bin, example, language, lexeme, span, span group and token.", "tokens": [50364, 821, 311, 264, 3211, 11, 264, 3211, 5171, 11, 1365, 11, 2856, 11, 476, 87, 5729, 11, 16174, 11, 16174, 1594, 293, 14862, 13, 50656], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 256, "seek": 106778, "start": 1073.62, "end": 1077.42, "text": " We're going to be dealing with the lexeme a little bit in this video series and we're", "tokens": [50656, 492, 434, 516, 281, 312, 6260, 365, 264, 476, 87, 5729, 257, 707, 857, 294, 341, 960, 2638, 293, 321, 434, 50846], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 257, "seek": 106778, "start": 1077.42, "end": 1081.58, "text": " going to be dealing with the language container a little bit in this video series, but really", "tokens": [50846, 516, 281, 312, 6260, 365, 264, 2856, 10129, 257, 707, 857, 294, 341, 960, 2638, 11, 457, 534, 51054], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 258, "seek": 106778, "start": 1081.58, "end": 1085.5, "text": " the three big things that we're going to be talking about again and again is the doc,", "tokens": [51054, 264, 1045, 955, 721, 300, 321, 434, 516, 281, 312, 1417, 466, 797, 293, 797, 307, 264, 3211, 11, 51250], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 259, "seek": 106778, "start": 1085.5, "end": 1088.42, "text": " the span and the token.", "tokens": [51250, 264, 16174, 293, 264, 14862, 13, 51396], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 260, "seek": 106778, "start": 1088.42, "end": 1092.58, "text": " And I think when you first come to Spacey, there's a little bit of a learning curve about", "tokens": [51396, 400, 286, 519, 562, 291, 700, 808, 281, 8705, 88, 11, 456, 311, 257, 707, 857, 295, 257, 2539, 7605, 466, 51604], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 261, "seek": 106778, "start": 1092.58, "end": 1097.26, "text": " what these things are, what they do, how they are structured hierarchically.", "tokens": [51604, 437, 613, 721, 366, 11, 437, 436, 360, 11, 577, 436, 366, 18519, 35250, 984, 13, 51838], "temperature": 0.0, "avg_logprob": -0.12452996707131676, "compression_ratio": 2.0186567164179103, "no_speech_prob": 0.4841342270374298}, {"id": 262, "seek": 109726, "start": 1097.42, "end": 1103.14, "text": " And for that reason, I've created this, in my opinion, kind of easy to understand image", "tokens": [50372, 400, 337, 300, 1778, 11, 286, 600, 2942, 341, 11, 294, 452, 4800, 11, 733, 295, 1858, 281, 1223, 3256, 50658], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 263, "seek": 109726, "start": 1103.14, "end": 1105.42, "text": " of what different containers are.", "tokens": [50658, 295, 437, 819, 17089, 366, 13, 50772], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 264, "seek": 109726, "start": 1105.42, "end": 1110.58, "text": " So if you think about what Spacey is as a pyramid, so a hierarchical system, we've", "tokens": [50772, 407, 498, 291, 519, 466, 437, 8705, 88, 307, 382, 257, 25950, 11, 370, 257, 35250, 804, 1185, 11, 321, 600, 51030], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 265, "seek": 109726, "start": 1110.58, "end": 1116.34, "text": " got all these different containers structured around really the doc object.", "tokens": [51030, 658, 439, 613, 819, 17089, 18519, 926, 534, 264, 3211, 2657, 13, 51318], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 266, "seek": 109726, "start": 1116.34, "end": 1122.3, "text": " Your doc container or your doc object contains a whole bunch of metadata about the text", "tokens": [51318, 2260, 3211, 10129, 420, 428, 3211, 2657, 8306, 257, 1379, 3840, 295, 26603, 466, 264, 2487, 51616], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 267, "seek": 109726, "start": 1122.3, "end": 1126.98, "text": " that you pass to the Spacey pipeline, which we're going to see in practice.", "tokens": [51616, 300, 291, 1320, 281, 264, 8705, 88, 15517, 11, 597, 321, 434, 516, 281, 536, 294, 3124, 13, 51850], "temperature": 0.0, "avg_logprob": -0.10726336006806275, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.02297304943203926}, {"id": 268, "seek": 112698, "start": 1126.98, "end": 1131.5, "text": " In just a few minutes, the doc object contains a bunch of different things.", "tokens": [50364, 682, 445, 257, 1326, 2077, 11, 264, 3211, 2657, 8306, 257, 3840, 295, 819, 721, 13, 50590], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 269, "seek": 112698, "start": 1131.5, "end": 1133.66, "text": " It contains attributes.", "tokens": [50590, 467, 8306, 17212, 13, 50698], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 270, "seek": 112698, "start": 1133.66, "end": 1137.1, "text": " These attributes can be things like sentences.", "tokens": [50698, 1981, 17212, 393, 312, 721, 411, 16579, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 271, "seek": 112698, "start": 1137.1, "end": 1142.14, "text": " So if you iterate over doc.sense, you can actually access all the different sentences", "tokens": [50870, 407, 498, 291, 44497, 670, 3211, 13, 82, 1288, 11, 291, 393, 767, 2105, 439, 264, 819, 16579, 51122], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 272, "seek": 112698, "start": 1142.14, "end": 1145.1, "text": " found within that doc object.", "tokens": [51122, 1352, 1951, 300, 3211, 2657, 13, 51270], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 273, "seek": 112698, "start": 1145.1, "end": 1150.94, "text": " If you iterate over each individual item or index in your doc object, you can get individual", "tokens": [51270, 759, 291, 44497, 670, 1184, 2609, 3174, 420, 8186, 294, 428, 3211, 2657, 11, 291, 393, 483, 2609, 51562], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 274, "seek": 112698, "start": 1150.94, "end": 1152.38, "text": " tokens.", "tokens": [51562, 22667, 13, 51634], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 275, "seek": 112698, "start": 1152.38, "end": 1156.54, "text": " Tokens are going to be things like words or punctuation marks.", "tokens": [51634, 11036, 694, 366, 516, 281, 312, 721, 411, 2283, 420, 27006, 16073, 10640, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14436658529134896, "compression_ratio": 1.8933333333333333, "no_speech_prob": 0.006487276870757341}, {"id": 276, "seek": 115654, "start": 1156.58, "end": 1163.02, "text": " Anything within your sentence or text that has a self contained important value, either", "tokens": [50366, 11998, 1951, 428, 8174, 420, 2487, 300, 575, 257, 2698, 16212, 1021, 2158, 11, 2139, 50688], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 277, "seek": 115654, "start": 1163.02, "end": 1165.6599999999999, "text": " syntactically or semantically.", "tokens": [50688, 23980, 578, 984, 420, 4361, 49505, 13, 50820], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 278, "seek": 115654, "start": 1165.6599999999999, "end": 1170.34, "text": " So this is going to be things like words, a comma, a period, a semi colon, a quotation", "tokens": [50820, 407, 341, 307, 516, 281, 312, 721, 411, 2283, 11, 257, 22117, 11, 257, 2896, 11, 257, 12909, 8255, 11, 257, 47312, 51054], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 279, "seek": 115654, "start": 1170.34, "end": 1174.1399999999999, "text": " mark, things like this, these are all going to be your tokens.", "tokens": [51054, 1491, 11, 721, 411, 341, 11, 613, 366, 439, 516, 281, 312, 428, 22667, 13, 51244], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 280, "seek": 115654, "start": 1174.1399999999999, "end": 1179.8999999999999, "text": " And we're going to see how tokens are a little different than just splitting words up with", "tokens": [51244, 400, 321, 434, 516, 281, 536, 577, 22667, 366, 257, 707, 819, 813, 445, 30348, 2283, 493, 365, 51532], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 281, "seek": 115654, "start": 1179.8999999999999, "end": 1183.8999999999999, "text": " traditional string methods and Python.", "tokens": [51532, 5164, 6798, 7150, 293, 15329, 13, 51732], "temperature": 0.0, "avg_logprob": -0.16039439329167002, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.0028006338980048895}, {"id": 282, "seek": 118390, "start": 1183.9, "end": 1187.5800000000002, "text": " The next thing that you should be kind of familiar with are spans.", "tokens": [50364, 440, 958, 551, 300, 291, 820, 312, 733, 295, 4963, 365, 366, 44086, 13, 50548], "temperature": 0.0, "avg_logprob": -0.08677602094762465, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.2067452073097229}, {"id": 283, "seek": 118390, "start": 1187.5800000000002, "end": 1194.38, "text": " So spans are important because they kind of exist within and without of the doc object.", "tokens": [50548, 407, 44086, 366, 1021, 570, 436, 733, 295, 2514, 1951, 293, 1553, 295, 264, 3211, 2657, 13, 50888], "temperature": 0.0, "avg_logprob": -0.08677602094762465, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.2067452073097229}, {"id": 284, "seek": 118390, "start": 1194.38, "end": 1201.22, "text": " So unlike the token, which is an index of the doc object, a span can be a token itself,", "tokens": [50888, 407, 8343, 264, 14862, 11, 597, 307, 364, 8186, 295, 264, 3211, 2657, 11, 257, 16174, 393, 312, 257, 14862, 2564, 11, 51230], "temperature": 0.0, "avg_logprob": -0.08677602094762465, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.2067452073097229}, {"id": 285, "seek": 118390, "start": 1201.22, "end": 1204.74, "text": " but it can also be a sequence of multiple tokens.", "tokens": [51230, 457, 309, 393, 611, 312, 257, 8310, 295, 3866, 22667, 13, 51406], "temperature": 0.0, "avg_logprob": -0.08677602094762465, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.2067452073097229}, {"id": 286, "seek": 118390, "start": 1204.74, "end": 1206.26, "text": " We're going to see that at play.", "tokens": [51406, 492, 434, 516, 281, 536, 300, 412, 862, 13, 51482], "temperature": 0.0, "avg_logprob": -0.08677602094762465, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.2067452073097229}, {"id": 287, "seek": 120626, "start": 1206.26, "end": 1215.26, "text": " So imagine if you had a span in its category, maybe group one are our places.", "tokens": [50364, 407, 3811, 498, 291, 632, 257, 16174, 294, 1080, 7719, 11, 1310, 1594, 472, 366, 527, 3190, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 288, "seek": 120626, "start": 1215.26, "end": 1218.74, "text": " So a single token might be like a city like Berlin.", "tokens": [50814, 407, 257, 2167, 14862, 1062, 312, 411, 257, 2307, 411, 13848, 13, 50988], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 289, "seek": 120626, "start": 1218.74, "end": 1223.22, "text": " But span group two, this could be something like full proper names.", "tokens": [50988, 583, 16174, 1594, 732, 11, 341, 727, 312, 746, 411, 1577, 2296, 5288, 13, 51212], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 290, "seek": 120626, "start": 1223.22, "end": 1227.5, "text": " So of a people, for example, so this could be like as we're going to see Martin Luther", "tokens": [51212, 407, 295, 257, 561, 11, 337, 1365, 11, 370, 341, 727, 312, 411, 382, 321, 434, 516, 281, 536, 9184, 20693, 51426], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 291, "seek": 120626, "start": 1227.5, "end": 1228.5, "text": " King.", "tokens": [51426, 3819, 13, 51476], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 292, "seek": 120626, "start": 1228.5, "end": 1233.26, "text": " This would be a sequence of tokens, a sequence of three different items in the sentence that", "tokens": [51476, 639, 576, 312, 257, 8310, 295, 22667, 11, 257, 8310, 295, 1045, 819, 4754, 294, 264, 8174, 300, 51714], "temperature": 0.0, "avg_logprob": -0.15059954710681028, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.23362639546394348}, {"id": 293, "seek": 123326, "start": 1233.26, "end": 1237.18, "text": " make up one span or one self contained item.", "tokens": [50364, 652, 493, 472, 16174, 420, 472, 2698, 16212, 3174, 13, 50560], "temperature": 0.0, "avg_logprob": -0.09316645969044078, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.12585820257663727}, {"id": 294, "seek": 123326, "start": 1237.18, "end": 1245.42, "text": " So Martin Luther King would be a person who's a collection of a sequence of individual tokens.", "tokens": [50560, 407, 9184, 20693, 3819, 576, 312, 257, 954, 567, 311, 257, 5765, 295, 257, 8310, 295, 2609, 22667, 13, 50972], "temperature": 0.0, "avg_logprob": -0.09316645969044078, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.12585820257663727}, {"id": 295, "seek": 123326, "start": 1245.42, "end": 1250.62, "text": " If that doesn't make sense right now, this image will be reinforced as we go through", "tokens": [50972, 759, 300, 1177, 380, 652, 2020, 558, 586, 11, 341, 3256, 486, 312, 31365, 382, 321, 352, 807, 51232], "temperature": 0.0, "avg_logprob": -0.09316645969044078, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.12585820257663727}, {"id": 296, "seek": 123326, "start": 1250.62, "end": 1254.74, "text": " and learn more about spacey in practice.", "tokens": [51232, 293, 1466, 544, 466, 1901, 88, 294, 3124, 13, 51438], "temperature": 0.0, "avg_logprob": -0.09316645969044078, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.12585820257663727}, {"id": 297, "seek": 123326, "start": 1254.74, "end": 1261.5, "text": " For right now, I want you to be just understanding that the doc object is the thing around which", "tokens": [51438, 1171, 558, 586, 11, 286, 528, 291, 281, 312, 445, 3701, 300, 264, 3211, 2657, 307, 264, 551, 926, 597, 51776], "temperature": 0.0, "avg_logprob": -0.09316645969044078, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.12585820257663727}, {"id": 298, "seek": 126150, "start": 1261.5, "end": 1263.42, "text": " all of spacey sits.", "tokens": [50364, 439, 295, 1901, 88, 12696, 13, 50460], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 299, "seek": 126150, "start": 1263.42, "end": 1266.28, "text": " This is going to be the object that you create.", "tokens": [50460, 639, 307, 516, 281, 312, 264, 2657, 300, 291, 1884, 13, 50603], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 300, "seek": 126150, "start": 1266.28, "end": 1270.74, "text": " This is going to be the object that contains all the metadata that you need to access.", "tokens": [50603, 639, 307, 516, 281, 312, 264, 2657, 300, 8306, 439, 264, 26603, 300, 291, 643, 281, 2105, 13, 50826], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 301, "seek": 126150, "start": 1270.74, "end": 1276.74, "text": " And this is going to be the object that you try to essentially improve with different", "tokens": [50826, 400, 341, 307, 516, 281, 312, 264, 2657, 300, 291, 853, 281, 4476, 3470, 365, 819, 51126], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 302, "seek": 126150, "start": 1276.74, "end": 1282.26, "text": " custom components, factories and pipelines as you go through and do more advanced things", "tokens": [51126, 2375, 6677, 11, 24813, 293, 40168, 382, 291, 352, 807, 293, 360, 544, 7339, 721, 51402], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 303, "seek": 126150, "start": 1282.26, "end": 1283.26, "text": " with spacey.", "tokens": [51402, 365, 1901, 88, 13, 51452], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 304, "seek": 126150, "start": 1283.26, "end": 1289.26, "text": " We're going to now see in just a few seconds how that doc object is kind of similar to the", "tokens": [51452, 492, 434, 516, 281, 586, 536, 294, 445, 257, 1326, 3949, 577, 300, 3211, 2657, 307, 733, 295, 2531, 281, 264, 51752], "temperature": 0.0, "avg_logprob": -0.09553806804050909, "compression_ratio": 1.8908296943231442, "no_speech_prob": 0.30058109760284424}, {"id": 305, "seek": 128926, "start": 1289.26, "end": 1295.86, "text": " text itself, but how it's very, very different and much more powerful.", "tokens": [50364, 2487, 2564, 11, 457, 577, 309, 311, 588, 11, 588, 819, 293, 709, 544, 4005, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 306, "seek": 128926, "start": 1295.86, "end": 1299.42, "text": " We're now going to be moving on to chapter two of this textbook, which is going to deal", "tokens": [50694, 492, 434, 586, 516, 281, 312, 2684, 322, 281, 7187, 732, 295, 341, 25591, 11, 597, 307, 516, 281, 2028, 50872], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 307, "seek": 128926, "start": 1299.42, "end": 1304.3799999999999, "text": " with kind of getting used to the in depth features of spacey.", "tokens": [50872, 365, 733, 295, 1242, 1143, 281, 264, 294, 7161, 4122, 295, 1901, 88, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 308, "seek": 128926, "start": 1304.3799999999999, "end": 1309.22, "text": " If you want to pause the video or keep this notebook or this book open up kind of separate", "tokens": [51120, 759, 291, 528, 281, 10465, 264, 960, 420, 1066, 341, 21060, 420, 341, 1446, 1269, 493, 733, 295, 4994, 51362], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 309, "seek": 128926, "start": 1309.22, "end": 1313.98, "text": " from this video and follow along as we go through and explore it in live coding.", "tokens": [51362, 490, 341, 960, 293, 1524, 2051, 382, 321, 352, 807, 293, 6839, 309, 294, 1621, 17720, 13, 51600], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 310, "seek": 128926, "start": 1313.98, "end": 1317.42, "text": " We're going to be talking about a few different things as we explore chapter two.", "tokens": [51600, 492, 434, 516, 281, 312, 1417, 466, 257, 1326, 819, 721, 382, 321, 6839, 7187, 732, 13, 51772], "temperature": 0.0, "avg_logprob": -0.1206615480602297, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.004609419498592615}, {"id": 311, "seek": 131742, "start": 1317.78, "end": 1320.18, "text": " This will be a lot longer than chapter one.", "tokens": [50382, 639, 486, 312, 257, 688, 2854, 813, 7187, 472, 13, 50502], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 312, "seek": 131742, "start": 1320.18, "end": 1324.38, "text": " We're going to be not only importing spacey, but actually going through and loading up", "tokens": [50502, 492, 434, 516, 281, 312, 406, 787, 43866, 1901, 88, 11, 457, 767, 516, 807, 293, 15114, 493, 50712], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 313, "seek": 131742, "start": 1324.38, "end": 1329.3000000000002, "text": " a model, creating a doc object around that model so that we're going to work with the", "tokens": [50712, 257, 2316, 11, 4084, 257, 3211, 2657, 926, 300, 2316, 370, 300, 321, 434, 516, 281, 589, 365, 264, 50958], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 314, "seek": 131742, "start": 1329.3000000000002, "end": 1331.18, "text": " doc container and practice.", "tokens": [50958, 3211, 10129, 293, 3124, 13, 51052], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 315, "seek": 131742, "start": 1331.18, "end": 1335.5, "text": " And then we're going to see how that doc container stores a lot of different features", "tokens": [51052, 400, 550, 321, 434, 516, 281, 536, 577, 300, 3211, 10129, 9512, 257, 688, 295, 819, 4122, 51268], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 316, "seek": 131742, "start": 1335.5, "end": 1338.66, "text": " or metadata or attributes about the text.", "tokens": [51268, 420, 26603, 420, 17212, 466, 264, 2487, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 317, "seek": 131742, "start": 1338.66, "end": 1343.02, "text": " And while they look the same on the surface, they're actually quite different.", "tokens": [51426, 400, 1339, 436, 574, 264, 912, 322, 264, 3753, 11, 436, 434, 767, 1596, 819, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14492766354062142, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.01322200894355774}, {"id": 318, "seek": 134302, "start": 1343.02, "end": 1348.7, "text": " So let's go ahead and work within our same Jupiter notebook where we've imported spacey", "tokens": [50364, 407, 718, 311, 352, 2286, 293, 589, 1951, 527, 912, 24567, 21060, 689, 321, 600, 25524, 1901, 88, 50648], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 319, "seek": 134302, "start": 1348.7, "end": 1351.94, "text": " and we have already created the NLP object.", "tokens": [50648, 293, 321, 362, 1217, 2942, 264, 426, 45196, 2657, 13, 50810], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 320, "seek": 134302, "start": 1351.94, "end": 1356.98, "text": " The first thing that I want to do is I want to open up a text to start working with within", "tokens": [50810, 440, 700, 551, 300, 286, 528, 281, 360, 307, 286, 528, 281, 1269, 493, 257, 2487, 281, 722, 1364, 365, 1951, 51062], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 321, "seek": 134302, "start": 1356.98, "end": 1359.22, "text": " this repo.", "tokens": [51062, 341, 49040, 13, 51174], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 322, "seek": 134302, "start": 1359.22, "end": 1362.58, "text": " We've got a data folder within this data sub folder.", "tokens": [51174, 492, 600, 658, 257, 1412, 10820, 1951, 341, 1412, 1422, 10820, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 323, "seek": 134302, "start": 1362.58, "end": 1365.26, "text": " I've got a couple of different Wikipedia openings.", "tokens": [51342, 286, 600, 658, 257, 1916, 295, 819, 28999, 35941, 13, 51476], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 324, "seek": 134302, "start": 1365.26, "end": 1369.42, "text": " I've got one on MLK that we're going to be using a little later in this video and I have", "tokens": [51476, 286, 600, 658, 472, 322, 21601, 42, 300, 321, 434, 516, 281, 312, 1228, 257, 707, 1780, 294, 341, 960, 293, 286, 362, 51684], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 325, "seek": 134302, "start": 1369.42, "end": 1371.5, "text": " one on the United States.", "tokens": [51684, 472, 322, 264, 2824, 3040, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15026125632041742, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.09533651173114777}, {"id": 326, "seek": 137150, "start": 1371.5, "end": 1374.18, "text": " This is wiki underscore us.", "tokens": [50364, 639, 307, 261, 9850, 37556, 505, 13, 50498], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 327, "seek": 137150, "start": 1374.18, "end": 1376.22, "text": " That's going to be what we work with right now.", "tokens": [50498, 663, 311, 516, 281, 312, 437, 321, 589, 365, 558, 586, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 328, "seek": 137150, "start": 1376.22, "end": 1384.26, "text": " So let's use our with operator and open up data backslash wiki underscore us dot txt.", "tokens": [50600, 407, 718, 311, 764, 527, 365, 12973, 293, 1269, 493, 1412, 646, 10418, 1299, 261, 9850, 37556, 505, 5893, 256, 734, 13, 51002], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 329, "seek": 137150, "start": 1384.26, "end": 1389.3, "text": " We're going to just read that in as F and then we're going to create this text object,", "tokens": [51002, 492, 434, 516, 281, 445, 1401, 300, 294, 382, 479, 293, 550, 321, 434, 516, 281, 1884, 341, 2487, 2657, 11, 51254], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 330, "seek": 137150, "start": 1389.3, "end": 1392.22, "text": " which is going to be equal to F dot read.", "tokens": [51254, 597, 307, 516, 281, 312, 2681, 281, 479, 5893, 1401, 13, 51400], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 331, "seek": 137150, "start": 1392.22, "end": 1396.26, "text": " And now that we've got our text object created, let's go ahead and see what this looks like.", "tokens": [51400, 400, 586, 300, 321, 600, 658, 527, 2487, 2657, 2942, 11, 718, 311, 352, 2286, 293, 536, 437, 341, 1542, 411, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 332, "seek": 137150, "start": 1396.26, "end": 1398.78, "text": " So let's print text.", "tokens": [51602, 407, 718, 311, 4482, 2487, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1610059330606053, "compression_ratio": 1.7719298245614035, "no_speech_prob": 0.022975612431764603}, {"id": 333, "seek": 139878, "start": 1398.78, "end": 1402.98, "text": " Then we see that it's a standard Wikipedia article kind of follows that same introductory", "tokens": [50364, 1396, 321, 536, 300, 309, 311, 257, 3832, 28999, 7222, 733, 295, 10002, 300, 912, 39048, 50574], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 334, "seek": 139878, "start": 1402.98, "end": 1408.42, "text": " format and it's about four or five paragraphs long with a lot of the features left in such", "tokens": [50574, 7877, 293, 309, 311, 466, 1451, 420, 1732, 48910, 938, 365, 257, 688, 295, 264, 4122, 1411, 294, 1270, 50846], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 335, "seek": 139878, "start": 1408.42, "end": 1411.8999999999999, "text": " as the brackets that delineate some kind of a footnote.", "tokens": [50846, 382, 264, 26179, 300, 1103, 533, 473, 512, 733, 295, 257, 2671, 22178, 13, 51020], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 336, "seek": 139878, "start": 1411.8999999999999, "end": 1415.3, "text": " We're not going to worry too much about cleaning this up right now because we're interested", "tokens": [51020, 492, 434, 406, 516, 281, 3292, 886, 709, 466, 8924, 341, 493, 558, 586, 570, 321, 434, 3102, 51190], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 337, "seek": 139878, "start": 1415.3, "end": 1421.46, "text": " not with cleaning our data so much as just starting to work with the doc object in spacey.", "tokens": [51190, 406, 365, 8924, 527, 1412, 370, 709, 382, 445, 2891, 281, 589, 365, 264, 3211, 2657, 294, 1901, 88, 13, 51498], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 338, "seek": 139878, "start": 1421.46, "end": 1426.22, "text": " So the first thing that you want to do is you're going to want to create a doc object.", "tokens": [51498, 407, 264, 700, 551, 300, 291, 528, 281, 360, 307, 291, 434, 516, 281, 528, 281, 1884, 257, 3211, 2657, 13, 51736], "temperature": 0.0, "avg_logprob": -0.10216113387561235, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.014956413768231869}, {"id": 339, "seek": 142622, "start": 1426.22, "end": 1430.26, "text": " It is oftentimes good practice if you're only ever working with one doc object in your", "tokens": [50364, 467, 307, 18349, 665, 3124, 498, 291, 434, 787, 1562, 1364, 365, 472, 3211, 2657, 294, 428, 50566], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 340, "seek": 142622, "start": 1430.26, "end": 1434.54, "text": " script to just call your only object doc.", "tokens": [50566, 5755, 281, 445, 818, 428, 787, 2657, 3211, 13, 50780], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 341, "seek": 142622, "start": 1434.54, "end": 1439.6200000000001, "text": " If you're working with multiple objects, sometimes you'll say doc one doc two doc three", "tokens": [50780, 759, 291, 434, 1364, 365, 3866, 6565, 11, 2171, 291, 603, 584, 3211, 472, 3211, 732, 3211, 1045, 51034], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 342, "seek": 142622, "start": 1439.6200000000001, "end": 1445.1000000000001, "text": " or give it some kind of specific name so that your variables can be unique and easily identifiable", "tokens": [51034, 420, 976, 309, 512, 733, 295, 2685, 1315, 370, 300, 428, 9102, 393, 312, 3845, 293, 3612, 2473, 30876, 51308], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 343, "seek": 142622, "start": 1445.1000000000001, "end": 1446.6000000000001, "text": " later in your script.", "tokens": [51308, 1780, 294, 428, 5755, 13, 51383], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 344, "seek": 142622, "start": 1446.6000000000001, "end": 1450.82, "text": " Since we're just working with one doc object right now, we're going to say doc is equal", "tokens": [51383, 4162, 321, 434, 445, 1364, 365, 472, 3211, 2657, 558, 586, 11, 321, 434, 516, 281, 584, 3211, 307, 2681, 51594], "temperature": 0.0, "avg_logprob": -0.11781929988487094, "compression_ratio": 1.8240343347639485, "no_speech_prob": 0.016402361914515495}, {"id": 345, "seek": 145082, "start": 1450.82, "end": 1452.8999999999999, "text": " to NLP.", "tokens": [50364, 281, 426, 45196, 13, 50468], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 346, "seek": 145082, "start": 1452.8999999999999, "end": 1458.6599999999999, "text": " So this is going to call our NLP model that we imported earlier in this case the English", "tokens": [50468, 407, 341, 307, 516, 281, 818, 527, 426, 45196, 2316, 300, 321, 25524, 3071, 294, 341, 1389, 264, 3669, 50756], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 347, "seek": 145082, "start": 1458.6599999999999, "end": 1461.46, "text": " Core Web SM model.", "tokens": [50756, 14798, 9573, 13115, 2316, 13, 50896], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 348, "seek": 145082, "start": 1461.46, "end": 1466.1, "text": " And that's going to for right now just take one argument and that's going to be the text", "tokens": [50896, 400, 300, 311, 516, 281, 337, 558, 586, 445, 747, 472, 6770, 293, 300, 311, 516, 281, 312, 264, 2487, 51128], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 349, "seek": 145082, "start": 1466.1, "end": 1467.1, "text": " itself.", "tokens": [51128, 2564, 13, 51178], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 350, "seek": 145082, "start": 1467.1, "end": 1473.96, "text": " So the text object, if you execute that cell, you should have a doc object now created.", "tokens": [51178, 407, 264, 2487, 2657, 11, 498, 291, 14483, 300, 2815, 11, 291, 820, 362, 257, 3211, 2657, 586, 2942, 13, 51521], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 351, "seek": 145082, "start": 1473.96, "end": 1478.0, "text": " Let's print off that doc object and see what it looks like.", "tokens": [51521, 961, 311, 4482, 766, 300, 3211, 2657, 293, 536, 437, 309, 1542, 411, 13, 51723], "temperature": 0.0, "avg_logprob": -0.16212392807006837, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.05032564327120781}, {"id": 352, "seek": 147800, "start": 1478.0, "end": 1484.0, "text": " And if you scroll down, you might be thinking to yourself, this looks very, very similar", "tokens": [50364, 400, 498, 291, 11369, 760, 11, 291, 1062, 312, 1953, 281, 1803, 11, 341, 1542, 588, 11, 588, 2531, 50664], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 353, "seek": 147800, "start": 1484.0, "end": 1487.76, "text": " if not identical to what I just saw a second ago.", "tokens": [50664, 498, 406, 14800, 281, 437, 286, 445, 1866, 257, 1150, 2057, 13, 50852], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 354, "seek": 147800, "start": 1487.76, "end": 1493.76, "text": " And in fact, on the surface, it is very similar to that text object that we gave to the NLP", "tokens": [50852, 400, 294, 1186, 11, 322, 264, 3753, 11, 309, 307, 588, 2531, 281, 300, 2487, 2657, 300, 321, 2729, 281, 264, 426, 45196, 51152], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 355, "seek": 147800, "start": 1493.76, "end": 1497.4, "text": " model or pipeline, but let's see how they're different.", "tokens": [51152, 2316, 420, 15517, 11, 457, 718, 311, 536, 577, 436, 434, 819, 13, 51334], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 356, "seek": 147800, "start": 1497.4, "end": 1501.04, "text": " Let's print off the length of text.", "tokens": [51334, 961, 311, 4482, 766, 264, 4641, 295, 2487, 13, 51516], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 357, "seek": 147800, "start": 1501.04, "end": 1505.52, "text": " And let's print off the length of the doc object.", "tokens": [51516, 400, 718, 311, 4482, 766, 264, 4641, 295, 264, 3211, 2657, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12099997202555339, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00884692557156086}, {"id": 358, "seek": 150552, "start": 1505.52, "end": 1508.16, "text": " And what we have here are two different numbers.", "tokens": [50364, 400, 437, 321, 362, 510, 366, 732, 819, 3547, 13, 50496], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 359, "seek": 150552, "start": 1508.16, "end": 1515.24, "text": " Our text is 3525 and our doc object is 152.", "tokens": [50496, 2621, 2487, 307, 6976, 6074, 293, 527, 3211, 2657, 307, 2119, 17, 13, 50850], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 360, "seek": 150552, "start": 1515.24, "end": 1516.6, "text": " What is going on here?", "tokens": [50850, 708, 307, 516, 322, 510, 30, 50918], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 361, "seek": 150552, "start": 1516.6, "end": 1522.44, "text": " Well, let's get a sense by trying to iterate over the text object and iterating over the", "tokens": [50918, 1042, 11, 718, 311, 483, 257, 2020, 538, 1382, 281, 44497, 670, 264, 2487, 2657, 293, 17138, 990, 670, 264, 51210], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 362, "seek": 150552, "start": 1522.44, "end": 1524.92, "text": " doc object with a simple for loop.", "tokens": [51210, 3211, 2657, 365, 257, 2199, 337, 6367, 13, 51334], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 363, "seek": 150552, "start": 1524.92, "end": 1528.8, "text": " So we're going to say for token and text, so we're going to iterate first over that", "tokens": [51334, 407, 321, 434, 516, 281, 584, 337, 14862, 293, 2487, 11, 370, 321, 434, 516, 281, 44497, 700, 670, 300, 51528], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 364, "seek": 150552, "start": 1528.8, "end": 1532.6399999999999, "text": " text object, we're going to print off the token.", "tokens": [51528, 2487, 2657, 11, 321, 434, 516, 281, 4482, 766, 264, 14862, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13612786247616723, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.007576638367027044}, {"id": 365, "seek": 153264, "start": 1532.64, "end": 1537.2800000000002, "text": " So the first 10 indices, and we get individual letters as one might expect.", "tokens": [50364, 407, 264, 700, 1266, 43840, 11, 293, 321, 483, 2609, 7825, 382, 472, 1062, 2066, 13, 50596], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 366, "seek": 153264, "start": 1537.2800000000002, "end": 1541.48, "text": " But when we do something the same thing with the doc object, let's go ahead and start writing", "tokens": [50596, 583, 562, 321, 360, 746, 264, 912, 551, 365, 264, 3211, 2657, 11, 718, 311, 352, 2286, 293, 722, 3579, 50806], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 367, "seek": 153264, "start": 1541.48, "end": 1542.48, "text": " this out.", "tokens": [50806, 341, 484, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 368, "seek": 153264, "start": 1542.48, "end": 1549.0, "text": " We're going to say for token and doc, we're going to iterate over the first 10.", "tokens": [50856, 492, 434, 516, 281, 584, 337, 14862, 293, 3211, 11, 321, 434, 516, 281, 44497, 670, 264, 700, 1266, 13, 51182], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 369, "seek": 153264, "start": 1549.0, "end": 1551.1200000000001, "text": " We're going to print off the token.", "tokens": [51182, 492, 434, 516, 281, 4482, 766, 264, 14862, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 370, "seek": 153264, "start": 1551.1200000000001, "end": 1553.1200000000001, "text": " We see something very different.", "tokens": [51288, 492, 536, 746, 588, 819, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 371, "seek": 153264, "start": 1553.1200000000001, "end": 1556.16, "text": " What we see here are tokens.", "tokens": [51388, 708, 321, 536, 510, 366, 22667, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 372, "seek": 153264, "start": 1556.16, "end": 1561.88, "text": " This is why the doc object is so much more valuable and this is why the doc object has", "tokens": [51540, 639, 307, 983, 264, 3211, 2657, 307, 370, 709, 544, 8263, 293, 341, 307, 983, 264, 3211, 2657, 575, 51826], "temperature": 0.0, "avg_logprob": -0.1050547583628509, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.03409726172685623}, {"id": 373, "seek": 156188, "start": 1561.88, "end": 1564.6000000000001, "text": " a different length than the text object.", "tokens": [50364, 257, 819, 4641, 813, 264, 2487, 2657, 13, 50500], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 374, "seek": 156188, "start": 1564.6000000000001, "end": 1569.5600000000002, "text": " The text object is just basically counting up every instance of a character, a white", "tokens": [50500, 440, 2487, 2657, 307, 445, 1936, 13251, 493, 633, 5197, 295, 257, 2517, 11, 257, 2418, 50748], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 375, "seek": 156188, "start": 1569.5600000000002, "end": 1571.7, "text": " space, a punctuation, etc.", "tokens": [50748, 1901, 11, 257, 27006, 16073, 11, 5183, 13, 50855], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 376, "seek": 156188, "start": 1571.7, "end": 1578.2800000000002, "text": " The doc object is counting individual tokens, so any word, any punctuation, etc.", "tokens": [50855, 440, 3211, 2657, 307, 13251, 2609, 22667, 11, 370, 604, 1349, 11, 604, 27006, 16073, 11, 5183, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 377, "seek": 156188, "start": 1578.2800000000002, "end": 1581.16, "text": " That's why they're of different length and that's why when we print them off, we see", "tokens": [51184, 663, 311, 983, 436, 434, 295, 819, 4641, 293, 300, 311, 983, 562, 321, 4482, 552, 766, 11, 321, 536, 51328], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 378, "seek": 156188, "start": 1581.16, "end": 1582.16, "text": " something different.", "tokens": [51328, 746, 819, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 379, "seek": 156188, "start": 1582.16, "end": 1585.92, "text": " So you might now already be seeing the power of spacey.", "tokens": [51378, 407, 291, 1062, 586, 1217, 312, 2577, 264, 1347, 295, 1901, 88, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 380, "seek": 156188, "start": 1585.92, "end": 1590.0400000000002, "text": " It allows for you to easily on the surface with nothing else being done, easily split", "tokens": [51566, 467, 4045, 337, 291, 281, 3612, 322, 264, 3753, 365, 1825, 1646, 885, 1096, 11, 3612, 7472, 51772], "temperature": 0.0, "avg_logprob": -0.1312157339300991, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.04601941630244255}, {"id": 381, "seek": 159004, "start": 1590.04, "end": 1593.72, "text": " up your text into individual tokens without any effort at all.", "tokens": [50364, 493, 428, 2487, 666, 2609, 22667, 1553, 604, 4630, 412, 439, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 382, "seek": 159004, "start": 1593.72, "end": 1598.48, "text": " Now, those of you familiar with Python and different string methods might be thinking", "tokens": [50548, 823, 11, 729, 295, 291, 4963, 365, 15329, 293, 819, 6798, 7150, 1062, 312, 1953, 50786], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 383, "seek": 159004, "start": 1598.48, "end": 1600.84, "text": " to yourself, but I've got the split method.", "tokens": [50786, 281, 1803, 11, 457, 286, 600, 658, 264, 7472, 3170, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 384, "seek": 159004, "start": 1600.84, "end": 1603.56, "text": " I can just use this to split up the text.", "tokens": [50904, 286, 393, 445, 764, 341, 281, 7472, 493, 264, 2487, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 385, "seek": 159004, "start": 1603.56, "end": 1605.96, "text": " I don't need anything fancy from spacey.", "tokens": [51040, 286, 500, 380, 643, 1340, 10247, 490, 1901, 88, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 386, "seek": 159004, "start": 1605.96, "end": 1607.56, "text": " Well, you'd be wrong.", "tokens": [51160, 1042, 11, 291, 1116, 312, 2085, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 387, "seek": 159004, "start": 1607.56, "end": 1609.32, "text": " Let me demonstrate this right now.", "tokens": [51240, 961, 385, 11698, 341, 558, 586, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 388, "seek": 159004, "start": 1609.32, "end": 1616.8799999999999, "text": " So if I were to say for token and text.split, so I'm splitting up that text into individual", "tokens": [51328, 407, 498, 286, 645, 281, 584, 337, 14862, 293, 2487, 13, 46535, 270, 11, 370, 286, 478, 30348, 493, 300, 2487, 666, 2609, 51706], "temperature": 0.0, "avg_logprob": -0.1933005316215649, "compression_ratio": 1.649805447470817, "no_speech_prob": 0.6144274473190308}, {"id": 389, "seek": 161688, "start": 1616.88, "end": 1621.72, "text": " and theory individual words, essentially, it's just a split method where it's splitting", "tokens": [50364, 293, 5261, 2609, 2283, 11, 4476, 11, 309, 311, 445, 257, 7472, 3170, 689, 309, 311, 30348, 50606], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 390, "seek": 161688, "start": 1621.72, "end": 1623.3600000000001, "text": " by individual white spaces.", "tokens": [50606, 538, 2609, 2418, 7673, 13, 50688], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 391, "seek": 161688, "start": 1623.3600000000001, "end": 1628.16, "text": " If I were to do that and iterate over the first 10 again.", "tokens": [50688, 759, 286, 645, 281, 360, 300, 293, 44497, 670, 264, 700, 1266, 797, 13, 50928], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 392, "seek": 161688, "start": 1628.16, "end": 1633.5600000000002, "text": " And I would just say print token, it looks good until you get down here.", "tokens": [50928, 400, 286, 576, 445, 584, 4482, 14862, 11, 309, 1542, 665, 1826, 291, 483, 760, 510, 13, 51198], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 393, "seek": 161688, "start": 1633.5600000000002, "end": 1638.16, "text": " So until you get to USA, well, why is it a problem?", "tokens": [51198, 407, 1826, 291, 483, 281, 10827, 11, 731, 11, 983, 307, 309, 257, 1154, 30, 51428], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 394, "seek": 161688, "start": 1638.16, "end": 1640.5600000000002, "text": " The problem is quite simple.", "tokens": [51428, 440, 1154, 307, 1596, 2199, 13, 51548], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 395, "seek": 161688, "start": 1640.5600000000002, "end": 1643.7600000000002, "text": " There is a parentheses mark right here.", "tokens": [51548, 821, 307, 257, 34153, 1491, 558, 510, 13, 51708], "temperature": 0.0, "avg_logprob": -0.17856557028634207, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1870379000902176}, {"id": 396, "seek": 164376, "start": 1643.76, "end": 1650.12, "text": " And this is where we have a huge advantage with spacey.", "tokens": [50364, 400, 341, 307, 689, 321, 362, 257, 2603, 5002, 365, 1901, 88, 13, 50682], "temperature": 0.0, "avg_logprob": -0.16780204551164493, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.07583768665790558}, {"id": 397, "seek": 164376, "start": 1650.12, "end": 1655.92, "text": " Spacey automatically separates out these these kind of punctuation marks and removes them", "tokens": [50682, 8705, 88, 6772, 34149, 484, 613, 613, 733, 295, 27006, 16073, 10640, 293, 30445, 552, 50972], "temperature": 0.0, "avg_logprob": -0.16780204551164493, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.07583768665790558}, {"id": 398, "seek": 164376, "start": 1655.92, "end": 1660.2, "text": " from individual tokens when they're not relevant to the token itself.", "tokens": [50972, 490, 2609, 22667, 562, 436, 434, 406, 7340, 281, 264, 14862, 2564, 13, 51186], "temperature": 0.0, "avg_logprob": -0.16780204551164493, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.07583768665790558}, {"id": 399, "seek": 164376, "start": 1660.2, "end": 1665.16, "text": " Notice that USA has got a period within the middle of it.", "tokens": [51186, 13428, 300, 10827, 575, 658, 257, 2896, 1951, 264, 2808, 295, 309, 13, 51434], "temperature": 0.0, "avg_logprob": -0.16780204551164493, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.07583768665790558}, {"id": 400, "seek": 164376, "start": 1665.16, "end": 1669.52, "text": " It's not looking at that and thinking that that is some kind of unique token, a you a", "tokens": [51434, 467, 311, 406, 1237, 412, 300, 293, 1953, 300, 300, 307, 512, 733, 295, 3845, 14862, 11, 257, 291, 257, 51652], "temperature": 0.0, "avg_logprob": -0.16780204551164493, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.07583768665790558}, {"id": 401, "seek": 166952, "start": 1669.52, "end": 1673.2, "text": " period and s a period and an a in a period.", "tokens": [50364, 2896, 293, 262, 257, 2896, 293, 364, 257, 294, 257, 2896, 13, 50548], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 402, "seek": 166952, "start": 1673.2, "end": 1677.4, "text": " It's not seeing these as four individual tokens rather it's automatically identifying them", "tokens": [50548, 467, 311, 406, 2577, 613, 382, 1451, 2609, 22667, 2831, 309, 311, 6772, 16696, 552, 50758], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 403, "seek": 166952, "start": 1677.4, "end": 1684.6399999999999, "text": " as one thing one tied together single token that's a string of characters and punctuation.", "tokens": [50758, 382, 472, 551, 472, 9601, 1214, 2167, 14862, 300, 311, 257, 6798, 295, 4342, 293, 27006, 16073, 13, 51120], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 404, "seek": 166952, "start": 1684.6399999999999, "end": 1689.6399999999999, "text": " This is where the power of spacey really lies just on the surface level and go ahead spend", "tokens": [51120, 639, 307, 689, 264, 1347, 295, 1901, 88, 534, 9134, 445, 322, 264, 3753, 1496, 293, 352, 2286, 3496, 51370], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 405, "seek": 166952, "start": 1689.6399999999999, "end": 1691.76, "text": " a few minutes and play around with this.", "tokens": [51370, 257, 1326, 2077, 293, 862, 926, 365, 341, 13, 51476], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 406, "seek": 166952, "start": 1691.76, "end": 1696.36, "text": " And then we're going to kind of jump back here and start talking about how the doc object", "tokens": [51476, 400, 550, 321, 434, 516, 281, 733, 295, 3012, 646, 510, 293, 722, 1417, 466, 577, 264, 3211, 2657, 51706], "temperature": 0.0, "avg_logprob": -0.16951559174735592, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.1402997076511383}, {"id": 407, "seek": 169636, "start": 1696.36, "end": 1699.7199999999998, "text": " has a lot more than just tokens within it.", "tokens": [50364, 575, 257, 688, 544, 813, 445, 22667, 1951, 309, 13, 50532], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 408, "seek": 169636, "start": 1699.7199999999998, "end": 1702.6, "text": " It's got sentences each token has attributes.", "tokens": [50532, 467, 311, 658, 16579, 1184, 14862, 575, 17212, 13, 50676], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 409, "seek": 169636, "start": 1702.6, "end": 1710.4799999999998, "text": " We're going to start exploring these when you pop back.", "tokens": [50676, 492, 434, 516, 281, 722, 12736, 613, 562, 291, 1665, 646, 13, 51070], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 410, "seek": 169636, "start": 1710.4799999999998, "end": 1714.56, "text": " If you're following along with the textbook, we're now going to be moving on to the next", "tokens": [51070, 759, 291, 434, 3480, 2051, 365, 264, 25591, 11, 321, 434, 586, 516, 281, 312, 2684, 322, 281, 264, 958, 51274], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 411, "seek": 169636, "start": 1714.56, "end": 1717.8799999999999, "text": " section, which is sentence boundary detection.", "tokens": [51274, 3541, 11, 597, 307, 8174, 12866, 17784, 13, 51440], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 412, "seek": 169636, "start": 1717.8799999999999, "end": 1725.36, "text": " An NLP sentence boundary detection is the identification of sentences within a text on the surface.", "tokens": [51440, 1107, 426, 45196, 8174, 12866, 17784, 307, 264, 22065, 295, 16579, 1951, 257, 2487, 322, 264, 3753, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15978189615102914, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.02930579148232937}, {"id": 413, "seek": 172536, "start": 1725.36, "end": 1726.6, "text": " This might look simple.", "tokens": [50364, 639, 1062, 574, 2199, 13, 50426], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 414, "seek": 172536, "start": 1726.6, "end": 1731.52, "text": " You might be thinking to yourself, I could simply use the split function and split up", "tokens": [50426, 509, 1062, 312, 1953, 281, 1803, 11, 286, 727, 2935, 764, 264, 7472, 2445, 293, 7472, 493, 50672], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 415, "seek": 172536, "start": 1731.52, "end": 1733.8, "text": " a text with a simple period.", "tokens": [50672, 257, 2487, 365, 257, 2199, 2896, 13, 50786], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 416, "seek": 172536, "start": 1733.8, "end": 1735.9199999999998, "text": " And that's going to give me all my sentences.", "tokens": [50786, 400, 300, 311, 516, 281, 976, 385, 439, 452, 16579, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 417, "seek": 172536, "start": 1735.9199999999998, "end": 1740.1999999999998, "text": " Those of you who have tried to do this might already be shaking your heads and saying no.", "tokens": [50892, 3950, 295, 291, 567, 362, 3031, 281, 360, 341, 1062, 1217, 312, 15415, 428, 8050, 293, 1566, 572, 13, 51106], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 418, "seek": 172536, "start": 1740.1999999999998, "end": 1744.56, "text": " If you do think about it, there's a really easy explanation for why this doesn't work.", "tokens": [51106, 759, 291, 360, 519, 466, 309, 11, 456, 311, 257, 534, 1858, 10835, 337, 983, 341, 1177, 380, 589, 13, 51324], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 419, "seek": 172536, "start": 1744.56, "end": 1750.56, "text": " Were you to try to split up a text by period and make a presumption that anything that occurs", "tokens": [51324, 12448, 291, 281, 853, 281, 7472, 493, 257, 2487, 538, 2896, 293, 652, 257, 18028, 1695, 300, 1340, 300, 11843, 51624], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 420, "seek": 172536, "start": 1750.56, "end": 1754.9199999999998, "text": " with between periods is going to be an individual sentence, you would have a serious mistake", "tokens": [51624, 365, 1296, 13804, 307, 516, 281, 312, 364, 2609, 8174, 11, 291, 576, 362, 257, 3156, 6146, 51842], "temperature": 0.0, "avg_logprob": -0.10248857469701056, "compression_ratio": 1.7620578778135048, "no_speech_prob": 0.49180883169174194}, {"id": 421, "seek": 175492, "start": 1754.96, "end": 1760.5600000000002, "text": " when you get to things like USA, especially in Western languages, where the punctuation", "tokens": [50366, 562, 291, 483, 281, 721, 411, 10827, 11, 2318, 294, 8724, 8650, 11, 689, 264, 27006, 16073, 50646], "temperature": 0.0, "avg_logprob": -0.14278479545347153, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.25645187497138977}, {"id": 422, "seek": 175492, "start": 1760.5600000000002, "end": 1766.8400000000001, "text": " of a period mark is used not only to delineate the change of its sentence, rather it's used", "tokens": [50646, 295, 257, 2896, 1491, 307, 1143, 406, 787, 281, 1103, 533, 473, 264, 1319, 295, 1080, 8174, 11, 2831, 309, 311, 1143, 50960], "temperature": 0.0, "avg_logprob": -0.14278479545347153, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.25645187497138977}, {"id": 423, "seek": 175492, "start": 1766.8400000000001, "end": 1769.92, "text": " to also delineate abbreviations.", "tokens": [50960, 281, 611, 1103, 533, 473, 35839, 763, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14278479545347153, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.25645187497138977}, {"id": 424, "seek": 175492, "start": 1769.92, "end": 1776.0, "text": " So United States of America, each period represents an abbreviated word.", "tokens": [51114, 407, 2824, 3040, 295, 3374, 11, 1184, 2896, 8855, 364, 35839, 770, 1349, 13, 51418], "temperature": 0.0, "avg_logprob": -0.14278479545347153, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.25645187497138977}, {"id": 425, "seek": 175492, "start": 1776.0, "end": 1780.88, "text": " So you could write in rules to kind of account for this, you could write in rules that could", "tokens": [51418, 407, 291, 727, 2464, 294, 4474, 281, 733, 295, 2696, 337, 341, 11, 291, 727, 2464, 294, 4474, 300, 727, 51662], "temperature": 0.0, "avg_logprob": -0.14278479545347153, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.25645187497138977}, {"id": 426, "seek": 178088, "start": 1780.88, "end": 1785.24, "text": " also include in other ways that sentences are created, such as question marks, such", "tokens": [50364, 611, 4090, 294, 661, 2098, 300, 16579, 366, 2942, 11, 1270, 382, 1168, 10640, 11, 1270, 50582], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 427, "seek": 178088, "start": 1785.24, "end": 1786.92, "text": " as exclamation marks.", "tokens": [50582, 382, 1624, 43233, 10640, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 428, "seek": 178088, "start": 1786.92, "end": 1788.2, "text": " But why do that?", "tokens": [50666, 583, 983, 360, 300, 30, 50730], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 429, "seek": 178088, "start": 1788.2, "end": 1790.2, "text": " That's a lot of effort.", "tokens": [50730, 663, 311, 257, 688, 295, 4630, 13, 50830], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 430, "seek": 178088, "start": 1790.2, "end": 1796.8200000000002, "text": " When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly", "tokens": [50830, 1133, 264, 3211, 2657, 294, 1901, 88, 775, 341, 337, 291, 11, 293, 718, 311, 352, 2286, 293, 11698, 2293, 51161], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 431, "seek": 178088, "start": 1796.8200000000002, "end": 1798.3200000000002, "text": " how that works.", "tokens": [51161, 577, 300, 1985, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 432, "seek": 178088, "start": 1798.3200000000002, "end": 1806.8000000000002, "text": " So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing", "tokens": [51236, 407, 718, 311, 352, 2286, 293, 584, 337, 19040, 293, 3211, 13, 82, 1288, 11, 3449, 300, 321, 434, 1566, 3211, 13, 82, 1288, 420, 23771, 51660], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 433, "seek": 178088, "start": 1806.8000000000002, "end": 1809.88, "text": " the sentence attribute of the doc object.", "tokens": [51660, 264, 8174, 19667, 295, 264, 3211, 2657, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1821979907674527, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.04602259024977684}, {"id": 434, "seek": 180988, "start": 1809.88, "end": 1813.0800000000002, "text": " Let's print off scent.", "tokens": [50364, 961, 311, 4482, 766, 19040, 13, 50524], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 435, "seek": 180988, "start": 1813.0800000000002, "end": 1817.5200000000002, "text": " And if you do that, you are now able to print off every individual sentence.", "tokens": [50524, 400, 498, 291, 360, 300, 11, 291, 366, 586, 1075, 281, 4482, 766, 633, 2609, 8174, 13, 50746], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 436, "seek": 180988, "start": 1817.5200000000002, "end": 1821.88, "text": " So the entire text has been tokenized at the sentence level.", "tokens": [50746, 407, 264, 2302, 2487, 575, 668, 14862, 1602, 412, 264, 8174, 1496, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 437, "seek": 180988, "start": 1821.88, "end": 1827.7600000000002, "text": " In other words, spacey has used its sentence boundary detection and done all that for you", "tokens": [50964, 682, 661, 2283, 11, 1901, 88, 575, 1143, 1080, 8174, 12866, 17784, 293, 1096, 439, 300, 337, 291, 51258], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 438, "seek": 180988, "start": 1827.7600000000002, "end": 1829.92, "text": " and giving you all the sentences.", "tokens": [51258, 293, 2902, 291, 439, 264, 16579, 13, 51366], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 439, "seek": 180988, "start": 1829.92, "end": 1833.6000000000001, "text": " If you work with different models of different sizes, you're going to notice that certain", "tokens": [51366, 759, 291, 589, 365, 819, 5245, 295, 819, 11602, 11, 291, 434, 516, 281, 3449, 300, 1629, 51550], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 440, "seek": 180988, "start": 1833.6000000000001, "end": 1837.8400000000001, "text": " models the larger they get tend to do better at sentence detection.", "tokens": [51550, 5245, 264, 4833, 436, 483, 3928, 281, 360, 1101, 412, 8174, 17784, 13, 51762], "temperature": 0.0, "avg_logprob": -0.13416899921738099, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.023683464154601097}, {"id": 441, "seek": 183784, "start": 1837.84, "end": 1841.3999999999999, "text": " And that's because machine learning models tend to do a little bit better than heuristic", "tokens": [50364, 400, 300, 311, 570, 3479, 2539, 5245, 3928, 281, 360, 257, 707, 857, 1101, 813, 415, 374, 3142, 50542], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 442, "seek": 183784, "start": 1841.3999999999999, "end": 1842.6, "text": " approaches.", "tokens": [50542, 11587, 13, 50602], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 443, "seek": 183784, "start": 1842.6, "end": 1847.9599999999998, "text": " The English core web SM model, while having some machine learning components in it, does", "tokens": [50602, 440, 3669, 4965, 3670, 13115, 2316, 11, 1339, 1419, 512, 3479, 2539, 6677, 294, 309, 11, 775, 50870], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 444, "seek": 183784, "start": 1847.9599999999998, "end": 1850.36, "text": " not save word vectors.", "tokens": [50870, 406, 3155, 1349, 18875, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 445, "seek": 183784, "start": 1850.36, "end": 1855.1999999999998, "text": " And so the larger you go with the models, typically the better you're going to have with regards", "tokens": [50990, 400, 370, 264, 4833, 291, 352, 365, 264, 5245, 11, 5850, 264, 1101, 291, 434, 516, 281, 362, 365, 14258, 51232], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 446, "seek": 183784, "start": 1855.1999999999998, "end": 1857.4399999999998, "text": " to sentence detection.", "tokens": [51232, 281, 8174, 17784, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 447, "seek": 183784, "start": 1857.4399999999998, "end": 1860.6799999999998, "text": " Let's go ahead and try to access one of these sentences.", "tokens": [51344, 961, 311, 352, 2286, 293, 853, 281, 2105, 472, 295, 613, 16579, 13, 51506], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 448, "seek": 183784, "start": 1860.6799999999998, "end": 1866.1999999999998, "text": " So let's create an object called sentence one, we're going to make that equal to doc.sense", "tokens": [51506, 407, 718, 311, 1884, 364, 2657, 1219, 8174, 472, 11, 321, 434, 516, 281, 652, 300, 2681, 281, 3211, 13, 82, 1288, 51782], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 449, "seek": 183784, "start": 1866.1999999999998, "end": 1867.1999999999998, "text": " zero.", "tokens": [51782, 4018, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1495585402822107, "compression_ratio": 1.6875, "no_speech_prob": 0.015423362143337727}, {"id": 450, "seek": 186720, "start": 1867.4, "end": 1874.52, "text": " We're going to try to grab that zero index and let's print off sentence one, we do this,", "tokens": [50374, 492, 434, 516, 281, 853, 281, 4444, 300, 4018, 8186, 293, 718, 311, 4482, 766, 8174, 472, 11, 321, 360, 341, 11, 50730], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 451, "seek": 186720, "start": 1874.52, "end": 1875.92, "text": " we get an error.", "tokens": [50730, 321, 483, 364, 6713, 13, 50800], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 452, "seek": 186720, "start": 1875.92, "end": 1877.6000000000001, "text": " Why have we gotten an error?", "tokens": [50800, 1545, 362, 321, 5768, 364, 6713, 30, 50884], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 453, "seek": 186720, "start": 1877.6000000000001, "end": 1880.1200000000001, "text": " Well, it tells you why right here, it's a type air.", "tokens": [50884, 1042, 11, 309, 5112, 291, 983, 558, 510, 11, 309, 311, 257, 2010, 1988, 13, 51010], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 454, "seek": 186720, "start": 1880.1200000000001, "end": 1886.72, "text": " And this means that this is not a type that can be kind of iterated over, it's not subscriptable.", "tokens": [51010, 400, 341, 1355, 300, 341, 307, 406, 257, 2010, 300, 393, 312, 733, 295, 17138, 770, 670, 11, 309, 311, 406, 2325, 662, 712, 13, 51340], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 455, "seek": 186720, "start": 1886.72, "end": 1888.88, "text": " And it's because it is a generator.", "tokens": [51340, 400, 309, 311, 570, 309, 307, 257, 19265, 13, 51448], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 456, "seek": 186720, "start": 1888.88, "end": 1892.96, "text": " Now in Python, if you're familiar with generators, you might be thinking to yourself, there's", "tokens": [51448, 823, 294, 15329, 11, 498, 291, 434, 4963, 365, 38662, 11, 291, 1062, 312, 1953, 281, 1803, 11, 456, 311, 51652], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 457, "seek": 186720, "start": 1892.96, "end": 1893.96, "text": " a solution for this.", "tokens": [51652, 257, 3827, 337, 341, 13, 51702], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 458, "seek": 186720, "start": 1893.96, "end": 1895.4, "text": " And in fact, there is.", "tokens": [51702, 400, 294, 1186, 11, 456, 307, 13, 51774], "temperature": 0.0, "avg_logprob": -0.18212500678168403, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.014061030000448227}, {"id": 459, "seek": 189540, "start": 1895.44, "end": 1900.0400000000002, "text": " If you want to work with generator objects, you need to convert them into a list.", "tokens": [50366, 759, 291, 528, 281, 589, 365, 19265, 6565, 11, 291, 643, 281, 7620, 552, 666, 257, 1329, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 460, "seek": 189540, "start": 1900.0400000000002, "end": 1903.3600000000001, "text": " So let's say sentence one is equal to list.", "tokens": [50596, 407, 718, 311, 584, 8174, 472, 307, 2681, 281, 1329, 13, 50762], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 461, "seek": 189540, "start": 1903.3600000000001, "end": 1908.68, "text": " So using the list function to convert doc.sense into a list.", "tokens": [50762, 407, 1228, 264, 1329, 2445, 281, 7620, 3211, 13, 82, 1288, 666, 257, 1329, 13, 51028], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 462, "seek": 189540, "start": 1908.68, "end": 1913.0800000000002, "text": " And then with outside of that, we're going to grab zero, the zero index, and then we're", "tokens": [51028, 400, 550, 365, 2380, 295, 300, 11, 321, 434, 516, 281, 4444, 4018, 11, 264, 4018, 8186, 11, 293, 550, 321, 434, 51248], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 463, "seek": 189540, "start": 1913.0800000000002, "end": 1915.96, "text": " going to print off sentence one.", "tokens": [51248, 516, 281, 4482, 766, 8174, 472, 13, 51392], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 464, "seek": 189540, "start": 1915.96, "end": 1920.3200000000002, "text": " And we grab the first sentence of that text.", "tokens": [51392, 400, 321, 4444, 264, 700, 8174, 295, 300, 2487, 13, 51610], "temperature": 0.0, "avg_logprob": -0.09622128476801607, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0019266651943325996}, {"id": 465, "seek": 192032, "start": 1920.3999999999999, "end": 1926.08, "text": " This as we go deeper and deeper in spacey one by one, you're going to see the immense", "tokens": [50368, 639, 382, 321, 352, 7731, 293, 7731, 294, 1901, 88, 472, 538, 472, 11, 291, 434, 516, 281, 536, 264, 22920, 50652], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 466, "seek": 192032, "start": 1926.08, "end": 1930.8799999999999, "text": " power that you can do with Pacea, all the immense incredible things you can use spacey", "tokens": [50652, 1347, 300, 291, 393, 360, 365, 430, 617, 64, 11, 439, 264, 22920, 4651, 721, 291, 393, 764, 1901, 88, 50892], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 467, "seek": 192032, "start": 1930.8799999999999, "end": 1934.24, "text": " for with very, very minimal code.", "tokens": [50892, 337, 365, 588, 11, 588, 13206, 3089, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 468, "seek": 192032, "start": 1934.24, "end": 1939.1599999999999, "text": " The doc object does a lot of things for you that would take hours to actually write out", "tokens": [51060, 440, 3211, 2657, 775, 257, 688, 295, 721, 337, 291, 300, 576, 747, 2496, 281, 767, 2464, 484, 51306], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 469, "seek": 192032, "start": 1939.1599999999999, "end": 1942.04, "text": " and code to do with heuristic approaches.", "tokens": [51306, 293, 3089, 281, 360, 365, 415, 374, 3142, 11587, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 470, "seek": 192032, "start": 1942.04, "end": 1947.28, "text": " This is now a great way to segment an entire text up by sentence.", "tokens": [51450, 639, 307, 586, 257, 869, 636, 281, 9469, 364, 2302, 2487, 493, 538, 8174, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1407861979502552, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0900598093867302}, {"id": 471, "seek": 194728, "start": 1947.28, "end": 1953.52, "text": " And if you work with text a lot, you will already know that this has a lot of applications.", "tokens": [50364, 400, 498, 291, 589, 365, 2487, 257, 688, 11, 291, 486, 1217, 458, 300, 341, 575, 257, 688, 295, 5821, 13, 50676], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 472, "seek": 194728, "start": 1953.52, "end": 1957.44, "text": " As we move forward, we're going to not just talk about sentences, we're also going to", "tokens": [50676, 1018, 321, 1286, 2128, 11, 321, 434, 516, 281, 406, 445, 751, 466, 16579, 11, 321, 434, 611, 516, 281, 50872], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 473, "seek": 194728, "start": 1957.44, "end": 1963.16, "text": " be talking about token attributes, because within the doc object are individual tokens.", "tokens": [50872, 312, 1417, 466, 14862, 17212, 11, 570, 1951, 264, 3211, 2657, 366, 2609, 22667, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 474, "seek": 194728, "start": 1963.16, "end": 1967.92, "text": " I encourage you to pause here and go ahead and play around with the doc.sense a little", "tokens": [51158, 286, 5373, 291, 281, 10465, 510, 293, 352, 2286, 293, 862, 926, 365, 264, 3211, 13, 82, 1288, 257, 707, 51396], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 475, "seek": 194728, "start": 1967.92, "end": 1972.68, "text": " bit and get familiar with how it works, what it contains, and try to convert it into a", "tokens": [51396, 857, 293, 483, 4963, 365, 577, 309, 1985, 11, 437, 309, 8306, 11, 293, 853, 281, 7620, 309, 666, 257, 51634], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 476, "seek": 194728, "start": 1972.68, "end": 1973.68, "text": " list.", "tokens": [51634, 1329, 13, 51684], "temperature": 0.0, "avg_logprob": -0.10578370094299316, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.0038242086302489042}, {"id": 477, "seek": 197368, "start": 1974.68, "end": 1978.92, "text": " And we'll continue talking about tokens.", "tokens": [50414, 400, 321, 603, 2354, 1417, 466, 22667, 13, 50626], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 478, "seek": 197368, "start": 1978.92, "end": 1982.8, "text": " This is where I really encourage you to spend a little bit of time with the textbook.", "tokens": [50626, 639, 307, 689, 286, 534, 5373, 291, 281, 3496, 257, 707, 857, 295, 565, 365, 264, 25591, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 479, "seek": 197368, "start": 1982.8, "end": 1988.6000000000001, "text": " Under token attributes in chapter two, I have all the different kind of major things that", "tokens": [50820, 6974, 14862, 17212, 294, 7187, 732, 11, 286, 362, 439, 264, 819, 733, 295, 2563, 721, 300, 51110], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 480, "seek": 197368, "start": 1988.6000000000001, "end": 1991.28, "text": " you're going to be using with regards to token attributes.", "tokens": [51110, 291, 434, 516, 281, 312, 1228, 365, 14258, 281, 14862, 17212, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 481, "seek": 197368, "start": 1991.28, "end": 1994.68, "text": " We're going to look and see how to access them in just a second.", "tokens": [51244, 492, 434, 516, 281, 574, 293, 536, 577, 281, 2105, 552, 294, 445, 257, 1150, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 482, "seek": 197368, "start": 1994.68, "end": 1998.64, "text": " I've provided for you kind of the most important ones that you should probably be familiar", "tokens": [51414, 286, 600, 5649, 337, 291, 733, 295, 264, 881, 1021, 2306, 300, 291, 820, 1391, 312, 4963, 51612], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 483, "seek": 197368, "start": 1998.64, "end": 1999.64, "text": " with.", "tokens": [51612, 365, 13, 51662], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 484, "seek": 197368, "start": 1999.64, "end": 2002.88, "text": " We're going to see this in code in just a second, and I'm going to explain with a little", "tokens": [51662, 492, 434, 516, 281, 536, 341, 294, 3089, 294, 445, 257, 1150, 11, 293, 286, 478, 516, 281, 2903, 365, 257, 707, 51824], "temperature": 0.0, "avg_logprob": -0.1574784205510066, "compression_ratio": 1.777027027027027, "no_speech_prob": 0.1440926194190979}, {"id": 485, "seek": 200288, "start": 2002.88, "end": 2007.4, "text": " bit more detail than what's in the spacey documentation about what these different things", "tokens": [50364, 857, 544, 2607, 813, 437, 311, 294, 264, 1901, 88, 14333, 466, 437, 613, 819, 721, 50590], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 486, "seek": 200288, "start": 2007.4, "end": 2010.3600000000001, "text": " are, why they're useful, and how they're used.", "tokens": [50590, 366, 11, 983, 436, 434, 4420, 11, 293, 577, 436, 434, 1143, 13, 50738], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 487, "seek": 200288, "start": 2010.3600000000001, "end": 2016.48, "text": " So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.", "tokens": [50738, 407, 718, 311, 352, 2286, 293, 3012, 646, 666, 527, 22125, 88, 391, 21060, 293, 722, 1417, 466, 14862, 17212, 13, 51044], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 488, "seek": 200288, "start": 2016.48, "end": 2020.1200000000001, "text": " If you remember, the doc object had a sequence of tokens.", "tokens": [51044, 759, 291, 1604, 11, 264, 3211, 2657, 632, 257, 8310, 295, 22667, 13, 51226], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 489, "seek": 200288, "start": 2020.1200000000001, "end": 2023.4, "text": " So for token and doc, you could print off token.", "tokens": [51226, 407, 337, 14862, 293, 3211, 11, 291, 727, 4482, 766, 14862, 13, 51390], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 490, "seek": 200288, "start": 2023.4, "end": 2026.48, "text": " And let's just do this with the first 10.", "tokens": [51390, 400, 718, 311, 445, 360, 341, 365, 264, 700, 1266, 13, 51544], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 491, "seek": 200288, "start": 2026.48, "end": 2029.44, "text": " And we've got each individual token.", "tokens": [51544, 400, 321, 600, 658, 1184, 2609, 14862, 13, 51692], "temperature": 0.0, "avg_logprob": -0.14759979248046876, "compression_ratio": 1.6293436293436294, "no_speech_prob": 0.009411824867129326}, {"id": 492, "seek": 202944, "start": 2029.44, "end": 2035.88, "text": " What you don't see here is that each individual token has a bunch of metadata buried within", "tokens": [50364, 708, 291, 500, 380, 536, 510, 307, 300, 1184, 2609, 14862, 575, 257, 3840, 295, 26603, 14101, 1951, 50686], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 493, "seek": 202944, "start": 2035.88, "end": 2036.88, "text": " it.", "tokens": [50686, 309, 13, 50736], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 494, "seek": 202944, "start": 2036.88, "end": 2042.8400000000001, "text": " These metadata are things that we call attributes or different things about that token that", "tokens": [50736, 1981, 26603, 366, 721, 300, 321, 818, 17212, 420, 819, 721, 466, 300, 14862, 300, 51034], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 495, "seek": 202944, "start": 2042.8400000000001, "end": 2047.16, "text": " you can access through the spacey framework.", "tokens": [51034, 291, 393, 2105, 807, 264, 1901, 88, 8388, 13, 51250], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 496, "seek": 202944, "start": 2047.16, "end": 2049.68, "text": " So let's go ahead and try to do that right now.", "tokens": [51250, 407, 718, 311, 352, 2286, 293, 853, 281, 360, 300, 558, 586, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 497, "seek": 202944, "start": 2049.68, "end": 2055.92, "text": " Let's just work with for right now token number two, which we're going to call sentence one,", "tokens": [51376, 961, 311, 445, 589, 365, 337, 558, 586, 14862, 1230, 732, 11, 597, 321, 434, 516, 281, 818, 8174, 472, 11, 51688], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 498, "seek": 202944, "start": 2055.92, "end": 2058.92, "text": " and we're going to grab from sentence one, the second index.", "tokens": [51688, 293, 321, 434, 516, 281, 4444, 490, 8174, 472, 11, 264, 1150, 8186, 13, 51838], "temperature": 0.0, "avg_logprob": -0.11820774608188206, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.017985304817557335}, {"id": 499, "seek": 205892, "start": 2059.44, "end": 2061.96, "text": " Let's print off that word.", "tokens": [50390, 961, 311, 4482, 766, 300, 1349, 13, 50516], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 500, "seek": 205892, "start": 2061.96, "end": 2063.36, "text": " And it should be states.", "tokens": [50516, 400, 309, 820, 312, 4368, 13, 50586], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 501, "seek": 205892, "start": 2063.36, "end": 2066.0, "text": " And in fact, it is fantastic.", "tokens": [50586, 400, 294, 1186, 11, 309, 307, 5456, 13, 50718], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 502, "seek": 205892, "start": 2066.0, "end": 2070.28, "text": " So now that we've got the word states accessed, we can start kind of going through and playing", "tokens": [50718, 407, 586, 300, 321, 600, 658, 264, 1349, 4368, 34211, 11, 321, 393, 722, 733, 295, 516, 807, 293, 2433, 50932], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 503, "seek": 205892, "start": 2070.28, "end": 2073.92, "text": " around with some of the attributes that that word actually has.", "tokens": [50932, 926, 365, 512, 295, 264, 17212, 300, 300, 1349, 767, 575, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 504, "seek": 205892, "start": 2073.92, "end": 2077.2400000000002, "text": " Now when you print it off, it looks like a regular piece of text, it looks like just", "tokens": [51114, 823, 562, 291, 4482, 309, 766, 11, 309, 1542, 411, 257, 3890, 2522, 295, 2487, 11, 309, 1542, 411, 445, 51280], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 505, "seek": 205892, "start": 2077.2400000000002, "end": 2082.4, "text": " a string, but it's got so much more buried within it now because it's been passed through", "tokens": [51280, 257, 6798, 11, 457, 309, 311, 658, 370, 709, 544, 14101, 1951, 309, 586, 570, 309, 311, 668, 4678, 807, 51538], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 506, "seek": 205892, "start": 2082.4, "end": 2086.44, "text": " our NLP model or pipeline from spacey.", "tokens": [51538, 527, 426, 45196, 2316, 420, 15517, 490, 1901, 88, 13, 51740], "temperature": 0.0, "avg_logprob": -0.11979541305668098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.03513653576374054}, {"id": 507, "seek": 208644, "start": 2086.48, "end": 2090.04, "text": " So let's go ahead and say token to dot text.", "tokens": [50366, 407, 718, 311, 352, 2286, 293, 584, 14862, 281, 5893, 2487, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 508, "seek": 208644, "start": 2090.04, "end": 2092.0, "text": " And I'm going to be saying token to dot text.", "tokens": [50544, 400, 286, 478, 516, 281, 312, 1566, 14862, 281, 5893, 2487, 13, 50642], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 509, "seek": 208644, "start": 2092.0, "end": 2096.44, "text": " If you're working within an IDE like Adam, you're going to need to say print token to", "tokens": [50642, 759, 291, 434, 1364, 1951, 364, 40930, 411, 7938, 11, 291, 434, 516, 281, 643, 281, 584, 4482, 14862, 281, 50864], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 510, "seek": 208644, "start": 2096.44, "end": 2097.44, "text": " dot text.", "tokens": [50864, 5893, 2487, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 511, "seek": 208644, "start": 2097.44, "end": 2102.6, "text": " When we do this, we see we get a string that just is states.", "tokens": [50914, 1133, 321, 360, 341, 11, 321, 536, 321, 483, 257, 6798, 300, 445, 307, 4368, 13, 51172], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 512, "seek": 208644, "start": 2102.6, "end": 2108.96, "text": " This is telling us that the dot text of the object, the pure text corresponds to the word", "tokens": [51172, 639, 307, 3585, 505, 300, 264, 5893, 2487, 295, 264, 2657, 11, 264, 6075, 2487, 23249, 281, 264, 1349, 51490], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 513, "seek": 208644, "start": 2108.96, "end": 2109.96, "text": " states.", "tokens": [51490, 4368, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 514, "seek": 208644, "start": 2109.96, "end": 2114.2000000000003, "text": " This is really important if you need to extract the text itself from the token and not work", "tokens": [51540, 639, 307, 534, 1021, 498, 291, 643, 281, 8947, 264, 2487, 2564, 490, 264, 14862, 293, 406, 589, 51752], "temperature": 0.0, "avg_logprob": -0.11587204933166503, "compression_ratio": 1.8208333333333333, "no_speech_prob": 0.002323050983250141}, {"id": 515, "seek": 211420, "start": 2114.24, "end": 2118.16, "text": " with the token object, which has behind it a whole bunch of different metadata that we're", "tokens": [50366, 365, 264, 14862, 2657, 11, 597, 575, 2261, 309, 257, 1379, 3840, 295, 819, 26603, 300, 321, 434, 50562], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 516, "seek": 211420, "start": 2118.16, "end": 2121.0, "text": " going to go through now and start accessing.", "tokens": [50562, 516, 281, 352, 807, 586, 293, 722, 26440, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 517, "seek": 211420, "start": 2121.0, "end": 2123.48, "text": " Let's use the token left edge.", "tokens": [50704, 961, 311, 764, 264, 14862, 1411, 4691, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 518, "seek": 211420, "start": 2123.48, "end": 2127.7599999999998, "text": " So we can say token to dot left underscore edge.", "tokens": [50828, 407, 321, 393, 584, 14862, 281, 5893, 1411, 37556, 4691, 13, 51042], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 519, "seek": 211420, "start": 2127.7599999999998, "end": 2128.7599999999998, "text": " And we can print that off.", "tokens": [51042, 400, 321, 393, 4482, 300, 766, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 520, "seek": 211420, "start": 2128.7599999999998, "end": 2130.4399999999996, "text": " Well, what's that telling us?", "tokens": [51092, 1042, 11, 437, 311, 300, 3585, 505, 30, 51176], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 521, "seek": 211420, "start": 2130.4399999999996, "end": 2135.64, "text": " It's telling us that this is part of a multi word token or a token that is multiple has", "tokens": [51176, 467, 311, 3585, 505, 300, 341, 307, 644, 295, 257, 4825, 1349, 14862, 420, 257, 14862, 300, 307, 3866, 575, 51436], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 522, "seek": 211420, "start": 2135.64, "end": 2139.72, "text": " multiple components to make up a larger span.", "tokens": [51436, 3866, 6677, 281, 652, 493, 257, 4833, 16174, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 523, "seek": 211420, "start": 2139.72, "end": 2143.7999999999997, "text": " And that this is the leftmost token that corresponds to it.", "tokens": [51640, 400, 300, 341, 307, 264, 1411, 1761, 14862, 300, 23249, 281, 309, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1208009487245141, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.007815469987690449}, {"id": 524, "seek": 214380, "start": 2143.8, "end": 2148.48, "text": " So this is going to be the word the as in the United States.", "tokens": [50364, 407, 341, 307, 516, 281, 312, 264, 1349, 264, 382, 294, 264, 2824, 3040, 13, 50598], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 525, "seek": 214380, "start": 2148.48, "end": 2151.36, "text": " Let's take a look at the right edge.", "tokens": [50598, 961, 311, 747, 257, 574, 412, 264, 558, 4691, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 526, "seek": 214380, "start": 2151.36, "end": 2157.0800000000004, "text": " We can say token to dot right underscore edge, print that off, and we get the word America.", "tokens": [50742, 492, 393, 584, 14862, 281, 5893, 558, 37556, 4691, 11, 4482, 300, 766, 11, 293, 321, 483, 264, 1349, 3374, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 527, "seek": 214380, "start": 2157.0800000000004, "end": 2162.7200000000003, "text": " So we're able to see where this token fits within a larger span in this case a noun chunk,", "tokens": [51028, 407, 321, 434, 1075, 281, 536, 689, 341, 14862, 9001, 1951, 257, 4833, 16174, 294, 341, 1389, 257, 23307, 16635, 11, 51310], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 528, "seek": 214380, "start": 2162.7200000000003, "end": 2164.76, "text": " which we're going to explore in just a few minutes.", "tokens": [51310, 597, 321, 434, 516, 281, 6839, 294, 445, 257, 1326, 2077, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 529, "seek": 214380, "start": 2164.76, "end": 2168.6000000000004, "text": " But we also learn a lot about it, kind of the different components, so we know where", "tokens": [51412, 583, 321, 611, 1466, 257, 688, 466, 309, 11, 733, 295, 264, 819, 6677, 11, 370, 321, 458, 689, 51604], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 530, "seek": 214380, "start": 2168.6000000000004, "end": 2172.28, "text": " to grab it from the beginning and from the very end.", "tokens": [51604, 281, 4444, 309, 490, 264, 2863, 293, 490, 264, 588, 917, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1475109372820173, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0008295620791614056}, {"id": 531, "seek": 217228, "start": 2172.28, "end": 2175.7200000000003, "text": " So that's how the left edge and the right edge work.", "tokens": [50364, 407, 300, 311, 577, 264, 1411, 4691, 293, 264, 558, 4691, 589, 13, 50536], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 532, "seek": 217228, "start": 2175.7200000000003, "end": 2181.1200000000003, "text": " We also have within this token to dot int type.", "tokens": [50536, 492, 611, 362, 1951, 341, 14862, 281, 5893, 560, 2010, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 533, "seek": 217228, "start": 2181.1200000000003, "end": 2183.36, "text": " This is going to be the type of entity.", "tokens": [50806, 639, 307, 516, 281, 312, 264, 2010, 295, 13977, 13, 50918], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 534, "seek": 217228, "start": 2183.36, "end": 2185.9, "text": " Now what you're seeing here is a integer.", "tokens": [50918, 823, 437, 291, 434, 2577, 510, 307, 257, 24922, 13, 51045], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 535, "seek": 217228, "start": 2185.9, "end": 2187.82, "text": " So this is 384.", "tokens": [51045, 407, 341, 307, 12843, 19, 13, 51141], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 536, "seek": 217228, "start": 2187.82, "end": 2193.1600000000003, "text": " In order to actually know what 384 means, I encourage you to not really use that so much", "tokens": [51141, 682, 1668, 281, 767, 458, 437, 12843, 19, 1355, 11, 286, 5373, 291, 281, 406, 534, 764, 300, 370, 709, 51408], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 537, "seek": 217228, "start": 2193.1600000000003, "end": 2197.28, "text": " as and type with an underscore after it.", "tokens": [51408, 382, 293, 2010, 365, 364, 37556, 934, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 538, "seek": 217228, "start": 2197.28, "end": 2201.96, "text": " This is going to give you the string corresponding to number 384.", "tokens": [51614, 639, 307, 516, 281, 976, 291, 264, 6798, 11760, 281, 1230, 12843, 19, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1263534958298142, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.02297506108880043}, {"id": 539, "seek": 220196, "start": 2201.96, "end": 2207.28, "text": " In this case, it is GPE or geopolitical entity.", "tokens": [50364, 682, 341, 1389, 11, 309, 307, 460, 5208, 420, 46615, 804, 13977, 13, 50630], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 540, "seek": 220196, "start": 2207.28, "end": 2210.64, "text": " We're going to be working with named entity a little bit in this video, but I have a whole", "tokens": [50630, 492, 434, 516, 281, 312, 1364, 365, 4926, 13977, 257, 707, 857, 294, 341, 960, 11, 457, 286, 362, 257, 1379, 50798], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 541, "seek": 220196, "start": 2210.64, "end": 2213.56, "text": " other book on named entity recognition.", "tokens": [50798, 661, 1446, 322, 4926, 13977, 11150, 13, 50944], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 542, "seek": 220196, "start": 2213.56, "end": 2219.36, "text": " It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning", "tokens": [50944, 467, 311, 412, 426, 1598, 5893, 38797, 18796, 1088, 13, 1112, 11, 294, 597, 286, 6839, 439, 295, 426, 1598, 11, 1293, 3479, 2539, 51234], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 543, "seek": 220196, "start": 2219.36, "end": 2222.16, "text": " and rules based in a lot more depth.", "tokens": [51234, 293, 4474, 2361, 294, 257, 688, 544, 7161, 13, 51374], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 544, "seek": 220196, "start": 2222.16, "end": 2226.56, "text": " Let's go ahead and keep on moving on though and looking at different entity types here", "tokens": [51374, 961, 311, 352, 2286, 293, 1066, 322, 2684, 322, 1673, 293, 1237, 412, 819, 13977, 3467, 510, 51594], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 545, "seek": 220196, "start": 2226.56, "end": 2227.56, "text": " as well.", "tokens": [51594, 382, 731, 13, 51644], "temperature": 0.0, "avg_logprob": -0.21366852847012607, "compression_ratio": 1.5703125, "no_speech_prob": 0.04741545394062996}, {"id": 546, "seek": 222756, "start": 2227.56, "end": 2229.84, "text": " Not entity types, attribute types.", "tokens": [50364, 1726, 13977, 3467, 11, 19667, 3467, 13, 50478], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 547, "seek": 222756, "start": 2229.84, "end": 2237.92, "text": " So we're going to say token to dot int IOB, all lowercase and again underscore at the", "tokens": [50478, 407, 321, 434, 516, 281, 584, 14862, 281, 5893, 560, 286, 46, 33, 11, 439, 3126, 9765, 293, 797, 37556, 412, 264, 50882], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 548, "seek": 222756, "start": 2237.92, "end": 2242.12, "text": " end and we get the string here, I.", "tokens": [50882, 917, 293, 321, 483, 264, 6798, 510, 11, 286, 13, 51092], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 549, "seek": 222756, "start": 2242.12, "end": 2247.04, "text": " Now IOB is a specific kind of named entity code.", "tokens": [51092, 823, 286, 46, 33, 307, 257, 2685, 733, 295, 4926, 13977, 3089, 13, 51338], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 550, "seek": 222756, "start": 2247.04, "end": 2251.4, "text": " B would mean that it's the beginning of an entity and I means that it's inside of an", "tokens": [51338, 363, 576, 914, 300, 309, 311, 264, 2863, 295, 364, 13977, 293, 286, 1355, 300, 309, 311, 1854, 295, 364, 51556], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 551, "seek": 222756, "start": 2251.4, "end": 2256.32, "text": " entity and O means that it's outside of an entity.", "tokens": [51556, 13977, 293, 422, 1355, 300, 309, 311, 2380, 295, 364, 13977, 13, 51802], "temperature": 0.0, "avg_logprob": -0.25304268817512354, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.1096886545419693}, {"id": 552, "seek": 225632, "start": 2256.32, "end": 2263.04, "text": " The fact that we're seeing I here tells us that this word states is inside of a larger", "tokens": [50364, 440, 1186, 300, 321, 434, 2577, 286, 510, 5112, 505, 300, 341, 1349, 4368, 307, 1854, 295, 257, 4833, 50700], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 553, "seek": 225632, "start": 2263.04, "end": 2264.04, "text": " entity.", "tokens": [50700, 13977, 13, 50750], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 554, "seek": 225632, "start": 2264.04, "end": 2267.48, "text": " In fact, we know that because we've seen the left edge and we've seen the right edge.", "tokens": [50750, 682, 1186, 11, 321, 458, 300, 570, 321, 600, 1612, 264, 1411, 4691, 293, 321, 600, 1612, 264, 558, 4691, 13, 50922], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 555, "seek": 225632, "start": 2267.48, "end": 2271.28, "text": " It's inside of the United States of America.", "tokens": [50922, 467, 311, 1854, 295, 264, 2824, 3040, 295, 3374, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 556, "seek": 225632, "start": 2271.28, "end": 2274.6000000000004, "text": " So it's part of a larger entity at hand.", "tokens": [51112, 407, 309, 311, 644, 295, 257, 4833, 13977, 412, 1011, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 557, "seek": 225632, "start": 2274.6000000000004, "end": 2282.0800000000004, "text": " We can also say token to dot lima and under case again after that and we get the word", "tokens": [51278, 492, 393, 611, 584, 14862, 281, 5893, 2364, 64, 293, 833, 1389, 797, 934, 300, 293, 321, 483, 264, 1349, 51652], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 558, "seek": 225632, "start": 2282.0800000000004, "end": 2283.0800000000004, "text": " states.", "tokens": [51652, 4368, 13, 51702], "temperature": 0.0, "avg_logprob": -0.1358540978762183, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11276170611381531}, {"id": 559, "seek": 228308, "start": 2283.08, "end": 2286.72, "text": " Lima form or the root form of the word.", "tokens": [50364, 16406, 64, 1254, 420, 264, 5593, 1254, 295, 264, 1349, 13, 50546], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 560, "seek": 228308, "start": 2286.72, "end": 2289.7599999999998, "text": " This means that this is what the word looks like with no inflection.", "tokens": [50546, 639, 1355, 300, 341, 307, 437, 264, 1349, 1542, 411, 365, 572, 1536, 5450, 13, 50698], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 561, "seek": 228308, "start": 2289.7599999999998, "end": 2294.56, "text": " If we were working with a verb, in fact, let's go ahead and do that right now.", "tokens": [50698, 759, 321, 645, 1364, 365, 257, 9595, 11, 294, 1186, 11, 718, 311, 352, 2286, 293, 360, 300, 558, 586, 13, 50938], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 562, "seek": 228308, "start": 2294.56, "end": 2296.56, "text": " Let's grab sentence.", "tokens": [50938, 961, 311, 4444, 8174, 13, 51038], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 563, "seek": 228308, "start": 2296.56, "end": 2302.0, "text": " We're going to grab sentence one index 12, which should be the word no and we're going", "tokens": [51038, 492, 434, 516, 281, 4444, 8174, 472, 8186, 2272, 11, 597, 820, 312, 264, 1349, 572, 293, 321, 434, 516, 51310], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 564, "seek": 228308, "start": 2302.0, "end": 2309.84, "text": " to print off the lima for the word or sorry, it's a verb and we see the verb lima as no.", "tokens": [51310, 281, 4482, 766, 264, 2364, 64, 337, 264, 1349, 420, 2597, 11, 309, 311, 257, 9595, 293, 321, 536, 264, 9595, 2364, 64, 382, 572, 13, 51702], "temperature": 0.0, "avg_logprob": -0.14792285646711076, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.5386366248130798}, {"id": 565, "seek": 230984, "start": 2309.84, "end": 2319.6000000000004, "text": " So if we were to print off sentence one specifically index 12, we see that its original form is", "tokens": [50364, 407, 498, 321, 645, 281, 4482, 766, 8174, 472, 4682, 8186, 2272, 11, 321, 536, 300, 1080, 3380, 1254, 307, 50852], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 566, "seek": 230984, "start": 2319.6000000000004, "end": 2320.78, "text": " known.", "tokens": [50852, 2570, 13, 50911], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 567, "seek": 230984, "start": 2320.78, "end": 2327.6400000000003, "text": " So the lima form uninflected is the verb no K N O W.", "tokens": [50911, 407, 264, 2364, 64, 1254, 517, 19920, 1809, 292, 307, 264, 9595, 572, 591, 426, 422, 343, 13, 51254], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 568, "seek": 230984, "start": 2327.6400000000003, "end": 2331.2400000000002, "text": " Another thing that we can access and we're going to see that have the power of this later", "tokens": [51254, 3996, 551, 300, 321, 393, 2105, 293, 321, 434, 516, 281, 536, 300, 362, 264, 1347, 295, 341, 1780, 51434], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 569, "seek": 230984, "start": 2331.2400000000002, "end": 2332.2400000000002, "text": " on.", "tokens": [51434, 322, 13, 51484], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 570, "seek": 230984, "start": 2332.2400000000002, "end": 2335.6800000000003, "text": " This might not seem important right now, but I promise you it will be.", "tokens": [51484, 639, 1062, 406, 1643, 1021, 558, 586, 11, 457, 286, 6228, 291, 309, 486, 312, 13, 51656], "temperature": 0.0, "avg_logprob": -0.18892131008944668, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0034833436366170645}, {"id": 571, "seek": 233568, "start": 2335.68, "end": 2341.3999999999996, "text": " Let's print off token that I call this again token to we're going to print that off, but", "tokens": [50364, 961, 311, 4482, 766, 14862, 300, 286, 818, 341, 797, 14862, 281, 321, 434, 516, 281, 4482, 300, 766, 11, 457, 50650], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 572, "seek": 233568, "start": 2341.3999999999996, "end": 2344.3199999999997, "text": " we're going to print off specifically the morph.", "tokens": [50650, 321, 434, 516, 281, 4482, 766, 4682, 264, 25778, 13, 50796], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 573, "seek": 233568, "start": 2344.3199999999997, "end": 2345.72, "text": " No underscore here.", "tokens": [50796, 883, 37556, 510, 13, 50866], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 574, "seek": 233568, "start": 2345.72, "end": 2346.72, "text": " Just morph.", "tokens": [50866, 1449, 25778, 13, 50916], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 575, "seek": 233568, "start": 2346.72, "end": 2352.3199999999997, "text": " What you get is what looks like a really weird output a string called noun type equal", "tokens": [50916, 708, 291, 483, 307, 437, 1542, 411, 257, 534, 3657, 5598, 257, 6798, 1219, 23307, 2010, 2681, 51196], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 576, "seek": 233568, "start": 2352.3199999999997, "end": 2353.3199999999997, "text": " to prop.", "tokens": [51196, 281, 2365, 13, 51246], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 577, "seek": 233568, "start": 2353.3199999999997, "end": 2358.6, "text": " In fact, this means proper noun, a number which corresponds to sing.", "tokens": [51246, 682, 1186, 11, 341, 1355, 2296, 23307, 11, 257, 1230, 597, 23249, 281, 1522, 13, 51510], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 578, "seek": 233568, "start": 2358.6, "end": 2364.48, "text": " We're going to talk a lot more about morphological analysis later on when we try to find an extract", "tokens": [51510, 492, 434, 516, 281, 751, 257, 688, 544, 466, 25778, 4383, 5215, 1780, 322, 562, 321, 853, 281, 915, 364, 8947, 51804], "temperature": 0.0, "avg_logprob": -0.19980486801692418, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.5848975777626038}, {"id": 579, "seek": 236448, "start": 2364.48, "end": 2367.32, "text": " information from our texts.", "tokens": [50364, 1589, 490, 527, 15765, 13, 50506], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 580, "seek": 236448, "start": 2367.32, "end": 2372.52, "text": " But for right now, understand that what you're looking at is the output of kind of what that", "tokens": [50506, 583, 337, 558, 586, 11, 1223, 300, 437, 291, 434, 1237, 412, 307, 264, 5598, 295, 733, 295, 437, 300, 50766], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 581, "seek": 236448, "start": 2372.52, "end": 2374.6, "text": " word is morphologically.", "tokens": [50766, 1349, 307, 25778, 17157, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 582, "seek": 236448, "start": 2374.6, "end": 2377.64, "text": " So in this case, it's a proper noun and it's singular.", "tokens": [50870, 407, 294, 341, 1389, 11, 309, 311, 257, 2296, 23307, 293, 309, 311, 20010, 13, 51022], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 583, "seek": 236448, "start": 2377.64, "end": 2385.36, "text": " If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb", "tokens": [51022, 759, 321, 645, 281, 360, 747, 341, 8174, 2272, 797, 293, 360, 25778, 11, 321, 1116, 915, 484, 437, 733, 295, 9595, 51408], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 584, "seek": 236448, "start": 2385.36, "end": 2386.36, "text": " it is.", "tokens": [51408, 309, 307, 13, 51458], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 585, "seek": 236448, "start": 2386.36, "end": 2393.04, "text": " So it's a perfect past participle known perfect past participle.", "tokens": [51458, 407, 309, 311, 257, 2176, 1791, 3421, 306, 2570, 2176, 1791, 3421, 306, 13, 51792], "temperature": 0.0, "avg_logprob": -0.14732885360717773, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.07367804646492004}, {"id": 586, "seek": 239304, "start": 2393.04, "end": 2396.96, "text": " For being good at NLP is also being good with language.", "tokens": [50364, 1171, 885, 665, 412, 426, 45196, 307, 611, 885, 665, 365, 2856, 13, 50560], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 587, "seek": 239304, "start": 2396.96, "end": 2400.24, "text": " So I encourage you to spend time and start getting familiar with those things that you", "tokens": [50560, 407, 286, 5373, 291, 281, 3496, 565, 293, 722, 1242, 4963, 365, 729, 721, 300, 291, 50724], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 588, "seek": 239304, "start": 2400.24, "end": 2404.54, "text": " might have forgotten about from like fifth grade grammar, such as perfect participles", "tokens": [50724, 1062, 362, 11832, 466, 490, 411, 9266, 7204, 22317, 11, 1270, 382, 2176, 1276, 72, 2622, 50939], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 589, "seek": 239304, "start": 2404.54, "end": 2406.24, "text": " and things like that.", "tokens": [50939, 293, 721, 411, 300, 13, 51024], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 590, "seek": 239304, "start": 2406.24, "end": 2410.62, "text": " Because when you need to start creating rules to extract information, you're going to find", "tokens": [51024, 1436, 562, 291, 643, 281, 722, 4084, 4474, 281, 8947, 1589, 11, 291, 434, 516, 281, 915, 51243], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 591, "seek": 239304, "start": 2410.62, "end": 2414.92, "text": " those pieces of information very important for writing rules.", "tokens": [51243, 729, 3755, 295, 1589, 588, 1021, 337, 3579, 4474, 13, 51458], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 592, "seek": 239304, "start": 2414.92, "end": 2416.92, "text": " We'll talk about that in a little bit though.", "tokens": [51458, 492, 603, 751, 466, 300, 294, 257, 707, 857, 1673, 13, 51558], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 593, "seek": 239304, "start": 2416.92, "end": 2420.24, "text": " Let's go back to our other attributes from the token.", "tokens": [51558, 961, 311, 352, 646, 281, 527, 661, 17212, 490, 264, 14862, 13, 51724], "temperature": 0.0, "avg_logprob": -0.11986010761584266, "compression_ratio": 1.6546052631578947, "no_speech_prob": 0.46832481026649475}, {"id": 594, "seek": 242024, "start": 2420.24, "end": 2425.68, "text": " So again, let's go to token two, and we're going to grab the POS part of speech, not", "tokens": [50364, 407, 797, 11, 718, 311, 352, 281, 14862, 732, 11, 293, 321, 434, 516, 281, 4444, 264, 430, 4367, 644, 295, 6218, 11, 406, 50636], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 595, "seek": 242024, "start": 2425.68, "end": 2427.12, "text": " what you might be thinking.", "tokens": [50636, 437, 291, 1062, 312, 1953, 13, 50708], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 596, "seek": 242024, "start": 2427.12, "end": 2433.02, "text": " So part of speech underscore POS underscore, and we output PROPN.", "tokens": [50708, 407, 644, 295, 6218, 37556, 430, 4367, 37556, 11, 293, 321, 5598, 11568, 12059, 45, 13, 51003], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 597, "seek": 242024, "start": 2433.02, "end": 2435.7599999999998, "text": " This means that it is a proper noun.", "tokens": [51003, 639, 1355, 300, 309, 307, 257, 2296, 23307, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 598, "seek": 242024, "start": 2435.7599999999998, "end": 2442.64, "text": " It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological", "tokens": [51140, 467, 311, 544, 295, 257, 295, 257, 18587, 733, 295, 17570, 267, 804, 30197, 11, 382, 8851, 281, 341, 25778, 4383, 51484], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 599, "seek": 242024, "start": 2442.64, "end": 2449.3199999999997, "text": " detailed extraction, what kind of noun it might be with regards to in this case, singular.", "tokens": [51484, 9942, 30197, 11, 437, 733, 295, 23307, 309, 1062, 312, 365, 14258, 281, 294, 341, 1389, 11, 20010, 13, 51818], "temperature": 0.0, "avg_logprob": -0.16610023078568484, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.050319917500019073}, {"id": 600, "seek": 244932, "start": 2449.32, "end": 2452.04, "text": " So that's going to be how you extract the part of speech.", "tokens": [50364, 407, 300, 311, 516, 281, 312, 577, 291, 8947, 264, 644, 295, 6218, 13, 50500], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 601, "seek": 244932, "start": 2452.04, "end": 2457.5800000000004, "text": " And the thing that you can do is you can extract the dependency relation.", "tokens": [50500, 400, 264, 551, 300, 291, 393, 360, 307, 291, 393, 8947, 264, 33621, 9721, 13, 50777], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 602, "seek": 244932, "start": 2457.5800000000004, "end": 2462.4, "text": " So in this case, we can figure out what role it plays in the sentence.", "tokens": [50777, 407, 294, 341, 1389, 11, 321, 393, 2573, 484, 437, 3090, 309, 5749, 294, 264, 8174, 13, 51018], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 603, "seek": 244932, "start": 2462.4, "end": 2465.6000000000004, "text": " In this case, the noun subject.", "tokens": [51018, 682, 341, 1389, 11, 264, 23307, 3983, 13, 51178], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 604, "seek": 244932, "start": 2465.6000000000004, "end": 2469.8, "text": " And then finally, the last thing I really want to talk about before we move into a more", "tokens": [51178, 400, 550, 2721, 11, 264, 1036, 551, 286, 534, 528, 281, 751, 466, 949, 321, 1286, 666, 257, 544, 51388], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 605, "seek": 244932, "start": 2469.8, "end": 2477.8, "text": " detailed analysis of part of speech is going to be the token two dot lane.", "tokens": [51388, 9942, 5215, 295, 644, 295, 6218, 307, 516, 281, 312, 264, 14862, 732, 5893, 12705, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15835967573147375, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.019715847447514534}, {"id": 606, "seek": 247780, "start": 2477.8, "end": 2483.28, "text": " And what this grabs for you is the language of the doc object in this case, we're working", "tokens": [50364, 400, 437, 341, 30028, 337, 291, 307, 264, 2856, 295, 264, 3211, 2657, 294, 341, 1389, 11, 321, 434, 1364, 50638], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 607, "seek": 247780, "start": 2483.28, "end": 2488.6800000000003, "text": " with something from the English language, so in every language is going to have two", "tokens": [50638, 365, 746, 490, 264, 3669, 2856, 11, 370, 294, 633, 2856, 307, 516, 281, 362, 732, 50908], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 608, "seek": 247780, "start": 2488.6800000000003, "end": 2490.32, "text": " letters that correspond to it.", "tokens": [50908, 7825, 300, 6805, 281, 309, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 609, "seek": 247780, "start": 2490.32, "end": 2492.54, "text": " These are universally recognized.", "tokens": [50990, 1981, 366, 43995, 9823, 13, 51101], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 610, "seek": 247780, "start": 2492.54, "end": 2498.88, "text": " So that's going to be how you access different kinds of attributes that each token has.", "tokens": [51101, 407, 300, 311, 516, 281, 312, 577, 291, 2105, 819, 3685, 295, 17212, 300, 1184, 14862, 575, 13, 51418], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 611, "seek": 247780, "start": 2498.88, "end": 2502.2400000000002, "text": " And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that", "tokens": [51418, 400, 456, 311, 466, 945, 544, 295, 613, 11, 420, 1310, 406, 945, 11, 1310, 466, 2119, 544, 295, 613, 300, 51586], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 612, "seek": 247780, "start": 2502.2400000000002, "end": 2503.6800000000003, "text": " I haven't covered.", "tokens": [51586, 286, 2378, 380, 5343, 13, 51658], "temperature": 0.0, "avg_logprob": -0.14430298315030393, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.023685220628976822}, {"id": 613, "seek": 250368, "start": 2503.68, "end": 2508.8799999999997, "text": " I gave you the ones that are the most important that I find to be used on a regular basis", "tokens": [50364, 286, 2729, 291, 264, 2306, 300, 366, 264, 881, 1021, 300, 286, 915, 281, 312, 1143, 322, 257, 3890, 5143, 50624], "temperature": 0.0, "avg_logprob": -0.10287714004516602, "compression_ratio": 1.672, "no_speech_prob": 0.0013669938780367374}, {"id": 614, "seek": 250368, "start": 2508.8799999999997, "end": 2515.3999999999996, "text": " to solve different problems with regards to information extraction from the text.", "tokens": [50624, 281, 5039, 819, 2740, 365, 14258, 281, 1589, 30197, 490, 264, 2487, 13, 50950], "temperature": 0.0, "avg_logprob": -0.10287714004516602, "compression_ratio": 1.672, "no_speech_prob": 0.0013669938780367374}, {"id": 615, "seek": 250368, "start": 2515.3999999999996, "end": 2519.58, "text": " So that's going to be where we stop here with token attributes, and we're going to be moving", "tokens": [50950, 407, 300, 311, 516, 281, 312, 689, 321, 1590, 510, 365, 14862, 17212, 11, 293, 321, 434, 516, 281, 312, 2684, 51159], "temperature": 0.0, "avg_logprob": -0.10287714004516602, "compression_ratio": 1.672, "no_speech_prob": 0.0013669938780367374}, {"id": 616, "seek": 250368, "start": 2519.58, "end": 2526.2, "text": " on to part 2.5 of the book, which is part of speech tagging.", "tokens": [51159, 322, 281, 644, 568, 13, 20, 295, 264, 1446, 11, 597, 307, 644, 295, 6218, 6162, 3249, 13, 51490], "temperature": 0.0, "avg_logprob": -0.10287714004516602, "compression_ratio": 1.672, "no_speech_prob": 0.0013669938780367374}, {"id": 617, "seek": 250368, "start": 2526.2, "end": 2532.3599999999997, "text": " I now want to move into kind of a more detailed analysis of part of speech within spacey and", "tokens": [51490, 286, 586, 528, 281, 1286, 666, 733, 295, 257, 544, 9942, 5215, 295, 644, 295, 6218, 1951, 1901, 88, 293, 51798], "temperature": 0.0, "avg_logprob": -0.10287714004516602, "compression_ratio": 1.672, "no_speech_prob": 0.0013669938780367374}, {"id": 618, "seek": 253236, "start": 2532.36, "end": 2538.6400000000003, "text": " the dependency parser and how to actually analyze it really nicely either in a notebook", "tokens": [50364, 264, 33621, 21156, 260, 293, 577, 281, 767, 12477, 309, 534, 9594, 2139, 294, 257, 21060, 50678], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 619, "seek": 253236, "start": 2538.6400000000003, "end": 2540.28, "text": " or outside of a notebook.", "tokens": [50678, 420, 2380, 295, 257, 21060, 13, 50760], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 620, "seek": 253236, "start": 2540.28, "end": 2543.1200000000003, "text": " So let's work with a different text for just a few minutes.", "tokens": [50760, 407, 718, 311, 589, 365, 257, 819, 2487, 337, 445, 257, 1326, 2077, 13, 50902], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 621, "seek": 253236, "start": 2543.1200000000003, "end": 2544.88, "text": " We're going to see why this is important.", "tokens": [50902, 492, 434, 516, 281, 536, 983, 341, 307, 1021, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 622, "seek": 253236, "start": 2544.88, "end": 2548.76, "text": " It's because I'm working on a zoomed in screen, and to make this sentence a little easier", "tokens": [50990, 467, 311, 570, 286, 478, 1364, 322, 257, 8863, 292, 294, 2568, 11, 293, 281, 652, 341, 8174, 257, 707, 3571, 51184], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 623, "seek": 253236, "start": 2548.76, "end": 2556.1200000000003, "text": " to understand, we're going to just use Mike and Joy's plain football, a very simple sentence.", "tokens": [51184, 281, 1223, 11, 321, 434, 516, 281, 445, 764, 6602, 293, 15571, 311, 11121, 7346, 11, 257, 588, 2199, 8174, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 624, "seek": 253236, "start": 2556.1200000000003, "end": 2559.96, "text": " And we're going to create a new doc object, and we're going to call this doc two.", "tokens": [51552, 400, 321, 434, 516, 281, 1884, 257, 777, 3211, 2657, 11, 293, 321, 434, 516, 281, 818, 341, 3211, 732, 13, 51744], "temperature": 0.0, "avg_logprob": -0.14823355102539063, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.04083813354372978}, {"id": 625, "seek": 255996, "start": 2559.96, "end": 2562.84, "text": " That's going to be equal to NLP text.", "tokens": [50364, 663, 311, 516, 281, 312, 2681, 281, 426, 45196, 2487, 13, 50508], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 626, "seek": 255996, "start": 2562.84, "end": 2567.44, "text": " Let's print off doc two just to make sure that it was created, and in fact that we see", "tokens": [50508, 961, 311, 4482, 766, 3211, 732, 445, 281, 652, 988, 300, 309, 390, 2942, 11, 293, 294, 1186, 300, 321, 536, 50738], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 627, "seek": 255996, "start": 2567.44, "end": 2568.44, "text": " that it was.", "tokens": [50738, 300, 309, 390, 13, 50788], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 628, "seek": 255996, "start": 2568.44, "end": 2574.2, "text": " Now that we've got it created, let's iterate over the tokens within this and say for token", "tokens": [50788, 823, 300, 321, 600, 658, 309, 2942, 11, 718, 311, 44497, 670, 264, 22667, 1951, 341, 293, 584, 337, 14862, 51076], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 629, "seek": 255996, "start": 2574.2, "end": 2579.48, "text": " in text, we want to print off token dot text.", "tokens": [51076, 294, 2487, 11, 321, 528, 281, 4482, 766, 14862, 5893, 2487, 13, 51340], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 630, "seek": 255996, "start": 2579.48, "end": 2581.6, "text": " We want to see what the text actually is.", "tokens": [51340, 492, 528, 281, 536, 437, 264, 2487, 767, 307, 13, 51446], "temperature": 0.0, "avg_logprob": -0.18595328538314157, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.2567695379257202}, {"id": 631, "seek": 258160, "start": 2581.6, "end": 2593.24, "text": " We want to see the token dot POS, and the token dot DEP helps if you actually iterate", "tokens": [50364, 492, 528, 281, 536, 264, 14862, 5893, 430, 4367, 11, 293, 264, 14862, 5893, 413, 8929, 3665, 498, 291, 767, 44497, 50946], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 632, "seek": 258160, "start": 2593.24, "end": 2596.12, "text": " over the correct object over the doc to object.", "tokens": [50946, 670, 264, 3006, 2657, 670, 264, 3211, 281, 2657, 13, 51090], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 633, "seek": 258160, "start": 2596.12, "end": 2600.64, "text": " And we see that we've got Mike, proper noun, noun, subject, and Joy's verb.", "tokens": [51090, 400, 321, 536, 300, 321, 600, 658, 6602, 11, 2296, 23307, 11, 23307, 11, 3983, 11, 293, 15571, 311, 9595, 13, 51316], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 634, "seek": 258160, "start": 2600.64, "end": 2602.64, "text": " It's the root plane.", "tokens": [51316, 467, 311, 264, 5593, 5720, 13, 51416], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 635, "seek": 258160, "start": 2602.64, "end": 2604.44, "text": " In this case, it's a verb.", "tokens": [51416, 682, 341, 1389, 11, 309, 311, 257, 9595, 13, 51506], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 636, "seek": 258160, "start": 2604.44, "end": 2608.2799999999997, "text": " And then we've got football, the noun, the direct object, and a period, which is the", "tokens": [51506, 400, 550, 321, 600, 658, 7346, 11, 264, 23307, 11, 264, 2047, 2657, 11, 293, 257, 2896, 11, 597, 307, 264, 51698], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 637, "seek": 258160, "start": 2608.2799999999997, "end": 2609.2799999999997, "text": " punctuation.", "tokens": [51698, 27006, 16073, 13, 51748], "temperature": 0.0, "avg_logprob": -0.23589125303464514, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.14412713050842285}, {"id": 638, "seek": 260928, "start": 2609.4, "end": 2613.6400000000003, "text": " So we can see the basic semantics of the sentence at play.", "tokens": [50370, 407, 321, 393, 536, 264, 3875, 4361, 45298, 295, 264, 8174, 412, 862, 13, 50582], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 639, "seek": 260928, "start": 2613.6400000000003, "end": 2617.2000000000003, "text": " What's really nice from spacey is we have a way to really visualize this information", "tokens": [50582, 708, 311, 534, 1481, 490, 1901, 88, 307, 321, 362, 257, 636, 281, 534, 23273, 341, 1589, 50760], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 640, "seek": 260928, "start": 2617.2000000000003, "end": 2619.36, "text": " and how these words relate to one another.", "tokens": [50760, 293, 577, 613, 2283, 10961, 281, 472, 1071, 13, 50868], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 641, "seek": 260928, "start": 2619.36, "end": 2627.4, "text": " So we can say from spacey, import, displacey, and we're going to do displacey, displacey", "tokens": [50868, 407, 321, 393, 584, 490, 1901, 88, 11, 974, 11, 717, 6742, 88, 11, 293, 321, 434, 516, 281, 360, 717, 6742, 88, 11, 717, 6742, 88, 51270], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 642, "seek": 260928, "start": 2627.4, "end": 2631.4, "text": " dot render.", "tokens": [51270, 5893, 15529, 13, 51470], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 643, "seek": 260928, "start": 2631.4, "end": 2635.44, "text": " And this is going to take two arguments, it's going to be the text, and then it's going", "tokens": [51470, 400, 341, 307, 516, 281, 747, 732, 12869, 11, 309, 311, 516, 281, 312, 264, 2487, 11, 293, 550, 309, 311, 516, 51672], "temperature": 0.0, "avg_logprob": -0.188737653336435, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.03307917341589928}, {"id": 644, "seek": 263544, "start": 2635.44, "end": 2641.32, "text": " to be the, actually, it's going to be doc two, and then it's going to be style.", "tokens": [50364, 281, 312, 264, 11, 767, 11, 309, 311, 516, 281, 312, 3211, 732, 11, 293, 550, 309, 311, 516, 281, 312, 3758, 13, 50658], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 645, "seek": 263544, "start": 2641.32, "end": 2645.44, "text": " In this case, we're going to be working with DEP, and we're going to print that off.", "tokens": [50658, 682, 341, 1389, 11, 321, 434, 516, 281, 312, 1364, 365, 413, 8929, 11, 293, 321, 434, 516, 281, 4482, 300, 766, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 646, "seek": 263544, "start": 2645.44, "end": 2650.44, "text": " And we actually see how that sentence is structured.", "tokens": [50864, 400, 321, 767, 536, 577, 300, 8174, 307, 18519, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 647, "seek": 263544, "start": 2650.44, "end": 2653.7200000000003, "text": " Now in the textbook, I use a more complicated sentence.", "tokens": [51114, 823, 294, 264, 25591, 11, 286, 764, 257, 544, 6179, 8174, 13, 51278], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 648, "seek": 263544, "start": 2653.7200000000003, "end": 2657.48, "text": " But for the reasons of this video, I've kept it a little shorter, just because I think", "tokens": [51278, 583, 337, 264, 4112, 295, 341, 960, 11, 286, 600, 4305, 309, 257, 707, 11639, 11, 445, 570, 286, 519, 51466], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 649, "seek": 263544, "start": 2657.48, "end": 2662.32, "text": " it displays better on this screen, because you can see that this becomes a little bit", "tokens": [51466, 309, 20119, 1101, 322, 341, 2568, 11, 570, 291, 393, 536, 300, 341, 3643, 257, 707, 857, 51708], "temperature": 0.0, "avg_logprob": -0.17333806462648535, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.02517353557050228}, {"id": 650, "seek": 266232, "start": 2662.32, "end": 2665.6000000000004, "text": " more difficult to understand when you're zoomed in.", "tokens": [50364, 544, 2252, 281, 1223, 562, 291, 434, 8863, 292, 294, 13, 50528], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 651, "seek": 266232, "start": 2665.6000000000004, "end": 2669.44, "text": " But this is one sentence from that Wikipedia article.", "tokens": [50528, 583, 341, 307, 472, 8174, 490, 300, 28999, 7222, 13, 50720], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 652, "seek": 266232, "start": 2669.44, "end": 2672.6400000000003, "text": " So go ahead and look at the textbook and see how elaborate this is.", "tokens": [50720, 407, 352, 2286, 293, 574, 412, 264, 25591, 293, 536, 577, 20945, 341, 307, 13, 50880], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 653, "seek": 266232, "start": 2672.6400000000003, "end": 2676.4, "text": " You can see how it's part of a compound, how it's preposition.", "tokens": [50880, 509, 393, 536, 577, 309, 311, 644, 295, 257, 14154, 11, 577, 309, 311, 2666, 5830, 13, 51068], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 654, "seek": 266232, "start": 2676.4, "end": 2682.6800000000003, "text": " You can see the more fine-grained aspects of the dependency parser and the part of speech", "tokens": [51068, 509, 393, 536, 264, 544, 2489, 12, 20735, 2001, 7270, 295, 264, 33621, 21156, 260, 293, 264, 644, 295, 6218, 51382], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 655, "seek": 266232, "start": 2682.6800000000003, "end": 2686.8, "text": " tagger really at play with more complicated sentences.", "tokens": [51382, 6162, 1321, 534, 412, 862, 365, 544, 6179, 16579, 13, 51588], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 656, "seek": 266232, "start": 2686.8, "end": 2690.96, "text": " So that's going to be how you really access part of speech and how you can start to visualize", "tokens": [51588, 407, 300, 311, 516, 281, 312, 577, 291, 534, 2105, 644, 295, 6218, 293, 577, 291, 393, 722, 281, 23273, 51796], "temperature": 0.0, "avg_logprob": -0.10461070757954061, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.009411734528839588}, {"id": 657, "seek": 269096, "start": 2690.96, "end": 2697.2400000000002, "text": " how words in a sentence are connected to other words in a sentence with regards to their", "tokens": [50364, 577, 2283, 294, 257, 8174, 366, 4582, 281, 661, 2283, 294, 257, 8174, 365, 14258, 281, 641, 50678], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 658, "seek": 269096, "start": 2697.2400000000002, "end": 2700.48, "text": " part of speech and their dependencies.", "tokens": [50678, 644, 295, 6218, 293, 641, 36606, 13, 50840], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 659, "seek": 269096, "start": 2700.48, "end": 2701.96, "text": " That's going to be where we stop with that.", "tokens": [50840, 663, 311, 516, 281, 312, 689, 321, 1590, 365, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 660, "seek": 269096, "start": 2701.96, "end": 2707.2, "text": " In the next section, we're going to be talking about named entity recognition and how to visualize", "tokens": [50914, 682, 264, 958, 3541, 11, 321, 434, 516, 281, 312, 1417, 466, 4926, 13977, 11150, 293, 577, 281, 23273, 51176], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 661, "seek": 269096, "start": 2707.2, "end": 2709.68, "text": " that information.", "tokens": [51176, 300, 1589, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 662, "seek": 269096, "start": 2709.68, "end": 2713.92, "text": " So named entity recognition is a very common NLP task.", "tokens": [51300, 407, 4926, 13977, 11150, 307, 257, 588, 2689, 426, 45196, 5633, 13, 51512], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 663, "seek": 269096, "start": 2713.92, "end": 2719.08, "text": " It's part of kind of data extraction or information extraction from texts.", "tokens": [51512, 467, 311, 644, 295, 733, 295, 1412, 30197, 420, 1589, 30197, 490, 15765, 13, 51770], "temperature": 0.0, "avg_logprob": -0.11959722306993273, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.008576525375247002}, {"id": 664, "seek": 271908, "start": 2719.08, "end": 2723.48, "text": " It's oftentimes just called NER, named entity recognition.", "tokens": [50364, 467, 311, 18349, 445, 1219, 426, 1598, 11, 4926, 13977, 11150, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 665, "seek": 271908, "start": 2723.48, "end": 2728.16, "text": " I have a whole book on how to do NER with Python and with Spacey, but we're not going", "tokens": [50584, 286, 362, 257, 1379, 1446, 322, 577, 281, 360, 426, 1598, 365, 15329, 293, 365, 8705, 88, 11, 457, 321, 434, 406, 516, 50818], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 666, "seek": 271908, "start": 2728.16, "end": 2730.12, "text": " to be talking about all the ins and outs right now.", "tokens": [50818, 281, 312, 1417, 466, 439, 264, 1028, 293, 14758, 558, 586, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 667, "seek": 271908, "start": 2730.12, "end": 2735.6, "text": " We're just going to be talking about how to access the pieces of information throughout", "tokens": [50916, 492, 434, 445, 516, 281, 312, 1417, 466, 577, 281, 2105, 264, 3755, 295, 1589, 3710, 51190], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 668, "seek": 271908, "start": 2735.6, "end": 2737.08, "text": " kind of our text.", "tokens": [51190, 733, 295, 527, 2487, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 669, "seek": 271908, "start": 2737.08, "end": 2742.56, "text": " And then we're going to be dealing with a lot of NER as we try to create elaborate systems", "tokens": [51264, 400, 550, 321, 434, 516, 281, 312, 6260, 365, 257, 688, 295, 426, 1598, 382, 321, 853, 281, 1884, 20945, 3652, 51538], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 670, "seek": 271908, "start": 2742.56, "end": 2747.88, "text": " to do named entity extraction for things like financial analysis.", "tokens": [51538, 281, 360, 4926, 13977, 30197, 337, 721, 411, 4669, 5215, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1405117758389177, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.01744108833372593}, {"id": 671, "seek": 274788, "start": 2747.88, "end": 2752.08, "text": " Let's go ahead and figure out how to iterate over a doc object.", "tokens": [50364, 961, 311, 352, 2286, 293, 2573, 484, 577, 281, 44497, 670, 257, 3211, 2657, 13, 50574], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 672, "seek": 274788, "start": 2752.08, "end": 2757.52, "text": " So we're going to say for int and doc.n, so we're going to go back to that original doc,", "tokens": [50574, 407, 321, 434, 516, 281, 584, 337, 560, 293, 3211, 13, 77, 11, 370, 321, 434, 516, 281, 352, 646, 281, 300, 3380, 3211, 11, 50846], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 673, "seek": 274788, "start": 2757.52, "end": 2763.8, "text": " the one that's got the text from Wikipedia on the United States.", "tokens": [50846, 264, 472, 300, 311, 658, 264, 2487, 490, 28999, 322, 264, 2824, 3040, 13, 51160], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 674, "seek": 274788, "start": 2763.8, "end": 2771.88, "text": " We're going to say print off int.text, so the text from it, and int.label, label underscore", "tokens": [51160, 492, 434, 516, 281, 584, 4482, 766, 560, 13, 25111, 11, 370, 264, 2487, 490, 309, 11, 293, 560, 13, 75, 18657, 11, 7645, 37556, 51564], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 675, "seek": 274788, "start": 2771.88, "end": 2772.88, "text": " here.", "tokens": [51564, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 676, "seek": 274788, "start": 2772.88, "end": 2776.2000000000003, "text": " That's going to tell us what label corresponds to that text.", "tokens": [51614, 663, 311, 516, 281, 980, 505, 437, 7645, 23249, 281, 300, 2487, 13, 51780], "temperature": 0.0, "avg_logprob": -0.16730382707383898, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.19186466932296753}, {"id": 677, "seek": 277620, "start": 2776.3199999999997, "end": 2777.3199999999997, "text": " Then we print this off.", "tokens": [50370, 1396, 321, 4482, 341, 766, 13, 50420], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 678, "seek": 277620, "start": 2777.3199999999997, "end": 2781.7999999999997, "text": " We've got a lot of GPEs, which are geopolitical entities, North America.", "tokens": [50420, 492, 600, 658, 257, 688, 295, 460, 5208, 82, 11, 597, 366, 46615, 804, 16667, 11, 4067, 3374, 13, 50644], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 679, "seek": 277620, "start": 2781.7999999999997, "end": 2783.2799999999997, "text": " This isn't a geopolitical entity.", "tokens": [50644, 639, 1943, 380, 257, 46615, 804, 13977, 13, 50718], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 680, "seek": 277620, "start": 2783.2799999999997, "end": 2790.08, "text": " It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in", "tokens": [50718, 467, 311, 445, 257, 2674, 4914, 11, 2625, 11, 257, 2920, 2071, 1230, 11, 1732, 2920, 2071, 1230, 11, 6051, 6427, 294, 51058], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 681, "seek": 277620, "start": 2790.08, "end": 2796.9199999999996, "text": " this case, which is a national or religious political entity, quantity, the number of", "tokens": [51058, 341, 1389, 11, 597, 307, 257, 4048, 420, 7185, 3905, 13977, 11, 11275, 11, 264, 1230, 295, 51400], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 682, "seek": 277620, "start": 2796.9199999999996, "end": 2804.4399999999996, "text": " miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke.", "tokens": [51400, 6193, 11, 6309, 11, 460, 5208, 11, 382, 291, 576, 2066, 11, 50007, 78, 23838, 11, 6051, 1564, 797, 11, 42608, 654, 11, 12859, 330, 13, 51776], "temperature": 0.0, "avg_logprob": -0.20348262786865234, "compression_ratio": 1.6375, "no_speech_prob": 0.577095091342926}, {"id": 683, "seek": 280444, "start": 2804.48, "end": 2808.4, "text": " Then we have date being extracted, so at least 12,000 years ago.", "tokens": [50366, 1396, 321, 362, 4002, 885, 34086, 11, 370, 412, 1935, 2272, 11, 1360, 924, 2057, 13, 50562], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 684, "seek": 280444, "start": 2808.4, "end": 2813.36, "text": " This is a small model, and it's extracting for us a lot of very important structured", "tokens": [50562, 639, 307, 257, 1359, 2316, 11, 293, 309, 311, 49844, 337, 505, 257, 688, 295, 588, 1021, 18519, 50810], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 685, "seek": 280444, "start": 2813.36, "end": 2814.52, "text": " data.", "tokens": [50810, 1412, 13, 50868], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 686, "seek": 280444, "start": 2814.52, "end": 2818.16, "text": " But we can see that the small model makes mistakes.", "tokens": [50868, 583, 321, 393, 536, 300, 264, 1359, 2316, 1669, 8038, 13, 51050], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 687, "seek": 280444, "start": 2818.16, "end": 2822.12, "text": " So the Revolutionary War is being considered an organization.", "tokens": [51050, 407, 264, 16617, 822, 3630, 307, 885, 4888, 364, 4475, 13, 51248], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 688, "seek": 280444, "start": 2822.12, "end": 2826.88, "text": " Were I to use a large model right now, which I can download separately from Spacey, and", "tokens": [51248, 12448, 286, 281, 764, 257, 2416, 2316, 558, 586, 11, 597, 286, 393, 5484, 14759, 490, 8705, 88, 11, 293, 51486], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 689, "seek": 280444, "start": 2826.88, "end": 2832.44, "text": " we're going to be seeing this later in this video, or were I to use the much larger transformer", "tokens": [51486, 321, 434, 516, 281, 312, 2577, 341, 1780, 294, 341, 960, 11, 420, 645, 286, 281, 764, 264, 709, 4833, 31782, 51764], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 690, "seek": 280444, "start": 2832.44, "end": 2833.52, "text": " model.", "tokens": [51764, 2316, 13, 51818], "temperature": 0.0, "avg_logprob": -0.17755085279961594, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.49980321526527405}, {"id": 691, "seek": 283352, "start": 2833.52, "end": 2839.72, "text": " This would be correctly identified most likely as an event, not as an organization, but because", "tokens": [50364, 639, 576, 312, 8944, 9234, 881, 3700, 382, 364, 2280, 11, 406, 382, 364, 4475, 11, 457, 570, 50674], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 692, "seek": 283352, "start": 2839.72, "end": 2844.36, "text": " this is a small model that doesn't contain word vectors, which we're going to talk about", "tokens": [50674, 341, 307, 257, 1359, 2316, 300, 1177, 380, 5304, 1349, 18875, 11, 597, 321, 434, 516, 281, 751, 466, 50906], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 693, "seek": 283352, "start": 2844.36, "end": 2850.52, "text": " in just a little bit, it does not generalize or make predictions well on this particular", "tokens": [50906, 294, 445, 257, 707, 857, 11, 309, 775, 406, 2674, 1125, 420, 652, 21264, 731, 322, 341, 1729, 51214], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 694, "seek": 283352, "start": 2850.52, "end": 2851.52, "text": " data.", "tokens": [51214, 1412, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 695, "seek": 283352, "start": 2851.52, "end": 2855.68, "text": " Nevertheless, we do see really good extraction here.", "tokens": [51264, 26554, 11, 321, 360, 536, 534, 665, 30197, 510, 13, 51472], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 696, "seek": 283352, "start": 2855.68, "end": 2859.06, "text": " We have the American Civil War being extracted as an event.", "tokens": [51472, 492, 362, 264, 2665, 13405, 3630, 885, 34086, 382, 364, 2280, 13, 51641], "temperature": 0.0, "avg_logprob": -0.12252456911148564, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.006487809121608734}, {"id": 697, "seek": 285906, "start": 2859.06, "end": 2863.94, "text": " We have the Spanish American War, even with this encoding typographical error here.", "tokens": [50364, 492, 362, 264, 8058, 2665, 3630, 11, 754, 365, 341, 43430, 2125, 48434, 6713, 510, 13, 50608], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 698, "seek": 285906, "start": 2863.94, "end": 2868.5, "text": " And World War being extracted as an event, World War II event, Cold War event.", "tokens": [50608, 400, 3937, 3630, 885, 34086, 382, 364, 2280, 11, 3937, 3630, 6351, 2280, 11, 16918, 3630, 2280, 13, 50836], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 699, "seek": 285906, "start": 2868.5, "end": 2870.58, "text": " All of this is looking good.", "tokens": [50836, 1057, 295, 341, 307, 1237, 665, 13, 50940], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 700, "seek": 285906, "start": 2870.58, "end": 2876.02, "text": " And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd", "tokens": [50940, 400, 406, 534, 11, 286, 787, 1866, 257, 1916, 3875, 8038, 11, 457, 337, 264, 881, 644, 11, 341, 307, 437, 291, 1116, 51212], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 701, "seek": 285906, "start": 2876.02, "end": 2877.02, "text": " expect to see.", "tokens": [51212, 2066, 281, 536, 13, 51262], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 702, "seek": 285906, "start": 2877.02, "end": 2880.74, "text": " We even see percentages extracted correctly here.", "tokens": [51262, 492, 754, 536, 42270, 34086, 8944, 510, 13, 51448], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 703, "seek": 285906, "start": 2880.74, "end": 2887.18, "text": " So this is how you access really vital information about your tokens, but more importantly about", "tokens": [51448, 407, 341, 307, 577, 291, 2105, 534, 11707, 1589, 466, 428, 22667, 11, 457, 544, 8906, 466, 51770], "temperature": 0.0, "avg_logprob": -0.16341004677868765, "compression_ratio": 1.6804511278195489, "no_speech_prob": 0.019122229889035225}, {"id": 704, "seek": 288718, "start": 2887.18, "end": 2890.66, "text": " the entities found within your text.", "tokens": [50364, 264, 16667, 1352, 1951, 428, 2487, 13, 50538], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 705, "seek": 288718, "start": 2890.66, "end": 2897.1, "text": " And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook.", "tokens": [50538, 400, 611, 11, 4208, 564, 326, 414, 7736, 257, 534, 1481, 636, 281, 23273, 341, 294, 257, 22125, 88, 391, 11633, 2939, 13, 50860], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 706, "seek": 288718, "start": 2897.1, "end": 2904.54, "text": " We can say displacie.render, we can say doc, style, we can say int.", "tokens": [50860, 492, 393, 584, 717, 564, 326, 414, 13, 13292, 11, 321, 393, 584, 3211, 11, 3758, 11, 321, 393, 584, 560, 13, 51232], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 707, "seek": 288718, "start": 2904.54, "end": 2910.06, "text": " And we get this really nice visualization where each entity has its own particular color.", "tokens": [51232, 400, 321, 483, 341, 534, 1481, 25801, 689, 1184, 13977, 575, 1080, 1065, 1729, 2017, 13, 51508], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 708, "seek": 288718, "start": 2910.06, "end": 2914.14, "text": " So you can see where these entities appear within the text, as you kind of just naturally", "tokens": [51508, 407, 291, 393, 536, 689, 613, 16667, 4204, 1951, 264, 2487, 11, 382, 291, 733, 295, 445, 8195, 51712], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 709, "seek": 288718, "start": 2914.14, "end": 2915.14, "text": " read it.", "tokens": [51712, 1401, 309, 13, 51762], "temperature": 0.0, "avg_logprob": -0.17556394188149463, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.2627248764038086}, {"id": 710, "seek": 291514, "start": 2915.14, "end": 2919.1, "text": " And you can do this with the text as long as you want, you can even change the max length", "tokens": [50364, 400, 291, 393, 360, 341, 365, 264, 2487, 382, 938, 382, 291, 528, 11, 291, 393, 754, 1319, 264, 11469, 4641, 50562], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 711, "seek": 291514, "start": 2919.1, "end": 2921.7799999999997, "text": " to be more than a million characters long.", "tokens": [50562, 281, 312, 544, 813, 257, 2459, 4342, 938, 13, 50696], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 712, "seek": 291514, "start": 2921.7799999999997, "end": 2926.7799999999997, "text": " And again, we can see right here, org is incorrectly identified as the American Revolutionary War", "tokens": [50696, 400, 797, 11, 321, 393, 536, 558, 510, 11, 14045, 307, 42892, 9234, 382, 264, 2665, 16617, 822, 3630, 50946], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 713, "seek": 291514, "start": 2926.7799999999997, "end": 2931.22, "text": " incorrectly identified as org, but nevertheless, we see really, really good results with a", "tokens": [50946, 42892, 9234, 382, 14045, 11, 457, 26924, 11, 321, 536, 534, 11, 534, 665, 3542, 365, 257, 51168], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 714, "seek": 291514, "start": 2931.22, "end": 2935.2999999999997, "text": " small English model without a lot of custom fine tune training.", "tokens": [51168, 1359, 3669, 2316, 1553, 257, 688, 295, 2375, 2489, 10864, 3097, 13, 51372], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 715, "seek": 291514, "start": 2935.2999999999997, "end": 2936.46, "text": " And there's a reason for this.", "tokens": [51372, 400, 456, 311, 257, 1778, 337, 341, 13, 51430], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 716, "seek": 291514, "start": 2936.46, "end": 2940.8599999999997, "text": " A lot of Wikipedia data gets included into machine learning models.", "tokens": [51430, 316, 688, 295, 28999, 1412, 2170, 5556, 666, 3479, 2539, 5245, 13, 51650], "temperature": 0.0, "avg_logprob": -0.16456406576591626, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.05664099007844925}, {"id": 717, "seek": 294086, "start": 2940.86, "end": 2945.9, "text": " The machine learning models on text typically make good predictions on Wikipedia data, because", "tokens": [50364, 440, 3479, 2539, 5245, 322, 2487, 5850, 652, 665, 21264, 322, 28999, 1412, 11, 570, 50616], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 718, "seek": 294086, "start": 2945.9, "end": 2948.06, "text": " it was included in their training process.", "tokens": [50616, 309, 390, 5556, 294, 641, 3097, 1399, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 719, "seek": 294086, "start": 2948.06, "end": 2950.6200000000003, "text": " Nevertheless, these are still good results.", "tokens": [50724, 26554, 11, 613, 366, 920, 665, 3542, 13, 50852], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 720, "seek": 294086, "start": 2950.6200000000003, "end": 2954.26, "text": " If I'm right or wrong on that, I'm not entirely certain.", "tokens": [50852, 759, 286, 478, 558, 420, 2085, 322, 300, 11, 286, 478, 406, 7696, 1629, 13, 51034], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 721, "seek": 294086, "start": 2954.26, "end": 2958.7000000000003, "text": " But that's going to be how you kind of extract important entities from your text, and most", "tokens": [51034, 583, 300, 311, 516, 281, 312, 577, 291, 733, 295, 8947, 1021, 16667, 490, 428, 2487, 11, 293, 881, 51256], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 722, "seek": 294086, "start": 2958.7000000000003, "end": 2960.58, "text": " importantly visualize it.", "tokens": [51256, 8906, 23273, 309, 13, 51350], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 723, "seek": 294086, "start": 2960.58, "end": 2964.3, "text": " This is where chapter two of my book kind of ends.", "tokens": [51350, 639, 307, 689, 7187, 732, 295, 452, 1446, 733, 295, 5314, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 724, "seek": 294086, "start": 2964.3, "end": 2968.86, "text": " After this chapter, you have a good understanding, hopefully, of kind of what the dot container", "tokens": [51536, 2381, 341, 7187, 11, 291, 362, 257, 665, 3701, 11, 4696, 11, 295, 733, 295, 437, 264, 5893, 10129, 51764], "temperature": 0.0, "avg_logprob": -0.16042387687553794, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.06186588481068611}, {"id": 725, "seek": 296886, "start": 2968.86, "end": 2975.98, "text": " is, what tokens are, and how the doc object contains the attributes such as since and", "tokens": [50364, 307, 11, 437, 22667, 366, 11, 293, 577, 264, 3211, 2657, 8306, 264, 17212, 1270, 382, 1670, 293, 50720], "temperature": 0.0, "avg_logprob": -0.13817485173543295, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.17321015894412994}, {"id": 726, "seek": 296886, "start": 2975.98, "end": 2981.1, "text": " ends, which allows for you to find sentences and entities within a text.", "tokens": [50720, 5314, 11, 597, 4045, 337, 291, 281, 915, 16579, 293, 16667, 1951, 257, 2487, 13, 50976], "temperature": 0.0, "avg_logprob": -0.13817485173543295, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.17321015894412994}, {"id": 727, "seek": 296886, "start": 2981.1, "end": 2987.1800000000003, "text": " Hopefully you also have a good understanding of how to access the linguistic features of", "tokens": [50976, 10429, 291, 611, 362, 257, 665, 3701, 295, 577, 281, 2105, 264, 43002, 4122, 295, 51280], "temperature": 0.0, "avg_logprob": -0.13817485173543295, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.17321015894412994}, {"id": 728, "seek": 296886, "start": 2987.1800000000003, "end": 2990.06, "text": " each token through token attributes.", "tokens": [51280, 1184, 14862, 807, 14862, 17212, 13, 51424], "temperature": 0.0, "avg_logprob": -0.13817485173543295, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.17321015894412994}, {"id": 729, "seek": 296886, "start": 2990.06, "end": 2996.7400000000002, "text": " I encourage you to spend a lot of time becoming familiar with these basics, as these basics", "tokens": [51424, 286, 5373, 291, 281, 3496, 257, 688, 295, 565, 5617, 4963, 365, 613, 14688, 11, 382, 613, 14688, 51758], "temperature": 0.0, "avg_logprob": -0.13817485173543295, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.17321015894412994}, {"id": 730, "seek": 299674, "start": 2996.74, "end": 3001.18, "text": " are the building block for really robust things that we're going to be getting into", "tokens": [50364, 366, 264, 2390, 3461, 337, 534, 13956, 721, 300, 321, 434, 516, 281, 312, 1242, 666, 50586], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 731, "seek": 299674, "start": 3001.18, "end": 3005.7, "text": " in the next few lessons.", "tokens": [50586, 294, 264, 958, 1326, 8820, 13, 50812], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 732, "seek": 299674, "start": 3005.7, "end": 3011.8599999999997, "text": " We're now moving into chapter three of our textbook on Spacey and Python.", "tokens": [50812, 492, 434, 586, 2684, 666, 7187, 1045, 295, 527, 25591, 322, 8705, 88, 293, 15329, 13, 51120], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 733, "seek": 299674, "start": 3011.8599999999997, "end": 3016.9399999999996, "text": " Now in chapter three, we're going to be continuing our theme of part one, where we're trying", "tokens": [51120, 823, 294, 7187, 1045, 11, 321, 434, 516, 281, 312, 9289, 527, 6314, 295, 644, 472, 11, 689, 321, 434, 1382, 51374], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 734, "seek": 299674, "start": 3016.9399999999996, "end": 3020.8999999999996, "text": " to understand the larger building blocks of Spacey.", "tokens": [51374, 281, 1223, 264, 4833, 2390, 8474, 295, 8705, 88, 13, 51572], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 735, "seek": 299674, "start": 3020.8999999999996, "end": 3025.4199999999996, "text": " Even though this video is not going to deal with Spacey machine learning approaches, our", "tokens": [51572, 2754, 1673, 341, 960, 307, 406, 516, 281, 2028, 365, 8705, 88, 3479, 2539, 11587, 11, 527, 51798], "temperature": 0.0, "avg_logprob": -0.15516056060791017, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.16877560317516327}, {"id": 736, "seek": 302542, "start": 3025.46, "end": 3030.26, "text": " custom ones, that is, it's still important to be familiar with what machine learning is", "tokens": [50366, 2375, 2306, 11, 300, 307, 11, 309, 311, 920, 1021, 281, 312, 4963, 365, 437, 3479, 2539, 307, 50606], "temperature": 0.0, "avg_logprob": -0.12698991402335788, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08754070848226547}, {"id": 737, "seek": 302542, "start": 3030.26, "end": 3036.62, "text": " and how it works, specifically with regards to language, because a lot of the Spacey models", "tokens": [50606, 293, 577, 309, 1985, 11, 4682, 365, 14258, 281, 2856, 11, 570, 257, 688, 295, 264, 8705, 88, 5245, 50924], "temperature": 0.0, "avg_logprob": -0.12698991402335788, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08754070848226547}, {"id": 738, "seek": 302542, "start": 3036.62, "end": 3044.06, "text": " such as the medium, large and transformer models, all are machine learning models that", "tokens": [50924, 1270, 382, 264, 6399, 11, 2416, 293, 31782, 5245, 11, 439, 366, 3479, 2539, 5245, 300, 51296], "temperature": 0.0, "avg_logprob": -0.12698991402335788, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08754070848226547}, {"id": 739, "seek": 302542, "start": 3044.06, "end": 3047.62, "text": " have word vectors stored within them.", "tokens": [51296, 362, 1349, 18875, 12187, 1951, 552, 13, 51474], "temperature": 0.0, "avg_logprob": -0.12698991402335788, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08754070848226547}, {"id": 740, "seek": 302542, "start": 3047.62, "end": 3054.42, "text": " This means that they're going to be larger, more accurate, and do the things a bit more", "tokens": [51474, 639, 1355, 300, 436, 434, 516, 281, 312, 4833, 11, 544, 8559, 11, 293, 360, 264, 721, 257, 857, 544, 51814], "temperature": 0.0, "avg_logprob": -0.12698991402335788, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08754070848226547}, {"id": 741, "seek": 305442, "start": 3054.42, "end": 3058.26, "text": " slowly, depending upon its size.", "tokens": [50364, 5692, 11, 5413, 3564, 1080, 2744, 13, 50556], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 742, "seek": 305442, "start": 3058.26, "end": 3063.9, "text": " We're going to be working through not only what kind of machine learning is generally,", "tokens": [50556, 492, 434, 516, 281, 312, 1364, 807, 406, 787, 437, 733, 295, 3479, 2539, 307, 5101, 11, 50838], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 743, "seek": 305442, "start": 3063.9, "end": 3068.54, "text": " but specifically how it works with regards to text.", "tokens": [50838, 457, 4682, 577, 309, 1985, 365, 14258, 281, 2487, 13, 51070], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 744, "seek": 305442, "start": 3068.54, "end": 3073.38, "text": " I think that this is where you're going to find this textbook to be somewhat helpful.", "tokens": [51070, 286, 519, 300, 341, 307, 689, 291, 434, 516, 281, 915, 341, 25591, 281, 312, 8344, 4961, 13, 51312], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 745, "seek": 305442, "start": 3073.38, "end": 3078.82, "text": " What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did", "tokens": [51312, 708, 286, 528, 281, 360, 307, 294, 527, 777, 22125, 88, 391, 11633, 2939, 11, 321, 434, 516, 281, 974, 8705, 88, 445, 382, 321, 630, 51584], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 746, "seek": 305442, "start": 3078.82, "end": 3083.86, "text": " before, but this time we're going to be installing a new model.", "tokens": [51584, 949, 11, 457, 341, 565, 321, 434, 516, 281, 312, 20762, 257, 777, 2316, 13, 51836], "temperature": 0.0, "avg_logprob": -0.16505177221565603, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.0009399217087775469}, {"id": 747, "seek": 308386, "start": 3083.9, "end": 3092.9, "text": " We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then", "tokens": [50366, 492, 434, 516, 281, 360, 15329, 11, 264, 1624, 43233, 1491, 11, 15329, 11, 376, 11, 8705, 88, 11, 5484, 11, 293, 550, 50816], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 748, "seek": 308386, "start": 3092.9, "end": 3097.5, "text": " we're going to download the Ncore Web MD model.", "tokens": [50816, 321, 434, 516, 281, 5484, 264, 426, 12352, 9573, 22521, 2316, 13, 51046], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 749, "seek": 308386, "start": 3097.5, "end": 3099.7000000000003, "text": " This is the medium English model.", "tokens": [51046, 639, 307, 264, 6399, 3669, 2316, 13, 51156], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 750, "seek": 308386, "start": 3099.7000000000003, "end": 3103.58, "text": " This is going to take a little longer to download, and the reason why I'm having you download", "tokens": [51156, 639, 307, 516, 281, 747, 257, 707, 2854, 281, 5484, 11, 293, 264, 1778, 983, 286, 478, 1419, 291, 5484, 51350], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 751, "seek": 308386, "start": 3103.58, "end": 3108.6600000000003, "text": " the medium model, and the reason why we're going to be using the medium model, is because", "tokens": [51350, 264, 6399, 2316, 11, 293, 264, 1778, 983, 321, 434, 516, 281, 312, 1228, 264, 6399, 2316, 11, 307, 570, 51604], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 752, "seek": 308386, "start": 3108.6600000000003, "end": 3113.6200000000003, "text": " the medium model has stored within it word vectors.", "tokens": [51604, 264, 6399, 2316, 575, 12187, 1951, 309, 1349, 18875, 13, 51852], "temperature": 0.0, "avg_logprob": -0.215871846900796, "compression_ratio": 1.9282296650717703, "no_speech_prob": 0.00406992994248867}, {"id": 753, "seek": 311362, "start": 3114.38, "end": 3121.3399999999997, "text": " Let's go ahead and talk a little bit about what word vectors are and how they're useful.", "tokens": [50402, 961, 311, 352, 2286, 293, 751, 257, 707, 857, 466, 437, 1349, 18875, 366, 293, 577, 436, 434, 4420, 13, 50750], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 754, "seek": 311362, "start": 3121.3399999999997, "end": 3124.7, "text": " So word vectors are word embeddings.", "tokens": [50750, 407, 1349, 18875, 366, 1349, 12240, 29432, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 755, "seek": 311362, "start": 3124.7, "end": 3132.8199999999997, "text": " So these are numerical representations of words in multi-dimensional space through matrices.", "tokens": [50918, 407, 613, 366, 29054, 33358, 295, 2283, 294, 4825, 12, 18759, 1901, 807, 32284, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 756, "seek": 311362, "start": 3132.8199999999997, "end": 3135.62, "text": " That's a very compacted sentence.", "tokens": [51324, 663, 311, 257, 588, 14679, 292, 8174, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 757, "seek": 311362, "start": 3135.62, "end": 3137.38, "text": " So let's break it down.", "tokens": [51464, 407, 718, 311, 1821, 309, 760, 13, 51552], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 758, "seek": 311362, "start": 3137.38, "end": 3139.2599999999998, "text": " What are word vectors used for?", "tokens": [51552, 708, 366, 1349, 18875, 1143, 337, 30, 51646], "temperature": 0.0, "avg_logprob": -0.2068648519395273, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006096803117543459}, {"id": 759, "seek": 313926, "start": 3139.26, "end": 3146.2200000000003, "text": " Well, they're used for a computer system to understand what a word actually means.", "tokens": [50364, 1042, 11, 436, 434, 1143, 337, 257, 3820, 1185, 281, 1223, 437, 257, 1349, 767, 1355, 13, 50712], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 760, "seek": 313926, "start": 3146.2200000000003, "end": 3149.7400000000002, "text": " So computers can't really parse text all that efficiently.", "tokens": [50712, 407, 10807, 393, 380, 534, 48377, 2487, 439, 300, 19621, 13, 50888], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 761, "seek": 313926, "start": 3149.7400000000002, "end": 3151.6600000000003, "text": " They can't parse it at all.", "tokens": [50888, 814, 393, 380, 48377, 309, 412, 439, 13, 50984], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 762, "seek": 313926, "start": 3151.6600000000003, "end": 3155.6600000000003, "text": " Every word needs to be converted into some kind of a number.", "tokens": [50984, 2048, 1349, 2203, 281, 312, 16424, 666, 512, 733, 295, 257, 1230, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 763, "seek": 313926, "start": 3155.6600000000003, "end": 3159.5400000000004, "text": " Now for some old approaches, you would use something like a bag of words approach where", "tokens": [51184, 823, 337, 512, 1331, 11587, 11, 291, 576, 764, 746, 411, 257, 3411, 295, 2283, 3109, 689, 51378], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 764, "seek": 313926, "start": 3159.5400000000004, "end": 3163.46, "text": " each individual word would have a corresponding number to it.", "tokens": [51378, 1184, 2609, 1349, 576, 362, 257, 11760, 1230, 281, 309, 13, 51574], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 765, "seek": 313926, "start": 3163.46, "end": 3167.5400000000004, "text": " This would be a unique number that corresponds just to that word.", "tokens": [51574, 639, 576, 312, 257, 3845, 1230, 300, 23249, 445, 281, 300, 1349, 13, 51778], "temperature": 0.0, "avg_logprob": -0.10271000412275207, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0028007314540445805}, {"id": 766, "seek": 316754, "start": 3167.54, "end": 3173.66, "text": " There are a lot of tasks that can work, but for something like text understanding or trying", "tokens": [50364, 821, 366, 257, 688, 295, 9608, 300, 393, 589, 11, 457, 337, 746, 411, 2487, 3701, 420, 1382, 50670], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 767, "seek": 316754, "start": 3173.66, "end": 3179.7799999999997, "text": " to get a computer system to be able to understand how a word functions within a sentence in general,", "tokens": [50670, 281, 483, 257, 3820, 1185, 281, 312, 1075, 281, 1223, 577, 257, 1349, 6828, 1951, 257, 8174, 294, 2674, 11, 50976], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 768, "seek": 316754, "start": 3179.7799999999997, "end": 3184.7, "text": " in other words, how it works in the language, how it relates to all other words, that doesn't", "tokens": [50976, 294, 661, 2283, 11, 577, 309, 1985, 294, 264, 2856, 11, 577, 309, 16155, 281, 439, 661, 2283, 11, 300, 1177, 380, 51222], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 769, "seek": 316754, "start": 3184.7, "end": 3186.86, "text": " really work for us.", "tokens": [51222, 534, 589, 337, 505, 13, 51330], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 770, "seek": 316754, "start": 3186.86, "end": 3191.06, "text": " So what a word vector is, is it's a multi-dimensional representation.", "tokens": [51330, 407, 437, 257, 1349, 8062, 307, 11, 307, 309, 311, 257, 4825, 12, 18759, 10290, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 771, "seek": 316754, "start": 3191.06, "end": 3195.7799999999997, "text": " So instead of a number having just a single integer that corresponds to it, it instead", "tokens": [51540, 407, 2602, 295, 257, 1230, 1419, 445, 257, 2167, 24922, 300, 23249, 281, 309, 11, 309, 2602, 51776], "temperature": 0.0, "avg_logprob": -0.13272855963025773, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0014549921033903956}, {"id": 772, "seek": 319578, "start": 3195.82, "end": 3202.1000000000004, "text": " has what looks like to an unsuspecting eye, essentially.", "tokens": [50366, 575, 437, 1542, 411, 281, 364, 2693, 301, 1043, 278, 3313, 11, 4476, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14678644364879978, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.09803920239210129}, {"id": 773, "seek": 319578, "start": 3202.1000000000004, "end": 3208.3, "text": " It has a very complex sequence of floating numbers that are stored as an array, which", "tokens": [50680, 467, 575, 257, 588, 3997, 8310, 295, 12607, 3547, 300, 366, 12187, 382, 364, 10225, 11, 597, 50990], "temperature": 0.0, "avg_logprob": -0.14678644364879978, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.09803920239210129}, {"id": 774, "seek": 319578, "start": 3208.3, "end": 3214.5800000000004, "text": " is a computationally less expensive form of a list in Python or just computing in general.", "tokens": [50990, 307, 257, 24903, 379, 1570, 5124, 1254, 295, 257, 1329, 294, 15329, 420, 445, 15866, 294, 2674, 13, 51304], "temperature": 0.0, "avg_logprob": -0.14678644364879978, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.09803920239210129}, {"id": 775, "seek": 319578, "start": 3214.5800000000004, "end": 3216.46, "text": " And this is what it looks like, a long sequence.", "tokens": [51304, 400, 341, 307, 437, 309, 1542, 411, 11, 257, 938, 8310, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14678644364879978, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.09803920239210129}, {"id": 776, "seek": 319578, "start": 3216.46, "end": 3222.7000000000003, "text": " In this case, I believe it's a 300-dimensional word that corresponds to a specific word.", "tokens": [51398, 682, 341, 1389, 11, 286, 1697, 309, 311, 257, 6641, 12, 18759, 1349, 300, 23249, 281, 257, 2685, 1349, 13, 51710], "temperature": 0.0, "avg_logprob": -0.14678644364879978, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.09803920239210129}, {"id": 777, "seek": 322270, "start": 3222.7, "end": 3227.8599999999997, "text": " So this is what an array or a word vector or a word embedding looks like.", "tokens": [50364, 407, 341, 307, 437, 364, 10225, 420, 257, 1349, 8062, 420, 257, 1349, 12240, 3584, 1542, 411, 13, 50622], "temperature": 0.0, "avg_logprob": -0.10908291556618431, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.002800733782351017}, {"id": 778, "seek": 322270, "start": 3227.8599999999997, "end": 3233.54, "text": " What this means to a computer system is it means syntactical and semantical meaning.", "tokens": [50622, 708, 341, 1355, 281, 257, 3820, 1185, 307, 309, 1355, 23980, 578, 804, 293, 4361, 394, 804, 3620, 13, 50906], "temperature": 0.0, "avg_logprob": -0.10908291556618431, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.002800733782351017}, {"id": 779, "seek": 322270, "start": 3233.54, "end": 3237.7799999999997, "text": " So the way word vectors are typically trained is, oh, there's a few different approaches,", "tokens": [50906, 407, 264, 636, 1349, 18875, 366, 5850, 8895, 307, 11, 1954, 11, 456, 311, 257, 1326, 819, 11587, 11, 51118], "temperature": 0.0, "avg_logprob": -0.10908291556618431, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.002800733782351017}, {"id": 780, "seek": 322270, "start": 3237.7799999999997, "end": 3243.22, "text": " but kind of the old-school word-to-vec approach is you give a computer system a whole bunch", "tokens": [51118, 457, 733, 295, 264, 1331, 12, 22779, 1349, 12, 1353, 12, 303, 66, 3109, 307, 291, 976, 257, 3820, 1185, 257, 1379, 3840, 51390], "temperature": 0.0, "avg_logprob": -0.10908291556618431, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.002800733782351017}, {"id": 781, "seek": 322270, "start": 3243.22, "end": 3249.2599999999998, "text": " of texts and different smaller, larger collections of texts, and what it does is it reads through", "tokens": [51390, 295, 15765, 293, 819, 4356, 11, 4833, 16641, 295, 15765, 11, 293, 437, 309, 775, 307, 309, 15700, 807, 51692], "temperature": 0.0, "avg_logprob": -0.10908291556618431, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.002800733782351017}, {"id": 782, "seek": 324926, "start": 3249.3, "end": 3255.82, "text": " all of them and figures out how words are used in relation to other words.", "tokens": [50366, 439, 295, 552, 293, 9624, 484, 577, 2283, 366, 1143, 294, 9721, 281, 661, 2283, 13, 50692], "temperature": 0.0, "avg_logprob": -0.08998998330563915, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.03020920790731907}, {"id": 783, "seek": 324926, "start": 3255.82, "end": 3261.0600000000004, "text": " And so what it's able to essentially do through this training process is figure out meaning.", "tokens": [50692, 400, 370, 437, 309, 311, 1075, 281, 4476, 360, 807, 341, 3097, 1399, 307, 2573, 484, 3620, 13, 50954], "temperature": 0.0, "avg_logprob": -0.08998998330563915, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.03020920790731907}, {"id": 784, "seek": 324926, "start": 3261.0600000000004, "end": 3265.94, "text": " And what that meaning allows for a computer system to do is understand how a word might", "tokens": [50954, 400, 437, 300, 3620, 4045, 337, 257, 3820, 1185, 281, 360, 307, 1223, 577, 257, 1349, 1062, 51198], "temperature": 0.0, "avg_logprob": -0.08998998330563915, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.03020920790731907}, {"id": 785, "seek": 324926, "start": 3265.94, "end": 3271.86, "text": " relate to other words within a sentence or within a language as a whole.", "tokens": [51198, 10961, 281, 661, 2283, 1951, 257, 8174, 420, 1951, 257, 2856, 382, 257, 1379, 13, 51494], "temperature": 0.0, "avg_logprob": -0.08998998330563915, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.03020920790731907}, {"id": 786, "seek": 324926, "start": 3271.86, "end": 3276.0600000000004, "text": " And in order to understand this, I think it's best if we move away from this textbook and", "tokens": [51494, 400, 294, 1668, 281, 1223, 341, 11, 286, 519, 309, 311, 1151, 498, 321, 1286, 1314, 490, 341, 25591, 293, 51704], "temperature": 0.0, "avg_logprob": -0.08998998330563915, "compression_ratio": 1.7416666666666667, "no_speech_prob": 0.03020920790731907}, {"id": 787, "seek": 327606, "start": 3276.06, "end": 3280.62, "text": " actually try to explore what word vectors look like in spacey.", "tokens": [50364, 767, 853, 281, 6839, 437, 1349, 18875, 574, 411, 294, 1901, 88, 13, 50592], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 788, "seek": 327606, "start": 3280.62, "end": 3285.2999999999997, "text": " So you can have a better sense of specifically what they do, why they're useful, and how", "tokens": [50592, 407, 291, 393, 362, 257, 1101, 2020, 295, 4682, 437, 436, 360, 11, 983, 436, 434, 4420, 11, 293, 577, 50826], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 789, "seek": 327606, "start": 3285.2999999999997, "end": 3291.58, "text": " you, as a NLP practitioner, can go ahead and start leveraging them.", "tokens": [50826, 291, 11, 382, 257, 426, 45196, 32125, 11, 393, 352, 2286, 293, 722, 32666, 552, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 790, "seek": 327606, "start": 3291.58, "end": 3294.7799999999997, "text": " So just like before, we're going to create an NLP object.", "tokens": [51140, 407, 445, 411, 949, 11, 321, 434, 516, 281, 1884, 364, 426, 45196, 2657, 13, 51300], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 791, "seek": 327606, "start": 3294.7799999999997, "end": 3299.82, "text": " This time, however, instead of loading in our Encore Web SM, we're going to load in", "tokens": [51300, 639, 565, 11, 4461, 11, 2602, 295, 15114, 294, 527, 2193, 12352, 9573, 13115, 11, 321, 434, 516, 281, 3677, 294, 51552], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 792, "seek": 327606, "start": 3299.82, "end": 3302.7799999999997, "text": " our Encore Web MD.", "tokens": [51552, 527, 2193, 12352, 9573, 22521, 13, 51700], "temperature": 0.0, "avg_logprob": -0.16221451532273065, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0032729171216487885}, {"id": 793, "seek": 330278, "start": 3302.78, "end": 3307.82, "text": " So the one that actually has these word vectors stored, the static vectors saved, and it's", "tokens": [50364, 407, 264, 472, 300, 767, 575, 613, 1349, 18875, 12187, 11, 264, 13437, 18875, 6624, 11, 293, 309, 311, 50616], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 794, "seek": 330278, "start": 3307.82, "end": 3309.0600000000004, "text": " going to be a larger model.", "tokens": [50616, 516, 281, 312, 257, 4833, 2316, 13, 50678], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 795, "seek": 330278, "start": 3309.0600000000004, "end": 3311.3, "text": " Let's go ahead and execute that cell.", "tokens": [50678, 961, 311, 352, 2286, 293, 14483, 300, 2815, 13, 50790], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 796, "seek": 330278, "start": 3311.3, "end": 3314.94, "text": " And while that's executing, we're going to start opening up our text.", "tokens": [50790, 400, 1339, 300, 311, 32368, 11, 321, 434, 516, 281, 722, 5193, 493, 527, 2487, 13, 50972], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 797, "seek": 330278, "start": 3314.94, "end": 3323.5, "text": " So we're going to say with open data wiki underscore us.txt, r as f, and we're going", "tokens": [50972, 407, 321, 434, 516, 281, 584, 365, 1269, 1412, 261, 9850, 37556, 505, 13, 83, 734, 11, 367, 382, 283, 11, 293, 321, 434, 516, 51400], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 798, "seek": 330278, "start": 3323.5, "end": 3328.46, "text": " to say text is equal to f.read, so we're going to successfully load in that text file and", "tokens": [51400, 281, 584, 2487, 307, 2681, 281, 283, 13, 2538, 11, 370, 321, 434, 516, 281, 10727, 3677, 294, 300, 2487, 3991, 293, 51648], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 799, "seek": 330278, "start": 3328.46, "end": 3329.46, "text": " open it up.", "tokens": [51648, 1269, 309, 493, 13, 51698], "temperature": 0.0, "avg_logprob": -0.19577065160719015, "compression_ratio": 1.8114035087719298, "no_speech_prob": 0.04335900396108627}, {"id": 800, "seek": 332946, "start": 3329.54, "end": 3333.98, "text": " Then we're going to create our doc object, which will be equal to NLP text.", "tokens": [50368, 1396, 321, 434, 516, 281, 1884, 527, 3211, 2657, 11, 597, 486, 312, 2681, 281, 426, 45196, 2487, 13, 50590], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 801, "seek": 332946, "start": 3333.98, "end": 3337.3, "text": " All the syntax is staying the exact same.", "tokens": [50590, 1057, 264, 28431, 307, 7939, 264, 1900, 912, 13, 50756], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 802, "seek": 332946, "start": 3337.3, "end": 3340.62, "text": " And just like before, let's grab the first sentence.", "tokens": [50756, 400, 445, 411, 949, 11, 718, 311, 4444, 264, 700, 8174, 13, 50922], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 803, "seek": 332946, "start": 3340.62, "end": 3347.54, "text": " So we're going to convert our doc.sense generator into a list, and we're going to grab index", "tokens": [50922, 407, 321, 434, 516, 281, 7620, 527, 3211, 13, 82, 1288, 19265, 666, 257, 1329, 11, 293, 321, 434, 516, 281, 4444, 8186, 51268], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 804, "seek": 332946, "start": 3347.54, "end": 3348.54, "text": " zero.", "tokens": [51268, 4018, 13, 51318], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 805, "seek": 332946, "start": 3348.54, "end": 3353.06, "text": " And let's go ahead and print off sentence one, just so you can kind of see it.", "tokens": [51318, 400, 718, 311, 352, 2286, 293, 4482, 766, 8174, 472, 11, 445, 370, 291, 393, 733, 295, 536, 309, 13, 51544], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 806, "seek": 332946, "start": 3353.06, "end": 3354.7200000000003, "text": " And there it is.", "tokens": [51544, 400, 456, 309, 307, 13, 51627], "temperature": 0.0, "avg_logprob": -0.15085833927370468, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.006097249686717987}, {"id": 807, "seek": 335472, "start": 3354.72, "end": 3360.0, "text": " So now that we've got that kind of in memory, we can start kind of working with it a little", "tokens": [50364, 407, 586, 300, 321, 600, 658, 300, 733, 295, 294, 4675, 11, 321, 393, 722, 733, 295, 1364, 365, 309, 257, 707, 50628], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 808, "seek": 335472, "start": 3360.0, "end": 3361.0, "text": " bit.", "tokens": [50628, 857, 13, 50678], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 809, "seek": 335472, "start": 3361.0, "end": 3366.3999999999996, "text": " So let's go ahead and just start tackling how we can actually use word vectors with", "tokens": [50678, 407, 718, 311, 352, 2286, 293, 445, 722, 34415, 577, 321, 393, 767, 764, 1349, 18875, 365, 50948], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 810, "seek": 335472, "start": 3366.3999999999996, "end": 3368.08, "text": " spacey.", "tokens": [50948, 1901, 88, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 811, "seek": 335472, "start": 3368.08, "end": 3372.72, "text": " So let's kind of think about a general question right now.", "tokens": [51032, 407, 718, 311, 733, 295, 519, 466, 257, 2674, 1168, 558, 586, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 812, "seek": 335472, "start": 3372.72, "end": 3382.7999999999997, "text": " Let's say I wanted to know how the word, let's say country is similar to other words within", "tokens": [51264, 961, 311, 584, 286, 1415, 281, 458, 577, 264, 1349, 11, 718, 311, 584, 1941, 307, 2531, 281, 661, 2283, 1951, 51768], "temperature": 0.0, "avg_logprob": -0.12462267668350883, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.0022517614997923374}, {"id": 813, "seek": 338280, "start": 3382.8, "end": 3385.0800000000004, "text": " our model's word embeddings.", "tokens": [50364, 527, 2316, 311, 1349, 12240, 29432, 13, 50478], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 814, "seek": 338280, "start": 3385.0800000000004, "end": 3387.84, "text": " So let's create a little way we can do this.", "tokens": [50478, 407, 718, 311, 1884, 257, 707, 636, 321, 393, 360, 341, 13, 50616], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 815, "seek": 338280, "start": 3387.84, "end": 3395.04, "text": " We're going to say your word, and this is going to be equal to the word country, country.", "tokens": [50616, 492, 434, 516, 281, 584, 428, 1349, 11, 293, 341, 307, 516, 281, 312, 2681, 281, 264, 1349, 1941, 11, 1941, 13, 50976], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 816, "seek": 338280, "start": 3395.04, "end": 3396.04, "text": " There we go.", "tokens": [50976, 821, 321, 352, 13, 51026], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 817, "seek": 338280, "start": 3396.04, "end": 3400.28, "text": " And what we can do is we can say MS is equal to NLP.", "tokens": [51026, 400, 437, 321, 393, 360, 307, 321, 393, 584, 7395, 307, 2681, 281, 426, 45196, 13, 51238], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 818, "seek": 338280, "start": 3400.28, "end": 3402.6000000000004, "text": " So we're going to go into that NLP object.", "tokens": [51238, 407, 321, 434, 516, 281, 352, 666, 300, 426, 45196, 2657, 13, 51354], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 819, "seek": 338280, "start": 3402.6000000000004, "end": 3410.44, "text": " We're going to grab the vocab.vectors, and we're going to say most similar.", "tokens": [51354, 492, 434, 516, 281, 4444, 264, 2329, 455, 13, 303, 5547, 11, 293, 321, 434, 516, 281, 584, 881, 2531, 13, 51746], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 820, "seek": 338280, "start": 3410.44, "end": 3412.44, "text": " And this is a little complicated way of doing it.", "tokens": [51746, 400, 341, 307, 257, 707, 6179, 636, 295, 884, 309, 13, 51846], "temperature": 0.0, "avg_logprob": -0.15750895864595243, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.020329775288701057}, {"id": 821, "seek": 341244, "start": 3412.44, "end": 3415.52, "text": " In fact, I'm going to go ahead and just kind of copy and paste this in.", "tokens": [50364, 682, 1186, 11, 286, 478, 516, 281, 352, 2286, 293, 445, 733, 295, 5055, 293, 9163, 341, 294, 13, 50518], "temperature": 0.0, "avg_logprob": -0.21671395773416038, "compression_ratio": 1.848314606741573, "no_speech_prob": 0.03731929510831833}, {"id": 822, "seek": 341244, "start": 3415.52, "end": 3421.52, "text": " You have the code already in your textbook that you can follow along with.", "tokens": [50518, 509, 362, 264, 3089, 1217, 294, 428, 25591, 300, 291, 393, 1524, 2051, 365, 13, 50818], "temperature": 0.0, "avg_logprob": -0.21671395773416038, "compression_ratio": 1.848314606741573, "no_speech_prob": 0.03731929510831833}, {"id": 823, "seek": 341244, "start": 3421.52, "end": 3430.96, "text": " And I'm going to go ahead and just copy and paste it in right here and print off this.", "tokens": [50818, 400, 286, 478, 516, 281, 352, 2286, 293, 445, 5055, 293, 9163, 309, 294, 558, 510, 293, 4482, 766, 341, 13, 51290], "temperature": 0.0, "avg_logprob": -0.21671395773416038, "compression_ratio": 1.848314606741573, "no_speech_prob": 0.03731929510831833}, {"id": 824, "seek": 341244, "start": 3430.96, "end": 3435.88, "text": " And what this is going to do is it is going to go ahead and just do this entirely.", "tokens": [51290, 400, 437, 341, 307, 516, 281, 360, 307, 309, 307, 516, 281, 352, 2286, 293, 445, 360, 341, 7696, 13, 51536], "temperature": 0.0, "avg_logprob": -0.21671395773416038, "compression_ratio": 1.848314606741573, "no_speech_prob": 0.03731929510831833}, {"id": 825, "seek": 341244, "start": 3435.88, "end": 3438.68, "text": " There we go.", "tokens": [51536, 821, 321, 352, 13, 51676], "temperature": 0.0, "avg_logprob": -0.21671395773416038, "compression_ratio": 1.848314606741573, "no_speech_prob": 0.03731929510831833}, {"id": 826, "seek": 343868, "start": 3439.68, "end": 3445.52, "text": " And we have to import numpy as MP.", "tokens": [50414, 400, 321, 362, 281, 974, 1031, 8200, 382, 14146, 13, 50706], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 827, "seek": 343868, "start": 3445.52, "end": 3448.7599999999998, "text": " This lets us actually work with the data as a numpy array.", "tokens": [50706, 639, 6653, 505, 767, 589, 365, 264, 1412, 382, 257, 1031, 8200, 10225, 13, 50868], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 828, "seek": 343868, "start": 3448.7599999999998, "end": 3454.04, "text": " And when we execute this cell, what we get is an output that tells us all the words", "tokens": [50868, 400, 562, 321, 14483, 341, 2815, 11, 437, 321, 483, 307, 364, 5598, 300, 5112, 505, 439, 264, 2283, 51132], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 829, "seek": 343868, "start": 3454.04, "end": 3457.2, "text": " that are most similar to the word country.", "tokens": [51132, 300, 366, 881, 2531, 281, 264, 1349, 1941, 13, 51290], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 830, "seek": 343868, "start": 3457.2, "end": 3461.52, "text": " So in this scenario, the word country, it has these kind of all these different similar", "tokens": [51290, 407, 294, 341, 9005, 11, 264, 1349, 1941, 11, 309, 575, 613, 733, 295, 439, 613, 819, 2531, 51506], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 831, "seek": 343868, "start": 3461.52, "end": 3467.44, "text": " words to it from the word country to the word country, capitalized nation, nation.", "tokens": [51506, 2283, 281, 309, 490, 264, 1349, 1941, 281, 264, 1349, 1941, 11, 4238, 1602, 4790, 11, 4790, 13, 51802], "temperature": 0.0, "avg_logprob": -0.14432753768621706, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.003593362867832184}, {"id": 832, "seek": 346744, "start": 3467.44, "end": 3469.96, "text": " Now it's important to understand what you're seeing here.", "tokens": [50364, 823, 309, 311, 1021, 281, 1223, 437, 291, 434, 2577, 510, 13, 50490], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 833, "seek": 346744, "start": 3469.96, "end": 3476.44, "text": " What you're seeing is not necessarily a synonym for the word country, rather what you're seeing", "tokens": [50490, 708, 291, 434, 2577, 307, 406, 4725, 257, 5451, 12732, 337, 264, 1349, 1941, 11, 2831, 437, 291, 434, 2577, 50814], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 834, "seek": 346744, "start": 3476.44, "end": 3480.16, "text": " is are the words that are the most similar.", "tokens": [50814, 307, 366, 264, 2283, 300, 366, 264, 881, 2531, 13, 51000], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 835, "seek": 346744, "start": 3480.16, "end": 3486.56, "text": " Now this can be anything from a synonym to a variant spelling of that word to something", "tokens": [51000, 823, 341, 393, 312, 1340, 490, 257, 5451, 12732, 281, 257, 17501, 22254, 295, 300, 1349, 281, 746, 51320], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 836, "seek": 346744, "start": 3486.56, "end": 3489.7200000000003, "text": " that occurs frequently alongside of it.", "tokens": [51320, 300, 11843, 10374, 12385, 295, 309, 13, 51478], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 837, "seek": 346744, "start": 3489.7200000000003, "end": 3494.64, "text": " So for example, world, while this isn't the same, we would never consider world to be", "tokens": [51478, 407, 337, 1365, 11, 1002, 11, 1339, 341, 1943, 380, 264, 912, 11, 321, 576, 1128, 1949, 1002, 281, 312, 51724], "temperature": 0.0, "avg_logprob": -0.10161809637995049, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.03513852134346962}, {"id": 838, "seek": 349464, "start": 3494.64, "end": 3496.72, "text": " the synonym of country.", "tokens": [50364, 264, 5451, 12732, 295, 1941, 13, 50468], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 839, "seek": 349464, "start": 3496.72, "end": 3501.64, "text": " But what happens is, is syntactically they're used in very similar situations.", "tokens": [50468, 583, 437, 2314, 307, 11, 307, 23980, 578, 984, 436, 434, 1143, 294, 588, 2531, 6851, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 840, "seek": 349464, "start": 3501.64, "end": 3505.44, "text": " So the way you describe a country is sometimes the way you would describe your world, or", "tokens": [50714, 407, 264, 636, 291, 6786, 257, 1941, 307, 2171, 264, 636, 291, 576, 6786, 428, 1002, 11, 420, 50904], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 841, "seek": 349464, "start": 3505.44, "end": 3510.12, "text": " maybe it's something to do with the hierarchy, so a country is found within the world.", "tokens": [50904, 1310, 309, 311, 746, 281, 360, 365, 264, 22333, 11, 370, 257, 1941, 307, 1352, 1951, 264, 1002, 13, 51138], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 842, "seek": 349464, "start": 3510.12, "end": 3511.52, "text": " This is a good way to understand it.", "tokens": [51138, 639, 307, 257, 665, 636, 281, 1223, 309, 13, 51208], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 843, "seek": 349464, "start": 3511.52, "end": 3519.56, "text": " So it's always good to use this word as most similar, not to be something like synonym.", "tokens": [51208, 407, 309, 311, 1009, 665, 281, 764, 341, 1349, 382, 881, 2531, 11, 406, 281, 312, 746, 411, 5451, 12732, 13, 51610], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 844, "seek": 349464, "start": 3519.56, "end": 3523.52, "text": " So when you're talking about word vectors similarity, you're not talking about synonym", "tokens": [51610, 407, 562, 291, 434, 1417, 466, 1349, 18875, 32194, 11, 291, 434, 406, 1417, 466, 5451, 12732, 51808], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 845, "seek": 349464, "start": 3523.52, "end": 3524.52, "text": " similarity.", "tokens": [51808, 32194, 13, 51858], "temperature": 0.0, "avg_logprob": -0.13777596180833232, "compression_ratio": 1.8943396226415095, "no_speech_prob": 0.11916960775852203}, {"id": 846, "seek": 352452, "start": 3525.4, "end": 3527.2, "text": " But this is a way you can kind of quickly get a sense.", "tokens": [50408, 583, 341, 307, 257, 636, 291, 393, 733, 295, 2661, 483, 257, 2020, 13, 50498], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 847, "seek": 352452, "start": 3527.2, "end": 3529.44, "text": " So what does this do for you?", "tokens": [50498, 407, 437, 775, 341, 360, 337, 291, 30, 50610], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 848, "seek": 352452, "start": 3529.44, "end": 3531.96, "text": " Why did I go through and explain all these things about word vectors?", "tokens": [50610, 1545, 630, 286, 352, 807, 293, 2903, 439, 613, 721, 466, 1349, 18875, 30, 50736], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 849, "seek": 352452, "start": 3531.96, "end": 3536.24, "text": " If I'm not going to be talking about machine learning a whole bunch throughout this video.", "tokens": [50736, 759, 286, 478, 406, 516, 281, 312, 1417, 466, 3479, 2539, 257, 1379, 3840, 3710, 341, 960, 13, 50950], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 850, "seek": 352452, "start": 3536.24, "end": 3539.56, "text": " Well, I did it so that you can do one thing that's really important.", "tokens": [50950, 1042, 11, 286, 630, 309, 370, 300, 291, 393, 360, 472, 551, 300, 311, 534, 1021, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 851, "seek": 352452, "start": 3539.56, "end": 3543.04, "text": " And that's calculate document similarity in the spacey.", "tokens": [51116, 400, 300, 311, 8873, 4166, 32194, 294, 264, 1901, 88, 13, 51290], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 852, "seek": 352452, "start": 3543.04, "end": 3545.68, "text": " So we've already got our NLP model loaded up.", "tokens": [51290, 407, 321, 600, 1217, 658, 527, 426, 45196, 2316, 13210, 493, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 853, "seek": 352452, "start": 3545.68, "end": 3547.56, "text": " Let's create one object.", "tokens": [51422, 961, 311, 1884, 472, 2657, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 854, "seek": 352452, "start": 3547.56, "end": 3551.4, "text": " So we're going to make doc one, we're going to make that equal to NLP.", "tokens": [51516, 407, 321, 434, 516, 281, 652, 3211, 472, 11, 321, 434, 516, 281, 652, 300, 2681, 281, 426, 45196, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 855, "seek": 352452, "start": 3551.4, "end": 3554.08, "text": " And we're going to create the text right here in this object.", "tokens": [51708, 400, 321, 434, 516, 281, 1884, 264, 2487, 558, 510, 294, 341, 2657, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1502105394999186, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.12929767370224}, {"id": 856, "seek": 355408, "start": 3554.08, "end": 3558.12, "text": " So let's say this is coming straight from the spacey documentation.", "tokens": [50364, 407, 718, 311, 584, 341, 307, 1348, 2997, 490, 264, 1901, 88, 14333, 13, 50566], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 857, "seek": 355408, "start": 3558.12, "end": 3563.6, "text": " I like salty fries and hamburgers.", "tokens": [50566, 286, 411, 18443, 20733, 293, 25172, 5476, 433, 13, 50840], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 858, "seek": 355408, "start": 3563.6, "end": 3566.7599999999998, "text": " And we're going to say doc two is equal to NLP.", "tokens": [50840, 400, 321, 434, 516, 281, 584, 3211, 732, 307, 2681, 281, 426, 45196, 13, 50998], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 859, "seek": 355408, "start": 3566.7599999999998, "end": 3573.04, "text": " And this is going to be the text fast food tastes very good.", "tokens": [50998, 400, 341, 307, 516, 281, 312, 264, 2487, 2370, 1755, 8666, 588, 665, 13, 51312], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 860, "seek": 355408, "start": 3573.04, "end": 3576.3199999999997, "text": " And now what we can do is let's go ahead and load those into memory.", "tokens": [51312, 400, 586, 437, 321, 393, 360, 307, 718, 311, 352, 2286, 293, 3677, 729, 666, 4675, 13, 51476], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 861, "seek": 355408, "start": 3576.3199999999997, "end": 3580.92, "text": " What we can do is we can actually make a calculation using spacey to find out how similar they", "tokens": [51476, 708, 321, 393, 360, 307, 321, 393, 767, 652, 257, 17108, 1228, 1901, 88, 281, 915, 484, 577, 2531, 436, 51706], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 862, "seek": 355408, "start": 3580.92, "end": 3583.6, "text": " actually are these two different sentences.", "tokens": [51706, 767, 366, 613, 732, 819, 16579, 13, 51840], "temperature": 0.0, "avg_logprob": -0.145542557175095, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.00017952383495867252}, {"id": 863, "seek": 358360, "start": 3583.64, "end": 3589.0, "text": " We can say print off doc one, and we're going to say this again, this is coming straight", "tokens": [50366, 492, 393, 584, 4482, 766, 3211, 472, 11, 293, 321, 434, 516, 281, 584, 341, 797, 11, 341, 307, 1348, 2997, 50634], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 864, "seek": 358360, "start": 3589.0, "end": 3593.24, "text": " from the spacey documentation doc two, so you're going to be able to see what both documents", "tokens": [50634, 490, 264, 1901, 88, 14333, 3211, 732, 11, 370, 291, 434, 516, 281, 312, 1075, 281, 536, 437, 1293, 8512, 50846], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 865, "seek": 358360, "start": 3593.24, "end": 3594.24, "text": " are.", "tokens": [50846, 366, 13, 50896], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 866, "seek": 358360, "start": 3594.24, "end": 3597.08, "text": " And then we're going to do doc one dot similarity.", "tokens": [50896, 400, 550, 321, 434, 516, 281, 360, 3211, 472, 5893, 32194, 13, 51038], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 867, "seek": 358360, "start": 3597.08, "end": 3602.12, "text": " So we can go into the doc one dot similarity method and we can compare it to doc two.", "tokens": [51038, 407, 321, 393, 352, 666, 264, 3211, 472, 5893, 32194, 3170, 293, 321, 393, 6794, 309, 281, 3211, 732, 13, 51290], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 868, "seek": 358360, "start": 3602.12, "end": 3603.92, "text": " We can print that off.", "tokens": [51290, 492, 393, 4482, 300, 766, 13, 51380], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 869, "seek": 358360, "start": 3603.92, "end": 3609.12, "text": " So what we're seeing here on the left is document one, this little divider thing that we printed", "tokens": [51380, 407, 437, 321, 434, 2577, 510, 322, 264, 1411, 307, 4166, 472, 11, 341, 707, 3414, 1438, 551, 300, 321, 13567, 51640], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 870, "seek": 358360, "start": 3609.12, "end": 3610.24, "text": " off here.", "tokens": [51640, 766, 510, 13, 51696], "temperature": 0.0, "avg_logprob": -0.17138671875, "compression_ratio": 1.879668049792531, "no_speech_prob": 0.0011335302842780948}, {"id": 871, "seek": 361024, "start": 3610.24, "end": 3615.3999999999996, "text": " On the right, we have document two, and then we can see the degree of similarity between", "tokens": [50364, 1282, 264, 558, 11, 321, 362, 4166, 732, 11, 293, 550, 321, 393, 536, 264, 4314, 295, 32194, 1296, 50622], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 872, "seek": 361024, "start": 3615.3999999999996, "end": 3617.8399999999997, "text": " document one and document two.", "tokens": [50622, 4166, 472, 293, 4166, 732, 13, 50744], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 873, "seek": 361024, "start": 3617.8399999999997, "end": 3618.8399999999997, "text": " Let's create another doc object.", "tokens": [50744, 961, 311, 1884, 1071, 3211, 2657, 13, 50794], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 874, "seek": 361024, "start": 3618.8399999999997, "end": 3622.9199999999996, "text": " We're going to call this NLP doc three, and we're going to make this NLP.", "tokens": [50794, 492, 434, 516, 281, 818, 341, 426, 45196, 3211, 1045, 11, 293, 321, 434, 516, 281, 652, 341, 426, 45196, 13, 50998], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 875, "seek": 361024, "start": 3622.9199999999996, "end": 3625.24, "text": " Let's come up with a sentence that's completely different.", "tokens": [50998, 961, 311, 808, 493, 365, 257, 8174, 300, 311, 2584, 819, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 876, "seek": 361024, "start": 3625.24, "end": 3632.8399999999997, "text": " The Empire State Building is in New York.", "tokens": [51114, 440, 12197, 4533, 18974, 307, 294, 1873, 3609, 13, 51494], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 877, "seek": 361024, "start": 3632.8399999999997, "end": 3636.64, "text": " So this is when I'm just making up off the top of my head right now.", "tokens": [51494, 407, 341, 307, 562, 286, 478, 445, 1455, 493, 766, 264, 1192, 295, 452, 1378, 558, 586, 13, 51684], "temperature": 0.0, "avg_logprob": -0.15254057778252494, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.01406291127204895}, {"id": 878, "seek": 363664, "start": 3636.64, "end": 3642.24, "text": " I'm going to copy and paste this down, and we're going to compare this to doc one.", "tokens": [50364, 286, 478, 516, 281, 5055, 293, 9163, 341, 760, 11, 293, 321, 434, 516, 281, 6794, 341, 281, 3211, 472, 13, 50644], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 879, "seek": 363664, "start": 3642.24, "end": 3646.16, "text": " We're going to compare it to doc three, and we get a score of point five one.", "tokens": [50644, 492, 434, 516, 281, 6794, 309, 281, 3211, 1045, 11, 293, 321, 483, 257, 6175, 295, 935, 1732, 472, 13, 50840], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 880, "seek": 363664, "start": 3646.16, "end": 3650.4, "text": " So this is less similar to than these two.", "tokens": [50840, 407, 341, 307, 1570, 2531, 281, 813, 613, 732, 13, 51052], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 881, "seek": 363664, "start": 3650.4, "end": 3652.8799999999997, "text": " So this is a way that you can take a whole bunch of documents.", "tokens": [51052, 407, 341, 307, 257, 636, 300, 291, 393, 747, 257, 1379, 3840, 295, 8512, 13, 51176], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 882, "seek": 363664, "start": 3652.8799999999997, "end": 3656.7599999999998, "text": " You can create a simple for loop, and you can find and start clustering the documents", "tokens": [51176, 509, 393, 1884, 257, 2199, 337, 6367, 11, 293, 291, 393, 915, 293, 722, 596, 48673, 264, 8512, 51370], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 883, "seek": 363664, "start": 3656.7599999999998, "end": 3659.8799999999997, "text": " that have a lot of overlap or similarity.", "tokens": [51370, 300, 362, 257, 688, 295, 19959, 420, 32194, 13, 51526], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 884, "seek": 363664, "start": 3659.8799999999997, "end": 3661.7599999999998, "text": " How is this similarity being calculated?", "tokens": [51526, 1012, 307, 341, 32194, 885, 15598, 30, 51620], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 885, "seek": 363664, "start": 3661.7599999999998, "end": 3666.08, "text": " Well, it's being calculated because what spacey is doing is it's going into its word", "tokens": [51620, 1042, 11, 309, 311, 885, 15598, 570, 437, 1901, 88, 307, 884, 307, 309, 311, 516, 666, 1080, 1349, 51836], "temperature": 0.0, "avg_logprob": -0.13751276392136177, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.09806189686059952}, {"id": 886, "seek": 366608, "start": 3666.08, "end": 3671.36, "text": " embeddings, and even though in these two situations, we're not using the word fast", "tokens": [50364, 12240, 29432, 11, 293, 754, 1673, 294, 613, 732, 6851, 11, 321, 434, 406, 1228, 264, 1349, 2370, 50628], "temperature": 0.0, "avg_logprob": -0.14330430374932043, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.013220489956438541}, {"id": 887, "seek": 366608, "start": 3671.36, "end": 3674.0, "text": " food ever in this document.", "tokens": [50628, 1755, 1562, 294, 341, 4166, 13, 50760], "temperature": 0.0, "avg_logprob": -0.14330430374932043, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.013220489956438541}, {"id": 888, "seek": 366608, "start": 3674.0, "end": 3679.6, "text": " It's going in and it knows that salty fries and hamburgers are probably in a close cluster", "tokens": [50760, 467, 311, 516, 294, 293, 309, 3255, 300, 18443, 20733, 293, 25172, 5476, 433, 366, 1391, 294, 257, 1998, 13630, 51040], "temperature": 0.0, "avg_logprob": -0.14330430374932043, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.013220489956438541}, {"id": 889, "seek": 366608, "start": 3679.6, "end": 3686.24, "text": " with the biogram or a token that's made up of two words, a biogram of fast food.", "tokens": [51040, 365, 264, 3228, 12820, 420, 257, 14862, 300, 311, 1027, 493, 295, 732, 2283, 11, 257, 3228, 12820, 295, 2370, 1755, 13, 51372], "temperature": 0.0, "avg_logprob": -0.14330430374932043, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.013220489956438541}, {"id": 890, "seek": 366608, "start": 3686.24, "end": 3691.24, "text": " So what it's doing is it's assigning a prediction that these two are still somewhat similar,", "tokens": [51372, 407, 437, 309, 311, 884, 307, 309, 311, 49602, 257, 17630, 300, 613, 732, 366, 920, 8344, 2531, 11, 51622], "temperature": 0.0, "avg_logprob": -0.14330430374932043, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.013220489956438541}, {"id": 891, "seek": 369124, "start": 3691.24, "end": 3696.6, "text": " more similar than these two, because of these overlapping in words.", "tokens": [50364, 544, 2531, 813, 613, 732, 11, 570, 295, 613, 33535, 294, 2283, 13, 50632], "temperature": 0.0, "avg_logprob": -0.11834534811317374, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.3275623619556427}, {"id": 892, "seek": 369124, "start": 3696.6, "end": 3701.14, "text": " So let's try one more example, see if we can get something that's really, really close.", "tokens": [50632, 407, 718, 311, 853, 472, 544, 1365, 11, 536, 498, 321, 393, 483, 746, 300, 311, 534, 11, 534, 1998, 13, 50859], "temperature": 0.0, "avg_logprob": -0.11834534811317374, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.3275623619556427}, {"id": 893, "seek": 369124, "start": 3701.14, "end": 3709.8399999999997, "text": " So let's take doc four, and this is going to be equal to NLP, I enjoy oranges.", "tokens": [50859, 407, 718, 311, 747, 3211, 1451, 11, 293, 341, 307, 516, 281, 312, 2681, 281, 426, 45196, 11, 286, 2103, 35474, 13, 51294], "temperature": 0.0, "avg_logprob": -0.11834534811317374, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.3275623619556427}, {"id": 894, "seek": 369124, "start": 3709.8399999999997, "end": 3715.4799999999996, "text": " And then we're going to have doc five is going to be equal to NLP, I enjoy apples.", "tokens": [51294, 400, 550, 321, 434, 516, 281, 362, 3211, 1732, 307, 516, 281, 312, 2681, 281, 426, 45196, 11, 286, 2103, 16814, 13, 51576], "temperature": 0.0, "avg_logprob": -0.11834534811317374, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.3275623619556427}, {"id": 895, "seek": 369124, "start": 3715.4799999999996, "end": 3720.3999999999996, "text": " So two, I would agree, I would argue very, very syntactically similar sentences.", "tokens": [51576, 407, 732, 11, 286, 576, 3986, 11, 286, 576, 9695, 588, 11, 588, 23980, 578, 984, 2531, 16579, 13, 51822], "temperature": 0.0, "avg_logprob": -0.11834534811317374, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.3275623619556427}, {"id": 896, "seek": 372040, "start": 3720.4, "end": 3725.2000000000003, "text": " And we're going to do doc four here, doc five here, and we're going to look and see", "tokens": [50364, 400, 321, 434, 516, 281, 360, 3211, 1451, 510, 11, 3211, 1732, 510, 11, 293, 321, 434, 516, 281, 574, 293, 536, 50604], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 897, "seek": 372040, "start": 3725.2000000000003, "end": 3728.0, "text": " a similarity between doc four and doc five.", "tokens": [50604, 257, 32194, 1296, 3211, 1451, 293, 3211, 1732, 13, 50744], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 898, "seek": 372040, "start": 3728.0, "end": 3733.0, "text": " And if we execute this, we get a similarity of 0.96.", "tokens": [50744, 400, 498, 321, 14483, 341, 11, 321, 483, 257, 32194, 295, 1958, 13, 22962, 13, 50994], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 899, "seek": 372040, "start": 3733.0, "end": 3734.6, "text": " So this is really high.", "tokens": [50994, 407, 341, 307, 534, 1090, 13, 51074], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 900, "seek": 372040, "start": 3734.6, "end": 3738.6, "text": " This is telling me that these two sentences are very similar, and it's not just that they're", "tokens": [51074, 639, 307, 3585, 385, 300, 613, 732, 16579, 366, 588, 2531, 11, 293, 309, 311, 406, 445, 300, 436, 434, 51274], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 901, "seek": 372040, "start": 3738.6, "end": 3745.1600000000003, "text": " similar because of the similar syntax here, that's definitely pushing the number up.", "tokens": [51274, 2531, 570, 295, 264, 2531, 28431, 510, 11, 300, 311, 2138, 7380, 264, 1230, 493, 13, 51602], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 902, "seek": 372040, "start": 3745.1600000000003, "end": 3749.2000000000003, "text": " It's that what the individual is liking in the scenario between these two texts, they're", "tokens": [51602, 467, 311, 300, 437, 264, 2609, 307, 16933, 294, 264, 9005, 1296, 613, 732, 15765, 11, 436, 434, 51804], "temperature": 0.0, "avg_logprob": -0.11933533871760134, "compression_ratio": 1.8470588235294119, "no_speech_prob": 0.06951414793729782}, {"id": 903, "seek": 374920, "start": 3749.2, "end": 3750.72, "text": " both fruits.", "tokens": [50364, 1293, 12148, 13, 50440], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 904, "seek": 374920, "start": 3750.72, "end": 3751.72, "text": " Let's try something different.", "tokens": [50440, 961, 311, 853, 746, 819, 13, 50490], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 905, "seek": 374920, "start": 3751.72, "end": 3753.9199999999996, "text": " Let's make doc five.", "tokens": [50490, 961, 311, 652, 3211, 1732, 13, 50600], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 906, "seek": 374920, "start": 3753.9199999999996, "end": 3762.24, "text": " Let's just make doc six here, and do something like this NLP, I enjoy, what's another word", "tokens": [50600, 961, 311, 445, 652, 3211, 2309, 510, 11, 293, 360, 746, 411, 341, 426, 45196, 11, 286, 2103, 11, 437, 311, 1071, 1349, 51016], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 907, "seek": 374920, "start": 3762.24, "end": 3764.7999999999997, "text": " we could say.", "tokens": [51016, 321, 727, 584, 13, 51144], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 908, "seek": 374920, "start": 3764.7999999999997, "end": 3769.9199999999996, "text": " Something that's different, let's say burgers, something different from a fruit.", "tokens": [51144, 6595, 300, 311, 819, 11, 718, 311, 584, 28403, 11, 746, 819, 490, 257, 6773, 13, 51400], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 909, "seek": 374920, "start": 3769.9199999999996, "end": 3774.08, "text": " So we're going to make doc six like that, and we're going to again copy and paste this", "tokens": [51400, 407, 321, 434, 516, 281, 652, 3211, 2309, 411, 300, 11, 293, 321, 434, 516, 281, 797, 5055, 293, 9163, 341, 51608], "temperature": 0.0, "avg_logprob": -0.19750117251747532, "compression_ratio": 1.8216216216216217, "no_speech_prob": 0.10371376574039459}, {"id": 910, "seek": 377408, "start": 3774.08, "end": 3781.4, "text": " down, copy and paste this down, we're going to put doc six here.", "tokens": [50364, 760, 11, 5055, 293, 9163, 341, 760, 11, 321, 434, 516, 281, 829, 3211, 2309, 510, 13, 50730], "temperature": 0.0, "avg_logprob": -0.12271047192950581, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.34845834970474243}, {"id": 911, "seek": 377408, "start": 3781.4, "end": 3783.0, "text": " And we see this drop.", "tokens": [50730, 400, 321, 536, 341, 3270, 13, 50810], "temperature": 0.0, "avg_logprob": -0.12271047192950581, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.34845834970474243}, {"id": 912, "seek": 377408, "start": 3783.0, "end": 3788.0, "text": " So what this demonstrates, and I'm really glad this worked because I improvised this,", "tokens": [50810, 407, 437, 341, 31034, 11, 293, 286, 478, 534, 5404, 341, 2732, 570, 286, 2530, 24420, 341, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12271047192950581, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.34845834970474243}, {"id": 913, "seek": 377408, "start": 3788.0, "end": 3794.84, "text": " what this demonstrates is that the similarity, the number that's given is not dependent on", "tokens": [51060, 437, 341, 31034, 307, 300, 264, 32194, 11, 264, 1230, 300, 311, 2212, 307, 406, 12334, 322, 51402], "temperature": 0.0, "avg_logprob": -0.12271047192950581, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.34845834970474243}, {"id": 914, "seek": 377408, "start": 3794.84, "end": 3803.08, "text": " the contextual words, rather it's dependent upon the semantic similarity of the words.", "tokens": [51402, 264, 35526, 2283, 11, 2831, 309, 311, 12334, 3564, 264, 47982, 32194, 295, 264, 2283, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12271047192950581, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.34845834970474243}, {"id": 915, "seek": 380308, "start": 3803.08, "end": 3811.16, "text": " So apples and oranges are in a similar cluster around fruit because of their word embeddings.", "tokens": [50364, 407, 16814, 293, 35474, 366, 294, 257, 2531, 13630, 926, 6773, 570, 295, 641, 1349, 12240, 29432, 13, 50768], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 916, "seek": 380308, "start": 3811.16, "end": 3817.72, "text": " The word burgers while still being food and still being plural is different from apples", "tokens": [50768, 440, 1349, 28403, 1339, 920, 885, 1755, 293, 920, 885, 25377, 307, 819, 490, 16814, 51096], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 917, "seek": 380308, "start": 3817.72, "end": 3818.96, "text": " and oranges.", "tokens": [51096, 293, 35474, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 918, "seek": 380308, "start": 3818.96, "end": 3822.96, "text": " So in other words, this similarity is being calculated based on something that we humans", "tokens": [51158, 407, 294, 661, 2283, 11, 341, 32194, 307, 885, 15598, 2361, 322, 746, 300, 321, 6255, 51358], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 919, "seek": 380308, "start": 3822.96, "end": 3829.0, "text": " would calculate difference in meaning based on a large understanding of a language as", "tokens": [51358, 576, 8873, 2649, 294, 3620, 2361, 322, 257, 2416, 3701, 295, 257, 2856, 382, 51660], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 920, "seek": 380308, "start": 3829.0, "end": 3830.16, "text": " a whole.", "tokens": [51660, 257, 1379, 13, 51718], "temperature": 0.0, "avg_logprob": -0.10812066549278167, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0362120121717453}, {"id": 921, "seek": 383016, "start": 3830.16, "end": 3834.52, "text": " That's where word vectors really come into play.", "tokens": [50364, 663, 311, 689, 1349, 18875, 534, 808, 666, 862, 13, 50582], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 922, "seek": 383016, "start": 3834.52, "end": 3837.24, "text": " This allows for you to calculate other things as well.", "tokens": [50582, 639, 4045, 337, 291, 281, 8873, 661, 721, 382, 731, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 923, "seek": 383016, "start": 3837.24, "end": 3842.72, "text": " So you could even calculate the difference between salty fries and hamburgers, for example,", "tokens": [50718, 407, 291, 727, 754, 8873, 264, 2649, 1296, 18443, 20733, 293, 25172, 5476, 433, 11, 337, 1365, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 924, "seek": 383016, "start": 3842.72, "end": 3847.8399999999997, "text": " I've got this example ready to go in the textbook, let's go ahead and try this as well.", "tokens": [50992, 286, 600, 658, 341, 1365, 1919, 281, 352, 294, 264, 25591, 11, 718, 311, 352, 2286, 293, 853, 341, 382, 731, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 925, "seek": 383016, "start": 3847.8399999999997, "end": 3853.8799999999997, "text": " So we're going to grab doc one, and print off these few things right here.", "tokens": [51248, 407, 321, 434, 516, 281, 4444, 3211, 472, 11, 293, 4482, 766, 613, 1326, 721, 558, 510, 13, 51550], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 926, "seek": 383016, "start": 3853.8799999999997, "end": 3858.3999999999996, "text": " So we're going to try to calculate the similarity between french fries and burgers and what", "tokens": [51550, 407, 321, 434, 516, 281, 853, 281, 8873, 264, 32194, 1296, 27598, 20733, 293, 28403, 293, 437, 51776], "temperature": 0.0, "avg_logprob": -0.1216598857532848, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04884660243988037}, {"id": 927, "seek": 385840, "start": 3858.4, "end": 3863.36, "text": " we get is a similarity of 0.73.", "tokens": [50364, 321, 483, 307, 257, 32194, 295, 1958, 13, 33396, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14989013297885073, "compression_ratio": 1.7478632478632479, "no_speech_prob": 0.2939499318599701}, {"id": 928, "seek": 385840, "start": 3863.36, "end": 3868.88, "text": " So if we were to maybe change this up a little bit and try to calculate the similarity between", "tokens": [50612, 407, 498, 321, 645, 281, 1310, 1319, 341, 493, 257, 707, 857, 293, 853, 281, 8873, 264, 32194, 1296, 50888], "temperature": 0.0, "avg_logprob": -0.14989013297885073, "compression_ratio": 1.7478632478632479, "no_speech_prob": 0.2939499318599701}, {"id": 929, "seek": 385840, "start": 3868.88, "end": 3877.56, "text": " maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity.", "tokens": [50888, 1310, 445, 264, 1349, 28403, 2831, 813, 25172, 5476, 433, 293, 25172, 5476, 433, 11, 321, 1116, 362, 257, 709, 2946, 32194, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14989013297885073, "compression_ratio": 1.7478632478632479, "no_speech_prob": 0.2939499318599701}, {"id": 930, "seek": 385840, "start": 3877.56, "end": 3882.0, "text": " So my point is, is play around with the similarity calculator, play around with the structure,", "tokens": [51322, 407, 452, 935, 307, 11, 307, 862, 926, 365, 264, 32194, 24993, 11, 862, 926, 365, 264, 3877, 11, 51544], "temperature": 0.0, "avg_logprob": -0.14989013297885073, "compression_ratio": 1.7478632478632479, "no_speech_prob": 0.2939499318599701}, {"id": 931, "seek": 385840, "start": 3882.0, "end": 3887.92, "text": " the code I provided here, and get familiar with how spacey can help you kind of find", "tokens": [51544, 264, 3089, 286, 5649, 510, 11, 293, 483, 4963, 365, 577, 1901, 88, 393, 854, 291, 733, 295, 915, 51840], "temperature": 0.0, "avg_logprob": -0.14989013297885073, "compression_ratio": 1.7478632478632479, "no_speech_prob": 0.2939499318599701}, {"id": 932, "seek": 388792, "start": 3887.92, "end": 3892.28, "text": " a similarity, not just between documents, but between words as well.", "tokens": [50364, 257, 32194, 11, 406, 445, 1296, 8512, 11, 457, 1296, 2283, 382, 731, 13, 50582], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 933, "seek": 388792, "start": 3892.28, "end": 3895.96, "text": " And we're going to be seeing how this is useful later on.", "tokens": [50582, 400, 321, 434, 516, 281, 312, 2577, 577, 341, 307, 4420, 1780, 322, 13, 50766], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 934, "seek": 388792, "start": 3895.96, "end": 3899.84, "text": " But again, it's good to be familiar with kind of generally how machine learning kind of", "tokens": [50766, 583, 797, 11, 309, 311, 665, 281, 312, 4963, 365, 733, 295, 5101, 577, 3479, 2539, 733, 295, 50960], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 935, "seek": 388792, "start": 3899.84, "end": 3905.2000000000003, "text": " functions here in this context, and why these medium and large models are so much bigger.", "tokens": [50960, 6828, 510, 294, 341, 4319, 11, 293, 983, 613, 6399, 293, 2416, 5245, 366, 370, 709, 3801, 13, 51228], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 936, "seek": 388792, "start": 3905.2000000000003, "end": 3910.0, "text": " They're so much bigger because they have more word vectors that are much deeper.", "tokens": [51228, 814, 434, 370, 709, 3801, 570, 436, 362, 544, 1349, 18875, 300, 366, 709, 7731, 13, 51468], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 937, "seek": 388792, "start": 3910.0, "end": 3915.08, "text": " And the transformer model is much larger because it was trained in a completely different method", "tokens": [51468, 400, 264, 31782, 2316, 307, 709, 4833, 570, 309, 390, 8895, 294, 257, 2584, 819, 3170, 51722], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 938, "seek": 388792, "start": 3915.08, "end": 3917.6800000000003, "text": " than the way the medium and large models were trained.", "tokens": [51722, 813, 264, 636, 264, 6399, 293, 2416, 5245, 645, 8895, 13, 51852], "temperature": 0.0, "avg_logprob": -0.10849288227112312, "compression_ratio": 1.79, "no_speech_prob": 0.026753531768918037}, {"id": 939, "seek": 391768, "start": 3917.68, "end": 3922.3599999999997, "text": " But again, that's out of the scope for this video.", "tokens": [50364, 583, 797, 11, 300, 311, 484, 295, 264, 11923, 337, 341, 960, 13, 50598], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 940, "seek": 391768, "start": 3922.3599999999997, "end": 3928.12, "text": " I now want to turn to really the last subject of this introduction to spacey part one, which", "tokens": [50598, 286, 586, 528, 281, 1261, 281, 534, 264, 1036, 3983, 295, 341, 9339, 281, 1901, 88, 644, 472, 11, 597, 50886], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 941, "seek": 391768, "start": 3928.12, "end": 3932.12, "text": " is when we're taking this large umbrella view of the spacey.", "tokens": [50886, 307, 562, 321, 434, 1940, 341, 2416, 21925, 1910, 295, 264, 1901, 88, 13, 51086], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 942, "seek": 391768, "start": 3932.12, "end": 3935.48, "text": " And in the textbook, it's going to correspond to chapter four.", "tokens": [51086, 400, 294, 264, 25591, 11, 309, 311, 516, 281, 6805, 281, 7187, 1451, 13, 51254], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 943, "seek": 391768, "start": 3935.48, "end": 3941.3199999999997, "text": " So what we go over in this textbook is kind of a large view of not just the dot container", "tokens": [51254, 407, 437, 321, 352, 670, 294, 341, 25591, 307, 733, 295, 257, 2416, 1910, 295, 406, 445, 264, 5893, 10129, 51546], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 944, "seek": 391768, "start": 3941.3199999999997, "end": 3946.7599999999998, "text": " and the word vectors and the linguistic annotations, but really kind of the structure of the spacey", "tokens": [51546, 293, 264, 1349, 18875, 293, 264, 43002, 25339, 763, 11, 457, 534, 733, 295, 264, 3877, 295, 264, 1901, 88, 51818], "temperature": 0.0, "avg_logprob": -0.14277579266092053, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.00857628509402275}, {"id": 945, "seek": 394676, "start": 3946.88, "end": 3950.2400000000002, "text": " framework, which comes around the pipeline.", "tokens": [50370, 8388, 11, 597, 1487, 926, 264, 15517, 13, 50538], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 946, "seek": 394676, "start": 3950.2400000000002, "end": 3956.28, "text": " So a pipeline is a very common expression in computer science and in data science.", "tokens": [50538, 407, 257, 15517, 307, 257, 588, 2689, 6114, 294, 3820, 3497, 293, 294, 1412, 3497, 13, 50840], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 947, "seek": 394676, "start": 3956.28, "end": 3959.6800000000003, "text": " Think of it as a traditional pipeline that you would see in a house.", "tokens": [50840, 6557, 295, 309, 382, 257, 5164, 15517, 300, 291, 576, 536, 294, 257, 1782, 13, 51010], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 948, "seek": 394676, "start": 3959.6800000000003, "end": 3964.76, "text": " Now think of a pipeline being a sequence of different pipes.", "tokens": [51010, 823, 519, 295, 257, 15517, 885, 257, 8310, 295, 819, 21882, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 949, "seek": 394676, "start": 3964.76, "end": 3971.2000000000003, "text": " Each pipe in a computer system is going to perform some kind of permutation or some action", "tokens": [51264, 6947, 11240, 294, 257, 3820, 1185, 307, 516, 281, 2042, 512, 733, 295, 4784, 11380, 420, 512, 3069, 51586], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 950, "seek": 394676, "start": 3971.2000000000003, "end": 3976.28, "text": " on a piece of data as it goes through the pipeline.", "tokens": [51586, 322, 257, 2522, 295, 1412, 382, 309, 1709, 807, 264, 15517, 13, 51840], "temperature": 0.0, "avg_logprob": -0.11826945856997842, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.307003378868103}, {"id": 951, "seek": 397628, "start": 3976.28, "end": 3981.92, "text": " And as each pipe has a chance to act and make changes to and additions to that data,", "tokens": [50364, 400, 382, 1184, 11240, 575, 257, 2931, 281, 605, 293, 652, 2962, 281, 293, 35113, 281, 300, 1412, 11, 50646], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 952, "seek": 397628, "start": 3981.92, "end": 3985.44, "text": " the later pipes get to benefit from those changes.", "tokens": [50646, 264, 1780, 21882, 483, 281, 5121, 490, 729, 2962, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 953, "seek": 397628, "start": 3985.44, "end": 3988.32, "text": " So this is very common when you're thinking about logic of code.", "tokens": [50822, 407, 341, 307, 588, 2689, 562, 291, 434, 1953, 466, 9952, 295, 3089, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 954, "seek": 397628, "start": 3988.32, "end": 3992.2400000000002, "text": " I provide it like a little image here that I think maybe might help you.", "tokens": [50966, 286, 2893, 309, 411, 257, 707, 3256, 510, 300, 286, 519, 1310, 1062, 854, 291, 13, 51162], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 955, "seek": 397628, "start": 3992.2400000000002, "end": 3998.6000000000004, "text": " So if we imagine some input sentence, right, so some input text is entering a spacey pipeline,", "tokens": [51162, 407, 498, 321, 3811, 512, 4846, 8174, 11, 558, 11, 370, 512, 4846, 2487, 307, 11104, 257, 1901, 88, 15517, 11, 51480], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 956, "seek": 397628, "start": 3998.6000000000004, "end": 4001.44, "text": " it's going to go through a bunch of things if you're working with the medium model or", "tokens": [51480, 309, 311, 516, 281, 352, 807, 257, 3840, 295, 721, 498, 291, 434, 1364, 365, 264, 6399, 2316, 420, 51622], "temperature": 0.0, "avg_logprob": -0.1270731276115485, "compression_ratio": 1.6691176470588236, "no_speech_prob": 0.002251645317301154}, {"id": 957, "seek": 400144, "start": 4001.48, "end": 4007.28, "text": " the small model, that'll tokenize it and give it a word and vector for different words.", "tokens": [50366, 264, 1359, 2316, 11, 300, 603, 14862, 1125, 309, 293, 976, 309, 257, 1349, 293, 8062, 337, 819, 2283, 13, 50656], "temperature": 0.0, "avg_logprob": -0.15788663350618803, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04884805157780647}, {"id": 958, "seek": 400144, "start": 4007.28, "end": 4013.0, "text": " It'll also find the POS, the part of speech, the dependency parser will act on it.", "tokens": [50656, 467, 603, 611, 915, 264, 430, 4367, 11, 264, 644, 295, 6218, 11, 264, 33621, 21156, 260, 486, 605, 322, 309, 13, 50942], "temperature": 0.0, "avg_logprob": -0.15788663350618803, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04884805157780647}, {"id": 959, "seek": 400144, "start": 4013.0, "end": 4020.04, "text": " But it might eventually get to an entity ruler, which we're going to see in just a few minutes.", "tokens": [50942, 583, 309, 1062, 4728, 483, 281, 364, 13977, 19661, 11, 597, 321, 434, 516, 281, 536, 294, 445, 257, 1326, 2077, 13, 51294], "temperature": 0.0, "avg_logprob": -0.15788663350618803, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04884805157780647}, {"id": 960, "seek": 400144, "start": 4020.04, "end": 4025.4, "text": " The entity ruler will be a series of rules-based NER named entity recognition.", "tokens": [51294, 440, 13977, 19661, 486, 312, 257, 2638, 295, 4474, 12, 6032, 426, 1598, 4926, 13977, 11150, 13, 51562], "temperature": 0.0, "avg_logprob": -0.15788663350618803, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04884805157780647}, {"id": 961, "seek": 400144, "start": 4025.4, "end": 4029.88, "text": " So it'll maybe assign a token to an entity.", "tokens": [51562, 407, 309, 603, 1310, 6269, 257, 14862, 281, 364, 13977, 13, 51786], "temperature": 0.0, "avg_logprob": -0.15788663350618803, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04884805157780647}, {"id": 962, "seek": 402988, "start": 4029.88, "end": 4033.32, "text": " Might be the beginning of an entity, might be the end of an entity,", "tokens": [50364, 23964, 312, 264, 2863, 295, 364, 13977, 11, 1062, 312, 264, 917, 295, 364, 13977, 11, 50536], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 963, "seek": 402988, "start": 4033.32, "end": 4035.96, "text": " might just be an individual token entity.", "tokens": [50536, 1062, 445, 312, 364, 2609, 14862, 13977, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 964, "seek": 402988, "start": 4035.96, "end": 4041.96, "text": " And then what will happen is that doc object, as it kind of goes through this pipeline,", "tokens": [50668, 400, 550, 437, 486, 1051, 307, 300, 3211, 2657, 11, 382, 309, 733, 295, 1709, 807, 341, 15517, 11, 50968], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 965, "seek": 402988, "start": 4041.96, "end": 4045.36, "text": " will now receive a bunch of doc.ins.", "tokens": [50968, 486, 586, 4774, 257, 3840, 295, 3211, 13, 1292, 13, 51138], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 966, "seek": 402988, "start": 4045.36, "end": 4051.6, "text": " So it'll be, this pipe will actually add to the doc object as it goes through the pipeline,", "tokens": [51138, 407, 309, 603, 312, 11, 341, 11240, 486, 767, 909, 281, 264, 3211, 2657, 382, 309, 1709, 807, 264, 15517, 11, 51450], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 967, "seek": 402988, "start": 4051.6, "end": 4053.2400000000002, "text": " the entity component.", "tokens": [51450, 264, 13977, 6542, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 968, "seek": 402988, "start": 4053.2400000000002, "end": 4058.12, "text": " And then the next pipeline, the entity linker, might take all those entities and try to find out", "tokens": [51532, 400, 550, 264, 958, 15517, 11, 264, 13977, 2113, 260, 11, 1062, 747, 439, 729, 16667, 293, 853, 281, 915, 484, 51776], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 969, "seek": 402988, "start": 4058.12, "end": 4059.4, "text": " which ones they are.", "tokens": [51776, 597, 2306, 436, 366, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1567756465223969, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.006691980641335249}, {"id": 970, "seek": 405940, "start": 4059.4, "end": 4065.12, "text": " So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number", "tokens": [50364, 407, 309, 603, 18349, 312, 4582, 281, 512, 733, 295, 261, 9850, 1412, 11, 512, 733, 295, 31677, 1230, 50650], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 971, "seek": 405940, "start": 4065.12, "end": 4067.32, "text": " that corresponds to a specific person.", "tokens": [50650, 300, 23249, 281, 257, 2685, 954, 13, 50760], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 972, "seek": 405940, "start": 4067.32, "end": 4072.96, "text": " So for example, if you were seeing a bunch of things like Paul something, Paul something,", "tokens": [50760, 407, 337, 1365, 11, 498, 291, 645, 2577, 257, 3840, 295, 721, 411, 4552, 746, 11, 4552, 746, 11, 51042], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 973, "seek": 405940, "start": 4072.96, "end": 4076.92, "text": " maybe that one Paul something might be Paul Hollywood from the Great British Bake Off,", "tokens": [51042, 1310, 300, 472, 4552, 746, 1062, 312, 4552, 11628, 490, 264, 3769, 6221, 42597, 6318, 11, 51240], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 974, "seek": 405940, "start": 4076.92, "end": 4080.52, "text": " and it might have to make a connection to a specific person.", "tokens": [51240, 293, 309, 1062, 362, 281, 652, 257, 4984, 281, 257, 2685, 954, 13, 51420], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 975, "seek": 405940, "start": 4080.52, "end": 4085.92, "text": " So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood,", "tokens": [51420, 407, 498, 309, 311, 264, 1349, 4552, 885, 1143, 5101, 11, 341, 13977, 2113, 260, 576, 6269, 309, 281, 4552, 11628, 11, 51690], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 976, "seek": 405940, "start": 4085.92, "end": 4087.7200000000003, "text": " depending on the context.", "tokens": [51690, 5413, 322, 264, 4319, 13, 51780], "temperature": 0.0, "avg_logprob": -0.135436522654998, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00017952246707864106}, {"id": 977, "seek": 408772, "start": 4087.8399999999997, "end": 4093.12, "text": " That's out of the scope of this video series, but keep in mind that that pipe would do something", "tokens": [50370, 663, 311, 484, 295, 264, 11923, 295, 341, 960, 2638, 11, 457, 1066, 294, 1575, 300, 300, 11240, 576, 360, 746, 50634], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 978, "seek": 408772, "start": 4093.12, "end": 4097.679999999999, "text": " else that would modify the ints that would give them greater specificity.", "tokens": [50634, 1646, 300, 576, 16927, 264, 560, 82, 300, 576, 976, 552, 5044, 2685, 507, 13, 50862], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 979, "seek": 408772, "start": 4097.679999999999, "end": 4102.2, "text": " And then what you'd be left with is the doc object on the output that not only has entities", "tokens": [50862, 400, 550, 437, 291, 1116, 312, 1411, 365, 307, 264, 3211, 2657, 322, 264, 5598, 300, 406, 787, 575, 16667, 51088], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 980, "seek": 408772, "start": 4102.2, "end": 4107.2, "text": " annotated, but it's also got entities linked to some generic specific data.", "tokens": [51088, 25339, 770, 11, 457, 309, 311, 611, 658, 16667, 9408, 281, 512, 19577, 2685, 1412, 13, 51338], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 981, "seek": 408772, "start": 4107.2, "end": 4109.36, "text": " So that's going to be how a pipeline works.", "tokens": [51338, 407, 300, 311, 516, 281, 312, 577, 257, 15517, 1985, 13, 51446], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 982, "seek": 408772, "start": 4109.36, "end": 4111.5199999999995, "text": " And this is really what spacey is.", "tokens": [51446, 400, 341, 307, 534, 437, 1901, 88, 307, 13, 51554], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 983, "seek": 408772, "start": 4111.5199999999995, "end": 4115.639999999999, "text": " It's a sequence of pipes that act on your data.", "tokens": [51554, 467, 311, 257, 8310, 295, 21882, 300, 605, 322, 428, 1412, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11553243001302084, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.03409583866596222}, {"id": 984, "seek": 411564, "start": 4115.64, "end": 4119.64, "text": " And that's important to understand, because it means that as you add things to a spacey", "tokens": [50364, 400, 300, 311, 1021, 281, 1223, 11, 570, 309, 1355, 300, 382, 291, 909, 721, 281, 257, 1901, 88, 50564], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 985, "seek": 411564, "start": 4119.64, "end": 4125.160000000001, "text": " pipeline, you need to be very conscientious about where they're outed and in what order.", "tokens": [50564, 15517, 11, 291, 643, 281, 312, 588, 44507, 851, 466, 689, 436, 434, 484, 292, 293, 294, 437, 1668, 13, 50840], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 986, "seek": 411564, "start": 4125.160000000001, "end": 4128.72, "text": " As we're going to see as we move over to kind of rules based spacey, when we start talking", "tokens": [50840, 1018, 321, 434, 516, 281, 536, 382, 321, 1286, 670, 281, 733, 295, 4474, 2361, 1901, 88, 11, 562, 321, 722, 1417, 51018], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 987, "seek": 411564, "start": 4128.72, "end": 4134.56, "text": " about these different pipes, the entity ruler, the matcher custom components, regex components,", "tokens": [51018, 466, 613, 819, 21882, 11, 264, 13977, 19661, 11, 264, 2995, 260, 2375, 6677, 11, 319, 432, 87, 6677, 11, 51310], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 988, "seek": 411564, "start": 4134.56, "end": 4136.76, "text": " you're going to need to know which order to put them in.", "tokens": [51310, 291, 434, 516, 281, 643, 281, 458, 597, 1668, 281, 829, 552, 294, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 989, "seek": 411564, "start": 4136.76, "end": 4138.8, "text": " It's going to be very important.", "tokens": [51420, 467, 311, 516, 281, 312, 588, 1021, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 990, "seek": 411564, "start": 4138.8, "end": 4141.0, "text": " So do please keep that in mind.", "tokens": [51522, 407, 360, 1767, 1066, 300, 294, 1575, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 991, "seek": 411564, "start": 4141.0, "end": 4145.400000000001, "text": " Now spacey has a bunch of different attribute rulers or different pipes that you can kind", "tokens": [51632, 823, 1901, 88, 575, 257, 3840, 295, 819, 19667, 35009, 420, 819, 21882, 300, 291, 393, 733, 51852], "temperature": 0.0, "avg_logprob": -0.1111910049229452, "compression_ratio": 1.8488745980707395, "no_speech_prob": 0.023686233907938004}, {"id": 992, "seek": 414540, "start": 4145.4, "end": 4146.96, "text": " of add into it.", "tokens": [50364, 295, 909, 666, 309, 13, 50442], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 993, "seek": 414540, "start": 4146.96, "end": 4151.0, "text": " You've got dependency parsers that are going to come standard with all of your models.", "tokens": [50442, 509, 600, 658, 33621, 21156, 433, 300, 366, 516, 281, 808, 3832, 365, 439, 295, 428, 5245, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 994, "seek": 414540, "start": 4151.0, "end": 4155.04, "text": " You've got the entity linker entity, recognizer entity ruler, you're going to have to make", "tokens": [50644, 509, 600, 658, 264, 13977, 2113, 260, 13977, 11, 3068, 6545, 13977, 19661, 11, 291, 434, 516, 281, 362, 281, 652, 50846], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 995, "seek": 414540, "start": 4155.04, "end": 4157.839999999999, "text": " these yourself and add them in oftentimes.", "tokens": [50846, 613, 1803, 293, 909, 552, 294, 18349, 13, 50986], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 996, "seek": 414540, "start": 4157.839999999999, "end": 4159.0, "text": " You've got a limitizer.", "tokens": [50986, 509, 600, 658, 257, 4948, 6545, 13, 51044], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 997, "seek": 414540, "start": 4159.0, "end": 4162.44, "text": " This is going to be on most of your standard models, your morphologue, that's going to", "tokens": [51044, 639, 307, 516, 281, 312, 322, 881, 295, 428, 3832, 5245, 11, 428, 25778, 42298, 11, 300, 311, 516, 281, 51216], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 998, "seek": 414540, "start": 4162.44, "end": 4165.96, "text": " be on on there as well, sentence recognizer, synthesizer.", "tokens": [51216, 312, 322, 322, 456, 382, 731, 11, 8174, 3068, 6545, 11, 26617, 6545, 13, 51392], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 999, "seek": 414540, "start": 4165.96, "end": 4171.5199999999995, "text": " This is what allow for you to have the doc.sense right here span categorizer.", "tokens": [51392, 639, 307, 437, 2089, 337, 291, 281, 362, 264, 3211, 13, 82, 1288, 558, 510, 16174, 19250, 6545, 13, 51670], "temperature": 0.0, "avg_logprob": -0.18905906677246093, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.09803397953510284}, {"id": 1000, "seek": 417152, "start": 4171.52, "end": 4177.320000000001, "text": " This will help categorize different spans, be them single token spans or sequence of", "tokens": [50364, 639, 486, 854, 19250, 1125, 819, 44086, 11, 312, 552, 2167, 14862, 44086, 420, 8310, 295, 50654], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1001, "seek": 417152, "start": 4177.320000000001, "end": 4182.56, "text": " token spans, your tagger, this will tag the different things in your text, which will", "tokens": [50654, 14862, 44086, 11, 428, 6162, 1321, 11, 341, 486, 6162, 264, 819, 721, 294, 428, 2487, 11, 597, 486, 50916], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1002, "seek": 417152, "start": 4182.56, "end": 4185.160000000001, "text": " help with part of speech, your text categorizer.", "tokens": [50916, 854, 365, 644, 295, 6218, 11, 428, 2487, 19250, 6545, 13, 51046], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1003, "seek": 417152, "start": 4185.160000000001, "end": 4189.200000000001, "text": " This is when you train a machine learning model to recognize different categories of", "tokens": [51046, 639, 307, 562, 291, 3847, 257, 3479, 2539, 2316, 281, 5521, 819, 10479, 295, 51248], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1004, "seek": 417152, "start": 4189.200000000001, "end": 4190.200000000001, "text": " a text.", "tokens": [51248, 257, 2487, 13, 51298], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1005, "seek": 417152, "start": 4190.200000000001, "end": 4195.76, "text": " So text classification, which is a very important machine learning task, tote to VEC.", "tokens": [51298, 407, 2487, 21538, 11, 597, 307, 257, 588, 1021, 3479, 2539, 5633, 11, 49019, 281, 691, 8140, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1006, "seek": 417152, "start": 4195.76, "end": 4201.4400000000005, "text": " This is going to be what assigns word embeddings to the different words in your doc object.", "tokens": [51576, 639, 307, 516, 281, 312, 437, 6269, 82, 1349, 12240, 29432, 281, 264, 819, 2283, 294, 428, 3211, 2657, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1458927219749516, "compression_ratio": 1.9140625, "no_speech_prob": 0.5924937129020691}, {"id": 1007, "seek": 420144, "start": 4201.44, "end": 4206.759999999999, "text": " Organizer is what breaks that thing up and all your text into individual tokens.", "tokens": [50364, 12538, 6545, 307, 437, 9857, 300, 551, 493, 293, 439, 428, 2487, 666, 2609, 22667, 13, 50630], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1008, "seek": 420144, "start": 4206.759999999999, "end": 4209.799999999999, "text": " And you've got things like transformer and trainable pipes.", "tokens": [50630, 400, 291, 600, 658, 721, 411, 31782, 293, 3847, 712, 21882, 13, 50782], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1009, "seek": 420144, "start": 4209.799999999999, "end": 4213.16, "text": " Then within this, you've also got some other things called matchers.", "tokens": [50782, 1396, 1951, 341, 11, 291, 600, 611, 658, 512, 661, 721, 1219, 2995, 433, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1010, "seek": 420144, "start": 4213.16, "end": 4214.839999999999, "text": " So you can do some dependency matching.", "tokens": [50950, 407, 291, 393, 360, 512, 33621, 14324, 13, 51034], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1011, "seek": 420144, "start": 4214.839999999999, "end": 4217.0, "text": " We're not going to get into that in this video.", "tokens": [51034, 492, 434, 406, 516, 281, 483, 666, 300, 294, 341, 960, 13, 51142], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1012, "seek": 420144, "start": 4217.0, "end": 4220.12, "text": " You've also got the ability to use matcher and phrase matcher.", "tokens": [51142, 509, 600, 611, 658, 264, 3485, 281, 764, 2995, 260, 293, 9535, 2995, 260, 13, 51298], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1013, "seek": 420144, "start": 4220.12, "end": 4224.719999999999, "text": " These are a lot of the times can do some similar things, but they're executed a little differently", "tokens": [51298, 1981, 366, 257, 688, 295, 264, 1413, 393, 360, 512, 2531, 721, 11, 457, 436, 434, 17577, 257, 707, 7614, 51528], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1014, "seek": 420144, "start": 4224.719999999999, "end": 4225.96, "text": " to make things less confusing.", "tokens": [51528, 281, 652, 721, 1570, 13181, 13, 51590], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1015, "seek": 420144, "start": 4225.96, "end": 4229.679999999999, "text": " I'm really only talking about the matcher of these two.", "tokens": [51590, 286, 478, 534, 787, 1417, 466, 264, 2995, 260, 295, 613, 732, 13, 51776], "temperature": 0.0, "avg_logprob": -0.13986700294661697, "compression_ratio": 1.7843137254901962, "no_speech_prob": 0.1519087553024292}, {"id": 1016, "seek": 422968, "start": 4229.8, "end": 4233.88, "text": " If there's a need for it, I'll add into the textbook the phrase matcher at a later date,", "tokens": [50370, 759, 456, 311, 257, 643, 337, 309, 11, 286, 603, 909, 666, 264, 25591, 264, 9535, 2995, 260, 412, 257, 1780, 4002, 11, 50574], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1017, "seek": 422968, "start": 4233.88, "end": 4235.76, "text": " but I'm not going to cover it in this video.", "tokens": [50574, 457, 286, 478, 406, 516, 281, 2060, 309, 294, 341, 960, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1018, "seek": 422968, "start": 4235.76, "end": 4239.88, "text": " And if I do add in the phrase matcher, it's going to be after this matcher section here.", "tokens": [50668, 400, 498, 286, 360, 909, 294, 264, 9535, 2995, 260, 11, 309, 311, 516, 281, 312, 934, 341, 2995, 260, 3541, 510, 13, 50874], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1019, "seek": 422968, "start": 4239.88, "end": 4241.4400000000005, "text": " I have it in the GitHub repo.", "tokens": [50874, 286, 362, 309, 294, 264, 23331, 49040, 13, 50952], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1020, "seek": 422968, "start": 4241.4400000000005, "end": 4245.200000000001, "text": " I just haven't included in the textbook to keep things a little bit simpler, at least", "tokens": [50952, 286, 445, 2378, 380, 5556, 294, 264, 25591, 281, 1066, 721, 257, 707, 857, 18587, 11, 412, 1935, 51140], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1021, "seek": 422968, "start": 4245.200000000001, "end": 4246.88, "text": " if you're just starting out.", "tokens": [51140, 498, 291, 434, 445, 2891, 484, 13, 51224], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1022, "seek": 422968, "start": 4246.88, "end": 4252.280000000001, "text": " So a big good question is, well, how do you add pipes to a spacey pipeline?", "tokens": [51224, 407, 257, 955, 665, 1168, 307, 11, 731, 11, 577, 360, 291, 909, 21882, 281, 257, 1901, 88, 15517, 30, 51494], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1023, "seek": 422968, "start": 4252.280000000001, "end": 4253.68, "text": " So let's go ahead and do that.", "tokens": [51494, 407, 718, 311, 352, 2286, 293, 360, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1024, "seek": 422968, "start": 4253.68, "end": 4257.4400000000005, "text": " We're going to make a blank spacey pipeline right now.", "tokens": [51564, 492, 434, 516, 281, 652, 257, 8247, 1901, 88, 15517, 558, 586, 13, 51752], "temperature": 0.0, "avg_logprob": -0.14123247184005439, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.046026162803173065}, {"id": 1025, "seek": 425744, "start": 4257.44, "end": 4262.799999999999, "text": " Let's go ahead and just make, we'll just work with the same live coding notebook that we", "tokens": [50364, 961, 311, 352, 2286, 293, 445, 652, 11, 321, 603, 445, 589, 365, 264, 912, 1621, 17720, 21060, 300, 321, 50632], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1026, "seek": 425744, "start": 4262.799999999999, "end": 4264.08, "text": " have open right now.", "tokens": [50632, 362, 1269, 558, 586, 13, 50696], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1027, "seek": 425744, "start": 4264.08, "end": 4267.5199999999995, "text": " So what we're going to do is we're going to make a blank model, and we're going to actually", "tokens": [50696, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 652, 257, 8247, 2316, 11, 293, 321, 434, 516, 281, 767, 50868], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1028, "seek": 425744, "start": 4267.5199999999995, "end": 4274.12, "text": " add in our own sentenizer to our, to our text.", "tokens": [50868, 909, 294, 527, 1065, 2279, 268, 6545, 281, 527, 11, 281, 527, 2487, 13, 51198], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1029, "seek": 425744, "start": 4274.12, "end": 4276.24, "text": " So let's go ahead and do that.", "tokens": [51198, 407, 718, 311, 352, 2286, 293, 360, 300, 13, 51304], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1030, "seek": 425744, "start": 4276.24, "end": 4280.679999999999, "text": " So I'm going to say NLP is equal to a spacey dot blank.", "tokens": [51304, 407, 286, 478, 516, 281, 584, 426, 45196, 307, 2681, 281, 257, 1901, 88, 5893, 8247, 13, 51526], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1031, "seek": 425744, "start": 4280.679999999999, "end": 4284.44, "text": " This is going to allow for me to make a blank spacey pipeline.", "tokens": [51526, 639, 307, 516, 281, 2089, 337, 385, 281, 652, 257, 8247, 1901, 88, 15517, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17436441052861573, "compression_ratio": 1.7847533632286996, "no_speech_prob": 0.005910616368055344}, {"id": 1032, "seek": 428444, "start": 4284.44, "end": 4288.639999999999, "text": " And I'm going to say Ian so that it knows that the tokenizer that I need to use is the", "tokens": [50364, 400, 286, 478, 516, 281, 584, 19595, 370, 300, 309, 3255, 300, 264, 14862, 6545, 300, 286, 643, 281, 764, 307, 264, 50574], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1033, "seek": 428444, "start": 4288.639999999999, "end": 4290.639999999999, "text": " English tokenizer.", "tokens": [50574, 3669, 14862, 6545, 13, 50674], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1034, "seek": 428444, "start": 4290.639999999999, "end": 4296.04, "text": " And now if I want to add a pipe to that, I can use one of the built-in spacey features.", "tokens": [50674, 400, 586, 498, 286, 528, 281, 909, 257, 11240, 281, 300, 11, 286, 393, 764, 472, 295, 264, 3094, 12, 259, 1901, 88, 4122, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1035, "seek": 428444, "start": 4296.04, "end": 4300.879999999999, "text": " So I can say add underscore pipe, and I can say sentenizer.", "tokens": [50944, 407, 286, 393, 584, 909, 37556, 11240, 11, 293, 286, 393, 584, 2279, 268, 6545, 13, 51186], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1036, "seek": 428444, "start": 4300.879999999999, "end": 4303.4, "text": " So I can add in a sentenizer.", "tokens": [51186, 407, 286, 393, 909, 294, 257, 2279, 268, 6545, 13, 51312], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1037, "seek": 428444, "start": 4303.4, "end": 4309.16, "text": " This is going to allow for me to create a pipeline now that has a sequence of two different", "tokens": [51312, 639, 307, 516, 281, 2089, 337, 385, 281, 1884, 257, 15517, 586, 300, 575, 257, 8310, 295, 732, 819, 51600], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1038, "seek": 428444, "start": 4309.16, "end": 4310.16, "text": " pipes.", "tokens": [51600, 21882, 13, 51650], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1039, "seek": 428444, "start": 4310.16, "end": 4313.5599999999995, "text": " And I demonstrate in the textbook why this is important.", "tokens": [51650, 400, 286, 11698, 294, 264, 25591, 983, 341, 307, 1021, 13, 51820], "temperature": 0.0, "avg_logprob": -0.13165367306686762, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.06753508746623993}, {"id": 1040, "seek": 431356, "start": 4313.56, "end": 4318.96, "text": " Sometimes what you need to do is you need to just only break down a text into individual", "tokens": [50364, 4803, 437, 291, 643, 281, 360, 307, 291, 643, 281, 445, 787, 1821, 760, 257, 2487, 666, 2609, 50634], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1041, "seek": 431356, "start": 4318.96, "end": 4319.96, "text": " sentences.", "tokens": [50634, 16579, 13, 50684], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1042, "seek": 431356, "start": 4319.96, "end": 4328.080000000001, "text": " So I grabbed a massive, massive corpus from the internet, which is on MIT.edu.", "tokens": [50684, 407, 286, 18607, 257, 5994, 11, 5994, 1181, 31624, 490, 264, 4705, 11, 597, 307, 322, 13100, 13, 22938, 13, 51090], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1043, "seek": 431356, "start": 4328.080000000001, "end": 4330.56, "text": " And it's the entire Shakespeare corpus.", "tokens": [51090, 400, 309, 311, 264, 2302, 22825, 1181, 31624, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1044, "seek": 431356, "start": 4330.56, "end": 4335.320000000001, "text": " And I just try to calculate the, the quantity of sentences found within it.", "tokens": [51214, 400, 286, 445, 853, 281, 8873, 264, 11, 264, 11275, 295, 16579, 1352, 1951, 309, 13, 51452], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1045, "seek": 431356, "start": 4335.320000000001, "end": 4342.76, "text": " There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and", "tokens": [51452, 821, 366, 30849, 11, 7668, 18, 16579, 11, 293, 309, 1890, 385, 787, 1614, 13, 19563, 3949, 281, 767, 352, 807, 293, 51824], "temperature": 0.0, "avg_logprob": -0.14191994808688022, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.1602099984884262}, {"id": 1046, "seek": 434276, "start": 4342.76, "end": 4346.04, "text": " count those sentences with the spacey model.", "tokens": [50364, 1207, 729, 16579, 365, 264, 1901, 88, 2316, 13, 50528], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1047, "seek": 434276, "start": 4346.04, "end": 4352.08, "text": " Using the small model, however, it took a total amount of time of 47 minutes to actually", "tokens": [50528, 11142, 264, 1359, 2316, 11, 4461, 11, 309, 1890, 257, 3217, 2372, 295, 565, 295, 16953, 2077, 281, 767, 50830], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1048, "seek": 434276, "start": 4352.08, "end": 4355.52, "text": " break down all those sentences and extract them.", "tokens": [50830, 1821, 760, 439, 729, 16579, 293, 8947, 552, 13, 51002], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1049, "seek": 434276, "start": 4355.52, "end": 4359.6, "text": " Why is there a difference in time between 7 seconds and 47 minutes?", "tokens": [51002, 1545, 307, 456, 257, 2649, 294, 565, 1296, 1614, 3949, 293, 16953, 2077, 30, 51206], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1050, "seek": 434276, "start": 4359.6, "end": 4364.320000000001, "text": " It's because that this spacey small model has a bunch of other pipes in it that are", "tokens": [51206, 467, 311, 570, 300, 341, 1901, 88, 1359, 2316, 575, 257, 3840, 295, 661, 21882, 294, 309, 300, 366, 51442], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1051, "seek": 434276, "start": 4364.320000000001, "end": 4366.52, "text": " trying to do a bunch of other things.", "tokens": [51442, 1382, 281, 360, 257, 3840, 295, 661, 721, 13, 51552], "temperature": 0.0, "avg_logprob": -0.12266897386120211, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.3699530363082886}, {"id": 1052, "seek": 436652, "start": 4366.52, "end": 4374.6, "text": " If you just need to do one task, it's always a good idea to just activate one pipe or maybe", "tokens": [50364, 759, 291, 445, 643, 281, 360, 472, 5633, 11, 309, 311, 1009, 257, 665, 1558, 281, 445, 13615, 472, 11240, 420, 1310, 50768], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1053, "seek": 436652, "start": 4374.6, "end": 4379.240000000001, "text": " make a blank model and just add that single pipe or the only pipes that you need to it.", "tokens": [50768, 652, 257, 8247, 2316, 293, 445, 909, 300, 2167, 11240, 420, 264, 787, 21882, 300, 291, 643, 281, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1054, "seek": 436652, "start": 4379.240000000001, "end": 4384.120000000001, "text": " A great example of this is needing to tokenize a whole bunch of sentences in relatively short", "tokens": [51000, 316, 869, 1365, 295, 341, 307, 18006, 281, 14862, 1125, 257, 1379, 3840, 295, 16579, 294, 7226, 2099, 51244], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1055, "seek": 436652, "start": 4384.120000000001, "end": 4385.120000000001, "text": " time.", "tokens": [51244, 565, 13, 51294], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1056, "seek": 436652, "start": 4385.120000000001, "end": 4391.280000000001, "text": " So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.", "tokens": [51294, 407, 286, 500, 380, 458, 466, 291, 11, 457, 286, 1116, 312, 709, 20423, 365, 1614, 3949, 5717, 16953, 2077, 13, 51602], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1057, "seek": 436652, "start": 4391.280000000001, "end": 4392.84, "text": " However comes at a trade-off.", "tokens": [51602, 2908, 1487, 412, 257, 4923, 12, 4506, 13, 51680], "temperature": 0.0, "avg_logprob": -0.11070502144949777, "compression_ratio": 1.5390625, "no_speech_prob": 0.19680000841617584}, {"id": 1058, "seek": 439284, "start": 4392.84, "end": 4398.64, "text": " The small model is going to be more accurate in how it finds sentence boundaries.", "tokens": [50364, 440, 1359, 2316, 307, 516, 281, 312, 544, 8559, 294, 577, 309, 10704, 8174, 13180, 13, 50654], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1059, "seek": 439284, "start": 4398.64, "end": 4401.08, "text": " So we have a difference in quantity here.", "tokens": [50654, 407, 321, 362, 257, 2649, 294, 11275, 510, 13, 50776], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1060, "seek": 439284, "start": 4401.08, "end": 4406.24, "text": " This difference in quantity indicates that this one messed up and made some mistakes because", "tokens": [50776, 639, 2649, 294, 11275, 16203, 300, 341, 472, 16507, 493, 293, 1027, 512, 8038, 570, 51034], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1061, "seek": 439284, "start": 4406.24, "end": 4407.96, "text": " it was just the sentenceizer.", "tokens": [51034, 309, 390, 445, 264, 8174, 6545, 13, 51120], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1062, "seek": 439284, "start": 4407.96, "end": 4411.76, "text": " The sentenceizer didn't have extra data being fed to it.", "tokens": [51120, 440, 8174, 6545, 994, 380, 362, 2857, 1412, 885, 4636, 281, 309, 13, 51310], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1063, "seek": 439284, "start": 4411.76, "end": 4416.56, "text": " In fact, if I probably used larger models, I might even have better results.", "tokens": [51310, 682, 1186, 11, 498, 286, 1391, 1143, 4833, 5245, 11, 286, 1062, 754, 362, 1101, 3542, 13, 51550], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1064, "seek": 439284, "start": 4416.56, "end": 4417.76, "text": " But always think about that.", "tokens": [51550, 583, 1009, 519, 466, 300, 13, 51610], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1065, "seek": 439284, "start": 4417.76, "end": 4422.400000000001, "text": " If time is of the essence and you don't care so much about accuracy, a great way to get", "tokens": [51610, 759, 565, 307, 295, 264, 12801, 293, 291, 500, 380, 1127, 370, 709, 466, 14170, 11, 257, 869, 636, 281, 483, 51842], "temperature": 0.0, "avg_logprob": -0.12868535714071305, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.2017177939414978}, {"id": 1066, "seek": 442240, "start": 4422.4, "end": 4426.799999999999, "text": " the quantity of sentences or at least a ballpark is to use this method where you simply add", "tokens": [50364, 264, 11275, 295, 16579, 420, 412, 1935, 257, 2594, 31239, 307, 281, 764, 341, 3170, 689, 291, 2935, 909, 50584], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1067, "seek": 442240, "start": 4426.799999999999, "end": 4430.12, "text": " in a sentenceizer to a blank model.", "tokens": [50584, 294, 257, 8174, 6545, 281, 257, 8247, 2316, 13, 50750], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1068, "seek": 442240, "start": 4430.12, "end": 4436.24, "text": " So that's how you actually add in different pipes to a spacey pipeline and we're going", "tokens": [50750, 407, 300, 311, 577, 291, 767, 909, 294, 819, 21882, 281, 257, 1901, 88, 15517, 293, 321, 434, 516, 51056], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1069, "seek": 442240, "start": 4436.24, "end": 4440.759999999999, "text": " to be reinforcing that skill as we go through, especially in part two, where we really kind", "tokens": [51056, 281, 312, 48262, 300, 5389, 382, 321, 352, 807, 11, 2318, 294, 644, 732, 11, 689, 321, 534, 733, 51282], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1070, "seek": 442240, "start": 4440.759999999999, "end": 4443.2, "text": " of work with this in a lot of detail.", "tokens": [51282, 295, 589, 365, 341, 294, 257, 688, 295, 2607, 13, 51404], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1071, "seek": 442240, "start": 4443.2, "end": 4448.799999999999, "text": " Right now I'm just interested in giving you the general understanding of how this might", "tokens": [51404, 1779, 586, 286, 478, 445, 3102, 294, 2902, 291, 264, 2674, 3701, 295, 577, 341, 1062, 51684], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1072, "seek": 442240, "start": 4448.799999999999, "end": 4449.799999999999, "text": " work.", "tokens": [51684, 589, 13, 51734], "temperature": 0.0, "avg_logprob": -0.12806282043457032, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.048843011260032654}, {"id": 1073, "seek": 444980, "start": 4449.8, "end": 4456.24, "text": " Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes.", "tokens": [50364, 961, 311, 352, 2286, 293, 853, 281, 12477, 527, 15517, 370, 321, 393, 360, 12477, 37556, 21882, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1628529618426067, "compression_ratio": 1.6972972972972973, "no_speech_prob": 0.02442016825079918}, {"id": 1074, "seek": 444980, "start": 4456.24, "end": 4460.16, "text": " We can analyze what our analyze, there we go.", "tokens": [50686, 492, 393, 12477, 437, 527, 12477, 11, 456, 321, 352, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1628529618426067, "compression_ratio": 1.6972972972972973, "no_speech_prob": 0.02442016825079918}, {"id": 1075, "seek": 444980, "start": 4460.16, "end": 4462.46, "text": " We can actually analyze our pipeline.", "tokens": [50882, 492, 393, 767, 12477, 527, 15517, 13, 50997], "temperature": 0.0, "avg_logprob": -0.1628529618426067, "compression_ratio": 1.6972972972972973, "no_speech_prob": 0.02442016825079918}, {"id": 1076, "seek": 444980, "start": 4462.46, "end": 4467.6, "text": " If we look at the NLP object, which is our blank model with the sentenceizer, we see", "tokens": [50997, 759, 321, 574, 412, 264, 426, 45196, 2657, 11, 597, 307, 527, 8247, 2316, 365, 264, 8174, 6545, 11, 321, 536, 51254], "temperature": 0.0, "avg_logprob": -0.1628529618426067, "compression_ratio": 1.6972972972972973, "no_speech_prob": 0.02442016825079918}, {"id": 1077, "seek": 444980, "start": 4467.6, "end": 4475.76, "text": " that our NLP pipeline ignore summary, ignore this bit here.", "tokens": [51254, 300, 527, 426, 45196, 15517, 11200, 12691, 11, 11200, 341, 857, 510, 13, 51662], "temperature": 0.0, "avg_logprob": -0.1628529618426067, "compression_ratio": 1.6972972972972973, "no_speech_prob": 0.02442016825079918}, {"id": 1078, "seek": 447576, "start": 4475.8, "end": 4479.84, "text": " But what you're actually able to kind of go through and see right away is that we've really", "tokens": [50366, 583, 437, 291, 434, 767, 1075, 281, 733, 295, 352, 807, 293, 536, 558, 1314, 307, 300, 321, 600, 534, 50568], "temperature": 0.0, "avg_logprob": -0.17956418669625615, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.16880978643894196}, {"id": 1079, "seek": 447576, "start": 4479.84, "end": 4482.64, "text": " just got the sentenceizer sitting in it.", "tokens": [50568, 445, 658, 264, 8174, 6545, 3798, 294, 309, 13, 50708], "temperature": 0.0, "avg_logprob": -0.17956418669625615, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.16880978643894196}, {"id": 1080, "seek": 447576, "start": 4482.64, "end": 4488.52, "text": " If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey", "tokens": [50708, 759, 321, 645, 281, 12477, 257, 709, 544, 13956, 15517, 11, 370, 718, 311, 1884, 426, 45196, 732, 307, 2681, 281, 1901, 88, 51002], "temperature": 0.0, "avg_logprob": -0.17956418669625615, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.16880978643894196}, {"id": 1081, "seek": 447576, "start": 4488.52, "end": 4495.92, "text": " dot load and core web SM, we're going to create that NLP two object around the small spacey", "tokens": [51002, 5893, 3677, 293, 4965, 3670, 13115, 11, 321, 434, 516, 281, 1884, 300, 426, 45196, 732, 2657, 926, 264, 1359, 1901, 88, 51372], "temperature": 0.0, "avg_logprob": -0.17956418669625615, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.16880978643894196}, {"id": 1082, "seek": 447576, "start": 4495.92, "end": 4499.76, "text": " English model.", "tokens": [51372, 3669, 2316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17956418669625615, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.16880978643894196}, {"id": 1083, "seek": 449976, "start": 4499.76, "end": 4506.76, "text": " We can analyze the pipes again, and we see a much more elaborate pipeline.", "tokens": [50364, 492, 393, 12477, 264, 21882, 797, 11, 293, 321, 536, 257, 709, 544, 20945, 15517, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1084, "seek": 449976, "start": 4506.76, "end": 4507.76, "text": " So what are we looking at?", "tokens": [50714, 407, 437, 366, 321, 1237, 412, 30, 50764], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1085, "seek": 449976, "start": 4507.76, "end": 4512.400000000001, "text": " Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger", "tokens": [50764, 1042, 11, 437, 321, 434, 1237, 412, 307, 264, 8310, 295, 721, 321, 600, 658, 294, 264, 15517, 11, 257, 6162, 1321, 50996], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1086, "seek": 449976, "start": 4512.400000000001, "end": 4518.88, "text": " after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got", "tokens": [50996, 934, 264, 751, 281, 691, 8140, 11, 321, 600, 658, 257, 6162, 1321, 11, 257, 21156, 260, 11, 321, 1066, 322, 516, 760, 11, 321, 600, 658, 51320], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1087, "seek": 449976, "start": 4518.88, "end": 4523.52, "text": " an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed", "tokens": [51320, 364, 19667, 19661, 11, 321, 600, 658, 257, 4948, 6545, 11, 321, 600, 658, 264, 426, 1598, 11, 300, 311, 437, 309, 311, 4761, 51552], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1088, "seek": 449976, "start": 4523.52, "end": 4526.400000000001, "text": " the doc dot ends, and we keep on going down.", "tokens": [51552, 264, 3211, 5893, 5314, 11, 293, 321, 1066, 322, 516, 760, 13, 51696], "temperature": 0.0, "avg_logprob": -0.22910577742779842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.26887616515159607}, {"id": 1089, "seek": 452640, "start": 4526.4, "end": 4531.24, "text": " We can see the limitizer, but we can see also a whole bunch of other things.", "tokens": [50364, 492, 393, 536, 264, 4948, 6545, 11, 457, 321, 393, 536, 611, 257, 1379, 3840, 295, 661, 721, 13, 50606], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1090, "seek": 452640, "start": 4531.24, "end": 4534.839999999999, "text": " We can see what these different things actually assign.", "tokens": [50606, 492, 393, 536, 437, 613, 819, 721, 767, 6269, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1091, "seek": 452640, "start": 4534.839999999999, "end": 4542.0, "text": " So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually", "tokens": [50786, 407, 3211, 5893, 5314, 307, 6269, 82, 264, 426, 1598, 293, 3651, 11, 293, 321, 393, 611, 536, 437, 1184, 11240, 1062, 767, 51144], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1092, "seek": 452640, "start": 4542.0, "end": 4543.12, "text": " require.", "tokens": [51144, 3651, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1093, "seek": 452640, "start": 4543.12, "end": 4548.04, "text": " So if we look up here, we see that the NER pipe, so the name to the recognition pipe", "tokens": [51200, 407, 498, 321, 574, 493, 510, 11, 321, 536, 300, 264, 426, 1598, 11240, 11, 370, 264, 1315, 281, 264, 11150, 11240, 51446], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1094, "seek": 452640, "start": 4548.04, "end": 4551.28, "text": " is responsible for assigning the doc dot ends.", "tokens": [51446, 307, 6250, 337, 49602, 264, 3211, 5893, 5314, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1095, "seek": 452640, "start": 4551.28, "end": 4556.08, "text": " So that attribute of the doc object, and it's also responsible at the token level for", "tokens": [51608, 407, 300, 19667, 295, 264, 3211, 2657, 11, 293, 309, 311, 611, 6250, 412, 264, 14862, 1496, 337, 51848], "temperature": 0.0, "avg_logprob": -0.1700335751060678, "compression_ratio": 1.842741935483871, "no_speech_prob": 0.01971745491027832}, {"id": 1096, "seek": 455608, "start": 4556.08, "end": 4562.48, "text": " assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes", "tokens": [50364, 49602, 264, 917, 5893, 39839, 33, 37556, 39839, 33, 11, 597, 307, 264, 11, 498, 291, 1604, 490, 257, 1326, 2077, 50684], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1097, "seek": 455608, "start": 4562.48, "end": 4570.08, "text": " ago when we talked about the IOB being the opening beginning or out beginning inside", "tokens": [50684, 2057, 562, 321, 2825, 466, 264, 39839, 33, 885, 264, 5193, 2863, 420, 484, 2863, 1854, 51064], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1098, "seek": 455608, "start": 4570.08, "end": 4576.72, "text": " for a different entity, it also assigns the end dot end underscore type for each token", "tokens": [51064, 337, 257, 819, 13977, 11, 309, 611, 6269, 82, 264, 917, 5893, 917, 37556, 2010, 337, 1184, 14862, 51396], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1099, "seek": 455608, "start": 4576.72, "end": 4577.72, "text": " attribute.", "tokens": [51396, 19667, 13, 51446], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1100, "seek": 455608, "start": 4577.72, "end": 4583.24, "text": " So you can see a lot of different things about your pipeline by using NLP dot analyze underscore", "tokens": [51446, 407, 291, 393, 536, 257, 688, 295, 819, 721, 466, 428, 15517, 538, 1228, 426, 45196, 5893, 12477, 37556, 51722], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1101, "seek": 455608, "start": 4583.24, "end": 4584.88, "text": " pipes.", "tokens": [51722, 21882, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1791566973147185, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.033077020198106766}, {"id": 1102, "seek": 458488, "start": 4584.88, "end": 4589.8, "text": " If you've gotten to this point in the video, then I think you should by now have a good", "tokens": [50364, 759, 291, 600, 5768, 281, 341, 935, 294, 264, 960, 11, 550, 286, 519, 291, 820, 538, 586, 362, 257, 665, 50610], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1103, "seek": 458488, "start": 4589.8, "end": 4596.04, "text": " really umbrella view of what spacey is, how it works, why it's useful.", "tokens": [50610, 534, 21925, 1910, 295, 437, 1901, 88, 307, 11, 577, 309, 1985, 11, 983, 309, 311, 4420, 13, 50922], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1104, "seek": 458488, "start": 4596.04, "end": 4599.56, "text": " And some of the basic features that it can do and how it can solve some pretty complex", "tokens": [50922, 400, 512, 295, 264, 3875, 4122, 300, 309, 393, 360, 293, 577, 309, 393, 5039, 512, 1238, 3997, 51098], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1105, "seek": 458488, "start": 4599.56, "end": 4603.32, "text": " problems with some pretty simple lines of code.", "tokens": [51098, 2740, 365, 512, 1238, 2199, 3876, 295, 3089, 13, 51286], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1106, "seek": 458488, "start": 4603.32, "end": 4608.28, "text": " What we're going to see now moving forward is how you as a practitioner of NLP cannot", "tokens": [51286, 708, 321, 434, 516, 281, 536, 586, 2684, 2128, 307, 577, 291, 382, 257, 32125, 295, 426, 45196, 2644, 51534], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1107, "seek": 458488, "start": 4608.28, "end": 4612.92, "text": " just take what's given to you with spacey, but start working with it and start leveraging", "tokens": [51534, 445, 747, 437, 311, 2212, 281, 291, 365, 1901, 88, 11, 457, 722, 1364, 365, 309, 293, 722, 32666, 51766], "temperature": 0.0, "avg_logprob": -0.0941290370488571, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.23921972513198853}, {"id": 1108, "seek": 461292, "start": 4612.92, "end": 4615.2, "text": " it for your own uses.", "tokens": [50364, 309, 337, 428, 1065, 4960, 13, 50478], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1109, "seek": 461292, "start": 4615.2, "end": 4617.28, "text": " So taking what is already available.", "tokens": [50478, 407, 1940, 437, 307, 1217, 2435, 13, 50582], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1110, "seek": 461292, "start": 4617.28, "end": 4622.12, "text": " So like these models like the English model and adding to them contributing to them.", "tokens": [50582, 407, 411, 613, 5245, 411, 264, 3669, 2316, 293, 5127, 281, 552, 19270, 281, 552, 13, 50824], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1111, "seek": 461292, "start": 4622.12, "end": 4627.0, "text": " Maybe you want to make an entity ruler where you can find more entities in a text based", "tokens": [50824, 2704, 291, 528, 281, 652, 364, 13977, 19661, 689, 291, 393, 915, 544, 16667, 294, 257, 2487, 2361, 51068], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1112, "seek": 461292, "start": 4627.0, "end": 4630.12, "text": " on some cousin tier or list that you have.", "tokens": [51068, 322, 512, 16207, 12362, 420, 1329, 300, 291, 362, 13, 51224], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1113, "seek": 461292, "start": 4630.12, "end": 4634.28, "text": " Maybe you want to make a matcher so you can find specific sequences within a text.", "tokens": [51224, 2704, 291, 528, 281, 652, 257, 2995, 260, 370, 291, 393, 915, 2685, 22978, 1951, 257, 2487, 13, 51432], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1114, "seek": 461292, "start": 4634.28, "end": 4636.88, "text": " Maybe that's important for information extraction.", "tokens": [51432, 2704, 300, 311, 1021, 337, 1589, 30197, 13, 51562], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1115, "seek": 461292, "start": 4636.88, "end": 4640.6, "text": " Maybe you need to add custom functions or components into your spacey pipeline.", "tokens": [51562, 2704, 291, 643, 281, 909, 2375, 6828, 420, 6677, 666, 428, 1901, 88, 15517, 13, 51748], "temperature": 0.0, "avg_logprob": -0.15076296904991413, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.7116739153862}, {"id": 1116, "seek": 464060, "start": 4640.6, "end": 4644.92, "text": " I'm going to be going through in part two rules based spacey and giving you all the", "tokens": [50364, 286, 478, 516, 281, 312, 516, 807, 294, 644, 732, 4474, 2361, 1901, 88, 293, 2902, 291, 439, 264, 50580], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1117, "seek": 464060, "start": 4644.92, "end": 4651.160000000001, "text": " basics of how to do some really robust custom things relatively quickly with within the", "tokens": [50580, 14688, 295, 577, 281, 360, 512, 534, 13956, 2375, 721, 7226, 2661, 365, 1951, 264, 50892], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1118, "seek": 464060, "start": 4651.160000000001, "end": 4652.92, "text": " spacey framework.", "tokens": [50892, 1901, 88, 8388, 13, 50980], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1119, "seek": 464060, "start": 4652.92, "end": 4656.72, "text": " All of that's going to lay the groundwork so that in part three, we can start applying", "tokens": [50980, 1057, 295, 300, 311, 516, 281, 2360, 264, 2727, 1902, 370, 300, 294, 644, 1045, 11, 321, 393, 722, 9275, 51170], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1120, "seek": 464060, "start": 4656.72, "end": 4660.76, "text": " all these skills and start solving some real world problems.", "tokens": [51170, 439, 613, 3942, 293, 722, 12606, 512, 957, 1002, 2740, 13, 51372], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1121, "seek": 464060, "start": 4660.76, "end": 4663.68, "text": " In this case, we're going to look at financial analysis.", "tokens": [51372, 682, 341, 1389, 11, 321, 434, 516, 281, 574, 412, 4669, 5215, 13, 51518], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1122, "seek": 464060, "start": 4663.68, "end": 4668.4400000000005, "text": " So that's going to be where we move to next is part two.", "tokens": [51518, 407, 300, 311, 516, 281, 312, 689, 321, 1286, 281, 958, 307, 644, 732, 13, 51756], "temperature": 0.0, "avg_logprob": -0.14634108754385888, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.009411769919097424}, {"id": 1123, "seek": 466844, "start": 4668.44, "end": 4673.28, "text": " We are now moving into part two of this Jupiter book on spacey and we're going to be working", "tokens": [50364, 492, 366, 586, 2684, 666, 644, 732, 295, 341, 24567, 1446, 322, 1901, 88, 293, 321, 434, 516, 281, 312, 1364, 50606], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1124, "seek": 466844, "start": 4673.28, "end": 4675.5199999999995, "text": " with rules based spacey.", "tokens": [50606, 365, 4474, 2361, 1901, 88, 13, 50718], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1125, "seek": 466844, "start": 4675.5199999999995, "end": 4678.44, "text": " Now this is really kind of the bread and butter of this video.", "tokens": [50718, 823, 341, 307, 534, 733, 295, 264, 5961, 293, 5517, 295, 341, 960, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1126, "seek": 466844, "start": 4678.44, "end": 4682.799999999999, "text": " You've gotten a sense of the umbrella structure of spacey as a framework.", "tokens": [50864, 509, 600, 5768, 257, 2020, 295, 264, 21925, 3877, 295, 1901, 88, 382, 257, 8388, 13, 51082], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1127, "seek": 466844, "start": 4682.799999999999, "end": 4685.839999999999, "text": " You've gotten a sense of what the container can contain.", "tokens": [51082, 509, 600, 5768, 257, 2020, 295, 437, 264, 10129, 393, 5304, 13, 51234], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1128, "seek": 466844, "start": 4685.839999999999, "end": 4690.919999999999, "text": " You've gotten a sense of the token attributes and the linguistic annotations from part one", "tokens": [51234, 509, 600, 5768, 257, 2020, 295, 264, 14862, 17212, 293, 264, 43002, 25339, 763, 490, 644, 472, 51488], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1129, "seek": 466844, "start": 4690.919999999999, "end": 4693.679999999999, "text": " of this book and the earlier part of this video.", "tokens": [51488, 295, 341, 1446, 293, 264, 3071, 644, 295, 341, 960, 13, 51626], "temperature": 0.0, "avg_logprob": -0.12828332343987658, "compression_ratio": 1.9439655172413792, "no_speech_prob": 0.3922213315963745}, {"id": 1130, "seek": 469368, "start": 4693.68, "end": 4698.88, "text": " Now we're going to move into taking those skills and really developing them into custom", "tokens": [50364, 823, 321, 434, 516, 281, 1286, 666, 1940, 729, 3942, 293, 534, 6416, 552, 666, 2375, 50624], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1131, "seek": 469368, "start": 4698.88, "end": 4703.52, "text": " components and modified pipes that exist within spacey.", "tokens": [50624, 6677, 293, 15873, 21882, 300, 2514, 1951, 1901, 88, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1132, "seek": 469368, "start": 4703.52, "end": 4707.96, "text": " In other words, I'm going to show you how to take what we've learned now and start really", "tokens": [50856, 682, 661, 2283, 11, 286, 478, 516, 281, 855, 291, 577, 281, 747, 437, 321, 600, 3264, 586, 293, 722, 534, 51078], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1133, "seek": 469368, "start": 4707.96, "end": 4712.72, "text": " doing more robust and sophisticated things with that knowledge.", "tokens": [51078, 884, 544, 13956, 293, 16950, 721, 365, 300, 3601, 13, 51316], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1134, "seek": 469368, "start": 4712.72, "end": 4716.320000000001, "text": " So we're going to be working first with the entity ruler, then with the matcher in the", "tokens": [51316, 407, 321, 434, 516, 281, 312, 1364, 700, 365, 264, 13977, 19661, 11, 550, 365, 264, 2995, 260, 294, 264, 51496], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1135, "seek": 469368, "start": 4716.320000000001, "end": 4719.08, "text": " next chapter, then in the components in spacey.", "tokens": [51496, 958, 7187, 11, 550, 294, 264, 6677, 294, 1901, 88, 13, 51634], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1136, "seek": 469368, "start": 4719.08, "end": 4723.16, "text": " So a custom component is a custom function that you can put into a pipeline.", "tokens": [51634, 407, 257, 2375, 6542, 307, 257, 2375, 2445, 300, 291, 393, 829, 666, 257, 15517, 13, 51838], "temperature": 0.0, "avg_logprob": -0.10834011236826578, "compression_ratio": 1.878228782287823, "no_speech_prob": 0.14802362024784088}, {"id": 1137, "seek": 472316, "start": 4723.16, "end": 4726.28, "text": " Then we're going to talk about regex or regular expressions.", "tokens": [50364, 1396, 321, 434, 516, 281, 751, 466, 319, 432, 87, 420, 3890, 15277, 13, 50520], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1138, "seek": 472316, "start": 4726.28, "end": 4728.88, "text": " And then we're going to talk about some advanced regex with spacey.", "tokens": [50520, 400, 550, 321, 434, 516, 281, 751, 466, 512, 7339, 319, 432, 87, 365, 1901, 88, 13, 50650], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1139, "seek": 472316, "start": 4728.88, "end": 4733.72, "text": " If you don't know what regex is, I'm going to cover this in chapter eight.", "tokens": [50650, 759, 291, 500, 380, 458, 437, 319, 432, 87, 307, 11, 286, 478, 516, 281, 2060, 341, 294, 7187, 3180, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1140, "seek": 472316, "start": 4733.72, "end": 4739.0, "text": " So let's go over to our Jupiter notebook that we're going to be using for our entity ruler", "tokens": [50892, 407, 718, 311, 352, 670, 281, 527, 24567, 21060, 300, 321, 434, 516, 281, 312, 1228, 337, 527, 13977, 19661, 51156], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1141, "seek": 472316, "start": 4739.0, "end": 4740.0, "text": " lesson.", "tokens": [51156, 6898, 13, 51206], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1142, "seek": 472316, "start": 4740.0, "end": 4742.68, "text": " So let's go ahead and execute some of these cells.", "tokens": [51206, 407, 718, 311, 352, 2286, 293, 14483, 512, 295, 613, 5438, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1143, "seek": 472316, "start": 4742.68, "end": 4745.599999999999, "text": " And then I'm going to be talking about it in just a second.", "tokens": [51340, 400, 550, 286, 478, 516, 281, 312, 1417, 466, 309, 294, 445, 257, 1150, 13, 51486], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1144, "seek": 472316, "start": 4745.599999999999, "end": 4750.96, "text": " First I want to take some time to explain what the entity ruler is as a pipe in spacey,", "tokens": [51486, 2386, 286, 528, 281, 747, 512, 565, 281, 2903, 437, 264, 13977, 19661, 307, 382, 257, 11240, 294, 1901, 88, 11, 51754], "temperature": 0.0, "avg_logprob": -0.10832210268293108, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.09532277286052704}, {"id": 1145, "seek": 475096, "start": 4750.96, "end": 4755.2, "text": " what it's used for, why you'd find it useful and when to actually implement it.", "tokens": [50364, 437, 309, 311, 1143, 337, 11, 983, 291, 1116, 915, 309, 4420, 293, 562, 281, 767, 4445, 309, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1146, "seek": 475096, "start": 4755.2, "end": 4759.84, "text": " So there are two different ways in which you can kind of add in custom features to a spacey", "tokens": [50576, 407, 456, 366, 732, 819, 2098, 294, 597, 291, 393, 733, 295, 909, 294, 2375, 4122, 281, 257, 1901, 88, 50808], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1147, "seek": 475096, "start": 4759.84, "end": 4761.32, "text": " language pipeline.", "tokens": [50808, 2856, 15517, 13, 50882], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1148, "seek": 475096, "start": 4761.32, "end": 4765.6, "text": " There is a rules based approach and a machine learning based approach.", "tokens": [50882, 821, 307, 257, 4474, 2361, 3109, 293, 257, 3479, 2539, 2361, 3109, 13, 51096], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1149, "seek": 475096, "start": 4765.6, "end": 4769.32, "text": " Rules based approaches should be used when you can think about how to generate a set", "tokens": [51096, 38897, 2361, 11587, 820, 312, 1143, 562, 291, 393, 519, 466, 577, 281, 8460, 257, 992, 51282], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1150, "seek": 475096, "start": 4769.32, "end": 4775.68, "text": " of rules based on either a list of known things or a set of rules that can be generated through", "tokens": [51282, 295, 4474, 2361, 322, 2139, 257, 1329, 295, 2570, 721, 420, 257, 992, 295, 4474, 300, 393, 312, 10833, 807, 51600], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1151, "seek": 475096, "start": 4775.68, "end": 4779.68, "text": " regex, code or linguistic features.", "tokens": [51600, 319, 432, 87, 11, 3089, 420, 43002, 4122, 13, 51800], "temperature": 0.0, "avg_logprob": -0.10504589409663759, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.12249451130628586}, {"id": 1152, "seek": 477968, "start": 4779.68, "end": 4784.16, "text": " Machine learning is when you don't know how to actually write out the rules or the rules", "tokens": [50364, 22155, 2539, 307, 562, 291, 500, 380, 458, 577, 281, 767, 2464, 484, 264, 4474, 420, 264, 4474, 50588], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1153, "seek": 477968, "start": 4784.16, "end": 4787.8, "text": " that you would need to write out would be exceptionally complicated.", "tokens": [50588, 300, 291, 576, 643, 281, 2464, 484, 576, 312, 37807, 6179, 13, 50770], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1154, "seek": 477968, "start": 4787.8, "end": 4791.92, "text": " A great example of a rules based approach versus a machine learning based approach and when", "tokens": [50770, 316, 869, 1365, 295, 257, 4474, 2361, 3109, 5717, 257, 3479, 2539, 2361, 3109, 293, 562, 50976], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1155, "seek": 477968, "start": 4791.92, "end": 4797.08, "text": " to use them is with entity types for named entity recognition.", "tokens": [50976, 281, 764, 552, 307, 365, 13977, 3467, 337, 4926, 13977, 11150, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1156, "seek": 477968, "start": 4797.08, "end": 4801.76, "text": " Imagine if you wanted to extract dates from a text.", "tokens": [51234, 11739, 498, 291, 1415, 281, 8947, 11691, 490, 257, 2487, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1157, "seek": 477968, "start": 4801.76, "end": 4806.200000000001, "text": " There are a finite, very finite number of ways that a date can appear in a text.", "tokens": [51468, 821, 366, 257, 19362, 11, 588, 19362, 1230, 295, 2098, 300, 257, 4002, 393, 4204, 294, 257, 2487, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1267267582463283, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.2627367377281189}, {"id": 1158, "seek": 480620, "start": 4806.2, "end": 4811.96, "text": " You could have something like January 1, 2005, you could have one January 2005, you could", "tokens": [50364, 509, 727, 362, 746, 411, 7061, 502, 11, 14394, 11, 291, 727, 362, 472, 7061, 14394, 11, 291, 727, 50652], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1159, "seek": 480620, "start": 4811.96, "end": 4818.84, "text": " have one Jan 2005, you could have one slash five slash 2005, there's there's different", "tokens": [50652, 362, 472, 4956, 14394, 11, 291, 727, 362, 472, 17330, 1732, 17330, 14394, 11, 456, 311, 456, 311, 819, 50996], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1160, "seek": 480620, "start": 4818.84, "end": 4821.32, "text": " ways that you can do this and there's a lot of them.", "tokens": [50996, 2098, 300, 291, 393, 360, 341, 293, 456, 311, 257, 688, 295, 552, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1161, "seek": 480620, "start": 4821.32, "end": 4825.4, "text": " But there really is a finite number that you could easily write a regex expression for", "tokens": [51120, 583, 456, 534, 307, 257, 19362, 1230, 300, 291, 727, 3612, 2464, 257, 319, 432, 87, 6114, 337, 51324], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1162, "seek": 480620, "start": 4825.4, "end": 4829.599999999999, "text": " a regular expression for to capture all of those.", "tokens": [51324, 257, 3890, 6114, 337, 281, 7983, 439, 295, 729, 13, 51534], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1163, "seek": 480620, "start": 4829.599999999999, "end": 4833.24, "text": " And in fact, those regex expressions already exist.", "tokens": [51534, 400, 294, 1186, 11, 729, 319, 432, 87, 15277, 1217, 2514, 13, 51716], "temperature": 0.0, "avg_logprob": -0.1367836448381532, "compression_ratio": 1.9904761904761905, "no_speech_prob": 0.569570779800415}, {"id": 1164, "seek": 483324, "start": 4833.24, "end": 4837.36, "text": " That's why spacey is already really good at identifying dates.", "tokens": [50364, 663, 311, 983, 1901, 88, 307, 1217, 534, 665, 412, 16696, 11691, 13, 50570], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1165, "seek": 483324, "start": 4837.36, "end": 4843.04, "text": " So dates are something that you would probably use a rules based approach for something that's", "tokens": [50570, 407, 11691, 366, 746, 300, 291, 576, 1391, 764, 257, 4474, 2361, 3109, 337, 746, 300, 311, 50854], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1166, "seek": 483324, "start": 4843.04, "end": 4847.88, "text": " a good machine learning approach for or something like names.", "tokens": [50854, 257, 665, 3479, 2539, 3109, 337, 420, 746, 411, 5288, 13, 51096], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1167, "seek": 483324, "start": 4847.88, "end": 4853.88, "text": " If you wanted to capture the names of people, you would have to generate an entity ruler", "tokens": [51096, 759, 291, 1415, 281, 7983, 264, 5288, 295, 561, 11, 291, 576, 362, 281, 8460, 364, 13977, 19661, 51396], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1168, "seek": 483324, "start": 4853.88, "end": 4857.679999999999, "text": " with a whole bunch of robust features.", "tokens": [51396, 365, 257, 1379, 3840, 295, 13956, 4122, 13, 51586], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1169, "seek": 483324, "start": 4857.679999999999, "end": 4862.84, "text": " So you would have to have a list of all known possible first names, all known possible last", "tokens": [51586, 407, 291, 576, 362, 281, 362, 257, 1329, 295, 439, 2570, 1944, 700, 5288, 11, 439, 2570, 1944, 1036, 51844], "temperature": 0.0, "avg_logprob": -0.11020167350769043, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.15197768807411194}, {"id": 1170, "seek": 486284, "start": 4862.92, "end": 4869.92, "text": " names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc.", "tokens": [50368, 5288, 11, 439, 2570, 1944, 18417, 36005, 411, 4631, 11, 2221, 293, 9814, 11, 5275, 11, 5275, 11, 6140, 11, 5183, 13, 50718], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1171, "seek": 486284, "start": 4870.32, "end": 4872.6, "text": " And you'd have to have a list of all known suffixes.", "tokens": [50738, 400, 291, 1116, 362, 281, 362, 257, 1329, 295, 439, 2570, 3889, 36005, 13, 50852], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1172, "seek": 486284, "start": 4872.6, "end": 4877.32, "text": " So junior, senior, the third, the fourth, etc. on down the list.", "tokens": [50852, 407, 16195, 11, 7965, 11, 264, 2636, 11, 264, 6409, 11, 5183, 13, 322, 760, 264, 1329, 13, 51088], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1173, "seek": 486284, "start": 4877.32, "end": 4882.52, "text": " This would be very, very difficult to write because first of all, the quantity of names", "tokens": [51088, 639, 576, 312, 588, 11, 588, 2252, 281, 2464, 570, 700, 295, 439, 11, 264, 11275, 295, 5288, 51348], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1174, "seek": 486284, "start": 4882.52, "end": 4885.4400000000005, "text": " that exist in the world are massive.", "tokens": [51348, 300, 2514, 294, 264, 1002, 366, 5994, 13, 51494], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1175, "seek": 486284, "start": 4885.4400000000005, "end": 4889.4400000000005, "text": " The quantity of last names that exist in the world is massive.", "tokens": [51494, 440, 11275, 295, 1036, 5288, 300, 2514, 294, 264, 1002, 307, 5994, 13, 51694], "temperature": 0.0, "avg_logprob": -0.22401417527243356, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.05831720307469368}, {"id": 1176, "seek": 488944, "start": 4889.44, "end": 4894.04, "text": " There's not a set gazetteer or set list out there of these anywhere.", "tokens": [50364, 821, 311, 406, 257, 992, 26232, 3007, 260, 420, 992, 1329, 484, 456, 295, 613, 4992, 13, 50594], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1177, "seek": 488944, "start": 4894.04, "end": 4899.04, "text": " So for this reason, oftentimes things like people names will be worked into machine learning", "tokens": [50594, 407, 337, 341, 1778, 11, 18349, 721, 411, 561, 5288, 486, 312, 2732, 666, 3479, 2539, 50844], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1178, "seek": 488944, "start": 4899.04, "end": 4900.04, "text": " components.", "tokens": [50844, 6677, 13, 50894], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1179, "seek": 488944, "start": 4900.04, "end": 4904.04, "text": " I'm going to address machine learning in another video at a later date, but right now we're", "tokens": [50894, 286, 478, 516, 281, 2985, 3479, 2539, 294, 1071, 960, 412, 257, 1780, 4002, 11, 457, 558, 586, 321, 434, 51094], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1180, "seek": 488944, "start": 4904.04, "end": 4907.32, "text": " going to focus on a rules based approach.", "tokens": [51094, 516, 281, 1879, 322, 257, 4474, 2361, 3109, 13, 51258], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1181, "seek": 488944, "start": 4907.32, "end": 4914.32, "text": " So using the rules based features that spacey offers, a good NLP practitioner will be excellent", "tokens": [51258, 407, 1228, 264, 4474, 2361, 4122, 300, 1901, 88, 7736, 11, 257, 665, 426, 45196, 32125, 486, 312, 7103, 51608], "temperature": 0.0, "avg_logprob": -0.15568833498610662, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.03845937177538872}, {"id": 1182, "seek": 491432, "start": 4915.0, "end": 4920.12, "text": " at both rules based approaches and machine learning based approaches and knowing when", "tokens": [50398, 412, 1293, 4474, 2361, 11587, 293, 3479, 2539, 2361, 11587, 293, 5276, 562, 50654], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1183, "seek": 491432, "start": 4920.12, "end": 4926.92, "text": " to use which approach and when maybe maybe a task is not appropriate for machine learning", "tokens": [50654, 281, 764, 597, 3109, 293, 562, 1310, 1310, 257, 5633, 307, 406, 6854, 337, 3479, 2539, 50994], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1184, "seek": 491432, "start": 4926.92, "end": 4930.799999999999, "text": " when it can be worked in with rules relatively well.", "tokens": [50994, 562, 309, 393, 312, 2732, 294, 365, 4474, 7226, 731, 13, 51188], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1185, "seek": 491432, "start": 4930.799999999999, "end": 4935.5599999999995, "text": " If you're taking a rules based approach, the approach that you take should have a high", "tokens": [51188, 759, 291, 434, 1940, 257, 4474, 2361, 3109, 11, 264, 3109, 300, 291, 747, 820, 362, 257, 1090, 51426], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1186, "seek": 491432, "start": 4935.5599999999995, "end": 4940.639999999999, "text": " degree of confidence that the rules will always return true positives.", "tokens": [51426, 4314, 295, 6687, 300, 264, 4474, 486, 1009, 2736, 2074, 35127, 13, 51680], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1187, "seek": 491432, "start": 4940.639999999999, "end": 4942.4, "text": " And you need to think about that.", "tokens": [51680, 400, 291, 643, 281, 519, 466, 300, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10566042817157248, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.009125139564275742}, {"id": 1188, "seek": 494240, "start": 4942.4, "end": 4947.639999999999, "text": " If you are okay with your rules, maybe catching a few false positives or missing a few true", "tokens": [50364, 759, 291, 366, 1392, 365, 428, 4474, 11, 1310, 16124, 257, 1326, 7908, 35127, 420, 5361, 257, 1326, 2074, 50626], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1189, "seek": 494240, "start": 4947.639999999999, "end": 4952.799999999999, "text": " positives, then maybe think about how you write the rules and allowing for those and", "tokens": [50626, 35127, 11, 550, 1310, 519, 466, 577, 291, 2464, 264, 4474, 293, 8293, 337, 729, 293, 50884], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1190, "seek": 494240, "start": 4952.799999999999, "end": 4955.04, "text": " making it known in your documentation.", "tokens": [50884, 1455, 309, 2570, 294, 428, 14333, 13, 50996], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1191, "seek": 494240, "start": 4955.04, "end": 4959.759999999999, "text": " So that's generally what a rules based approach is in an entity ruler is a way that we can", "tokens": [50996, 407, 300, 311, 5101, 437, 257, 4474, 2361, 3109, 307, 294, 364, 13977, 19661, 307, 257, 636, 300, 321, 393, 51232], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1192, "seek": 494240, "start": 4959.759999999999, "end": 4966.759999999999, "text": " use a list or a series of features, language features to add tokens into the entity, the", "tokens": [51232, 764, 257, 1329, 420, 257, 2638, 295, 4122, 11, 2856, 4122, 281, 909, 22667, 666, 264, 13977, 11, 264, 51582], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1193, "seek": 494240, "start": 4967.32, "end": 4971.0599999999995, "text": " dot ints container within the dot container.", "tokens": [51610, 5893, 560, 82, 10129, 1951, 264, 5893, 10129, 13, 51797], "temperature": 0.0, "avg_logprob": -0.15020165993617132, "compression_ratio": 1.76, "no_speech_prob": 0.1008366271853447}, {"id": 1194, "seek": 497106, "start": 4971.06, "end": 4973.9400000000005, "text": " So let's go ahead and try to do this right now.", "tokens": [50364, 407, 718, 311, 352, 2286, 293, 853, 281, 360, 341, 558, 586, 13, 50508], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1195, "seek": 497106, "start": 4973.9400000000005, "end": 4977.18, "text": " The text we're going to be working with is a kind of fun one, I think.", "tokens": [50508, 440, 2487, 321, 434, 516, 281, 312, 1364, 365, 307, 257, 733, 295, 1019, 472, 11, 286, 519, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1196, "seek": 497106, "start": 4977.18, "end": 4981.820000000001, "text": " So if you've already gotten the reference, congratulations, it's kind of obscure.", "tokens": [50670, 407, 498, 291, 600, 1217, 5768, 264, 6408, 11, 13568, 11, 309, 311, 733, 295, 34443, 13, 50902], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1197, "seek": 497106, "start": 4981.820000000001, "end": 4984.54, "text": " But we're going to have a sentence right here that I just wrote out.", "tokens": [50902, 583, 321, 434, 516, 281, 362, 257, 8174, 558, 510, 300, 286, 445, 4114, 484, 13, 51038], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1198, "seek": 497106, "start": 4984.54, "end": 4988.18, "text": " West Chesterton Fieldville was referenced in Mr. Deeds.", "tokens": [51038, 4055, 47981, 42112, 17952, 8386, 390, 32734, 294, 2221, 13, 1346, 5147, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1199, "seek": 497106, "start": 4988.18, "end": 4991.18, "text": " So in this context, we are going to have a few different entities.", "tokens": [51220, 407, 294, 341, 4319, 11, 321, 366, 516, 281, 362, 257, 1326, 819, 16667, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1200, "seek": 497106, "start": 4991.18, "end": 4996.9800000000005, "text": " We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.", "tokens": [51370, 492, 528, 527, 2316, 420, 527, 15517, 281, 8947, 4055, 47981, 42112, 17952, 8386, 382, 257, 460, 5208, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1201, "seek": 497106, "start": 4996.9800000000005, "end": 4999.02, "text": " It's a fake place that doesn't really exist.", "tokens": [51660, 467, 311, 257, 7592, 1081, 300, 1177, 380, 534, 2514, 13, 51762], "temperature": 0.0, "avg_logprob": -0.1196815274285932, "compression_ratio": 1.7416107382550337, "no_speech_prob": 0.16016194224357605}, {"id": 1202, "seek": 499902, "start": 4999.02, "end": 5001.38, "text": " It was made up in the movie Mr. Deeds.", "tokens": [50364, 467, 390, 1027, 493, 294, 264, 3169, 2221, 13, 1346, 5147, 13, 50482], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1203, "seek": 499902, "start": 5001.38, "end": 5005.18, "text": " And what we want is for Mr. Deeds to be grabbed as an entity as well.", "tokens": [50482, 400, 437, 321, 528, 307, 337, 2221, 13, 1346, 5147, 281, 312, 18607, 382, 364, 13977, 382, 731, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1204, "seek": 499902, "start": 5005.18, "end": 5008.1, "text": " And this would ideally be labeled as a film.", "tokens": [50672, 400, 341, 576, 22915, 312, 21335, 382, 257, 2007, 13, 50818], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1205, "seek": 499902, "start": 5008.1, "end": 5010.780000000001, "text": " But in this case, that's probably not going to happen.", "tokens": [50818, 583, 294, 341, 1389, 11, 300, 311, 1391, 406, 516, 281, 1051, 13, 50952], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1206, "seek": 499902, "start": 5010.780000000001, "end": 5012.740000000001, "text": " Let's go ahead and see what does happen.", "tokens": [50952, 961, 311, 352, 2286, 293, 536, 437, 775, 1051, 13, 51050], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1207, "seek": 499902, "start": 5012.740000000001, "end": 5019.580000000001, "text": " So we're going to say for int and doc dot ints, print off int dot text, int dot label,", "tokens": [51050, 407, 321, 434, 516, 281, 584, 337, 560, 293, 3211, 5893, 560, 82, 11, 4482, 766, 560, 5893, 2487, 11, 560, 5893, 7645, 11, 51392], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1208, "seek": 499902, "start": 5019.580000000001, "end": 5024.02, "text": " like we learned from our NER lesson a few moments ago.", "tokens": [51392, 411, 321, 3264, 490, 527, 426, 1598, 6898, 257, 1326, 6065, 2057, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1209, "seek": 499902, "start": 5024.02, "end": 5026.1, "text": " And we see that the output looks like this.", "tokens": [51614, 400, 321, 536, 300, 264, 5598, 1542, 411, 341, 13, 51718], "temperature": 0.0, "avg_logprob": -0.15635395789331244, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.011686663143336773}, {"id": 1210, "seek": 502610, "start": 5026.1, "end": 5028.700000000001, "text": " It's gotten almost all the entities that we wanted.", "tokens": [50364, 467, 311, 5768, 1920, 439, 264, 16667, 300, 321, 1415, 13, 50494], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1211, "seek": 502610, "start": 5028.700000000001, "end": 5030.9400000000005, "text": " Mr. was left off of Deeds.", "tokens": [50494, 2221, 13, 390, 1411, 766, 295, 1346, 5147, 13, 50606], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1212, "seek": 502610, "start": 5030.9400000000005, "end": 5034.620000000001, "text": " And it's grabbed the West Chesterton Fieldville and labeled it as a person.", "tokens": [50606, 400, 309, 311, 18607, 264, 4055, 47981, 42112, 17952, 8386, 293, 21335, 309, 382, 257, 954, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1213, "seek": 502610, "start": 5034.620000000001, "end": 5036.02, "text": " So what's gone wrong here?", "tokens": [50790, 407, 437, 311, 2780, 2085, 510, 30, 50860], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1214, "seek": 502610, "start": 5036.02, "end": 5038.18, "text": " Well, there's a few different things that have gone wrong.", "tokens": [50860, 1042, 11, 456, 311, 257, 1326, 819, 721, 300, 362, 2780, 2085, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1215, "seek": 502610, "start": 5038.18, "end": 5042.9800000000005, "text": " The NCORE Web SM model is a machine learning model for NER.", "tokens": [50968, 440, 20786, 16193, 9573, 13115, 2316, 307, 257, 3479, 2539, 2316, 337, 426, 1598, 13, 51208], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1216, "seek": 502610, "start": 5042.9800000000005, "end": 5045.14, "text": " The word vectors are not saved.", "tokens": [51208, 440, 1349, 18875, 366, 406, 6624, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1217, "seek": 502610, "start": 5045.14, "end": 5047.860000000001, "text": " So the static vectors are not in it.", "tokens": [51316, 407, 264, 13437, 18875, 366, 406, 294, 309, 13, 51452], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1218, "seek": 502610, "start": 5047.860000000001, "end": 5050.02, "text": " So it's making the best prediction that it can.", "tokens": [51452, 407, 309, 311, 1455, 264, 1151, 17630, 300, 309, 393, 13, 51560], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1219, "seek": 502610, "start": 5050.02, "end": 5054.780000000001, "text": " But even with a very robust machine learning model, unless it has seen West Chesterton", "tokens": [51560, 583, 754, 365, 257, 588, 13956, 3479, 2539, 2316, 11, 5969, 309, 575, 1612, 4055, 47981, 42112, 51798], "temperature": 0.0, "avg_logprob": -0.12124461021976195, "compression_ratio": 1.68561872909699, "no_speech_prob": 0.0031726257875561714}, {"id": 1220, "seek": 505478, "start": 5054.78, "end": 5061.179999999999, "text": " Fieldville, there is not really a good way for the model to actually know that that's", "tokens": [50364, 17952, 8386, 11, 456, 307, 406, 534, 257, 665, 636, 337, 264, 2316, 281, 767, 458, 300, 300, 311, 50684], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1221, "seek": 505478, "start": 5061.179999999999, "end": 5062.7, "text": " a place.", "tokens": [50684, 257, 1081, 13, 50760], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1222, "seek": 505478, "start": 5062.7, "end": 5068.74, "text": " Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer", "tokens": [50760, 16581, 309, 311, 1612, 257, 3877, 411, 4055, 47981, 42112, 11, 293, 1310, 309, 393, 652, 493, 257, 2041, 11, 257, 31782, 51062], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1223, "seek": 505478, "start": 5068.74, "end": 5071.5, "text": " model might actually get this right.", "tokens": [51062, 2316, 1062, 767, 483, 341, 558, 13, 51200], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1224, "seek": 505478, "start": 5071.5, "end": 5073.54, "text": " But for the most part, this is a very challenging thing.", "tokens": [51200, 583, 337, 264, 881, 644, 11, 341, 307, 257, 588, 7595, 551, 13, 51302], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1225, "seek": 505478, "start": 5073.54, "end": 5074.9, "text": " This would be challenging for a human.", "tokens": [51302, 639, 576, 312, 7595, 337, 257, 1952, 13, 51370], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1226, "seek": 505478, "start": 5074.9, "end": 5079.62, "text": " There's not a lot of context here to tell you what this kind of entity is, unless you", "tokens": [51370, 821, 311, 406, 257, 688, 295, 4319, 510, 281, 980, 291, 437, 341, 733, 295, 13977, 307, 11, 5969, 291, 51606], "temperature": 0.0, "avg_logprob": -0.13761390580071342, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.18707622587680817}, {"id": 1227, "seek": 507962, "start": 5079.62, "end": 5088.54, "text": " knew a lot about how maybe northeastern villages and towns in North America would be called.", "tokens": [50364, 2586, 257, 688, 466, 577, 1310, 6830, 68, 32579, 20444, 293, 18104, 294, 4067, 3374, 576, 312, 1219, 13, 50810], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1228, "seek": 507962, "start": 5088.54, "end": 5094.0199999999995, "text": " Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.", "tokens": [50810, 2743, 11, 2221, 13, 1346, 5147, 307, 406, 34086, 382, 257, 1379, 13977, 11, 445, 1346, 5147, 307, 13, 51084], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1229, "seek": 507962, "start": 5094.0199999999995, "end": 5099.42, "text": " Now ideally, we would have an NER model that would label West Chesterton Fieldville as", "tokens": [51084, 823, 22915, 11, 321, 576, 362, 364, 426, 1598, 2316, 300, 576, 7645, 4055, 47981, 42112, 17952, 8386, 382, 51354], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1230, "seek": 507962, "start": 5099.42, "end": 5101.94, "text": " a GPE and Mr. Deeds as a film.", "tokens": [51354, 257, 460, 5208, 293, 2221, 13, 1346, 5147, 382, 257, 2007, 13, 51480], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1231, "seek": 507962, "start": 5101.94, "end": 5103.66, "text": " But we've got two problems.", "tokens": [51480, 583, 321, 600, 658, 732, 2740, 13, 51566], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1232, "seek": 507962, "start": 5103.66, "end": 5108.46, "text": " One, the machine learning model doesn't have film as an entity type.", "tokens": [51566, 1485, 11, 264, 3479, 2539, 2316, 1177, 380, 362, 2007, 382, 364, 13977, 2010, 13, 51806], "temperature": 0.0, "avg_logprob": -0.17478359767368862, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.4147949516773224}, {"id": 1233, "seek": 510846, "start": 5108.46, "end": 5113.9800000000005, "text": " And on top of that, West Chesterton Fieldville is not coming out correct as GPE.", "tokens": [50364, 400, 322, 1192, 295, 300, 11, 4055, 47981, 42112, 17952, 8386, 307, 406, 1348, 484, 3006, 382, 460, 5208, 13, 50640], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1234, "seek": 510846, "start": 5113.9800000000005, "end": 5118.82, "text": " So our goal right now is to fix both of these problems with an entity ruler.", "tokens": [50640, 407, 527, 3387, 558, 586, 307, 281, 3191, 1293, 295, 613, 2740, 365, 364, 13977, 19661, 13, 50882], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1235, "seek": 510846, "start": 5118.82, "end": 5124.3, "text": " This would be useful if I were maybe doing some text analysis on fictional places referenced", "tokens": [50882, 639, 576, 312, 4420, 498, 286, 645, 1310, 884, 512, 2487, 5215, 322, 28911, 3190, 32734, 51156], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1236, "seek": 510846, "start": 5124.3, "end": 5125.46, "text": " in films.", "tokens": [51156, 294, 7796, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1237, "seek": 510846, "start": 5125.46, "end": 5129.7, "text": " So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all", "tokens": [51214, 407, 721, 411, 426, 1083, 654, 11, 1310, 10775, 4755, 11, 4055, 47981, 42112, 17952, 8386, 11, 613, 576, 439, 51426], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1238, "seek": 510846, "start": 5129.7, "end": 5131.82, "text": " be classified as kind of fictional places.", "tokens": [51426, 312, 20627, 382, 733, 295, 28911, 3190, 13, 51532], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1239, "seek": 510846, "start": 5131.82, "end": 5136.66, "text": " So let's go ahead and make a ruler to correct this problem.", "tokens": [51532, 407, 718, 311, 352, 2286, 293, 652, 257, 19661, 281, 3006, 341, 1154, 13, 51774], "temperature": 0.0, "avg_logprob": -0.09562219653213233, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.12248095124959946}, {"id": 1240, "seek": 513666, "start": 5136.66, "end": 5142.099999999999, "text": " So what we're going to do is first we're going to make a ruler by saying ruler is equal", "tokens": [50364, 407, 437, 321, 434, 516, 281, 360, 307, 700, 321, 434, 516, 281, 652, 257, 19661, 538, 1566, 19661, 307, 2681, 50636], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1241, "seek": 513666, "start": 5142.099999999999, "end": 5146.92, "text": " to NLP dot add pipe.", "tokens": [50636, 281, 426, 45196, 5893, 909, 11240, 13, 50877], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1242, "seek": 513666, "start": 5146.92, "end": 5150.62, "text": " And this is going to take one argument here, you're going to find out when we start working", "tokens": [50877, 400, 341, 307, 516, 281, 747, 472, 6770, 510, 11, 291, 434, 516, 281, 915, 484, 562, 321, 722, 1364, 51062], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1243, "seek": 513666, "start": 5150.62, "end": 5154.3, "text": " with custom components that you can have a few different arguments here, especially", "tokens": [51062, 365, 2375, 6677, 300, 291, 393, 362, 257, 1326, 819, 12869, 510, 11, 2318, 51246], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1244, "seek": 513666, "start": 5154.3, "end": 5156.3, "text": " if you create your own custom components.", "tokens": [51246, 498, 291, 1884, 428, 1065, 2375, 6677, 13, 51346], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1245, "seek": 513666, "start": 5156.3, "end": 5160.3, "text": " But for right now, we're working with the components that come standard with spacey.", "tokens": [51346, 583, 337, 558, 586, 11, 321, 434, 1364, 365, 264, 6677, 300, 808, 3832, 365, 1901, 88, 13, 51546], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1246, "seek": 513666, "start": 5160.3, "end": 5161.86, "text": " There's about 18 of them.", "tokens": [51546, 821, 311, 466, 2443, 295, 552, 13, 51624], "temperature": 0.0, "avg_logprob": -0.14712495202416773, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.10667559504508972}, {"id": 1247, "seek": 516186, "start": 5161.86, "end": 5167.219999999999, "text": " One of them is the entity underscore ruler, all lowercase.", "tokens": [50364, 1485, 295, 552, 307, 264, 13977, 37556, 19661, 11, 439, 3126, 9765, 13, 50632], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1248, "seek": 516186, "start": 5167.219999999999, "end": 5170.98, "text": " We're going to add that ruler into our NLP model.", "tokens": [50632, 492, 434, 516, 281, 909, 300, 19661, 666, 527, 426, 45196, 2316, 13, 50820], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1249, "seek": 516186, "start": 5170.98, "end": 5177.98, "text": " And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER", "tokens": [50820, 400, 498, 321, 360, 426, 45196, 5893, 12477, 37556, 21882, 293, 14483, 300, 11, 321, 393, 586, 574, 412, 527, 426, 1598, 51170], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1250, "seek": 516186, "start": 5177.98, "end": 5185.78, "text": " model and see as we go down that the NER pipe is here and the entity ruler is now the exit,", "tokens": [51170, 2316, 293, 536, 382, 321, 352, 760, 300, 264, 426, 1598, 11240, 307, 510, 293, 264, 13977, 19661, 307, 586, 264, 11043, 11, 51560], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1251, "seek": 516186, "start": 5185.78, "end": 5187.74, "text": " the final pipe in our pipeline.", "tokens": [51560, 264, 2572, 11240, 294, 527, 15517, 13, 51658], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1252, "seek": 516186, "start": 5187.74, "end": 5190.66, "text": " So we see that it has been successfully added.", "tokens": [51658, 407, 321, 536, 300, 309, 575, 668, 10727, 3869, 13, 51804], "temperature": 0.0, "avg_logprob": -0.16565225638595282, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.1259050965309143}, {"id": 1253, "seek": 519066, "start": 5190.66, "end": 5195.54, "text": " Let's go ahead now and try to add patterns into that pipeline.", "tokens": [50364, 961, 311, 352, 2286, 586, 293, 853, 281, 909, 8294, 666, 300, 15517, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1254, "seek": 519066, "start": 5195.54, "end": 5199.78, "text": " Patterns are the things that the spacey model is going to look for in the label that it's", "tokens": [50608, 34367, 3695, 366, 264, 721, 300, 264, 1901, 88, 2316, 307, 516, 281, 574, 337, 294, 264, 7645, 300, 309, 311, 50820], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1255, "seek": 519066, "start": 5199.78, "end": 5203.3, "text": " going to assign when it finds something that meets that pattern.", "tokens": [50820, 516, 281, 6269, 562, 309, 10704, 746, 300, 13961, 300, 5102, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1256, "seek": 519066, "start": 5203.3, "end": 5206.26, "text": " This will always be a list of lists.", "tokens": [50996, 639, 486, 1009, 312, 257, 1329, 295, 14511, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1257, "seek": 519066, "start": 5206.26, "end": 5208.139999999999, "text": " So let's go ahead and do this right now.", "tokens": [51144, 407, 718, 311, 352, 2286, 293, 360, 341, 558, 586, 13, 51238], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1258, "seek": 519066, "start": 5208.139999999999, "end": 5210.099999999999, "text": " Sorry, a list of dictionaries.", "tokens": [51238, 4919, 11, 257, 1329, 295, 22352, 4889, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1259, "seek": 519066, "start": 5210.099999999999, "end": 5217.38, "text": " So the first pattern that we're really looking for here is going to be a dictionary.", "tokens": [51336, 407, 264, 700, 5102, 300, 321, 434, 534, 1237, 337, 510, 307, 516, 281, 312, 257, 25890, 13, 51700], "temperature": 0.0, "avg_logprob": -0.1098862569266503, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.022283893078565598}, {"id": 1260, "seek": 521738, "start": 5217.38, "end": 5224.34, "text": " It's going to have one key of label, which is going to be equal to GPE and another label", "tokens": [50364, 467, 311, 516, 281, 362, 472, 2141, 295, 7645, 11, 597, 307, 516, 281, 312, 2681, 281, 460, 5208, 293, 1071, 7645, 50712], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1261, "seek": 521738, "start": 5224.34, "end": 5230.06, "text": " of pattern, which is going to be equal to, in this case, we want to find West Chesterton", "tokens": [50712, 295, 5102, 11, 597, 307, 516, 281, 312, 2681, 281, 11, 294, 341, 1389, 11, 321, 528, 281, 915, 4055, 47981, 42112, 50998], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1262, "seek": 521738, "start": 5230.06, "end": 5231.06, "text": " Fieldville.", "tokens": [50998, 17952, 8386, 13, 51048], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1263, "seek": 521738, "start": 5231.06, "end": 5239.02, "text": " Let me go ahead and just copy and paste it so I don't make a mistake here.", "tokens": [51048, 961, 385, 352, 2286, 293, 445, 5055, 293, 9163, 309, 370, 286, 500, 380, 652, 257, 6146, 510, 13, 51446], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1264, "seek": 521738, "start": 5239.02, "end": 5243.62, "text": " And what we want to do is we want our entity ruler to see West Chesterton Fieldville.", "tokens": [51446, 400, 437, 321, 528, 281, 360, 307, 321, 528, 527, 13977, 19661, 281, 536, 4055, 47981, 42112, 17952, 8386, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1265, "seek": 521738, "start": 5243.62, "end": 5246.38, "text": " And when it sees it, assign the label of GPE.", "tokens": [51676, 400, 562, 309, 8194, 309, 11, 6269, 264, 7645, 295, 460, 5208, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11932798402499309, "compression_ratio": 1.7918552036199096, "no_speech_prob": 0.020331354811787605}, {"id": 1266, "seek": 524638, "start": 5246.38, "end": 5247.9400000000005, "text": " So it's a geopolitical entity.", "tokens": [50364, 407, 309, 311, 257, 46615, 804, 13977, 13, 50442], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1267, "seek": 524638, "start": 5247.9400000000005, "end": 5249.74, "text": " So it's a place.", "tokens": [50442, 407, 309, 311, 257, 1081, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1268, "seek": 524638, "start": 5249.74, "end": 5251.34, "text": " So let's go ahead and execute that.", "tokens": [50532, 407, 718, 311, 352, 2286, 293, 14483, 300, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1269, "seek": 524638, "start": 5251.34, "end": 5252.34, "text": " Great.", "tokens": [50612, 3769, 13, 50662], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1270, "seek": 524638, "start": 5252.34, "end": 5253.34, "text": " We've got the patterns.", "tokens": [50662, 492, 600, 658, 264, 8294, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1271, "seek": 524638, "start": 5253.34, "end": 5255.74, "text": " Now comes time to load them into the ruler.", "tokens": [50712, 823, 1487, 565, 281, 3677, 552, 666, 264, 19661, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1272, "seek": 524638, "start": 5255.74, "end": 5260.02, "text": " So we can say ruler.add underscore patterns.", "tokens": [50832, 407, 321, 393, 584, 19661, 13, 25224, 37556, 8294, 13, 51046], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1273, "seek": 524638, "start": 5260.02, "end": 5261.34, "text": " This is going to take one argument.", "tokens": [51046, 639, 307, 516, 281, 747, 472, 6770, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1274, "seek": 524638, "start": 5261.34, "end": 5266.38, "text": " It's going to be our list of patterns added in.", "tokens": [51112, 467, 311, 516, 281, 312, 527, 1329, 295, 8294, 3869, 294, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1275, "seek": 524638, "start": 5266.38, "end": 5267.54, "text": " Cool.", "tokens": [51364, 8561, 13, 51422], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1276, "seek": 524638, "start": 5267.54, "end": 5269.02, "text": " Now let's create a new doc object.", "tokens": [51422, 823, 718, 311, 1884, 257, 777, 3211, 2657, 13, 51496], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1277, "seek": 524638, "start": 5269.02, "end": 5272.54, "text": " We're going to call this doc to that's going to be equal to NLP.", "tokens": [51496, 492, 434, 516, 281, 818, 341, 3211, 281, 300, 311, 516, 281, 312, 2681, 281, 426, 45196, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1278, "seek": 524638, "start": 5272.54, "end": 5275.7, "text": " We're going to pass in that same text.", "tokens": [51672, 492, 434, 516, 281, 1320, 294, 300, 912, 2487, 13, 51830], "temperature": 0.0, "avg_logprob": -0.2121852653613989, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.048851124942302704}, {"id": 1279, "seek": 527570, "start": 5275.7, "end": 5283.78, "text": " We're going to say for int n doc to dot ints print off int dot text and end dot label.", "tokens": [50364, 492, 434, 516, 281, 584, 337, 560, 297, 3211, 281, 5893, 560, 82, 4482, 766, 560, 5893, 2487, 293, 917, 5893, 7645, 13, 50768], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1280, "seek": 527570, "start": 5283.78, "end": 5288.94, "text": " And you're going to notice that nothing has changed.", "tokens": [50768, 400, 291, 434, 516, 281, 3449, 300, 1825, 575, 3105, 13, 51026], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1281, "seek": 527570, "start": 5288.94, "end": 5290.7, "text": " So why has nothing changed?", "tokens": [51026, 407, 983, 575, 1825, 3105, 30, 51114], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1282, "seek": 527570, "start": 5290.7, "end": 5292.66, "text": " We're still getting the same results.", "tokens": [51114, 492, 434, 920, 1242, 264, 912, 3542, 13, 51212], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1283, "seek": 527570, "start": 5292.66, "end": 5294.78, "text": " And we've added the correct pattern in.", "tokens": [51212, 400, 321, 600, 3869, 264, 3006, 5102, 294, 13, 51318], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1284, "seek": 527570, "start": 5294.78, "end": 5297.46, "text": " The answer lies into one key thing.", "tokens": [51318, 440, 1867, 9134, 666, 472, 2141, 551, 13, 51452], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1285, "seek": 527570, "start": 5297.46, "end": 5303.34, "text": " If we look back up here, we see that our entity ruler comes after our NER.", "tokens": [51452, 759, 321, 574, 646, 493, 510, 11, 321, 536, 300, 527, 13977, 19661, 1487, 934, 527, 426, 1598, 13, 51746], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1286, "seek": 527570, "start": 5303.34, "end": 5304.34, "text": " What does that mean?", "tokens": [51746, 708, 775, 300, 914, 30, 51796], "temperature": 0.0, "avg_logprob": -0.24053434568030813, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.01322182733565569}, {"id": 1287, "seek": 530434, "start": 5304.34, "end": 5308.06, "text": " Well, imagine how the pipeline works that I talked about a little while ago in this", "tokens": [50364, 1042, 11, 3811, 577, 264, 15517, 1985, 300, 286, 2825, 466, 257, 707, 1339, 2057, 294, 341, 50550], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1288, "seek": 530434, "start": 5308.06, "end": 5309.06, "text": " video.", "tokens": [50550, 960, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1289, "seek": 530434, "start": 5309.06, "end": 5314.42, "text": " A pipeline works by different components, adding things to an object and making changes", "tokens": [50600, 316, 15517, 1985, 538, 819, 6677, 11, 5127, 721, 281, 364, 2657, 293, 1455, 2962, 50868], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1290, "seek": 530434, "start": 5314.42, "end": 5321.46, "text": " to it, in this case, adding ints to it, and then making those things isolated from later", "tokens": [50868, 281, 309, 11, 294, 341, 1389, 11, 5127, 560, 82, 281, 309, 11, 293, 550, 1455, 729, 721, 14621, 490, 1780, 51220], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1291, "seek": 530434, "start": 5321.46, "end": 5324.9400000000005, "text": " pipes from being able to overwrite them unless specified.", "tokens": [51220, 21882, 490, 885, 1075, 281, 670, 21561, 552, 5969, 22206, 13, 51394], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1292, "seek": 530434, "start": 5324.9400000000005, "end": 5329.860000000001, "text": " What this means is that when West Chesterton field bill goes through and is identified", "tokens": [51394, 708, 341, 1355, 307, 300, 562, 4055, 47981, 42112, 2519, 2961, 1709, 807, 293, 307, 9234, 51640], "temperature": 0.0, "avg_logprob": -0.13951819141705832, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.003272868227213621}, {"id": 1293, "seek": 532986, "start": 5329.86, "end": 5337.0199999999995, "text": " by the NER pipe as a person, it can no longer be identified as anything else.", "tokens": [50364, 538, 264, 426, 1598, 11240, 382, 257, 954, 11, 309, 393, 572, 2854, 312, 9234, 382, 1340, 1646, 13, 50722], "temperature": 0.0, "avg_logprob": -0.13214061910455877, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.11918189376592636}, {"id": 1294, "seek": 532986, "start": 5337.0199999999995, "end": 5341.5199999999995, "text": " What this means is that you need to do one of two things give your ruler the ability", "tokens": [50722, 708, 341, 1355, 307, 300, 291, 643, 281, 360, 472, 295, 732, 721, 976, 428, 19661, 264, 3485, 50947], "temperature": 0.0, "avg_logprob": -0.13214061910455877, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.11918189376592636}, {"id": 1295, "seek": 532986, "start": 5341.5199999999995, "end": 5349.16, "text": " to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline.", "tokens": [50947, 281, 670, 21561, 264, 426, 1598, 11, 420, 341, 307, 452, 2973, 17502, 11, 829, 309, 949, 264, 426, 1598, 294, 264, 15517, 13, 51329], "temperature": 0.0, "avg_logprob": -0.13214061910455877, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.11918189376592636}, {"id": 1296, "seek": 532986, "start": 5349.16, "end": 5353.219999999999, "text": " So let's go through and solve this common problem right now.", "tokens": [51329, 407, 718, 311, 352, 807, 293, 5039, 341, 2689, 1154, 558, 586, 13, 51532], "temperature": 0.0, "avg_logprob": -0.13214061910455877, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.11918189376592636}, {"id": 1297, "seek": 532986, "start": 5353.219999999999, "end": 5358.46, "text": " We're going to create a new NLP object called NLP to, which is going to be equal to spacey", "tokens": [51532, 492, 434, 516, 281, 1884, 257, 777, 426, 45196, 2657, 1219, 426, 45196, 281, 11, 597, 307, 516, 281, 312, 2681, 281, 1901, 88, 51794], "temperature": 0.0, "avg_logprob": -0.13214061910455877, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.11918189376592636}, {"id": 1298, "seek": 535846, "start": 5358.46, "end": 5359.46, "text": " dot load.", "tokens": [50364, 5893, 3677, 13, 50414], "temperature": 0.0, "avg_logprob": -0.28907269150463505, "compression_ratio": 1.5474452554744527, "no_speech_prob": 0.3627748489379883}, {"id": 1299, "seek": 535846, "start": 5359.46, "end": 5366.22, "text": " And again, we're going to load in the English core web SM's model and core web SM.", "tokens": [50414, 400, 797, 11, 321, 434, 516, 281, 3677, 294, 264, 3669, 4965, 3670, 13115, 311, 2316, 293, 4965, 3670, 13115, 13, 50752], "temperature": 0.0, "avg_logprob": -0.28907269150463505, "compression_ratio": 1.5474452554744527, "no_speech_prob": 0.3627748489379883}, {"id": 1300, "seek": 535846, "start": 5366.22, "end": 5368.06, "text": " Great.", "tokens": [50752, 3769, 13, 50844], "temperature": 0.0, "avg_logprob": -0.28907269150463505, "compression_ratio": 1.5474452554744527, "no_speech_prob": 0.3627748489379883}, {"id": 1301, "seek": 535846, "start": 5368.06, "end": 5381.54, "text": " And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going", "tokens": [50844, 400, 797, 11, 321, 434, 516, 281, 360, 19661, 5893, 426, 45196, 281, 909, 11240, 13977, 19661, 11, 293, 321, 434, 516, 51518], "temperature": 0.0, "avg_logprob": -0.28907269150463505, "compression_ratio": 1.5474452554744527, "no_speech_prob": 0.3627748489379883}, {"id": 1302, "seek": 535846, "start": 5381.54, "end": 5385.42, "text": " to make that an object too.", "tokens": [51518, 281, 652, 300, 364, 2657, 886, 13, 51712], "temperature": 0.0, "avg_logprob": -0.28907269150463505, "compression_ratio": 1.5474452554744527, "no_speech_prob": 0.3627748489379883}, {"id": 1303, "seek": 538542, "start": 5385.42, "end": 5389.9800000000005, "text": " Now what we can do is we can say ruler dot add patterns, again, we're going to go through", "tokens": [50364, 823, 437, 321, 393, 360, 307, 321, 393, 584, 19661, 5893, 909, 8294, 11, 797, 11, 321, 434, 516, 281, 352, 807, 50592], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1304, "seek": 538542, "start": 5389.9800000000005, "end": 5393.06, "text": " all of these steps that we just went through, we're going to add in those patterns that", "tokens": [50592, 439, 295, 613, 4439, 300, 321, 445, 1437, 807, 11, 321, 434, 516, 281, 909, 294, 729, 8294, 300, 50746], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1305, "seek": 538542, "start": 5393.06, "end": 5394.9400000000005, "text": " we created up above.", "tokens": [50746, 321, 2942, 493, 3673, 13, 50840], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1306, "seek": 538542, "start": 5394.9400000000005, "end": 5398.9400000000005, "text": " And now what we're going to do is we're going to actually do one thing a little different", "tokens": [50840, 400, 586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 767, 360, 472, 551, 257, 707, 819, 51040], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1307, "seek": 538542, "start": 5398.9400000000005, "end": 5400.38, "text": " than what we did.", "tokens": [51040, 813, 437, 321, 630, 13, 51112], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1308, "seek": 538542, "start": 5400.38, "end": 5404.4400000000005, "text": " What we're going to do is we're going to load this up again, and we're going to do an extra", "tokens": [51112, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3677, 341, 493, 797, 11, 293, 321, 434, 516, 281, 360, 364, 2857, 51315], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1309, "seek": 538542, "start": 5404.4400000000005, "end": 5405.82, "text": " keyword argument.", "tokens": [51315, 20428, 6770, 13, 51384], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1310, "seek": 538542, "start": 5405.82, "end": 5411.62, "text": " Now we can say either after or before here, we're going to say before NER, what this is", "tokens": [51384, 823, 321, 393, 584, 2139, 934, 420, 949, 510, 11, 321, 434, 516, 281, 584, 949, 426, 1598, 11, 437, 341, 307, 51674], "temperature": 0.0, "avg_logprob": -0.113367163616678, "compression_ratio": 2.0826446280991737, "no_speech_prob": 0.13292455673217773}, {"id": 1311, "seek": 541162, "start": 5411.62, "end": 5418.54, "text": " going to do is it's going to place our NER before our entity will ever for the NER component.", "tokens": [50364, 516, 281, 360, 307, 309, 311, 516, 281, 1081, 527, 426, 1598, 949, 527, 13977, 486, 1562, 337, 264, 426, 1598, 6542, 13, 50710], "temperature": 0.0, "avg_logprob": -0.21730587094329123, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.022284667938947678}, {"id": 1312, "seek": 541162, "start": 5418.54, "end": 5424.82, "text": " And now when we add our patterns in, we can now create a new doc object.", "tokens": [50710, 400, 586, 562, 321, 909, 527, 8294, 294, 11, 321, 393, 586, 1884, 257, 777, 3211, 2657, 13, 51024], "temperature": 0.0, "avg_logprob": -0.21730587094329123, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.022284667938947678}, {"id": 1313, "seek": 541162, "start": 5424.82, "end": 5432.3, "text": " Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print", "tokens": [51024, 16024, 307, 516, 281, 312, 2681, 281, 426, 45196, 281, 2487, 11, 321, 434, 516, 281, 584, 337, 560, 293, 3211, 5893, 560, 82, 11, 4482, 51398], "temperature": 0.0, "avg_logprob": -0.21730587094329123, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.022284667938947678}, {"id": 1314, "seek": 541162, "start": 5432.3, "end": 5437.34, "text": " off int dot text, and dot label.", "tokens": [51398, 766, 560, 5893, 2487, 11, 293, 5893, 7645, 13, 51650], "temperature": 0.0, "avg_logprob": -0.21730587094329123, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.022284667938947678}, {"id": 1315, "seek": 543734, "start": 5437.46, "end": 5441.78, "text": " Now we notice that it is correctly labeled as a GPE.", "tokens": [50370, 823, 321, 3449, 300, 309, 307, 8944, 21335, 382, 257, 460, 5208, 13, 50586], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1316, "seek": 543734, "start": 5441.78, "end": 5442.78, "text": " Why is this?", "tokens": [50586, 1545, 307, 341, 30, 50636], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1317, "seek": 543734, "start": 5442.78, "end": 5450.54, "text": " Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will", "tokens": [50636, 1042, 11, 718, 311, 747, 257, 574, 412, 527, 426, 45196, 281, 2657, 11, 12477, 21882, 11, 293, 498, 321, 11369, 760, 11, 321, 486, 51024], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1318, "seek": 543734, "start": 5450.54, "end": 5455.22, "text": " notice that our entity ruler now in the pipeline sits before the NER model.", "tokens": [51024, 3449, 300, 527, 13977, 19661, 586, 294, 264, 15517, 12696, 949, 264, 426, 1598, 2316, 13, 51258], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1319, "seek": 543734, "start": 5455.22, "end": 5460.1, "text": " In other words, we've given primacy to our custom entity ruler, so that it's going to", "tokens": [51258, 682, 661, 2283, 11, 321, 600, 2212, 2886, 2551, 281, 527, 2375, 13977, 19661, 11, 370, 300, 309, 311, 516, 281, 51502], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1320, "seek": 543734, "start": 5460.1, "end": 5464.58, "text": " have the first shot at actually correctly identifying these things, but we've got another", "tokens": [51502, 362, 264, 700, 3347, 412, 767, 8944, 16696, 613, 721, 11, 457, 321, 600, 658, 1071, 51726], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1321, "seek": 543734, "start": 5464.58, "end": 5466.06, "text": " problem here.", "tokens": [51726, 1154, 510, 13, 51800], "temperature": 0.0, "avg_logprob": -0.16196600596110025, "compression_ratio": 1.6145038167938932, "no_speech_prob": 0.1008608341217041}, {"id": 1322, "seek": 546606, "start": 5466.06, "end": 5474.26, "text": " This is coming out as a person, it should be Mr. Deeds as the entire collective multi", "tokens": [50364, 639, 307, 1348, 484, 382, 257, 954, 11, 309, 820, 312, 2221, 13, 1346, 5147, 382, 264, 2302, 12590, 4825, 50774], "temperature": 0.0, "avg_logprob": -0.20610781466023306, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.019121753051877022}, {"id": 1323, "seek": 546606, "start": 5474.26, "end": 5476.660000000001, "text": " word token, and that should be a new entity.", "tokens": [50774, 1349, 14862, 11, 293, 300, 820, 312, 257, 777, 13977, 13, 50894], "temperature": 0.0, "avg_logprob": -0.20610781466023306, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.019121753051877022}, {"id": 1324, "seek": 546606, "start": 5476.660000000001, "end": 5481.660000000001, "text": " We can use the entity ruler to add in custom types of labels here.", "tokens": [50894, 492, 393, 764, 264, 13977, 19661, 281, 909, 294, 2375, 3467, 295, 16949, 510, 13, 51144], "temperature": 0.0, "avg_logprob": -0.20610781466023306, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.019121753051877022}, {"id": 1325, "seek": 546606, "start": 5481.660000000001, "end": 5484.900000000001, "text": " So let's go ahead and do this same thing.", "tokens": [51144, 407, 718, 311, 352, 2286, 293, 360, 341, 912, 551, 13, 51306], "temperature": 0.0, "avg_logprob": -0.20610781466023306, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.019121753051877022}, {"id": 1326, "seek": 546606, "start": 5484.900000000001, "end": 5491.22, "text": " Let's go ahead and just copy and paste our patterns, and we're going to create one more", "tokens": [51306, 961, 311, 352, 2286, 293, 445, 5055, 293, 9163, 527, 8294, 11, 293, 321, 434, 516, 281, 1884, 472, 544, 51622], "temperature": 0.0, "avg_logprob": -0.20610781466023306, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.019121753051877022}, {"id": 1327, "seek": 549122, "start": 5491.38, "end": 5499.3, "text": " NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM.", "tokens": [50372, 426, 45196, 2657, 11, 321, 434, 516, 281, 818, 341, 426, 45196, 1045, 307, 2681, 281, 1901, 88, 5893, 3677, 294, 4965, 3670, 13115, 13, 50768], "temperature": 0.0, "avg_logprob": -0.29082901176364945, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.20428889989852905}, {"id": 1328, "seek": 549122, "start": 5499.3, "end": 5502.02, "text": " Great, we've got that loaded up.", "tokens": [50768, 3769, 11, 321, 600, 658, 300, 13210, 493, 13, 50904], "temperature": 0.0, "avg_logprob": -0.29082901176364945, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.20428889989852905}, {"id": 1329, "seek": 549122, "start": 5502.02, "end": 5508.26, "text": " We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to", "tokens": [50904, 492, 434, 516, 281, 360, 264, 912, 551, 321, 630, 1036, 565, 426, 45196, 1045, 11, 420, 2597, 11, 19661, 307, 2681, 281, 51216], "temperature": 0.0, "avg_logprob": -0.29082901176364945, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.20428889989852905}, {"id": 1330, "seek": 549122, "start": 5508.26, "end": 5514.860000000001, "text": " NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it", "tokens": [51216, 426, 45196, 5893, 909, 37556, 11240, 13977, 19661, 11, 321, 434, 516, 281, 1081, 309, 1604, 658, 281, 1081, 309, 51546], "temperature": 0.0, "avg_logprob": -0.29082901176364945, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.20428889989852905}, {"id": 1331, "seek": 551486, "start": 5514.86, "end": 5521.62, "text": " before the NER pipe, NLP three, there we go.", "tokens": [50364, 949, 264, 426, 1598, 11240, 11, 426, 45196, 1045, 11, 456, 321, 352, 13, 50702], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1332, "seek": 551486, "start": 5521.62, "end": 5524.82, "text": " And what we need to do now is we need to copy in these patterns, and we're going to add", "tokens": [50702, 400, 437, 321, 643, 281, 360, 586, 307, 321, 643, 281, 5055, 294, 613, 8294, 11, 293, 321, 434, 516, 281, 909, 50862], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1333, "seek": 551486, "start": 5524.82, "end": 5526.62, "text": " in one more pattern.", "tokens": [50862, 294, 472, 544, 5102, 13, 50952], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1334, "seek": 551486, "start": 5526.62, "end": 5528.58, "text": " Remember this can be a list here.", "tokens": [50952, 5459, 341, 393, 312, 257, 1329, 510, 13, 51050], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1335, "seek": 551486, "start": 5528.58, "end": 5535.0199999999995, "text": " So this pattern, we're going to have a new label called film, and we're going to look", "tokens": [51050, 407, 341, 5102, 11, 321, 434, 516, 281, 362, 257, 777, 7645, 1219, 2007, 11, 293, 321, 434, 516, 281, 574, 51372], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1336, "seek": 551486, "start": 5535.0199999999995, "end": 5540.179999999999, "text": " for the sequence Mr. Deeds.", "tokens": [51372, 337, 264, 8310, 2221, 13, 1346, 5147, 13, 51630], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1337, "seek": 551486, "start": 5540.179999999999, "end": 5543.94, "text": " And that's going to be our pattern that we want to add in to our ruler.", "tokens": [51630, 400, 300, 311, 516, 281, 312, 527, 5102, 300, 321, 528, 281, 909, 294, 281, 527, 19661, 13, 51818], "temperature": 0.0, "avg_logprob": -0.16425541469029017, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.009708176366984844}, {"id": 1338, "seek": 554394, "start": 5544.0199999999995, "end": 5550.0599999999995, "text": " So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember", "tokens": [50368, 407, 321, 393, 360, 19661, 5893, 909, 37556, 8294, 11, 293, 321, 434, 516, 281, 909, 294, 8294, 11, 1604, 50670], "temperature": 0.0, "avg_logprob": -0.2526075687814266, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.008577043190598488}, {"id": 1339, "seek": 554394, "start": 5550.0599999999995, "end": 5554.98, "text": " that one keyword argument, or one argument is going to be the list itself.", "tokens": [50670, 300, 472, 20428, 6770, 11, 420, 472, 6770, 307, 516, 281, 312, 264, 1329, 2564, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2526075687814266, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.008577043190598488}, {"id": 1340, "seek": 554394, "start": 5554.98, "end": 5559.379999999999, "text": " And now we can create a new doc object, which is going to be equal to NLP three, I think", "tokens": [50916, 400, 586, 321, 393, 1884, 257, 777, 3211, 2657, 11, 597, 307, 516, 281, 312, 2681, 281, 426, 45196, 1045, 11, 286, 519, 51136], "temperature": 0.0, "avg_logprob": -0.2526075687814266, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.008577043190598488}, {"id": 1341, "seek": 554394, "start": 5559.379999999999, "end": 5568.259999999999, "text": " I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text", "tokens": [51136, 286, 1219, 309, 11, 1338, 11, 2487, 11, 293, 321, 393, 584, 337, 560, 293, 3211, 5893, 560, 82, 11, 4482, 766, 293, 5893, 2487, 51580], "temperature": 0.0, "avg_logprob": -0.2526075687814266, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.008577043190598488}, {"id": 1342, "seek": 556826, "start": 5568.26, "end": 5571.06, "text": " and and dot label.", "tokens": [50364, 293, 293, 5893, 7645, 13, 50504], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1343, "seek": 556826, "start": 5571.06, "end": 5575.860000000001, "text": " And if we execute this, we see now that not only have you gotten the entity ruler to correctly", "tokens": [50504, 400, 498, 321, 14483, 341, 11, 321, 536, 586, 300, 406, 787, 362, 291, 5768, 264, 13977, 19661, 281, 8944, 50744], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1344, "seek": 556826, "start": 5575.860000000001, "end": 5582.9400000000005, "text": " identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly,", "tokens": [50744, 5876, 4055, 47981, 42112, 17952, 8386, 11, 321, 600, 611, 5768, 264, 13977, 19661, 281, 5876, 8944, 11, 51098], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1345, "seek": 556826, "start": 5582.9400000000005, "end": 5584.9800000000005, "text": " Mr. Deeds as a film.", "tokens": [51098, 2221, 13, 1346, 5147, 382, 257, 2007, 13, 51200], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1346, "seek": 556826, "start": 5584.9800000000005, "end": 5588.58, "text": " Now some of you might be realizing the problem here, this is actually a problem for machine", "tokens": [51200, 823, 512, 295, 291, 1062, 312, 16734, 264, 1154, 510, 11, 341, 307, 767, 257, 1154, 337, 3479, 51380], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1347, "seek": 556826, "start": 5588.58, "end": 5589.58, "text": " learning models.", "tokens": [51380, 2539, 5245, 13, 51430], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1348, "seek": 556826, "start": 5589.58, "end": 5594.18, "text": " And the reason for this is because Mr. Deeds in some instances could be the person and", "tokens": [51430, 400, 264, 1778, 337, 341, 307, 570, 2221, 13, 1346, 5147, 294, 512, 14519, 727, 312, 264, 954, 293, 51660], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1349, "seek": 556826, "start": 5594.18, "end": 5597.9800000000005, "text": " Mr. Deeds in other instances could be the movie itself.", "tokens": [51660, 2221, 13, 1346, 5147, 294, 661, 14519, 727, 312, 264, 3169, 2564, 13, 51850], "temperature": 0.0, "avg_logprob": -0.13998861781886365, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.14217804372310638}, {"id": 1350, "seek": 559798, "start": 5597.98, "end": 5600.219999999999, "text": " This is what we would call a toponym.", "tokens": [50364, 639, 307, 437, 321, 576, 818, 257, 1192, 12732, 13, 50476], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1351, "seek": 559798, "start": 5600.219999999999, "end": 5604.0199999999995, "text": " So spelled like this, and this is a common problem in natural language processing.", "tokens": [50476, 407, 34388, 411, 341, 11, 293, 341, 307, 257, 2689, 1154, 294, 3303, 2856, 9007, 13, 50666], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1352, "seek": 559798, "start": 5604.0199999999995, "end": 5608.0199999999995, "text": " And it's actually one of the few problems or one of many problems really, that remain", "tokens": [50666, 400, 309, 311, 767, 472, 295, 264, 1326, 2740, 420, 472, 295, 867, 2740, 534, 11, 300, 6222, 50866], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1353, "seek": 559798, "start": 5608.0199999999995, "end": 5615.419999999999, "text": " a little bit unsolved toponym resolution, spelled like this, or TR is the resolution", "tokens": [50866, 257, 707, 857, 2693, 29110, 1192, 12732, 8669, 11, 34388, 411, 341, 11, 420, 15176, 307, 264, 8669, 51236], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1354, "seek": 559798, "start": 5615.419999999999, "end": 5616.419999999999, "text": " of toponym.", "tokens": [51236, 295, 1192, 12732, 13, 51286], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1355, "seek": 559798, "start": 5616.419999999999, "end": 5621.259999999999, "text": " So things that can have multiple labels that are dependent upon context.", "tokens": [51286, 407, 721, 300, 393, 362, 3866, 16949, 300, 366, 12334, 3564, 4319, 13, 51528], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1356, "seek": 559798, "start": 5621.259999999999, "end": 5626.86, "text": " Another example of toponym resolution is something like this, if you were to look at this word,", "tokens": [51528, 3996, 1365, 295, 1192, 12732, 8669, 307, 746, 411, 341, 11, 498, 291, 645, 281, 574, 412, 341, 1349, 11, 51808], "temperature": 0.0, "avg_logprob": -0.19026661748471468, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.05339384078979492}, {"id": 1357, "seek": 562686, "start": 5626.86, "end": 5631.46, "text": " let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology.", "tokens": [50364, 718, 311, 584, 11, 718, 311, 11200, 8380, 389, 15200, 11, 718, 311, 11200, 8380, 490, 10281, 30871, 13, 50594], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1358, "seek": 562686, "start": 5631.46, "end": 5633.94, "text": " Let's say it's only going to ever be a GPE.", "tokens": [50594, 961, 311, 584, 309, 311, 787, 516, 281, 1562, 312, 257, 460, 5208, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1359, "seek": 562686, "start": 5633.94, "end": 5639.7, "text": " The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas.", "tokens": [50718, 440, 1349, 8380, 727, 2864, 281, 8380, 11, 6190, 11, 8380, 11, 22369, 11, 420, 8380, 11, 7885, 13, 51006], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1360, "seek": 562686, "start": 5639.7, "end": 5645.46, "text": " Toponym resolution is also the ability to resolve problems like this, when in context", "tokens": [51006, 8840, 12732, 8669, 307, 611, 264, 3485, 281, 14151, 2740, 411, 341, 11, 562, 294, 4319, 51294], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1361, "seek": 562686, "start": 5645.46, "end": 5651.54, "text": " is Paris was kind of talking about Paris, France, when in context is it talking about", "tokens": [51294, 307, 8380, 390, 733, 295, 1417, 466, 8380, 11, 6190, 11, 562, 294, 4319, 307, 309, 1417, 466, 51598], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1362, "seek": 562686, "start": 5651.54, "end": 5655.58, "text": " Kentucky, and when in context is it talking about Texas.", "tokens": [51598, 22369, 11, 293, 562, 294, 4319, 307, 309, 1417, 466, 7885, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1643674373626709, "compression_ratio": 1.9111111111111112, "no_speech_prob": 0.14026513695716858}, {"id": 1363, "seek": 565558, "start": 5655.58, "end": 5659.5, "text": " So that's something that you really want to think about when you're generating your", "tokens": [50364, 407, 300, 311, 746, 300, 291, 534, 528, 281, 519, 466, 562, 291, 434, 17746, 428, 50560], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1364, "seek": 565558, "start": 5659.5, "end": 5665.54, "text": " rules for an entity ruler, is this ever going to be a false positive?", "tokens": [50560, 4474, 337, 364, 13977, 19661, 11, 307, 341, 1562, 516, 281, 312, 257, 7908, 3353, 30, 50862], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1365, "seek": 565558, "start": 5665.54, "end": 5670.18, "text": " And if the answer is that it's going to be a false positive half the time, or it's a", "tokens": [50862, 400, 498, 264, 1867, 307, 300, 309, 311, 516, 281, 312, 257, 7908, 3353, 1922, 264, 565, 11, 420, 309, 311, 257, 51094], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1366, "seek": 565558, "start": 5670.18, "end": 5676.1, "text": " 50-50 shot, then really consider incorporating that kind of an entity into a machine learning", "tokens": [51094, 2625, 12, 2803, 3347, 11, 550, 534, 1949, 33613, 300, 733, 295, 364, 13977, 666, 257, 3479, 2539, 51390], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1367, "seek": 565558, "start": 5676.1, "end": 5681.74, "text": " model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as", "tokens": [51390, 2316, 538, 2902, 309, 5110, 295, 1293, 2221, 13, 1346, 5147, 11, 294, 341, 1389, 11, 382, 257, 2007, 11, 293, 2221, 13, 1346, 5147, 382, 51672], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1368, "seek": 565558, "start": 5681.74, "end": 5682.74, "text": " a person.", "tokens": [51672, 257, 954, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1551484481148098, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.012052644044160843}, {"id": 1369, "seek": 568274, "start": 5682.74, "end": 5688.46, "text": " And learn with word embeddings when that context means it's a film and when that context means", "tokens": [50364, 400, 1466, 365, 1349, 12240, 29432, 562, 300, 4319, 1355, 309, 311, 257, 2007, 293, 562, 300, 4319, 1355, 50650], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1370, "seek": 568274, "start": 5688.46, "end": 5689.46, "text": " it's a person.", "tokens": [50650, 309, 311, 257, 954, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1371, "seek": 568274, "start": 5689.46, "end": 5692.0199999999995, "text": " That's just a little toy example.", "tokens": [50700, 663, 311, 445, 257, 707, 12058, 1365, 13, 50828], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1372, "seek": 568274, "start": 5692.0199999999995, "end": 5695.3, "text": " What we're going to see moving forward, though, and we're going to do this with a matcher,", "tokens": [50828, 708, 321, 434, 516, 281, 536, 2684, 2128, 11, 1673, 11, 293, 321, 434, 516, 281, 360, 341, 365, 257, 2995, 260, 11, 50992], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1373, "seek": 568274, "start": 5695.3, "end": 5699.62, "text": " not with the entity ruler, is that spacey can do a lot of things.", "tokens": [50992, 406, 365, 264, 13977, 19661, 11, 307, 300, 1901, 88, 393, 360, 257, 688, 295, 721, 13, 51208], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1374, "seek": 568274, "start": 5699.62, "end": 5705.0599999999995, "text": " You might be thinking to yourself, now I could easily just come up with a list and just check", "tokens": [51208, 509, 1062, 312, 1953, 281, 1803, 11, 586, 286, 727, 3612, 445, 808, 493, 365, 257, 1329, 293, 445, 1520, 51480], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1375, "seek": 568274, "start": 5705.0599999999995, "end": 5709.82, "text": " and see whenever Mr. Deeds pops up and just inject that into the doc.ins.", "tokens": [51480, 293, 536, 5699, 2221, 13, 1346, 5147, 16795, 493, 293, 445, 10711, 300, 666, 264, 3211, 13, 1292, 13, 51718], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1376, "seek": 568274, "start": 5709.82, "end": 5712.179999999999, "text": " I could do the same thing with West Chesterton Field Bill.", "tokens": [51718, 286, 727, 360, 264, 912, 551, 365, 4055, 47981, 42112, 17952, 5477, 13, 51836], "temperature": 0.0, "avg_logprob": -0.17664200151470347, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.561943769454956}, {"id": 1377, "seek": 571218, "start": 5712.18, "end": 5715.5, "text": " Why do I need an NLP framework to do this?", "tokens": [50364, 1545, 360, 286, 643, 364, 426, 45196, 8388, 281, 360, 341, 30, 50530], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1378, "seek": 571218, "start": 5715.5, "end": 5719.780000000001, "text": " And the answer is going to come up in just a few minutes when we start realizing that", "tokens": [50530, 400, 264, 1867, 307, 516, 281, 808, 493, 294, 445, 257, 1326, 2077, 562, 321, 722, 16734, 300, 50744], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1379, "seek": 571218, "start": 5719.780000000001, "end": 5725.740000000001, "text": " spacey can do a lot more than things like regex or things like just a basic gazetteer", "tokens": [50744, 1901, 88, 393, 360, 257, 688, 544, 813, 721, 411, 319, 432, 87, 420, 721, 411, 445, 257, 3875, 26232, 3007, 260, 51042], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1380, "seek": 571218, "start": 5725.740000000001, "end": 5727.820000000001, "text": " check or a list check.", "tokens": [51042, 1520, 420, 257, 1329, 1520, 13, 51146], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1381, "seek": 571218, "start": 5727.820000000001, "end": 5732.38, "text": " What you can do with spacey is you can have the pattern not just take a sequence of characters", "tokens": [51146, 708, 291, 393, 360, 365, 1901, 88, 307, 291, 393, 362, 264, 5102, 406, 445, 747, 257, 8310, 295, 4342, 51374], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1382, "seek": 571218, "start": 5732.38, "end": 5738.18, "text": " and look for a match, but a sequence of linguistic features as well, that earlier pipes have", "tokens": [51374, 293, 574, 337, 257, 2995, 11, 457, 257, 8310, 295, 43002, 4122, 382, 731, 11, 300, 3071, 21882, 362, 51664], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1383, "seek": 571218, "start": 5738.18, "end": 5739.18, "text": " identified.", "tokens": [51664, 9234, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11607314400050951, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.020962342619895935}, {"id": 1384, "seek": 573918, "start": 5739.18, "end": 5743.42, "text": " And I think it's best if we save that for just a second when we start talking about", "tokens": [50364, 400, 286, 519, 309, 311, 1151, 498, 321, 3155, 300, 337, 445, 257, 1150, 562, 321, 722, 1417, 466, 50576], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1385, "seek": 573918, "start": 5743.42, "end": 5749.14, "text": " the matcher, which is, in my opinion, one of the more robust things that you can do", "tokens": [50576, 264, 2995, 260, 11, 597, 307, 11, 294, 452, 4800, 11, 472, 295, 264, 544, 13956, 721, 300, 291, 393, 360, 50862], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1386, "seek": 573918, "start": 5749.14, "end": 5754.860000000001, "text": " with spacey and what sets spacey apart from things like regex or other fancier string", "tokens": [50862, 365, 1901, 88, 293, 437, 6352, 1901, 88, 4936, 490, 721, 411, 319, 432, 87, 420, 661, 3429, 27674, 6798, 51148], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1387, "seek": 573918, "start": 5754.860000000001, "end": 5756.860000000001, "text": " matching approaches.", "tokens": [51148, 14324, 11587, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1388, "seek": 573918, "start": 5756.860000000001, "end": 5763.18, "text": " Okay, we're now moving into chapter six of this book, and this is really kind of, in", "tokens": [51248, 1033, 11, 321, 434, 586, 2684, 666, 7187, 2309, 295, 341, 1446, 11, 293, 341, 307, 534, 733, 295, 11, 294, 51564], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1389, "seek": 573918, "start": 5763.18, "end": 5767.3, "text": " my opinion, one of the most important areas in this entire video.", "tokens": [51564, 452, 4800, 11, 472, 295, 264, 881, 1021, 3179, 294, 341, 2302, 960, 13, 51770], "temperature": 0.0, "avg_logprob": -0.1438455241067069, "compression_ratio": 1.7, "no_speech_prob": 0.0037070668768137693}, {"id": 1390, "seek": 576730, "start": 5767.3, "end": 5771.62, "text": " If you can master the techniques I'm going to show you for the next maybe 20 minutes", "tokens": [50364, 759, 291, 393, 4505, 264, 7512, 286, 478, 516, 281, 855, 291, 337, 264, 958, 1310, 945, 2077, 50580], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1391, "seek": 576730, "start": 5771.62, "end": 5776.3, "text": " or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really", "tokens": [50580, 420, 370, 11, 1310, 2217, 2077, 11, 291, 434, 516, 281, 312, 1075, 281, 360, 257, 688, 365, 1901, 88, 293, 291, 434, 534, 50814], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1392, "seek": 576730, "start": 5776.3, "end": 5780.06, "text": " going to see really kind of its true power.", "tokens": [50814, 516, 281, 536, 534, 733, 295, 1080, 2074, 1347, 13, 51002], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1393, "seek": 576730, "start": 5780.06, "end": 5784.38, "text": " A lot of the stuff that we talk about here in the matcher can also be implemented in", "tokens": [51002, 316, 688, 295, 264, 1507, 300, 321, 751, 466, 510, 294, 264, 2995, 260, 393, 611, 312, 12270, 294, 51218], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1394, "seek": 576730, "start": 5784.38, "end": 5788.5, "text": " the entity ruler as well with a pattern.", "tokens": [51218, 264, 13977, 19661, 382, 731, 365, 257, 5102, 13, 51424], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1395, "seek": 576730, "start": 5788.5, "end": 5793.66, "text": " The key difference between the entity ruler and the matcher is in how data the data is", "tokens": [51424, 440, 2141, 2649, 1296, 264, 13977, 19661, 293, 264, 2995, 260, 307, 294, 577, 1412, 264, 1412, 307, 51682], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1396, "seek": 576730, "start": 5793.66, "end": 5795.08, "text": " kind of extracted.", "tokens": [51682, 733, 295, 34086, 13, 51753], "temperature": 0.0, "avg_logprob": -0.10970286191520044, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.08033957332372665}, {"id": 1397, "seek": 579508, "start": 5795.08, "end": 5799.28, "text": " So the matcher is going to store information a little differently.", "tokens": [50364, 407, 264, 2995, 260, 307, 516, 281, 3531, 1589, 257, 707, 7614, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1398, "seek": 579508, "start": 5799.28, "end": 5803.12, "text": " It's going to store it as within the vocab of the NLP model.", "tokens": [50574, 467, 311, 516, 281, 3531, 309, 382, 1951, 264, 2329, 455, 295, 264, 426, 45196, 2316, 13, 50766], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1399, "seek": 579508, "start": 5803.12, "end": 5808.92, "text": " It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk", "tokens": [50766, 467, 311, 516, 281, 3531, 309, 382, 257, 3845, 45690, 420, 257, 476, 87, 5729, 11, 637, 2018, 476, 87, 11, 308, 1398, 11, 286, 478, 516, 281, 751, 51056], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1400, "seek": 579508, "start": 5808.92, "end": 5810.92, "text": " about that more in just a second.", "tokens": [51056, 466, 300, 544, 294, 445, 257, 1150, 13, 51156], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1401, "seek": 579508, "start": 5810.92, "end": 5813.36, "text": " And it's not going to store it in the doc ends.", "tokens": [51156, 400, 309, 311, 406, 516, 281, 3531, 309, 294, 264, 3211, 5314, 13, 51278], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1402, "seek": 579508, "start": 5813.36, "end": 5817.36, "text": " So matchers don't put things in your doc.ends.", "tokens": [51278, 407, 2995, 433, 500, 380, 829, 721, 294, 428, 3211, 13, 2581, 13, 51478], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1403, "seek": 579508, "start": 5817.36, "end": 5821.54, "text": " So when do you want to use a matcher over an entity ruler?", "tokens": [51478, 407, 562, 360, 291, 528, 281, 764, 257, 2995, 260, 670, 364, 13977, 19661, 30, 51687], "temperature": 0.0, "avg_logprob": -0.17902671137163717, "compression_ratio": 1.7816593886462881, "no_speech_prob": 0.0006461840821430087}, {"id": 1404, "seek": 582154, "start": 5821.54, "end": 5826.62, "text": " You want to use the entity ruler when the thing that you're trying to extract is something", "tokens": [50364, 509, 528, 281, 764, 264, 13977, 19661, 562, 264, 551, 300, 291, 434, 1382, 281, 8947, 307, 746, 50618], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1405, "seek": 582154, "start": 5826.62, "end": 5831.34, "text": " that is important to have a label that corresponds to it within the entities that are coming", "tokens": [50618, 300, 307, 1021, 281, 362, 257, 7645, 300, 23249, 281, 309, 1951, 264, 16667, 300, 366, 1348, 50854], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1406, "seek": 582154, "start": 5831.34, "end": 5832.62, "text": " out.", "tokens": [50854, 484, 13, 50918], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1407, "seek": 582154, "start": 5832.62, "end": 5837.3, "text": " So in my research, I use this for anything from like, let's say stocks, if I'm working", "tokens": [50918, 407, 294, 452, 2132, 11, 286, 764, 341, 337, 1340, 490, 411, 11, 718, 311, 584, 12966, 11, 498, 286, 478, 1364, 51152], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1408, "seek": 582154, "start": 5837.3, "end": 5843.66, "text": " with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm", "tokens": [51152, 365, 25123, 11, 286, 603, 764, 341, 337, 498, 286, 478, 1364, 365, 28399, 1412, 412, 264, 2546, 39, 17365, 11, 689, 286, 478, 51470], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1409, "seek": 582154, "start": 5843.66, "end": 5851.5, "text": " a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside", "tokens": [51470, 257, 2183, 39966, 11, 286, 603, 853, 281, 909, 294, 16573, 293, 33937, 3093, 329, 570, 729, 366, 439, 1021, 25339, 770, 12385, 51862], "temperature": 0.0, "avg_logprob": -0.13393584887186685, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0330808088183403}, {"id": 1410, "seek": 585150, "start": 5851.62, "end": 5852.62, "text": " other entities.", "tokens": [50370, 661, 16667, 13, 50420], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1411, "seek": 585150, "start": 5852.62, "end": 5858.26, "text": " I'll also work in things like ships, so the names of ships, streets, things like that.", "tokens": [50420, 286, 603, 611, 589, 294, 721, 411, 11434, 11, 370, 264, 5288, 295, 11434, 11, 8481, 11, 721, 411, 300, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1412, "seek": 585150, "start": 5858.26, "end": 5863.46, "text": " When I use the the matcher, it's when I'm looking for something that is not necessarily", "tokens": [50702, 1133, 286, 764, 264, 264, 2995, 260, 11, 309, 311, 562, 286, 478, 1237, 337, 746, 300, 307, 406, 4725, 50962], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1413, "seek": 585150, "start": 5863.46, "end": 5871.06, "text": " an entity type, but something that is a structure within the text that will help me extract", "tokens": [50962, 364, 13977, 2010, 11, 457, 746, 300, 307, 257, 3877, 1951, 264, 2487, 300, 486, 854, 385, 8947, 51342], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1414, "seek": 585150, "start": 5871.06, "end": 5872.06, "text": " information.", "tokens": [51342, 1589, 13, 51392], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1415, "seek": 585150, "start": 5872.06, "end": 5875.5, "text": " And I think that'll make more sense as we go through and I show you kind of how to improve", "tokens": [51392, 400, 286, 519, 300, 603, 652, 544, 2020, 382, 321, 352, 807, 293, 286, 855, 291, 733, 295, 577, 281, 3470, 51564], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1416, "seek": 585150, "start": 5875.5, "end": 5881.06, "text": " examples going through it, we're kind of using the matcher as you would in the real world.", "tokens": [51564, 5110, 516, 807, 309, 11, 321, 434, 733, 295, 1228, 264, 2995, 260, 382, 291, 576, 294, 264, 957, 1002, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1313849120843606, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.06185411289334297}, {"id": 1417, "seek": 588106, "start": 5881.1, "end": 5885.9800000000005, "text": " But remember, all the patterns that I show you can also be implemented in the entity", "tokens": [50366, 583, 1604, 11, 439, 264, 8294, 300, 286, 855, 291, 393, 611, 312, 12270, 294, 264, 13977, 50610], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1418, "seek": 588106, "start": 5885.9800000000005, "end": 5886.9800000000005, "text": " ruler.", "tokens": [50610, 19661, 13, 50660], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1419, "seek": 588106, "start": 5886.9800000000005, "end": 5891.14, "text": " And I'm also going to talk about when we get to chapter eight, how rejects can actually", "tokens": [50660, 400, 286, 478, 611, 516, 281, 751, 466, 562, 321, 483, 281, 7187, 3180, 11, 577, 8248, 82, 393, 767, 50868], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1420, "seek": 588106, "start": 5891.14, "end": 5894.46, "text": " be used to do similar things, but in a different way.", "tokens": [50868, 312, 1143, 281, 360, 2531, 721, 11, 457, 294, 257, 819, 636, 13, 51034], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1421, "seek": 588106, "start": 5894.46, "end": 5900.18, "text": " Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic", "tokens": [51034, 23596, 11, 562, 291, 528, 281, 764, 264, 2995, 260, 420, 264, 13977, 19661, 670, 8248, 82, 307, 562, 43002, 51320], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1422, "seek": 588106, "start": 5900.18, "end": 5907.26, "text": " components, so the lemma of a word or the identifying if the word is a specific type", "tokens": [51320, 6677, 11, 370, 264, 7495, 1696, 295, 257, 1349, 420, 264, 16696, 498, 264, 1349, 307, 257, 2685, 2010, 51674], "temperature": 0.0, "avg_logprob": -0.14502650669642858, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.014955855906009674}, {"id": 1423, "seek": 590726, "start": 5907.34, "end": 5912.3, "text": " of an entity, that's when you're going to want to use the matcher over rejects.", "tokens": [50368, 295, 364, 13977, 11, 300, 311, 562, 291, 434, 516, 281, 528, 281, 764, 264, 2995, 260, 670, 8248, 82, 13, 50616], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1424, "seek": 590726, "start": 5912.3, "end": 5916.38, "text": " And when you're going to use rejects is when you really have a complicated pattern that", "tokens": [50616, 400, 562, 291, 434, 516, 281, 764, 8248, 82, 307, 562, 291, 534, 362, 257, 6179, 5102, 300, 50820], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1425, "seek": 590726, "start": 5916.38, "end": 5918.54, "text": " you need to extract.", "tokens": [50820, 291, 643, 281, 8947, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1426, "seek": 590726, "start": 5918.54, "end": 5923.38, "text": " And that pattern is not dependent upon specific parts of speech, you're going to see with", "tokens": [50928, 400, 300, 5102, 307, 406, 12334, 3564, 2685, 3166, 295, 6218, 11, 291, 434, 516, 281, 536, 365, 51170], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1427, "seek": 590726, "start": 5923.38, "end": 5927.820000000001, "text": " that how that works as we kind of go through the rest of part two, but keep that in the", "tokens": [51170, 300, 577, 300, 1985, 382, 321, 733, 295, 352, 807, 264, 1472, 295, 644, 732, 11, 457, 1066, 300, 294, 264, 51392], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1428, "seek": 590726, "start": 5927.820000000001, "end": 5929.22, "text": " back of your mind.", "tokens": [51392, 646, 295, 428, 1575, 13, 51462], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1429, "seek": 590726, "start": 5929.22, "end": 5934.1, "text": " So let's go ahead and take our work over to our blank Jupiter notebook again.", "tokens": [51462, 407, 718, 311, 352, 2286, 293, 747, 527, 589, 670, 281, 527, 8247, 24567, 21060, 797, 13, 51706], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1430, "seek": 590726, "start": 5934.1, "end": 5936.780000000001, "text": " So what we're going to do is we're going to just set up with a basic example.", "tokens": [51706, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 445, 992, 493, 365, 257, 3875, 1365, 13, 51840], "temperature": 0.0, "avg_logprob": -0.12105800388576267, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.37007662653923035}, {"id": 1431, "seek": 593678, "start": 5936.78, "end": 5938.78, "text": " We need to import spacey.", "tokens": [50364, 492, 643, 281, 974, 1901, 88, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1432, "seek": 593678, "start": 5938.78, "end": 5944.78, "text": " And since we're working with the matcher, we also need to say from spacey dot matcher,", "tokens": [50464, 400, 1670, 321, 434, 1364, 365, 264, 2995, 260, 11, 321, 611, 643, 281, 584, 490, 1901, 88, 5893, 2995, 260, 11, 50764], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1433, "seek": 593678, "start": 5944.78, "end": 5950.0199999999995, "text": " import matcher with a capital M, very important capital M.", "tokens": [50764, 974, 2995, 260, 365, 257, 4238, 376, 11, 588, 1021, 4238, 376, 13, 51026], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1434, "seek": 593678, "start": 5950.0199999999995, "end": 5953.9, "text": " Once we have this loaded up, we can start actually working with the matcher.", "tokens": [51026, 3443, 321, 362, 341, 13210, 493, 11, 321, 393, 722, 767, 1364, 365, 264, 2995, 260, 13, 51220], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1435, "seek": 593678, "start": 5953.9, "end": 5960.259999999999, "text": " And we're going to be putting the matcher in a just the small English model.", "tokens": [51220, 400, 321, 434, 516, 281, 312, 3372, 264, 2995, 260, 294, 257, 445, 264, 1359, 3669, 2316, 13, 51538], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1436, "seek": 593678, "start": 5960.259999999999, "end": 5964.099999999999, "text": " And we're going to say NLP is equal to spacey dot load.", "tokens": [51538, 400, 321, 434, 516, 281, 584, 426, 45196, 307, 2681, 281, 1901, 88, 5893, 3677, 13, 51730], "temperature": 0.0, "avg_logprob": -0.18449694196754526, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.047416187822818756}, {"id": 1437, "seek": 596410, "start": 5964.1, "end": 5969.58, "text": " And you should be getting familiar with this in core web SM, the small English model.", "tokens": [50364, 400, 291, 820, 312, 1242, 4963, 365, 341, 294, 4965, 3670, 13115, 11, 264, 1359, 3669, 2316, 13, 50638], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1438, "seek": 596410, "start": 5969.58, "end": 5974.780000000001, "text": " Once we've got that loaded, and we do now, we can start actually working with the matcher.", "tokens": [50638, 3443, 321, 600, 658, 300, 13210, 11, 293, 321, 360, 586, 11, 321, 393, 722, 767, 1364, 365, 264, 2995, 260, 13, 50898], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1439, "seek": 596410, "start": 5974.780000000001, "end": 5976.38, "text": " So how do you create the matcher?", "tokens": [50898, 407, 577, 360, 291, 1884, 264, 2995, 260, 30, 50978], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1440, "seek": 596410, "start": 5976.38, "end": 5979.900000000001, "text": " Well, the Pythonic way to do this and the weights in the documentation is to call the", "tokens": [50978, 1042, 11, 264, 15329, 299, 636, 281, 360, 341, 293, 264, 17443, 294, 264, 14333, 307, 281, 818, 264, 51154], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1441, "seek": 596410, "start": 5979.900000000001, "end": 5984.740000000001, "text": " object a matcher, that's going to be equal to matcher with a capital M. So we're calling", "tokens": [51154, 2657, 257, 2995, 260, 11, 300, 311, 516, 281, 312, 2681, 281, 2995, 260, 365, 257, 4238, 376, 13, 407, 321, 434, 5141, 51396], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1442, "seek": 596410, "start": 5984.740000000001, "end": 5987.58, "text": " this class right here.", "tokens": [51396, 341, 1508, 558, 510, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1443, "seek": 596410, "start": 5987.58, "end": 5990.9800000000005, "text": " And now what we need to do is we need to pass in one argument.", "tokens": [51538, 400, 586, 437, 321, 643, 281, 360, 307, 321, 643, 281, 1320, 294, 472, 6770, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1461614966392517, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.09007743000984192}, {"id": 1444, "seek": 599098, "start": 5990.98, "end": 5994.099999999999, "text": " This is going to be NLP dot vocab.", "tokens": [50364, 639, 307, 516, 281, 312, 426, 45196, 5893, 2329, 455, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1445, "seek": 599098, "start": 5994.099999999999, "end": 5997.5, "text": " We're going to see that we can add in some extra features here in just a little bit.", "tokens": [50520, 492, 434, 516, 281, 536, 300, 321, 393, 909, 294, 512, 2857, 4122, 510, 294, 445, 257, 707, 857, 13, 50690], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1446, "seek": 599098, "start": 5997.5, "end": 6000.74, "text": " I'm going to show you why you want to add in extra features at this stage, but we're", "tokens": [50690, 286, 478, 516, 281, 855, 291, 983, 291, 528, 281, 909, 294, 2857, 4122, 412, 341, 3233, 11, 457, 321, 434, 50852], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1447, "seek": 599098, "start": 6000.74, "end": 6002.58, "text": " going to ignore that for right now.", "tokens": [50852, 516, 281, 11200, 300, 337, 558, 586, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1448, "seek": 599098, "start": 6002.58, "end": 6007.219999999999, "text": " What we're going to try to do is we're going to try to find email addresses within a text,", "tokens": [50944, 708, 321, 434, 516, 281, 853, 281, 360, 307, 321, 434, 516, 281, 853, 281, 915, 3796, 16862, 1951, 257, 2487, 11, 51176], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1449, "seek": 599098, "start": 6007.219999999999, "end": 6011.299999999999, "text": " a very simple task that's really not that difficult to do.", "tokens": [51176, 257, 588, 2199, 5633, 300, 311, 534, 406, 300, 2252, 281, 360, 13, 51380], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1450, "seek": 599098, "start": 6011.299999999999, "end": 6015.379999999999, "text": " We can do it with a very simple pattern because spacey has given us that ability.", "tokens": [51380, 492, 393, 360, 309, 365, 257, 588, 2199, 5102, 570, 1901, 88, 575, 2212, 505, 300, 3485, 13, 51584], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1451, "seek": 599098, "start": 6015.379999999999, "end": 6017.82, "text": " So let's create a pattern.", "tokens": [51584, 407, 718, 311, 1884, 257, 5102, 13, 51706], "temperature": 0.0, "avg_logprob": -0.12292195748591768, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.2507997155189514}, {"id": 1452, "seek": 601782, "start": 6017.82, "end": 6025.98, "text": " And that's going to be equal to a list, which is going to contain a dictionary.", "tokens": [50364, 400, 300, 311, 516, 281, 312, 2681, 281, 257, 1329, 11, 597, 307, 516, 281, 5304, 257, 25890, 13, 50772], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1453, "seek": 601782, "start": 6025.98, "end": 6033.86, "text": " The first item in the dictionary, or the first key, is going to be the thing that you're", "tokens": [50772, 440, 700, 3174, 294, 264, 25890, 11, 420, 264, 700, 2141, 11, 307, 516, 281, 312, 264, 551, 300, 291, 434, 51166], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1454, "seek": 601782, "start": 6033.86, "end": 6035.0599999999995, "text": " looking for.", "tokens": [51166, 1237, 337, 13, 51226], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1455, "seek": 601782, "start": 6035.0599999999995, "end": 6038.58, "text": " So in this case, we have a bunch of different things that the matcher can look for.", "tokens": [51226, 407, 294, 341, 1389, 11, 321, 362, 257, 3840, 295, 819, 721, 300, 264, 2995, 260, 393, 574, 337, 13, 51402], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1456, "seek": 601782, "start": 6038.58, "end": 6040.54, "text": " And I'm going to be talking about all those in just a second.", "tokens": [51402, 400, 286, 478, 516, 281, 312, 1417, 466, 439, 729, 294, 445, 257, 1150, 13, 51500], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1457, "seek": 601782, "start": 6040.54, "end": 6045.54, "text": " But one of them is very handily, this label of like email.", "tokens": [51500, 583, 472, 295, 552, 307, 588, 1011, 953, 11, 341, 7645, 295, 411, 3796, 13, 51750], "temperature": 0.0, "avg_logprob": -0.13872425578464972, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.008577137254178524}, {"id": 1458, "seek": 604554, "start": 6045.54, "end": 6051.7, "text": " So if the if the string or the sequence of tokens or the token is looking like an email,", "tokens": [50364, 407, 498, 264, 498, 264, 6798, 420, 264, 8310, 295, 22667, 420, 264, 14862, 307, 1237, 411, 364, 3796, 11, 50672], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1459, "seek": 604554, "start": 6051.7, "end": 6058.58, "text": " and that's true, then that is what we want to extract, we want to extract everything that", "tokens": [50672, 293, 300, 311, 2074, 11, 550, 300, 307, 437, 321, 528, 281, 8947, 11, 321, 528, 281, 8947, 1203, 300, 51016], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1460, "seek": 604554, "start": 6058.58, "end": 6059.9, "text": " looks like an email.", "tokens": [51016, 1542, 411, 364, 3796, 13, 51082], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1461, "seek": 604554, "start": 6059.9, "end": 6064.54, "text": " And to make sure that this occurs, we're going to say matcher dot add.", "tokens": [51082, 400, 281, 652, 988, 300, 341, 11843, 11, 321, 434, 516, 281, 584, 2995, 260, 5893, 909, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1462, "seek": 604554, "start": 6064.54, "end": 6069.18, "text": " And then here, we're going to pass in two arguments, argument one is going to be the", "tokens": [51314, 400, 550, 510, 11, 321, 434, 516, 281, 1320, 294, 732, 12869, 11, 6770, 472, 307, 516, 281, 312, 264, 51546], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1463, "seek": 604554, "start": 6069.18, "end": 6073.84, "text": " think of it as a label that we want to assign to it.", "tokens": [51546, 519, 295, 309, 382, 257, 7645, 300, 321, 528, 281, 6269, 281, 309, 13, 51779], "temperature": 0.0, "avg_logprob": -0.14041870563953845, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.01133094821125269}, {"id": 1464, "seek": 607384, "start": 6073.84, "end": 6078.4800000000005, "text": " And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll", "tokens": [50364, 400, 341, 307, 437, 311, 516, 281, 312, 3869, 666, 264, 297, 75, 79, 5893, 2329, 455, 382, 257, 476, 87, 5729, 11, 597, 321, 603, 50596], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1465, "seek": 607384, "start": 6078.4800000000005, "end": 6080.0, "text": " see in just a second.", "tokens": [50596, 536, 294, 445, 257, 1150, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1466, "seek": 607384, "start": 6080.0, "end": 6083.52, "text": " And the next thing is a pattern.", "tokens": [50672, 400, 264, 958, 551, 307, 257, 5102, 13, 50848], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1467, "seek": 607384, "start": 6083.52, "end": 6087.2, "text": " And it's important here to note that this is a list.", "tokens": [50848, 400, 309, 311, 1021, 510, 281, 3637, 300, 341, 307, 257, 1329, 13, 51032], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1468, "seek": 607384, "start": 6087.2, "end": 6090.88, "text": " The argument here takes a list of lists.", "tokens": [51032, 440, 6770, 510, 2516, 257, 1329, 295, 14511, 13, 51216], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1469, "seek": 607384, "start": 6090.88, "end": 6095.400000000001, "text": " And because this is just one list right now, I'm making it into a list.", "tokens": [51216, 400, 570, 341, 307, 445, 472, 1329, 558, 586, 11, 286, 478, 1455, 309, 666, 257, 1329, 13, 51442], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1470, "seek": 607384, "start": 6095.400000000001, "end": 6101.68, "text": " So each one of these different patterns would be a list within a list, essentially the let's", "tokens": [51442, 407, 1184, 472, 295, 613, 819, 8294, 576, 312, 257, 1329, 1951, 257, 1329, 11, 4476, 264, 718, 311, 51756], "temperature": 0.0, "avg_logprob": -0.15255397299061652, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.002800788264721632}, {"id": 1471, "seek": 610168, "start": 6101.68, "end": 6103.92, "text": " go ahead and execute that.", "tokens": [50364, 352, 2286, 293, 14483, 300, 13, 50476], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1472, "seek": 610168, "start": 6103.92, "end": 6109.16, "text": " And now we're going to say doc is equal to nlp.", "tokens": [50476, 400, 586, 321, 434, 516, 281, 584, 3211, 307, 2681, 281, 297, 75, 79, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1473, "seek": 610168, "start": 6109.16, "end": 6117.320000000001, "text": " And I'm going to add in a text that I have in the textbook.", "tokens": [50738, 400, 286, 478, 516, 281, 909, 294, 257, 2487, 300, 286, 362, 294, 264, 25591, 13, 51146], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1474, "seek": 610168, "start": 6117.320000000001, "end": 6121.6, "text": " And this is my email address w mattingly at aol.com.", "tokens": [51146, 400, 341, 307, 452, 3796, 2985, 261, 3803, 783, 356, 412, 257, 401, 13, 1112, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1475, "seek": 610168, "start": 6121.6, "end": 6122.84, "text": " That might be a real email address.", "tokens": [51360, 663, 1062, 312, 257, 957, 3796, 2985, 13, 51422], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1476, "seek": 610168, "start": 6122.84, "end": 6124.92, "text": " I don't believe it is, it's definitely not mine.", "tokens": [51422, 286, 500, 380, 1697, 309, 307, 11, 309, 311, 2138, 406, 3892, 13, 51526], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1477, "seek": 610168, "start": 6124.92, "end": 6127.360000000001, "text": " So don't try and email it.", "tokens": [51526, 407, 500, 380, 853, 293, 3796, 309, 13, 51648], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1478, "seek": 610168, "start": 6127.360000000001, "end": 6131.200000000001, "text": " And then we're going to say matches is equal to matcher doc.", "tokens": [51648, 400, 550, 321, 434, 516, 281, 584, 10676, 307, 2681, 281, 2995, 260, 3211, 13, 51840], "temperature": 0.0, "avg_logprob": -0.17445736760678499, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.06753873080015182}, {"id": 1479, "seek": 613120, "start": 6131.24, "end": 6133.76, "text": " This is going to be how we find our matches.", "tokens": [50366, 639, 307, 516, 281, 312, 577, 321, 915, 527, 10676, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1480, "seek": 613120, "start": 6133.76, "end": 6140.04, "text": " We pass that doc object into our matcher class.", "tokens": [50492, 492, 1320, 300, 3211, 2657, 666, 527, 2995, 260, 1508, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1481, "seek": 613120, "start": 6140.04, "end": 6143.5199999999995, "text": " And now what we have is the ability to print off our matches.", "tokens": [50806, 400, 586, 437, 321, 362, 307, 264, 3485, 281, 4482, 766, 527, 10676, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1482, "seek": 613120, "start": 6143.5199999999995, "end": 6145.76, "text": " And what we get is a list.", "tokens": [50980, 400, 437, 321, 483, 307, 257, 1329, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1483, "seek": 613120, "start": 6145.76, "end": 6150.4, "text": " And this list is a set of tuples that will always have three indices.", "tokens": [51092, 400, 341, 1329, 307, 257, 992, 295, 2604, 2622, 300, 486, 1009, 362, 1045, 43840, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1484, "seek": 613120, "start": 6150.4, "end": 6154.44, "text": " So index zero is going to be this very long number.", "tokens": [51324, 407, 8186, 4018, 307, 516, 281, 312, 341, 588, 938, 1230, 13, 51526], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1485, "seek": 613120, "start": 6154.44, "end": 6160.599999999999, "text": " What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook.", "tokens": [51526, 708, 341, 307, 11, 307, 341, 307, 257, 476, 87, 5729, 11, 637, 2018, 411, 341, 12020, 1783, 462, 15454, 11, 309, 311, 294, 264, 25591, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1518212359884511, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.01640114188194275}, {"id": 1486, "seek": 616060, "start": 6160.6, "end": 6164.88, "text": " And the next thing is the start token and the end token.", "tokens": [50364, 400, 264, 958, 551, 307, 264, 722, 14862, 293, 264, 917, 14862, 13, 50578], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1487, "seek": 616060, "start": 6164.88, "end": 6167.6, "text": " So you might be seeing the importance here already.", "tokens": [50578, 407, 291, 1062, 312, 2577, 264, 7379, 510, 1217, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1488, "seek": 616060, "start": 6167.6, "end": 6173.84, "text": " What we can do with this is we can actually go into the nlp vocab where this integer lies", "tokens": [50714, 708, 321, 393, 360, 365, 341, 307, 321, 393, 767, 352, 666, 264, 297, 75, 79, 2329, 455, 689, 341, 24922, 9134, 51026], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1489, "seek": 616060, "start": 6173.84, "end": 6176.08, "text": " and find what it corresponds to.", "tokens": [51026, 293, 915, 437, 309, 23249, 281, 13, 51138], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1490, "seek": 616060, "start": 6176.08, "end": 6178.0, "text": " So this is where this is pretty cool.", "tokens": [51138, 407, 341, 307, 689, 341, 307, 1238, 1627, 13, 51234], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1491, "seek": 616060, "start": 6178.0, "end": 6179.280000000001, "text": " Check this out.", "tokens": [51234, 6881, 341, 484, 13, 51298], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1492, "seek": 616060, "start": 6179.280000000001, "end": 6182.0, "text": " So you print off nlp dot vocab.", "tokens": [51298, 407, 291, 4482, 766, 297, 75, 79, 5893, 2329, 455, 13, 51434], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1493, "seek": 616060, "start": 6182.0, "end": 6184.72, "text": " So we're going into that vocab object.", "tokens": [51434, 407, 321, 434, 516, 666, 300, 2329, 455, 2657, 13, 51570], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1494, "seek": 616060, "start": 6184.72, "end": 6188.6, "text": " We're going to index it matches zero.", "tokens": [51570, 492, 434, 516, 281, 8186, 309, 10676, 4018, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15381988857103432, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0004044587258249521}, {"id": 1495, "seek": 618860, "start": 6188.6, "end": 6193.8, "text": " So this is going to be the first index, so this tuple at this point.", "tokens": [50364, 407, 341, 307, 516, 281, 312, 264, 700, 8186, 11, 370, 341, 2604, 781, 412, 341, 935, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1496, "seek": 618860, "start": 6193.8, "end": 6195.96, "text": " And then we're going to grab index zero.", "tokens": [50624, 400, 550, 321, 434, 516, 281, 4444, 8186, 4018, 13, 50732], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1497, "seek": 618860, "start": 6195.96, "end": 6198.240000000001, "text": " So now we've gone into this list.", "tokens": [50732, 407, 586, 321, 600, 2780, 666, 341, 1329, 13, 50846], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1498, "seek": 618860, "start": 6198.240000000001, "end": 6204.4800000000005, "text": " We've gone to index zero, this first tuple, and now we're grabbing that first item there.", "tokens": [50846, 492, 600, 2780, 281, 8186, 4018, 11, 341, 700, 2604, 781, 11, 293, 586, 321, 434, 23771, 300, 700, 3174, 456, 13, 51158], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1499, "seek": 618860, "start": 6204.4800000000005, "end": 6210.8, "text": " Now what we need to do is we need to say dot text, you need to do it right here.", "tokens": [51158, 823, 437, 321, 643, 281, 360, 307, 321, 643, 281, 584, 5893, 2487, 11, 291, 643, 281, 360, 309, 558, 510, 13, 51474], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1500, "seek": 618860, "start": 6210.8, "end": 6217.04, "text": " And if you print this off, we get this email address, that label that we gave it up there", "tokens": [51474, 400, 498, 291, 4482, 341, 766, 11, 321, 483, 341, 3796, 2985, 11, 300, 7645, 300, 321, 2729, 309, 493, 456, 51786], "temperature": 0.0, "avg_logprob": -0.15765492669467268, "compression_ratio": 1.853211009174312, "no_speech_prob": 0.005910827312618494}, {"id": 1501, "seek": 621704, "start": 6217.04, "end": 6222.44, "text": " was added into the nlp vocab with this unique lexeme that allows for us to understand what", "tokens": [50364, 390, 3869, 666, 264, 297, 75, 79, 2329, 455, 365, 341, 3845, 476, 87, 5729, 300, 4045, 337, 505, 281, 1223, 437, 50634], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1502, "seek": 621704, "start": 6222.44, "end": 6227.4, "text": " that number corresponds to within the nlp framework.", "tokens": [50634, 300, 1230, 23249, 281, 1951, 264, 297, 75, 79, 8388, 13, 50882], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1503, "seek": 621704, "start": 6227.4, "end": 6234.12, "text": " So this is a very simple example of how a matcher works and how you can use it to do", "tokens": [50882, 407, 341, 307, 257, 588, 2199, 1365, 295, 577, 257, 2995, 260, 1985, 293, 577, 291, 393, 764, 309, 281, 360, 51218], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1504, "seek": 621704, "start": 6234.12, "end": 6236.04, "text": " some pretty cool things.", "tokens": [51218, 512, 1238, 1627, 721, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1505, "seek": 621704, "start": 6236.04, "end": 6241.12, "text": " But let's take a moment, let's pause and let's see what we can do with this matcher.", "tokens": [51314, 583, 718, 311, 747, 257, 1623, 11, 718, 311, 10465, 293, 718, 311, 536, 437, 321, 393, 360, 365, 341, 2995, 260, 13, 51568], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1506, "seek": 621704, "start": 6241.12, "end": 6246.36, "text": " So if we go up into spacey's documentation on the matcher, we'll see that you got a couple", "tokens": [51568, 407, 498, 321, 352, 493, 666, 1901, 88, 311, 14333, 322, 264, 2995, 260, 11, 321, 603, 536, 300, 291, 658, 257, 1916, 51830], "temperature": 0.0, "avg_logprob": -0.09653586700183003, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.14410163462162018}, {"id": 1507, "seek": 624636, "start": 6246.36, "end": 6248.0, "text": " different attributes you can work with.", "tokens": [50364, 819, 17212, 291, 393, 589, 365, 13, 50446], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1508, "seek": 624636, "start": 6248.0, "end": 6250.759999999999, "text": " Now we've, we're going to be seeing this a little bit.", "tokens": [50446, 823, 321, 600, 11, 321, 434, 516, 281, 312, 2577, 341, 257, 707, 857, 13, 50584], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1509, "seek": 624636, "start": 6250.759999999999, "end": 6254.5599999999995, "text": " The orth, this is the exact verbatim of a token.", "tokens": [50584, 440, 420, 392, 11, 341, 307, 264, 1900, 9595, 267, 332, 295, 257, 14862, 13, 50774], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1510, "seek": 624636, "start": 6254.5599999999995, "end": 6260.16, "text": " And we're also going to see text, the exact verbatim, text of a token.", "tokens": [50774, 400, 321, 434, 611, 516, 281, 536, 2487, 11, 264, 1900, 9595, 267, 332, 11, 2487, 295, 257, 14862, 13, 51054], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1511, "seek": 624636, "start": 6260.16, "end": 6261.92, "text": " What we also have is lower.", "tokens": [51054, 708, 321, 611, 362, 307, 3126, 13, 51142], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1512, "seek": 624636, "start": 6261.92, "end": 6267.799999999999, "text": " So what you can do here is you can use lower to say when the item is lowercase and it looks", "tokens": [51142, 407, 437, 291, 393, 360, 510, 307, 291, 393, 764, 3126, 281, 584, 562, 264, 3174, 307, 3126, 9765, 293, 309, 1542, 51436], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1513, "seek": 624636, "start": 6267.799999999999, "end": 6271.04, "text": " like and then give some lowercase pattern.", "tokens": [51436, 411, 293, 550, 976, 512, 3126, 9765, 5102, 13, 51598], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1514, "seek": 624636, "start": 6271.04, "end": 6275.719999999999, "text": " This is going to be very useful for capturing things that might be at the start of a sentence.", "tokens": [51598, 639, 307, 516, 281, 312, 588, 4420, 337, 23384, 721, 300, 1062, 312, 412, 264, 722, 295, 257, 8174, 13, 51832], "temperature": 0.0, "avg_logprob": -0.13862602692797668, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.09804718941450119}, {"id": 1515, "seek": 627572, "start": 6275.72, "end": 6282.4400000000005, "text": " For example, if you were to look for the penguin in the text, anywhere you saw the penguin.", "tokens": [50364, 1171, 1365, 11, 498, 291, 645, 281, 574, 337, 264, 45752, 294, 264, 2487, 11, 4992, 291, 1866, 264, 45752, 13, 50700], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1516, "seek": 627572, "start": 6282.4400000000005, "end": 6287.76, "text": " If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the", "tokens": [50700, 759, 291, 1143, 257, 5102, 300, 390, 445, 3126, 9765, 11, 291, 2759, 380, 3745, 264, 45752, 885, 412, 264, 50966], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1517, "seek": 627572, "start": 6287.76, "end": 6289.04, "text": " start of a sentence.", "tokens": [50966, 722, 295, 257, 8174, 13, 51030], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1518, "seek": 627572, "start": 6289.04, "end": 6291.56, "text": " It would miss it because the T would be capitalized.", "tokens": [51030, 467, 576, 1713, 309, 570, 264, 314, 576, 312, 4238, 1602, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1519, "seek": 627572, "start": 6291.56, "end": 6295.64, "text": " By using lower, you can ensure that your pattern that you're giving it is going to be looking", "tokens": [51156, 3146, 1228, 3126, 11, 291, 393, 5586, 300, 428, 5102, 300, 291, 434, 2902, 309, 307, 516, 281, 312, 1237, 51360], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1520, "seek": 627572, "start": 6295.64, "end": 6300.360000000001, "text": " for any pattern that matches that when the text is lowercase.", "tokens": [51360, 337, 604, 5102, 300, 10676, 300, 562, 264, 2487, 307, 3126, 9765, 13, 51596], "temperature": 0.0, "avg_logprob": -0.11901133401053292, "compression_ratio": 1.7835497835497836, "no_speech_prob": 0.01168584544211626}, {"id": 1521, "seek": 630036, "start": 6300.36, "end": 6307.759999999999, "text": " If is going to be the, the length of your token text is alpha is ASCII is digit.", "tokens": [50364, 759, 307, 516, 281, 312, 264, 11, 264, 4641, 295, 428, 14862, 2487, 307, 8961, 307, 7469, 34, 9503, 307, 14293, 13, 50734], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1522, "seek": 630036, "start": 6307.759999999999, "end": 6311.5599999999995, "text": " This is when your characters are either going to be alphabetical ASCII characters.", "tokens": [50734, 639, 307, 562, 428, 4342, 366, 2139, 516, 281, 312, 23339, 804, 7469, 34, 9503, 4342, 13, 50924], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1523, "seek": 630036, "start": 6311.5599999999995, "end": 6316.639999999999, "text": " So the American standard coding initiative, I can't remember what it stands for, but it's", "tokens": [50924, 407, 264, 2665, 3832, 17720, 11552, 11, 286, 393, 380, 1604, 437, 309, 7382, 337, 11, 457, 309, 311, 51178], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1524, "seek": 630036, "start": 6316.639999999999, "end": 6322.92, "text": " that, I think it's 128 bit thing that America came up with when they started in coding text.", "tokens": [51178, 300, 11, 286, 519, 309, 311, 29810, 857, 551, 300, 3374, 1361, 493, 365, 562, 436, 1409, 294, 17720, 2487, 13, 51492], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1525, "seek": 630036, "start": 6322.92, "end": 6327.2, "text": " It's now replaced with UTF eight and is digit is going to look for something if it is a", "tokens": [51492, 467, 311, 586, 10772, 365, 624, 20527, 3180, 293, 307, 14293, 307, 516, 281, 574, 337, 746, 498, 309, 307, 257, 51706], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1526, "seek": 630036, "start": 6327.2, "end": 6328.2, "text": " digit.", "tokens": [51706, 14293, 13, 51756], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1527, "seek": 630036, "start": 6328.2, "end": 6329.4, "text": " So think of each of these as a token.", "tokens": [51756, 407, 519, 295, 1184, 295, 613, 382, 257, 14862, 13, 51816], "temperature": 0.0, "avg_logprob": -0.15335632860660553, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.8219284415245056}, {"id": 1528, "seek": 632940, "start": 6329.4, "end": 6334.799999999999, "text": " So if the token is a digit, then that counts in the pattern is lower is upper is title.", "tokens": [50364, 407, 498, 264, 14862, 307, 257, 14293, 11, 550, 300, 14893, 294, 264, 5102, 307, 3126, 307, 6597, 307, 4876, 13, 50634], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1529, "seek": 632940, "start": 6334.799999999999, "end": 6336.36, "text": " These should be all self explanatory.", "tokens": [50634, 1981, 820, 312, 439, 2698, 9045, 4745, 13, 50712], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1530, "seek": 632940, "start": 6336.36, "end": 6340.799999999999, "text": " If it's lowercase, if it's uppercase, if it's a title, so capitalized.", "tokens": [50712, 759, 309, 311, 3126, 9765, 11, 498, 309, 311, 11775, 2869, 651, 11, 498, 309, 311, 257, 4876, 11, 370, 4238, 1602, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1531, "seek": 632940, "start": 6340.799999999999, "end": 6344.0, "text": " And if you don't understand what all of these do right now, I'm going to be going through", "tokens": [50934, 400, 498, 291, 500, 380, 1223, 437, 439, 295, 613, 360, 558, 586, 11, 286, 478, 516, 281, 312, 516, 807, 51094], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1532, "seek": 632940, "start": 6344.0, "end": 6347.639999999999, "text": " and showing you in just a second, just giving you an overview of different things that can", "tokens": [51094, 293, 4099, 291, 294, 445, 257, 1150, 11, 445, 2902, 291, 364, 12492, 295, 819, 721, 300, 393, 51276], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1533, "seek": 632940, "start": 6347.639999999999, "end": 6352.639999999999, "text": " be included within the, the matcher or the entity ruler here.", "tokens": [51276, 312, 5556, 1951, 264, 11, 264, 2995, 260, 420, 264, 13977, 19661, 510, 13, 51526], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1534, "seek": 632940, "start": 6352.639999999999, "end": 6359.24, "text": " So what we can also do is find something that if the token is actually the start of a sentence,", "tokens": [51526, 407, 437, 321, 393, 611, 360, 307, 915, 746, 300, 498, 264, 14862, 307, 767, 264, 722, 295, 257, 8174, 11, 51856], "temperature": 0.0, "avg_logprob": -0.13473450298040685, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.03209292143583298}, {"id": 1535, "seek": 635924, "start": 6359.24, "end": 6363.4, "text": " if it's like a number, like a URL, like an email, you can extract it.", "tokens": [50364, 498, 309, 311, 411, 257, 1230, 11, 411, 257, 12905, 11, 411, 364, 3796, 11, 291, 393, 8947, 309, 13, 50572], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1536, "seek": 635924, "start": 6363.4, "end": 6367.0, "text": " And here is the main part I want to talk about because this is where you're really going", "tokens": [50572, 400, 510, 307, 264, 2135, 644, 286, 528, 281, 751, 466, 570, 341, 307, 689, 291, 434, 534, 516, 50752], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1537, "seek": 635924, "start": 6367.0, "end": 6372.5199999999995, "text": " to find spacey out shines any other string matching system out there.", "tokens": [50752, 281, 915, 1901, 88, 484, 28056, 604, 661, 6798, 14324, 1185, 484, 456, 13, 51028], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1538, "seek": 635924, "start": 6372.5199999999995, "end": 6377.8, "text": " So what you can do is you can use the tokens, part of speech tag, morphological analysis,", "tokens": [51028, 407, 437, 291, 393, 360, 307, 291, 393, 764, 264, 22667, 11, 644, 295, 6218, 6162, 11, 25778, 4383, 5215, 11, 51292], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1539, "seek": 635924, "start": 6377.8, "end": 6382.36, "text": " dependency label, lima and shape to actually make matches.", "tokens": [51292, 33621, 7645, 11, 2364, 64, 293, 3909, 281, 767, 652, 10676, 13, 51520], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1540, "seek": 635924, "start": 6382.36, "end": 6387.38, "text": " So not just matching a sequence of characters, but matching a sequence of linguistic features.", "tokens": [51520, 407, 406, 445, 14324, 257, 8310, 295, 4342, 11, 457, 14324, 257, 8310, 295, 43002, 4122, 13, 51771], "temperature": 0.0, "avg_logprob": -0.125039548955412, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.04207553714513779}, {"id": 1541, "seek": 638738, "start": 6387.38, "end": 6388.66, "text": " So think about this.", "tokens": [50364, 407, 519, 466, 341, 13, 50428], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1542, "seek": 638738, "start": 6388.66, "end": 6393.900000000001, "text": " If you wanted to capture all instances of a proper noun followed by a verb, you would", "tokens": [50428, 759, 291, 1415, 281, 7983, 439, 14519, 295, 257, 2296, 23307, 6263, 538, 257, 9595, 11, 291, 576, 50690], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1543, "seek": 638738, "start": 6393.900000000001, "end": 6396.62, "text": " not be able to do that with regex.", "tokens": [50690, 406, 312, 1075, 281, 360, 300, 365, 319, 432, 87, 13, 50826], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1544, "seek": 638738, "start": 6396.62, "end": 6397.72, "text": " There's not a way to do it.", "tokens": [50826, 821, 311, 406, 257, 636, 281, 360, 309, 13, 50881], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1545, "seek": 638738, "start": 6397.72, "end": 6400.1, "text": " You can't give regex if this is a verb.", "tokens": [50881, 509, 393, 380, 976, 319, 432, 87, 498, 341, 307, 257, 9595, 13, 51000], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1546, "seek": 638738, "start": 6400.1, "end": 6402.32, "text": " Regex is just a string matching framework.", "tokens": [51000, 1300, 432, 87, 307, 445, 257, 6798, 14324, 8388, 13, 51111], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1547, "seek": 638738, "start": 6402.32, "end": 6406.5, "text": " It's not a framework for actually identifying linguistic features, using them and extracting", "tokens": [51111, 467, 311, 406, 257, 8388, 337, 767, 16696, 43002, 4122, 11, 1228, 552, 293, 49844, 51320], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1548, "seek": 638738, "start": 6406.5, "end": 6407.5, "text": " them.", "tokens": [51320, 552, 13, 51370], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1549, "seek": 638738, "start": 6407.5, "end": 6412.22, "text": " So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological", "tokens": [51370, 407, 341, 307, 689, 321, 393, 13982, 439, 264, 1347, 295, 7673, 3071, 21882, 11, 264, 6162, 1321, 11, 264, 25778, 4383, 51606], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1550, "seek": 638738, "start": 6412.22, "end": 6416.900000000001, "text": " analysis, the depth, the lemma, et cetera.", "tokens": [51606, 5215, 11, 264, 7161, 11, 264, 7495, 1696, 11, 1030, 11458, 13, 51840], "temperature": 0.0, "avg_logprob": -0.14921926594466614, "compression_ratio": 1.70446735395189, "no_speech_prob": 0.010327528230845928}, {"id": 1551, "seek": 641690, "start": 6417.54, "end": 6421.94, "text": " We can actually use all those things that have gone through the pipeline and the matcher", "tokens": [50396, 492, 393, 767, 764, 439, 729, 721, 300, 362, 2780, 807, 264, 15517, 293, 264, 2995, 260, 50616], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1552, "seek": 641690, "start": 6421.94, "end": 6427.46, "text": " can leverage those linguistic features and make some really cool, allow us to make really", "tokens": [50616, 393, 13982, 729, 43002, 4122, 293, 652, 512, 534, 1627, 11, 2089, 505, 281, 652, 534, 50892], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1553, "seek": 641690, "start": 6427.46, "end": 6431.379999999999, "text": " cool patterns that can match really robust and complicated things.", "tokens": [50892, 1627, 8294, 300, 393, 2995, 534, 13956, 293, 6179, 721, 13, 51088], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1554, "seek": 641690, "start": 6431.379999999999, "end": 6434.7, "text": " And the final thing I'm going to talk about is right here, the OP.", "tokens": [51088, 400, 264, 2572, 551, 286, 478, 516, 281, 751, 466, 307, 558, 510, 11, 264, 23324, 13, 51254], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1555, "seek": 641690, "start": 6434.7, "end": 6439.339999999999, "text": " This is the operator or quantifier and determines how often to match a token.", "tokens": [51254, 639, 307, 264, 12973, 420, 4426, 9902, 293, 24799, 577, 2049, 281, 2995, 257, 14862, 13, 51486], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1556, "seek": 641690, "start": 6439.339999999999, "end": 6440.98, "text": " So there's a few different things you can use here.", "tokens": [51486, 407, 456, 311, 257, 1326, 819, 721, 291, 393, 764, 510, 13, 51568], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1557, "seek": 641690, "start": 6440.98, "end": 6446.0199999999995, "text": " There's the exclamation mark, negate the pattern, requiring it to match zero times.", "tokens": [51568, 821, 311, 264, 1624, 43233, 1491, 11, 2485, 473, 264, 5102, 11, 24165, 309, 281, 2995, 4018, 1413, 13, 51820], "temperature": 0.0, "avg_logprob": -0.15115862507973948, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.13651913404464722}, {"id": 1558, "seek": 644602, "start": 6446.06, "end": 6449.26, "text": " So in this scenario, the sequence would never occur.", "tokens": [50366, 407, 294, 341, 9005, 11, 264, 8310, 576, 1128, 5160, 13, 50526], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1559, "seek": 644602, "start": 6449.26, "end": 6453.9800000000005, "text": " There's the question mark, make the pattern optional, allowing it to match zero or one", "tokens": [50526, 821, 311, 264, 1168, 1491, 11, 652, 264, 5102, 17312, 11, 8293, 309, 281, 2995, 4018, 420, 472, 50762], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1560, "seek": 644602, "start": 6453.9800000000005, "end": 6459.580000000001, "text": " times require the pattern to match one or more times with the plus and the asterisk,", "tokens": [50762, 1413, 3651, 264, 5102, 281, 2995, 472, 420, 544, 1413, 365, 264, 1804, 293, 264, 257, 3120, 7797, 11, 51042], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1561, "seek": 644602, "start": 6459.580000000001, "end": 6463.5, "text": " the thing on the shift eight, allow the pattern to match zero or more times.", "tokens": [51042, 264, 551, 322, 264, 5513, 3180, 11, 2089, 264, 5102, 281, 2995, 4018, 420, 544, 1413, 13, 51238], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1562, "seek": 644602, "start": 6463.5, "end": 6467.620000000001, "text": " There's other things as well that you can do to make this match or a bit more robust.", "tokens": [51238, 821, 311, 661, 721, 382, 731, 300, 291, 393, 360, 281, 652, 341, 2995, 420, 257, 857, 544, 13956, 13, 51444], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1563, "seek": 644602, "start": 6467.620000000001, "end": 6471.700000000001, "text": " But for right now, let's jump into the basics and see how we can really kind of take these", "tokens": [51444, 583, 337, 558, 586, 11, 718, 311, 3012, 666, 264, 14688, 293, 536, 577, 321, 393, 534, 733, 295, 747, 613, 51648], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1564, "seek": 644602, "start": 6471.700000000001, "end": 6475.580000000001, "text": " and apply them in a real world question.", "tokens": [51648, 293, 3079, 552, 294, 257, 957, 1002, 1168, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12468174941667164, "compression_ratio": 1.8602150537634408, "no_speech_prob": 0.0005527663743123412}, {"id": 1565, "seek": 647558, "start": 6475.58, "end": 6480.58, "text": " So what I'm going to do is I'm going to work with another data set or another piece of", "tokens": [50364, 407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 589, 365, 1071, 1412, 992, 420, 1071, 2522, 295, 50614], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1566, "seek": 647558, "start": 6480.58, "end": 6482.98, "text": " data that I've grabbed off of Wikipedia.", "tokens": [50614, 1412, 300, 286, 600, 18607, 766, 295, 28999, 13, 50734], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1567, "seek": 647558, "start": 6482.98, "end": 6487.1, "text": " And this is the Wikipedia article entry on Martin Luther King, Jr.", "tokens": [50734, 400, 341, 307, 264, 28999, 7222, 8729, 322, 9184, 20693, 3819, 11, 17261, 13, 50940], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1568, "seek": 647558, "start": 6487.1, "end": 6492.82, "text": " It's the opening opening few paragraphs, let's print it off and just take a quick look.", "tokens": [50940, 467, 311, 264, 5193, 5193, 1326, 48910, 11, 718, 311, 4482, 309, 766, 293, 445, 747, 257, 1702, 574, 13, 51226], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1569, "seek": 647558, "start": 6492.82, "end": 6493.82, "text": " And this is what it looks like.", "tokens": [51226, 400, 341, 307, 437, 309, 1542, 411, 13, 51276], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1570, "seek": 647558, "start": 6493.82, "end": 6495.0199999999995, "text": " You can go through and read it.", "tokens": [51276, 509, 393, 352, 807, 293, 1401, 309, 13, 51336], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1571, "seek": 647558, "start": 6495.0199999999995, "end": 6497.82, "text": " We're not too concerned about what it says right now.", "tokens": [51336, 492, 434, 406, 886, 5922, 466, 437, 309, 1619, 558, 586, 13, 51476], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1572, "seek": 647558, "start": 6497.82, "end": 6501.66, "text": " We're concerned about trying to extract a very specific set of patterns.", "tokens": [51476, 492, 434, 5922, 466, 1382, 281, 8947, 257, 588, 2685, 992, 295, 8294, 13, 51668], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1573, "seek": 647558, "start": 6501.66, "end": 6505.0599999999995, "text": " What we're interested in grabbing are all proper nouns.", "tokens": [51668, 708, 321, 434, 3102, 294, 23771, 366, 439, 2296, 48184, 13, 51838], "temperature": 0.0, "avg_logprob": -0.120234385024022, "compression_ratio": 1.7574750830564785, "no_speech_prob": 0.003945194184780121}, {"id": 1574, "seek": 650506, "start": 6505.06, "end": 6506.54, "text": " That's the task ahead of us.", "tokens": [50364, 663, 311, 264, 5633, 2286, 295, 505, 13, 50438], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1575, "seek": 650506, "start": 6506.54, "end": 6512.06, "text": " Somebody has asked us to take this text in, extract all the proper nouns for me, but we're", "tokens": [50438, 13463, 575, 2351, 505, 281, 747, 341, 2487, 294, 11, 8947, 439, 264, 2296, 48184, 337, 385, 11, 457, 321, 434, 50714], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1576, "seek": 650506, "start": 6512.06, "end": 6516.740000000001, "text": " going to do a lot more and not just the proper nouns, but we want to get multi word tokens.", "tokens": [50714, 516, 281, 360, 257, 688, 544, 293, 406, 445, 264, 2296, 48184, 11, 457, 321, 528, 281, 483, 4825, 1349, 22667, 13, 50948], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1577, "seek": 650506, "start": 6516.740000000001, "end": 6523.3, "text": " So we want to have Martin Luther King, Jr. extracted as one token, so one export.", "tokens": [50948, 407, 321, 528, 281, 362, 9184, 20693, 3819, 11, 17261, 13, 34086, 382, 472, 14862, 11, 370, 472, 10725, 13, 51276], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1578, "seek": 650506, "start": 6523.3, "end": 6529.02, "text": " So the other things that we want to have are these kind of structured in sequential order.", "tokens": [51276, 407, 264, 661, 721, 300, 321, 528, 281, 362, 366, 613, 733, 295, 18519, 294, 42881, 1668, 13, 51562], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1579, "seek": 650506, "start": 6529.02, "end": 6534.26, "text": " So find out where they appear and extract them based on their start token.", "tokens": [51562, 407, 915, 484, 689, 436, 4204, 293, 8947, 552, 2361, 322, 641, 722, 14862, 13, 51824], "temperature": 0.0, "avg_logprob": -0.109066292390985, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.09007088094949722}, {"id": 1580, "seek": 653426, "start": 6534.26, "end": 6537.66, "text": " So let's go ahead and start trying to do some of these things right now.", "tokens": [50364, 407, 718, 311, 352, 2286, 293, 722, 1382, 281, 360, 512, 295, 613, 721, 558, 586, 13, 50534], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1581, "seek": 653426, "start": 6537.66, "end": 6539.18, "text": " Let's scroll down here.", "tokens": [50534, 961, 311, 11369, 760, 510, 13, 50610], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1582, "seek": 653426, "start": 6539.18, "end": 6540.22, "text": " Great.", "tokens": [50610, 3769, 13, 50662], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1583, "seek": 653426, "start": 6540.22, "end": 6544.42, "text": " So we need to create really a new NLP object now at this point.", "tokens": [50662, 407, 321, 643, 281, 1884, 534, 257, 777, 426, 45196, 2657, 586, 412, 341, 935, 13, 50872], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1584, "seek": 653426, "start": 6544.42, "end": 6545.42, "text": " So let's create a new one.", "tokens": [50872, 407, 718, 311, 1884, 257, 777, 472, 13, 50922], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1585, "seek": 653426, "start": 6545.42, "end": 6550.76, "text": " We're going to start working with the Ncore Web SM model.", "tokens": [50922, 492, 434, 516, 281, 722, 1364, 365, 264, 426, 12352, 9573, 13115, 2316, 13, 51189], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1586, "seek": 653426, "start": 6550.76, "end": 6555.3, "text": " If you're working with a different model, like the large or the transformer, you're", "tokens": [51189, 759, 291, 434, 1364, 365, 257, 819, 2316, 11, 411, 264, 2416, 420, 264, 31782, 11, 291, 434, 51416], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1587, "seek": 653426, "start": 6555.3, "end": 6557.42, "text": " going to have more accurate results.", "tokens": [51416, 516, 281, 362, 544, 8559, 3542, 13, 51522], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1588, "seek": 653426, "start": 6557.42, "end": 6561.46, "text": " But for right now, we're just trying to do this quickly for demonstration purposes.", "tokens": [51522, 583, 337, 558, 586, 11, 321, 434, 445, 1382, 281, 360, 341, 2661, 337, 16520, 9932, 13, 51724], "temperature": 0.0, "avg_logprob": -0.19223789978027345, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.030209612101316452}, {"id": 1589, "seek": 656146, "start": 6561.46, "end": 6566.78, "text": " So again, just like before, we're creating that with NLP dot vocab.", "tokens": [50364, 407, 797, 11, 445, 411, 949, 11, 321, 434, 4084, 300, 365, 426, 45196, 5893, 2329, 455, 13, 50630], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1590, "seek": 656146, "start": 6566.78, "end": 6569.06, "text": " And then we're going to create a pattern.", "tokens": [50630, 400, 550, 321, 434, 516, 281, 1884, 257, 5102, 13, 50744], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1591, "seek": 656146, "start": 6569.06, "end": 6571.38, "text": " So this is the pattern that we're going to work with.", "tokens": [50744, 407, 341, 307, 264, 5102, 300, 321, 434, 516, 281, 589, 365, 13, 50860], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1592, "seek": 656146, "start": 6571.38, "end": 6578.9800000000005, "text": " We want to find any occurrence of a POS part of speech that corresponds to proper noun.", "tokens": [50860, 492, 528, 281, 915, 604, 36122, 295, 257, 430, 4367, 644, 295, 6218, 300, 23249, 281, 2296, 23307, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1593, "seek": 656146, "start": 6578.9800000000005, "end": 6585.22, "text": " That's the way in which POS labels proper nouns is prop in.", "tokens": [51240, 663, 311, 264, 636, 294, 597, 430, 4367, 16949, 2296, 48184, 307, 2365, 294, 13, 51552], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1594, "seek": 656146, "start": 6585.22, "end": 6588.34, "text": " And we should be able to with that extract all proper nouns.", "tokens": [51552, 400, 321, 820, 312, 1075, 281, 365, 300, 8947, 439, 2296, 48184, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13437347786099302, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.05183611437678337}, {"id": 1595, "seek": 658834, "start": 6588.34, "end": 6594.54, "text": " So we can say matcher dot add, and we're going to say proper noun.", "tokens": [50364, 407, 321, 393, 584, 2995, 260, 5893, 909, 11, 293, 321, 434, 516, 281, 584, 2296, 23307, 13, 50674], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1596, "seek": 658834, "start": 6594.54, "end": 6598.06, "text": " And that's going to be our pattern.", "tokens": [50674, 400, 300, 311, 516, 281, 312, 527, 5102, 13, 50850], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1597, "seek": 658834, "start": 6598.06, "end": 6600.82, "text": " And then what we can do just like before, we're going to create the doc object.", "tokens": [50850, 400, 550, 437, 321, 393, 360, 445, 411, 949, 11, 321, 434, 516, 281, 1884, 264, 3211, 2657, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1598, "seek": 658834, "start": 6600.82, "end": 6604.22, "text": " This is going to be NLP text.", "tokens": [50988, 639, 307, 516, 281, 312, 426, 45196, 2487, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1599, "seek": 658834, "start": 6604.22, "end": 6608.3, "text": " And then we're going to say matches is equal to matcher doc.", "tokens": [51158, 400, 550, 321, 434, 516, 281, 584, 10676, 307, 2681, 281, 2995, 260, 3211, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1600, "seek": 658834, "start": 6608.3, "end": 6614.42, "text": " So we're going to create the matches by passing that doc object into our matcher class.", "tokens": [51362, 407, 321, 434, 516, 281, 1884, 264, 10676, 538, 8437, 300, 3211, 2657, 666, 527, 2995, 260, 1508, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1601, "seek": 658834, "start": 6614.42, "end": 6617.860000000001, "text": " And then we're going to print off the length of the matches.", "tokens": [51668, 400, 550, 321, 434, 516, 281, 4482, 766, 264, 4641, 295, 264, 10676, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1449886129683807, "compression_ratio": 2.099502487562189, "no_speech_prob": 0.008577191270887852}, {"id": 1602, "seek": 661786, "start": 6617.86, "end": 6622.54, "text": " So how many matches were found, and then we're going to say for match in matches.", "tokens": [50364, 407, 577, 867, 10676, 645, 1352, 11, 293, 550, 321, 434, 516, 281, 584, 337, 2995, 294, 10676, 13, 50598], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1603, "seek": 661786, "start": 6622.54, "end": 6625.42, "text": " And we're just going to grab the first 10 because I've done this and there's a lot", "tokens": [50598, 400, 321, 434, 445, 516, 281, 4444, 264, 700, 1266, 570, 286, 600, 1096, 341, 293, 456, 311, 257, 688, 50742], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1604, "seek": 661786, "start": 6625.42, "end": 6630.94, "text": " and you'll see why let's print off.", "tokens": [50742, 293, 291, 603, 536, 983, 718, 311, 4482, 766, 13, 51018], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1605, "seek": 661786, "start": 6630.94, "end": 6633.38, "text": " Let's print off in this case, match.", "tokens": [51018, 961, 311, 4482, 766, 294, 341, 1389, 11, 2995, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1606, "seek": 661786, "start": 6633.38, "end": 6636.259999999999, "text": " And then we're going to print off specifically what that text is.", "tokens": [51140, 400, 550, 321, 434, 516, 281, 4482, 766, 4682, 437, 300, 2487, 307, 13, 51284], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1607, "seek": 661786, "start": 6636.259999999999, "end": 6641.9, "text": " Remember, the output is the lexine followed by the start token and the end token, which", "tokens": [51284, 5459, 11, 264, 5598, 307, 264, 476, 87, 533, 6263, 538, 264, 722, 14862, 293, 264, 917, 14862, 11, 597, 51566], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1608, "seek": 661786, "start": 6641.9, "end": 6644.42, "text": " means we can go into the doc object.", "tokens": [51566, 1355, 321, 393, 352, 666, 264, 3211, 2657, 13, 51692], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1609, "seek": 661786, "start": 6644.42, "end": 6646.259999999999, "text": " And we can set up something like this.", "tokens": [51692, 400, 321, 393, 992, 493, 746, 411, 341, 13, 51784], "temperature": 0.0, "avg_logprob": -0.16076337829116702, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.0006070599774830043}, {"id": 1610, "seek": 664626, "start": 6646.26, "end": 6653.820000000001, "text": " We can say match one, so index one, which is the start token and match two, which is", "tokens": [50364, 492, 393, 584, 2995, 472, 11, 370, 8186, 472, 11, 597, 307, 264, 722, 14862, 293, 2995, 732, 11, 597, 307, 50742], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1611, "seek": 664626, "start": 6653.820000000001, "end": 6655.02, "text": " the end token.", "tokens": [50742, 264, 917, 14862, 13, 50802], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1612, "seek": 664626, "start": 6655.02, "end": 6658.62, "text": " And that'll allow us to actually index what these words are.", "tokens": [50802, 400, 300, 603, 2089, 505, 281, 767, 8186, 437, 613, 2283, 366, 13, 50982], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1613, "seek": 664626, "start": 6658.62, "end": 6660.900000000001, "text": " And when we do this, we can see all these printed out.", "tokens": [50982, 400, 562, 321, 360, 341, 11, 321, 393, 536, 439, 613, 13567, 484, 13, 51096], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1614, "seek": 664626, "start": 6660.900000000001, "end": 6667.14, "text": " So this is the match, the lexine here, which is going to be proper down all the way down.", "tokens": [51096, 407, 341, 307, 264, 2995, 11, 264, 476, 87, 533, 510, 11, 597, 307, 516, 281, 312, 2296, 760, 439, 264, 636, 760, 13, 51408], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1615, "seek": 664626, "start": 6667.14, "end": 6673.14, "text": " We've got the zero here, which corresponds to the start token, the end token.", "tokens": [51408, 492, 600, 658, 264, 4018, 510, 11, 597, 23249, 281, 264, 722, 14862, 11, 264, 917, 14862, 13, 51708], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1616, "seek": 664626, "start": 6673.14, "end": 6675.06, "text": " And this is the the token that we extracted.", "tokens": [51708, 400, 341, 307, 264, 264, 14862, 300, 321, 34086, 13, 51804], "temperature": 0.0, "avg_logprob": -0.15215957959493, "compression_ratio": 1.927927927927928, "no_speech_prob": 0.0033765104599297047}, {"id": 1617, "seek": 667506, "start": 6675.06, "end": 6681.02, "text": " Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right?", "tokens": [50364, 9184, 11, 20693, 11, 3819, 11, 21954, 11, 5116, 11, 3819, 11, 21954, 11, 321, 600, 658, 257, 1154, 510, 11, 558, 30, 50662], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1618, "seek": 667506, "start": 6681.02, "end": 6683.780000000001, "text": " So the problem should be pretty obvious right now.", "tokens": [50662, 407, 264, 1154, 820, 312, 1238, 6322, 558, 586, 13, 50800], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1619, "seek": 667506, "start": 6683.780000000001, "end": 6691.900000000001, "text": " And the problem is that we have grabbed all proper nouns, but these proper nouns are just", "tokens": [50800, 400, 264, 1154, 307, 300, 321, 362, 18607, 439, 2296, 48184, 11, 457, 613, 2296, 48184, 366, 445, 51206], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1620, "seek": 667506, "start": 6691.900000000001, "end": 6693.54, "text": " individual tokens.", "tokens": [51206, 2609, 22667, 13, 51288], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1621, "seek": 667506, "start": 6693.54, "end": 6696.22, "text": " We haven't grabbed the multi word tokens.", "tokens": [51288, 492, 2378, 380, 18607, 264, 4825, 1349, 22667, 13, 51422], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1622, "seek": 667506, "start": 6696.22, "end": 6697.780000000001, "text": " So how do we go about doing that?", "tokens": [51422, 407, 577, 360, 321, 352, 466, 884, 300, 30, 51500], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1623, "seek": 667506, "start": 6697.780000000001, "end": 6702.18, "text": " Well, we can solve this problem by let's go ahead and just copy and paste all this from", "tokens": [51500, 1042, 11, 321, 393, 5039, 341, 1154, 538, 718, 311, 352, 2286, 293, 445, 5055, 293, 9163, 439, 341, 490, 51720], "temperature": 0.0, "avg_logprob": -0.16158266420717593, "compression_ratio": 1.718487394957983, "no_speech_prob": 0.025175657123327255}, {"id": 1624, "seek": 670218, "start": 6702.18, "end": 6703.18, "text": " here.", "tokens": [50364, 510, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1625, "seek": 670218, "start": 6703.18, "end": 6707.54, "text": " And we're going to make one small adjustment here.", "tokens": [50414, 400, 321, 434, 516, 281, 652, 472, 1359, 17132, 510, 13, 50632], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1626, "seek": 670218, "start": 6707.54, "end": 6713.9400000000005, "text": " We're going to change this to OP with a plus.", "tokens": [50632, 492, 434, 516, 281, 1319, 341, 281, 23324, 365, 257, 1804, 13, 50952], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1627, "seek": 670218, "start": 6713.9400000000005, "end": 6715.18, "text": " So what does that mean?", "tokens": [50952, 407, 437, 775, 300, 914, 30, 51014], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1628, "seek": 670218, "start": 6715.18, "end": 6719.740000000001, "text": " Well, let's pop back into our matcher under spacey and check it out.", "tokens": [51014, 1042, 11, 718, 311, 1665, 646, 666, 527, 2995, 260, 833, 1901, 88, 293, 1520, 309, 484, 13, 51242], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1629, "seek": 670218, "start": 6719.740000000001, "end": 6725.8, "text": " So OP members, the operator or quantifier, we're going to use the plus symbol.", "tokens": [51242, 407, 23324, 2679, 11, 264, 12973, 420, 4426, 9902, 11, 321, 434, 516, 281, 764, 264, 1804, 5986, 13, 51545], "temperature": 0.0, "avg_logprob": -0.16173802352533106, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.1008540689945221}, {"id": 1630, "seek": 672580, "start": 6725.8, "end": 6732.56, "text": " So it's going to look for a proper noun that occurs one or more times.", "tokens": [50364, 407, 309, 311, 516, 281, 574, 337, 257, 2296, 23307, 300, 11843, 472, 420, 544, 1413, 13, 50702], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1631, "seek": 672580, "start": 6732.56, "end": 6736.56, "text": " So in theory, right, this should allow us to grab multi word tokens.", "tokens": [50702, 407, 294, 5261, 11, 558, 11, 341, 820, 2089, 505, 281, 4444, 4825, 1349, 22667, 13, 50902], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1632, "seek": 672580, "start": 6736.56, "end": 6739.6, "text": " It's going to look for a proper noun and grab as many as there are.", "tokens": [50902, 467, 311, 516, 281, 574, 337, 257, 2296, 23307, 293, 4444, 382, 867, 382, 456, 366, 13, 51054], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1633, "seek": 672580, "start": 6739.6, "end": 6744.4400000000005, "text": " So anything that occurs one or more times, if we run this, though, we see a problem.", "tokens": [51054, 407, 1340, 300, 11843, 472, 420, 544, 1413, 11, 498, 321, 1190, 341, 11, 1673, 11, 321, 536, 257, 1154, 13, 51296], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1634, "seek": 672580, "start": 6744.4400000000005, "end": 6748.56, "text": " We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther", "tokens": [51296, 492, 600, 5768, 9184, 11, 321, 658, 9184, 20693, 11, 437, 321, 658, 20693, 11, 437, 321, 658, 9184, 20693, 51502], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1635, "seek": 672580, "start": 6748.56, "end": 6753.0, "text": " King, Luther King, King Martin Luther King, Junior, what what is going on here?", "tokens": [51502, 3819, 11, 20693, 3819, 11, 3819, 9184, 20693, 3819, 11, 21954, 11, 437, 437, 307, 516, 322, 510, 30, 51724], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1636, "seek": 672580, "start": 6753.0, "end": 6755.56, "text": " Well, you might already have figured it out.", "tokens": [51724, 1042, 11, 291, 1062, 1217, 362, 8932, 309, 484, 13, 51852], "temperature": 0.0, "avg_logprob": -0.15616451992708094, "compression_ratio": 1.9921259842519685, "no_speech_prob": 0.05032668262720108}, {"id": 1637, "seek": 675556, "start": 6755.56, "end": 6758.320000000001, "text": " It has done exactly what we told it to do.", "tokens": [50364, 467, 575, 1096, 2293, 437, 321, 1907, 309, 281, 360, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1638, "seek": 675556, "start": 6758.320000000001, "end": 6762.8, "text": " It's grabbed all sequence of tokens that were proper nouns that occurred one or more", "tokens": [50502, 467, 311, 18607, 439, 8310, 295, 22667, 300, 645, 2296, 48184, 300, 11068, 472, 420, 544, 50726], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1639, "seek": 675556, "start": 6762.8, "end": 6764.4400000000005, "text": " times.", "tokens": [50726, 1413, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1640, "seek": 675556, "start": 6764.4400000000005, "end": 6766.280000000001, "text": " Just so happens some of these overlap.", "tokens": [50808, 1449, 370, 2314, 512, 295, 613, 19959, 13, 50900], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1641, "seek": 675556, "start": 6766.280000000001, "end": 6770.96, "text": " So token that's doc zero to one, zero to two.", "tokens": [50900, 407, 14862, 300, 311, 3211, 4018, 281, 472, 11, 4018, 281, 732, 13, 51134], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1642, "seek": 675556, "start": 6770.96, "end": 6776.360000000001, "text": " So you can see the problem here is it's grabbing all of these and any combination of them.", "tokens": [51134, 407, 291, 393, 536, 264, 1154, 510, 307, 309, 311, 23771, 439, 295, 613, 293, 604, 6562, 295, 552, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1643, "seek": 675556, "start": 6776.360000000001, "end": 6780.400000000001, "text": " What we can do, though, is we can add an extra layer to this.", "tokens": [51404, 708, 321, 393, 360, 11, 1673, 11, 307, 321, 393, 909, 364, 2857, 4583, 281, 341, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1644, "seek": 675556, "start": 6780.400000000001, "end": 6784.56, "text": " So let's again, copy what we've just done because it was, it was almost there.", "tokens": [51606, 407, 718, 311, 797, 11, 5055, 437, 321, 600, 445, 1096, 570, 309, 390, 11, 309, 390, 1920, 456, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1601350555419922, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.012819118797779083}, {"id": 1645, "seek": 678456, "start": 6784.56, "end": 6786.72, "text": " It was good, but it wasn't great.", "tokens": [50364, 467, 390, 665, 11, 457, 309, 2067, 380, 869, 13, 50472], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1646, "seek": 678456, "start": 6786.72, "end": 6791.080000000001, "text": " We're going to do one new thing here when we add in the patterns, we're going to pass", "tokens": [50472, 492, 434, 516, 281, 360, 472, 777, 551, 510, 562, 321, 909, 294, 264, 8294, 11, 321, 434, 516, 281, 1320, 50690], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1647, "seek": 678456, "start": 6791.080000000001, "end": 6797.4800000000005, "text": " in the keyword argument, greedy, we're going to say longest capital, all capital letters", "tokens": [50690, 294, 264, 20428, 6770, 11, 28228, 11, 321, 434, 516, 281, 584, 15438, 4238, 11, 439, 4238, 7825, 51010], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1648, "seek": 678456, "start": 6797.4800000000005, "end": 6798.4800000000005, "text": " here.", "tokens": [51010, 510, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1649, "seek": 678456, "start": 6798.4800000000005, "end": 6802.56, "text": " And if we execute that, it's going to look for the longest token out of that mix, and", "tokens": [51060, 400, 498, 321, 14483, 300, 11, 309, 311, 516, 281, 574, 337, 264, 15438, 14862, 484, 295, 300, 2890, 11, 293, 51264], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1650, "seek": 678456, "start": 6802.56, "end": 6806.72, "text": " it's going to give that one, make that one the only token that it extracts.", "tokens": [51264, 309, 311, 516, 281, 976, 300, 472, 11, 652, 300, 472, 264, 787, 14862, 300, 309, 8947, 82, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1651, "seek": 678456, "start": 6806.72, "end": 6813.56, "text": " We noticed that our length has changed from what was it up here, 175 to 61.", "tokens": [51472, 492, 5694, 300, 527, 4641, 575, 3105, 490, 437, 390, 309, 493, 510, 11, 41165, 281, 28294, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14112509155273437, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.04083871468901634}, {"id": 1652, "seek": 681356, "start": 6813.56, "end": 6814.88, "text": " So this is much better.", "tokens": [50364, 407, 341, 307, 709, 1101, 13, 50430], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1653, "seek": 681356, "start": 6814.88, "end": 6819.72, "text": " However, we should have recognized right now, another problem.", "tokens": [50430, 2908, 11, 321, 820, 362, 9823, 558, 586, 11, 1071, 1154, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1654, "seek": 681356, "start": 6819.72, "end": 6820.96, "text": " What have we done wrong?", "tokens": [50672, 708, 362, 321, 1096, 2085, 30, 50734], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1655, "seek": 681356, "start": 6820.96, "end": 6825.240000000001, "text": " Well, what we've done wrong is these are all out of order.", "tokens": [50734, 1042, 11, 437, 321, 600, 1096, 2085, 307, 613, 366, 439, 484, 295, 1668, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1656, "seek": 681356, "start": 6825.240000000001, "end": 6828.88, "text": " In fact, what happens is when you do this, I don't have evidence to support this, but", "tokens": [50948, 682, 1186, 11, 437, 2314, 307, 562, 291, 360, 341, 11, 286, 500, 380, 362, 4467, 281, 1406, 341, 11, 457, 51130], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1657, "seek": 681356, "start": 6828.88, "end": 6831.360000000001, "text": " I believe it's right.", "tokens": [51130, 286, 1697, 309, 311, 558, 13, 51254], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1658, "seek": 681356, "start": 6831.360000000001, "end": 6836.200000000001, "text": " What will always happen is the, the greedy longest will result in all of your tokens", "tokens": [51254, 708, 486, 1009, 1051, 307, 264, 11, 264, 28228, 15438, 486, 1874, 294, 439, 295, 428, 22667, 51496], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1659, "seek": 681356, "start": 6836.200000000001, "end": 6841.360000000001, "text": " being organized or all your matches being organized from longest to shortest.", "tokens": [51496, 885, 9983, 420, 439, 428, 10676, 885, 9983, 490, 15438, 281, 31875, 13, 51754], "temperature": 0.0, "avg_logprob": -0.1442199338946426, "compression_ratio": 1.72265625, "no_speech_prob": 0.13657353818416595}, {"id": 1660, "seek": 684136, "start": 6841.36, "end": 6846.32, "text": " So if we were to scroll down the list and look at maybe negative one, negative, let's", "tokens": [50364, 407, 498, 321, 645, 281, 11369, 760, 264, 1329, 293, 574, 412, 1310, 3671, 472, 11, 3671, 11, 718, 311, 50612], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1661, "seek": 684136, "start": 6846.32, "end": 6850.799999999999, "text": " do negative 10 on, you'll see single word tokens.", "tokens": [50612, 360, 3671, 1266, 322, 11, 291, 603, 536, 2167, 1349, 22667, 13, 50836], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1662, "seek": 684136, "start": 6850.799999999999, "end": 6854.2, "text": " And again, this is me just guessing, but I think based on what you've just seen, that's", "tokens": [50836, 400, 797, 11, 341, 307, 385, 445, 17939, 11, 457, 286, 519, 2361, 322, 437, 291, 600, 445, 1612, 11, 300, 311, 51006], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1663, "seek": 684136, "start": 6854.2, "end": 6856.08, "text": " a fairly good guess.", "tokens": [51006, 257, 6457, 665, 2041, 13, 51100], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1664, "seek": 684136, "start": 6856.08, "end": 6859.08, "text": " So let's go ahead and just kind of so we can see what the output is here.", "tokens": [51100, 407, 718, 311, 352, 2286, 293, 445, 733, 295, 370, 321, 393, 536, 437, 264, 5598, 307, 510, 13, 51250], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1665, "seek": 684136, "start": 6859.08, "end": 6862.719999999999, "text": " So how would you go about organizing these sequentially?", "tokens": [51250, 407, 577, 576, 291, 352, 466, 17608, 613, 5123, 3137, 30, 51432], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1666, "seek": 684136, "start": 6862.719999999999, "end": 6869.839999999999, "text": " Well, this is where really kind of a sort comes in handy when you can pass a lambda to", "tokens": [51432, 1042, 11, 341, 307, 689, 534, 733, 295, 257, 1333, 1487, 294, 13239, 562, 291, 393, 1320, 257, 13607, 281, 51788], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1667, "seek": 684136, "start": 6869.839999999999, "end": 6870.839999999999, "text": " it.", "tokens": [51788, 309, 13, 51838], "temperature": 0.0, "avg_logprob": -0.10587759535442028, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.07584638893604279}, {"id": 1668, "seek": 687084, "start": 6870.84, "end": 6875.24, "text": " I can copy all this again, because again, we almost had this right.", "tokens": [50364, 286, 393, 5055, 439, 341, 797, 11, 570, 797, 11, 321, 1920, 632, 341, 558, 13, 50584], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1669, "seek": 687084, "start": 6875.24, "end": 6881.16, "text": " Here we're going to sort our matches though, we can say matches.sort, and this is going", "tokens": [50584, 1692, 321, 434, 516, 281, 1333, 527, 10676, 1673, 11, 321, 393, 584, 10676, 13, 82, 477, 11, 293, 341, 307, 516, 50880], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1670, "seek": 687084, "start": 6881.16, "end": 6886.6, "text": " to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb", "tokens": [50880, 281, 747, 257, 20428, 6770, 295, 2141, 11, 597, 307, 516, 281, 312, 2681, 281, 10097, 11, 43763, 11, 293, 10097, 51152], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1671, "seek": 687084, "start": 6886.6, "end": 6893.6, "text": " is going to allow us to actually iterate over all this and find any instance where X occurs.", "tokens": [51152, 307, 516, 281, 2089, 505, 281, 767, 44497, 670, 439, 341, 293, 915, 604, 5197, 689, 1783, 11843, 13, 51502], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1672, "seek": 687084, "start": 6893.6, "end": 6896.32, "text": " And we're going to say to sort by X one.", "tokens": [51502, 400, 321, 434, 516, 281, 584, 281, 1333, 538, 1783, 472, 13, 51638], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1673, "seek": 687084, "start": 6896.32, "end": 6898.52, "text": " So what this is, it's a list of tuples.", "tokens": [51638, 407, 437, 341, 307, 11, 309, 311, 257, 1329, 295, 2604, 2622, 13, 51748], "temperature": 0.0, "avg_logprob": -0.18650222843528813, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.03963268920779228}, {"id": 1674, "seek": 689852, "start": 6898.52, "end": 6902.76, "text": " And what we're using lambda for is we're going to say sort this whole list of tuples out,", "tokens": [50364, 400, 437, 321, 434, 1228, 13607, 337, 307, 321, 434, 516, 281, 584, 1333, 341, 1379, 1329, 295, 2604, 2622, 484, 11, 50576], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1675, "seek": 689852, "start": 6902.76, "end": 6907.52, "text": " but sort it by the first index, in other words, sort it by the start token.", "tokens": [50576, 457, 1333, 309, 538, 264, 700, 8186, 11, 294, 661, 2283, 11, 1333, 309, 538, 264, 722, 14862, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1676, "seek": 689852, "start": 6907.52, "end": 6912.52, "text": " And when we execute that, we've got everything now coming out as we would expect and nor", "tokens": [50814, 400, 562, 321, 14483, 300, 11, 321, 600, 658, 1203, 586, 1348, 484, 382, 321, 576, 2066, 293, 6051, 51064], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1677, "seek": 689852, "start": 6912.52, "end": 6914.6, "text": " these typos that exist.", "tokens": [51064, 613, 2125, 329, 300, 2514, 13, 51168], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1678, "seek": 689852, "start": 6914.6, "end": 6917.88, "text": " We've got zero to four, six to nine.", "tokens": [51168, 492, 600, 658, 4018, 281, 1451, 11, 2309, 281, 4949, 13, 51332], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1679, "seek": 689852, "start": 6917.88, "end": 6923.040000000001, "text": " So we actually are extracting these things in sequential order as they appear in our", "tokens": [51332, 407, 321, 767, 366, 49844, 613, 721, 294, 42881, 1668, 382, 436, 4204, 294, 527, 51590], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1680, "seek": 689852, "start": 6923.040000000001, "end": 6924.040000000001, "text": " text.", "tokens": [51590, 2487, 13, 51640], "temperature": 0.0, "avg_logprob": -0.15329354026100853, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.00941214058548212}, {"id": 1681, "seek": 692404, "start": 6924.04, "end": 6930.0, "text": " You can actually go through and sort the appearance of the, of the matcher.", "tokens": [50364, 509, 393, 767, 352, 807, 293, 1333, 264, 8967, 295, 264, 11, 295, 264, 2995, 260, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1682, "seek": 692404, "start": 6930.0, "end": 6935.6, "text": " But what if our, the person who kind of gave us this job, they were happy with this, but", "tokens": [50662, 583, 437, 498, 527, 11, 264, 954, 567, 733, 295, 2729, 505, 341, 1691, 11, 436, 645, 2055, 365, 341, 11, 457, 50942], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1683, "seek": 692404, "start": 6935.6, "end": 6937.16, "text": " they came back and said, okay, that's cool.", "tokens": [50942, 436, 1361, 646, 293, 848, 11, 1392, 11, 300, 311, 1627, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1684, "seek": 692404, "start": 6937.16, "end": 6941.44, "text": " But what we're really interested in what we really want to know is every instance where", "tokens": [51020, 583, 437, 321, 434, 534, 3102, 294, 437, 321, 534, 528, 281, 458, 307, 633, 5197, 689, 51234], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1685, "seek": 692404, "start": 6941.44, "end": 6948.5199999999995, "text": " a proper noun of any length, grab the multi word token still, but we want to know anytime", "tokens": [51234, 257, 2296, 23307, 295, 604, 4641, 11, 4444, 264, 4825, 1349, 14862, 920, 11, 457, 321, 528, 281, 458, 13038, 51588], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1686, "seek": 692404, "start": 6948.5199999999995, "end": 6950.56, "text": " that occurs after a verb.", "tokens": [51588, 300, 11843, 934, 257, 9595, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1687, "seek": 692404, "start": 6950.56, "end": 6953.6, "text": " So anytime this proper noun is followed by a verb.", "tokens": [51690, 407, 13038, 341, 2296, 23307, 307, 6263, 538, 257, 9595, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1281577417673158, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.18705786764621735}, {"id": 1688, "seek": 695360, "start": 6953.6, "end": 6956.4800000000005, "text": " So what we can do is we can add in, okay, okay, we can do this.", "tokens": [50364, 407, 437, 321, 393, 360, 307, 321, 393, 909, 294, 11, 1392, 11, 1392, 11, 321, 393, 360, 341, 13, 50508], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1689, "seek": 695360, "start": 6956.4800000000005, "end": 6957.8, "text": " We're going to have a comma here.", "tokens": [50508, 492, 434, 516, 281, 362, 257, 22117, 510, 13, 50574], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1690, "seek": 695360, "start": 6957.8, "end": 6960.52, "text": " So the same pattern is going to be a sequence now.", "tokens": [50574, 407, 264, 912, 5102, 307, 516, 281, 312, 257, 8310, 586, 13, 50710], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1691, "seek": 695360, "start": 6960.52, "end": 6962.76, "text": " It's not just going to be one thing.", "tokens": [50710, 467, 311, 406, 445, 516, 281, 312, 472, 551, 13, 50822], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1692, "seek": 695360, "start": 6962.76, "end": 6968.04, "text": " We're going to say token one needs to be a proper noun and grab as many of those tokens", "tokens": [50822, 492, 434, 516, 281, 584, 14862, 472, 2203, 281, 312, 257, 2296, 23307, 293, 4444, 382, 867, 295, 729, 22667, 51086], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1693, "seek": 695360, "start": 6968.04, "end": 6971.72, "text": " as you can zero or one to more times.", "tokens": [51086, 382, 291, 393, 4018, 420, 472, 281, 544, 1413, 13, 51270], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1694, "seek": 695360, "start": 6971.72, "end": 6977.360000000001, "text": " And then after those are done comma, this is where the next thing has to occur POS.", "tokens": [51270, 400, 550, 934, 729, 366, 1096, 22117, 11, 341, 307, 689, 264, 958, 551, 575, 281, 5160, 430, 4367, 13, 51552], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1695, "seek": 695360, "start": 6977.360000000001, "end": 6980.4400000000005, "text": " So the part of speech needs to be a verb.", "tokens": [51552, 407, 264, 644, 295, 6218, 2203, 281, 312, 257, 9595, 13, 51706], "temperature": 0.0, "avg_logprob": -0.11785560754629282, "compression_ratio": 1.790983606557377, "no_speech_prob": 0.00831510592252016}, {"id": 1696, "seek": 698044, "start": 6980.44, "end": 6984.0, "text": " So the next thing that comes out needs to be a verb.", "tokens": [50364, 407, 264, 958, 551, 300, 1487, 484, 2203, 281, 312, 257, 9595, 13, 50542], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1697, "seek": 698044, "start": 6984.0, "end": 6985.679999999999, "text": " And we want that to be the case.", "tokens": [50542, 400, 321, 528, 300, 281, 312, 264, 1389, 13, 50626], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1698, "seek": 698044, "start": 6985.679999999999, "end": 6989.839999999999, "text": " Well, when we do this, we can kind of go through and see the results of the first instance", "tokens": [50626, 1042, 11, 562, 321, 360, 341, 11, 321, 393, 733, 295, 352, 807, 293, 536, 264, 3542, 295, 264, 700, 5197, 50834], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1699, "seek": 698044, "start": 6989.839999999999, "end": 6998.04, "text": " of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced", "tokens": [50834, 295, 341, 11, 689, 257, 2296, 23307, 307, 39053, 538, 257, 9595, 1487, 294, 14862, 2625, 281, 18079, 3819, 7339, 51244], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1700, "seek": 698044, "start": 6998.04, "end": 7002.28, "text": " 258 director J Edgar Hoover considered.", "tokens": [51244, 3552, 23, 5391, 508, 42981, 46382, 4888, 13, 51456], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1701, "seek": 698044, "start": 7002.28, "end": 7009.0, "text": " Now we're able to use those linguistic features that make Spacey amazing and actually extract", "tokens": [51456, 823, 321, 434, 1075, 281, 764, 729, 43002, 4122, 300, 652, 8705, 88, 2243, 293, 767, 8947, 51792], "temperature": 0.0, "avg_logprob": -0.19725522808000154, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0026316281873732805}, {"id": 1702, "seek": 700900, "start": 7009.0, "end": 7011.16, "text": " some vital information.", "tokens": [50364, 512, 11707, 1589, 13, 50472], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1703, "seek": 700900, "start": 7011.16, "end": 7017.84, "text": " So we've been able to figure out where in this text a a proper noun is proceeded by", "tokens": [50472, 407, 321, 600, 668, 1075, 281, 2573, 484, 689, 294, 341, 2487, 257, 257, 2296, 23307, 307, 39053, 538, 50806], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1704, "seek": 700900, "start": 7017.84, "end": 7018.84, "text": " a verb.", "tokens": [50806, 257, 9595, 13, 50856], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1705, "seek": 700900, "start": 7018.84, "end": 7022.48, "text": " So you can already start to probably see the implications here.", "tokens": [50856, 407, 291, 393, 1217, 722, 281, 1391, 536, 264, 16602, 510, 13, 51038], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1706, "seek": 700900, "start": 7022.48, "end": 7026.12, "text": " And we can we can create very elaborate things with this.", "tokens": [51038, 400, 321, 393, 321, 393, 1884, 588, 20945, 721, 365, 341, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1707, "seek": 700900, "start": 7026.12, "end": 7029.88, "text": " We can use any of these as long of a sequence as you can imagine.", "tokens": [51220, 492, 393, 764, 604, 295, 613, 382, 938, 295, 257, 8310, 382, 291, 393, 3811, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1708, "seek": 700900, "start": 7029.88, "end": 7033.64, "text": " We're going to work with a different text and kind of demonstrate that it's a fun toy", "tokens": [51408, 492, 434, 516, 281, 589, 365, 257, 819, 2487, 293, 733, 295, 11698, 300, 309, 311, 257, 1019, 12058, 51596], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1709, "seek": 700900, "start": 7033.64, "end": 7034.64, "text": " example.", "tokens": [51596, 1365, 13, 51646], "temperature": 0.0, "avg_logprob": -0.14669193086170015, "compression_ratio": 1.6244897959183673, "no_speech_prob": 0.1823679506778717}, {"id": 1710, "seek": 703464, "start": 7034.64, "end": 7040.240000000001, "text": " We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file.", "tokens": [50364, 492, 600, 658, 257, 15461, 16146, 5055, 295, 16004, 294, 13224, 1661, 12187, 382, 257, 11181, 3991, 13, 50644], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1711, "seek": 703464, "start": 7040.240000000001, "end": 7043.160000000001, "text": " I'm going to load it in right now.", "tokens": [50644, 286, 478, 516, 281, 3677, 309, 294, 558, 586, 13, 50790], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1712, "seek": 703464, "start": 7043.160000000001, "end": 7049.400000000001, "text": " And then I'm going to just grab the first sentence from the first chapter.", "tokens": [50790, 400, 550, 286, 478, 516, 281, 445, 4444, 264, 700, 8174, 490, 264, 700, 7187, 13, 51102], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1713, "seek": 703464, "start": 7049.400000000001, "end": 7052.0, "text": " And what we have here is the first sentence.", "tokens": [51102, 400, 437, 321, 362, 510, 307, 264, 700, 8174, 13, 51232], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1714, "seek": 703464, "start": 7052.0, "end": 7053.8, "text": " So here's our scenario.", "tokens": [51232, 407, 510, 311, 527, 9005, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1715, "seek": 703464, "start": 7053.8, "end": 7061.320000000001, "text": " Somebody has asked us to grab all the quotation marks and try to identify the person described", "tokens": [51322, 13463, 575, 2351, 505, 281, 4444, 439, 264, 47312, 10640, 293, 853, 281, 5876, 264, 954, 7619, 51698], "temperature": 0.0, "avg_logprob": -0.14882343417995578, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14410126209259033}, {"id": 1716, "seek": 706132, "start": 7061.32, "end": 7065.32, "text": " or the person described the person who's doing the speaking or the thinking.", "tokens": [50364, 420, 264, 954, 7619, 264, 954, 567, 311, 884, 264, 4124, 420, 264, 1953, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1717, "seek": 706132, "start": 7065.32, "end": 7068.5599999999995, "text": " In other words, we want to be able to grab Alice thought.", "tokens": [50564, 682, 661, 2283, 11, 321, 528, 281, 312, 1075, 281, 4444, 16004, 1194, 13, 50726], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1718, "seek": 706132, "start": 7068.5599999999995, "end": 7074.12, "text": " Now I picked Alice in Wonderland because of the complexity of the text, not complexity", "tokens": [50726, 823, 286, 6183, 16004, 294, 13224, 1661, 570, 295, 264, 14024, 295, 264, 2487, 11, 406, 14024, 51004], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1719, "seek": 706132, "start": 7074.12, "end": 7079.679999999999, "text": " in the sense of the language used children's book, but complexity and the syntax.", "tokens": [51004, 294, 264, 2020, 295, 264, 2856, 1143, 2227, 311, 1446, 11, 457, 14024, 293, 264, 28431, 13, 51282], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1720, "seek": 706132, "start": 7079.679999999999, "end": 7085.719999999999, "text": " These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly", "tokens": [51282, 1981, 28431, 279, 5405, 36891, 9460, 293, 406, 9460, 17412, 11, 7925, 17412, 383, 7925, 390, 5405, 51584], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1721, "seek": 706132, "start": 7085.719999999999, "end": 7090.04, "text": " inconsistent in how we structured these kind of sequences of quotes.", "tokens": [51584, 36891, 294, 577, 321, 18519, 613, 733, 295, 22978, 295, 19963, 13, 51800], "temperature": 0.0, "avg_logprob": -0.20773400000806125, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.29401257634162903}, {"id": 1722, "seek": 709004, "start": 7090.04, "end": 7094.68, "text": " And the other thing I chose to do as I left in one mistake here, and that is this non", "tokens": [50364, 400, 264, 661, 551, 286, 5111, 281, 360, 382, 286, 1411, 294, 472, 6146, 510, 11, 293, 300, 307, 341, 2107, 50596], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1723, "seek": 709004, "start": 7094.68, "end": 7096.68, "text": " standardized quotation mark.", "tokens": [50596, 31677, 47312, 1491, 13, 50696], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1724, "seek": 709004, "start": 7096.68, "end": 7099.82, "text": " So remember, when you need to do this, things need to match perfectly.", "tokens": [50696, 407, 1604, 11, 562, 291, 643, 281, 360, 341, 11, 721, 643, 281, 2995, 6239, 13, 50853], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1725, "seek": 709004, "start": 7099.82, "end": 7104.36, "text": " So we're going to replace this first things first is to create a cleaner text, or we do", "tokens": [50853, 407, 321, 434, 516, 281, 7406, 341, 700, 721, 700, 307, 281, 1884, 257, 16532, 2487, 11, 420, 321, 360, 51080], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1726, "seek": 709004, "start": 7104.36, "end": 7109.16, "text": " text equals text dot replace, and we're going to replace the instance of I believe it's", "tokens": [51080, 2487, 6915, 2487, 5893, 7406, 11, 293, 321, 434, 516, 281, 7406, 264, 5197, 295, 286, 1697, 309, 311, 51320], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1727, "seek": 709004, "start": 7109.16, "end": 7113.4, "text": " that mark, but let's just copy and paste it in to make sure we're going to replace that", "tokens": [51320, 300, 1491, 11, 457, 718, 311, 445, 5055, 293, 9163, 309, 294, 281, 652, 988, 321, 434, 516, 281, 7406, 300, 51532], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1728, "seek": 709004, "start": 7113.4, "end": 7119.24, "text": " with a, with a single quotation mark, and we can print off text just to make sure that", "tokens": [51532, 365, 257, 11, 365, 257, 2167, 47312, 1491, 11, 293, 321, 393, 4482, 766, 2487, 445, 281, 652, 988, 300, 51824], "temperature": 0.0, "avg_logprob": -0.15876502712277601, "compression_ratio": 1.9420289855072463, "no_speech_prob": 0.05499683693051338}, {"id": 1729, "seek": 711924, "start": 7119.4, "end": 7120.4, "text": " was done correctly.", "tokens": [50372, 390, 1096, 8944, 13, 50422], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1730, "seek": 711924, "start": 7120.4, "end": 7121.4, "text": " Cool.", "tokens": [50422, 8561, 13, 50472], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1731, "seek": 711924, "start": 7121.4, "end": 7122.4, "text": " Great.", "tokens": [50472, 3769, 13, 50522], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1732, "seek": 711924, "start": 7122.4, "end": 7123.4, "text": " It was it's now looking good.", "tokens": [50522, 467, 390, 309, 311, 586, 1237, 665, 13, 50572], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1733, "seek": 711924, "start": 7123.4, "end": 7127.88, "text": " Remember, whenever you're doing information extraction, standardize the texts as much as", "tokens": [50572, 5459, 11, 5699, 291, 434, 884, 1589, 30197, 11, 3832, 1125, 264, 15765, 382, 709, 382, 50796], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1734, "seek": 711924, "start": 7127.88, "end": 7129.08, "text": " possible.", "tokens": [50796, 1944, 13, 50856], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1735, "seek": 711924, "start": 7129.08, "end": 7133.719999999999, "text": " Things like quotation marks will always throw off your data.", "tokens": [50856, 9514, 411, 47312, 10640, 486, 1009, 3507, 766, 428, 1412, 13, 51088], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1736, "seek": 711924, "start": 7133.719999999999, "end": 7140.48, "text": " Now that we've got that, let's go ahead and start trying to create a fairly robust pattern", "tokens": [51088, 823, 300, 321, 600, 658, 300, 11, 718, 311, 352, 2286, 293, 722, 1382, 281, 1884, 257, 6457, 13956, 5102, 51426], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1737, "seek": 711924, "start": 7140.48, "end": 7147.639999999999, "text": " to try to grab all instances where there is a quotation mark, thought, something like", "tokens": [51426, 281, 853, 281, 4444, 439, 14519, 689, 456, 307, 257, 47312, 1491, 11, 1194, 11, 746, 411, 51784], "temperature": 0.0, "avg_logprob": -0.18905490052466298, "compression_ratio": 1.55859375, "no_speech_prob": 0.002889431780204177}, {"id": 1738, "seek": 714764, "start": 7147.72, "end": 7151.4800000000005, "text": " this, and then followed by another quotation mark.", "tokens": [50368, 341, 11, 293, 550, 6263, 538, 1071, 47312, 1491, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1739, "seek": 714764, "start": 7151.4800000000005, "end": 7155.76, "text": " So the first thing I'm going to try and do is I'm going to try to just capture all quotation", "tokens": [50556, 407, 264, 700, 551, 286, 478, 516, 281, 853, 293, 360, 307, 286, 478, 516, 281, 853, 281, 445, 7983, 439, 47312, 50770], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1740, "seek": 714764, "start": 7155.76, "end": 7157.4400000000005, "text": " marks and a text.", "tokens": [50770, 10640, 293, 257, 2487, 13, 50854], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1741, "seek": 714764, "start": 7157.4400000000005, "end": 7161.52, "text": " So let's go through and try to figure out how to do that right now.", "tokens": [50854, 407, 718, 311, 352, 807, 293, 853, 281, 2573, 484, 577, 281, 360, 300, 558, 586, 13, 51058], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1742, "seek": 714764, "start": 7161.52, "end": 7165.4800000000005, "text": " So we're going to copy in a lot of the same things that we used up above, but we're going", "tokens": [51058, 407, 321, 434, 516, 281, 5055, 294, 257, 688, 295, 264, 912, 721, 300, 321, 1143, 493, 3673, 11, 457, 321, 434, 516, 51256], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1743, "seek": 714764, "start": 7165.4800000000005, "end": 7167.04, "text": " to make some modifications to it.", "tokens": [51256, 281, 652, 512, 26881, 281, 309, 13, 51334], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1744, "seek": 714764, "start": 7167.04, "end": 7172.64, "text": " Let's go ahead and copy and paste all that we're going to completely change our pattern.", "tokens": [51334, 961, 311, 352, 2286, 293, 5055, 293, 9163, 439, 300, 321, 434, 516, 281, 2584, 1319, 527, 5102, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1745, "seek": 714764, "start": 7172.64, "end": 7174.04, "text": " So let's get rid of this.", "tokens": [51614, 407, 718, 311, 483, 3973, 295, 341, 13, 51684], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1746, "seek": 714764, "start": 7174.04, "end": 7175.400000000001, "text": " So what are we looking for?", "tokens": [51684, 407, 437, 366, 321, 1237, 337, 30, 51752], "temperature": 0.0, "avg_logprob": -0.12386500317117442, "compression_ratio": 1.8716981132075472, "no_speech_prob": 0.015904085710644722}, {"id": 1747, "seek": 717540, "start": 7175.4, "end": 7181.24, "text": " Well, first of all, the first thing that's going to occur in this pattern is this quotation", "tokens": [50364, 1042, 11, 700, 295, 439, 11, 264, 700, 551, 300, 311, 516, 281, 5160, 294, 341, 5102, 307, 341, 47312, 50656], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1748, "seek": 717540, "start": 7181.24, "end": 7182.08, "text": " mark.", "tokens": [50656, 1491, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1749, "seek": 717540, "start": 7182.08, "end": 7187.32, "text": " So that's going to be a full text match, which is an or if you remember, and we're going", "tokens": [50698, 407, 300, 311, 516, 281, 312, 257, 1577, 2487, 2995, 11, 597, 307, 364, 420, 498, 291, 1604, 11, 293, 321, 434, 516, 50960], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1750, "seek": 717540, "start": 7187.32, "end": 7191.599999999999, "text": " to have to use double quotation marks to add in that single quotation mark.", "tokens": [50960, 281, 362, 281, 764, 3834, 47312, 10640, 281, 909, 294, 300, 2167, 47312, 1491, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1751, "seek": 717540, "start": 7191.599999999999, "end": 7192.719999999999, "text": " So that's what we grabbed first.", "tokens": [51174, 407, 300, 311, 437, 321, 18607, 700, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1752, "seek": 717540, "start": 7192.719999999999, "end": 7197.16, "text": " We're going to look for anything that is an or and the next thing that's going to occur", "tokens": [51230, 492, 434, 516, 281, 574, 337, 1340, 300, 307, 364, 420, 293, 264, 958, 551, 300, 311, 516, 281, 5160, 51452], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1753, "seek": 717540, "start": 7197.16, "end": 7202.5199999999995, "text": " after that, I think this is good to probably do this now on a line by line basis.", "tokens": [51452, 934, 300, 11, 286, 519, 341, 307, 665, 281, 1391, 360, 341, 586, 322, 257, 1622, 538, 1622, 5143, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1754, "seek": 717540, "start": 7202.5199999999995, "end": 7205.12, "text": " So we can keep this straight.", "tokens": [51720, 407, 321, 393, 1066, 341, 2997, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1297241727212318, "compression_ratio": 1.98, "no_speech_prob": 0.0007096569170244038}, {"id": 1755, "seek": 720512, "start": 7205.12, "end": 7208.4, "text": " So the next thing that's going to occur is we're looking for anything in between.", "tokens": [50364, 407, 264, 958, 551, 300, 311, 516, 281, 5160, 307, 321, 434, 1237, 337, 1340, 294, 1296, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1756, "seek": 720512, "start": 7208.4, "end": 7214.24, "text": " So anything that is an alpha character, we're going to just grab it all.", "tokens": [50528, 407, 1340, 300, 307, 364, 8961, 2517, 11, 321, 434, 516, 281, 445, 4444, 309, 439, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1757, "seek": 720512, "start": 7214.24, "end": 7222.16, "text": " So is alpha and then we need to say true.", "tokens": [50820, 407, 307, 8961, 293, 550, 321, 643, 281, 584, 2074, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1758, "seek": 720512, "start": 7222.16, "end": 7226.96, "text": " But within this, we need to specify how many times that occurs because if we say is true,", "tokens": [51216, 583, 1951, 341, 11, 321, 643, 281, 16500, 577, 867, 1413, 300, 11843, 570, 498, 321, 584, 307, 2074, 11, 51456], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1759, "seek": 720512, "start": 7226.96, "end": 7232.599999999999, "text": " it's just going to look at the next token in this case and and then say that's the end.", "tokens": [51456, 309, 311, 445, 516, 281, 574, 412, 264, 958, 14862, 294, 341, 1389, 293, 293, 550, 584, 300, 311, 264, 917, 13, 51738], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1760, "seek": 720512, "start": 7232.599999999999, "end": 7233.599999999999, "text": " That's it.", "tokens": [51738, 663, 311, 309, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1761, "seek": 720512, "start": 7233.599999999999, "end": 7234.599999999999, "text": " That's the pattern.", "tokens": [51788, 663, 311, 264, 5102, 13, 51838], "temperature": 0.0, "avg_logprob": -0.1361967694442884, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.0005527705070562661}, {"id": 1762, "seek": 723460, "start": 7235.08, "end": 7240.88, "text": " But we want to grab not just and but and what is the use of a everything.", "tokens": [50388, 583, 321, 528, 281, 4444, 406, 445, 293, 457, 293, 437, 307, 264, 764, 295, 257, 1203, 13, 50678], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1763, "seek": 723460, "start": 7240.88, "end": 7247.280000000001, "text": " So we need to grab not only that, but when you say OP, so our operator again.", "tokens": [50678, 407, 321, 643, 281, 4444, 406, 787, 300, 11, 457, 562, 291, 584, 23324, 11, 370, 527, 12973, 797, 13, 50998], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1764, "seek": 723460, "start": 7247.280000000001, "end": 7250.280000000001, "text": " And if you said plus, you would be right here.", "tokens": [50998, 400, 498, 291, 848, 1804, 11, 291, 576, 312, 558, 510, 13, 51148], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1765, "seek": 723460, "start": 7250.280000000001, "end": 7253.96, "text": " We need to make sure that it's a plus sign, so it's grabbing everything.", "tokens": [51148, 492, 643, 281, 652, 988, 300, 309, 311, 257, 1804, 1465, 11, 370, 309, 311, 23771, 1203, 13, 51332], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1766, "seek": 723460, "start": 7253.96, "end": 7259.72, "text": " Now in this scenario, this is a common construct is when you have a injection here in the middle", "tokens": [51332, 823, 294, 341, 9005, 11, 341, 307, 257, 2689, 7690, 307, 562, 291, 362, 257, 22873, 510, 294, 264, 2808, 51620], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1767, "seek": 723460, "start": 7259.72, "end": 7260.72, "text": " of the sentence.", "tokens": [51620, 295, 264, 8174, 13, 51670], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1768, "seek": 723460, "start": 7260.72, "end": 7263.18, "text": " So thought or said, and it's the character doing it.", "tokens": [51670, 407, 1194, 420, 848, 11, 293, 309, 311, 264, 2517, 884, 309, 13, 51793], "temperature": 0.0, "avg_logprob": -0.17546505573367285, "compression_ratio": 1.7109375, "no_speech_prob": 0.025946559384465218}, {"id": 1769, "seek": 726318, "start": 7263.18, "end": 7266.02, "text": " This oftentimes got a comma right here.", "tokens": [50364, 639, 18349, 658, 257, 22117, 558, 510, 13, 50506], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1770, "seek": 726318, "start": 7266.02, "end": 7268.58, "text": " So we need to add in that kind of a feature.", "tokens": [50506, 407, 321, 643, 281, 909, 294, 300, 733, 295, 257, 4111, 13, 50634], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1771, "seek": 726318, "start": 7268.58, "end": 7272.38, "text": " So there could be is punked.", "tokens": [50634, 407, 456, 727, 312, 307, 25188, 292, 13, 50824], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1772, "seek": 726318, "start": 7272.38, "end": 7275.22, "text": " There could be a punked here.", "tokens": [50824, 821, 727, 312, 257, 25188, 292, 510, 13, 50966], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1773, "seek": 726318, "start": 7275.22, "end": 7278.76, "text": " And we're going to say that that is equal to true.", "tokens": [50966, 400, 321, 434, 516, 281, 584, 300, 300, 307, 2681, 281, 2074, 13, 51143], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1774, "seek": 726318, "start": 7278.76, "end": 7281.3, "text": " But that might not always be the case.", "tokens": [51143, 583, 300, 1062, 406, 1009, 312, 264, 1389, 13, 51270], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1775, "seek": 726318, "start": 7281.3, "end": 7283.18, "text": " There might not always be one there.", "tokens": [51270, 821, 1062, 406, 1009, 312, 472, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1776, "seek": 726318, "start": 7283.18, "end": 7288.3, "text": " So we're going to say OP is equal to a star.", "tokens": [51364, 407, 321, 434, 516, 281, 584, 23324, 307, 2681, 281, 257, 3543, 13, 51620], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1777, "seek": 726318, "start": 7288.3, "end": 7289.3, "text": " We go back.", "tokens": [51620, 492, 352, 646, 13, 51670], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1778, "seek": 726318, "start": 7289.3, "end": 7290.3, "text": " We'll see why.", "tokens": [51670, 492, 603, 536, 983, 13, 51720], "temperature": 0.0, "avg_logprob": -0.16475214524702592, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0029808958061039448}, {"id": 1779, "seek": 729030, "start": 7290.3, "end": 7295.3, "text": " To our OP, the star allow the pattern to match zero or more time.", "tokens": [50364, 1407, 527, 23324, 11, 264, 3543, 2089, 264, 5102, 281, 2995, 4018, 420, 544, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1780, "seek": 729030, "start": 7295.3, "end": 7300.12, "text": " So in this scenario, the punctuation may or may not be there.", "tokens": [50614, 407, 294, 341, 9005, 11, 264, 27006, 16073, 815, 420, 815, 406, 312, 456, 13, 50855], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1781, "seek": 729030, "start": 7300.12, "end": 7301.860000000001, "text": " So that's the next thing that occurs.", "tokens": [50855, 407, 300, 311, 264, 958, 551, 300, 11843, 13, 50942], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1782, "seek": 729030, "start": 7301.860000000001, "end": 7306.820000000001, "text": " Once we've got that, the last thing that we need to match is the exact same thing that", "tokens": [50942, 3443, 321, 600, 658, 300, 11, 264, 1036, 551, 300, 321, 643, 281, 2995, 307, 264, 1900, 912, 551, 300, 51190], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1783, "seek": 729030, "start": 7306.820000000001, "end": 7311.9800000000005, "text": " we had at the start is this or appear.", "tokens": [51190, 321, 632, 412, 264, 722, 307, 341, 420, 4204, 13, 51448], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1784, "seek": 729030, "start": 7311.9800000000005, "end": 7313.42, "text": " And that's our sequence.", "tokens": [51448, 400, 300, 311, 527, 8310, 13, 51520], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1785, "seek": 729030, "start": 7313.42, "end": 7317.02, "text": " So this is going to look for anything that starts with a quotation mark has a series", "tokens": [51520, 407, 341, 307, 516, 281, 574, 337, 1340, 300, 3719, 365, 257, 47312, 1491, 575, 257, 2638, 51700], "temperature": 0.0, "avg_logprob": -0.14243074294623978, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.053390711545944214}, {"id": 1786, "seek": 731702, "start": 7317.02, "end": 7323.700000000001, "text": " of alpha characters has a punctuation like a comma possibly, and then closes the quotation", "tokens": [50364, 295, 8961, 4342, 575, 257, 27006, 16073, 411, 257, 22117, 6264, 11, 293, 550, 24157, 264, 47312, 50698], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1787, "seek": 731702, "start": 7323.700000000001, "end": 7324.700000000001, "text": " marks.", "tokens": [50698, 10640, 13, 50748], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1788, "seek": 731702, "start": 7324.700000000001, "end": 7327.580000000001, "text": " If we execute this, we succeeded.", "tokens": [50748, 759, 321, 14483, 341, 11, 321, 20263, 13, 50892], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1789, "seek": 731702, "start": 7327.580000000001, "end": 7328.580000000001, "text": " We got it.", "tokens": [50892, 492, 658, 309, 13, 50942], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1790, "seek": 731702, "start": 7328.580000000001, "end": 7331.580000000001, "text": " We extracted both matches from that first sentence.", "tokens": [50942, 492, 34086, 1293, 10676, 490, 300, 700, 8174, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1791, "seek": 731702, "start": 7331.580000000001, "end": 7333.820000000001, "text": " There are no other quotation marks in there.", "tokens": [51092, 821, 366, 572, 661, 47312, 10640, 294, 456, 13, 51204], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1792, "seek": 731702, "start": 7333.820000000001, "end": 7337.3, "text": " But our task was not just to extract this information.", "tokens": [51204, 583, 527, 5633, 390, 406, 445, 281, 8947, 341, 1589, 13, 51378], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1793, "seek": 731702, "start": 7337.3, "end": 7342.34, "text": " Our task was also to match who is the speaker.", "tokens": [51378, 2621, 5633, 390, 611, 281, 2995, 567, 307, 264, 8145, 13, 51630], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1794, "seek": 731702, "start": 7342.34, "end": 7345.580000000001, "text": " Now we can do this in a few different ways and you're going to see why this is such a", "tokens": [51630, 823, 321, 393, 360, 341, 294, 257, 1326, 819, 2098, 293, 291, 434, 516, 281, 536, 983, 341, 307, 1270, 257, 51792], "temperature": 0.0, "avg_logprob": -0.12666008302143641, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.01168626919388771}, {"id": 1795, "seek": 734558, "start": 7345.58, "end": 7348.42, "text": " complicated problem in just a second.", "tokens": [50364, 6179, 1154, 294, 445, 257, 1150, 13, 50506], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1796, "seek": 734558, "start": 7348.42, "end": 7350.26, "text": " So let's go ahead and do this.", "tokens": [50506, 407, 718, 311, 352, 2286, 293, 360, 341, 13, 50598], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1797, "seek": 734558, "start": 7350.26, "end": 7352.0599999999995, "text": " How can we make this better?", "tokens": [50598, 1012, 393, 321, 652, 341, 1101, 30, 50688], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1798, "seek": 734558, "start": 7352.0599999999995, "end": 7356.18, "text": " Well, we're going to have this occur twice.", "tokens": [50688, 1042, 11, 321, 434, 516, 281, 362, 341, 5160, 6091, 13, 50894], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1799, "seek": 734558, "start": 7356.18, "end": 7360.38, "text": " But in the middle, we need to figure out when somebody is speaking.", "tokens": [50894, 583, 294, 264, 2808, 11, 321, 643, 281, 2573, 484, 562, 2618, 307, 4124, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1800, "seek": 734558, "start": 7360.38, "end": 7363.18, "text": " So one of the things that we can do is we can make a list.", "tokens": [51104, 407, 472, 295, 264, 721, 300, 321, 393, 360, 307, 321, 393, 652, 257, 1329, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1801, "seek": 734558, "start": 7363.18, "end": 7369.24, "text": " So let's make a list of limitized forms of our verbs.", "tokens": [51244, 407, 718, 311, 652, 257, 1329, 295, 4948, 1602, 6422, 295, 527, 30051, 13, 51547], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1802, "seek": 734558, "start": 7369.24, "end": 7373.98, "text": " So we're going to say, let's call this speak underscore limits.", "tokens": [51547, 407, 321, 434, 516, 281, 584, 11, 718, 311, 818, 341, 1710, 37556, 10406, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1565333604812622, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.03845842555165291}, {"id": 1803, "seek": 737398, "start": 7373.98, "end": 7376.0599999999995, "text": " This can be equal to a list.", "tokens": [50364, 639, 393, 312, 2681, 281, 257, 1329, 13, 50468], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1804, "seek": 737398, "start": 7376.0599999999995, "end": 7379.5, "text": " And the first thing we're going to say is think, because we know that think is in there", "tokens": [50468, 400, 264, 700, 551, 321, 434, 516, 281, 584, 307, 519, 11, 570, 321, 458, 300, 519, 307, 294, 456, 50640], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1805, "seek": 737398, "start": 7379.5, "end": 7384.719999999999, "text": " and say this is the limitized form of thought and said.", "tokens": [50640, 293, 584, 341, 307, 264, 4948, 1602, 1254, 295, 1194, 293, 848, 13, 50901], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1806, "seek": 737398, "start": 7384.719999999999, "end": 7388.44, "text": " So what we can do now is after that occurs, it's adding a new thing.", "tokens": [50901, 407, 437, 321, 393, 360, 586, 307, 934, 300, 11843, 11, 309, 311, 5127, 257, 777, 551, 13, 51087], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1807, "seek": 737398, "start": 7388.44, "end": 7392.0599999999995, "text": " We're going to be able to now add in a new pattern that we're looking for.", "tokens": [51087, 492, 434, 516, 281, 312, 1075, 281, 586, 909, 294, 257, 777, 5102, 300, 321, 434, 1237, 337, 13, 51268], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1808, "seek": 737398, "start": 7392.0599999999995, "end": 7397.7, "text": " And so not just the start of a quotation mark, not just the end of a quotation mark, but", "tokens": [51268, 400, 370, 406, 445, 264, 722, 295, 257, 47312, 1491, 11, 406, 445, 264, 917, 295, 257, 47312, 1491, 11, 457, 51550], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1809, "seek": 737398, "start": 7397.7, "end": 7400.98, "text": " also a sequence that'll be something like this.", "tokens": [51550, 611, 257, 8310, 300, 603, 312, 746, 411, 341, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15577870178222655, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.02930864691734314}, {"id": 1810, "seek": 740098, "start": 7400.98, "end": 7402.98, "text": " So it's going to be a part of speech.", "tokens": [50364, 407, 309, 311, 516, 281, 312, 257, 644, 295, 6218, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14096208052201706, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0010649437317624688}, {"id": 1811, "seek": 740098, "start": 7402.98, "end": 7406.379999999999, "text": " So it's going to be a verb that occurs first, right?", "tokens": [50464, 407, 309, 311, 516, 281, 312, 257, 9595, 300, 11843, 700, 11, 558, 30, 50634], "temperature": 0.0, "avg_logprob": -0.14096208052201706, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0010649437317624688}, {"id": 1812, "seek": 740098, "start": 7406.379999999999, "end": 7408.62, "text": " And that's going to be a verb.", "tokens": [50634, 400, 300, 311, 516, 281, 312, 257, 9595, 13, 50746], "temperature": 0.0, "avg_logprob": -0.14096208052201706, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0010649437317624688}, {"id": 1813, "seek": 740098, "start": 7408.62, "end": 7421.179999999999, "text": " But more importantly, it's going to be a lemma that is in what did I call you speak lemmas?", "tokens": [50746, 583, 544, 8906, 11, 309, 311, 516, 281, 312, 257, 7495, 1696, 300, 307, 294, 437, 630, 286, 818, 291, 1710, 7495, 3799, 30, 51374], "temperature": 0.0, "avg_logprob": -0.14096208052201706, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0010649437317624688}, {"id": 1814, "seek": 740098, "start": 7421.179999999999, "end": 7423.74, "text": " So let's break this down.", "tokens": [51374, 407, 718, 311, 1821, 341, 760, 13, 51502], "temperature": 0.0, "avg_logprob": -0.14096208052201706, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0010649437317624688}, {"id": 1815, "seek": 742374, "start": 7423.74, "end": 7428.179999999999, "text": " The next token needs to be a verb.", "tokens": [50364, 440, 958, 14862, 2203, 281, 312, 257, 9595, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1816, "seek": 742374, "start": 7428.179999999999, "end": 7437.46, "text": " And it needs to have a limitized form that is contained within the speak lemmas list.", "tokens": [50586, 400, 309, 2203, 281, 362, 257, 4948, 1602, 1254, 300, 307, 16212, 1951, 264, 1710, 7495, 3799, 1329, 13, 51050], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1817, "seek": 742374, "start": 7437.46, "end": 7441.62, "text": " So if it's got that fantastic, let's execute this and see what happens.", "tokens": [51050, 407, 498, 309, 311, 658, 300, 5456, 11, 718, 311, 14483, 341, 293, 536, 437, 2314, 13, 51258], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1818, "seek": 742374, "start": 7441.62, "end": 7443.62, "text": " We should only have one hit.", "tokens": [51258, 492, 820, 787, 362, 472, 2045, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1819, "seek": 742374, "start": 7443.62, "end": 7444.62, "text": " Cool.", "tokens": [51358, 8561, 13, 51408], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1820, "seek": 742374, "start": 7444.62, "end": 7445.62, "text": " We do.", "tokens": [51408, 492, 360, 13, 51458], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1821, "seek": 742374, "start": 7445.62, "end": 7446.62, "text": " So we've got that first hit.", "tokens": [51458, 407, 321, 600, 658, 300, 700, 2045, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1822, "seek": 742374, "start": 7446.62, "end": 7450.0199999999995, "text": " And the second one hasn't appeared anymore because that second quotation mark wasn't", "tokens": [51508, 400, 264, 1150, 472, 6132, 380, 8516, 3602, 570, 300, 1150, 47312, 1491, 2067, 380, 51678], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1823, "seek": 742374, "start": 7450.0199999999995, "end": 7452.98, "text": " proceeded by a verb.", "tokens": [51678, 39053, 538, 257, 9595, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1396252677554176, "compression_ratio": 1.64, "no_speech_prob": 0.015424049459397793}, {"id": 1824, "seek": 745298, "start": 7452.98, "end": 7456.66, "text": " Let's go ahead and make some modifications that we can improve this a little bit.", "tokens": [50364, 961, 311, 352, 2286, 293, 652, 512, 26881, 300, 321, 393, 3470, 341, 257, 707, 857, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1825, "seek": 745298, "start": 7456.66, "end": 7460.86, "text": " Because we want to know not just what that person's doing.", "tokens": [50548, 1436, 321, 528, 281, 458, 406, 445, 437, 300, 954, 311, 884, 13, 50758], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1826, "seek": 745298, "start": 7460.86, "end": 7462.98, "text": " We also need to know who the speaker is.", "tokens": [50758, 492, 611, 643, 281, 458, 567, 264, 8145, 307, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1827, "seek": 745298, "start": 7462.98, "end": 7464.94, "text": " So let's grab it.", "tokens": [50864, 407, 718, 311, 4444, 309, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1828, "seek": 745298, "start": 7464.94, "end": 7466.78, "text": " Let's figure out who that speaker is.", "tokens": [50962, 961, 311, 2573, 484, 567, 300, 8145, 307, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1829, "seek": 745298, "start": 7466.78, "end": 7468.82, "text": " So we can use part of speech.", "tokens": [51054, 407, 321, 393, 764, 644, 295, 6218, 13, 51156], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1830, "seek": 745298, "start": 7468.82, "end": 7471.0199999999995, "text": " Again, another feature here.", "tokens": [51156, 3764, 11, 1071, 4111, 510, 13, 51266], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1831, "seek": 745298, "start": 7471.0199999999995, "end": 7474.78, "text": " We know that it's going to be a proper noun because oftentimes proper nouns are doing", "tokens": [51266, 492, 458, 300, 309, 311, 516, 281, 312, 257, 2296, 23307, 570, 18349, 2296, 48184, 366, 884, 51454], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1832, "seek": 745298, "start": 7474.78, "end": 7476.299999999999, "text": " the speaking.", "tokens": [51454, 264, 4124, 13, 51530], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1833, "seek": 745298, "start": 7476.299999999999, "end": 7477.66, "text": " Sometimes it might not be.", "tokens": [51530, 4803, 309, 1062, 406, 312, 13, 51598], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1834, "seek": 745298, "start": 7477.66, "end": 7481.799999999999, "text": " Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those", "tokens": [51598, 4803, 309, 1062, 312, 411, 264, 2013, 420, 264, 3237, 3126, 9765, 11, 457, 321, 434, 516, 281, 11200, 729, 51805], "temperature": 0.0, "avg_logprob": -0.14570070170670105, "compression_ratio": 1.812720848056537, "no_speech_prob": 0.08266468346118927}, {"id": 1835, "seek": 748180, "start": 7481.8, "end": 7484.360000000001, "text": " situations for just right now.", "tokens": [50364, 6851, 337, 445, 558, 586, 13, 50492], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1836, "seek": 748180, "start": 7484.360000000001, "end": 7486.0, "text": " So we're looking for a proper noun.", "tokens": [50492, 407, 321, 434, 1237, 337, 257, 2296, 23307, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1837, "seek": 748180, "start": 7486.0, "end": 7491.4800000000005, "text": " But remember proper nouns, as we saw just a second ago, could be multiple tokens.", "tokens": [50574, 583, 1604, 2296, 48184, 11, 382, 321, 1866, 445, 257, 1150, 2057, 11, 727, 312, 3866, 22667, 13, 50848], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1838, "seek": 748180, "start": 7491.4800000000005, "end": 7493.84, "text": " So we're going to say OP plus.", "tokens": [50848, 407, 321, 434, 516, 281, 584, 23324, 1804, 13, 50966], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1839, "seek": 748180, "start": 7493.84, "end": 7495.92, "text": " So it could be a sequence of tokens.", "tokens": [50966, 407, 309, 727, 312, 257, 8310, 295, 22667, 13, 51070], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1840, "seek": 748180, "start": 7495.92, "end": 7497.68, "text": " Let's execute this.", "tokens": [51070, 961, 311, 14483, 341, 13, 51158], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1841, "seek": 748180, "start": 7497.68, "end": 7499.96, "text": " Now we've captured Alice here as well.", "tokens": [51158, 823, 321, 600, 11828, 16004, 510, 382, 731, 13, 51272], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1842, "seek": 748180, "start": 7499.96, "end": 7504.0, "text": " So and is the use and what is the use of a book thought Alice.", "tokens": [51272, 407, 293, 307, 264, 764, 293, 437, 307, 264, 764, 295, 257, 1446, 1194, 16004, 13, 51474], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1843, "seek": 748180, "start": 7504.0, "end": 7508.12, "text": " Now we know who the speaker is, but this is a partial quotation.", "tokens": [51474, 823, 321, 458, 567, 264, 8145, 307, 11, 457, 341, 307, 257, 14641, 47312, 13, 51680], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1844, "seek": 748180, "start": 7508.12, "end": 7509.24, "text": " This is not the whole thing.", "tokens": [51680, 639, 307, 406, 264, 1379, 551, 13, 51736], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1845, "seek": 748180, "start": 7509.24, "end": 7511.16, "text": " We need to grab the other quote.", "tokens": [51736, 492, 643, 281, 4444, 264, 661, 6513, 13, 51832], "temperature": 0.0, "avg_logprob": -0.17107940956398293, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0012448024936020374}, {"id": 1846, "seek": 751116, "start": 7511.16, "end": 7512.96, "text": " Oh, how will we ever do that?", "tokens": [50364, 876, 11, 577, 486, 321, 1562, 360, 300, 30, 50454], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1847, "seek": 751116, "start": 7512.96, "end": 7515.0, "text": " Well, we've already solved that.", "tokens": [50454, 1042, 11, 321, 600, 1217, 13041, 300, 13, 50556], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1848, "seek": 751116, "start": 7515.0, "end": 7521.12, "text": " We can copy and paste all of this that we already have done right down here.", "tokens": [50556, 492, 393, 5055, 293, 9163, 439, 295, 341, 300, 321, 1217, 362, 1096, 558, 760, 510, 13, 50862], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1849, "seek": 751116, "start": 7521.12, "end": 7526.4, "text": " And now we've successfully extracted that entire quote.", "tokens": [50862, 400, 586, 321, 600, 10727, 34086, 300, 2302, 6513, 13, 51126], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1850, "seek": 751116, "start": 7526.4, "end": 7528.639999999999, "text": " So you might be thinking to yourself, yeah, we did it.", "tokens": [51126, 407, 291, 1062, 312, 1953, 281, 1803, 11, 1338, 11, 321, 630, 309, 13, 51238], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1851, "seek": 751116, "start": 7528.639999999999, "end": 7536.599999999999, "text": " We can now extract quotation marks and we can even extract, extract, you know, any instance", "tokens": [51238, 492, 393, 586, 8947, 47312, 10640, 293, 321, 393, 754, 8947, 11, 8947, 11, 291, 458, 11, 604, 5197, 51636], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1852, "seek": 751116, "start": 7536.599999999999, "end": 7539.8, "text": " where there's a quote and somebody speaking.", "tokens": [51636, 689, 456, 311, 257, 6513, 293, 2618, 4124, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1853, "seek": 751116, "start": 7539.8, "end": 7540.8, "text": " Not so fast.", "tokens": [51796, 1726, 370, 2370, 13, 51846], "temperature": 0.0, "avg_logprob": -0.15200189307883935, "compression_ratio": 1.6528925619834711, "no_speech_prob": 0.01590433157980442}, {"id": 1854, "seek": 754080, "start": 7540.8, "end": 7542.360000000001, "text": " We're going to try to iterate over this data.", "tokens": [50364, 492, 434, 516, 281, 853, 281, 44497, 670, 341, 1412, 13, 50442], "temperature": 0.0, "avg_logprob": -0.22927024147727273, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.056633319705724716}, {"id": 1855, "seek": 754080, "start": 7542.360000000001, "end": 7550.04, "text": " So we're going to say for text in data, zero twos, we're going to iterate over the first", "tokens": [50442, 407, 321, 434, 516, 281, 584, 337, 2487, 294, 1412, 11, 4018, 683, 329, 11, 321, 434, 516, 281, 44497, 670, 264, 700, 50826], "temperature": 0.0, "avg_logprob": -0.22927024147727273, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.056633319705724716}, {"id": 1856, "seek": 754080, "start": 7550.04, "end": 7552.400000000001, "text": " chapter.", "tokens": [50826, 7187, 13, 50944], "temperature": 0.0, "avg_logprob": -0.22927024147727273, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.056633319705724716}, {"id": 1857, "seek": 754080, "start": 7552.400000000001, "end": 7559.92, "text": " And we're going to go ahead and let's let's do all of this.", "tokens": [50944, 400, 321, 434, 516, 281, 352, 2286, 293, 718, 311, 718, 311, 360, 439, 295, 341, 13, 51320], "temperature": 0.0, "avg_logprob": -0.22927024147727273, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.056633319705724716}, {"id": 1858, "seek": 754080, "start": 7559.92, "end": 7567.6, "text": " Doc is going to be equal to that sort that out.", "tokens": [51320, 16024, 307, 516, 281, 312, 2681, 281, 300, 1333, 300, 484, 13, 51704], "temperature": 0.0, "avg_logprob": -0.22927024147727273, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.056633319705724716}, {"id": 1859, "seek": 756760, "start": 7567.6, "end": 7573.04, "text": " And then again, we're going to be printing out this information, the same stuff I did", "tokens": [50364, 400, 550, 797, 11, 321, 434, 516, 281, 312, 14699, 484, 341, 1589, 11, 264, 912, 1507, 286, 630, 50636], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1860, "seek": 756760, "start": 7573.04, "end": 7576.360000000001, "text": " before, just now it's going to be iterating over the whole chapter.", "tokens": [50636, 949, 11, 445, 586, 309, 311, 516, 281, 312, 17138, 990, 670, 264, 1379, 7187, 13, 50802], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1861, "seek": 756760, "start": 7576.360000000001, "end": 7583.08, "text": " And if we let this run, we've got a serious, serious problem.", "tokens": [50802, 400, 498, 321, 718, 341, 1190, 11, 321, 600, 658, 257, 3156, 11, 3156, 1154, 13, 51138], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1862, "seek": 756760, "start": 7583.08, "end": 7586.68, "text": " And it doesn't actually grab us anything.", "tokens": [51138, 400, 309, 1177, 380, 767, 4444, 505, 1340, 13, 51318], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1863, "seek": 756760, "start": 7586.68, "end": 7589.200000000001, "text": " Nothing has been grabbed successfully.", "tokens": [51318, 6693, 575, 668, 18607, 10727, 13, 51444], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1864, "seek": 756760, "start": 7589.200000000001, "end": 7594.200000000001, "text": " What is going on?", "tokens": [51444, 708, 307, 516, 322, 30, 51694], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1865, "seek": 756760, "start": 7594.200000000001, "end": 7595.84, "text": " We've got a problem.", "tokens": [51694, 492, 600, 658, 257, 1154, 13, 51776], "temperature": 0.0, "avg_logprob": -0.149392210027223, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03963445499539375}, {"id": 1866, "seek": 759584, "start": 7595.84, "end": 7604.52, "text": " And that problem stems from the fact that our patterns and the problem is that we don't", "tokens": [50364, 400, 300, 1154, 27600, 490, 264, 1186, 300, 527, 8294, 293, 264, 1154, 307, 300, 321, 500, 380, 50798], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1867, "seek": 759584, "start": 7604.52, "end": 7611.28, "text": " have our our text correctly, we're being removing the quotation mark that was the problem up", "tokens": [50798, 362, 527, 527, 2487, 8944, 11, 321, 434, 885, 12720, 264, 47312, 1491, 300, 390, 264, 1154, 493, 51136], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1868, "seek": 759584, "start": 7611.28, "end": 7612.28, "text": " above.", "tokens": [51136, 3673, 13, 51186], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1869, "seek": 759584, "start": 7612.28, "end": 7615.64, "text": " So we're going to add this bit of code in.", "tokens": [51186, 407, 321, 434, 516, 281, 909, 341, 857, 295, 3089, 294, 13, 51354], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1870, "seek": 759584, "start": 7615.64, "end": 7617.64, "text": " And we're going to be able to fix it.", "tokens": [51354, 400, 321, 434, 516, 281, 312, 1075, 281, 3191, 309, 13, 51454], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1871, "seek": 759584, "start": 7617.64, "end": 7620.88, "text": " So now when we execute this, we see that we've only grabbed one match.", "tokens": [51454, 407, 586, 562, 321, 14483, 341, 11, 321, 536, 300, 321, 600, 787, 18607, 472, 2995, 13, 51616], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1872, "seek": 759584, "start": 7620.88, "end": 7623.72, "text": " Now you might be thinking to yourself, there's an issue here and there there is, let's go", "tokens": [51616, 823, 291, 1062, 312, 1953, 281, 1803, 11, 456, 311, 364, 2734, 510, 293, 456, 456, 307, 11, 718, 311, 352, 51758], "temperature": 0.0, "avg_logprob": -0.16801333220108697, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.009124938398599625}, {"id": 1873, "seek": 762372, "start": 7623.72, "end": 7627.0, "text": " ahead and print off the length of matches.", "tokens": [50364, 2286, 293, 4482, 766, 264, 4641, 295, 10676, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1874, "seek": 762372, "start": 7627.0, "end": 7629.72, "text": " And we see that we've only grabbed one match.", "tokens": [50528, 400, 321, 536, 300, 321, 600, 787, 18607, 472, 2995, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1875, "seek": 762372, "start": 7629.72, "end": 7631.0, "text": " And then we haven't grabbed anything else.", "tokens": [50664, 400, 550, 321, 2378, 380, 18607, 1340, 1646, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1876, "seek": 762372, "start": 7631.0, "end": 7632.400000000001, "text": " Well, what's the problem here?", "tokens": [50728, 1042, 11, 437, 311, 264, 1154, 510, 30, 50798], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1877, "seek": 762372, "start": 7632.400000000001, "end": 7636.76, "text": " Are there are there no other instances of quotation marks in the rest of the first chapter?", "tokens": [50798, 2014, 456, 366, 456, 572, 661, 14519, 295, 47312, 10640, 294, 264, 1472, 295, 264, 700, 7187, 30, 51016], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1878, "seek": 762372, "start": 7636.76, "end": 7638.4800000000005, "text": " And the answer is no, there are.", "tokens": [51016, 400, 264, 1867, 307, 572, 11, 456, 366, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1879, "seek": 762372, "start": 7638.4800000000005, "end": 7643.320000000001, "text": " There absolutely are other quotation marks and other paragraphs from the first chapter.", "tokens": [51102, 821, 3122, 366, 661, 47312, 10640, 293, 661, 48910, 490, 264, 700, 7187, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1880, "seek": 762372, "start": 7643.320000000001, "end": 7647.84, "text": " The problem is, is that our pattern is singular.", "tokens": [51344, 440, 1154, 307, 11, 307, 300, 527, 5102, 307, 20010, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1881, "seek": 762372, "start": 7647.84, "end": 7649.280000000001, "text": " It's not multivariate.", "tokens": [51570, 467, 311, 406, 2120, 592, 3504, 473, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1523994869656033, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.09265874326229095}, {"id": 1882, "seek": 764928, "start": 7649.28, "end": 7655.5599999999995, "text": " We need to add in additional ways in which a text might be structured.", "tokens": [50364, 492, 643, 281, 909, 294, 4497, 2098, 294, 597, 257, 2487, 1062, 312, 18519, 13, 50678], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1883, "seek": 764928, "start": 7655.5599999999995, "end": 7660.8, "text": " So let's go ahead and try and do this with some more patterns.", "tokens": [50678, 407, 718, 311, 352, 2286, 293, 853, 293, 360, 341, 365, 512, 544, 8294, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1884, "seek": 764928, "start": 7660.8, "end": 7664.84, "text": " I'm going to go ahead and copy and paste these in from the textbook.", "tokens": [50940, 286, 478, 516, 281, 352, 2286, 293, 5055, 293, 9163, 613, 294, 490, 264, 25591, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1885, "seek": 764928, "start": 7664.84, "end": 7670.0, "text": " So you'll be able to actually see them at work.", "tokens": [51142, 407, 291, 603, 312, 1075, 281, 767, 536, 552, 412, 589, 13, 51400], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1886, "seek": 764928, "start": 7670.0, "end": 7674.32, "text": " And so what I've did, I've done is I've added in more patterns, pattern two and pattern", "tokens": [51400, 400, 370, 437, 286, 600, 630, 11, 286, 600, 1096, 307, 286, 600, 3869, 294, 544, 8294, 11, 5102, 732, 293, 5102, 51616], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1887, "seek": 764928, "start": 7674.32, "end": 7678.92, "text": " three allow for instances like this, well thought Alice.", "tokens": [51616, 1045, 2089, 337, 14519, 411, 341, 11, 731, 1194, 16004, 13, 51846], "temperature": 0.0, "avg_logprob": -0.1865454537527902, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0384625680744648}, {"id": 1888, "seek": 767892, "start": 7678.92, "end": 7683.16, "text": " So an instance where there's a punctuation, but there's no proceeding quotation after", "tokens": [50364, 407, 364, 5197, 689, 456, 311, 257, 27006, 16073, 11, 457, 456, 311, 572, 41163, 47312, 934, 50576], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1889, "seek": 767892, "start": 7683.16, "end": 7687.76, "text": " this, and then which certainly said before an instance where there's a comma followed", "tokens": [50576, 341, 11, 293, 550, 597, 3297, 848, 949, 364, 5197, 689, 456, 311, 257, 22117, 6263, 50806], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1890, "seek": 767892, "start": 7687.76, "end": 7688.76, "text": " by that.", "tokens": [50806, 538, 300, 13, 50856], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1891, "seek": 767892, "start": 7688.76, "end": 7692.64, "text": " So we've been able to capture more variants and more ways in which quotation marks might", "tokens": [50856, 407, 321, 600, 668, 1075, 281, 7983, 544, 21669, 293, 544, 2098, 294, 597, 47312, 10640, 1062, 51050], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1892, "seek": 767892, "start": 7692.64, "end": 7694.92, "text": " exist followed by the speaker.", "tokens": [51050, 2514, 6263, 538, 264, 8145, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1893, "seek": 767892, "start": 7694.92, "end": 7698.32, "text": " Now this is where being a domain expert comes into play.", "tokens": [51164, 823, 341, 307, 689, 885, 257, 9274, 5844, 1487, 666, 862, 13, 51334], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1894, "seek": 767892, "start": 7698.32, "end": 7701.4400000000005, "text": " You'd have to kind of look through and see the different ways that Louis C. Carroll", "tokens": [51334, 509, 1116, 362, 281, 733, 295, 574, 807, 293, 536, 264, 819, 2098, 300, 9763, 383, 13, 48456, 51490], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1895, "seek": 767892, "start": 7701.4400000000005, "end": 7705.64, "text": " structures quotation marks and write out patterns for capturing them.", "tokens": [51490, 9227, 47312, 10640, 293, 2464, 484, 8294, 337, 23384, 552, 13, 51700], "temperature": 0.0, "avg_logprob": -0.15797969042244603, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.04335521161556244}, {"id": 1896, "seek": 770564, "start": 7705.64, "end": 7709.280000000001, "text": " I'm not going to go through and try to capture everything from Alice in Wonderland because", "tokens": [50364, 286, 478, 406, 516, 281, 352, 807, 293, 853, 281, 7983, 1203, 490, 16004, 294, 13224, 1661, 570, 50546], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1897, "seek": 770564, "start": 7709.280000000001, "end": 7711.6, "text": " that would take a good deal of time.", "tokens": [50546, 300, 576, 747, 257, 665, 2028, 295, 565, 13, 50662], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1898, "seek": 770564, "start": 7711.6, "end": 7715.76, "text": " And it's not really in the best interest because it doesn't matter to me at all.", "tokens": [50662, 400, 309, 311, 406, 534, 294, 264, 1151, 1179, 570, 309, 1177, 380, 1871, 281, 385, 412, 439, 13, 50870], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1899, "seek": 770564, "start": 7715.76, "end": 7719.400000000001, "text": " What I encourage you to do, if this is something interesting to you is try to apply it to your", "tokens": [50870, 708, 286, 5373, 291, 281, 360, 11, 498, 341, 307, 746, 1880, 281, 291, 307, 853, 281, 3079, 309, 281, 428, 51052], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1900, "seek": 770564, "start": 7719.400000000001, "end": 7724.64, "text": " own texts, different authors, structure quotation marks a little differently than what patterns", "tokens": [51052, 1065, 15765, 11, 819, 16552, 11, 3877, 47312, 10640, 257, 707, 7614, 813, 437, 8294, 51314], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1901, "seek": 770564, "start": 7724.64, "end": 7727.56, "text": " that I've gotten written here are a good starting point.", "tokens": [51314, 300, 286, 600, 5768, 3720, 510, 366, 257, 665, 2891, 935, 13, 51460], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1902, "seek": 770564, "start": 7727.56, "end": 7730.4800000000005, "text": " But I would encourage you to start playing around with them a little bit more.", "tokens": [51460, 583, 286, 576, 5373, 291, 281, 722, 2433, 926, 365, 552, 257, 707, 857, 544, 13, 51606], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1903, "seek": 770564, "start": 7730.4800000000005, "end": 7735.52, "text": " And what you can do is when you actually have this match extracted, you know that the", "tokens": [51606, 400, 437, 291, 393, 360, 307, 562, 291, 767, 362, 341, 2995, 34086, 11, 291, 458, 300, 264, 51858], "temperature": 0.0, "avg_logprob": -0.12066296355365073, "compression_ratio": 1.8, "no_speech_prob": 0.03621367737650871}, {"id": 1904, "seek": 773552, "start": 7735.88, "end": 7741.96, "text": " instance of a proper noun that occurs between these quotation marks or after one is probably", "tokens": [50382, 5197, 295, 257, 2296, 23307, 300, 11843, 1296, 613, 47312, 10640, 420, 934, 472, 307, 1391, 50686], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1905, "seek": 773552, "start": 7741.96, "end": 7748.040000000001, "text": " going to be the person or thing that is doing the speaking or the thinking.", "tokens": [50686, 516, 281, 312, 264, 954, 420, 551, 300, 307, 884, 264, 4124, 420, 264, 1953, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1906, "seek": 773552, "start": 7748.040000000001, "end": 7750.080000000001, "text": " So that's kind of how the matcher works.", "tokens": [50990, 407, 300, 311, 733, 295, 577, 264, 2995, 260, 1985, 13, 51092], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1907, "seek": 773552, "start": 7750.080000000001, "end": 7755.68, "text": " It allows for you to do these things, these robust type data extractions without relying", "tokens": [51092, 467, 4045, 337, 291, 281, 360, 613, 721, 11, 613, 13956, 2010, 1412, 8947, 626, 1553, 24140, 51372], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1908, "seek": 773552, "start": 7755.68, "end": 7756.92, "text": " on entity ruler.", "tokens": [51372, 322, 13977, 19661, 13, 51434], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1909, "seek": 773552, "start": 7756.92, "end": 7761.0, "text": " And remember, you can use a lot of these same things with an entity ruler as well.", "tokens": [51434, 400, 1604, 11, 291, 393, 764, 257, 688, 295, 613, 912, 721, 365, 364, 13977, 19661, 382, 731, 13, 51638], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1910, "seek": 773552, "start": 7761.0, "end": 7765.400000000001, "text": " But we don't want this in this case, we don't want things like this to be labeled as entities.", "tokens": [51638, 583, 321, 500, 380, 528, 341, 294, 341, 1389, 11, 321, 500, 380, 528, 721, 411, 341, 281, 312, 21335, 382, 16667, 13, 51858], "temperature": 0.0, "avg_logprob": -0.14625468019579277, "compression_ratio": 1.7992700729927007, "no_speech_prob": 0.028431888669729233}, {"id": 1911, "seek": 776540, "start": 7765.44, "end": 7769.32, "text": " We want them to just be separate things that we can extract outside of the of the", "tokens": [50366, 492, 528, 552, 281, 445, 312, 4994, 721, 300, 321, 393, 8947, 2380, 295, 264, 295, 264, 50560], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1912, "seek": 776540, "start": 7769.32, "end": 7770.96, "text": " ints dot doc dot ints.", "tokens": [50560, 560, 82, 5893, 3211, 5893, 560, 82, 13, 50642], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1913, "seek": 776540, "start": 7771.32, "end": 7776.5599999999995, "text": " That's going to be where we conclude our chapter on on the on the matcher.", "tokens": [50660, 663, 311, 516, 281, 312, 689, 321, 16886, 527, 7187, 322, 322, 264, 322, 264, 2995, 260, 13, 50922], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1914, "seek": 776540, "start": 7776.92, "end": 7780.92, "text": " In the next section of this video, we're going to be talking about custom components in", "tokens": [50940, 682, 264, 958, 3541, 295, 341, 960, 11, 321, 434, 516, 281, 312, 1417, 466, 2375, 6677, 294, 51140], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1915, "seek": 776540, "start": 7780.92, "end": 7786.879999999999, "text": " spacey, which allow for us to do some pretty cool things such as add in special functions", "tokens": [51140, 1901, 88, 11, 597, 2089, 337, 505, 281, 360, 512, 1238, 1627, 721, 1270, 382, 909, 294, 2121, 6828, 51438], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1916, "seek": 776540, "start": 7786.879999999999, "end": 7793.839999999999, "text": " that allow for us to kind of do different custom shapes, permutations on our data with", "tokens": [51438, 300, 2089, 337, 505, 281, 733, 295, 360, 819, 2375, 10854, 11, 4784, 325, 763, 322, 527, 1412, 365, 51786], "temperature": 0.0, "avg_logprob": -0.19265559711287508, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.0018100955057889223}, {"id": 1917, "seek": 779384, "start": 7794.52, "end": 7798.68, "text": " components that don't exist like an entity ruler would be a component components that", "tokens": [50398, 6677, 300, 500, 380, 2514, 411, 364, 13977, 19661, 576, 312, 257, 6542, 6677, 300, 50606], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1918, "seek": 779384, "start": 7798.68, "end": 7800.88, "text": " don't exist within the spacey framework.", "tokens": [50606, 500, 380, 2514, 1951, 264, 1901, 88, 8388, 13, 50716], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1919, "seek": 779384, "start": 7800.88, "end": 7806.76, "text": " So add in custom things like an entity ruler that do very specific things to your data.", "tokens": [50716, 407, 909, 294, 2375, 721, 411, 364, 13977, 19661, 300, 360, 588, 2685, 721, 281, 428, 1412, 13, 51010], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1920, "seek": 779384, "start": 7809.08, "end": 7814.16, "text": " Hello, we're now moving into a more advanced aspect of the textbook specifically chapter", "tokens": [51126, 2425, 11, 321, 434, 586, 2684, 666, 257, 544, 7339, 4171, 295, 264, 25591, 4682, 7187, 51380], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1921, "seek": 779384, "start": 7814.16, "end": 7814.76, "text": " seven.", "tokens": [51380, 3407, 13, 51410], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1922, "seek": 779384, "start": 7814.76, "end": 7817.0, "text": " And that's working with custom components.", "tokens": [51410, 400, 300, 311, 1364, 365, 2375, 6677, 13, 51522], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1923, "seek": 779384, "start": 7817.32, "end": 7821.24, "text": " A good way to think about a custom component is something that you need to do to the doc", "tokens": [51538, 316, 665, 636, 281, 519, 466, 257, 2375, 6542, 307, 746, 300, 291, 643, 281, 360, 281, 264, 3211, 51734], "temperature": 0.0, "avg_logprob": -0.13346766499639715, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.001244804821908474}, {"id": 1924, "seek": 782124, "start": 7821.24, "end": 7825.08, "text": " object or the doc container that spacey can't do off the shelf.", "tokens": [50364, 2657, 420, 264, 3211, 10129, 300, 1901, 88, 393, 380, 360, 766, 264, 15222, 13, 50556], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1925, "seek": 782124, "start": 7825.08, "end": 7827.88, "text": " You want to modify it at some point in the pipeline.", "tokens": [50556, 509, 528, 281, 16927, 309, 412, 512, 935, 294, 264, 15517, 13, 50696], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1926, "seek": 782124, "start": 7828.2, "end": 7832.5599999999995, "text": " So I'm going to use a basic toy example that demonstrates the power of this.", "tokens": [50712, 407, 286, 478, 516, 281, 764, 257, 3875, 12058, 1365, 300, 31034, 264, 1347, 295, 341, 13, 50930], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1927, "seek": 782124, "start": 7832.84, "end": 7835.719999999999, "text": " Let's look at this basic example that I've already loaded into memory.", "tokens": [50944, 961, 311, 574, 412, 341, 3875, 1365, 300, 286, 600, 1217, 13210, 666, 4675, 13, 51088], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1928, "seek": 782124, "start": 7835.92, "end": 7838.76, "text": " It's two sentences that are in the doc object now.", "tokens": [51098, 467, 311, 732, 16579, 300, 366, 294, 264, 3211, 2657, 586, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1929, "seek": 782124, "start": 7839.44, "end": 7840.639999999999, "text": " And that's Britain is a place.", "tokens": [51274, 400, 300, 311, 12960, 307, 257, 1081, 13, 51334], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1930, "seek": 782124, "start": 7840.84, "end": 7842.0, "text": " Mary is a doctor.", "tokens": [51344, 6059, 307, 257, 4631, 13, 51402], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1931, "seek": 782124, "start": 7842.36, "end": 7849.08, "text": " So let's do for int and doc dot ints print off int dot text and dot label.", "tokens": [51420, 407, 718, 311, 360, 337, 560, 293, 3211, 5893, 560, 82, 4482, 766, 560, 5893, 2487, 293, 5893, 7645, 13, 51756], "temperature": 0.0, "avg_logprob": -0.15151452236488216, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.004905074834823608}, {"id": 1932, "seek": 784908, "start": 7850.08, "end": 7851.5199999999995, "text": " And we see what we'd expect.", "tokens": [50414, 400, 321, 536, 437, 321, 1116, 2066, 13, 50486], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1933, "seek": 784908, "start": 7851.5199999999995, "end": 7854.5199999999995, "text": " Britain is GPE a geopolitical entity.", "tokens": [50486, 12960, 307, 460, 5208, 257, 46615, 804, 13977, 13, 50636], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1934, "seek": 784908, "start": 7854.76, "end": 7856.4, "text": " Mary is a person.", "tokens": [50648, 6059, 307, 257, 954, 13, 50730], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1935, "seek": 784908, "start": 7857.12, "end": 7858.0, "text": " That's fantastic.", "tokens": [50766, 663, 311, 5456, 13, 50810], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1936, "seek": 784908, "start": 7858.28, "end": 7864.5599999999995, "text": " But I've just been told by somebody higher up that they want the model to never ever", "tokens": [50824, 583, 286, 600, 445, 668, 1907, 538, 2618, 2946, 493, 300, 436, 528, 264, 2316, 281, 1128, 1562, 51138], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1937, "seek": 784908, "start": 7864.5599999999995, "end": 7871.48, "text": " give anything as GPE or maybe they want any instance of GPE to be flagged as LOC.", "tokens": [51138, 976, 1340, 382, 460, 5208, 420, 1310, 436, 528, 604, 5197, 295, 460, 5208, 281, 312, 7166, 3004, 382, 15731, 34, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1938, "seek": 784908, "start": 7872.28, "end": 7877.64, "text": " So all the different locations all have LOC as a label or we just want to remove them", "tokens": [51524, 407, 439, 264, 819, 9253, 439, 362, 15731, 34, 382, 257, 7645, 420, 321, 445, 528, 281, 4159, 552, 51792], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1939, "seek": 784908, "start": 7877.64, "end": 7878.24, "text": " entirely.", "tokens": [51792, 7696, 13, 51822], "temperature": 0.0, "avg_logprob": -0.2277505602155413, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.0015487034106627107}, {"id": 1940, "seek": 787824, "start": 7878.92, "end": 7880.679999999999, "text": " So I'm going to work with that latter example.", "tokens": [50398, 407, 286, 478, 516, 281, 589, 365, 300, 18481, 1365, 13, 50486], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1941, "seek": 787824, "start": 7881.0, "end": 7887.76, "text": " We need to create a custom pipe that removes all instances of GPE from the doc dot", "tokens": [50502, 492, 643, 281, 1884, 257, 2375, 11240, 300, 30445, 439, 14519, 295, 460, 5208, 490, 264, 3211, 5893, 50840], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1942, "seek": 787824, "start": 7887.8, "end": 7889.04, "text": " ints container.", "tokens": [50842, 560, 82, 10129, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1943, "seek": 787824, "start": 7889.32, "end": 7890.08, "text": " So how do we do that?", "tokens": [50918, 407, 577, 360, 321, 360, 300, 30, 50956], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1944, "seek": 787824, "start": 7890.12, "end": 7892.88, "text": " Well, we need to use a custom component.", "tokens": [50958, 1042, 11, 321, 643, 281, 764, 257, 2375, 6542, 13, 51096], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1945, "seek": 787824, "start": 7893.2, "end": 7898.719999999999, "text": " We can do this very easily in spacey by saying from spacey dot language import", "tokens": [51112, 492, 393, 360, 341, 588, 3612, 294, 1901, 88, 538, 1566, 490, 1901, 88, 5893, 2856, 974, 51388], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1946, "seek": 787824, "start": 7899.08, "end": 7901.24, "text": " language capital L very important there.", "tokens": [51406, 2856, 4238, 441, 588, 1021, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1947, "seek": 787824, "start": 7901.24, "end": 7904.599999999999, "text": " Capital L now that we've got that class loaded up.", "tokens": [51514, 21502, 441, 586, 300, 321, 600, 658, 300, 1508, 13210, 493, 13, 51682], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1948, "seek": 787824, "start": 7905.08, "end": 7906.32, "text": " Let's start working with this.", "tokens": [51706, 961, 311, 722, 1364, 365, 341, 13, 51768], "temperature": 0.0, "avg_logprob": -0.16366064021017698, "compression_ratio": 1.64, "no_speech_prob": 0.0029807633254677057}, {"id": 1949, "seek": 790632, "start": 7906.32, "end": 7909.28, "text": " What we need to do first is we need to use a flag.", "tokens": [50364, 708, 321, 643, 281, 360, 700, 307, 321, 643, 281, 764, 257, 7166, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1950, "seek": 790632, "start": 7909.28, "end": 7913.5199999999995, "text": " So the symbol and we need to say at language dot component.", "tokens": [50512, 407, 264, 5986, 293, 321, 643, 281, 584, 412, 2856, 5893, 6542, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1951, "seek": 790632, "start": 7914.48, "end": 7916.28, "text": " And we need to give that component a name.", "tokens": [50772, 400, 321, 643, 281, 976, 300, 6542, 257, 1315, 13, 50862], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1952, "seek": 790632, "start": 7916.719999999999, "end": 7920.16, "text": " We're going to say in this case, let's say remove GPE.", "tokens": [50884, 492, 434, 516, 281, 584, 294, 341, 1389, 11, 718, 311, 584, 4159, 460, 5208, 13, 51056], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1953, "seek": 790632, "start": 7921.48, "end": 7923.5599999999995, "text": " And now we need to create a function to do this.", "tokens": [51122, 400, 586, 321, 643, 281, 1884, 257, 2445, 281, 360, 341, 13, 51226], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1954, "seek": 790632, "start": 7923.92, "end": 7927.2, "text": " So we're going to call this remove GPE.", "tokens": [51244, 407, 321, 434, 516, 281, 818, 341, 4159, 460, 5208, 13, 51408], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1955, "seek": 790632, "start": 7927.4, "end": 7929.24, "text": " I always kind of keep these as the same.", "tokens": [51418, 286, 1009, 733, 295, 1066, 613, 382, 264, 912, 13, 51510], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1956, "seek": 790632, "start": 7929.679999999999, "end": 7931.28, "text": " That's my personal preference.", "tokens": [51532, 663, 311, 452, 2973, 17502, 13, 51612], "temperature": 0.0, "avg_logprob": -0.16878544200550427, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0004878357285633683}, {"id": 1957, "seek": 793128, "start": 7931.759999999999, "end": 7935.04, "text": " And this is going to take one, one, one thing.", "tokens": [50388, 400, 341, 307, 516, 281, 747, 472, 11, 472, 11, 472, 551, 13, 50552], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1958, "seek": 793128, "start": 7935.08, "end": 7936.4, "text": " That's going to be the doc object.", "tokens": [50554, 663, 311, 516, 281, 312, 264, 3211, 2657, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1959, "seek": 793128, "start": 7936.679999999999, "end": 7939.599999999999, "text": " So the doc object, think about how it moves through the pipeline.", "tokens": [50634, 407, 264, 3211, 2657, 11, 519, 466, 577, 309, 6067, 807, 264, 15517, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1960, "seek": 793128, "start": 7939.84, "end": 7942.639999999999, "text": " This component is another pipe and that pipeline.", "tokens": [50792, 639, 6542, 307, 1071, 11240, 293, 300, 15517, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1961, "seek": 793128, "start": 7942.639999999999, "end": 7947.4, "text": " It needs to receive the doc object and send off the doc object.", "tokens": [50932, 467, 2203, 281, 4774, 264, 3211, 2657, 293, 2845, 766, 264, 3211, 2657, 13, 51170], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1962, "seek": 793128, "start": 7947.639999999999, "end": 7948.759999999999, "text": " You could do a lot of other things.", "tokens": [51182, 509, 727, 360, 257, 688, 295, 661, 721, 13, 51238], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1963, "seek": 793128, "start": 7949.08, "end": 7950.599999999999, "text": " It could print off entity found.", "tokens": [51254, 467, 727, 4482, 766, 13977, 1352, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1964, "seek": 793128, "start": 7950.599999999999, "end": 7952.8, "text": " It could do really any number of things.", "tokens": [51330, 467, 727, 360, 534, 604, 1230, 295, 721, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1965, "seek": 793128, "start": 7952.8, "end": 7956.44, "text": " It could add stuff to the data coming out of the pipeline.", "tokens": [51440, 467, 727, 909, 1507, 281, 264, 1412, 1348, 484, 295, 264, 15517, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1966, "seek": 793128, "start": 7957.04, "end": 7960.24, "text": " All we're concerned with right now is modifying the doc dot ints.", "tokens": [51652, 1057, 321, 434, 5922, 365, 558, 586, 307, 42626, 264, 3211, 5893, 560, 82, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1509021787501093, "compression_ratio": 1.915057915057915, "no_speech_prob": 0.010012367740273476}, {"id": 1967, "seek": 796024, "start": 7961.08, "end": 7962.4, "text": " So we can do something like this.", "tokens": [50406, 407, 321, 393, 360, 746, 411, 341, 13, 50472], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1968, "seek": 796024, "start": 7962.88, "end": 7968.12, "text": " We can say original ends is equal to a list of the doc dot ends.", "tokens": [50496, 492, 393, 584, 3380, 5314, 307, 2681, 281, 257, 1329, 295, 264, 3211, 5893, 5314, 13, 50758], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1969, "seek": 796024, "start": 7968.24, "end": 7972.8, "text": " So remember, we have to convert the ends from a generator into a list.", "tokens": [50764, 407, 1604, 11, 321, 362, 281, 7620, 264, 5314, 490, 257, 19265, 666, 257, 1329, 13, 50992], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1970, "seek": 796024, "start": 7973.04, "end": 7978.0, "text": " Now what we can do is we can say for int and doc dot ends, if the end not label.", "tokens": [51004, 823, 437, 321, 393, 360, 307, 321, 393, 584, 337, 560, 293, 3211, 5893, 5314, 11, 498, 264, 917, 406, 7645, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1971, "seek": 796024, "start": 7978.0, "end": 7985.04, "text": " So if that label is equal to GPE, then what we want to do is we want to just", "tokens": [51252, 407, 498, 300, 7645, 307, 2681, 281, 460, 5208, 11, 550, 437, 321, 528, 281, 360, 307, 321, 528, 281, 445, 51604], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1972, "seek": 796024, "start": 7985.16, "end": 7986.24, "text": " we just want to remove it.", "tokens": [51610, 321, 445, 528, 281, 4159, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2090760927933913, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0010321831796318293}, {"id": 1973, "seek": 798624, "start": 7986.36, "end": 7991.36, "text": " So let's say original ints.remove and we're going to remove the int.", "tokens": [50370, 407, 718, 311, 584, 3380, 560, 82, 13, 2579, 1682, 293, 321, 434, 516, 281, 4159, 264, 560, 13, 50620], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1974, "seek": 798624, "start": 7991.639999999999, "end": 7992.719999999999, "text": " Remember, it's now a list.", "tokens": [50634, 5459, 11, 309, 311, 586, 257, 1329, 13, 50688], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1975, "seek": 798624, "start": 7993.0, "end": 7994.5599999999995, "text": " Sorry, I executed that too soon.", "tokens": [50702, 4919, 11, 286, 17577, 300, 886, 2321, 13, 50780], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1976, "seek": 798624, "start": 7994.84, "end": 7996.04, "text": " Remember, it's now a list.", "tokens": [50794, 5459, 11, 309, 311, 586, 257, 1329, 13, 50854], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1977, "seek": 798624, "start": 7996.2, "end": 8000.96, "text": " So what we can do is we can go ahead now and convert those original", "tokens": [50862, 407, 437, 321, 393, 360, 307, 321, 393, 352, 2286, 586, 293, 7620, 729, 3380, 51100], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1978, "seek": 798624, "start": 8000.96, "end": 8006.32, "text": " ends back into doc dot ends by saying doc dot ends equals original ends.", "tokens": [51100, 5314, 646, 666, 3211, 5893, 5314, 538, 1566, 3211, 5893, 5314, 6915, 3380, 5314, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1979, "seek": 798624, "start": 8006.719999999999, "end": 8010.8, "text": " And if we've done things correctly, we can return the doc object and it will", "tokens": [51388, 400, 498, 321, 600, 1096, 721, 8944, 11, 321, 393, 2736, 264, 3211, 2657, 293, 309, 486, 51592], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1980, "seek": 798624, "start": 8010.8, "end": 8012.84, "text": " have all of those things removed.", "tokens": [51592, 362, 439, 295, 729, 721, 7261, 13, 51694], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1981, "seek": 798624, "start": 8013.04, "end": 8015.28, "text": " So this is what we would call a custom component.", "tokens": [51704, 407, 341, 307, 437, 321, 576, 818, 257, 2375, 6542, 13, 51816], "temperature": 0.0, "avg_logprob": -0.17324742674827576, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.003593365428969264}, {"id": 1982, "seek": 801528, "start": 8015.28, "end": 8019.759999999999, "text": " Something that changes the doc object along the way in the pipeline, but", "tokens": [50364, 6595, 300, 2962, 264, 3211, 2657, 2051, 264, 636, 294, 264, 15517, 11, 457, 50588], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1983, "seek": 801528, "start": 8019.759999999999, "end": 8021.44, "text": " we need to add it to NLP.", "tokens": [50588, 321, 643, 281, 909, 309, 281, 426, 45196, 13, 50672], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1984, "seek": 801528, "start": 8021.88, "end": 8024.12, "text": " So we can do NLP dot add pipe.", "tokens": [50694, 407, 321, 393, 360, 426, 45196, 5893, 909, 11240, 13, 50806], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1985, "seek": 801528, "start": 8025.04, "end": 8027.24, "text": " We want to make sure that it comes after the NER.", "tokens": [50852, 492, 528, 281, 652, 988, 300, 309, 1487, 934, 264, 426, 1598, 13, 50962], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1986, "seek": 801528, "start": 8027.48, "end": 8032.88, "text": " So we're just going to say, uh, add the pipe or move GPE corresponds", "tokens": [50974, 407, 321, 434, 445, 516, 281, 584, 11, 2232, 11, 909, 264, 11240, 420, 1286, 460, 5208, 23249, 51244], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1987, "seek": 801528, "start": 8032.88, "end": 8034.16, "text": " to the component name.", "tokens": [51244, 281, 264, 6542, 1315, 13, 51308], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1988, "seek": 801528, "start": 8035.88, "end": 8039.4, "text": " And now let's go ahead and NLP dot analyze pipes.", "tokens": [51394, 400, 586, 718, 311, 352, 2286, 293, 426, 45196, 5893, 12477, 21882, 13, 51570], "temperature": 0.0, "avg_logprob": -0.17726460281683473, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.000315027660690248}, {"id": 1989, "seek": 803940, "start": 8040.4, "end": 8044.879999999999, "text": " And you'll be able to see that it sits at the end of our pipeline right there.", "tokens": [50414, 400, 291, 603, 312, 1075, 281, 536, 300, 309, 12696, 412, 264, 917, 295, 527, 15517, 558, 456, 13, 50638], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1990, "seek": 803940, "start": 8044.879999999999, "end": 8045.879999999999, "text": " Remove GPE.", "tokens": [50638, 18831, 460, 5208, 13, 50688], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1991, "seek": 803940, "start": 8046.36, "end": 8048.4, "text": " Now comes time to see if it actually works.", "tokens": [50712, 823, 1487, 565, 281, 536, 498, 309, 767, 1985, 13, 50814], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1992, "seek": 803940, "start": 8048.719999999999, "end": 8051.799999999999, "text": " So we're going to copy and paste our code from earlier up here.", "tokens": [50830, 407, 321, 434, 516, 281, 5055, 293, 9163, 527, 3089, 490, 3071, 493, 510, 13, 50984], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1993, "seek": 803940, "start": 8057.04, "end": 8061.48, "text": " Let's go ahead and copy this.", "tokens": [51246, 961, 311, 352, 2286, 293, 5055, 341, 13, 51468], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1994, "seek": 803940, "start": 8063.679999999999, "end": 8068.599999999999, "text": " And now we're going to say for int and doc dot ends print off int dot text.", "tokens": [51578, 400, 586, 321, 434, 516, 281, 584, 337, 560, 293, 3211, 5893, 5314, 4482, 766, 560, 5893, 2487, 13, 51824], "temperature": 0.0, "avg_logprob": -0.27743073795618634, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.000804065028205514}, {"id": 1995, "seek": 806860, "start": 8069.400000000001, "end": 8070.4800000000005, "text": " And dot label.", "tokens": [50404, 400, 5893, 7645, 13, 50458], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 1996, "seek": 806860, "start": 8070.92, "end": 8075.200000000001, "text": " And we should see, as we would expect, just marry coming out.", "tokens": [50480, 400, 321, 820, 536, 11, 382, 321, 576, 2066, 11, 445, 9747, 1348, 484, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 1997, "seek": 806860, "start": 8075.64, "end": 8078.400000000001, "text": " Our pipeline has successfully worked.", "tokens": [50716, 2621, 15517, 575, 10727, 2732, 13, 50854], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 1998, "seek": 806860, "start": 8078.84, "end": 8082.56, "text": " Now, as we're going to see when we move into red checks, you can do a lot", "tokens": [50876, 823, 11, 382, 321, 434, 516, 281, 536, 562, 321, 1286, 666, 2182, 13834, 11, 291, 393, 360, 257, 688, 51062], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 1999, "seek": 806860, "start": 8082.56, "end": 8086.280000000001, "text": " of really, really cool things with custom components.", "tokens": [51062, 295, 534, 11, 534, 1627, 721, 365, 2375, 6677, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 2000, "seek": 806860, "start": 8086.64, "end": 8090.4800000000005, "text": " I'm going to kind of save the, the advanced features for, I think I've", "tokens": [51266, 286, 478, 516, 281, 733, 295, 3155, 264, 11, 264, 7339, 4122, 337, 11, 286, 519, 286, 600, 51458], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 2001, "seek": 806860, "start": 8090.4800000000005, "end": 8094.400000000001, "text": " got it scheduled for chapter here, chapter nine in our textbook.", "tokens": [51458, 658, 309, 15678, 337, 7187, 510, 11, 7187, 4949, 294, 527, 25591, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1675879721548043, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0012447864282876253}, {"id": 2002, "seek": 809440, "start": 8094.679999999999, "end": 8099.32, "text": " This is just a very, very basic example of how you can introduce a custom", "tokens": [50378, 639, 307, 445, 257, 588, 11, 588, 3875, 1365, 295, 577, 291, 393, 5366, 257, 2375, 50610], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2003, "seek": 809440, "start": 8099.32, "end": 8101.5599999999995, "text": " component to your spacey pipeline.", "tokens": [50610, 6542, 281, 428, 1901, 88, 15517, 13, 50722], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2004, "seek": 809440, "start": 8101.92, "end": 8104.08, "text": " If you can do this, you can do a lot more.", "tokens": [50740, 759, 291, 393, 360, 341, 11, 291, 393, 360, 257, 688, 544, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2005, "seek": 809440, "start": 8104.4, "end": 8106.44, "text": " You can maybe change a different entity.", "tokens": [50864, 509, 393, 1310, 1319, 257, 819, 13977, 13, 50966], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2006, "seek": 809440, "start": 8106.44, "end": 8107.639999999999, "text": " So they have different labels.", "tokens": [50966, 407, 436, 362, 819, 16949, 13, 51026], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2007, "seek": 809440, "start": 8107.799999999999, "end": 8110.679999999999, "text": " You can make it where GPEs and locks all agree.", "tokens": [51034, 509, 393, 652, 309, 689, 460, 5208, 82, 293, 20703, 439, 3986, 13, 51178], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2008, "seek": 809440, "start": 8110.92, "end": 8112.2, "text": " You can remove certain things.", "tokens": [51190, 509, 393, 4159, 1629, 721, 13, 51254], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2009, "seek": 809440, "start": 8112.2, "end": 8115.28, "text": " You can have it print off place found person found.", "tokens": [51254, 509, 393, 362, 309, 4482, 766, 1081, 1352, 954, 1352, 13, 51408], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2010, "seek": 809440, "start": 8115.5599999999995, "end": 8116.4, "text": " You can do a lot.", "tokens": [51422, 509, 393, 360, 257, 688, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2011, "seek": 809440, "start": 8117.24, "end": 8120.799999999999, "text": " So really the sky's the limit here, but a lot of the times you're going", "tokens": [51506, 407, 534, 264, 5443, 311, 264, 4948, 510, 11, 457, 257, 688, 295, 264, 1413, 291, 434, 516, 51684], "temperature": 0.0, "avg_logprob": -0.13465851072281126, "compression_ratio": 1.734375, "no_speech_prob": 0.0340929739177227}, {"id": 2012, "seek": 812080, "start": 8120.8, "end": 8122.6, "text": " to need to modify that doc object.", "tokens": [50364, 281, 643, 281, 16927, 300, 3211, 2657, 13, 50454], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2013, "seek": 812080, "start": 8122.92, "end": 8126.24, "text": " And this is how you do it with a custom pipe so that you don't have to write", "tokens": [50470, 400, 341, 307, 577, 291, 360, 309, 365, 257, 2375, 11240, 370, 300, 291, 500, 380, 362, 281, 2464, 50636], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2014, "seek": 812080, "start": 8126.24, "end": 8131.88, "text": " a bunch of code for a user outside of that NLP object, the NLP object.", "tokens": [50636, 257, 3840, 295, 3089, 337, 257, 4195, 2380, 295, 300, 426, 45196, 2657, 11, 264, 426, 45196, 2657, 13, 50918], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2015, "seek": 812080, "start": 8131.88, "end": 8138.56, "text": " Once you save it to disk by doing something like NLP dot to disk data,", "tokens": [50918, 3443, 291, 3155, 309, 281, 12355, 538, 884, 746, 411, 426, 45196, 5893, 281, 12355, 1412, 11, 51252], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2016, "seek": 812080, "start": 8139.2, "end": 8145.64, "text": " new and core web SM, it's going to actually be able to go to the disk", "tokens": [51284, 777, 293, 4965, 3670, 13115, 11, 309, 311, 516, 281, 767, 312, 1075, 281, 352, 281, 264, 12355, 51606], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2017, "seek": 812080, "start": 8145.96, "end": 8147.64, "text": " and be saved with everything.", "tokens": [51622, 293, 312, 6624, 365, 1203, 13, 51706], "temperature": 0.0, "avg_logprob": -0.20518748594982789, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.03513507917523384}, {"id": 2018, "seek": 814764, "start": 8148.08, "end": 8152.4400000000005, "text": " But one thing that you should note is that the component that you have", "tokens": [50386, 583, 472, 551, 300, 291, 820, 3637, 307, 300, 264, 6542, 300, 291, 362, 50604], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2019, "seek": 814764, "start": 8152.4400000000005, "end": 8156.52, "text": " here is not automatically saved with your data.", "tokens": [50604, 510, 307, 406, 6772, 6624, 365, 428, 1412, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2020, "seek": 814764, "start": 8156.92, "end": 8160.320000000001, "text": " So in order for your component to actually be saved with your data,", "tokens": [50828, 407, 294, 1668, 337, 428, 6542, 281, 767, 312, 6624, 365, 428, 1412, 11, 50998], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2021, "seek": 814764, "start": 8160.64, "end": 8166.0, "text": " you need to store that outside of this entire script.", "tokens": [51014, 291, 643, 281, 3531, 300, 2380, 295, 341, 2302, 5755, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2022, "seek": 814764, "start": 8166.280000000001, "end": 8171.360000000001, "text": " You need to save it as a library that can be given to the model", "tokens": [51296, 509, 643, 281, 3155, 309, 382, 257, 6405, 300, 393, 312, 2212, 281, 264, 2316, 51550], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2023, "seek": 814764, "start": 8171.400000000001, "end": 8172.76, "text": " when you go to package it.", "tokens": [51552, 562, 291, 352, 281, 7372, 309, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2024, "seek": 814764, "start": 8172.96, "end": 8175.280000000001, "text": " That's beyond the scope of this video for right now.", "tokens": [51630, 663, 311, 4399, 264, 11923, 295, 341, 960, 337, 558, 586, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1223244569739517, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.005219398532062769}, {"id": 2025, "seek": 817528, "start": 8175.639999999999, "end": 8179.88, "text": " In order for this to work in a different Jupyter notebook, if you were to try", "tokens": [50382, 682, 1668, 337, 341, 281, 589, 294, 257, 819, 22125, 88, 391, 21060, 11, 498, 291, 645, 281, 853, 50594], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2026, "seek": 817528, "start": 8179.88, "end": 8185.32, "text": " to use this, this container, this component has to actually be in the script.", "tokens": [50594, 281, 764, 341, 11, 341, 10129, 11, 341, 6542, 575, 281, 767, 312, 294, 264, 5755, 13, 50866], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2027, "seek": 817528, "start": 8185.5599999999995, "end": 8189.4, "text": " When it comes time to package your model, your pipeline and distribute it,", "tokens": [50878, 1133, 309, 1487, 565, 281, 7372, 428, 2316, 11, 428, 15517, 293, 20594, 309, 11, 51070], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2028, "seek": 817528, "start": 8189.679999999999, "end": 8190.8, "text": " that's a different scenario.", "tokens": [51084, 300, 311, 257, 819, 9005, 13, 51140], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2029, "seek": 817528, "start": 8190.8, "end": 8194.48, "text": " And that scenario, you're going to make sure that you've got a special my", "tokens": [51140, 400, 300, 9005, 11, 291, 434, 516, 281, 652, 988, 300, 291, 600, 658, 257, 2121, 452, 51324], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2030, "seek": 817528, "start": 8194.48, "end": 8199.56, "text": " component dot pie file with this bit of code in there so that, so that spacing", "tokens": [51324, 6542, 5893, 1730, 3991, 365, 341, 857, 295, 3089, 294, 456, 370, 300, 11, 370, 300, 27739, 51578], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2031, "seek": 817528, "start": 8199.56, "end": 8202.56, "text": " knows how to handle your particular data.", "tokens": [51578, 3255, 577, 281, 4813, 428, 1729, 1412, 13, 51728], "temperature": 0.0, "avg_logprob": -0.19983855537746265, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.005554238334298134}, {"id": 2032, "seek": 820256, "start": 8203.56, "end": 8206.8, "text": " It's now time to move on to chapter eight of this textbook.", "tokens": [50414, 467, 311, 586, 565, 281, 1286, 322, 281, 7187, 3180, 295, 341, 25591, 13, 50576], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2033, "seek": 820256, "start": 8206.8, "end": 8208.68, "text": " And this is where a spacey gets really interesting.", "tokens": [50576, 400, 341, 307, 689, 257, 1901, 88, 2170, 534, 1880, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2034, "seek": 820256, "start": 8208.68, "end": 8213.279999999999, "text": " You can start applying regular expressions into a spacey component", "tokens": [50670, 509, 393, 722, 9275, 3890, 15277, 666, 257, 1901, 88, 6542, 50900], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2035, "seek": 820256, "start": 8213.279999999999, "end": 8217.359999999999, "text": " like an entity ruler or a custom component, as we're going to see in just", "tokens": [50900, 411, 364, 13977, 19661, 420, 257, 2375, 6542, 11, 382, 321, 434, 516, 281, 536, 294, 445, 51104], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2036, "seek": 820256, "start": 8217.359999999999, "end": 8218.84, "text": " a moment with chapter nine.", "tokens": [51104, 257, 1623, 365, 7187, 4949, 13, 51178], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2037, "seek": 820256, "start": 8219.199999999999, "end": 8222.68, "text": " I'm not going to spend a good deal of time talking about regular expressions.", "tokens": [51196, 286, 478, 406, 516, 281, 3496, 257, 665, 2028, 295, 565, 1417, 466, 3890, 15277, 13, 51370], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2038, "seek": 820256, "start": 8222.68, "end": 8227.0, "text": " I could spend five hours talking about regex and what all it can do.", "tokens": [51370, 286, 727, 3496, 1732, 2496, 1417, 466, 319, 432, 87, 293, 437, 439, 309, 393, 360, 13, 51586], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2039, "seek": 820256, "start": 8227.119999999999, "end": 8230.88, "text": " In the textbook, I go over what you really need to know, which is what regular", "tokens": [51592, 682, 264, 25591, 11, 286, 352, 670, 437, 291, 534, 643, 281, 458, 11, 597, 307, 437, 3890, 51780], "temperature": 0.0, "avg_logprob": -0.14093127846717834, "compression_ratio": 1.7879858657243817, "no_speech_prob": 0.00271466001868248}, {"id": 2040, "seek": 823088, "start": 8230.92, "end": 8235.92, "text": " expressions is, which is as a way to do a really robust string pattern matching.", "tokens": [50366, 15277, 307, 11, 597, 307, 382, 257, 636, 281, 360, 257, 534, 13956, 6798, 5102, 14324, 13, 50616], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2041, "seek": 823088, "start": 8236.279999999999, "end": 8239.439999999999, "text": " I talk about the strengths of it, the weaknesses of it, its drawbacks,", "tokens": [50634, 286, 751, 466, 264, 16986, 295, 309, 11, 264, 24381, 295, 309, 11, 1080, 2642, 17758, 11, 50792], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2042, "seek": 823088, "start": 8239.8, "end": 8243.08, "text": " how to implement it in Python and how to really work with regex.", "tokens": [50810, 577, 281, 4445, 309, 294, 15329, 293, 577, 281, 534, 589, 365, 319, 432, 87, 13, 50974], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2043, "seek": 823088, "start": 8243.24, "end": 8245.64, "text": " But this is a video series on spacey.", "tokens": [50982, 583, 341, 307, 257, 960, 2638, 322, 1901, 88, 13, 51102], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2044, "seek": 823088, "start": 8245.88, "end": 8249.32, "text": " What I want to talk about is how to use regex with spacey.", "tokens": [51114, 708, 286, 528, 281, 751, 466, 307, 577, 281, 764, 319, 432, 87, 365, 1901, 88, 13, 51286], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2045, "seek": 823088, "start": 8249.64, "end": 8252.4, "text": " And so let's move over to a Jupiter notebook where we actually have this", "tokens": [51302, 400, 370, 718, 311, 1286, 670, 281, 257, 24567, 21060, 689, 321, 767, 362, 341, 51440], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2046, "seek": 823088, "start": 8252.4, "end": 8254.24, "text": " code to execute and play around with.", "tokens": [51440, 3089, 281, 14483, 293, 862, 926, 365, 13, 51532], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2047, "seek": 823088, "start": 8255.0, "end": 8257.839999999998, "text": " If we look here, we have the same example that we saw before.", "tokens": [51570, 759, 321, 574, 510, 11, 321, 362, 264, 912, 1365, 300, 321, 1866, 949, 13, 51712], "temperature": 0.0, "avg_logprob": -0.13125012333231761, "compression_ratio": 1.6933797909407666, "no_speech_prob": 0.005384446121752262}, {"id": 2048, "seek": 825784, "start": 8258.16, "end": 8260.8, "text": " What my goal is is not to extract the whole phone number,", "tokens": [50380, 708, 452, 3387, 307, 307, 406, 281, 8947, 264, 1379, 2593, 1230, 11, 50512], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2049, "seek": 825784, "start": 8260.8, "end": 8263.2, "text": " rather try to grab this sequence here.", "tokens": [50512, 2831, 853, 281, 4444, 341, 8310, 510, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2050, "seek": 825784, "start": 8263.52, "end": 8265.76, "text": " And we do this with a regular expression pattern.", "tokens": [50648, 400, 321, 360, 341, 365, 257, 3890, 6114, 5102, 13, 50760], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2051, "seek": 825784, "start": 8266.08, "end": 8269.68, "text": " What this says is it tells it to look for a sequence of tokens or sequence", "tokens": [50776, 708, 341, 1619, 307, 309, 5112, 309, 281, 574, 337, 257, 8310, 295, 22667, 420, 8310, 50956], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2052, "seek": 825784, "start": 8269.68, "end": 8271.4, "text": " of characters like this.", "tokens": [50956, 295, 4342, 411, 341, 13, 51042], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2053, "seek": 825784, "start": 8271.68, "end": 8276.84, "text": " It's going to be three digits followed by a dash followed by four digits.", "tokens": [51056, 467, 311, 516, 281, 312, 1045, 27011, 6263, 538, 257, 8240, 6263, 538, 1451, 27011, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2054, "seek": 825784, "start": 8277.16, "end": 8280.44, "text": " And if I were to execute this whole code, nothing is printed out.", "tokens": [51330, 400, 498, 286, 645, 281, 14483, 341, 1379, 3089, 11, 1825, 307, 13567, 484, 13, 51494], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2055, "seek": 825784, "start": 8280.84, "end": 8283.4, "text": " Does that mean that I failed to write good regex?", "tokens": [51514, 4402, 300, 914, 300, 286, 7612, 281, 2464, 665, 319, 432, 87, 30, 51642], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2056, "seek": 825784, "start": 8283.4, "end": 8284.76, "text": " No, it does not at all.", "tokens": [51642, 883, 11, 309, 775, 406, 412, 439, 13, 51710], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2057, "seek": 825784, "start": 8285.12, "end": 8287.36, "text": " It's failed for one very important reason.", "tokens": [51728, 467, 311, 7612, 337, 472, 588, 1021, 1778, 13, 51840], "temperature": 0.0, "avg_logprob": -0.13299248306839553, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.01032663881778717}, {"id": 2058, "seek": 828736, "start": 8287.44, "end": 8291.720000000001, "text": " And this is the whole reason why I have this chapter in here is that regex,", "tokens": [50368, 400, 341, 307, 264, 1379, 1778, 983, 286, 362, 341, 7187, 294, 510, 307, 300, 319, 432, 87, 11, 50582], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2059, "seek": 828736, "start": 8292.0, "end": 8295.32, "text": " when it comes to pattern matching, pattern matching only really works", "tokens": [50596, 562, 309, 1487, 281, 5102, 14324, 11, 5102, 14324, 787, 534, 1985, 50762], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2060, "seek": 828736, "start": 8296.16, "end": 8299.84, "text": " when it comes to regex for single tokens.", "tokens": [50804, 562, 309, 1487, 281, 319, 432, 87, 337, 2167, 22667, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2061, "seek": 828736, "start": 8300.04, "end": 8305.480000000001, "text": " You can't use regex across multi-word tokens, at least as of spacey 3.1.", "tokens": [50998, 509, 393, 380, 764, 319, 432, 87, 2108, 4825, 12, 7462, 22667, 11, 412, 1935, 382, 295, 1901, 88, 805, 13, 16, 13, 51270], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2062, "seek": 828736, "start": 8305.92, "end": 8307.2, "text": " So what does that mean?", "tokens": [51292, 407, 437, 775, 300, 914, 30, 51356], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2063, "seek": 828736, "start": 8307.2, "end": 8310.92, "text": " Well, it means that that dash right there in our phone number is causing", "tokens": [51356, 1042, 11, 309, 1355, 300, 300, 8240, 558, 456, 294, 527, 2593, 1230, 307, 9853, 51542], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2064, "seek": 828736, "start": 8310.92, "end": 8312.2, "text": " all kinds of problems.", "tokens": [51542, 439, 3685, 295, 2740, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2065, "seek": 828736, "start": 8312.52, "end": 8315.960000000001, "text": " If we move down to our second example, it's going to be the exact same pattern.", "tokens": [51622, 759, 321, 1286, 760, 281, 527, 1150, 1365, 11, 309, 311, 516, 281, 312, 264, 1900, 912, 5102, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2066, "seek": 828736, "start": 8316.28, "end": 8317.08, "text": " A little different.", "tokens": [51810, 316, 707, 819, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1428827598913392, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0010004013311117887}, {"id": 2067, "seek": 831708, "start": 8317.08, "end": 8319.64, "text": " Let me go ahead and move this over so you can see it a bit better.", "tokens": [50364, 961, 385, 352, 2286, 293, 1286, 341, 670, 370, 291, 393, 536, 309, 257, 857, 1101, 13, 50492], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2068, "seek": 831708, "start": 8320.84, "end": 8324.92, "text": " It's going to be regex that looks like this, where we just look for a sequence", "tokens": [50552, 467, 311, 516, 281, 312, 319, 432, 87, 300, 1542, 411, 341, 11, 689, 321, 445, 574, 337, 257, 8310, 50756], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2069, "seek": 831708, "start": 8324.92, "end": 8329.039999999999, "text": " of five digits, we execute that, we find it just fine.", "tokens": [50756, 295, 1732, 27011, 11, 321, 14483, 300, 11, 321, 915, 309, 445, 2489, 13, 50962], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2070, "seek": 831708, "start": 8329.08, "end": 8332.16, "text": " And the reason for that is because this does not have a dash.", "tokens": [50964, 400, 264, 1778, 337, 300, 307, 570, 341, 775, 406, 362, 257, 8240, 13, 51118], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2071, "seek": 831708, "start": 8332.52, "end": 8336.8, "text": " So regex, if you're familiar with it, if you've worked with it, it's very powerful.", "tokens": [51136, 407, 319, 432, 87, 11, 498, 291, 434, 4963, 365, 309, 11, 498, 291, 600, 2732, 365, 309, 11, 309, 311, 588, 4005, 13, 51350], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2072, "seek": 831708, "start": 8337.0, "end": 8338.88, "text": " You can do a lot of cool things.", "tokens": [51360, 509, 393, 360, 257, 688, 295, 1627, 721, 13, 51454], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2073, "seek": 831708, "start": 8339.32, "end": 8344.44, "text": " When you're going to use this in Python, if you're using just the standard", "tokens": [51476, 1133, 291, 434, 516, 281, 764, 341, 294, 15329, 11, 498, 291, 434, 1228, 445, 264, 3832, 51732], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2074, "seek": 831708, "start": 8344.44, "end": 8345.96, "text": " off the shelf components.", "tokens": [51732, 766, 264, 15222, 6677, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12461912459221439, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00022340822033584118}, {"id": 2075, "seek": 834596, "start": 8346.199999999999, "end": 8349.679999999998, "text": " So the entity ruler, the matcher, you're going to be using this when", "tokens": [50376, 407, 264, 13977, 19661, 11, 264, 2995, 260, 11, 291, 434, 516, 281, 312, 1228, 341, 562, 50550], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2076, "seek": 834596, "start": 8349.679999999998, "end": 8354.16, "text": " you want to match regex to a single token.", "tokens": [50550, 291, 528, 281, 2995, 319, 432, 87, 281, 257, 2167, 14862, 13, 50774], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2077, "seek": 834596, "start": 8354.439999999999, "end": 8359.48, "text": " So think about this, if you're looking for a word that starts off with a capital", "tokens": [50788, 407, 519, 466, 341, 11, 498, 291, 434, 1237, 337, 257, 1349, 300, 3719, 766, 365, 257, 4238, 51040], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2078, "seek": 834596, "start": 8359.48, "end": 8364.32, "text": " D, and you want to just grab all words that start with a capital D, that would", "tokens": [51040, 413, 11, 293, 291, 528, 281, 445, 4444, 439, 2283, 300, 722, 365, 257, 4238, 413, 11, 300, 576, 51282], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2079, "seek": 834596, "start": 8364.32, "end": 8368.359999999999, "text": " be an example of when you would want to use it in a standard off the shelf component.", "tokens": [51282, 312, 364, 1365, 295, 562, 291, 576, 528, 281, 764, 309, 294, 257, 3832, 766, 264, 15222, 6542, 13, 51484], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2080, "seek": 834596, "start": 8368.8, "end": 8371.519999999999, "text": " But that's not all you can do in spacey.", "tokens": [51506, 583, 300, 311, 406, 439, 291, 393, 360, 294, 1901, 88, 13, 51642], "temperature": 0.0, "avg_logprob": -0.11821230467375335, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.0017005401896312833}, {"id": 2081, "seek": 837152, "start": 8371.880000000001, "end": 8375.800000000001, "text": " You can use regex to actually capture multi word tokens.", "tokens": [50382, 509, 393, 764, 319, 432, 87, 281, 767, 7983, 4825, 1349, 22667, 13, 50578], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2082, "seek": 837152, "start": 8376.08, "end": 8378.32, "text": " So capture things like Mr.", "tokens": [50592, 407, 7983, 721, 411, 2221, 13, 50704], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2083, "seek": 837152, "start": 8378.4, "end": 8379.28, "text": " Deeds.", "tokens": [50708, 1346, 5147, 13, 50752], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2084, "seek": 837152, "start": 8379.48, "end": 8381.2, "text": " So any instance of Mr.", "tokens": [50762, 407, 604, 5197, 295, 2221, 13, 50848], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2085, "seek": 837152, "start": 8381.2, "end": 8384.800000000001, "text": " Period Space Name, a sequence of proper nouns.", "tokens": [50848, 34976, 8705, 13866, 11, 257, 8310, 295, 2296, 48184, 13, 51028], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2086, "seek": 837152, "start": 8385.68, "end": 8390.880000000001, "text": " You can also use it to, but yet in order to do that, you have to actually", "tokens": [51072, 509, 393, 611, 764, 309, 281, 11, 457, 1939, 294, 1668, 281, 360, 300, 11, 291, 362, 281, 767, 51332], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2087, "seek": 837152, "start": 8390.880000000001, "end": 8394.52, "text": " understand how to add in a custom component for it.", "tokens": [51332, 1223, 577, 281, 909, 294, 257, 2375, 6542, 337, 309, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2088, "seek": 837152, "start": 8394.800000000001, "end": 8399.32, "text": " And we're going to be seeing that in just a second as we move on to chapter nine,", "tokens": [51528, 400, 321, 434, 516, 281, 312, 2577, 300, 294, 445, 257, 1150, 382, 321, 1286, 322, 281, 7187, 4949, 11, 51754], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2089, "seek": 837152, "start": 8399.560000000001, "end": 8401.12, "text": " which is advanced regex.", "tokens": [51766, 597, 307, 7339, 319, 432, 87, 13, 51844], "temperature": 0.0, "avg_logprob": -0.18090045763098675, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.04335583373904228}, {"id": 2090, "seek": 840112, "start": 8401.320000000002, "end": 8405.36, "text": " If you're not familiar with regex at all, take a few minutes, read chapter eight.", "tokens": [50374, 759, 291, 434, 406, 4963, 365, 319, 432, 87, 412, 439, 11, 747, 257, 1326, 2077, 11, 1401, 7187, 3180, 13, 50576], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2091, "seek": 840112, "start": 8405.36, "end": 8410.800000000001, "text": " I encourage you to do so because I go over in detail and I talk about how to", "tokens": [50576, 286, 5373, 291, 281, 360, 370, 570, 286, 352, 670, 294, 2607, 293, 286, 751, 466, 577, 281, 50848], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2092, "seek": 840112, "start": 8410.800000000001, "end": 8415.08, "text": " actually engage in regex and Python and its strengths and weaknesses.", "tokens": [50848, 767, 4683, 294, 319, 432, 87, 293, 15329, 293, 1080, 16986, 293, 24381, 13, 51062], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2093, "seek": 840112, "start": 8415.44, "end": 8418.68, "text": " What I want you to really focus on though, and get away from, get from all this", "tokens": [51080, 708, 286, 528, 291, 281, 534, 1879, 322, 1673, 11, 293, 483, 1314, 490, 11, 483, 490, 439, 341, 51242], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2094, "seek": 840112, "start": 8418.92, "end": 8424.160000000002, "text": " is how to do some really complex multi word token matching with regex.", "tokens": [51254, 307, 577, 281, 360, 512, 534, 3997, 4825, 1349, 14862, 14324, 365, 319, 432, 87, 13, 51516], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2095, "seek": 840112, "start": 8424.160000000002, "end": 8427.44, "text": " Remember, you're going to want to use regular expressions when the pattern", "tokens": [51516, 5459, 11, 291, 434, 516, 281, 528, 281, 764, 3890, 15277, 562, 264, 5102, 51680], "temperature": 0.0, "avg_logprob": -0.13699869451851682, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007793019176460803}, {"id": 2096, "seek": 842744, "start": 8427.44, "end": 8433.84, "text": " matching that you want to do is unindependent of the, the lima, the POS,", "tokens": [50364, 14324, 300, 291, 528, 281, 360, 307, 517, 471, 4217, 317, 295, 264, 11, 264, 2364, 64, 11, 264, 430, 4367, 11, 50684], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2097, "seek": 842744, "start": 8433.84, "end": 8436.36, "text": " or any of the linguistic features that space is going to use.", "tokens": [50684, 420, 604, 295, 264, 43002, 4122, 300, 1901, 307, 516, 281, 764, 13, 50810], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2098, "seek": 842744, "start": 8436.68, "end": 8440.04, "text": " If you're working with linguistic features, you have to use the", "tokens": [50826, 759, 291, 434, 1364, 365, 43002, 4122, 11, 291, 362, 281, 764, 264, 50994], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2099, "seek": 842744, "start": 8440.04, "end": 8445.04, "text": " spacey pattern, pattern matching things like the morph, the orth, the lima,", "tokens": [50994, 1901, 88, 5102, 11, 5102, 14324, 721, 411, 264, 25778, 11, 264, 19052, 11, 264, 2364, 64, 11, 51244], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2100, "seek": 842744, "start": 8445.04, "end": 8445.84, "text": " things like that.", "tokens": [51244, 721, 411, 300, 13, 51284], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2101, "seek": 842744, "start": 8445.84, "end": 8450.4, "text": " But if your sequence of strings is not dependent on that, so you're looking", "tokens": [51284, 583, 498, 428, 8310, 295, 13985, 307, 406, 12334, 322, 300, 11, 370, 291, 434, 1237, 51512], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2102, "seek": 842744, "start": 8450.4, "end": 8453.84, "text": " for any instance of, in this case, we're going to talk about in just a second,", "tokens": [51512, 337, 604, 5197, 295, 11, 294, 341, 1389, 11, 321, 434, 516, 281, 751, 466, 294, 445, 257, 1150, 11, 51684], "temperature": 0.0, "avg_logprob": -0.17209815979003906, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.017982980236411095}, {"id": 2103, "seek": 845384, "start": 8454.24, "end": 8459.880000000001, "text": " a, a case where Paul is followed by a capitalized letter and then a word break.", "tokens": [50384, 257, 11, 257, 1389, 689, 4552, 307, 6263, 538, 257, 4238, 1602, 5063, 293, 550, 257, 1349, 1821, 13, 50666], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2104, "seek": 845384, "start": 8460.68, "end": 8463.64, "text": " Then you're going to want to use regular expressions because in this case,", "tokens": [50706, 1396, 291, 434, 516, 281, 528, 281, 764, 3890, 15277, 570, 294, 341, 1389, 11, 50854], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2105, "seek": 845384, "start": 8463.960000000001, "end": 8468.84, "text": " this is independent of any linguistic features and regular expressions", "tokens": [50870, 341, 307, 6695, 295, 604, 43002, 4122, 293, 3890, 15277, 51114], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2106, "seek": 845384, "start": 8468.84, "end": 8472.04, "text": " allows for you to write much more robust patterns, much more quickly.", "tokens": [51114, 4045, 337, 291, 281, 2464, 709, 544, 13956, 8294, 11, 709, 544, 2661, 13, 51274], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2107, "seek": 845384, "start": 8472.04, "end": 8474.960000000001, "text": " If you know how to use it well, and it allows for you to do much more", "tokens": [51274, 759, 291, 458, 577, 281, 764, 309, 731, 11, 293, 309, 4045, 337, 291, 281, 360, 709, 544, 51420], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2108, "seek": 845384, "start": 8474.960000000001, "end": 8478.44, "text": " quick robust things within a custom component.", "tokens": [51420, 1702, 13956, 721, 1951, 257, 2375, 6542, 13, 51594], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2109, "seek": 845384, "start": 8478.68, "end": 8481.0, "text": " And that's going to be where we move to now.", "tokens": [51606, 400, 300, 311, 516, 281, 312, 689, 321, 1286, 281, 586, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15364599227905273, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.035137638449668884}, {"id": 2110, "seek": 848100, "start": 8482.0, "end": 8485.4, "text": " Now that we know a little bit about regex and how it can be implemented in", "tokens": [50414, 823, 300, 321, 458, 257, 707, 857, 466, 319, 432, 87, 293, 577, 309, 393, 312, 12270, 294, 50584], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2111, "seek": 848100, "start": 8485.4, "end": 8490.0, "text": " Python, let's go ahead and also in spacey, let's go ahead and try and see", "tokens": [50584, 15329, 11, 718, 311, 352, 2286, 293, 611, 294, 1901, 88, 11, 718, 311, 352, 2286, 293, 853, 293, 536, 50814], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2112, "seek": 848100, "start": 8490.2, "end": 8497.4, "text": " how we can get regex to actually find multi word tokens for us within spacey", "tokens": [50824, 577, 321, 393, 483, 319, 432, 87, 281, 767, 915, 4825, 1349, 22667, 337, 505, 1951, 1901, 88, 51184], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2113, "seek": 848100, "start": 8497.6, "end": 8499.4, "text": " using everything in the spacey framework.", "tokens": [51194, 1228, 1203, 294, 264, 1901, 88, 8388, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2114, "seek": 848100, "start": 8499.6, "end": 8503.0, "text": " So the first thing I'm going to do to kind of demonstrate all this is I'm going", "tokens": [51294, 407, 264, 700, 551, 286, 478, 516, 281, 360, 281, 733, 295, 11698, 439, 341, 307, 286, 478, 516, 51464], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2115, "seek": 848100, "start": 8503.0, "end": 8505.0, "text": " to import regex.", "tokens": [51464, 281, 974, 319, 432, 87, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2116, "seek": 848100, "start": 8505.0, "end": 8509.4, "text": " This comes standard with Python and you can import it as RE just that way.", "tokens": [51564, 639, 1487, 3832, 365, 15329, 293, 291, 393, 974, 309, 382, 10869, 445, 300, 636, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2141951884119964, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.0018101277528330684}, {"id": 2117, "seek": 850940, "start": 8509.4, "end": 8512.4, "text": " Import RE and that's going to import regex.", "tokens": [50364, 26391, 10869, 293, 300, 311, 516, 281, 974, 319, 432, 87, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2118, "seek": 850940, "start": 8512.8, "end": 8516.8, "text": " I'm going to work from the textbook and work with this sample text.", "tokens": [50534, 286, 478, 516, 281, 589, 490, 264, 25591, 293, 589, 365, 341, 6889, 2487, 13, 50734], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2119, "seek": 850940, "start": 8517.0, "end": 8521.8, "text": " So this is Paul Newman was an American actor, but Paul Hollywood is a British TV", "tokens": [50744, 407, 341, 307, 4552, 49377, 390, 364, 2665, 8747, 11, 457, 4552, 11628, 307, 257, 6221, 3558, 50984], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2120, "seek": 850940, "start": 8522.0, "end": 8522.8, "text": " TV host.", "tokens": [50994, 3558, 3975, 13, 51034], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2121, "seek": 850940, "start": 8523.0, "end": 8525.0, "text": " The name Paul is quite common.", "tokens": [51044, 440, 1315, 4552, 307, 1596, 2689, 13, 51144], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2122, "seek": 850940, "start": 8525.199999999999, "end": 8529.0, "text": " So it's going to be the text that we work with throughout this entire chapter.", "tokens": [51154, 407, 309, 311, 516, 281, 312, 264, 2487, 300, 321, 589, 365, 3710, 341, 2302, 7187, 13, 51344], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2123, "seek": 850940, "start": 8529.4, "end": 8533.6, "text": " Now a regex pattern that I could write to capture all instances of things like", "tokens": [51364, 823, 257, 319, 432, 87, 5102, 300, 286, 727, 2464, 281, 7983, 439, 14519, 295, 721, 411, 51574], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2124, "seek": 850940, "start": 8533.6, "end": 8537.8, "text": " Paul Newman and Paul Hollywood, which is what my goal is, could look something", "tokens": [51574, 4552, 49377, 293, 4552, 11628, 11, 597, 307, 437, 452, 3387, 307, 11, 727, 574, 746, 51784], "temperature": 0.0, "avg_logprob": -0.15998241549632589, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.006096739787608385}, {"id": 2125, "seek": 853780, "start": 8538.4, "end": 8544.599999999999, "text": " like this, I could say or make an R string here and say Paul, and then I'm going", "tokens": [50394, 411, 341, 11, 286, 727, 584, 420, 652, 364, 497, 6798, 510, 293, 584, 4552, 11, 293, 550, 286, 478, 516, 50704], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2126, "seek": 853780, "start": 8544.599999999999, "end": 8548.199999999999, "text": " to grab everything that starts with a capital letter and then my grab", "tokens": [50704, 281, 4444, 1203, 300, 3719, 365, 257, 4238, 5063, 293, 550, 452, 4444, 50884], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2127, "seek": 853780, "start": 8548.4, "end": 8550.199999999999, "text": " everything until a word break.", "tokens": [50894, 1203, 1826, 257, 1349, 1821, 13, 50984], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2128, "seek": 853780, "start": 8550.4, "end": 8554.199999999999, "text": " And that's going to be a pattern that I can use in regex with this formula", "tokens": [50994, 400, 300, 311, 516, 281, 312, 257, 5102, 300, 286, 393, 764, 294, 319, 432, 87, 365, 341, 8513, 51184], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2129, "seek": 853780, "start": 8554.199999999999, "end": 8559.099999999999, "text": " means is find any instance of Paul proceeded by a in this case, a capital", "tokens": [51184, 1355, 307, 915, 604, 5197, 295, 4552, 39053, 538, 257, 294, 341, 1389, 11, 257, 4238, 51429], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2130, "seek": 853780, "start": 8559.099999999999, "end": 8561.9, "text": " letter until the actual word break.", "tokens": [51429, 5063, 1826, 264, 3539, 1349, 1821, 13, 51569], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2131, "seek": 853780, "start": 8561.9, "end": 8565.3, "text": " So grab the first name Paul and then what we can make a presumption is going", "tokens": [51569, 407, 4444, 264, 700, 1315, 4552, 293, 550, 437, 321, 393, 652, 257, 18028, 1695, 307, 516, 51739], "temperature": 0.0, "avg_logprob": -0.17878855829653534, "compression_ratio": 1.8081632653061224, "no_speech_prob": 0.004609194118529558}, {"id": 2132, "seek": 856530, "start": 8565.3, "end": 8569.699999999999, "text": " to be that individual's last name in the text, a simple example, but one", "tokens": [50364, 281, 312, 300, 2609, 311, 1036, 1315, 294, 264, 2487, 11, 257, 2199, 1365, 11, 457, 472, 50584], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2133, "seek": 856530, "start": 8569.699999999999, "end": 8572.4, "text": " that will demonstrate our kind of purpose right now.", "tokens": [50584, 300, 486, 11698, 527, 733, 295, 4334, 558, 586, 13, 50719], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2134, "seek": 856530, "start": 8572.699999999999, "end": 8577.099999999999, "text": " So how we can do this is we can create an object called matches and use regex", "tokens": [50734, 407, 577, 321, 393, 360, 341, 307, 321, 393, 1884, 364, 2657, 1219, 10676, 293, 764, 319, 432, 87, 50954], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2135, "seek": 856530, "start": 8577.099999999999, "end": 8583.199999999999, "text": " dot find it or we can pass in the pattern and we can pass in the text.", "tokens": [50954, 5893, 915, 309, 420, 321, 393, 1320, 294, 264, 5102, 293, 321, 393, 1320, 294, 264, 2487, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2136, "seek": 856530, "start": 8583.4, "end": 8587.099999999999, "text": " So what this is going to do is it's going to use regex to try to find this", "tokens": [51269, 407, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 764, 319, 432, 87, 281, 853, 281, 915, 341, 51454], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2137, "seek": 856530, "start": 8587.099999999999, "end": 8589.3, "text": " pattern within this text.", "tokens": [51454, 5102, 1951, 341, 2487, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2138, "seek": 856530, "start": 8589.5, "end": 8591.9, "text": " And then what we can do is we can iterate over those matches.", "tokens": [51574, 400, 550, 437, 321, 393, 360, 307, 321, 393, 44497, 670, 729, 10676, 13, 51694], "temperature": 0.0, "avg_logprob": -0.11166252295176188, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.005384296178817749}, {"id": 2139, "seek": 859190, "start": 8591.9, "end": 8599.199999999999, "text": " So for match and matches, we can grab and print off the match and we have", "tokens": [50364, 407, 337, 2995, 293, 10676, 11, 321, 393, 4444, 293, 4482, 766, 264, 2995, 293, 321, 362, 50729], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2140, "seek": 859190, "start": 8599.199999999999, "end": 8601.5, "text": " something that looks like this.", "tokens": [50729, 746, 300, 1542, 411, 341, 13, 50844], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2141, "seek": 859190, "start": 8603.1, "end": 8606.9, "text": " What we're looking at here is what we would call it a regex match object.", "tokens": [50924, 708, 321, 434, 1237, 412, 510, 307, 437, 321, 576, 818, 309, 257, 319, 432, 87, 2995, 2657, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2142, "seek": 859190, "start": 8607.1, "end": 8608.8, "text": " It's got a couple of different components here.", "tokens": [51124, 467, 311, 658, 257, 1916, 295, 819, 6677, 510, 13, 51209], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2143, "seek": 859190, "start": 8609.0, "end": 8614.1, "text": " It's got a span, which tells us the start character and the end character.", "tokens": [51219, 467, 311, 658, 257, 16174, 11, 597, 5112, 505, 264, 722, 2517, 293, 264, 917, 2517, 13, 51474], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2144, "seek": 859190, "start": 8615.199999999999, "end": 8619.5, "text": " And then it has a match and what this match means is the actual text itself.", "tokens": [51529, 400, 550, 309, 575, 257, 2995, 293, 437, 341, 2995, 1355, 307, 264, 3539, 2487, 2564, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12768078804016114, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0003569606924429536}, {"id": 2145, "seek": 861950, "start": 8619.7, "end": 8623.3, "text": " So the match here is Paul Newman and the match here is Paul Hollywood.", "tokens": [50374, 407, 264, 2995, 510, 307, 4552, 49377, 293, 264, 2995, 510, 307, 4552, 11628, 13, 50554], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2146, "seek": 861950, "start": 8623.5, "end": 8627.9, "text": " So we've been able to extract the two entities in the text that begin with", "tokens": [50564, 407, 321, 600, 668, 1075, 281, 8947, 264, 732, 16667, 294, 264, 2487, 300, 1841, 365, 50784], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2147, "seek": 861950, "start": 8627.9, "end": 8632.5, "text": " Paul and have a proper last name structured with a capital letter.", "tokens": [50784, 4552, 293, 362, 257, 2296, 1036, 1315, 18519, 365, 257, 4238, 5063, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2148, "seek": 861950, "start": 8632.7, "end": 8634.5, "text": " We grabbed everything up until the word break.", "tokens": [51024, 492, 18607, 1203, 493, 1826, 264, 1349, 1821, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2149, "seek": 861950, "start": 8634.9, "end": 8635.5, "text": " That's great.", "tokens": [51134, 663, 311, 869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2150, "seek": 861950, "start": 8635.7, "end": 8638.5, "text": " That's going to be what you need to know kind of going forward because what", "tokens": [51174, 663, 311, 516, 281, 312, 437, 291, 643, 281, 458, 733, 295, 516, 2128, 570, 437, 51314], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2151, "seek": 861950, "start": 8638.5, "end": 8643.2, "text": " we're going to do now is we're going to implement this in a custom spacey pipe.", "tokens": [51314, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 4445, 341, 294, 257, 2375, 1901, 88, 11240, 13, 51549], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2152, "seek": 861950, "start": 8643.4, "end": 8647.1, "text": " But first let's go through and write the code so that we can then easily kind", "tokens": [51559, 583, 700, 718, 311, 352, 807, 293, 2464, 264, 3089, 370, 300, 321, 393, 550, 3612, 733, 51744], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2153, "seek": 861950, "start": 8647.1, "end": 8648.6, "text": " of create the pipe afterwards.", "tokens": [51744, 295, 1884, 264, 11240, 10543, 13, 51819], "temperature": 0.0, "avg_logprob": -0.09051570471595316, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.0015011061914265156}, {"id": 2154, "seek": 864860, "start": 8649.4, "end": 8654.0, "text": " So what we need to do is we need to import spacey and we also need to say", "tokens": [50404, 407, 437, 321, 643, 281, 360, 307, 321, 643, 281, 974, 1901, 88, 293, 321, 611, 643, 281, 584, 50634], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2155, "seek": 864860, "start": 8654.0, "end": 8659.5, "text": " from spacey dot tokens import span and we're going to be importing a couple", "tokens": [50634, 490, 1901, 88, 5893, 22667, 974, 16174, 293, 321, 434, 516, 281, 312, 43866, 257, 1916, 50909], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2156, "seek": 864860, "start": 8659.5, "end": 8662.5, "text": " of different things as we move forward because we're going to see that we're", "tokens": [50909, 295, 819, 721, 382, 321, 1286, 2128, 570, 321, 434, 516, 281, 536, 300, 321, 434, 51059], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2157, "seek": 864860, "start": 8662.5, "end": 8664.300000000001, "text": " going to make a couple of mistakes intentionally.", "tokens": [51059, 516, 281, 652, 257, 1916, 295, 8038, 22062, 13, 51149], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2158, "seek": 864860, "start": 8664.300000000001, "end": 8667.4, "text": " I'm going to show you how to kind of address these common mistakes that might", "tokens": [51149, 286, 478, 516, 281, 855, 291, 577, 281, 733, 295, 2985, 613, 2689, 8038, 300, 1062, 51304], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2159, "seek": 864860, "start": 8667.4, "end": 8669.9, "text": " surface in trying to do something like this.", "tokens": [51304, 3753, 294, 1382, 281, 360, 746, 411, 341, 13, 51429], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2160, "seek": 864860, "start": 8670.4, "end": 8673.9, "text": " So once we've imported those two things, we can start actually writing out our", "tokens": [51454, 407, 1564, 321, 600, 25524, 729, 732, 721, 11, 321, 393, 722, 767, 3579, 484, 527, 51629], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2161, "seek": 864860, "start": 8673.9, "end": 8674.300000000001, "text": " code.", "tokens": [51629, 3089, 13, 51649], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2162, "seek": 864860, "start": 8674.6, "end": 8677.300000000001, "text": " Again, we're going to stick with the exact same text and again, we're going", "tokens": [51664, 3764, 11, 321, 434, 516, 281, 2897, 365, 264, 1900, 912, 2487, 293, 797, 11, 321, 434, 516, 51799], "temperature": 0.0, "avg_logprob": -0.1004528356782088, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0008829786092974246}, {"id": 2163, "seek": 867730, "start": 8677.3, "end": 8681.9, "text": " to stick with the exact same pattern that we've got stored in memory up above.", "tokens": [50364, 281, 2897, 365, 264, 1900, 912, 5102, 300, 321, 600, 658, 12187, 294, 4675, 493, 3673, 13, 50594], "temperature": 0.0, "avg_logprob": -0.1003444587791359, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0016483431681990623}, {"id": 2164, "seek": 867730, "start": 8682.599999999999, "end": 8686.5, "text": " So what we need to do now is we need to create a blank spacey object or sorry,", "tokens": [50629, 407, 437, 321, 643, 281, 360, 586, 307, 321, 643, 281, 1884, 257, 8247, 1901, 88, 2657, 420, 2597, 11, 50824], "temperature": 0.0, "avg_logprob": -0.1003444587791359, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0016483431681990623}, {"id": 2165, "seek": 867730, "start": 8686.5, "end": 8690.699999999999, "text": " a blank spacey pipeline that we can kind of put all this information into.", "tokens": [50824, 257, 8247, 1901, 88, 15517, 300, 321, 393, 733, 295, 829, 439, 341, 1589, 666, 13, 51034], "temperature": 0.0, "avg_logprob": -0.1003444587791359, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0016483431681990623}, {"id": 2166, "seek": 867730, "start": 8692.199999999999, "end": 8697.5, "text": " And for right now what we're going to do is we're just going to kind of go", "tokens": [51109, 400, 337, 558, 586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 445, 516, 281, 733, 295, 352, 51374], "temperature": 0.0, "avg_logprob": -0.1003444587791359, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0016483431681990623}, {"id": 2167, "seek": 867730, "start": 8697.5, "end": 8700.3, "text": " through and look at these individual entities.", "tokens": [51374, 807, 293, 574, 412, 613, 2609, 16667, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1003444587791359, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0016483431681990623}, {"id": 2168, "seek": 870730, "start": 8708.3, "end": 8716.099999999999, "text": " So again, we're going to create the doc object, which is going to be equal to", "tokens": [50414, 407, 797, 11, 321, 434, 516, 281, 1884, 264, 3211, 2657, 11, 597, 307, 516, 281, 312, 2681, 281, 50804], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2169, "seek": 870730, "start": 8716.099999999999, "end": 8721.5, "text": " nlp text and this is not going to be necessary for right now, but I'm", "tokens": [50804, 297, 75, 79, 2487, 293, 341, 307, 406, 516, 281, 312, 4818, 337, 558, 586, 11, 457, 286, 478, 51074], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2170, "seek": 870730, "start": 8721.5, "end": 8725.3, "text": " establishing a kind of a consistent workflow for us and you're going to see", "tokens": [51074, 22494, 257, 733, 295, 257, 8398, 20993, 337, 505, 293, 291, 434, 516, 281, 536, 51264], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2171, "seek": 870730, "start": 8725.3, "end": 8728.4, "text": " how we kind of take all this and implement it inside of a pipeline.", "tokens": [51264, 577, 321, 733, 295, 747, 439, 341, 293, 4445, 309, 1854, 295, 257, 15517, 13, 51419], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2172, "seek": 870730, "start": 8728.699999999999, "end": 8732.199999999999, "text": " So we're going to say original ends is equal to list doc dot ends.", "tokens": [51434, 407, 321, 434, 516, 281, 584, 3380, 5314, 307, 2681, 281, 1329, 3211, 5893, 5314, 13, 51609], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2173, "seek": 870730, "start": 8732.199999999999, "end": 8735.5, "text": " Now in this scenario, there's not going to be any entities because we don't", "tokens": [51609, 823, 294, 341, 9005, 11, 456, 311, 406, 516, 281, 312, 604, 16667, 570, 321, 500, 380, 51774], "temperature": 0.0, "avg_logprob": -0.14619911857273268, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0007793247350491583}, {"id": 2174, "seek": 873550, "start": 8735.5, "end": 8740.2, "text": " have an any R or an entity ruler in our blank spacey pipeline.", "tokens": [50364, 362, 364, 604, 497, 420, 364, 13977, 19661, 294, 527, 8247, 1901, 88, 15517, 13, 50599], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2175, "seek": 873550, "start": 8741.1, "end": 8743.8, "text": " What we're going to do next is we're going to create something called an", "tokens": [50644, 708, 321, 434, 516, 281, 360, 958, 307, 321, 434, 516, 281, 1884, 746, 1219, 364, 50779], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2176, "seek": 873550, "start": 8743.8, "end": 8749.0, "text": " nwt int and that's going to stand for multi word token entity.", "tokens": [50779, 297, 86, 83, 560, 293, 300, 311, 516, 281, 1463, 337, 4825, 1349, 14862, 13977, 13, 51039], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2177, "seek": 873550, "start": 8750.0, "end": 8751.4, "text": " You can name this whatever you like.", "tokens": [51089, 509, 393, 1315, 341, 2035, 291, 411, 13, 51159], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2178, "seek": 873550, "start": 8751.4, "end": 8754.8, "text": " This is just what I kind of stick to and then we're going to do and this", "tokens": [51159, 639, 307, 445, 437, 286, 733, 295, 2897, 281, 293, 550, 321, 434, 516, 281, 360, 293, 341, 51329], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2179, "seek": 873550, "start": 8754.8, "end": 8756.6, "text": " is straight from the spacey documentation.", "tokens": [51329, 307, 2997, 490, 264, 1901, 88, 14333, 13, 51419], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2180, "seek": 873550, "start": 8756.9, "end": 8760.1, "text": " We're going to say for match an RE dot find it or the same thing", "tokens": [51434, 492, 434, 516, 281, 584, 337, 2995, 364, 10869, 5893, 915, 309, 420, 264, 912, 551, 51594], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2181, "seek": 873550, "start": 8760.1, "end": 8764.2, "text": " that we saw above pattern doc dot text.", "tokens": [51594, 300, 321, 1866, 3673, 5102, 3211, 5893, 2487, 13, 51799], "temperature": 0.0, "avg_logprob": -0.15176988417102444, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.009707233868539333}, {"id": 2182, "seek": 876420, "start": 8764.5, "end": 8767.300000000001, "text": " So what this is going to do is it's going to take that doc object.", "tokens": [50379, 407, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 747, 300, 3211, 2657, 13, 50519], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2183, "seek": 876420, "start": 8768.2, "end": 8772.0, "text": " Look at it as raw text because remember the doc object is a container", "tokens": [50564, 2053, 412, 309, 382, 8936, 2487, 570, 1604, 264, 3211, 2657, 307, 257, 10129, 50754], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2184, "seek": 876420, "start": 8772.300000000001, "end": 8776.2, "text": " that doesn't actually have raw text in it until you actually call the dot", "tokens": [50769, 300, 1177, 380, 767, 362, 8936, 2487, 294, 309, 1826, 291, 767, 818, 264, 5893, 50964], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2185, "seek": 876420, "start": 8776.2, "end": 8779.900000000001, "text": " text attribute and then our goal is for each of these things.", "tokens": [50964, 2487, 19667, 293, 550, 527, 3387, 307, 337, 1184, 295, 613, 721, 13, 51149], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2186, "seek": 876420, "start": 8779.900000000001, "end": 8783.2, "text": " We're going to look and call in this span.", "tokens": [51149, 492, 434, 516, 281, 574, 293, 818, 294, 341, 16174, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2187, "seek": 876420, "start": 8783.5, "end": 8789.300000000001, "text": " So we're going to say is start and the end is equal to match dot span.", "tokens": [51329, 407, 321, 434, 516, 281, 584, 307, 722, 293, 264, 917, 307, 2681, 281, 2995, 5893, 16174, 13, 51619], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2188, "seek": 876420, "start": 8789.6, "end": 8793.2, "text": " So what we're doing here is we're going in and grabbing the span attribute", "tokens": [51634, 407, 437, 321, 434, 884, 510, 307, 321, 434, 516, 294, 293, 23771, 264, 16174, 19667, 51814], "temperature": 0.0, "avg_logprob": -0.09121870994567871, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.0004442022764123976}, {"id": 2189, "seek": 879320, "start": 8793.7, "end": 8797.0, "text": " and we're grabbing these two components the start and the end.", "tokens": [50389, 293, 321, 434, 23771, 613, 732, 6677, 264, 722, 293, 264, 917, 13, 50554], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2190, "seek": 879320, "start": 8797.2, "end": 8798.1, "text": " But we have a problem.", "tokens": [50564, 583, 321, 362, 257, 1154, 13, 50609], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2191, "seek": 879320, "start": 8798.400000000001, "end": 8800.400000000001, "text": " These are character spans.", "tokens": [50624, 1981, 366, 2517, 44086, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2192, "seek": 879320, "start": 8800.400000000001, "end": 8803.6, "text": " Remember the doc object works on a token level.", "tokens": [50724, 5459, 264, 3211, 2657, 1985, 322, 257, 14862, 1496, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2193, "seek": 879320, "start": 8803.800000000001, "end": 8807.300000000001, "text": " So we've got to kind of figure out a way to reverse engineer this almost", "tokens": [50894, 407, 321, 600, 658, 281, 733, 295, 2573, 484, 257, 636, 281, 9943, 11403, 341, 1920, 51069], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2194, "seek": 879320, "start": 8807.5, "end": 8810.2, "text": " to actually get this into a spacey form.", "tokens": [51079, 281, 767, 483, 341, 666, 257, 1901, 88, 1254, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2195, "seek": 879320, "start": 8810.400000000001, "end": 8815.300000000001, "text": " Fortunately the doc object also has an attribute called character span.", "tokens": [51224, 20652, 264, 3211, 2657, 611, 575, 364, 19667, 1219, 2517, 16174, 13, 51469], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2196, "seek": 879320, "start": 8816.0, "end": 8821.5, "text": " So what we can do is we can say the span is equal to doc dot char span", "tokens": [51504, 407, 437, 321, 393, 360, 307, 321, 393, 584, 264, 16174, 307, 2681, 281, 3211, 5893, 1290, 16174, 51779], "temperature": 0.0, "avg_logprob": -0.1278810324492278, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.0004172952612861991}, {"id": 2197, "seek": 882150, "start": 8822.5, "end": 8823.7, "text": " start and end.", "tokens": [50414, 722, 293, 917, 13, 50474], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2198, "seek": 882150, "start": 8823.7, "end": 8827.1, "text": " So what this is going to do is it's going to print off essentially for us.", "tokens": [50474, 407, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 4482, 766, 4476, 337, 505, 13, 50644], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2199, "seek": 882150, "start": 8827.3, "end": 8828.8, "text": " Let's go ahead and do that.", "tokens": [50654, 961, 311, 352, 2286, 293, 360, 300, 13, 50729], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2200, "seek": 882150, "start": 8829.1, "end": 8832.0, "text": " It would print off for us where we worry to actually have an entity here.", "tokens": [50744, 467, 576, 4482, 766, 337, 505, 689, 321, 3292, 281, 767, 362, 364, 13977, 510, 13, 50889], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2201, "seek": 882150, "start": 8832.6, "end": 8836.4, "text": " It would print off for us as we can see Paul Newman and Paul Hollywood.", "tokens": [50919, 467, 576, 4482, 766, 337, 505, 382, 321, 393, 536, 4552, 49377, 293, 4552, 11628, 13, 51109], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2202, "seek": 882150, "start": 8836.7, "end": 8842.7, "text": " So what we need to do now is we need to get this span into our entities.", "tokens": [51124, 407, 437, 321, 643, 281, 360, 586, 307, 321, 643, 281, 483, 341, 16174, 666, 527, 16667, 13, 51424], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2203, "seek": 882150, "start": 8843.4, "end": 8849.1, "text": " So what we can do is instead of printing things off we can say if span is not", "tokens": [51459, 407, 437, 321, 393, 360, 307, 2602, 295, 14699, 721, 766, 321, 393, 584, 498, 16174, 307, 406, 51744], "temperature": 0.0, "avg_logprob": -0.11357717346726802, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0010321959853172302}, {"id": 2204, "seek": 884910, "start": 8849.1, "end": 8852.1, "text": " none because in some instance instances this will be the case.", "tokens": [50364, 6022, 570, 294, 512, 5197, 14519, 341, 486, 312, 264, 1389, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2205, "seek": 884910, "start": 8852.5, "end": 8855.9, "text": " You're going to say NWT ends dot append.", "tokens": [50534, 509, 434, 516, 281, 584, 426, 54, 51, 5314, 5893, 34116, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2206, "seek": 884910, "start": 8856.7, "end": 8863.6, "text": " You're going to append a tuple here span dot start span dot end span dot", "tokens": [50744, 509, 434, 516, 281, 34116, 257, 2604, 781, 510, 16174, 5893, 722, 16174, 5893, 917, 16174, 5893, 51089], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2207, "seek": 884910, "start": 8863.6, "end": 8864.300000000001, "text": " text.", "tokens": [51089, 2487, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2208, "seek": 884910, "start": 8864.4, "end": 8868.6, "text": " So this is going to be the start the end and the text itself.", "tokens": [51129, 407, 341, 307, 516, 281, 312, 264, 722, 264, 917, 293, 264, 2487, 2564, 13, 51339], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2209, "seek": 884910, "start": 8869.1, "end": 8873.9, "text": " And once we've done that we've managed to get our multi word tokens into", "tokens": [51364, 400, 1564, 321, 600, 1096, 300, 321, 600, 6453, 281, 483, 527, 4825, 1349, 22667, 666, 51604], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2210, "seek": 884910, "start": 8875.1, "end": 8877.1, "text": " a list that looks like this.", "tokens": [51664, 257, 1329, 300, 1542, 411, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1784455997427714, "compression_ratio": 1.7563451776649746, "no_speech_prob": 0.0037069981917738914}, {"id": 2211, "seek": 887710, "start": 8878.1, "end": 8886.6, "text": " Start and Paul Newman Paul Hollywood and notice that our span dot start is", "tokens": [50414, 6481, 293, 4552, 49377, 4552, 11628, 293, 3449, 300, 527, 16174, 5893, 722, 307, 50839], "temperature": 0.0, "avg_logprob": -0.14558031717936198, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0008830050937831402}, {"id": 2212, "seek": 887710, "start": 8886.6, "end": 8889.9, "text": " aligning not with a character span.", "tokens": [50839, 419, 9676, 406, 365, 257, 2517, 16174, 13, 51004], "temperature": 0.0, "avg_logprob": -0.14558031717936198, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0008830050937831402}, {"id": 2213, "seek": 887710, "start": 8889.9, "end": 8894.2, "text": " Now it's rather aligning with a token span.", "tokens": [51004, 823, 309, 311, 2831, 419, 9676, 365, 257, 14862, 16174, 13, 51219], "temperature": 0.0, "avg_logprob": -0.14558031717936198, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0008830050937831402}, {"id": 2214, "seek": 887710, "start": 8894.4, "end": 8897.800000000001, "text": " So what we've done is we've taken this character span here and been able to", "tokens": [51229, 407, 437, 321, 600, 1096, 307, 321, 600, 2726, 341, 2517, 16174, 510, 293, 668, 1075, 281, 51399], "temperature": 0.0, "avg_logprob": -0.14558031717936198, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0008830050937831402}, {"id": 2215, "seek": 887710, "start": 8897.800000000001, "end": 8903.6, "text": " find out where they start and end within the the token sequence.", "tokens": [51399, 915, 484, 689, 436, 722, 293, 917, 1951, 264, 264, 14862, 8310, 13, 51689], "temperature": 0.0, "avg_logprob": -0.14558031717936198, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0008830050937831402}, {"id": 2216, "seek": 890360, "start": 8903.6, "end": 8904.9, "text": " So we have zero and two.", "tokens": [50364, 407, 321, 362, 4018, 293, 732, 13, 50429], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2217, "seek": 890360, "start": 8905.300000000001, "end": 8908.300000000001, "text": " So Paul Newman one this was the zero index.", "tokens": [50449, 407, 4552, 49377, 472, 341, 390, 264, 4018, 8186, 13, 50599], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2218, "seek": 890360, "start": 8908.300000000001, "end": 8910.2, "text": " It goes up until the second index.", "tokens": [50599, 467, 1709, 493, 1826, 264, 1150, 8186, 13, 50694], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2219, "seek": 890360, "start": 8910.4, "end": 8914.300000000001, "text": " So it grabs index token zero and token one and we've done the same thing", "tokens": [50704, 407, 309, 30028, 8186, 14862, 4018, 293, 14862, 472, 293, 321, 600, 1096, 264, 912, 551, 50899], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2220, "seek": 890360, "start": 8914.300000000001, "end": 8915.300000000001, "text": " with Paul Hollywood.", "tokens": [50899, 365, 4552, 11628, 13, 50949], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2221, "seek": 890360, "start": 8915.7, "end": 8917.0, "text": " Now we've got that data.", "tokens": [50969, 823, 321, 600, 658, 300, 1412, 13, 51034], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2222, "seek": 890360, "start": 8917.2, "end": 8924.9, "text": " We can actually start to inject these entities into our original entities.", "tokens": [51044, 492, 393, 767, 722, 281, 10711, 613, 16667, 666, 527, 3380, 16667, 13, 51429], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2223, "seek": 890360, "start": 8924.9, "end": 8926.6, "text": " So let's go through and do that right now.", "tokens": [51429, 407, 718, 311, 352, 807, 293, 360, 300, 558, 586, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2224, "seek": 890360, "start": 8927.1, "end": 8930.2, "text": " So we can do once we've got these things appended to this list.", "tokens": [51539, 407, 321, 393, 360, 1564, 321, 600, 658, 613, 721, 724, 3502, 281, 341, 1329, 13, 51694], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2225, "seek": 890360, "start": 8930.2, "end": 8933.2, "text": " We can start injecting them into our original entities.", "tokens": [51694, 492, 393, 722, 10711, 278, 552, 666, 527, 3380, 16667, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11771435546875, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.0024725045077502728}, {"id": 2226, "seek": 893320, "start": 8933.2, "end": 8937.300000000001, "text": " So we can say for end in MWT ends.", "tokens": [50364, 407, 321, 393, 584, 337, 917, 294, 376, 54, 51, 5314, 13, 50569], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2227, "seek": 893320, "start": 8937.800000000001, "end": 8941.5, "text": " What we want to do is we want to say the start the end and the name is equal", "tokens": [50594, 708, 321, 528, 281, 360, 307, 321, 528, 281, 584, 264, 722, 264, 917, 293, 264, 1315, 307, 2681, 50779], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2228, "seek": 893320, "start": 8941.5, "end": 8945.900000000001, "text": " to end because this is going to correspond to the tuple the start the", "tokens": [50779, 281, 917, 570, 341, 307, 516, 281, 6805, 281, 264, 2604, 781, 264, 722, 264, 50999], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2229, "seek": 893320, "start": 8945.900000000001, "end": 8948.7, "text": " end and the entity text.", "tokens": [50999, 917, 293, 264, 13977, 2487, 13, 51139], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2230, "seek": 893320, "start": 8950.0, "end": 8952.0, "text": " Now what we can do is we can say per end.", "tokens": [51204, 823, 437, 321, 393, 360, 307, 321, 393, 584, 680, 917, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2231, "seek": 893320, "start": 8952.0, "end": 8953.6, "text": " So this is going to be the individual end.", "tokens": [51304, 407, 341, 307, 516, 281, 312, 264, 2609, 917, 13, 51384], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2232, "seek": 893320, "start": 8953.6, "end": 8956.800000000001, "text": " We're going to create a span object in spacey.", "tokens": [51384, 492, 434, 516, 281, 1884, 257, 16174, 2657, 294, 1901, 88, 13, 51544], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2233, "seek": 893320, "start": 8957.900000000001, "end": 8959.0, "text": " It's going to look like this.", "tokens": [51599, 467, 311, 516, 281, 574, 411, 341, 13, 51654], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2234, "seek": 893320, "start": 8959.300000000001, "end": 8960.7, "text": " So a capital S here.", "tokens": [51669, 407, 257, 4238, 318, 510, 13, 51739], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2235, "seek": 893320, "start": 8960.7, "end": 8962.800000000001, "text": " Remember we imported it right up here.", "tokens": [51739, 5459, 321, 25524, 309, 558, 493, 510, 13, 51844], "temperature": 0.0, "avg_logprob": -0.13524718651404746, "compression_ratio": 1.829059829059829, "no_speech_prob": 0.00032501565874554217}, {"id": 2236, "seek": 896280, "start": 8963.0, "end": 8967.3, "text": " This is where we're going to be working with the span class and this is", "tokens": [50374, 639, 307, 689, 321, 434, 516, 281, 312, 1364, 365, 264, 16174, 1508, 293, 341, 307, 50589], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2237, "seek": 896280, "start": 8967.3, "end": 8971.3, "text": " going to create for us a span object that we can now safely inject into", "tokens": [50589, 516, 281, 1884, 337, 505, 257, 16174, 2657, 300, 321, 393, 586, 11750, 10711, 666, 50789], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2238, "seek": 896280, "start": 8971.3, "end": 8973.8, "text": " the spacey doc.ins list.", "tokens": [50789, 264, 1901, 88, 3211, 13, 1292, 1329, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2239, "seek": 896280, "start": 8974.3, "end": 8979.8, "text": " So we can say doc start and label and this is going to be the label that", "tokens": [50939, 407, 321, 393, 584, 3211, 722, 293, 7645, 293, 341, 307, 516, 281, 312, 264, 7645, 300, 51214], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2240, "seek": 896280, "start": 8979.8, "end": 8983.3, "text": " we want to actually assign it and this is going to be person in this", "tokens": [51214, 321, 528, 281, 767, 6269, 309, 293, 341, 307, 516, 281, 312, 954, 294, 341, 51389], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2241, "seek": 896280, "start": 8983.3, "end": 8988.099999999999, "text": " case because these are all people we can do now as we can go through and", "tokens": [51389, 1389, 570, 613, 366, 439, 561, 321, 393, 360, 586, 382, 321, 393, 352, 807, 293, 51629], "temperature": 0.0, "avg_logprob": -0.12819320376556698, "compression_ratio": 1.896039603960396, "no_speech_prob": 0.0005527233588509262}, {"id": 2242, "seek": 898810, "start": 8988.1, "end": 8992.9, "text": " say doc we can inject this into the original ends.", "tokens": [50364, 584, 3211, 321, 393, 10711, 341, 666, 264, 3380, 5314, 13, 50604], "temperature": 0.0, "avg_logprob": -0.21809008542229147, "compression_ratio": 1.64375, "no_speech_prob": 0.0049051851965487}, {"id": 2243, "seek": 898810, "start": 8996.1, "end": 9002.5, "text": " Original ins dot append and we're going to append the per end which is", "tokens": [50764, 30022, 1028, 5893, 34116, 293, 321, 434, 516, 281, 34116, 264, 680, 917, 597, 307, 51084], "temperature": 0.0, "avg_logprob": -0.21809008542229147, "compression_ratio": 1.64375, "no_speech_prob": 0.0049051851965487}, {"id": 2244, "seek": 898810, "start": 9002.5, "end": 9008.9, "text": " going to be this span object and finally what we can say is doc.ins is", "tokens": [51084, 516, 281, 312, 341, 16174, 2657, 293, 2721, 437, 321, 393, 584, 307, 3211, 13, 1292, 307, 51404], "temperature": 0.0, "avg_logprob": -0.21809008542229147, "compression_ratio": 1.64375, "no_speech_prob": 0.0049051851965487}, {"id": 2245, "seek": 898810, "start": 9008.9, "end": 9014.5, "text": " equal to original ends kind of like what we saw just a few moments ago", "tokens": [51404, 2681, 281, 3380, 5314, 733, 295, 411, 437, 321, 1866, 445, 257, 1326, 6065, 2057, 51684], "temperature": 0.0, "avg_logprob": -0.21809008542229147, "compression_ratio": 1.64375, "no_speech_prob": 0.0049051851965487}, {"id": 2246, "seek": 901450, "start": 9015.3, "end": 9017.2, "text": " and let's go ahead and print off.", "tokens": [50404, 293, 718, 311, 352, 2286, 293, 4482, 766, 13, 50499], "temperature": 0.0, "avg_logprob": -0.16835802654887355, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.001754517201334238}, {"id": 2247, "seek": 901450, "start": 9023.1, "end": 9026.6, "text": " We've got our entities right there or we to do this up here when we first", "tokens": [50794, 492, 600, 658, 527, 16667, 558, 456, 420, 321, 281, 360, 341, 493, 510, 562, 321, 700, 50969], "temperature": 0.0, "avg_logprob": -0.16835802654887355, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.001754517201334238}, {"id": 2248, "seek": 901450, "start": 9026.6, "end": 9030.9, "text": " kind of create the doc object you'll see nothing an empty list but now", "tokens": [50969, 733, 295, 1884, 264, 3211, 2657, 291, 603, 536, 1825, 364, 6707, 1329, 457, 586, 51184], "temperature": 0.0, "avg_logprob": -0.16835802654887355, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.001754517201334238}, {"id": 2249, "seek": 901450, "start": 9030.9, "end": 9036.3, "text": " what we've been able to do is inject these into the doc object the doc.ins", "tokens": [51184, 437, 321, 600, 668, 1075, 281, 360, 307, 10711, 613, 666, 264, 3211, 2657, 264, 3211, 13, 1292, 51454], "temperature": 0.0, "avg_logprob": -0.16835802654887355, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.001754517201334238}, {"id": 2250, "seek": 901450, "start": 9036.3, "end": 9040.3, "text": " attribute and we can say for end and doc.ins just like everything else", "tokens": [51454, 19667, 293, 321, 393, 584, 337, 917, 293, 3211, 13, 1292, 445, 411, 1203, 1646, 51654], "temperature": 0.0, "avg_logprob": -0.16835802654887355, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.001754517201334238}, {"id": 2251, "seek": 904030, "start": 9040.3, "end": 9045.199999999999, "text": " and dot text and dot label and because we converted it into a span we", "tokens": [50364, 293, 5893, 2487, 293, 5893, 7645, 293, 570, 321, 16424, 309, 666, 257, 16174, 321, 50609], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2252, "seek": 904030, "start": 9045.199999999999, "end": 9050.9, "text": " were able to inject it into the entity attribute from the doc object kind", "tokens": [50609, 645, 1075, 281, 10711, 309, 666, 264, 13977, 19667, 490, 264, 3211, 2657, 733, 50894], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2253, "seek": 904030, "start": 9050.9, "end": 9053.4, "text": " of natively so that spacey can actually understand it.", "tokens": [50894, 295, 8470, 356, 370, 300, 1901, 88, 393, 767, 1223, 309, 13, 51019], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2254, "seek": 904030, "start": 9053.8, "end": 9057.099999999999, "text": " So what can we do with this well one of the things that we could do is", "tokens": [51039, 407, 437, 393, 321, 360, 365, 341, 731, 472, 295, 264, 721, 300, 321, 727, 360, 307, 51204], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2255, "seek": 904030, "start": 9057.099999999999, "end": 9060.199999999999, "text": " we can use the knowledge that we just acquired about custom components", "tokens": [51204, 321, 393, 764, 264, 3601, 300, 321, 445, 17554, 466, 2375, 6677, 51359], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2256, "seek": 904030, "start": 9060.5, "end": 9064.0, "text": " and build a custom component around all of this.", "tokens": [51374, 293, 1322, 257, 2375, 6542, 926, 439, 295, 341, 13, 51549], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2257, "seek": 904030, "start": 9064.099999999999, "end": 9067.4, "text": " So how might we do that well let's go through and try it out.", "tokens": [51554, 407, 577, 1062, 321, 360, 300, 731, 718, 311, 352, 807, 293, 853, 309, 484, 13, 51719], "temperature": 0.0, "avg_logprob": -0.12867076056344168, "compression_ratio": 1.804, "no_speech_prob": 0.0039450014010071754}, {"id": 2258, "seek": 906740, "start": 9068.4, "end": 9074.6, "text": " The first thing that we need to do is we need to import our language class so", "tokens": [50414, 440, 700, 551, 300, 321, 643, 281, 360, 307, 321, 643, 281, 974, 527, 2856, 1508, 370, 50724], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2259, "seek": 906740, "start": 9074.6, "end": 9077.6, "text": " if you remember from a few moments ago whenever you need to work with a", "tokens": [50724, 498, 291, 1604, 490, 257, 1326, 6065, 2057, 5699, 291, 643, 281, 589, 365, 257, 50874], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2260, "seek": 906740, "start": 9077.6, "end": 9084.6, "text": " custom component you need to say from spacey dot language import language", "tokens": [50874, 2375, 6542, 291, 643, 281, 584, 490, 1901, 88, 5893, 2856, 974, 2856, 51224], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2261, "seek": 906740, "start": 9084.6, "end": 9088.1, "text": " with a capital L what we're going to do now is we're going to take the code", "tokens": [51224, 365, 257, 4238, 441, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 747, 264, 3089, 51399], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2262, "seek": 906740, "start": 9088.1, "end": 9091.5, "text": " that we just wrote and we're going to try to convert that into an actual", "tokens": [51399, 300, 321, 445, 4114, 293, 321, 434, 516, 281, 853, 281, 7620, 300, 666, 364, 3539, 51569], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2263, "seek": 906740, "start": 9091.5, "end": 9096.699999999999, "text": " custom pipe that can fit inside of our pipeline as kind of our own custom", "tokens": [51569, 2375, 11240, 300, 393, 3318, 1854, 295, 527, 15517, 382, 733, 295, 527, 1065, 2375, 51829], "temperature": 0.0, "avg_logprob": -0.12312004544319363, "compression_ratio": 1.9307359307359306, "no_speech_prob": 0.0005883891717530787}, {"id": 2264, "seek": 909670, "start": 9096.7, "end": 9097.900000000001, "text": " entity ruler if you will.", "tokens": [50364, 13977, 19661, 498, 291, 486, 13, 50424], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2265, "seek": 909670, "start": 9098.7, "end": 9101.7, "text": " So what we're going to do now is we're going to call this language dot", "tokens": [50464, 407, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 818, 341, 2856, 5893, 50614], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2266, "seek": 909670, "start": 9101.7, "end": 9107.5, "text": " component and we're going to call this let's call this Paul NER something", "tokens": [50614, 6542, 293, 321, 434, 516, 281, 818, 341, 718, 311, 818, 341, 4552, 426, 1598, 746, 50904], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2267, "seek": 909670, "start": 9107.6, "end": 9111.400000000001, "text": " not too not too clever but kind of very descriptive we're going to call", "tokens": [50909, 406, 886, 406, 886, 13494, 457, 733, 295, 588, 42585, 321, 434, 516, 281, 818, 51099], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2268, "seek": 909670, "start": 9111.400000000001, "end": 9116.0, "text": " this Paul NER and this is going to take that single doc object because", "tokens": [51099, 341, 4552, 426, 1598, 293, 341, 307, 516, 281, 747, 300, 2167, 3211, 2657, 570, 51329], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2269, "seek": 909670, "start": 9116.0, "end": 9119.800000000001, "text": " remember this pipe needs to receive the doc object and do stuff to it.", "tokens": [51329, 1604, 341, 11240, 2203, 281, 4774, 264, 3211, 2657, 293, 360, 1507, 281, 309, 13, 51519], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2270, "seek": 909670, "start": 9120.2, "end": 9123.1, "text": " So what we can do is we can take all this code that we just wrote.", "tokens": [51539, 407, 437, 321, 393, 360, 307, 321, 393, 747, 439, 341, 3089, 300, 321, 445, 4114, 13, 51684], "temperature": 0.0, "avg_logprob": -0.09452934100710113, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.001867468818090856}, {"id": 2271, "seek": 912670, "start": 9126.7, "end": 9134.300000000001, "text": " From here down and paste it into our function and what we have is the", "tokens": [50364, 3358, 510, 760, 293, 9163, 309, 666, 527, 2445, 293, 437, 321, 362, 307, 264, 50744], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2272, "seek": 912670, "start": 9134.300000000001, "end": 9137.1, "text": " ability now to implement this as a custom pipe.", "tokens": [50744, 3485, 586, 281, 4445, 341, 382, 257, 2375, 11240, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2273, "seek": 912670, "start": 9138.2, "end": 9141.5, "text": " We don't need to do this because we don't want to print things off but", "tokens": [50939, 492, 500, 380, 643, 281, 360, 341, 570, 321, 500, 380, 528, 281, 4482, 721, 766, 457, 51104], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2274, "seek": 912670, "start": 9141.5, "end": 9143.900000000001, "text": " here we're going to return the doc object.", "tokens": [51104, 510, 321, 434, 516, 281, 2736, 264, 3211, 2657, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2275, "seek": 912670, "start": 9143.900000000001, "end": 9148.5, "text": " So we have now is a custom kind of entity ruler that uses regex across", "tokens": [51224, 407, 321, 362, 586, 307, 257, 2375, 733, 295, 13977, 19661, 300, 4960, 319, 432, 87, 2108, 51454], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2276, "seek": 912670, "start": 9148.5, "end": 9149.900000000001, "text": " multiple tokens.", "tokens": [51454, 3866, 22667, 13, 51524], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2277, "seek": 912670, "start": 9150.1, "end": 9154.7, "text": " If you want to use regex in spacey across multiple tokens as of spacey", "tokens": [51534, 759, 291, 528, 281, 764, 319, 432, 87, 294, 1901, 88, 2108, 3866, 22667, 382, 295, 1901, 88, 51764], "temperature": 0.0, "avg_logprob": -0.1040591113972214, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0008426193380728364}, {"id": 2278, "seek": 915470, "start": 9154.7, "end": 9157.5, "text": " 3.1 this is the only way to implement this.", "tokens": [50364, 805, 13, 16, 341, 307, 264, 787, 636, 281, 4445, 341, 13, 50504], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2279, "seek": 915470, "start": 9158.1, "end": 9165.2, "text": " So now we can take this pipe and we can actually add it to a blank custom", "tokens": [50534, 407, 586, 321, 393, 747, 341, 11240, 293, 321, 393, 767, 909, 309, 281, 257, 8247, 2375, 50889], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2280, "seek": 915470, "start": 9165.300000000001, "end": 9165.800000000001, "text": " model.", "tokens": [50894, 2316, 13, 50919], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2281, "seek": 915470, "start": 9166.1, "end": 9172.6, "text": " So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're", "tokens": [50934, 407, 718, 311, 652, 257, 777, 297, 75, 79, 5498, 297, 75, 79, 17, 307, 2681, 281, 1901, 88, 5893, 8247, 293, 321, 434, 51259], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2282, "seek": 915470, "start": 9172.6, "end": 9177.900000000001, "text": " going to create a blank English model nlp2 dot add pipe.", "tokens": [51259, 516, 281, 1884, 257, 8247, 3669, 2316, 297, 75, 79, 17, 5893, 909, 11240, 13, 51524], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2283, "seek": 915470, "start": 9179.300000000001, "end": 9181.5, "text": " We're going to add in Paul NER.", "tokens": [51594, 492, 434, 516, 281, 909, 294, 4552, 426, 1598, 13, 51704], "temperature": 0.0, "avg_logprob": -0.16455648278677334, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.01406186819076538}, {"id": 2284, "seek": 918470, "start": 9185.1, "end": 9187.7, "text": " And now we see that we've actually created that successfully.", "tokens": [50384, 400, 586, 321, 536, 300, 321, 600, 767, 2942, 300, 10727, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2285, "seek": 918470, "start": 9187.7, "end": 9190.400000000001, "text": " So we have one pipe kind of sitting in all of this.", "tokens": [50514, 407, 321, 362, 472, 11240, 733, 295, 3798, 294, 439, 295, 341, 13, 50649], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2286, "seek": 918470, "start": 9190.800000000001, "end": 9194.5, "text": " Now what we can do is we can go through and we need to probably add in our", "tokens": [50669, 823, 437, 321, 393, 360, 307, 321, 393, 352, 807, 293, 321, 643, 281, 1391, 909, 294, 527, 50854], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2287, "seek": 918470, "start": 9194.5, "end": 9199.300000000001, "text": " pattern as well here just for good practice because this should be", "tokens": [50854, 5102, 382, 731, 510, 445, 337, 665, 3124, 570, 341, 820, 312, 51094], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2288, "seek": 918470, "start": 9199.300000000001, "end": 9201.1, "text": " stored somewhat adjacent.", "tokens": [51094, 12187, 8344, 24441, 13, 51184], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2289, "seek": 918470, "start": 9201.1, "end": 9204.1, "text": " I like to sometimes to keep it up here when I'm doing this but you can", "tokens": [51184, 286, 411, 281, 2171, 281, 1066, 309, 493, 510, 562, 286, 478, 884, 341, 457, 291, 393, 51334], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2290, "seek": 918470, "start": 9204.1, "end": 9207.0, "text": " also keep it kind of inside of the function itself.", "tokens": [51334, 611, 1066, 309, 733, 295, 1854, 295, 264, 2445, 2564, 13, 51479], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2291, "seek": 918470, "start": 9208.2, "end": 9211.5, "text": " Let's go ahead and just kind of save that and we're going to rerun this", "tokens": [51539, 961, 311, 352, 2286, 293, 445, 733, 295, 3155, 300, 293, 321, 434, 516, 281, 43819, 409, 341, 51704], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2292, "seek": 918470, "start": 9212.300000000001, "end": 9212.7, "text": " cool.", "tokens": [51744, 1627, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08623195257712538, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.00288945809006691}, {"id": 2293, "seek": 921270, "start": 9213.400000000001, "end": 9217.2, "text": " Now what we can do is we can say doc to is equal to nlp2 we're going", "tokens": [50399, 823, 437, 321, 393, 360, 307, 321, 393, 584, 3211, 281, 307, 2681, 281, 297, 75, 79, 17, 321, 434, 516, 50589], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2294, "seek": 921270, "start": 9217.2, "end": 9221.2, "text": " to go over that exact same text and we're going to print off our doc", "tokens": [50589, 281, 352, 670, 300, 1900, 912, 2487, 293, 321, 434, 516, 281, 4482, 766, 527, 3211, 50789], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2295, "seek": 921270, "start": 9221.300000000001, "end": 9226.300000000001, "text": " to dot ints and we've now managed to implement that as a custom", "tokens": [50794, 281, 5893, 560, 82, 293, 321, 600, 586, 6453, 281, 4445, 300, 382, 257, 2375, 51044], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2296, "seek": 921270, "start": 9226.400000000001, "end": 9229.5, "text": " spacey pipe but we've got one big problem.", "tokens": [51049, 1901, 88, 11240, 457, 321, 600, 658, 472, 955, 1154, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2297, "seek": 921270, "start": 9229.900000000001, "end": 9235.900000000001, "text": " Let's say just hypothetically we wanted to also kind of work in really", "tokens": [51224, 961, 311, 584, 445, 24371, 22652, 321, 1415, 281, 611, 733, 295, 589, 294, 534, 51524], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2298, "seek": 921270, "start": 9235.900000000001, "end": 9241.900000000001, "text": " a another kind of something into our actual pipeline.", "tokens": [51524, 257, 1071, 733, 295, 746, 666, 527, 3539, 15517, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1638305888456457, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.0012065210612490773}, {"id": 2299, "seek": 924190, "start": 9241.9, "end": 9246.9, "text": " We wanted this pipeline to sit on top of maybe an existing spacey model", "tokens": [50364, 492, 1415, 341, 15517, 281, 1394, 322, 1192, 295, 1310, 364, 6741, 1901, 88, 2316, 50614], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2300, "seek": 924190, "start": 9247.3, "end": 9251.8, "text": " and for whatever reason we don't want Paul Hollywood to have that title.", "tokens": [50634, 293, 337, 2035, 1778, 321, 500, 380, 528, 4552, 11628, 281, 362, 300, 4876, 13, 50859], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2301, "seek": 924190, "start": 9251.8, "end": 9253.199999999999, "text": " We wanted to have the title.", "tokens": [50859, 492, 1415, 281, 362, 264, 4876, 13, 50929], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2302, "seek": 924190, "start": 9253.699999999999, "end": 9257.0, "text": " Maybe we want to just kind of keep Paul Hollywood as a person but we", "tokens": [50954, 2704, 321, 528, 281, 445, 733, 295, 1066, 4552, 11628, 382, 257, 954, 457, 321, 51119], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2303, "seek": 924190, "start": 9257.0, "end": 9261.6, "text": " also want to find maybe other cinema style entities.", "tokens": [51119, 611, 528, 281, 915, 1310, 661, 17178, 3758, 16667, 13, 51349], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2304, "seek": 924190, "start": 9261.6, "end": 9265.4, "text": " So we're going to create another entity here instead of all this that's", "tokens": [51349, 407, 321, 434, 516, 281, 1884, 1071, 13977, 510, 2602, 295, 439, 341, 300, 311, 51539], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2305, "seek": 924190, "start": 9265.4, "end": 9270.1, "text": " going to be something like let's go ahead and make a new a new container", "tokens": [51539, 516, 281, 312, 746, 411, 718, 311, 352, 2286, 293, 652, 257, 777, 257, 777, 10129, 51774], "temperature": 0.0, "avg_logprob": -0.10214464380106794, "compression_ratio": 1.76, "no_speech_prob": 0.0009697111090645194}, {"id": 2306, "seek": 927010, "start": 9270.1, "end": 9272.2, "text": " down here a new component down here.", "tokens": [50364, 760, 510, 257, 777, 6542, 760, 510, 13, 50469], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2307, "seek": 927010, "start": 9273.1, "end": 9276.6, "text": " We're going to just look for any instance of Hollywood and we're going", "tokens": [50514, 492, 434, 516, 281, 445, 574, 337, 604, 5197, 295, 11628, 293, 321, 434, 516, 50689], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2308, "seek": 927010, "start": 9276.6, "end": 9279.300000000001, "text": " to call that the word the label of cinema.", "tokens": [50689, 281, 818, 300, 264, 1349, 264, 7645, 295, 17178, 13, 50824], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2309, "seek": 927010, "start": 9279.800000000001, "end": 9282.0, "text": " So I want to demonstrate this because this is going to show you", "tokens": [50849, 407, 286, 528, 281, 11698, 341, 570, 341, 307, 516, 281, 855, 291, 50959], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2310, "seek": 927010, "start": 9282.0, "end": 9284.800000000001, "text": " something that you are going to encounter when you try to implement", "tokens": [50959, 746, 300, 291, 366, 516, 281, 8593, 562, 291, 853, 281, 4445, 51099], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2311, "seek": 927010, "start": 9284.800000000001, "end": 9287.2, "text": " this in the real world and I'm going to show you how to kind of", "tokens": [51099, 341, 294, 264, 957, 1002, 293, 286, 478, 516, 281, 855, 291, 577, 281, 733, 295, 51219], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2312, "seek": 927010, "start": 9287.5, "end": 9289.5, "text": " address the problem that you're going to encounter.", "tokens": [51234, 2985, 264, 1154, 300, 291, 434, 516, 281, 8593, 13, 51334], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2313, "seek": 927010, "start": 9289.9, "end": 9293.1, "text": " So if we had a component that looked like this now it's going to look", "tokens": [51354, 407, 498, 321, 632, 257, 6542, 300, 2956, 411, 341, 586, 309, 311, 516, 281, 574, 51514], "temperature": 0.0, "avg_logprob": -0.08124168848587295, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.006096771452575922}, {"id": 2314, "seek": 929310, "start": 9293.1, "end": 9297.2, "text": " for just instance instances of Hollywood and let's call this Holly", "tokens": [50364, 337, 445, 5197, 14519, 295, 11628, 293, 718, 311, 818, 341, 10055, 50569], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2315, "seek": 929310, "start": 9298.9, "end": 9302.9, "text": " Cinema NER and change this here as well.", "tokens": [50654, 42502, 426, 1598, 293, 1319, 341, 510, 382, 731, 13, 50854], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2316, "seek": 929310, "start": 9303.4, "end": 9305.7, "text": " What we can do now is go ahead and load that up into memories.", "tokens": [50879, 708, 321, 393, 360, 586, 307, 352, 2286, 293, 3677, 300, 493, 666, 8495, 13, 50994], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2317, "seek": 929310, "start": 9305.7, "end": 9309.5, "text": " We've got this new component called Cinema NER and just like before", "tokens": [50994, 492, 600, 658, 341, 777, 6542, 1219, 42502, 426, 1598, 293, 445, 411, 949, 51184], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2318, "seek": 929310, "start": 9309.5, "end": 9312.2, "text": " we're going to create an LP three now this is going to be spacey dot", "tokens": [51184, 321, 434, 516, 281, 1884, 364, 38095, 1045, 586, 341, 307, 516, 281, 312, 1901, 88, 5893, 51319], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2319, "seek": 929310, "start": 9312.2, "end": 9314.4, "text": " load in core web.", "tokens": [51319, 3677, 294, 4965, 3670, 13, 51429], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2320, "seek": 929310, "start": 9315.5, "end": 9320.5, "text": " SM and so what this is going to do is it's going to load up the spacey", "tokens": [51484, 13115, 293, 370, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 3677, 493, 264, 1901, 88, 51734], "temperature": 0.0, "avg_logprob": -0.18843883938259548, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.19187301397323608}, {"id": 2321, "seek": 932050, "start": 9320.5, "end": 9326.7, "text": " small model and LP three dot add pipe and it's going to be the what did", "tokens": [50364, 1359, 2316, 293, 38095, 1045, 5893, 909, 11240, 293, 309, 311, 516, 281, 312, 264, 437, 630, 50674], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2322, "seek": 932050, "start": 9326.7, "end": 9332.3, "text": " I call this again the cinema NER and if we were to go through", "tokens": [50674, 286, 818, 341, 797, 264, 17178, 426, 1598, 293, 498, 321, 645, 281, 352, 807, 50954], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2323, "seek": 932050, "start": 9332.3, "end": 9336.9, "text": " and add that and create a new object called doc three make that", "tokens": [50954, 293, 909, 300, 293, 1884, 257, 777, 2657, 1219, 3211, 1045, 652, 300, 51184], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2324, "seek": 932050, "start": 9336.9, "end": 9339.9, "text": " equal to an LP three text.", "tokens": [51184, 2681, 281, 364, 38095, 1045, 2487, 13, 51334], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2325, "seek": 932050, "start": 9341.7, "end": 9345.6, "text": " We're going to get this air and this is a common air and if you", "tokens": [51424, 492, 434, 516, 281, 483, 341, 1988, 293, 341, 307, 257, 2689, 1988, 293, 498, 291, 51619], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2326, "seek": 932050, "start": 9345.6, "end": 9347.7, "text": " Google it you'll eventually find the right answer.", "tokens": [51619, 3329, 309, 291, 603, 4728, 915, 264, 558, 1867, 13, 51724], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2327, "seek": 932050, "start": 9347.7, "end": 9349.5, "text": " I'm just going to give it to you right now.", "tokens": [51724, 286, 478, 445, 516, 281, 976, 309, 281, 291, 558, 586, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14939180410133218, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.003884158795699477}, {"id": 2328, "seek": 934950, "start": 9349.8, "end": 9353.8, "text": " So what this is telling you is that there are spans that overlap", "tokens": [50379, 407, 437, 341, 307, 3585, 291, 307, 300, 456, 366, 44086, 300, 19959, 50579], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2329, "seek": 934950, "start": 9354.8, "end": 9361.2, "text": " that don't actually work because one of the spans for cinema is Hollywood", "tokens": [50629, 300, 500, 380, 767, 589, 570, 472, 295, 264, 44086, 337, 17178, 307, 11628, 50949], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2330, "seek": 934950, "start": 9361.5, "end": 9367.5, "text": " and the small model is extracting not only that Hollywood as a cinema", "tokens": [50964, 293, 264, 1359, 2316, 307, 49844, 406, 787, 300, 11628, 382, 257, 17178, 51264], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2331, "seek": 934950, "start": 9367.5, "end": 9371.6, "text": " but it's also extracting Paul Hollywood as part of a longer token.", "tokens": [51264, 457, 309, 311, 611, 49844, 4552, 11628, 382, 644, 295, 257, 2854, 14862, 13, 51469], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2332, "seek": 934950, "start": 9371.9, "end": 9377.1, "text": " So what's happened here is we're trying to assign a span to two of", "tokens": [51484, 407, 437, 311, 2011, 510, 307, 321, 434, 1382, 281, 6269, 257, 16174, 281, 732, 295, 51744], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2333, "seek": 934950, "start": 9377.1, "end": 9379.4, "text": " the same tokens and that doesn't work in spacey.", "tokens": [51744, 264, 912, 22667, 293, 300, 1177, 380, 589, 294, 1901, 88, 13, 51859], "temperature": 0.0, "avg_logprob": -0.11881500482559204, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0004878215549979359}, {"id": 2334, "seek": 937940, "start": 9379.4, "end": 9381.5, "text": " It'll break so what can you do?", "tokens": [50364, 467, 603, 1821, 370, 437, 393, 291, 360, 30, 50469], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2335, "seek": 937940, "start": 9381.699999999999, "end": 9386.3, "text": " Well a common method of solving this issue is to work with the filter", "tokens": [50479, 1042, 257, 2689, 3170, 295, 12606, 341, 2734, 307, 281, 589, 365, 264, 6608, 50709], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2336, "seek": 937940, "start": 9386.3, "end": 9388.9, "text": " spans from the spacey dot util.", "tokens": [50709, 44086, 490, 264, 1901, 88, 5893, 4976, 13, 50839], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2337, "seek": 937940, "start": 9389.8, "end": 9393.9, "text": " Let's go ahead and do this right now so you can say from spacey dot", "tokens": [50884, 961, 311, 352, 2286, 293, 360, 341, 558, 586, 370, 291, 393, 584, 490, 1901, 88, 5893, 51089], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2338, "seek": 937940, "start": 9393.9, "end": 9397.199999999999, "text": " util import filter spans.", "tokens": [51089, 4976, 974, 6608, 44086, 13, 51254], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2339, "seek": 937940, "start": 9397.4, "end": 9402.199999999999, "text": " What filter spans allows for you to do is to actually filter out all", "tokens": [51264, 708, 6608, 44086, 4045, 337, 291, 281, 360, 307, 281, 767, 6608, 484, 439, 51504], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2340, "seek": 937940, "start": 9402.199999999999, "end": 9404.6, "text": " of the the spans that are being identified.", "tokens": [51504, 295, 264, 264, 44086, 300, 366, 885, 9234, 13, 51624], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2341, "seek": 937940, "start": 9404.9, "end": 9407.699999999999, "text": " So what we can do is we can say at this stage.", "tokens": [51639, 407, 437, 321, 393, 360, 307, 321, 393, 584, 412, 341, 3233, 13, 51779], "temperature": 0.0, "avg_logprob": -0.14166345236436376, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0008558466215617955}, {"id": 2342, "seek": 940940, "start": 9410.4, "end": 9415.4, "text": " Before you get to the dock dot ends you can say filtered is equal", "tokens": [50414, 4546, 291, 483, 281, 264, 20929, 5893, 5314, 291, 393, 584, 37111, 307, 2681, 50664], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2343, "seek": 940940, "start": 9415.4, "end": 9419.4, "text": " to filter spans original ends.", "tokens": [50664, 281, 6608, 44086, 3380, 5314, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2344, "seek": 940940, "start": 9419.5, "end": 9420.5, "text": " So what does this do?", "tokens": [50869, 407, 437, 775, 341, 360, 30, 50919], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2345, "seek": 940940, "start": 9420.5, "end": 9423.199999999999, "text": " Well what this does is it goes through and looks at all of the", "tokens": [50919, 1042, 437, 341, 775, 307, 309, 1709, 807, 293, 1542, 412, 439, 295, 264, 51054], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2346, "seek": 940940, "start": 9423.199999999999, "end": 9427.8, "text": " different start and end sections from all of your entities.", "tokens": [51054, 819, 722, 293, 917, 10863, 490, 439, 295, 428, 16667, 13, 51284], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2347, "seek": 940940, "start": 9428.199999999999, "end": 9432.4, "text": " And if there is an ever an instance where there is a an overlap", "tokens": [51304, 400, 498, 456, 307, 364, 1562, 364, 5197, 689, 456, 307, 257, 364, 19959, 51514], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2348, "seek": 940940, "start": 9432.4, "end": 9436.699999999999, "text": " of tokens so 8 to 10 and 9 to 10.", "tokens": [51514, 295, 22667, 370, 1649, 281, 1266, 293, 1722, 281, 1266, 13, 51729], "temperature": 0.0, "avg_logprob": -0.21631518654201343, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00044419834739528596}, {"id": 2349, "seek": 943670, "start": 9437.300000000001, "end": 9442.0, "text": " Primacy and priority is going to be given to the longer token.", "tokens": [50394, 19671, 2551, 293, 9365, 307, 516, 281, 312, 2212, 281, 264, 2854, 14862, 13, 50629], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2350, "seek": 943670, "start": 9442.400000000001, "end": 9445.800000000001, "text": " So what we can do is we can set this now to filtered and it helps", "tokens": [50649, 407, 437, 321, 393, 360, 307, 321, 393, 992, 341, 586, 281, 37111, 293, 309, 3665, 50819], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2351, "seek": 943670, "start": 9445.800000000001, "end": 9448.2, "text": " if you call it correctly filtered.", "tokens": [50819, 498, 291, 818, 309, 8944, 37111, 13, 50939], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2352, "seek": 943670, "start": 9448.2, "end": 9448.800000000001, "text": " There we go.", "tokens": [50939, 821, 321, 352, 13, 50969], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2353, "seek": 943670, "start": 9449.800000000001, "end": 9452.800000000001, "text": " We can set that to filtered instead of the original entities.", "tokens": [51019, 492, 393, 992, 300, 281, 37111, 2602, 295, 264, 3380, 16667, 13, 51169], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2354, "seek": 943670, "start": 9453.1, "end": 9454.2, "text": " Go ahead and save that.", "tokens": [51184, 1037, 2286, 293, 3155, 300, 13, 51239], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2355, "seek": 943670, "start": 9454.5, "end": 9458.7, "text": " We're going to add this again and we're going to do doc 3 and", "tokens": [51254, 492, 434, 516, 281, 909, 341, 797, 293, 321, 434, 516, 281, 360, 3211, 805, 293, 51464], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2356, "seek": 943670, "start": 9458.7, "end": 9463.2, "text": " we're going to say for int and doc 3 dot ends print int dot", "tokens": [51464, 321, 434, 516, 281, 584, 337, 560, 293, 3211, 805, 5893, 5314, 4482, 560, 5893, 51689], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2357, "seek": 943670, "start": 9463.2, "end": 9465.1, "text": " text and int dot label.", "tokens": [51689, 2487, 293, 560, 5893, 7645, 13, 51784], "temperature": 0.0, "avg_logprob": -0.15524145652507915, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.003075043438002467}, {"id": 2358, "seek": 946510, "start": 9466.1, "end": 9470.4, "text": " And if we've done this correctly we're not going to see the", "tokens": [50414, 400, 498, 321, 600, 1096, 341, 8944, 321, 434, 406, 516, 281, 536, 264, 50629], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2359, "seek": 946510, "start": 9470.6, "end": 9474.9, "text": " cinema label come out at all because Paul Hollywood is a", "tokens": [50639, 17178, 7645, 808, 484, 412, 439, 570, 4552, 11628, 307, 257, 50854], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2360, "seek": 946510, "start": 9474.9, "end": 9479.0, "text": " longer token than just Hollywood.", "tokens": [50854, 2854, 14862, 813, 445, 11628, 13, 51059], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2361, "seek": 946510, "start": 9479.0, "end": 9483.4, "text": " So what we've done is we've set told spacey give the primacy", "tokens": [51059, 407, 437, 321, 600, 1096, 307, 321, 600, 992, 1907, 1901, 88, 976, 264, 2886, 2551, 51279], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2362, "seek": 946510, "start": 9483.4, "end": 9487.6, "text": " to the longer tokens and assign that label by filtering out", "tokens": [51279, 281, 264, 2854, 22667, 293, 6269, 300, 7645, 538, 30822, 484, 51489], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2363, "seek": 946510, "start": 9487.6, "end": 9490.800000000001, "text": " the tokens you can prevent that air from ever surfacing.", "tokens": [51489, 264, 22667, 291, 393, 4871, 300, 1988, 490, 1562, 9684, 5615, 13, 51649], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2364, "seek": 946510, "start": 9491.1, "end": 9494.1, "text": " But this is a very common thing that you're going to have to", "tokens": [51664, 583, 341, 307, 257, 588, 2689, 551, 300, 291, 434, 516, 281, 362, 281, 51814], "temperature": 0.0, "avg_logprob": -0.15044919967651368, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0003799647674895823}, {"id": 2365, "seek": 949410, "start": 9494.1, "end": 9498.300000000001, "text": " implement sometimes rejects really is the easiest way to", "tokens": [50364, 4445, 2171, 8248, 82, 534, 307, 264, 12889, 636, 281, 50574], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2366, "seek": 949410, "start": 9498.300000000001, "end": 9500.7, "text": " inject and do pattern matching in the entity.", "tokens": [50574, 10711, 293, 360, 5102, 14324, 294, 264, 13977, 13, 50694], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2367, "seek": 949410, "start": 9501.300000000001, "end": 9501.6, "text": " Okay.", "tokens": [50724, 1033, 13, 50739], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2368, "seek": 949410, "start": 9501.6, "end": 9504.300000000001, "text": " So here's the scenario that we have before us in order to make", "tokens": [50739, 407, 510, 311, 264, 9005, 300, 321, 362, 949, 505, 294, 1668, 281, 652, 50874], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2369, "seek": 949410, "start": 9504.300000000001, "end": 9507.5, "text": " this live this kind of live coding and applied spacey a", "tokens": [50874, 341, 1621, 341, 733, 295, 1621, 17720, 293, 6456, 1901, 88, 257, 51034], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2370, "seek": 949410, "start": 9507.5, "end": 9510.300000000001, "text": " little bit more interesting imagine in this scenario we", "tokens": [51034, 707, 857, 544, 1880, 3811, 294, 341, 9005, 321, 51174], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2371, "seek": 949410, "start": 9510.300000000001, "end": 9514.1, "text": " have a client and the client is a stockbroker or somebody", "tokens": [51174, 362, 257, 6423, 293, 264, 6423, 307, 257, 4127, 9120, 5767, 420, 2618, 51364], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2372, "seek": 949410, "start": 9514.1, "end": 9516.9, "text": " who's interested in investing and what they want to be able", "tokens": [51364, 567, 311, 3102, 294, 10978, 293, 437, 436, 528, 281, 312, 1075, 51504], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2373, "seek": 949410, "start": 9516.9, "end": 9519.800000000001, "text": " to do is look at news articles like those coming out of Reuters", "tokens": [51504, 281, 360, 307, 574, 412, 2583, 11290, 411, 729, 1348, 484, 295, 1300, 48396, 51649], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2374, "seek": 949410, "start": 9520.2, "end": 9522.2, "text": " and they want to find the news articles that are the most", "tokens": [51669, 293, 436, 528, 281, 915, 264, 2583, 11290, 300, 366, 264, 881, 51769], "temperature": 0.0, "avg_logprob": -0.10480709223784218, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.005219234619289637}, {"id": 2375, "seek": 952220, "start": 9522.2, "end": 9526.400000000001, "text": " relevant to what they need to actually search for and read", "tokens": [50364, 7340, 281, 437, 436, 643, 281, 767, 3164, 337, 293, 1401, 50574], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2376, "seek": 952220, "start": 9526.400000000001, "end": 9526.900000000001, "text": " for the day.", "tokens": [50574, 337, 264, 786, 13, 50599], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2377, "seek": 952220, "start": 9526.900000000001, "end": 9529.0, "text": " So they want to find the ones that deal with their their", "tokens": [50599, 407, 436, 528, 281, 915, 264, 2306, 300, 2028, 365, 641, 641, 50704], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2378, "seek": 952220, "start": 9529.0, "end": 9532.6, "text": " personal stocks their holdings or maybe their the specific", "tokens": [50704, 2973, 12966, 641, 1797, 1109, 420, 1310, 641, 264, 2685, 50884], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2379, "seek": 952220, "start": 9532.6, "end": 9534.300000000001, "text": " index that they're actually interested in.", "tokens": [50884, 8186, 300, 436, 434, 767, 3102, 294, 13, 50969], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2380, "seek": 952220, "start": 9534.6, "end": 9540.6, "text": " So what this client wants is a way to use spacey to automatically", "tokens": [50984, 407, 437, 341, 6423, 2738, 307, 257, 636, 281, 764, 1901, 88, 281, 6772, 51284], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2381, "seek": 952220, "start": 9540.6, "end": 9545.5, "text": " find all companies referenced within a text all stocks", "tokens": [51284, 915, 439, 3431, 32734, 1951, 257, 2487, 439, 12966, 51529], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2382, "seek": 952220, "start": 9545.5, "end": 9549.5, "text": " referenced within a text and all indexes referenced within", "tokens": [51529, 32734, 1951, 257, 2487, 293, 439, 8186, 279, 32734, 1951, 51729], "temperature": 0.0, "avg_logprob": -0.10001937548319499, "compression_ratio": 1.9431279620853081, "no_speech_prob": 0.0100112808868289}, {"id": 2383, "seek": 954950, "start": 9549.5, "end": 9552.6, "text": " the next text and maybe even some stock exchanges as well.", "tokens": [50364, 264, 958, 2487, 293, 1310, 754, 512, 4127, 27374, 382, 731, 13, 50519], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2384, "seek": 954950, "start": 9552.8, "end": 9555.8, "text": " Now on the actual textbook if you go through to this chapter", "tokens": [50529, 823, 322, 264, 3539, 25591, 498, 291, 352, 807, 281, 341, 7187, 50679], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2385, "seek": 954950, "start": 9555.8, "end": 9558.9, "text": " which is number 10 you're going to find all the kind of", "tokens": [50679, 597, 307, 1230, 1266, 291, 434, 516, 281, 915, 439, 264, 733, 295, 50834], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2386, "seek": 954950, "start": 9558.9, "end": 9562.2, "text": " solutions laid out for you what I'm going to do throughout", "tokens": [50834, 6547, 9897, 484, 337, 291, 437, 286, 478, 516, 281, 360, 3710, 50999], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2387, "seek": 954950, "start": 9562.2, "end": 9566.5, "text": " the next 30 or 40 minutes is kind of walk through how I might", "tokens": [50999, 264, 958, 2217, 420, 3356, 2077, 307, 733, 295, 1792, 807, 577, 286, 1062, 51214], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2388, "seek": 954950, "start": 9566.5, "end": 9568.8, "text": " solve this problem at least on the surface.", "tokens": [51214, 5039, 341, 1154, 412, 1935, 322, 264, 3753, 13, 51329], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2389, "seek": 954950, "start": 9568.8, "end": 9572.3, "text": " This is going to be a rudimentary solution that demonstrates", "tokens": [51329, 639, 307, 516, 281, 312, 257, 32109, 2328, 822, 3827, 300, 31034, 51504], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2390, "seek": 954950, "start": 9572.3, "end": 9575.7, "text": " the power of spacey and how you can apply it in a very short", "tokens": [51504, 264, 1347, 295, 1901, 88, 293, 577, 291, 393, 3079, 309, 294, 257, 588, 2099, 51674], "temperature": 0.0, "avg_logprob": -0.08408231001633865, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.013633985072374344}, {"id": 2391, "seek": 957570, "start": 9575.7, "end": 9580.1, "text": " period of time to do some pretty custom tasks such as financial", "tokens": [50364, 2896, 295, 565, 281, 360, 512, 1238, 2375, 9608, 1270, 382, 4669, 50584], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2392, "seek": 957570, "start": 9580.1, "end": 9584.0, "text": " analysis with that structured data that you've extracted you", "tokens": [50584, 5215, 365, 300, 18519, 1412, 300, 291, 600, 34086, 291, 50779], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2393, "seek": 957570, "start": 9584.0, "end": 9587.7, "text": " can then do any number of things what we're going to start off", "tokens": [50779, 393, 550, 360, 604, 1230, 295, 721, 437, 321, 434, 516, 281, 722, 766, 50964], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2394, "seek": 957570, "start": 9587.7, "end": 9595.0, "text": " with though is importing spacey and importing pandas as PD if", "tokens": [50964, 365, 1673, 307, 43866, 1901, 88, 293, 43866, 4565, 296, 382, 10464, 498, 51329], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2395, "seek": 957570, "start": 9595.0, "end": 9597.900000000001, "text": " you're not familiar with pandas I've got a whole tutorial", "tokens": [51329, 291, 434, 406, 4963, 365, 4565, 296, 286, 600, 658, 257, 1379, 7073, 51474], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2396, "seek": 957570, "start": 9597.900000000001, "end": 9600.900000000001, "text": " series on that on my channel Python tutorials for digital", "tokens": [51474, 2638, 322, 300, 322, 452, 2269, 15329, 17616, 337, 4562, 51624], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2397, "seek": 957570, "start": 9600.900000000001, "end": 9603.7, "text": " humanities even though it has digital humanities in the title", "tokens": [51624, 36140, 754, 1673, 309, 575, 4562, 36140, 294, 264, 4876, 51764], "temperature": 0.0, "avg_logprob": -0.11279264761477101, "compression_ratio": 1.708, "no_speech_prob": 0.24484127759933472}, {"id": 2398, "seek": 960370, "start": 9603.7, "end": 9606.900000000001, "text": " it's for kind of everyone but go through if you're not familiar", "tokens": [50364, 309, 311, 337, 733, 295, 1518, 457, 352, 807, 498, 291, 434, 406, 4963, 50524], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2399, "seek": 960370, "start": 9606.900000000001, "end": 9609.5, "text": " with pandas and check that out you're not really going to need", "tokens": [50524, 365, 4565, 296, 293, 1520, 300, 484, 291, 434, 406, 534, 516, 281, 643, 50654], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2400, "seek": 960370, "start": 9609.5, "end": 9613.0, "text": " it for for this video here you're going to just need to", "tokens": [50654, 309, 337, 337, 341, 960, 510, 291, 434, 516, 281, 445, 643, 281, 50829], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2401, "seek": 960370, "start": 9613.0, "end": 9617.0, "text": " understand that I'm using pandas to access and grab the data", "tokens": [50829, 1223, 300, 286, 478, 1228, 4565, 296, 281, 2105, 293, 4444, 264, 1412, 51029], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2402, "seek": 960370, "start": 9617.0, "end": 9621.2, "text": " that I need from a couple CSV files or comma separated value", "tokens": [51029, 300, 286, 643, 490, 257, 1916, 48814, 7098, 420, 22117, 12005, 2158, 51239], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2403, "seek": 960370, "start": 9621.2, "end": 9622.2, "text": " files that I have.", "tokens": [51239, 7098, 300, 286, 362, 13, 51289], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2404, "seek": 960370, "start": 9623.400000000001, "end": 9625.400000000001, "text": " So the first thing that we need to do is we need to create", "tokens": [51349, 407, 264, 700, 551, 300, 321, 643, 281, 360, 307, 321, 643, 281, 1884, 51449], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2405, "seek": 960370, "start": 9625.400000000001, "end": 9628.400000000001, "text": " what's known as a pandas data frame and this is going to be", "tokens": [51449, 437, 311, 2570, 382, 257, 4565, 296, 1412, 3920, 293, 341, 307, 516, 281, 312, 51599], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2406, "seek": 960370, "start": 9628.400000000001, "end": 9632.7, "text": " equal to PD dot read CSV and I actually have these stored in", "tokens": [51599, 2681, 281, 10464, 5893, 1401, 48814, 293, 286, 767, 362, 613, 12187, 294, 51814], "temperature": 0.0, "avg_logprob": -0.08434401777453888, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.011330359615385532}, {"id": 2407, "seek": 963270, "start": 9632.7, "end": 9636.7, "text": " the data sub folder in the repo you have free access to these", "tokens": [50364, 264, 1412, 1422, 10820, 294, 264, 49040, 291, 362, 1737, 2105, 281, 613, 50564], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2408, "seek": 963270, "start": 9636.7, "end": 9639.5, "text": " they're a little tiny data sets that I cultivated pretty", "tokens": [50564, 436, 434, 257, 707, 5870, 1412, 6352, 300, 286, 46770, 1238, 50704], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2409, "seek": 963270, "start": 9639.5, "end": 9642.300000000001, "text": " quickly they're not perfect but they're good enough for our", "tokens": [50704, 2661, 436, 434, 406, 2176, 457, 436, 434, 665, 1547, 337, 527, 50844], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2410, "seek": 963270, "start": 9642.300000000001, "end": 9645.900000000001, "text": " purposes and we're going to use the separator keyword argument", "tokens": [50844, 9932, 293, 321, 434, 516, 281, 764, 264, 3128, 1639, 20428, 6770, 51024], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2411, "seek": 963270, "start": 9645.900000000001, "end": 9649.5, "text": " which is going to say to separate everything out by tab", "tokens": [51024, 597, 307, 516, 281, 584, 281, 4994, 1203, 484, 538, 4421, 51204], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2412, "seek": 963270, "start": 9649.5, "end": 9653.400000000001, "text": " because these are CSV files tab separated value files and we", "tokens": [51204, 570, 613, 366, 48814, 7098, 4421, 12005, 2158, 7098, 293, 321, 51399], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2413, "seek": 963270, "start": 9653.400000000001, "end": 9656.1, "text": " have something that looks like this so what this stocks dot", "tokens": [51399, 362, 746, 300, 1542, 411, 341, 370, 437, 341, 12966, 5893, 51534], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2414, "seek": 963270, "start": 9656.1, "end": 9660.1, "text": " CSV file is is it's all the symbols company names industry", "tokens": [51534, 48814, 3991, 307, 307, 309, 311, 439, 264, 16944, 2237, 5288, 3518, 51734], "temperature": 0.0, "avg_logprob": -0.11866295445072758, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004608888179063797}, {"id": 2415, "seek": 966010, "start": 9660.1, "end": 9663.300000000001, "text": " and market caps for I think it's around five thousand seven", "tokens": [50364, 293, 2142, 13855, 337, 286, 519, 309, 311, 926, 1732, 4714, 3407, 50524], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2416, "seek": 966010, "start": 9663.300000000001, "end": 9665.7, "text": " hundred different stocks five thousand eight hundred and seventy", "tokens": [50524, 3262, 819, 12966, 1732, 4714, 3180, 3262, 293, 25662, 50644], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2417, "seek": 966010, "start": 9665.7, "end": 9668.9, "text": " nine and so what we're going to use this for is as a way to", "tokens": [50644, 4949, 293, 370, 437, 321, 434, 516, 281, 764, 341, 337, 307, 382, 257, 636, 281, 50804], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2418, "seek": 966010, "start": 9668.9, "end": 9673.300000000001, "text": " start working into an entity ruler all these different symbols", "tokens": [50804, 722, 1364, 666, 364, 13977, 19661, 439, 613, 819, 16944, 51024], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2419, "seek": 966010, "start": 9673.300000000001, "end": 9676.4, "text": " and company names what we want to do is we want to use these", "tokens": [51024, 293, 2237, 5288, 437, 321, 528, 281, 360, 307, 321, 528, 281, 764, 613, 51179], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2420, "seek": 966010, "start": 9676.4, "end": 9679.9, "text": " symbols to work into a model as a way to grab stocks that might", "tokens": [51179, 16944, 281, 589, 666, 257, 2316, 382, 257, 636, 281, 4444, 12966, 300, 1062, 51354], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2421, "seek": 966010, "start": 9679.9, "end": 9682.4, "text": " be referenced and you can already probably start to see a", "tokens": [51354, 312, 32734, 293, 291, 393, 1217, 1391, 722, 281, 536, 257, 51479], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2422, "seek": 966010, "start": 9682.4, "end": 9685.4, "text": " problem with this capital a here we're going to get to that", "tokens": [51479, 1154, 365, 341, 4238, 257, 510, 321, 434, 516, 281, 483, 281, 300, 51629], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2423, "seek": 966010, "start": 9685.4, "end": 9687.9, "text": " in a little bit and we want to grab all the company names", "tokens": [51629, 294, 257, 707, 857, 293, 321, 528, 281, 4444, 439, 264, 2237, 5288, 51754], "temperature": 0.0, "avg_logprob": -0.079714348821929, "compression_ratio": 2.022140221402214, "no_speech_prob": 0.044663943350315094}, {"id": 2424, "seek": 968790, "start": 9687.9, "end": 9690.3, "text": " so we can maybe create two different entity types from", "tokens": [50364, 370, 321, 393, 1310, 1884, 732, 819, 13977, 3467, 490, 50484], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2425, "seek": 968790, "start": 9690.3, "end": 9695.9, "text": " this data set stock and company so let's go through and make", "tokens": [50484, 341, 1412, 992, 4127, 293, 2237, 370, 718, 311, 352, 807, 293, 652, 50764], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2426, "seek": 968790, "start": 9695.9, "end": 9699.1, "text": " these into lists so they're a little bit more so let's go", "tokens": [50764, 613, 666, 14511, 370, 436, 434, 257, 707, 857, 544, 370, 718, 311, 352, 50924], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2427, "seek": 968790, "start": 9699.1, "end": 9701.5, "text": " through and make these into lists so they're a little bit", "tokens": [50924, 807, 293, 652, 613, 666, 14511, 370, 436, 434, 257, 707, 857, 51044], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2428, "seek": 968790, "start": 9701.5, "end": 9706.1, "text": " more manageable what we need to do is we need to create a list", "tokens": [51044, 544, 38798, 437, 321, 643, 281, 360, 307, 321, 643, 281, 1884, 257, 1329, 51274], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2429, "seek": 968790, "start": 9706.1, "end": 9710.5, "text": " of symbols and that's going to be equal to DF dot symbol dot", "tokens": [51274, 295, 16944, 293, 300, 311, 516, 281, 312, 2681, 281, 48336, 5893, 5986, 5893, 51494], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2430, "seek": 968790, "start": 9710.5, "end": 9713.9, "text": " two list this is a great way to do it and pandas so you can", "tokens": [51494, 732, 1329, 341, 307, 257, 869, 636, 281, 360, 309, 293, 4565, 296, 370, 291, 393, 51664], "temperature": 0.0, "avg_logprob": -0.13745490782851472, "compression_ratio": 1.985645933014354, "no_speech_prob": 0.007120658177882433}, {"id": 2431, "seek": 971390, "start": 9713.9, "end": 9717.3, "text": " kind of easily convert all these different columns into", "tokens": [50364, 733, 295, 3612, 7620, 439, 613, 819, 13766, 666, 50534], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2432, "seek": 971390, "start": 9718.5, "end": 9721.4, "text": " different lists that you can work with in Python so companies", "tokens": [50594, 819, 14511, 300, 291, 393, 589, 365, 294, 15329, 370, 3431, 50739], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2433, "seek": 971390, "start": 9721.4, "end": 9725.1, "text": " is going to be equal to DF dot company and name I believe", "tokens": [50739, 307, 516, 281, 312, 2681, 281, 48336, 5893, 2237, 293, 1315, 286, 1697, 50924], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2434, "seek": 971390, "start": 9725.1, "end": 9728.8, "text": " the name was two list and just to demonstrate how this works", "tokens": [50924, 264, 1315, 390, 732, 1329, 293, 445, 281, 11698, 577, 341, 1985, 51109], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2435, "seek": 971390, "start": 9728.8, "end": 9732.699999999999, "text": " let's print off symbols we're going to print up to 10 and", "tokens": [51109, 718, 311, 4482, 766, 16944, 321, 434, 516, 281, 4482, 493, 281, 1266, 293, 51304], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2436, "seek": 971390, "start": 9732.699999999999, "end": 9735.4, "text": " you can kind of see we've managed to take these columns now", "tokens": [51304, 291, 393, 733, 295, 536, 321, 600, 6453, 281, 747, 613, 13766, 586, 51439], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2437, "seek": 971390, "start": 9735.4, "end": 9739.1, "text": " and kind of build them into a simple Python list so what can", "tokens": [51439, 293, 733, 295, 1322, 552, 666, 257, 2199, 15329, 1329, 370, 437, 393, 51624], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2438, "seek": 971390, "start": 9739.1, "end": 9742.1, "text": " we do with that well one of the things that we can do is we", "tokens": [51624, 321, 360, 365, 300, 731, 472, 295, 264, 721, 300, 321, 393, 360, 307, 321, 51774], "temperature": 0.0, "avg_logprob": -0.08963222826941539, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.06276826560497284}, {"id": 2439, "seek": 974210, "start": 9742.1, "end": 9745.9, "text": " can use that information to start cultivating an entity", "tokens": [50364, 393, 764, 300, 1589, 281, 722, 15298, 990, 364, 13977, 50554], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2440, "seek": 974210, "start": 9745.9, "end": 9749.7, "text": " ruler but remember we want more things than just one or two", "tokens": [50554, 19661, 457, 1604, 321, 528, 544, 721, 813, 445, 472, 420, 732, 50744], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2441, "seek": 974210, "start": 9749.7, "end": 9752.7, "text": " kind of in our entity ruler we don't just want stocks and we", "tokens": [50744, 733, 295, 294, 527, 13977, 19661, 321, 500, 380, 445, 528, 12966, 293, 321, 50894], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2442, "seek": 974210, "start": 9752.7, "end": 9755.9, "text": " don't just want companies we also want things like indexes", "tokens": [50894, 500, 380, 445, 528, 3431, 321, 611, 528, 721, 411, 8186, 279, 51054], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2443, "seek": 974210, "start": 9755.9, "end": 9757.800000000001, "text": " we're going to get to that in just a second though for right", "tokens": [51054, 321, 434, 516, 281, 483, 281, 300, 294, 445, 257, 1150, 1673, 337, 558, 51149], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2444, "seek": 974210, "start": 9757.800000000001, "end": 9761.6, "text": " now let's try to work these two things into an entity ruler", "tokens": [51149, 586, 718, 311, 853, 281, 589, 613, 732, 721, 666, 364, 13977, 19661, 51339], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2445, "seek": 974210, "start": 9761.6, "end": 9765.300000000001, "text": " how might we go about doing that well as you might expect", "tokens": [51339, 577, 1062, 321, 352, 466, 884, 300, 731, 382, 291, 1062, 2066, 51524], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2446, "seek": 974210, "start": 9765.300000000001, "end": 9769.0, "text": " we're going to create a fairly simple entity ruler so we're", "tokens": [51524, 321, 434, 516, 281, 1884, 257, 6457, 2199, 13977, 19661, 370, 321, 434, 51709], "temperature": 0.0, "avg_logprob": -0.07327127456665039, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.012428426183760166}, {"id": 2447, "seek": 976900, "start": 9769.0, "end": 9771.8, "text": " going to say is nlp is going to be equal to spacey dot blank", "tokens": [50364, 516, 281, 584, 307, 297, 75, 79, 307, 516, 281, 312, 2681, 281, 1901, 88, 5893, 8247, 50504], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2448, "seek": 976900, "start": 9771.8, "end": 9774.5, "text": " we don't need a lot of fancy features here we're just going", "tokens": [50504, 321, 500, 380, 643, 257, 688, 295, 10247, 4122, 510, 321, 434, 445, 516, 50639], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2449, "seek": 976900, "start": 9774.5, "end": 9778.2, "text": " to have a blank model that's just going to hose host an", "tokens": [50639, 281, 362, 257, 8247, 2316, 300, 311, 445, 516, 281, 20061, 3975, 364, 50824], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2450, "seek": 976900, "start": 9778.7, "end": 9782.9, "text": " single entity ruler that's going to be equal to nlp dot add", "tokens": [50849, 2167, 13977, 19661, 300, 311, 516, 281, 312, 2681, 281, 297, 75, 79, 5893, 909, 51059], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2451, "seek": 976900, "start": 9783.5, "end": 9788.1, "text": " underscore pipe and this is going to be entity ruler and now", "tokens": [51089, 37556, 11240, 293, 341, 307, 516, 281, 312, 13977, 19661, 293, 586, 51319], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2452, "seek": 976900, "start": 9788.1, "end": 9790.7, "text": " what we need to do is we need to come up with a way to go", "tokens": [51319, 437, 321, 643, 281, 360, 307, 321, 643, 281, 808, 493, 365, 257, 636, 281, 352, 51449], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2453, "seek": 976900, "start": 9790.7, "end": 9794.8, "text": " through all of these different symbols and add them in so we", "tokens": [51449, 807, 439, 295, 613, 819, 16944, 293, 909, 552, 294, 370, 321, 51654], "temperature": 0.0, "avg_logprob": -0.13516289254893427, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.09395434707403183}, {"id": 2454, "seek": 979480, "start": 9794.8, "end": 9800.199999999999, "text": " can say for symbol and symbols we want to say patterns dot", "tokens": [50364, 393, 584, 337, 5986, 293, 16944, 321, 528, 281, 584, 8294, 5893, 50634], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2455, "seek": 979480, "start": 9800.199999999999, "end": 9803.699999999999, "text": " append and we're going to make a an empty list of patterns", "tokens": [50634, 34116, 293, 321, 434, 516, 281, 652, 257, 364, 6707, 1329, 295, 8294, 50809], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2456, "seek": 979480, "start": 9803.699999999999, "end": 9809.099999999999, "text": " up here and what we're going to append is that dictionary that", "tokens": [50809, 493, 510, 293, 437, 321, 434, 516, 281, 34116, 307, 300, 25890, 300, 51079], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2457, "seek": 979480, "start": 9809.099999999999, "end": 9812.099999999999, "text": " you met when we talked about the entity ruler and I believe", "tokens": [51079, 291, 1131, 562, 321, 2825, 466, 264, 13977, 19661, 293, 286, 1697, 51229], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2458, "seek": 979480, "start": 9812.099999999999, "end": 9816.099999999999, "text": " it was chapter five yeah and what this is going to have", "tokens": [51229, 309, 390, 7187, 1732, 1338, 293, 437, 341, 307, 516, 281, 362, 51429], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2459, "seek": 979480, "start": 9816.099999999999, "end": 9819.0, "text": " there are two things label which is going to correspond to", "tokens": [51429, 456, 366, 732, 721, 7645, 597, 307, 516, 281, 6805, 281, 51574], "temperature": 0.0, "avg_logprob": -0.15437178776181978, "compression_ratio": 1.775, "no_speech_prob": 0.019716402515769005}, {"id": 2460, "seek": 981900, "start": 9819.1, "end": 9824.6, "text": " stock in this case and it's going to have a pattern and", "tokens": [50369, 4127, 294, 341, 1389, 293, 309, 311, 516, 281, 362, 257, 5102, 293, 50644], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2461, "seek": 981900, "start": 9824.6, "end": 9829.0, "text": " that's going to correspond to the pattern of the symbol so", "tokens": [50644, 300, 311, 516, 281, 6805, 281, 264, 5102, 295, 264, 5986, 370, 50864], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2462, "seek": 981900, "start": 9829.0, "end": 9832.5, "text": " we're going to say symbol and what that lets us do is kind of", "tokens": [50864, 321, 434, 516, 281, 584, 5986, 293, 437, 300, 6653, 505, 360, 307, 733, 295, 51039], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2463, "seek": 981900, "start": 9832.5, "end": 9836.1, "text": " go through and easily create and add these patterns and and", "tokens": [51039, 352, 807, 293, 3612, 1884, 293, 909, 613, 8294, 293, 293, 51219], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2464, "seek": 981900, "start": 9836.1, "end": 9839.5, "text": " we can do the same thing for company remember it's never a", "tokens": [51219, 321, 393, 360, 264, 912, 551, 337, 2237, 1604, 309, 311, 1128, 257, 51389], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2465, "seek": 981900, "start": 9839.5, "end": 9843.2, "text": " good idea to copy and paste in your code I am simply doing", "tokens": [51389, 665, 1558, 281, 5055, 293, 9163, 294, 428, 3089, 286, 669, 2935, 884, 51574], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2466, "seek": 981900, "start": 9843.2, "end": 9846.2, "text": " it for demonstration purposes right now this is not polished", "tokens": [51574, 309, 337, 16520, 9932, 558, 586, 341, 307, 406, 29079, 51724], "temperature": 0.0, "avg_logprob": -0.10632748279756712, "compression_ratio": 1.7735042735042734, "no_speech_prob": 0.046022720634937286}, {"id": 2467, "seek": 984620, "start": 9846.2, "end": 9849.6, "text": " code by any stretch of the imagination and what we can do", "tokens": [50364, 3089, 538, 604, 5985, 295, 264, 12938, 293, 437, 321, 393, 360, 50534], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2468, "seek": 984620, "start": 9849.6, "end": 9852.0, "text": " here now is we can do the same thing loop over the different", "tokens": [50534, 510, 586, 307, 321, 393, 360, 264, 912, 551, 6367, 670, 264, 819, 50654], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2469, "seek": 984620, "start": 9852.0, "end": 9855.6, "text": " companies and add each company and so what this is doing is", "tokens": [50654, 3431, 293, 909, 1184, 2237, 293, 370, 437, 341, 307, 884, 307, 50834], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2470, "seek": 984620, "start": 9855.6, "end": 9858.5, "text": " it's creating a large list of different patterns that the", "tokens": [50834, 309, 311, 4084, 257, 2416, 1329, 295, 819, 8294, 300, 264, 50979], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2471, "seek": 984620, "start": 9858.5, "end": 9862.5, "text": " entity ruler will use to then go through and as we create the", "tokens": [50979, 13977, 19661, 486, 764, 281, 550, 352, 807, 293, 382, 321, 1884, 264, 51179], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2472, "seek": 984620, "start": 9862.5, "end": 9867.1, "text": " a doc object over that sample Reuters text I just showed you", "tokens": [51179, 257, 3211, 2657, 670, 300, 6889, 1300, 48396, 2487, 286, 445, 4712, 291, 51409], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2473, "seek": 984620, "start": 9867.1, "end": 9870.2, "text": " a second ago which we should probably just go ahead and", "tokens": [51409, 257, 1150, 2057, 597, 321, 820, 1391, 445, 352, 2286, 293, 51564], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2474, "seek": 984620, "start": 9870.2, "end": 9873.1, "text": " pull up right now I'm going to copy and paste it straight", "tokens": [51564, 2235, 493, 558, 586, 286, 478, 516, 281, 5055, 293, 9163, 309, 2997, 51709], "temperature": 0.0, "avg_logprob": -0.10714199625212571, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.012050764635205269}, {"id": 2475, "seek": 987310, "start": 9873.1, "end": 9875.1, "text": " from the textbook.", "tokens": [50364, 490, 264, 25591, 13, 50464], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2476, "seek": 987310, "start": 9878.6, "end": 9881.6, "text": " Let's go ahead and execute that cell and we're going to add", "tokens": [50639, 961, 311, 352, 2286, 293, 14483, 300, 2815, 293, 321, 434, 516, 281, 909, 50789], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2477, "seek": 987310, "start": 9881.6, "end": 9885.0, "text": " in this text here it is a little lengthy but it'll be all", "tokens": [50789, 294, 341, 2487, 510, 309, 307, 257, 707, 35374, 457, 309, 603, 312, 439, 50959], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2478, "seek": 987310, "start": 9885.0, "end": 9887.800000000001, "text": " right and what we're going to do now is we're going to iterate", "tokens": [50959, 558, 293, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 44497, 51099], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2479, "seek": 987310, "start": 9887.800000000001, "end": 9891.0, "text": " over create a doc object to iterate over all of that.", "tokens": [51099, 670, 1884, 257, 3211, 2657, 281, 44497, 670, 439, 295, 300, 13, 51259], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2480, "seek": 987310, "start": 9894.0, "end": 9897.800000000001, "text": " And our goal here is going to be able to say for int and doc", "tokens": [51409, 400, 527, 3387, 510, 307, 516, 281, 312, 1075, 281, 584, 337, 560, 293, 3211, 51599], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2481, "seek": 987310, "start": 9897.800000000001, "end": 9901.800000000001, "text": " dot ends we want to have extracted all of these different", "tokens": [51599, 5893, 5314, 321, 528, 281, 362, 34086, 439, 295, 613, 819, 51799], "temperature": 0.0, "avg_logprob": -0.16439186700499883, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0018100955057889223}, {"id": 2482, "seek": 990180, "start": 9902.099999999999, "end": 9908.099999999999, "text": " entities so we can say print off and dot text and dot label", "tokens": [50379, 16667, 370, 321, 393, 584, 4482, 766, 293, 5893, 2487, 293, 5893, 7645, 50679], "temperature": 0.0, "avg_logprob": -0.16198109090328217, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.0025907796807587147}, {"id": 2483, "seek": 990180, "start": 9910.0, "end": 9911.3, "text": " and let's see if we succeeded.", "tokens": [50774, 293, 718, 311, 536, 498, 321, 20263, 13, 50839], "temperature": 0.0, "avg_logprob": -0.16198109090328217, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.0025907796807587147}, {"id": 2484, "seek": 990180, "start": 9915.4, "end": 9919.0, "text": " And we have to add in our patterns to our entity ruler so", "tokens": [51044, 400, 321, 362, 281, 909, 294, 527, 8294, 281, 527, 13977, 19661, 370, 51224], "temperature": 0.0, "avg_logprob": -0.16198109090328217, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.0025907796807587147}, {"id": 2485, "seek": 990180, "start": 9919.0, "end": 9922.599999999999, "text": " remember we can do this by saying ruler dot add patterns.", "tokens": [51224, 1604, 321, 393, 360, 341, 538, 1566, 19661, 5893, 909, 8294, 13, 51404], "temperature": 0.0, "avg_logprob": -0.16198109090328217, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.0025907796807587147}, {"id": 2486, "seek": 990180, "start": 9924.9, "end": 9926.0, "text": " Patterns there we go.", "tokens": [51519, 34367, 3695, 456, 321, 352, 13, 51574], "temperature": 0.0, "avg_logprob": -0.16198109090328217, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.0025907796807587147}, {"id": 2487, "seek": 992600, "start": 9927.0, "end": 9932.0, "text": " That's what this error actually means and now when we do it", "tokens": [50414, 663, 311, 437, 341, 6713, 767, 1355, 293, 586, 562, 321, 360, 309, 50664], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2488, "seek": 992600, "start": 9932.0, "end": 9936.5, "text": " we see that we've been able to extract Apple as a company", "tokens": [50664, 321, 536, 300, 321, 600, 668, 1075, 281, 8947, 6373, 382, 257, 2237, 50889], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2489, "seek": 992600, "start": 9936.5, "end": 9940.7, "text": " Apple as a company Nasdaq everything's looking pretty good", "tokens": [50889, 6373, 382, 257, 2237, 16151, 2675, 80, 1203, 311, 1237, 1238, 665, 51099], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2490, "seek": 992600, "start": 9940.7, "end": 9943.9, "text": " but I notice really quickly that I wasn't actually able to", "tokens": [51099, 457, 286, 3449, 534, 2661, 300, 286, 2067, 380, 767, 1075, 281, 51259], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2491, "seek": 992600, "start": 9943.9, "end": 9948.8, "text": " extract Apple as a stock and I've also got another problem", "tokens": [51259, 8947, 6373, 382, 257, 4127, 293, 286, 600, 611, 658, 1071, 1154, 51504], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2492, "seek": 992600, "start": 9949.1, "end": 9954.7, "text": " I've extracted to the lowercase TWO as a stock as well why", "tokens": [51519, 286, 600, 34086, 281, 264, 3126, 9765, 23737, 46, 382, 257, 4127, 382, 731, 983, 51799], "temperature": 0.0, "avg_logprob": -0.16671941568563273, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0024723673705011606}, {"id": 2493, "seek": 995470, "start": 9954.7, "end": 9957.900000000001, "text": " have these two things are as a company. Well it turns out in", "tokens": [50364, 362, 613, 732, 721, 366, 382, 257, 2237, 13, 1042, 309, 4523, 484, 294, 50524], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2494, "seek": 995470, "start": 9957.900000000001, "end": 9963.5, "text": " our data set we've got to TWO that is a company name that's", "tokens": [50524, 527, 1412, 992, 321, 600, 658, 281, 23737, 46, 300, 307, 257, 2237, 1315, 300, 311, 50804], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2495, "seek": 995470, "start": 9963.5, "end": 9967.2, "text": " almost always going to be a false positive and we know that", "tokens": [50804, 1920, 1009, 516, 281, 312, 257, 7908, 3353, 293, 321, 458, 300, 50989], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2496, "seek": 995470, "start": 9967.2, "end": 9970.1, "text": " that kind of thing might be better off worked into a machine", "tokens": [50989, 300, 733, 295, 551, 1062, 312, 1101, 766, 2732, 666, 257, 3479, 51134], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2497, "seek": 995470, "start": 9970.1, "end": 9973.1, "text": " learning model for right now though we're going to work", "tokens": [51134, 2539, 2316, 337, 558, 586, 1673, 321, 434, 516, 281, 589, 51284], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2498, "seek": 995470, "start": 9973.1, "end": 9975.7, "text": " under the presumption that anytime we encounter this kind", "tokens": [51284, 833, 264, 18028, 1695, 300, 13038, 321, 8593, 341, 733, 51414], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2499, "seek": 995470, "start": 9975.7, "end": 9979.5, "text": " of obscure company TWO as a lowercase it's going to be a", "tokens": [51414, 295, 34443, 2237, 23737, 46, 382, 257, 3126, 9765, 309, 311, 516, 281, 312, 257, 51604], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2500, "seek": 995470, "start": 9979.5, "end": 9982.900000000001, "text": " false positive. I also have another problem I know for a", "tokens": [51604, 7908, 3353, 13, 286, 611, 362, 1071, 1154, 286, 458, 337, 257, 51774], "temperature": 0.0, "avg_logprob": -0.11950969696044922, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.029302598908543587}, {"id": 2501, "seek": 998290, "start": 9982.9, "end": 9988.5, "text": " fact that Apple the stock is referenced within this text to", "tokens": [50364, 1186, 300, 6373, 264, 4127, 307, 32734, 1951, 341, 2487, 281, 50644], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2502, "seek": 998290, "start": 9988.5, "end": 9991.4, "text": " make it a little easier. Let's see it right here and notice", "tokens": [50644, 652, 309, 257, 707, 3571, 13, 961, 311, 536, 309, 558, 510, 293, 3449, 50789], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2503, "seek": 998290, "start": 9991.4, "end": 9993.9, "text": " that it didn't find it to make this a little easier to", "tokens": [50789, 300, 309, 994, 380, 915, 309, 281, 652, 341, 257, 707, 3571, 281, 50914], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2504, "seek": 998290, "start": 9993.9, "end": 9997.3, "text": " display. Let's go ahead and display what we're looking at", "tokens": [50914, 4674, 13, 961, 311, 352, 2286, 293, 4674, 437, 321, 434, 1237, 412, 51084], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2505, "seek": 998290, "start": 9997.5, "end": 10001.699999999999, "text": " as the splacy render so what we can do is we can use that the", "tokens": [51094, 382, 264, 4732, 2551, 15529, 370, 437, 321, 393, 360, 307, 321, 393, 764, 300, 264, 51304], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2506, "seek": 998290, "start": 10001.699999999999, "end": 10005.199999999999, "text": " splacy render that we met a little bit ago in this video.", "tokens": [51304, 4732, 2551, 15529, 300, 321, 1131, 257, 707, 857, 2057, 294, 341, 960, 13, 51479], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2507, "seek": 998290, "start": 10006.699999999999, "end": 10009.699999999999, "text": " So in order to import this if you remember we need to say", "tokens": [51554, 407, 294, 1668, 281, 974, 341, 498, 291, 1604, 321, 643, 281, 584, 51704], "temperature": 0.0, "avg_logprob": -0.12422504425048828, "compression_ratio": 1.8552036199095023, "no_speech_prob": 0.009124577045440674}, {"id": 2508, "seek": 1000970, "start": 10010.7, "end": 10016.800000000001, "text": " from spacey import display see and that's going to allow us", "tokens": [50414, 490, 1901, 88, 974, 4674, 536, 293, 300, 311, 516, 281, 2089, 505, 50719], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2509, "seek": 1000970, "start": 10016.800000000001, "end": 10020.2, "text": " to actually display our entities. Let's go ahead and put", "tokens": [50719, 281, 767, 4674, 527, 16667, 13, 961, 311, 352, 2286, 293, 829, 50889], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2510, "seek": 1000970, "start": 10020.2, "end": 10023.400000000001, "text": " this however on a different cell just so we don't have to", "tokens": [50889, 341, 4461, 322, 257, 819, 2815, 445, 370, 321, 500, 380, 362, 281, 51049], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2511, "seek": 1000970, "start": 10023.400000000001, "end": 10029.400000000001, "text": " execute that every time and we're going to say at splacy render", "tokens": [51049, 14483, 300, 633, 565, 293, 321, 434, 516, 281, 584, 412, 4732, 2551, 15529, 51349], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2512, "seek": 1000970, "start": 10029.6, "end": 10033.5, "text": " and we're going to render the doc object with a style that's", "tokens": [51359, 293, 321, 434, 516, 281, 15529, 264, 3211, 2657, 365, 257, 3758, 300, 311, 51554], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2513, "seek": 1000970, "start": 10033.5, "end": 10037.800000000001, "text": " equal to ENT and we can see that we've got our text now", "tokens": [51554, 2681, 281, 15244, 51, 293, 321, 393, 536, 300, 321, 600, 658, 527, 2487, 586, 51769], "temperature": 0.0, "avg_logprob": -0.14868008463006271, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.004331125412136316}, {"id": 2514, "seek": 1003780, "start": 10037.8, "end": 10041.3, "text": " popping out with our things labeled and you can see pretty", "tokens": [50364, 18374, 484, 365, 527, 721, 21335, 293, 291, 393, 536, 1238, 50539], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2515, "seek": 1003780, "start": 10041.3, "end": 10044.099999999999, "text": " quickly where we've made some mistakes where we need to", "tokens": [50539, 2661, 689, 321, 600, 1027, 512, 8038, 689, 321, 643, 281, 50679], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2516, "seek": 1003780, "start": 10044.099999999999, "end": 10048.3, "text": " incorporate some things into our entity ruler. So for example", "tokens": [50679, 16091, 512, 721, 666, 527, 13977, 19661, 13, 407, 337, 1365, 50889], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2517, "seek": 1003780, "start": 10048.3, "end": 10051.5, "text": " if I'm scrolling through this is gray little ugly we can change", "tokens": [50889, 498, 286, 478, 29053, 807, 341, 307, 10855, 707, 12246, 321, 393, 1319, 51049], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2518, "seek": 1003780, "start": 10051.5, "end": 10054.5, "text": " the colors that's beyond the scope of this video though but", "tokens": [51049, 264, 4577, 300, 311, 4399, 264, 11923, 295, 341, 960, 1673, 457, 51199], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2519, "seek": 1003780, "start": 10054.5, "end": 10057.9, "text": " let's keep on going down we notice that we have Apple dot", "tokens": [51199, 718, 311, 1066, 322, 516, 760, 321, 3449, 300, 321, 362, 6373, 5893, 51369], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2520, "seek": 1003780, "start": 10057.9, "end": 10061.599999999999, "text": " IO and yet this has been missed by our entity ruler. Why has", "tokens": [51369, 39839, 293, 1939, 341, 575, 668, 6721, 538, 527, 13977, 19661, 13, 1545, 575, 51554], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2521, "seek": 1003780, "start": 10061.599999999999, "end": 10066.4, "text": " this been missed well. Spacey as a tokenizer is seeing this", "tokens": [51554, 341, 668, 6721, 731, 13, 8705, 88, 382, 257, 14862, 6545, 307, 2577, 341, 51794], "temperature": 0.0, "avg_logprob": -0.11033544784937149, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.017982734367251396}, {"id": 2522, "seek": 1006640, "start": 10066.4, "end": 10072.199999999999, "text": " as a single token so Apple dot Oh the letter Oh capital letter", "tokens": [50364, 382, 257, 2167, 14862, 370, 6373, 5893, 876, 264, 5063, 876, 4238, 5063, 50654], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2523, "seek": 1006640, "start": 10072.199999999999, "end": 10075.9, "text": " Oh why is that well I didn't know about this but apparently", "tokens": [50654, 876, 983, 307, 300, 731, 286, 994, 380, 458, 466, 341, 457, 7970, 50839], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2524, "seek": 1006640, "start": 10075.9, "end": 10079.8, "text": " it does has to deal with kind of the way in which stock indices", "tokens": [50839, 309, 775, 575, 281, 2028, 365, 733, 295, 264, 636, 294, 597, 4127, 43840, 51034], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2525, "seek": 1006640, "start": 10079.8, "end": 10083.5, "text": " are I think it's on the NASDAQ kind of structure things so", "tokens": [51034, 366, 286, 519, 309, 311, 322, 264, 10182, 7509, 48, 733, 295, 3877, 721, 370, 51219], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2526, "seek": 1006640, "start": 10083.5, "end": 10086.199999999999, "text": " what can we do well we've got a couple different options here", "tokens": [51219, 437, 393, 321, 360, 731, 321, 600, 658, 257, 1916, 819, 3956, 510, 51354], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2527, "seek": 1006640, "start": 10086.6, "end": 10089.4, "text": " I know that these go through all different letters from A to", "tokens": [51374, 286, 458, 300, 613, 352, 807, 439, 819, 7825, 490, 316, 281, 51514], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2528, "seek": 1006640, "start": 10089.4, "end": 10093.0, "text": " Z so we can either work with the string library or we can do", "tokens": [51514, 1176, 370, 321, 393, 2139, 589, 365, 264, 6798, 6405, 420, 321, 393, 360, 51694], "temperature": 0.0, "avg_logprob": -0.1437908259305087, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0028892727568745613}, {"id": 2529, "seek": 1009300, "start": 10093.0, "end": 10096.5, "text": " is we can import a quick list that I've already written out", "tokens": [50364, 307, 321, 393, 974, 257, 1702, 1329, 300, 286, 600, 1217, 3720, 484, 50539], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2530, "seek": 1009300, "start": 10096.8, "end": 10100.0, "text": " of all the different letters of the alphabet and iterate", "tokens": [50554, 295, 439, 264, 819, 7825, 295, 264, 23339, 293, 44497, 50714], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2531, "seek": 1009300, "start": 10100.0, "end": 10103.8, "text": " through those with our ruler up here.", "tokens": [50714, 807, 729, 365, 527, 19661, 493, 510, 13, 50904], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2532, "seek": 1009300, "start": 10106.0, "end": 10109.1, "text": " Let's go ahead and add these letters right there and we can", "tokens": [51014, 961, 311, 352, 2286, 293, 909, 613, 7825, 558, 456, 293, 321, 393, 51169], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2533, "seek": 1009300, "start": 10109.1, "end": 10111.1, "text": " kind of iterate through those and whenever a stock kind of", "tokens": [51169, 733, 295, 44497, 807, 729, 293, 5699, 257, 4127, 733, 295, 51269], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2534, "seek": 1009300, "start": 10111.1, "end": 10115.8, "text": " pops out with that kind of symbol plus any occurrence where", "tokens": [51269, 16795, 484, 365, 300, 733, 295, 5986, 1804, 604, 36122, 689, 51504], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2535, "seek": 1009300, "start": 10115.8, "end": 10120.4, "text": " it's got a period followed by a letter in those scenarios we", "tokens": [51504, 309, 311, 658, 257, 2896, 6263, 538, 257, 5063, 294, 729, 15077, 321, 51734], "temperature": 0.0, "avg_logprob": -0.0906131342837685, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.003075167303904891}, {"id": 2536, "seek": 1012040, "start": 10120.4, "end": 10124.6, "text": " want that to be flagged as a stock as well so what we can do", "tokens": [50364, 528, 300, 281, 312, 7166, 3004, 382, 257, 4127, 382, 731, 370, 437, 321, 393, 360, 50574], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2537, "seek": 1012040, "start": 10124.6, "end": 10127.4, "text": " is we can add in another thing right here add in another", "tokens": [50574, 307, 321, 393, 909, 294, 1071, 551, 558, 510, 909, 294, 1071, 50714], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2538, "seek": 1012040, "start": 10127.4, "end": 10131.8, "text": " pattern and this is now going to be symbol plus we're going", "tokens": [50714, 5102, 293, 341, 307, 586, 516, 281, 312, 5986, 1804, 321, 434, 516, 50934], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2539, "seek": 1012040, "start": 10131.8, "end": 10135.9, "text": " to add in F string right here a formatted string any occurrence", "tokens": [50934, 281, 909, 294, 479, 6798, 558, 510, 257, 1254, 32509, 6798, 604, 36122, 51139], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2540, "seek": 1012040, "start": 10135.9, "end": 10145.3, "text": " of L we can set up a loop to say for L and letters do this", "tokens": [51139, 295, 441, 321, 393, 992, 493, 257, 6367, 281, 584, 337, 441, 293, 7825, 360, 341, 51609], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2541, "seek": 1012040, "start": 10145.699999999999, "end": 10149.4, "text": " and what this is going to allow us to do is to look for any", "tokens": [51629, 293, 437, 341, 307, 516, 281, 2089, 505, 281, 360, 307, 281, 574, 337, 604, 51814], "temperature": 0.0, "avg_logprob": -0.13098742745139383, "compression_ratio": 1.875, "no_speech_prob": 0.010012183338403702}, {"id": 2542, "seek": 1014940, "start": 10149.4, "end": 10154.9, "text": " instance where there is a symbol followed by a period", "tokens": [50364, 5197, 689, 456, 307, 257, 5986, 6263, 538, 257, 2896, 50639], "temperature": 0.0, "avg_logprob": -0.09395691466658082, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.003075056243687868}, {"id": 2543, "seek": 1014940, "start": 10154.9, "end": 10158.4, "text": " followed by one of these capitalized letters that I just", "tokens": [50639, 6263, 538, 472, 295, 613, 4238, 1602, 7825, 300, 286, 445, 50814], "temperature": 0.0, "avg_logprob": -0.09395691466658082, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.003075056243687868}, {"id": 2544, "seek": 1014940, "start": 10158.4, "end": 10162.4, "text": " copied and pasted in so if we do that we can execute that cell", "tokens": [50814, 25365, 293, 1791, 292, 294, 370, 498, 321, 360, 300, 321, 393, 14483, 300, 2815, 51014], "temperature": 0.0, "avg_logprob": -0.09395691466658082, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.003075056243687868}, {"id": 2545, "seek": 1014940, "start": 10164.4, "end": 10167.6, "text": " and we can scroll down and we can now do the exact same thing", "tokens": [51114, 293, 321, 393, 11369, 760, 293, 321, 393, 586, 360, 264, 1900, 912, 551, 51274], "temperature": 0.0, "avg_logprob": -0.09395691466658082, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.003075056243687868}, {"id": 2546, "seek": 1014940, "start": 10167.6, "end": 10171.8, "text": " that we just did a second ago and actually display this", "tokens": [51274, 300, 321, 445, 630, 257, 1150, 2057, 293, 767, 4674, 341, 51484], "temperature": 0.0, "avg_logprob": -0.09395691466658082, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.003075056243687868}, {"id": 2547, "seek": 1017180, "start": 10172.8, "end": 10180.0, "text": " and now we're finding these stocks highlighted as stock so", "tokens": [50414, 293, 586, 321, 434, 5006, 613, 12966, 17173, 382, 4127, 370, 50774], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2548, "seek": 1017180, "start": 10180.0, "end": 10182.8, "text": " we're successfully getting these stocks and extracting them", "tokens": [50774, 321, 434, 10727, 1242, 613, 12966, 293, 49844, 552, 50914], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2549, "seek": 1017180, "start": 10182.8, "end": 10185.8, "text": " we've got a few different things that our client wants to", "tokens": [50914, 321, 600, 658, 257, 1326, 819, 721, 300, 527, 6423, 2738, 281, 51064], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2550, "seek": 1017180, "start": 10185.8, "end": 10189.0, "text": " also extract though they don't want to just extract companies", "tokens": [51064, 611, 8947, 1673, 436, 500, 380, 528, 281, 445, 8947, 3431, 51224], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2551, "seek": 1017180, "start": 10189.0, "end": 10193.199999999999, "text": " and they don't want to just extract stock and they want to", "tokens": [51224, 293, 436, 500, 380, 528, 281, 445, 8947, 4127, 293, 436, 528, 281, 51434], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2552, "seek": 1017180, "start": 10193.199999999999, "end": 10197.199999999999, "text": " also extract stock exchanges and indexes but we have one", "tokens": [51434, 611, 8947, 4127, 27374, 293, 8186, 279, 457, 321, 362, 472, 51634], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2553, "seek": 1017180, "start": 10197.199999999999, "end": 10200.5, "text": " other problem and go ahead and get rid of this as the display", "tokens": [51634, 661, 1154, 293, 352, 2286, 293, 483, 3973, 295, 341, 382, 264, 4674, 51799], "temperature": 0.0, "avg_logprob": -0.12216533223787944, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.010984879918396473}, {"id": 2554, "seek": 1020050, "start": 10200.5, "end": 10203.6, "text": " mode and switch back to just our set of entities because", "tokens": [50364, 4391, 293, 3679, 646, 281, 445, 527, 992, 295, 16667, 570, 50519], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2555, "seek": 1020050, "start": 10203.6, "end": 10206.4, "text": " it's a little easier to read for this example we've got", "tokens": [50519, 309, 311, 257, 707, 3571, 281, 1401, 337, 341, 1365, 321, 600, 658, 50659], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2556, "seek": 1020050, "start": 10206.4, "end": 10208.4, "text": " another problem and we see we have a couple other stocks", "tokens": [50659, 1071, 1154, 293, 321, 536, 321, 362, 257, 1916, 661, 12966, 50759], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2557, "seek": 1020050, "start": 10208.4, "end": 10212.6, "text": " popping out we now know that Kroger stock is here the n i", "tokens": [50759, 18374, 484, 321, 586, 458, 300, 591, 340, 1321, 4127, 307, 510, 264, 297, 741, 50969], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2558, "seek": 1020050, "start": 10212.6, "end": 10216.0, "text": " o dot n stock is in this text as well now we're starting to", "tokens": [50969, 277, 5893, 297, 4127, 307, 294, 341, 2487, 382, 731, 586, 321, 434, 2891, 281, 51139], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2559, "seek": 1020050, "start": 10216.0, "end": 10219.7, "text": " see a greater degree of specificity for right now I'm", "tokens": [51139, 536, 257, 5044, 4314, 295, 2685, 507, 337, 558, 586, 286, 478, 51324], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2560, "seek": 1020050, "start": 10219.7, "end": 10224.0, "text": " going to include two as a set of a stop technical term would", "tokens": [51324, 516, 281, 4090, 732, 382, 257, 992, 295, 257, 1590, 6191, 1433, 576, 51539], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2561, "seek": 1020050, "start": 10224.0, "end": 10226.5, "text": " be like a stop or something that I don't want to be included", "tokens": [51539, 312, 411, 257, 1590, 420, 746, 300, 286, 500, 380, 528, 281, 312, 5556, 51664], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2562, "seek": 1020050, "start": 10226.5, "end": 10229.7, "text": " into the model so I'm going to make a list of stops and", "tokens": [51664, 666, 264, 2316, 370, 286, 478, 516, 281, 652, 257, 1329, 295, 10094, 293, 51824], "temperature": 0.0, "avg_logprob": -0.12351215749547101, "compression_ratio": 1.789655172413793, "no_speech_prob": 0.027579639106988907}, {"id": 2563, "seek": 1022970, "start": 10229.7, "end": 10232.400000000001, "text": " we're just going to include two in that and we're going to", "tokens": [50364, 321, 434, 445, 516, 281, 4090, 732, 294, 300, 293, 321, 434, 516, 281, 50499], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2564, "seek": 1022970, "start": 10232.400000000001, "end": 10240.2, "text": " say for company and companies do all this if company not in", "tokens": [50499, 584, 337, 2237, 293, 3431, 360, 439, 341, 498, 2237, 406, 294, 50889], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2565, "seek": 1022970, "start": 10240.2, "end": 10244.900000000001, "text": " stops we want this to occur what this means now is that our", "tokens": [50889, 10094, 321, 528, 341, 281, 5160, 437, 341, 1355, 586, 307, 300, 527, 51124], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2566, "seek": 1022970, "start": 10244.900000000001, "end": 10247.900000000001, "text": " our pipeline while going through and having all of these", "tokens": [51124, 527, 15517, 1339, 516, 807, 293, 1419, 439, 295, 613, 51274], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2567, "seek": 1022970, "start": 10247.900000000001, "end": 10250.900000000001, "text": " different things all these different rules it's also going", "tokens": [51274, 819, 721, 439, 613, 819, 4474, 309, 311, 611, 516, 51424], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2568, "seek": 1022970, "start": 10250.900000000001, "end": 10254.2, "text": " to have another rule that looks to see if there's a stop or", "tokens": [51424, 281, 362, 1071, 4978, 300, 1542, 281, 536, 498, 456, 311, 257, 1590, 420, 51589], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2569, "seek": 1022970, "start": 10254.2, "end": 10258.400000000001, "text": " if this company name is this stop and if it is then we want", "tokens": [51589, 498, 341, 2237, 1315, 307, 341, 1590, 293, 498, 309, 307, 550, 321, 528, 51799], "temperature": 0.0, "avg_logprob": -0.10248013838980961, "compression_ratio": 1.943661971830986, "no_speech_prob": 0.003172314027324319}, {"id": 2570, "seek": 1025840, "start": 10258.4, "end": 10263.0, "text": " it to just kind of skip over and ignore it and if we go", "tokens": [50364, 309, 281, 445, 733, 295, 10023, 670, 293, 11200, 309, 293, 498, 321, 352, 50594], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2571, "seek": 1025840, "start": 10263.0, "end": 10265.8, "text": " through we notice that now we've successfully eliminated", "tokens": [50594, 807, 321, 3449, 300, 586, 321, 600, 10727, 20308, 50734], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2572, "seek": 1025840, "start": 10265.8, "end": 10270.3, "text": " this what we would presume to be a consistent false positive", "tokens": [50734, 341, 437, 321, 576, 43283, 281, 312, 257, 8398, 7908, 3353, 50959], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2573, "seek": 1025840, "start": 10270.3, "end": 10273.1, "text": " something that's going to come up again and again as a false", "tokens": [50959, 746, 300, 311, 516, 281, 808, 493, 797, 293, 797, 382, 257, 7908, 51099], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2574, "seek": 1025840, "start": 10273.1, "end": 10277.1, "text": " positive great so we've been able to get this where it works", "tokens": [51099, 3353, 869, 370, 321, 600, 668, 1075, 281, 483, 341, 689, 309, 1985, 51299], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2575, "seek": 1025840, "start": 10277.1, "end": 10280.3, "text": " now pretty well what I also want to work into this model if", "tokens": [51299, 586, 1238, 731, 437, 286, 611, 528, 281, 589, 666, 341, 2316, 498, 51459], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2576, "seek": 1025840, "start": 10280.3, "end": 10283.6, "text": " you remember though are things like indexes fortunately", "tokens": [51459, 291, 1604, 1673, 366, 721, 411, 8186, 279, 25511, 51624], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2577, "seek": 1025840, "start": 10283.6, "end": 10288.199999999999, "text": " I've also provided for us a list of all different indexes that", "tokens": [51624, 286, 600, 611, 5649, 337, 505, 257, 1329, 295, 439, 819, 8186, 279, 300, 51854], "temperature": 0.0, "avg_logprob": -0.10569277980871368, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.012621702626347542}, {"id": 2578, "seek": 1028820, "start": 10288.2, "end": 10291.5, "text": " are available from I believe it's like everything like the", "tokens": [50364, 366, 2435, 490, 286, 1697, 309, 311, 411, 1203, 411, 264, 50529], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2579, "seek": 1028820, "start": 10291.5, "end": 10295.7, "text": " Dow Jones is about 13 or 14 of them let's go ahead and import", "tokens": [50529, 20947, 10512, 307, 466, 3705, 420, 3499, 295, 552, 718, 311, 352, 2286, 293, 974, 50739], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2580, "seek": 1028820, "start": 10295.7, "end": 10301.300000000001, "text": " those up above and let's do that right here in this cell so", "tokens": [50739, 729, 493, 3673, 293, 718, 311, 360, 300, 558, 510, 294, 341, 2815, 370, 51019], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2581, "seek": 1028820, "start": 10301.300000000001, "end": 10303.800000000001, "text": " it kind of goes in sequential order that follows better with", "tokens": [51019, 309, 733, 295, 1709, 294, 42881, 1668, 300, 10002, 1101, 365, 51144], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2582, "seek": 1028820, "start": 10303.800000000001, "end": 10306.900000000001, "text": " the textbook to so it's a new data frame object this is going", "tokens": [51144, 264, 25591, 281, 370, 309, 311, 257, 777, 1412, 3920, 2657, 341, 307, 516, 51299], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2583, "seek": 1028820, "start": 10306.900000000001, "end": 10310.800000000001, "text": " to be equal to P a PD dot read CSV we're going to read in that", "tokens": [51299, 281, 312, 2681, 281, 430, 257, 10464, 5893, 1401, 48814, 321, 434, 516, 281, 1401, 294, 300, 51494], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2584, "seek": 1028820, "start": 10310.800000000001, "end": 10313.7, "text": " data file that I've given us and that's going to be the indexes", "tokens": [51494, 1412, 3991, 300, 286, 600, 2212, 505, 293, 300, 311, 516, 281, 312, 264, 8186, 279, 51639], "temperature": 0.0, "avg_logprob": -0.13733897292823122, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.0053834556601941586}, {"id": 2585, "seek": 1031370, "start": 10313.7, "end": 10319.900000000001, "text": " dot T SV with a separator that's equal to a tab let's see what", "tokens": [50364, 5893, 314, 31910, 365, 257, 3128, 1639, 300, 311, 2681, 281, 257, 4421, 718, 311, 536, 437, 50674], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2586, "seek": 1031370, "start": 10319.900000000001, "end": 10324.0, "text": " that looks like and this is what it looks like so all these", "tokens": [50674, 300, 1542, 411, 293, 341, 307, 437, 309, 1542, 411, 370, 439, 613, 50879], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2587, "seek": 1031370, "start": 10324.0, "end": 10326.900000000001, "text": " different indices now I know I'm going to have a problem right", "tokens": [50879, 819, 43840, 586, 286, 458, 286, 478, 516, 281, 362, 257, 1154, 558, 51024], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2588, "seek": 1031370, "start": 10326.900000000001, "end": 10331.1, "text": " out of the gate and that's going to be that sometimes you're", "tokens": [51024, 484, 295, 264, 8539, 293, 300, 311, 516, 281, 312, 300, 2171, 291, 434, 51234], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2589, "seek": 1031370, "start": 10331.1, "end": 10334.6, "text": " going to see things referenced as SNP 500 I don't know a lot", "tokens": [51234, 516, 281, 536, 721, 32734, 382, 13955, 47, 5923, 286, 500, 380, 458, 257, 688, 51409], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2590, "seek": 1031370, "start": 10334.6, "end": 10337.1, "text": " about finances but I know that you don't always see it as", "tokens": [51409, 466, 25123, 457, 286, 458, 300, 291, 500, 380, 1009, 536, 309, 382, 51534], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2591, "seek": 1031370, "start": 10337.1, "end": 10342.2, "text": " SNP 500 index but I do think that these index symbols are", "tokens": [51534, 13955, 47, 5923, 8186, 457, 286, 360, 519, 300, 613, 8186, 16944, 366, 51789], "temperature": 0.0, "avg_logprob": -0.1246445890058551, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.05031553655862808}, {"id": 2592, "seek": 1034220, "start": 10342.2, "end": 10345.7, "text": " also going to be useful so like I did before I'm going to convert", "tokens": [50364, 611, 516, 281, 312, 4420, 370, 411, 286, 630, 949, 286, 478, 516, 281, 7620, 50539], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2593, "seek": 1034220, "start": 10345.7, "end": 10348.5, "text": " these things into a list so it's a little easier for me to work", "tokens": [50539, 613, 721, 666, 257, 1329, 370, 309, 311, 257, 707, 3571, 337, 385, 281, 589, 50679], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2594, "seek": 1034220, "start": 10348.5, "end": 10353.900000000001, "text": " with in a for loop and I'm going to say indexes is equal to", "tokens": [50679, 365, 294, 257, 337, 6367, 293, 286, 478, 516, 281, 584, 8186, 279, 307, 2681, 281, 50949], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2595, "seek": 1034220, "start": 10353.900000000001, "end": 10361.800000000001, "text": " DF2 dot index name so grabbing that column to list and index", "tokens": [50949, 48336, 17, 5893, 8186, 1315, 370, 23771, 300, 7738, 281, 1329, 293, 8186, 51344], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2596, "seek": 1034220, "start": 10361.800000000001, "end": 10368.400000000001, "text": " symbols is equal to DF2 dot index symbol dot to list and", "tokens": [51344, 16944, 307, 2681, 281, 48336, 17, 5893, 8186, 5986, 5893, 281, 1329, 293, 51674], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2597, "seek": 1034220, "start": 10368.400000000001, "end": 10371.5, "text": " both of these are going to be different and they're both going", "tokens": [51674, 1293, 295, 613, 366, 516, 281, 312, 819, 293, 436, 434, 1293, 516, 51829], "temperature": 0.0, "avg_logprob": -0.10878158338142163, "compression_ratio": 1.85, "no_speech_prob": 0.01798275299370289}, {"id": 2598, "seek": 1037150, "start": 10371.5, "end": 10376.2, "text": " to have the same exact entity label which is going to be an", "tokens": [50364, 281, 362, 264, 912, 1900, 13977, 7645, 597, 307, 516, 281, 312, 364, 50599], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2599, "seek": 1037150, "start": 10376.2, "end": 10380.2, "text": " index and so let's go ahead and iterate over these and add them", "tokens": [50599, 8186, 293, 370, 718, 311, 352, 2286, 293, 44497, 670, 613, 293, 909, 552, 50799], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2600, "seek": 1037150, "start": 10380.2, "end": 10383.3, "text": " in as well so I'm going to go ahead and do that right now", "tokens": [50799, 294, 382, 731, 370, 286, 478, 516, 281, 352, 2286, 293, 360, 300, 558, 586, 50954], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2601, "seek": 1037150, "start": 10384.0, "end": 10393.7, "text": " for indexes and indexes we want this label to be index we", "tokens": [50989, 337, 8186, 279, 293, 8186, 279, 321, 528, 341, 7645, 281, 312, 8186, 321, 51474], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2602, "seek": 1037150, "start": 10393.7, "end": 10397.7, "text": " want this to be index here so it's going to allow us to kind", "tokens": [51474, 528, 341, 281, 312, 8186, 510, 370, 309, 311, 516, 281, 2089, 505, 281, 733, 51674], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2603, "seek": 1037150, "start": 10397.7, "end": 10400.2, "text": " of go through and grab all those and we want to do the same", "tokens": [51674, 295, 352, 807, 293, 4444, 439, 729, 293, 321, 528, 281, 360, 264, 912, 51799], "temperature": 0.0, "avg_logprob": -0.13046093179722024, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0032728114165365696}, {"id": 2604, "seek": 1040020, "start": 10400.2, "end": 10406.0, "text": " thing with index symbols keep these a little separated here", "tokens": [50364, 551, 365, 8186, 16944, 1066, 613, 257, 707, 12005, 510, 50654], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2605, "seek": 1040020, "start": 10406.0, "end": 10412.300000000001, "text": " index symbols and that allows for us to do that and let's go", "tokens": [50654, 8186, 16944, 293, 300, 4045, 337, 505, 281, 360, 300, 293, 718, 311, 352, 50969], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2606, "seek": 1040020, "start": 10412.300000000001, "end": 10415.2, "text": " ahead and without making any adjustments let's see let's see", "tokens": [50969, 2286, 293, 1553, 1455, 604, 18624, 718, 311, 536, 718, 311, 536, 51114], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2607, "seek": 1040020, "start": 10415.2, "end": 10417.900000000001, "text": " how this does with these new patterns that we've added in", "tokens": [51114, 577, 341, 775, 365, 613, 777, 8294, 300, 321, 600, 3869, 294, 51249], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2608, "seek": 1040020, "start": 10418.400000000001, "end": 10421.1, "text": " and because we've already got this text loaded into memory", "tokens": [51274, 293, 570, 321, 600, 1217, 658, 341, 2487, 13210, 666, 4675, 51409], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2609, "seek": 1040020, "start": 10421.1, "end": 10424.5, "text": " I'm going to go ahead and put this right here doc is going to", "tokens": [51409, 286, 478, 516, 281, 352, 2286, 293, 829, 341, 558, 510, 3211, 307, 516, 281, 51579], "temperature": 0.0, "avg_logprob": -0.0844979936426336, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.011506066657602787}, {"id": 2610, "seek": 1042450, "start": 10424.5, "end": 10432.9, "text": " be equal to nlp text for int and doc and print off and dot", "tokens": [50364, 312, 2681, 281, 297, 75, 79, 2487, 337, 560, 293, 3211, 293, 4482, 766, 293, 5893, 50784], "temperature": 0.0, "avg_logprob": -0.17373669611943232, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.027578188106417656}, {"id": 2611, "seek": 1042450, "start": 10432.9, "end": 10438.4, "text": " text and dot label and we can kind of go through and we're", "tokens": [50784, 2487, 293, 5893, 7645, 293, 321, 393, 733, 295, 352, 807, 293, 321, 434, 51059], "temperature": 0.0, "avg_logprob": -0.17373669611943232, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.027578188106417656}, {"id": 2612, "seek": 1042450, "start": 10438.4, "end": 10442.2, "text": " actually now able to extract some indexes and I believe when", "tokens": [51059, 767, 586, 1075, 281, 8947, 512, 8186, 279, 293, 286, 1697, 562, 51249], "temperature": 0.0, "avg_logprob": -0.17373669611943232, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.027578188106417656}, {"id": 2613, "seek": 1042450, "start": 10442.2, "end": 10445.2, "text": " I was looking at this text really quickly though I noticed", "tokens": [51249, 286, 390, 1237, 412, 341, 2487, 534, 2661, 1673, 286, 5694, 51399], "temperature": 0.0, "avg_logprob": -0.17373669611943232, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.027578188106417656}, {"id": 2614, "seek": 1042450, "start": 10445.2, "end": 10449.6, "text": " that there was one instance at least where we had not only", "tokens": [51399, 300, 456, 390, 472, 5197, 412, 1935, 689, 321, 632, 406, 787, 51619], "temperature": 0.0, "avg_logprob": -0.17373669611943232, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.027578188106417656}, {"id": 2615, "seek": 1044960, "start": 10449.6, "end": 10455.300000000001, "text": " the index referenced but also a name like S&P 500 right here", "tokens": [50364, 264, 8186, 32734, 457, 611, 257, 1315, 411, 318, 5, 47, 5923, 558, 510, 50649], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2616, "seek": 1044960, "start": 10455.300000000001, "end": 10458.7, "text": " S&P 500 notice that it isn't found because it doesn't have", "tokens": [50649, 318, 5, 47, 5923, 3449, 300, 309, 1943, 380, 1352, 570, 309, 1177, 380, 362, 50819], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2617, "seek": 1044960, "start": 10458.7, "end": 10461.800000000001, "text": " the name index after it and notice also that none of our", "tokens": [50819, 264, 1315, 8186, 934, 309, 293, 3449, 611, 300, 6022, 295, 527, 50974], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2618, "seek": 1044960, "start": 10462.300000000001, "end": 10465.0, "text": " our symbols are being found because they all seem to be", "tokens": [50999, 527, 16944, 366, 885, 1352, 570, 436, 439, 1643, 281, 312, 51134], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2619, "seek": 1044960, "start": 10465.0, "end": 10471.5, "text": " preceded by a dot so in this case a dot J a DJI and so that's", "tokens": [51134, 16969, 292, 538, 257, 5893, 370, 294, 341, 1389, 257, 5893, 508, 257, 13078, 40, 293, 370, 300, 311, 51459], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2620, "seek": 1044960, "start": 10471.5, "end": 10473.6, "text": " something else that I have to work into this model and the", "tokens": [51459, 746, 1646, 300, 286, 362, 281, 589, 666, 341, 2316, 293, 264, 51564], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2621, "seek": 1044960, "start": 10473.6, "end": 10477.0, "text": " list I gave the data set that's not there so I need to collect", "tokens": [51564, 1329, 286, 2729, 264, 1412, 992, 300, 311, 406, 456, 370, 286, 643, 281, 2500, 51734], "temperature": 0.0, "avg_logprob": -0.1308288072284899, "compression_ratio": 1.7261410788381744, "no_speech_prob": 0.21190358698368073}, {"id": 2622, "seek": 1047700, "start": 10477.1, "end": 10480.6, "text": " a list of these different names and work those into an entity", "tokens": [50369, 257, 1329, 295, 613, 819, 5288, 293, 589, 729, 666, 364, 13977, 50544], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2623, "seek": 1047700, "start": 10480.6, "end": 10483.5, "text": " ruler as well but for right now let's ignore that and focus", "tokens": [50544, 19661, 382, 731, 457, 337, 558, 586, 718, 311, 11200, 300, 293, 1879, 50689], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2624, "seek": 1047700, "start": 10483.5, "end": 10488.5, "text": " on including this S&P 500 so how can I get the S&P 500 in", "tokens": [50689, 322, 3009, 341, 318, 5, 47, 5923, 370, 577, 393, 286, 483, 264, 318, 5, 47, 5923, 294, 50939], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2625, "seek": 1047700, "start": 10488.5, "end": 10491.8, "text": " there from the list I already gave it well what I can do is", "tokens": [50939, 456, 490, 264, 1329, 286, 1217, 2729, 309, 731, 437, 286, 393, 360, 307, 51104], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2626, "seek": 1047700, "start": 10491.8, "end": 10496.8, "text": " I can say okay so under these indices not only do I want to", "tokens": [51104, 286, 393, 584, 1392, 370, 833, 613, 43840, 406, 787, 360, 286, 528, 281, 51354], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2627, "seek": 1047700, "start": 10496.8, "end": 10500.3, "text": " add that specific pattern let's go ahead and break these things", "tokens": [51354, 909, 300, 2685, 5102, 718, 311, 352, 2286, 293, 1821, 613, 721, 51529], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2628, "seek": 1047700, "start": 10500.3, "end": 10503.7, "text": " up into different words and so I'm going to have the words is", "tokens": [51529, 493, 666, 819, 2283, 293, 370, 286, 478, 516, 281, 362, 264, 2283, 307, 51699], "temperature": 0.0, "avg_logprob": -0.078492991692197, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.01065024547278881}, {"id": 2629, "seek": 1050370, "start": 10503.7, "end": 10507.400000000001, "text": " equal to index dot split and then I'm going to make a", "tokens": [50364, 2681, 281, 8186, 5893, 7472, 293, 550, 286, 478, 516, 281, 652, 257, 50549], "temperature": 0.0, "avg_logprob": -0.08827732227466724, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.07917222380638123}, {"id": 2630, "seek": 1050370, "start": 10507.400000000001, "end": 10513.6, "text": " presumption that the the first two words so the S&P 500 the", "tokens": [50549, 18028, 1695, 300, 264, 264, 700, 732, 2283, 370, 264, 318, 5, 47, 5923, 264, 50859], "temperature": 0.0, "avg_logprob": -0.08827732227466724, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.07917222380638123}, {"id": 2631, "seek": 1050370, "start": 10513.6, "end": 10518.800000000001, "text": " S&P 400 are sometimes going to be referenced by themselves so", "tokens": [50859, 318, 5, 47, 8423, 366, 2171, 516, 281, 312, 32734, 538, 2969, 370, 51119], "temperature": 0.0, "avg_logprob": -0.08827732227466724, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.07917222380638123}, {"id": 2632, "seek": 1050370, "start": 10518.800000000001, "end": 10521.400000000001, "text": " what I want to do is I want to work that into the model as", "tokens": [51119, 437, 286, 528, 281, 360, 307, 286, 528, 281, 589, 300, 666, 264, 2316, 382, 51249], "temperature": 0.0, "avg_logprob": -0.08827732227466724, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.07917222380638123}, {"id": 2633, "seek": 1050370, "start": 10521.400000000001, "end": 10526.6, "text": " well and I want to say we're going to say patterns dot", "tokens": [51249, 731, 293, 286, 528, 281, 584, 321, 434, 516, 281, 584, 8294, 5893, 51509], "temperature": 0.0, "avg_logprob": -0.08827732227466724, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.07917222380638123}, {"id": 2634, "seek": 1052660, "start": 10526.6, "end": 10534.7, "text": " append copy this as well we can say something like dot join", "tokens": [50364, 34116, 5055, 341, 382, 731, 321, 393, 584, 746, 411, 5893, 3917, 50769], "temperature": 0.0, "avg_logprob": -0.1317154444181002, "compression_ratio": 1.505, "no_speech_prob": 0.09803456813097}, {"id": 2635, "seek": 1052660, "start": 10536.300000000001, "end": 10541.800000000001, "text": " words up until the second index and let's go ahead and work", "tokens": [50849, 2283, 493, 1826, 264, 1150, 8186, 293, 718, 311, 352, 2286, 293, 589, 51124], "temperature": 0.0, "avg_logprob": -0.1317154444181002, "compression_ratio": 1.505, "no_speech_prob": 0.09803456813097}, {"id": 2636, "seek": 1052660, "start": 10541.800000000001, "end": 10545.7, "text": " that into our model in our patterns or pipeline and print", "tokens": [51124, 300, 666, 527, 2316, 294, 527, 8294, 420, 15517, 293, 4482, 51319], "temperature": 0.0, "avg_logprob": -0.1317154444181002, "compression_ratio": 1.505, "no_speech_prob": 0.09803456813097}, {"id": 2637, "seek": 1052660, "start": 10545.7, "end": 10550.300000000001, "text": " off our NLP again and you'll find that we've now been able to", "tokens": [51319, 766, 527, 426, 45196, 797, 293, 291, 603, 915, 300, 321, 600, 586, 668, 1075, 281, 51549], "temperature": 0.0, "avg_logprob": -0.1317154444181002, "compression_ratio": 1.505, "no_speech_prob": 0.09803456813097}, {"id": 2638, "seek": 1052660, "start": 10550.300000000001, "end": 10556.300000000001, "text": " capture things like S&P 500 that aren't proceeded by the word", "tokens": [51549, 7983, 721, 411, 318, 5, 47, 5923, 300, 3212, 380, 39053, 538, 264, 1349, 51849], "temperature": 0.0, "avg_logprob": -0.1317154444181002, "compression_ratio": 1.505, "no_speech_prob": 0.09803456813097}, {"id": 2639, "seek": 1055630, "start": 10556.3, "end": 10560.699999999999, "text": " index and we see that we in fact have S&P 500 is now popping", "tokens": [50364, 8186, 293, 321, 536, 300, 321, 294, 1186, 362, 318, 5, 47, 5923, 307, 586, 18374, 50584], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2640, "seek": 1055630, "start": 10560.699999999999, "end": 10564.099999999999, "text": " out time and again that's fantastic I'm pretty happy with", "tokens": [50584, 484, 565, 293, 797, 300, 311, 5456, 286, 478, 1238, 2055, 365, 50754], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2641, "seek": 1055630, "start": 10564.099999999999, "end": 10566.8, "text": " that now we're we're getting a deeper sense of what this", "tokens": [50754, 300, 586, 321, 434, 321, 434, 1242, 257, 7731, 2020, 295, 437, 341, 50889], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2642, "seek": 1055630, "start": 10566.8, "end": 10569.699999999999, "text": " text is about without actually having to read it we know that", "tokens": [50889, 2487, 307, 466, 1553, 767, 1419, 281, 1401, 309, 321, 458, 300, 51034], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2643, "seek": 1055630, "start": 10569.699999999999, "end": 10572.699999999999, "text": " it's going to deal heavily with Apple and we know that it's", "tokens": [51034, 309, 311, 516, 281, 2028, 10950, 365, 6373, 293, 321, 458, 300, 309, 311, 51184], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2644, "seek": 1055630, "start": 10572.699999999999, "end": 10575.0, "text": " also going to tangentially deal with some of these other", "tokens": [51184, 611, 516, 281, 10266, 3137, 2028, 365, 512, 295, 613, 661, 51299], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2645, "seek": 1055630, "start": 10575.0, "end": 10579.199999999999, "text": " things as well but I also want to include into this into this", "tokens": [51299, 721, 382, 731, 457, 286, 611, 528, 281, 4090, 666, 341, 666, 341, 51509], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2646, "seek": 1055630, "start": 10579.199999999999, "end": 10583.199999999999, "text": " pipeline the ability for the entity ruler to not just find", "tokens": [51509, 15517, 264, 3485, 337, 264, 13977, 19661, 281, 406, 445, 915, 51709], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2647, "seek": 1055630, "start": 10583.199999999999, "end": 10586.199999999999, "text": " these things but I also wanted to be able to find different", "tokens": [51709, 613, 721, 457, 286, 611, 1415, 281, 312, 1075, 281, 915, 819, 51859], "temperature": 0.0, "avg_logprob": -0.07228213637622434, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.00955667719244957}, {"id": 2648, "seek": 1058620, "start": 10586.2, "end": 10590.0, "text": " stock exchanges so I've got a list I cultivated for different", "tokens": [50364, 4127, 27374, 370, 286, 600, 658, 257, 1329, 286, 46770, 337, 819, 50554], "temperature": 0.0, "avg_logprob": -0.12365809782051745, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.005059415474534035}, {"id": 2649, "seek": 1058620, "start": 10590.0, "end": 10593.800000000001, "text": " stock exchanges which are things like NYSE things like that", "tokens": [50554, 4127, 27374, 597, 366, 721, 411, 26032, 5879, 721, 411, 300, 50744], "temperature": 0.0, "avg_logprob": -0.12365809782051745, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.005059415474534035}, {"id": 2650, "seek": 1058620, "start": 10593.800000000001, "end": 10600.5, "text": " so I can say DS3 is going to be equal to PD dot read CSV data", "tokens": [50744, 370, 286, 393, 584, 15816, 18, 307, 516, 281, 312, 2681, 281, 10464, 5893, 1401, 48814, 1412, 51079], "temperature": 0.0, "avg_logprob": -0.12365809782051745, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.005059415474534035}, {"id": 2651, "seek": 1058620, "start": 10600.5, "end": 10607.800000000001, "text": " backslash stock exchanges dot TSV and then the separator is", "tokens": [51079, 646, 10418, 1299, 4127, 27374, 5893, 37645, 53, 293, 550, 264, 3128, 1639, 307, 51444], "temperature": 0.0, "avg_logprob": -0.12365809782051745, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.005059415474534035}, {"id": 2652, "seek": 1058620, "start": 10607.800000000001, "end": 10611.300000000001, "text": " going to be again a tab and let's take a look at what this", "tokens": [51444, 516, 281, 312, 797, 257, 4421, 293, 718, 311, 747, 257, 574, 412, 437, 341, 51619], "temperature": 0.0, "avg_logprob": -0.12365809782051745, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.005059415474534035}, {"id": 2653, "seek": 1061130, "start": 10611.3, "end": 10612.0, "text": " looks like.", "tokens": [50364, 1542, 411, 13, 50399], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2654, "seek": 1061130, "start": 10614.699999999999, "end": 10616.0, "text": " Stanges there we go.", "tokens": [50534, 745, 10350, 456, 321, 352, 13, 50599], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2655, "seek": 1061130, "start": 10620.599999999999, "end": 10623.3, "text": " There we are and we have something that looks like this", "tokens": [50829, 821, 321, 366, 293, 321, 362, 746, 300, 1542, 411, 341, 50964], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2656, "seek": 1061130, "start": 10623.3, "end": 10629.599999999999, "text": " a pretty a pretty large CSV file CSV file sorry that's got a", "tokens": [50964, 257, 1238, 257, 1238, 2416, 48814, 3991, 48814, 3991, 2597, 300, 311, 658, 257, 51279], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2657, "seek": 1061130, "start": 10629.599999999999, "end": 10633.099999999999, "text": " bunch of different rows the ones I'm most interested in well", "tokens": [51279, 3840, 295, 819, 13241, 264, 2306, 286, 478, 881, 3102, 294, 731, 51454], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2658, "seek": 1061130, "start": 10633.099999999999, "end": 10636.699999999999, "text": " there's a couple actually I'm interested in specifically the", "tokens": [51454, 456, 311, 257, 1916, 767, 286, 478, 3102, 294, 4682, 264, 51634], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2659, "seek": 1061130, "start": 10636.699999999999, "end": 10640.9, "text": " Google Prefix and this description the description has", "tokens": [51634, 3329, 6001, 69, 970, 293, 341, 3855, 264, 3855, 575, 51844], "temperature": 0.0, "avg_logprob": -0.2179978152355516, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.03962678834795952}, {"id": 2660, "seek": 1064090, "start": 10640.9, "end": 10644.4, "text": " the actual name and the Prefix has this really nice abbreviation", "tokens": [50364, 264, 3539, 1315, 293, 264, 6001, 69, 970, 575, 341, 534, 1481, 35839, 399, 50539], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2661, "seek": 1064090, "start": 10644.4, "end": 10647.9, "text": " that I've seen pop out a few different times such as Nasdaq", "tokens": [50539, 300, 286, 600, 1612, 1665, 484, 257, 1326, 819, 1413, 1270, 382, 16151, 2675, 80, 50714], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2662, "seek": 1064090, "start": 10647.9, "end": 10651.1, "text": " here if we keep on going down we would see different things", "tokens": [50714, 510, 498, 321, 1066, 322, 516, 760, 321, 576, 536, 819, 721, 50874], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2663, "seek": 1064090, "start": 10651.1, "end": 10655.199999999999, "text": " as well NYSE these are kind of different stock exchanges.", "tokens": [50874, 382, 731, 26032, 5879, 613, 366, 733, 295, 819, 4127, 27374, 13, 51079], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2664, "seek": 1064090, "start": 10656.0, "end": 10660.6, "text": " So let's pop back down here and let's go ahead and convert", "tokens": [51119, 407, 718, 311, 1665, 646, 760, 510, 293, 718, 311, 352, 2286, 293, 7620, 51349], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2665, "seek": 1064090, "start": 10660.6, "end": 10664.6, "text": " those two things into individual lists as well so we're going", "tokens": [51349, 729, 732, 721, 666, 2609, 14511, 382, 731, 370, 321, 434, 516, 51549], "temperature": 0.0, "avg_logprob": -0.11345725931147094, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0027147100772708654}, {"id": 2666, "seek": 1066460, "start": 10664.6, "end": 10669.2, "text": " to say exchanges it's going to be equal to DF3 dot ISO Mike", "tokens": [50364, 281, 584, 27374, 309, 311, 516, 281, 312, 2681, 281, 48336, 18, 5893, 25042, 6602, 50594], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2667, "seek": 1066460, "start": 10669.9, "end": 10676.800000000001, "text": " dot to list and then I'm also going to grab the F3 dot sorry", "tokens": [50629, 5893, 281, 1329, 293, 550, 286, 478, 611, 516, 281, 4444, 264, 479, 18, 5893, 2597, 50974], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2668, "seek": 1066460, "start": 10676.800000000001, "end": 10680.2, "text": " Google I have to do this as a dictionary because it's the", "tokens": [50974, 3329, 286, 362, 281, 360, 341, 382, 257, 25890, 570, 309, 311, 264, 51144], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2669, "seek": 1066460, "start": 10680.2, "end": 10683.7, "text": " way the data sets cultivated it's got a space in the middle", "tokens": [51144, 636, 264, 1412, 6352, 46770, 309, 311, 658, 257, 1901, 294, 264, 2808, 51319], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2670, "seek": 1066460, "start": 10683.7, "end": 10687.9, "text": " this is a common problem that you run into and then I also", "tokens": [51319, 341, 307, 257, 2689, 1154, 300, 291, 1190, 666, 293, 550, 286, 611, 51529], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2671, "seek": 1066460, "start": 10687.9, "end": 10692.1, "text": " want to know grab all of these exchanges as well so I'm going", "tokens": [51529, 528, 281, 458, 4444, 439, 295, 613, 27374, 382, 731, 370, 286, 478, 516, 51739], "temperature": 0.0, "avg_logprob": -0.16783227725904815, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.41072317957878113}, {"id": 2672, "seek": 1069210, "start": 10692.1, "end": 10700.4, "text": " to say also on top of that DF3 dot description to list so I'm", "tokens": [50364, 281, 584, 611, 322, 1192, 295, 300, 48336, 18, 5893, 3855, 281, 1329, 370, 286, 478, 50779], "temperature": 0.0, "avg_logprob": -0.1310570181869879, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.00912399310618639}, {"id": 2673, "seek": 1069210, "start": 10700.4, "end": 10706.7, "text": " making a large list exchanges and I get this here because it", "tokens": [50779, 1455, 257, 2416, 1329, 27374, 293, 286, 483, 341, 510, 570, 309, 51094], "temperature": 0.0, "avg_logprob": -0.1310570181869879, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.00912399310618639}, {"id": 2674, "seek": 1069210, "start": 10706.7, "end": 10710.4, "text": " says Google Prefix isn't an actual thing and in fact it's", "tokens": [51094, 1619, 3329, 6001, 69, 970, 1943, 380, 364, 3539, 551, 293, 294, 1186, 309, 311, 51279], "temperature": 0.0, "avg_logprob": -0.1310570181869879, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.00912399310618639}, {"id": 2675, "seek": 1069210, "start": 10710.4, "end": 10714.5, "text": " prefix with an I and now we actually are able to get all", "tokens": [51279, 46969, 365, 364, 286, 293, 586, 321, 767, 366, 1075, 281, 483, 439, 51484], "temperature": 0.0, "avg_logprob": -0.1310570181869879, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.00912399310618639}, {"id": 2676, "seek": 1069210, "start": 10714.5, "end": 10719.4, "text": " these things extracted so what I want to do now is I want to", "tokens": [51484, 613, 721, 34086, 370, 437, 286, 528, 281, 360, 586, 307, 286, 528, 281, 51729], "temperature": 0.0, "avg_logprob": -0.1310570181869879, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.00912399310618639}, {"id": 2677, "seek": 1071940, "start": 10719.4, "end": 10723.3, "text": " work all these different symbols and descriptions into into", "tokens": [50364, 589, 439, 613, 819, 16944, 293, 24406, 666, 666, 50559], "temperature": 0.0, "avg_logprob": -0.14493466678418612, "compression_ratio": 1.6954022988505748, "no_speech_prob": 0.015421302057802677}, {"id": 2678, "seek": 1071940, "start": 10723.3, "end": 10726.5, "text": " the model as well or into the pipeline as well so I can say", "tokens": [50559, 264, 2316, 382, 731, 420, 666, 264, 15517, 382, 731, 370, 286, 393, 584, 50719], "temperature": 0.0, "avg_logprob": -0.14493466678418612, "compression_ratio": 1.6954022988505748, "no_speech_prob": 0.015421302057802677}, {"id": 2679, "seek": 1071940, "start": 10726.5, "end": 10734.5, "text": " for for E and exchanges I want to say patterns dot append", "tokens": [50719, 337, 337, 462, 293, 27374, 286, 528, 281, 584, 8294, 5893, 34116, 51119], "temperature": 0.0, "avg_logprob": -0.14493466678418612, "compression_ratio": 1.6954022988505748, "no_speech_prob": 0.015421302057802677}, {"id": 2680, "seek": 1071940, "start": 10737.699999999999, "end": 10741.699999999999, "text": " and I want to do a label that's going to be let's do stock", "tokens": [51279, 293, 286, 528, 281, 360, 257, 7645, 300, 311, 516, 281, 312, 718, 311, 360, 4127, 51479], "temperature": 0.0, "avg_logprob": -0.14493466678418612, "compression_ratio": 1.6954022988505748, "no_speech_prob": 0.015421302057802677}, {"id": 2681, "seek": 1071940, "start": 10742.4, "end": 10747.3, "text": " exchange and then the next thing I want to do is a pattern", "tokens": [51514, 7742, 293, 550, 264, 958, 551, 286, 528, 281, 360, 307, 257, 5102, 51759], "temperature": 0.0, "avg_logprob": -0.14493466678418612, "compression_ratio": 1.6954022988505748, "no_speech_prob": 0.015421302057802677}, {"id": 2682, "seek": 1074730, "start": 10747.599999999999, "end": 10751.099999999999, "text": " and that's going to be equal to in this case E as we're going", "tokens": [50379, 293, 300, 311, 516, 281, 312, 2681, 281, 294, 341, 1389, 462, 382, 321, 434, 516, 50554], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2683, "seek": 1074730, "start": 10751.099999999999, "end": 10754.599999999999, "text": " to see this is not adequate enough we need to do a few", "tokens": [50554, 281, 536, 341, 307, 406, 20927, 1547, 321, 643, 281, 360, 257, 1326, 50729], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2684, "seek": 1074730, "start": 10754.599999999999, "end": 10758.099999999999, "text": " different things to really kind of work this out but it's going", "tokens": [50729, 819, 721, 281, 534, 733, 295, 589, 341, 484, 457, 309, 311, 516, 50904], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2685, "seek": 1074730, "start": 10758.099999999999, "end": 10760.199999999999, "text": " to be a good enough to at least get started", "tokens": [50904, 281, 312, 257, 665, 1547, 281, 412, 1935, 483, 1409, 51009], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2686, "seek": 1074730, "start": 10765.199999999999, "end": 10766.599999999999, "text": " and it's going to take it just a second", "tokens": [51259, 293, 309, 311, 516, 281, 747, 309, 445, 257, 1150, 51329], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2687, "seek": 1074730, "start": 10772.0, "end": 10774.199999999999, "text": " and the main thing that's happening right now are these", "tokens": [51599, 293, 264, 2135, 551, 300, 311, 2737, 558, 586, 366, 613, 51709], "temperature": 0.0, "avg_logprob": -0.09999032952319617, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.012051519937813282}, {"id": 2688, "seek": 1077420, "start": 10774.2, "end": 10778.2, "text": " different for loops so if we keep on going down we now see", "tokens": [50364, 819, 337, 16121, 370, 498, 321, 1066, 322, 516, 760, 321, 586, 536, 50564], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2689, "seek": 1077420, "start": 10778.2, "end": 10781.800000000001, "text": " that we were able to extract the NYSE stock exchange so we've", "tokens": [50564, 300, 321, 645, 1075, 281, 8947, 264, 26032, 5879, 4127, 7742, 370, 321, 600, 50744], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2690, "seek": 1077420, "start": 10781.800000000001, "end": 10784.6, "text": " not only been able to work into a pipeline in a very short", "tokens": [50744, 406, 787, 668, 1075, 281, 589, 666, 257, 15517, 294, 257, 588, 2099, 50884], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2691, "seek": 1077420, "start": 10784.6, "end": 10787.2, "text": " order maybe about 20 30 minutes we've been able to work", "tokens": [50884, 1668, 1310, 466, 945, 2217, 2077, 321, 600, 668, 1075, 281, 589, 51014], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2692, "seek": 1077420, "start": 10787.2, "end": 10790.6, "text": " into a pipeline all of these different things that are coming", "tokens": [51014, 666, 257, 15517, 439, 295, 613, 819, 721, 300, 366, 1348, 51184], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2693, "seek": 1077420, "start": 10790.6, "end": 10794.0, "text": " out we do however see a couple problems and this is where I'm", "tokens": [51184, 484, 321, 360, 4461, 536, 257, 1916, 2740, 293, 341, 307, 689, 286, 478, 51354], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2694, "seek": 1077420, "start": 10794.0, "end": 10796.0, "text": " going to leave it though because you've got the basic", "tokens": [51354, 516, 281, 1856, 309, 1673, 570, 291, 600, 658, 264, 3875, 51454], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2695, "seek": 1077420, "start": 10796.0, "end": 10799.6, "text": " mechanics down now comes time for you being a domain expert", "tokens": [51454, 12939, 760, 586, 1487, 565, 337, 291, 885, 257, 9274, 5844, 51634], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2696, "seek": 1077420, "start": 10799.800000000001, "end": 10802.1, "text": " to work out and come up with rules to solve some of these", "tokens": [51644, 281, 589, 484, 293, 808, 493, 365, 4474, 281, 5039, 512, 295, 613, 51759], "temperature": 0.0, "avg_logprob": -0.09919899160211737, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.07050487399101257}, {"id": 2697, "seek": 1080210, "start": 10802.1, "end": 10806.300000000001, "text": " problems Nasdaq is not a company so there's a problem with", "tokens": [50364, 2740, 16151, 2675, 80, 307, 406, 257, 2237, 370, 456, 311, 257, 1154, 365, 50574], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2698, "seek": 1080210, "start": 10806.300000000001, "end": 10810.1, "text": " the data set or Nasdaq is listed as a company name and one of", "tokens": [50574, 264, 1412, 992, 420, 16151, 2675, 80, 307, 10052, 382, 257, 2237, 1315, 293, 472, 295, 50764], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2699, "seek": 1080210, "start": 10810.1, "end": 10813.5, "text": " the data sets we need to work that out where Nasdaq is never", "tokens": [50764, 264, 1412, 6352, 321, 643, 281, 589, 300, 484, 689, 16151, 2675, 80, 307, 1128, 50934], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2700, "seek": 1080210, "start": 10813.5, "end": 10817.1, "text": " referenced as a company we have the S&P and is now being", "tokens": [50934, 32734, 382, 257, 2237, 321, 362, 264, 318, 5, 47, 293, 307, 586, 885, 51114], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2701, "seek": 1080210, "start": 10817.1, "end": 10820.2, "text": " coming out correctly as S&P 500 there might be instances", "tokens": [51114, 1348, 484, 8944, 382, 318, 5, 47, 5923, 456, 1062, 312, 14519, 51269], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2702, "seek": 1080210, "start": 10820.2, "end": 10823.6, "text": " where just S&P is referenced which I think in that context", "tokens": [51269, 689, 445, 318, 5, 47, 307, 32734, 597, 286, 519, 294, 300, 4319, 51439], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2703, "seek": 1080210, "start": 10823.6, "end": 10827.1, "text": " would probably be the S&P 500 but nevertheless we've been", "tokens": [51439, 576, 1391, 312, 264, 318, 5, 47, 5923, 457, 26924, 321, 600, 668, 51614], "temperature": 0.0, "avg_logprob": -0.08947352181493709, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.02594544179737568}, {"id": 2704, "seek": 1082710, "start": 10827.1, "end": 10833.2, "text": " able to actually extract these things sometimes the Dow Jones", "tokens": [50364, 1075, 281, 767, 8947, 613, 721, 2171, 264, 20947, 10512, 50669], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2705, "seek": 1082710, "start": 10833.2, "end": 10837.800000000001, "text": " Industrial Average might just be referenced to Dow Jones so", "tokens": [50669, 32059, 316, 3623, 1062, 445, 312, 32734, 281, 20947, 10512, 370, 50899], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2706, "seek": 1082710, "start": 10837.800000000001, "end": 10840.4, "text": " this index might just be these first two words I know that's", "tokens": [50899, 341, 8186, 1062, 445, 312, 613, 700, 732, 2283, 286, 458, 300, 311, 51029], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2707, "seek": 1082710, "start": 10840.4, "end": 10842.9, "text": " a common occurrence we've also seen that we weren't able to", "tokens": [51029, 257, 2689, 36122, 321, 600, 611, 1612, 300, 321, 4999, 380, 1075, 281, 51154], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2708, "seek": 1082710, "start": 10842.9, "end": 10845.7, "text": " extract some of those things that were a period followed by", "tokens": [51154, 8947, 512, 295, 729, 721, 300, 645, 257, 2896, 6263, 538, 51294], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2709, "seek": 1082710, "start": 10845.7, "end": 10850.1, "text": " a symbol that referenced the actual index itself nevertheless", "tokens": [51294, 257, 5986, 300, 32734, 264, 3539, 8186, 2564, 26924, 51514], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2710, "seek": 1082710, "start": 10850.1, "end": 10852.6, "text": " this is a really good starting point and you can see how just", "tokens": [51514, 341, 307, 257, 534, 665, 2891, 935, 293, 291, 393, 536, 577, 445, 51639], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2711, "seek": 1082710, "start": 10852.6, "end": 10855.2, "text": " in a few minutes you're able to generate this thing that can", "tokens": [51639, 294, 257, 1326, 2077, 291, 434, 1075, 281, 8460, 341, 551, 300, 393, 51769], "temperature": 0.0, "avg_logprob": -0.10129511034166491, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.22799815237522125}, {"id": 2712, "seek": 1085520, "start": 10855.2, "end": 10859.7, "text": " extract information from unstructured text at the end of", "tokens": [50364, 8947, 1589, 490, 18799, 46847, 2487, 412, 264, 917, 295, 50589], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2713, "seek": 1085520, "start": 10859.7, "end": 10862.1, "text": " the day like I said in the introduction to this entire", "tokens": [50589, 264, 786, 411, 286, 848, 294, 264, 9339, 281, 341, 2302, 50709], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2714, "seek": 1085520, "start": 10862.1, "end": 10867.1, "text": " video that's one of the essential tasks of NLP designing", "tokens": [50709, 960, 300, 311, 472, 295, 264, 7115, 9608, 295, 426, 45196, 14685, 50959], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2715, "seek": 1085520, "start": 10867.1, "end": 10870.6, "text": " this and implementing it is pretty quick and easy perfecting", "tokens": [50959, 341, 293, 18114, 309, 307, 1238, 1702, 293, 1858, 2176, 278, 51134], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2716, "seek": 1085520, "start": 10870.6, "end": 10874.2, "text": " it is where the time really is to get this financial analysis", "tokens": [51134, 309, 307, 689, 264, 565, 534, 307, 281, 483, 341, 4669, 5215, 51314], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2717, "seek": 1085520, "start": 10874.800000000001, "end": 10879.0, "text": " entity ruler working really well where it has almost no false", "tokens": [51344, 13977, 19661, 1364, 534, 731, 689, 309, 575, 1920, 572, 7908, 51554], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2718, "seek": 1085520, "start": 10879.0, "end": 10884.6, "text": " positives and almost never misses a true a true positive it", "tokens": [51554, 35127, 293, 1920, 1128, 29394, 257, 2074, 257, 2074, 3353, 309, 51834], "temperature": 0.0, "avg_logprob": -0.09369298245044465, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.021609995514154434}, {"id": 2719, "seek": 1088460, "start": 10884.6, "end": 10887.1, "text": " would take maybe a few more hours of just some kind of working", "tokens": [50364, 576, 747, 1310, 257, 1326, 544, 2496, 295, 445, 512, 733, 295, 1364, 50489], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2720, "seek": 1088460, "start": 10887.1, "end": 10889.300000000001, "text": " and eventually there are certain things you might find that", "tokens": [50489, 293, 4728, 456, 366, 1629, 721, 291, 1062, 915, 300, 50599], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2721, "seek": 1088460, "start": 10889.300000000001, "end": 10892.6, "text": " would work better in a machine learning model nevertheless", "tokens": [50599, 576, 589, 1101, 294, 257, 3479, 2539, 2316, 26924, 50764], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2722, "seek": 1088460, "start": 10892.6, "end": 10896.0, "text": " you can see the degree to which rules based approaches in", "tokens": [50764, 291, 393, 536, 264, 4314, 281, 597, 4474, 2361, 11587, 294, 50934], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2723, "seek": 1088460, "start": 10896.0, "end": 10899.6, "text": " Spacey can really accomplish some pretty robust tasks with", "tokens": [50934, 8705, 88, 393, 534, 9021, 512, 1238, 13956, 9608, 365, 51114], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2724, "seek": 1088460, "start": 10899.6, "end": 10903.4, "text": " minimal minimal amount of code so long as you have access to", "tokens": [51114, 13206, 13206, 2372, 295, 3089, 370, 938, 382, 291, 362, 2105, 281, 51304], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2725, "seek": 1088460, "start": 10903.4, "end": 10906.4, "text": " or have already cultivated the data sets required.", "tokens": [51304, 420, 362, 1217, 46770, 264, 1412, 6352, 4739, 13, 51454], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2726, "seek": 1088460, "start": 10909.1, "end": 10912.9, "text": " Thank you so much for watching this video series on Spacey", "tokens": [51589, 1044, 291, 370, 709, 337, 1976, 341, 960, 2638, 322, 8705, 88, 51779], "temperature": 0.0, "avg_logprob": -0.10399748728825496, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.015421522781252861}, {"id": 2727, "seek": 1091290, "start": 10912.9, "end": 10915.699999999999, "text": " an introduction to basic concepts of natural language", "tokens": [50364, 364, 9339, 281, 3875, 10392, 295, 3303, 2856, 50504], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2728, "seek": 1091290, "start": 10915.699999999999, "end": 10920.699999999999, "text": " processing linguistic annotations in Spacey vectors pipelines", "tokens": [50504, 9007, 43002, 25339, 763, 294, 8705, 88, 18875, 40168, 50754], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2729, "seek": 1091290, "start": 10920.699999999999, "end": 10923.8, "text": " and kind of rules based Spacey you've enjoyed this video", "tokens": [50754, 293, 733, 295, 4474, 2361, 8705, 88, 291, 600, 4626, 341, 960, 50909], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2730, "seek": 1091290, "start": 10923.8, "end": 10927.199999999999, "text": " please like and subscribe down below and if you've also found", "tokens": [50909, 1767, 411, 293, 3022, 760, 2507, 293, 498, 291, 600, 611, 1352, 51079], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2731, "seek": 1091290, "start": 10927.199999999999, "end": 10930.6, "text": " this video useful consider joining me on my channel Python", "tokens": [51079, 341, 960, 4420, 1949, 5549, 385, 322, 452, 2269, 15329, 51249], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2732, "seek": 1091290, "start": 10930.6, "end": 10933.699999999999, "text": " tutorials for digital humanities if you have like this and", "tokens": [51249, 17616, 337, 4562, 36140, 498, 291, 362, 411, 341, 293, 51404], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2733, "seek": 1091290, "start": 10933.699999999999, "end": 10937.3, "text": " found this video useful I'm envisioning a second part to", "tokens": [51404, 1352, 341, 960, 4420, 286, 478, 24739, 278, 257, 1150, 644, 281, 51584], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2734, "seek": 1091290, "start": 10937.3, "end": 10941.5, "text": " this video where I go with the machine learning aspects of", "tokens": [51584, 341, 960, 689, 286, 352, 365, 264, 3479, 2539, 7270, 295, 51794], "temperature": 0.0, "avg_logprob": -0.12299891079173368, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.13283394277095795}, {"id": 2735, "seek": 1094150, "start": 10941.5, "end": 10944.0, "text": " Spacey if you're interested in that let me know in the", "tokens": [50364, 8705, 88, 498, 291, 434, 3102, 294, 300, 718, 385, 458, 294, 264, 50489], "temperature": 0.0, "avg_logprob": -0.09748893976211548, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.00926473829895258}, {"id": 2736, "seek": 1094150, "start": 10944.0, "end": 10946.7, "text": " comments down below and I'll make a second video that", "tokens": [50489, 3053, 760, 2507, 293, 286, 603, 652, 257, 1150, 960, 300, 50624], "temperature": 0.0, "avg_logprob": -0.09748893976211548, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.00926473829895258}, {"id": 2737, "seek": 1094150, "start": 10946.7, "end": 10948.0, "text": " corresponds to this one.", "tokens": [50624, 23249, 281, 341, 472, 13, 50689], "temperature": 0.0, "avg_logprob": -0.09748893976211548, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.00926473829895258}, {"id": 2738, "seek": 1094150, "start": 10948.7, "end": 10950.6, "text": " Thank you for watching and have a great day.", "tokens": [50724, 1044, 291, 337, 1976, 293, 362, 257, 869, 786, 13, 50819], "temperature": 0.0, "avg_logprob": -0.09748893976211548, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.00926473829895258}], "language": "en"}