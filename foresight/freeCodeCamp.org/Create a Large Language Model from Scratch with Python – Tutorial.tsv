start	end	text
0	4000	Learn how to build your own large language model from scratch.
4360	9840	This course goes into the data handling, math and transformers behind large language models.
9960	11800	Elliot Arledge created this course.
12040	18640	He will help you gain a deep understanding of how LLMs work and how they can be used in various applications.
19040	20600	So let's get started.
21000	22880	Welcome to Intro to Language Modeling.
22920	25280	In this course, you're going to learn a lot of crazy stuff.
25320	27080	Okay, I'm just going to give you a heads up.
27120	28960	It's going to be a lot of crazy stuff we learn here.
29240	31400	However, it will not be insanely hard.
31640	35720	I don't expect you have any any experience in calculus or linear algebra.
36640	39040	A lot of courses out there do assume that, but I will not.
39920	41200	We're going to build up from square one.
41320	46240	We're going to take baby steps when it comes to new fundamental concepts in math and machine learning.
46600	51240	And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out.
51920	55520	That way we don't take forever just taking baby steps through every little concept.
56000	58560	This course is inspired by Andre Karpathy's.
59400	61320	Building a GPT from scratch lecture.
62240	63240	So shout out to him.
63880	69680	And yeah, we don't assume you have any experience, maybe three months of Python experience.
70480	73000	Just so the syntax is sort of familiar and you can.
74080	79120	You're able to follow along that way, but no matter how smart you are, how quick you learn.
80080	85240	The willingness to put in the hours is the most important because this is material that you won't normally come across.
86200	95800	So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water.
96240	97280	Water is very important.
97760	101440	And yeah, hopefully you can make it to the end of this.
102400	103040	You can do it.
104280	111640	Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing.
112280	117600	We'll be scaling the data to about 45 gigabytes for the entire training data set.
118160	123560	So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45.
124440	129240	So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine.
129240	136640	You can just download a different data set and sort of follow the same data pipeline that I do in this video.
137640	141040	Through the course, you may see me switch between Mac OS and Windows.
141440	146640	The code still works all the same, both operating systems, and I'll be using a tool called SSH.
146640	162640	It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook.
163640	169640	So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording.
170640	173640	So why am I creating this course?
173640	183640	Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space.
183640	189640	So I intend to build up from baby steps and then larger steps when things are fairly simple to work with.
190640	199640	And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you.
199640	204640	So with that being said, let's go ahead and jump in to the good stuff.
204640	209640	So in order to develop this project step by step, we're going to use something called Jupyter notebooks.
209640	214640	And you can sort of play with these in the Anaconda prompt or at least launch them from here.
214640	218640	So Anaconda prompt is just great for anything machine learning related.
218640	220640	So make sure to have this installed.
220640	227640	I will link a video in the description so that you can sort of set this up and install it step by step guide in there.
227640	233640	So we can do from this point is sort of just set up our project and initialize everything.
233640	243640	So I'm going to do is just head over into my directory that I want to be Python testing.
243640	250640	We're going to make a directory free code camp GPT course.
250640	254640	And then from this point, we're going to go and make a virtual environment.
254640	266640	So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around.
266640	269640	And what the virtual environment does is it sort of separates that.
269640	274640	So you have this isolated environment over here, and you can just play around with this however you want.
274640	283640	And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system.
283640	286640	When you're not in a virtual environment, if that makes sense.
286640	291640	So we're going to go ahead and set that up right now by using Python dash M.
291640	295640	And then we're going to go V and V for virtual V and V and then CUDA.
295640	306640	So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs.
306640	309640	GPUs are going to accelerate this a ton.
309640	313640	And basically CUDA is just that little feature in the GPU that lets us do that.
313640	316640	So we're going to make an environment called CUDA.
316640	317640	I'm going to go and press enter.
317640	320640	It's going to do that for us going to take a few seconds.
320640	323640	So now that's done, we can go ahead and do CUDA.
323640	328640	And we're just going to basically activate this environment so we can start developing in it.
328640	333640	I'm going to go backslash, we're going to go scripts, and then activate.
333640	335640	So now you can see it says CUDA base.
335640	339640	So we're in CUDA and then secondary base.
339640	341640	So it's going to prioritize CUDA.
341640	346640	So from this point, we can actually start installing some stuff, some libraries here.
346640	353640	So we can go pip three install Matt plot lib numpy.
354640	361640	We're going to use p y l m z a l z m a.
361640	364640	And then what are some other ones?
364640	367640	We're going to do IPY kernel.
367640	374640	This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks.
374640	376640	So that's why that's important.
376640	379640	And then just the actual Jupyter notebook feature.
379640	381640	So go and press enter.
381640	382640	Those are going to install.
382640	385640	That's going to take a few seconds to do.
385640	393640	So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm.
393640	397640	And don't quote me on this, but I'm pretty sure it's based in C plus plus.
397640	400640	So you actually need some build tools for this.
400640	405640	And you can get that with visual studio build tools.
405640	410640	So what you're you might see, you might see a little error and basically go to that website.
410640	411640	You're going to get this right here.
411640	414640	So just go ahead and download build tools.
414640	417640	What's going to download here, you're going to click on that.
417640	419640	It's going to, it's going to set up.
419640	425640	And then you're going to go ahead and click continue.
425640	429640	And then at this point, you can go ahead and click modify if you see this here.
429640	434640	And then you might get to a little workloads section here.
434640	436640	So once you're at workloads, that's good.
436640	441640	What you're going to make sure is that you have these two checked off right here.
441640	443640	Just make sure that you have these two.
443640	445640	I'm not sure what desktop particularly does.
445640	452640	It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects.
452640	455640	So just get these two for now.
455640	456640	That'll be good.
456640	460640	And then you can click modify over here if you wanted to modify just like that.
460640	465640	And then you should be good to rerun that command.
465640	477640	So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch.
477640	479640	We're not going to do it like this.
479640	487640	What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch.
487640	492640	So it's going to install the CUDA extension, which will allow us to utilize the GPU.
492640	494640	So it's just this command right here.
494640	505640	And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started.
505640	509640	And then you'll be able to see this right here.
509640	515640	So we have stable windows pip Python and CUDA 11.7 or 11.8.
515640	516640	So I just clicked on this.
516640	523640	And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch.
523640	529640	And then with this index URL for the CUDA 11.8.
529640	533640	So that's pretty much all we're doing there to install CUDA.
533640	534640	That's part of our torch.
534640	538640	So we can go ahead and click enter on this.
538640	539640	So great.
539640	544640	We've installed a lot of things, a lot of libraries, a lot of setup has been done already.
544640	548640	What I want to check now is just to make sure that our Python version is what we want.
548640	551640	So I've done version 3.10.9.
551640	552640	That's great.
552640	557640	If you're between 3.9, 3.10, 3.11, that's perfect.
557640	560640	So if you're in between those, it should be fine.
560640	564640	At this point, we can just jump right into our Jupyter Notebook.
564640	567640	So the command for that is just Jupyter Notebook.
567640	568640	It's about like that.
568640	571640	Click enter.
571640	574640	It's going to send us into here.
575640	582640	And I've created this little biogram.ipynb here in my VS Code.
582640	586640	So pretty much you need to actually type some stuff in it.
586640	592640	And you need to make sure that it has the ipynb extension or else it won't work.
592640	598640	So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason.
598640	600640	So just make sure you type some stuff in it.
600640	601640	Open that in VS Code.
601640	605640	Type, I don't know, a equals 3 or str equals banana.
605640	607640	I don't care.
607640	612640	At this point, let's go ahead and pop into here.
612640	614640	So this is what our notebook is going to look like.
614640	619640	And we're going to be working with this quite a bit throughout this course.
619640	627640	So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook.
627640	633640	And make sure that we can interact with it from this kernel rather than just through the command prompt.
633640	635640	So we're going to go ahead and check here.
635640	637640	And I have a virtual environment here.
637640	642640	You may not, but all we're going to do is basically go into here.
642640	644640	We're going to end this.
644640	656640	And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install.
657640	664640	User, you'll see why we're doing this in the second user name equals CUDA.
664640	667640	This is from the virtual environment we initialized before.
667640	669640	So that's the name of the virtual environment.
669640	680640	And then the display name, how it's actually going to look in the terminal is going to be display name.
681640	688640	We'll just call it CUDA GPT.
688640	689640	I don't know.
689640	690640	That sounds like a cool name.
690640	692640	And I'm going to press enter.
692640	697640	It's going to make this environment for us great installed.
697640	698640	Good.
698640	705640	So we can go and run our notebook again and we'll see if this changes.
705640	712640	So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT.
712640	714640	Let's click that.
714640	715640	Sweet.
715640	721640	So now we can actually start doing more and just sort of experimenting with how the notebooks
721640	728640	work and actually how we can build up this bi-gram model and sort of learning how language
728640	730640	models work from scratch.
730640	731640	So let's go ahead and do that.
731640	739640	Now that we jump into this actual code here, what I want to do is delete all of these.
739640	740640	Good.
740640	745640	So now what I'm going to do is just get a small little data set, just very small for us to
745640	750640	work with that we can sort of try to make a bi-gram out of, something very small.
750640	756640	So what we can do is go to this website called Project Gutenberg and they basically just
756640	761640	have a bunch of free books that are licensed under Creative Commons.
761640	765640	So we can use all of these for free.
765640	769640	So let's use the Wizard of Oz.
769640	773640	Put it at the end of Wizard of Oz.
773640	775640	Great.
775640	778640	So what we're going to want to do is just click on plain text here.
778640	779640	Great.
779640	789640	So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore
789640	791640	of underscore Oz.
791640	792640	Good.
792640	804640	So now what I'm going to do is we should probably drag this into, we should drag this into our
804640	805640	folder here.
805640	810640	I'm just going to pop that into there.
810640	811640	Good stuff.
811640	813640	Did that work?
813640	814640	Sweet.
814640	817640	So now we have our Wizard of Oz text in here, we can open that.
817640	821640	What we can do is start of this book.
821640	822640	Okay.
822640	829640	So we can go ahead and go down to when it starts.
829640	838640	Sweet.
838640	841640	So maybe we'll just cut it here.
841640	843640	That'd be a good place to start.
843640	844640	Just like that.
844640	847640	I'll put a few spaces.
847640	848640	Good.
848640	850640	So now we have this book.
850640	857640	We go to the bottom here just to get rid of some of this other licensing stuff, which
857640	863640	might get in the way with our predictions in the context of the entire book.
863640	866640	So let's just go down to when that starts.
866640	869640	End of the book.
869640	871640	Okay.
871640	877640	So we've gotten all that.
877640	878640	That is done.
878640	881640	Get rid of the illustration there.
881640	882640	Perfect.
882640	885640	So now we have this Wizard of Oz text that we can work with.
885640	887640	Let's close that up.
887640	888640	233 kilobytes.
888640	889640	Awesome.
889640	890640	Very small size.
890640	891640	We can work with this.
891640	892640	This is great.
892640	894640	So we have this wizard of Oz dot txt file.
894640	895640	And what are we going to do with that?
895640	900640	Well, we're going to try to train a transformer or at least a background language model on
900640	901640	this text.
901640	905640	So in order to do that, we need to sort of learn how to manage this text file, how to
905640	907640	open it, et cetera.
907640	915640	So we're going to go ahead and open this and do wizard of Oz.
915640	916640	Like that.
916640	917640	And we're going to open in read mode.
917640	922640	And then we're going to use the encoding utf 8 just like that.
922640	925640	So this is the file mode that you're going to open in.
925640	926640	There's read mode.
926640	927640	There's write mode.
927640	928640	There's read binary.
928640	929640	There's write binary.
929640	936640	And those are really the only ones we're going to be worrying about for this video.
936640	940640	The other ones you can look into in your spare time if you'd like to.
940640	943640	I've already seen using those four for now.
943640	947640	And then the encoding is just what type of character coding are we using?
947640	948640	That's pretty much it.
948640	951640	We can just open this as F short for file.
951640	954640	I'm going to go text equals f dot read.
955640	958640	I'm going to read this file stored in a string variable.
958640	961640	And then we can print some stuff about it.
961640	965640	So we can go print the length of this text.
965640	966640	Run that.
966640	970640	We get the length of the text.
970640	976640	We could print the first 200 characters of the text.
976640	977640	Sure.
977640	979640	So you have the first 200 characters.
979640	980640	Great.
981640	984640	So now we know how to, you know, just play with characters.
984640	988640	At least just see what the characters actually look like.
988640	993640	So now we can do a little bit more from this point, which is going to be encoders.
993640	1000640	And before we get into that, what I'm going to do is put these into a little vocabulary
1000640	1002640	list that we can work with.
1002640	1008640	So all I'm going to do is I'm going to say we're going to make a charge variable.
1008640	1014640	So the charge is going to be all the charge or all the characters in this text piece.
1014640	1028640	So we're going to make a sorted set of text here, and we're going to just print out charge.
1028640	1029640	So look at that.
1029640	1033640	We have a giant array of all these characters.
1033640	1039640	So now we can, what we can do is we can use something called a tokenizer and a tokenizer
1039640	1042640	consists of an encoder and a decoder.
1042640	1048640	What an encoder does is it's actually going to convert each character or sorry, each element
1048640	1051640	of this array to an integer.
1051640	1054640	So maybe this would be a zero.
1054640	1056640	This would be a one, right?
1056640	1062640	So a new, a new line or an enter would be a zero, a space would be a one exclamation
1062640	1064640	mark would be a two, et cetera, right?
1064640	1066640	All the way to the length of them.
1066640	1071640	And then what we could do is we could even, we could even print the length of these characters.
1071640	1073640	So you can see how many there actually are.
1073640	1078640	So there's 81 characters in the entire, in the entire Wizard of Oz book.
1078640	1082640	So I've written some code here that is going to do that job for us, the job of tokenizers.
1082640	1087640	So what we do is we just use a little generator, some generator for loops here,
1087640	1093640	a generator for loops rather, and we make a little mapping from strings to integers
1093640	1096640	and integers to strings, given the vocabulary.
1096640	1099640	So we just enumerate through each of these.
1099640	1104640	We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right?
1104640	1106640	That's basically all we're doing here.
1106640	1108640	And we have an encoder and a decoder.
1108640	1114640	So let's say we wanted to convert the string hello to integers.
1114640	1120640	So we go encode, and we could do hello, just like that.
1120640	1125640	And then we could go ahead and print this out.
1125640	1126640	Perfect.
1126640	1128640	Let's go ahead and run that.
1128640	1129640	Boom.
1129640	1133640	So now we have a conversion from characters to integers.
1133640	1139640	And then if we wanted to maybe convert this back, so decode it,
1139640	1145640	sort this in a little, maybe decoded hello equals that.
1145640	1153640	And then we could go or encoded rather encoded hello.
1153640	1158640	And then we could go decoded.
1158640	1164640	Hello is equal to we go decode and we can use the encoded hello.
1164640	1168640	So we're going to go ahead and encode this into integers.
1168640	1173640	And then we're going to decode the integers back to a character format.
1173640	1176640	So let's go ahead and print that out.
1176640	1181640	We're going to go ahead and print the decoded hello.
1181640	1182640	Perfect.
1182640	1184640	So now we get that.
1184640	1188640	So I'm going to fill you in on a little background information about these tokenizers.
1188640	1193640	So right now we're using the character level tokenizer, which takes basically each character
1193640	1196640	and converts it to an integer equivalent.
1196640	1203640	So we have a very small vocabulary and a very large amount of tokens to convert.
1203640	1208640	So if we have 40,000 individual characters, it means we have a small vocabulary to work
1208640	1212640	with, but a lot of characters to encode and decode, right?
1212640	1219640	If we have, if we work with maybe a word level tokenizer, that means we have a ton, like
1219640	1224640	every single word in the English language, I mean, if you're working with multiple languages,
1224640	1230640	this could be like, you know, a lot, very large amount of tokens.
1230640	1235640	So you're going to have like maybe millions or billions or trillions if you're, if you're
1235640	1236640	doing something weird.
1236640	1242640	But in that case, you're going to have a way smaller set to work with.
1242640	1248640	So you're going to have very large vocabulary, but a very small amount to encode and decode.
1248640	1254640	So if you have a subword tokenizer, that means you're going to be somewhere in between a character
1254640	1257640	level and a word level tokenizer, if that makes sense.
1257640	1262640	So in the context of language models, it's really important that we're efficient with our data
1262640	1265640	and just having a giant string might not work the best.
1265640	1270640	And we're going to be using a machine learning framework called pi torch or torch.
1270640	1273640	So I've imported this right here.
1274640	1281640	And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well.
1281640	1288640	A lot of the linear algebra, which involves a type of data structure called tensors.
1288640	1290640	So tensors are pretty much matrices.
1290640	1292640	If you're not familiar with those, that's fine.
1292640	1294640	We'll go over them more in the course.
1294640	1301640	But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with.
1301640	1303640	So I'm going to go ahead and delete these here.
1303640	1309640	And all we can do is just make our data element.
1309640	1313640	We could this is going to be the entire text data of the entire Wizard of Oz.
1313640	1323640	So we could go ahead and make this data equals and we're going to go torch tensor.
1323640	1326640	And then we're going to go and code.
1326640	1328640	We're going to put the text inside of that.
1328640	1331640	So we're going to go ahead and encode this text right here.
1331640	1344640	And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long.
1344640	1350640	This basically means we're just going to have this as a super long sequence of integers.
1350640	1357640	And yeah, let's go see what we can do with this torch tensor element right here.
1357640	1364640	So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data.
1364640	1368640	So it's pretty much the same thing in terms of working with arrays.
1368640	1379640	It's just a different type of data structure in the context of pi torch sort of easier to work within that way.
1379640	1391640	Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot.
1391640	1396640	But we're going to go over some of this stuff later in the course just about how to do all this math.
1396640	1408640	We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff.
1408640	1413640	So next I'm going to talk about is something called validation and training splits.
1413640	1420640	So why don't we just, you know, use the entire text document and only train on that entire text corpus?
1420640	1421640	Why don't we train on that?
1421640	1427640	Well, the reason we actually split into training and validation sets, I'm going to show you right here.
1427640	1429640	So we have this giant text corpus.
1429640	1430640	It's a super long text file.
1430640	1434640	Think of it as a, you know, an essay, but a lot of pages.
1434640	1440640	So this is our entire corpus and we make our training set, you know, 80% of it.
1440640	1441640	So maybe this much.
1441640	1445640	And then the other validation is this 20% right here.
1445640	1446640	Okay.
1446640	1457640	So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out.
1457640	1459640	It would have it in the entire thing memorized.
1459640	1461640	It wouldn't really get anything useful out of that.
1461640	1464640	You would only know this document.
1464640	1469640	But what the purpose of language modeling is, is to generate text that's like the training data.
1469640	1472640	And this is exactly why we put into splits.
1472640	1480640	So if we, if we run our training split right here, it's only going to know 80% of that entire corpus.
1480640	1484640	And it's only going to generate on that 80% instead of the entire thing.
1484640	1489640	And then we have our other 20%, which only knows 20% of the entire corpus.
1489640	1496640	So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document.
1496640	1500640	We're trying to generate text that's like the document.
1500640	1508640	Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare.
1508640	1515640	And the point is to generate Shakespearean like text, but not exactly what it looked like.
1515640	1521640	Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right?
1521640	1523640	We're trying to generate text that's like it.
1523640	1529640	So that's the entire reason, or at least that's most of the reason why we use train and vowel splits.
1529640	1533640	So you might be wondering, you know, like, why is this even called the bi-gram language model?
1533640	1536640	I'm actually going to show you how that works right now.
1536640	1540640	So if we go back to our whiteboard here, I've drawn a little sketch.
1540640	1547640	So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now.
1547640	1549640	We're just working with characters.
1549640	1552640	Pretty much we have two, right?
1552640	1555640	So by means to the by prefix means two.
1555640	1558640	So we're going to, we're going to have a bi-gram.
1558640	1563640	So given maybe, I mean, there's nothing before an H in this content.
1563640	1568640	So we just assume that's the start of content, and then that's going to point to an H.
1568640	1572640	So H is the most likely to come after the start.
1572640	1579640	And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L,
1579640	1582640	we're going to have another L, and then L leads to O, right?
1582640	1588640	So maybe there's going to be some probabilities associated with these.
1588640	1591640	So that's pretty much how it's how it's going to predict right now.
1591640	1595640	It's only going to consider the previous character to predict the next.
1595640	1598640	So we have given this one, we predict the next.
1598640	1601640	So there's two, which is why it's called bi-gram language model.
1601640	1608640	So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want,
1608640	1615640	how we can actually implement this into a neural network, an artificial neural network, and train it.
1615640	1624640	So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here,
1624640	1631640	just a small snippet, and we're going to make some predictions and we're going to make some targets out of that.
1631640	1638640	So our block size is just a bunch of encoded characters or integers that we have predictions and targets.
1638640	1644640	So let's say we take a small little size of maybe block size of five, okay?
1644640	1650640	So we have this tiny little tensor of five integers and these are our predictions.
1650640	1662640	So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one.
1662640	1668640	So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside.
1668640	1676640	So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one.
1676640	1679640	So we're going to be accessing the same indices.
1679640	1683640	So at index zero, it's going to be five, index zero is going to be 67, right?
1683640	1687640	So 67 is following five in the bi-gram language model.
1687640	1690640	So that's pretty much all we do.
1690640	1700640	So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target.
1700640	1705640	And then we can optimize for reducing that error.
1705640	1719640	So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here.
1719640	1723640	We would take a little snippet random.
1723640	1738640	It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size.
1738640	1743640	So five.
1743640	1746640	Ignore my terrible writing again.
1746640	1762640	And then this one would just be it would just be one up to block size or five plus one.
1762640	1764640	So we'll be up to six, right?
1764640	1766640	And that's that's pretty much all we do.
1766640	1768640	This is exactly what it's going to look like in the code.
1768640	1773640	So I've written some code here that does exactly what we just talked about in Python.
1773640	1781640	So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger.
1781640	1791640	And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one.
1791640	1796640	So we make it position one up to block size plus one little offset there.
1796640	1808640	And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are.
1808640	1812640	So this is what this looks like in Python, right, we can do predictions.
1812640	1815640	But this isn't really scalable yet.
1815640	1818640	This is sequential right sequential.
1818640	1825640	It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly.
1825640	1830640	That only happens sequentially it's this one and this task and this task and this task, right.
1830640	1837640	But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel.
1837640	1852640	So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them.
1852640	1865640	So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot.
1865640	1868640	So I'm going to illustrate that for you right now.
1868640	1870640	So let's just say we have a block.
1870640	1872640	Okay, block looks like this.
1872640	1879640	And we have some we have some integers in between here.
1879640	1882640	So this is a block.
1882640	1884640	Okay.
1884640	1888640	Now, if we want to make multiple of these, we're just going to stack them.
1888640	1892640	So we're going to make another one.
1892640	1895640	Another one.
1895640	1896640	Another one.
1896640	1898640	So let's say we have four batches.
1898640	1899640	Okay.
1899640	1900640	Or sorry, four blocks.
1900640	1905640	So we have four different blocks that are just stacked on top of each other.
1905640	1910640	And we can represent this as a new hyper parameter called batch size.
1910640	1915640	This is going to tell us how many of these sequences can we actually process in parallel.
1915640	1923640	So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time.
1923640	1927640	So this is a really good way to scale language models.
1927640	1932640	And without these, you can't really expect any fast training or good performance at all.
1932640	1939640	So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process.
1939640	1944640	And we can, it just takes one line to do this actually.
1944640	1957640	So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation.
1958640	1964640	And if it's available, like it says it's available, we'll set the device to CUDA else CPU.
1964640	1967640	So we're going to go and print out the device here.
1967640	1970640	So that's going to run and we get CUDA.
1970640	1975640	So that means we can use the GPU for a lot of our processing here.
1975640	1982640	And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size.
1982640	1987640	And then we're going to use batch size, which is how many blocks we're doing in parallel.
1987640	1990640	And we're just going to make this four for now.
1990640	1994640	So these are our two hyper parameters that are very, very important for training.
1994640	2008640	And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it.
2008640	2019640	And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first.
2019640	2023640	It'll shut down and you just go up Jupyter notebook again and then enter.
2023640	2032640	And then after this is done, you should be able to just restart that and it will work.
2032640	2033640	Hopefully.
2033640	2035640	There we go.
2035640	2040640	So I go in restart and clear outputs.
2040640	2042640	And we can run that.
2042640	2043640	See, we get boo.
2043640	2045640	So awesome.
2045640	2050640	Now, let's try to do some actual cool pie torch stop.
2050640	2054640	So we're going to go in and import torch here.
2054640	2059640	And then let's go ahead and try this random feature.
2059640	2061640	So you go random.
2061640	2064640	We'll do equals torch dot random.
2064640	2070640	And then let's say we go minus 100 to 100.
2070640	2074640	And then in brackets, we go six, just like that.
2074640	2080640	So if we want to print this out here, or we could just go random like that.
2080640	2082640	Run this block first.
2082640	2083640	Good.
2083640	2084640	And boom.
2084640	2087640	So we get a tensor type.
2087640	2091640	And all these numbers are we have we have six of them.
2091640	2093640	So 123456.
2093640	2096640	And they're between negative 100 and 100.
2096640	2102640	So we're going to have to keep this in mind right here when we're getting our random
2102640	2105640	batches from this giant text corpus.
2105640	2107640	So let's try out a new one.
2107640	2109640	Let's just try.
2109640	2111640	We can make we can make tensors.
2111640	2112640	We've done this before.
2112640	2117640	So you do tensor equals torch dot tensor.
2117640	2130640	So if you go 0.1, 1.2, here, I'll just copy and paste one right here.
2130640	2132640	So we do this.
2132640	2138640	And we can just do tensor and we'll get exactly this.
2138640	2145640	So we get a three by two matrix.
2145640	2148640	Now we're going to try a different one called zeros.
2148640	2151640	So zeros is just torch dot zeros.
2151640	2156640	And then inside of here, we could just do the dimensions or the shape of this.
2156640	2160640	So two by three, and then we could just do zeros.
2160640	2162640	And then go ahead and run that.
2162640	2166640	So we get a two by three of zeros.
2166640	2170640	And these are all floating point numbers, by the way.
2170640	2171640	Maybe we could try ones.
2171640	2173640	Now I know ones is pretty fun ones.
2173640	2177640	So we both torch dot ones.
2177640	2179640	It's pretty much the same as zeros.
2179640	2185640	We could just do like maybe three by four and then print that ones out.
2185640	2188640	So we have a three by four of ones.
2188640	2189640	Sweet.
2189640	2197640	So what if we do input equals torch dot empty.
2197640	2205640	We can make this two by three.
2205640	2208640	So these are interesting.
2208640	2215640	These are pretty much a bunch of very either very large or very small numbers.
2215640	2218640	I haven't particularly found a use case for this yet,
2218640	2221640	but just another feature that PyTorch has.
2221640	2223640	We have a range.
2223640	2229640	So we go arrange equals torch dot arrange.
2229640	2234640	I could do like five, for example, just do range.
2234640	2242640	So now we have a tensor just sorted zero or rather starting at zero up to four.
2242640	2247640	So five, just just like that.
2247640	2256640	Line space equals torch dot line line space.
2256640	2264640	Spelling is weird to three, 10, and then steps, for example, equals five.
2264640	2267640	So it makes sense in a second here, go run.
2267640	2271640	And we got a line space of steps equals five.
2271640	2274640	So we have five different ones, boom, boom, boom, boom, boom.
2274640	2276640	And we go all the way from three to 10.
2276640	2283640	So pretty much getting all of the constant increments from three all the way up to 10 over five steps.
2283640	2287640	So you're doing, you're basically adding the same amount every time.
2287640	2295640	So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right?
2295640	2299640	So just over five steps, we want to find what that constant increment is.
2299640	2302640	So that's a pretty cool one.
2302640	2307640	And then we have, we'll do log space, which is interesting.
2307640	2311640	Log space equals torch dot log space.
2311640	2328640	And then we'll go start, start equals negative 10 and equals 10.
2328640	2330640	These are both start and end.
2330640	2331640	You can either put these here.
2331640	2335640	You can either put the start with them, start equals, or you don't have to.
2335640	2337640	It's honestly up to you.
2337640	2340640	And then we can put our steps again.
2340640	2343640	So steps equals every five.
2343640	2345640	Let's go ahead and run that.
2345640	2350640	Oops, need to put log space there.
2350640	2351640	So we get that.
2351640	2355640	So we start at one of the negative 10.
2355640	2357640	And then we just do this little increments here.
2357640	2359640	So it goes 10, negative five, zero plus five times.
2359640	2360640	Just over five steps.
2360640	2362640	So that's pretty cool.
2362640	2364640	What else do we have here?
2364640	2368640	So we have I, torch dot I.
2368640	2370640	I just have all these on my second screen here.
2370640	2376640	So a bunch of examples just written out and we're just kind of visualizing what these can do.
2376640	2387640	And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do.
2387640	2390640	So we're just kind of experimenting with these.
2390640	2395640	What we can do with the basics of pytorch and some of the very basic functions.
2395640	2401640	So first I will print this out here.
2401640	2409640	So we get pretty much just a diagonal line and it's in five.
2409640	2419640	So you get a five by five matrix and pretty much just reduced row each long form.
2419640	2423640	I don't know how to pronounce it, but that's pretty much what it looks like.
2423640	2426640	So pretty cool stuff.
2426640	2429640	Let's see what else we have.
2429640	2435640	We have empty like.
2435640	2453640	We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty.
2453640	2469640	And then we can go two by three and then data type torch dot int 64 64 bit integers.
2469640	2477640	And then let's see what happens here empty.
2477640	2480640	So that's pretty cool.
2480640	2481640	What else do we have?
2481640	2483640	Yes, we can do timing as well.
2483640	2487640	So I'm just going to erase all of these.
2487640	2498640	You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit.
2498640	2501640	But yeah, I'm actually going to delete all of these here.
2501640	2517640	And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment.
2517640	2536640	Cuda if torch dot Cuda underscore is dot Cuda is available.
2536640	2548640	And then else you print out our device here and run this Cuda suite.
2548640	2564640	So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality.
2564640	2569640	We're just doing different experiments with a bunch of different tensors.
2569640	2576640	So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time.
2576640	2580640	So this comes with the operating system or sorry with with Python.
2580640	2583640	You don't have to actually install this manually.
2583640	2593640	So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now.
2593640	2600640	So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds.
2600640	2606640	So if we subtract end time, start time will get a three second difference and that would be the total elapsed time.
2606640	2614640	And then this little number here, this four will be just how many decimal places we have.
2614640	2616640	So I can go ahead and run this here.
2616640	2620640	Time is not defined. Let's run that first.
2620640	2625640	It's going to take, you know, almost no time at all.
2625640	2629640	So we can actually increase this if we want to 10 and then run that again.
2629640	2633640	Again, it's, you know, we're making up pretty much a one by one matrix.
2633640	2635640	So just a just a zero.
2635640	2641640	So we're not really going to get anything significant from that.
2641640	2657640	But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network.
2657640	2667640	That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem.
2667640	2671640	It'll take maybe a few seconds at most like maybe 30 seconds to do that.
2671640	2675640	And that's not going to be any time at all in the entire training process.
2675640	2686640	So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU.
2686640	2688640	So I have some code for that right here.
2688640	2692640	So we're going to initialize a bunch of matrices here.
2692640	2698640	So our sorry, tensors, and we have just basically random ones.
2698640	2703640	So we have a 10,000 by 10,000, all random, all random floating point numbers.
2703640	2705640	And then we're going to push these to the GPU.
2705640	2709640	And we have two of these and then same thing for NumPy.
2710640	2717640	So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.
2717640	2729640	So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply.
2729640	2737640	So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds.
2737640	2740640	And then we have, see, look at that.
2740640	2745640	So for the GPU, it took a little while to do that.
2745640	2749640	And then for the CPU, it didn't take as long.
2749640	2756640	So this is because there's the shape of these matrices are not really that big.
2756640	2758640	They're just two dimensional, right?
2758640	2764640	So it's see, this is something that the CPU can do very quickly because there's not that much to do.
2764640	2767640	But let's say we want to bump it up a notch.
2767640	2775640	So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.
2775640	2776640	Hopefully that works.
2776640	2779640	And then we can do, we'll just do the same thing.
2779640	2781640	So just paste this.
2782640	2798640	Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did.
2798640	2803640	And this is because there's, you know, a lot more going on here.
2803640	2807640	There's a lot more simple multiplication to do.
2807640	2815640	So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model,
2815640	2821640	we're not going to be doing very complex operations between all these tensors.
2821640	2824640	They're going to be very similar to what we saw in here.
2824640	2829640	The dimensionality and shape is going to be very similar to what we're seeing right now.
2829640	2831640	You know, maybe three or four dimensions.
2831640	2835640	And it's going to be very easy for a GPU to do this.
2835640	2838640	They're not complex tasks that we need the CPU to do.
2838640	2840640	They're not very hard at all.
2840640	2848640	So when we give this task to parallel processing, it's going to be a ton quicker.
2848640	2851640	So you're going to see why this matters later in the course.
2851640	2855640	You're going to see this with some of the hyper parameters we're going to use,
2855640	2860640	which I'm not going to get into quite yet, but over the next little bit,
2860640	2868640	you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process.
2868640	2869640	So this is great.
2869640	2877640	Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency.
2877640	2883640	So there's actually another term that we can use called a percentage percentage time.
2883640	2887640	I don't know if that's exactly how you're supposed to call it, but that's what it is.
2887640	2892640	And pretty much what it'll do is time how long it takes to execute a block.
2892640	2897640	So we can see here there's CPU times zero nanoseconds.
2897640	2902640	The end is for nano billionth of a second is a nanosecond and then wall time.
2902640	2908640	So CPU time is how long it takes to execute on the CPU.
2908640	2916640	The time that it's doing operations for and then the wall time would be how long it actually takes like in real time.
2916640	2920640	How long do you have to wait? Do you have to wait until it's finished?
2920640	2925640	So the only thing that the CPU CPU time doesn't include is waiting.
2925640	2930640	So in an entire process, there's going to be some operations and there's going to be some waiting.
2930640	2936640	Wall time is going to have both of those and CPU time is just the execution.
2936640	2942640	So let's go ahead and continue with some of the basic PyTorch functions.
2942640	2946640	So I've written some stuff down here.
2946640	2954640	So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu.
2954640	2957640	I don't think that's how you pronounce it, but we'll get into that more.
2957640	2964640	Transposing, linear, concatenating, and the softmax function.
2964640	2968640	So let's first start off here with the Torch.multinomial.
2968640	2974640	So this is essentially a probability distribution based on the index that you give it.
2974640	2976640	So we have probabilities here.
2976640	2979640	We say 0.1 and 0.9.
2979640	2982640	These numbers have to add up to one to make 100%.
2982640	2984640	100% is one, one whole.
2984640	2987640	So I have 10% and 90%.
2987640	2989640	This is an index zero.
2989640	2995640	So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one.
2995640	3005640	So if I go ahead and run these up here.
3005640	3007640	Give this a second to do its thing.
3007640	3011640	So you can see that in the end we have our numSample set to 10.
3011640	3013640	So it's going to give us 10 of these.
3013640	3015640	1, 2, 3, 4, 5, 6, 7, 9, 10.
3015640	3017640	And all of them are ones.
3017640	3020640	If we run it again, we make it slightly different results.
3020640	3022640	So now we have some zeros in there.
3022640	3025640	But the zeros have very low probability of happening.
3025640	3029640	As a matter of fact, exactly a 10% probability of happening.
3029640	3036640	So we're going to use this later in predicting what word is going to come next.
3036640	3042640	Let's move on to Torch.cat or short for Torch.concatenate.
3042640	3046640	So this will essentially concatenate two tensors into one.
3046640	3050640	So I initialize this tensor here, torch.tensor, 1, 2, 3, 4.
3050640	3052640	It's one dimensional.
3052640	3056640	And we have another tensor here that just contains five.
3056640	3063640	So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5.
3063640	3068640	We just combine them together and this is what will come out in the end.
3068640	3070640	So we run that 1, 2, 3, 4, 5.
3070640	3072640	Perfect.
3072640	3075640	So we're going to actually use this when we're generating.
3075640	3078640	When we're generating text given a context.
3078640	3082640	So it's going to start from zero.
3082640	3085640	We're going to use our probability distribution to pick the first one.
3085640	3093640	And then based on the first one, we're going to, you know, we're going to predict the next character.
3093640	3101640	And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted.
3101640	3104640	So we have this, maybe like 100 characters over here.
3104640	3106640	And then the next character that we're predicting is over here.
3106640	3108640	We just concatenate these.
3108640	3114640	And by the end, we will have all of the integers that we've predicted.
3114640	3117640	So next up, we have torch.trill.
3117640	3123640	And what this stands for, what the trail stands for is a triangle lower.
3123640	3127640	So it's going to be in a sort of a triangle formation like this diagonal.
3127640	3132640	It's going to be going from top left to bottom right.
3132640	3136640	And so you're going to see a little bit more why later in this course.
3136640	3150640	But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history.
3150640	3152640	We're trying to predict the future.
3152640	3156640	So giving the answers in the future isn't what we want to do at all.
3156640	3160640	So maybe we've just predicted one and the rest of them we haven't predicted yet.
3160640	3162640	So we set all these to zero.
3162640	3164640	And then we predicted another one.
3164640	3165640	And these are still zero.
3165640	3167640	So these are talking to each other in history.
3167640	3178640	And as and as our predictions add up, we have more and more history to look back to and less future, right?
3178640	3184640	Basically, the premise of this is just making sure we can't communicate with the answer.
3184640	3191640	We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet.
3191640	3193640	They don't give you the answer sheet.
3193640	3199640	So you have to know based on your history of knowledge, which answers to predict.
3199640	3202640	And that's all that's going on here.
3202640	3206640	And we have, I mean, you could probably guess this triangle upper.
3206640	3208640	So we have all the upper ones.
3208640	3212640	These are, you know, lower on the lower side and then these are on the upper side.
3212640	3214640	So same concept there.
3214640	3217640	And then we have a masked fill.
3217640	3228640	So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here.
3228640	3233640	So if you exponentiate zero, if you exponentiate zero, it'll become one.
3233640	3237640	If you exponentiate negative infinity, it'll become zero.
3237640	3243640	All that's going on here is we're doing approximately 2.71.
3243640	3248640	And this is a constant that we use in the dot exp function.
3248640	3254640	And then we're putting this to whatever power is in that current slot.
3254640	3256640	So we have a zero here.
3256640	3266640	So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71.
3267640	3280640	And then 2.71 to the negative infinity is, of course, zero.
3280640	3283640	So that's pretty much how we get from this to this.
3283640	3288640	And we're just, we're simply just masking these over.
3288640	3290640	So that's great.
3290640	3294640	And I sort of showcase what the exp does.
3294640	3296640	And we're just using this one right here.
3296640	3300640	We're using this output and we're just plugging it into here.
3300640	3305640	So it'll go from negative infinity to zero and then zero to one.
3305640	3308640	So that's how we get from here to here.
3308640	3311640	Now we have transposing.
3311640	3316640	So transposing is when we sort of flip or swap the dimensions of a tensor.
3316640	3323640	So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four.
3323640	3330640	And we can use the transpose function to essentially flip any dimensions that we want.
3330640	3337640	So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension,
3337640	3341640	but we're pretty much swapping the zero with position with the second.
3341640	3345640	So zero, one, two, we're swapping this one with this one.
3345640	3351640	So the end result, like you would probably guess the shape of this is going to be 432 instead of 234.
3351640	3355640	So you kind of just take a look at this and see, you know, which ones are being flipped.
3355640	3359640	And those are the dimensions and that's the output.
3359640	3361640	So hopefully that makes sense.
3361640	3363640	Next up, we have torch dot stack.
3363640	3366640	And this is where we're actually going to go.
3366640	3368640	We're going to we're going to do more of this.
3368640	3375640	We're actually going to use torch dot stack stack very shortly here when we're getting our batches.
3375640	3383640	So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant,
3383640	3388640	a giant length of integers or tokens.
3388640	3394640	And all we're doing is we're just stacking them together in blocks or to make a batch.
3394640	3397640	So that's pretty much what we're going to end up doing.
3397640	3399640	And that's what torch dot stack does.
3399640	3406640	We can take something that's one dimensional and then we can stack it to make it two dimensional.
3406640	3412640	We can take something that's two dimensional and stack it a bunch of times to make it three dimensional.
3412640	3418640	Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other.
3418640	3419640	Now it's four dimensional.
3419640	3421640	So hopefully that makes sense.
3421640	3425640	All we're doing is we're just passing in each tensor that we're going to stack in order.
3425640	3429640	So this is our little output here and that's pretty much all it is.
3429640	3437640	The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish.
3437640	3438640	It's really important.
3438640	3440640	It's called the nn dot linear function.
3440640	3445640	So it is a pretty much a function of the nn dot module.
3445640	3455640	And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters.
3455640	3461640	So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false.
3461640	3470640	But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better.
3470640	3479640	And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output.
3479640	3486640	So pretty much anything with nn dot linear is going to be very important and it's going to be learnable.
3486640	3488640	So we can see over here.
3488640	3492640	This is the tors.nn little site here on the docs.
3492640	3499640	So we have containers, a bunch of different layers like activations, layers, pretty much just layers.
3499640	3500640	That's all it is.
3500640	3503640	And so these are these are important.
3503640	3506640	We're going to, we're basically going to learn from these.
3506640	3512640	And you're going to see why we're going to use something called keys and values, keys, values and queers later on.
3512640	3513640	You'll see why those are important.
3513640	3518640	But if that doesn't make sense yet, help me, let me illustrate value for you right now.
3518640	3520640	So I drew this out here.
3520640	3528640	So if we look back at our examples, we have a, we make, we initialize a term.
3528640	3531640	We make, we initialize a tensor.
3531640	3533640	It's 10, 10 and 10.
3533640	3536640	What we're going to do is we're going to do a linear transformation.
3536640	3538640	This linear stands for linear transformation.
3538640	3544640	So pretty much we're just going to apply a weight and a bias through each of these layers here.
3544640	3549640	So we have an input and we have an output x is our input, y is our output.
3549640	3553640	And this is of size three and this is of size three.
3553640	3556640	So pretty much we just need to make sure that these are lining up.
3556640	3566640	And for more context, the nn.sequential is sort of built off nn.linear.
3566640	3572640	So if we go ahead and search that up right now, this will make sense in a second here.
3572640	3577640	This is also some good prerequisite knowledge in general for machine learning.
3577640	3585640	So let's see nn.sequential doesn't show it here, but pretty much.
3585640	3593640	If you have, let's say, two, you have two input neurons and maybe you have one output neuron.
3593640	3595640	Okay, you have a bunch of hidden layers in between here.
3595640	3602640	Let's say we have one, two, three, four, and then one, two, three.
3602640	3609640	So pretty much you need to make sure that the inputs aligns with this hidden layer.
3609640	3612640	This hidden layer aligns with this one and this one aligns with this one.
3612640	3617640	So you're going to have a transformation of two to four.
3617640	3625640	So two, four, and then this one's going to be four to three, four to three,
3625640	3627640	and then you're going to have a final one.
3627640	3631640	This is two to four right here, four to three here, and then this final one.
3631640	3633640	It's going to be three to one.
3633640	3637640	So you pretty much just need to make sure that these are lining up.
3637640	3643640	So we can see that we have two, four, and then this four is carried on from this output here.
3643640	3647640	And pretty much this will just make sure that our shapes are consistent.
3647640	3652640	And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work.
3652640	3654640	So we need to make sure that our shapes are consistent.
3654640	3659640	If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets,
3659640	3664640	but if you're really interested, you could use chatGPT, of course.
3664640	3669640	And that's a really good learning resource, chatGPT, going on to get up discussions, maybe,
3669640	3672640	or just looking at documentation.
3672640	3679640	And if you're not good at reading documentation, then you could take maybe some little keywords from here,
3679640	3682640	like a sequential container.
3682640	3684640	Well, what is a sequential container?
3684640	3689640	You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation
3689640	3691640	and figure things out step by step.
3691640	3698640	It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on.
3698640	3700640	You don't need to memorize them.
3700640	3704640	But while you're working with them, it's important to understand what they're really doing behind the scenes,
3704640	3710640	especially if you want to make an efficient and popular working neural net.
3710640	3713640	So that's that.
3713640	3720640	And pretty much what's going to happen here with these linear layers is we're just going to simply transform
3720640	3723640	from one to the other input to output, no hidden layers.
3723640	3726640	And we're just going to be able to learn best parameters for doing that.
3726640	3730640	You're going to see why that's useful later.
3730640	3732640	Now we have the softmax function.
3732640	3734640	So that sounds scary.
3734640	3738640	And the softmax function isn't actually what it sounds like at all.
3738640	3740640	Let me illustrate that for you right now.
3740640	3745640	So let's go ahead and change the color here.
3745640	3755640	So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc.
3755640	3757640	Right, floating points, whatever.
3757640	3768640	So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these.
3768640	3773640	And we're going to divide them by the sum of all of these exponentiated.
3773640	3777640	So pretty much what's going to happen, let's say we exponentiate one.
3777640	3785640	So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp.
3785640	3788640	And I think I talked about this up here.
3788640	3795640	This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating.
3795640	3803640	So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71.
3803640	3815640	And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two.
3815640	3817640	Okay, so we're going to get 7.34.
3818640	3820640	I'm going to get 7.34.
3820640	3822640	Gorg my writing, it's terrible.
3822640	3827640	2.71 to 3 cubed.
3827640	3828640	So 19.9.
3831640	3835640	So pretty much what's going to happen is we can rearrange this in a new array.
3835640	3840640	7.34 and 19.9.
3840640	3846640	So if we add all these up together, we add all these up together, we're going to get 2.71 plus this.
3846640	3848640	Let's do this math real quick.
3848640	3852640	I'm just going to walk you through this to help you understand what the softmax function is doing.
3852640	3860640	7.34 plus 19.9.
3860640	3863640	That's going to give us a total of 29.95.
3863640	3864640	Great.
3864640	3869640	29.95.
3869640	3876640	So all we do is we just divide each of these elements by the total.
3876640	3880640	So 2.71 divided by this is going to give us maybe x.
3880640	3884640	And we do 7.34 divided by this is going to give us y.
3884640	3889640	And then we have 19.9 divided by this is going to give us z.
3889640	3894640	So pretty much you're going to exponentiate all of these.
3894640	3897640	You're going to add them together to create a total.
3897640	3902640	And then you're going to divide each of those exponentiate elements by the exponentiated total.
3902640	3909640	So after that, this x right here is just, we're just going to wrap these again.
3909640	3917640	And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z.
3917640	3919640	That's all it's doing.
3919640	3922640	And yeah, it's not really crazy.
3922640	3925640	There's a weird formula for it.
3925640	3930640	Softmax, softmax function.
3930640	3935640	So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here
3935640	3939640	and a lot of math that's above the high school level.
3939640	3943640	But yeah, like this formula here, I believe this is what it is.
3943640	3946640	Or standard unit, softmax function, there you go.
3946640	3948640	So pretty much this is what it does.
3948640	3951640	And there's your easy explanation of what it does.
3951640	3956640	So you're going to see why this is useful later, but it's just important to know what's going on
3956640	3962640	so that you won't lag behind later in the course when this background knowledge becomes important.
3962640	3968640	So if we go over a little example of that, of the softmax function in code, it looks like this right here.
3968640	3973640	So we import torsha and n dot functional as f, f short for functional.
3973640	3977640	And we pretty much just do f dot softmax and then plug in a tensor.
3977640	3983640	And what we want the dimension to be the output dimension.
3983640	3989640	So if we plug this into here and we print it out, we go and print it out.
3989640	3995640	It's going to take a second.
3995640	3999640	Torch is not defined. So let's run this from the top here.
3999640	4002640	Boom.
4002640	4004640	And let's try that again. Boom. There we go.
4004640	4008640	So if you took all those values, let's actually do this again from scratch.
4008640	4016640	So we do 2.71, 2.71 divided by 29.95.
4016640	4022640	We get 0.09, 0.09. Good.
4022640	4032640	And then if we do 7.34 divided by 29.95, we get 0.245.
4032640	4036640	So 0.245. Well, it's kind of close.
4036640	4042640	Really close actually. And then 66.52. So if we go, what was that last one there?
4042640	4050640	19.9. So we do 19.9 divided by 29.95.
4050640	4055640	66.4. So 66.5. It's pretty close.
4055640	4061640	Again, we're rounding, so it's not perfectly accurate.
4061640	4067640	As you can see, they're very close and for only having two decimal places, we did pretty good.
4067640	4072640	So that's just sort of illustrating what the softmax function does and what it looks like in code.
4072640	4081640	We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line.
4081640	4084640	It's just like that.
4084640	4087640	So now we're going to go over embeddings.
4087640	4090640	And I'm not actually, I don't have any code for this yet.
4090640	4096640	We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills
4096640	4103640	and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning.
4103640	4113640	So if we pop in a chat GPT here, we say, what is an end dot embedding?
4113640	4125640	And then dots. Let me type in a non-bedding class in the PyTorch library.
4125640	4131640	Okay, actual language processing max maps each discrete input to a dense vector representation.
4131640	4134640	Okay, how does this work? Let's see.
4134640	4138640	So we have some vocab. So that's probably our vocabulary size.
4138640	4145640	I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set.
4145640	4151640	That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter.
4151640	4156640	So let's see. This doesn't quite make sense to me yet.
4156640	4159640	So maybe I want to learn what does this actually look like?
4159640	4173640	Can you explain this to a, maybe an eighth grader and provide a visualization?
4173640	4177640	Certainly. Okay.
4177640	4182640	Little secret codes that represent the meaning of the words. Okay, that helps.
4182640	4187640	So if we have cat, okay, so cat, cat's a word.
4187640	4193640	So maybe we want to know what it would look like on a character level.
4193640	4203640	What about on a character level instead of word level?
4203640	4205640	So it's probably going to look very similar.
4205640	4210640	We have this little vector here storing some information about whatever this is.
4210640	4218640	So a, it means this here. Okay, so as your point to, and this is really useful.
4218640	4221640	So we've pretty much just learned what embedding vectors does.
4221640	4230640	And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character.
4230640	4234640	And we don't even know what each of these elements mean.
4234640	4235640	We don't know what they mean.
4235640	4241640	This could be maybe positivity or should be the start of a word or it could be any piece of information,
4241640	4244640	maybe something we can't even comprehend yet.
4244640	4255640	But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before,
4255640	4262640	nn.embedding right here is a part of the nn.module.
4262640	4265640	So these are learnable parameters, which is great.
4265640	4268640	So it's actually going to learn the importance of each letter,
4268640	4271640	and it's going to be able to produce some amazing results.
4271640	4281640	So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter.
4281640	4285640	In our case, it's character level, not subword, not word, it's character level.
4285640	4288640	So it's going to represent some meaning about those.
4288640	4290640	So that's what embedding vectors are.
4290640	4292640	Let's go figure out how they work in code.
4292640	4298640	We have this little character level embedding vector and it contains a list.
4298640	4305640	There's five elements in here, one, two, three, four, five, and it's by the vocab size.
4305640	4311640	So we have all of our vocabulary by the length of each embedding vector.
4311640	4316640	So this actually makes sense because our vocab size by the embedding dimension,
4316640	4322640	which is how much information is actually being stored in each of these characters.
4322640	4324640	So this now is very easy to understand.
4324640	4329640	I'm just going to copy this code from here and I'm going to paste it down here.
4329640	4336640	And let's just get rid of the torch because we already initialized that above.
4337640	4342640	So if we just run this, actually, let's turn that down to maybe a thousand characters.
4342640	4344640	Let's try that out.
4345640	4347640	And it's not defined.
4347640	4349640	We did not initialize it.
4360640	4363640	So let's go back down here and look at that.
4363640	4370640	So this dot shape is going to essentially show the shape of it this much by this much.
4370640	4372640	So it's four by a hundred.
4372640	4379640	And yeah, so we can we can work with these and we can store stuff about characters in them.
4379640	4385640	And you're going to see this in the next lecture, how we actually use embedding vectors.
4385640	4389640	So no need to worry if a lot of this doesn't make sense yet.
4389640	4390640	That's fine.
4390640	4393640	You're going to learn a little bit more about how we use these over the course.
4393640	4397640	You're going to get more confident with using them even in your own projects.
4397640	4399640	So don't don't stress about it too much right now.
4399640	4403640	Embeddings are pretty tricky at first to learn.
4403640	4405640	So don't worry about that too much.
4405640	4417640	But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks.
4417640	4429640	So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say.
4429640	4433640	But that's the architecture of this multilayer perceptron.
4433640	4438640	But pretty much what's happening is we have a little input here and we have a white matrix.
4438640	4441640	So white matrix is looks like this.
4441640	4451640	It's like this and we have some, we have some values in between X1, Y1 and maybe Z1.
4451640	4457640	So a bunch of weights and maybe biases to that we add to it.
4457640	4463640	So the tricky part is how do we actually multiply our input by this white matrix?
4463640	4465640	We're just doing one matrix times another.
4465640	4467640	Well, that's called matrix multiplication.
4467640	4470640	And I'm going to show you how to do that right now.
4470640	4474640	So first off, we have to learn something called dot products.
4474640	4480640	So dot products are actually pretty easy and you might have actually done them before.
4480640	4486640	So let's say we go ahead and take, we go ahead and take this right here we go.
4487640	4489640	One, two, three.
4489640	4491640	That's going to be what A is.
4491640	4495640	And then we have four, five, six.
4495640	4504640	So if we want to find the dot product between these two, all we have to do is simply take the index of both of these,
4504640	4508640	the first ones and the second ones and third ones, multiply them together and then add.
4508640	4520640	So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five,
4520640	4524640	and then add it to three times six.
4526640	4532640	So one times four is four, two times five is ten, three times six is eighteen.
4532640	4536640	So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two.
4536640	4542640	So the dot product of this is going to be thirty-two.
4542640	4545640	And that's pretty much how simple dot products are.
4545640	4554640	It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up.
4554640	4555640	That's a dot product.
4555640	4559640	So we actually need dot products for matrix multiplication.
4559640	4561640	So let's go ahead and jump into that right now.
4561640	4566640	So I'm just going to create two matrices that are going to be pretty easy to work with.
4566640	4572640	So let's say we have A and have one matrix over here.
4572640	4580640	It's going to be one, two, three, four, five and six.
4580640	4586640	This is going to be equal to A and then B is going to be another matrix.
4586640	4595640	So we're going to have seven, eight, nine, ten, eleven, twelve.
4595640	4597640	Ignore my terrible writing.
4597640	4601640	Pretty much what we do is to multiply these together.
4601640	4605640	First we need to make sure that they can multiply together.
4605640	4609640	So we need to take a look at the amount of rows and columns at this half.
4609640	4612640	So this one right here is three rows, one, two, three.
4612640	4614640	Three rows and two columns.
4614640	4617640	So this is going to be a three by two matrix.
4617640	4621640	And this one has two rows and three columns.
4621640	4624640	So it's a two by three matrix.
4624640	4631640	So all we have to make sure that if we're multiplying A dot product with B,
4631640	4635640	and this is the PyTorch syntax for multiplying matrices,
4635640	4641640	if we're multiplying A by B, then we have to make sure the following is true.
4641640	4649640	So if we use three by two and then dot product with two times three,
4649640	4654640	we have to make sure that these two inner values are the same.
4654640	4657640	So two is equal to two, so we cross these out,
4657640	4660640	and then the ones that we have left over are three by three.
4660640	4664640	So the resulting matrix would be A three by three.
4664640	4672640	Or if you had like a three by four times A five by five by one,
4672640	4675640	that doesn't work because these values aren't the same.
4675640	4678640	So these two matrices couldn't multiply.
4678640	4682640	And sometimes you actually have to flip these to make them work.
4682640	4687640	So maybe we change this value here to A three.
4687640	4689640	We change this value to a three.
4689640	4692640	In this order, they do not multiply.
4692640	4705640	But if we switch them around, we have a three by five with A five by three,
4705640	4709640	sorry, five by three with A three by four.
4709640	4711640	So these two numbers are the same.
4711640	4712640	That works.
4712640	4714640	The resulting matrix is a five by four.
4714640	4718640	So that's how you make sure that two matrices are compatible.
4718640	4721640	So now to actually multiply these together,
4721640	4723640	what we're going to do, I'm going to make a new line here.
4723640	4727640	So we're going to rewrite these.
4727640	4729640	Now we don't have to rewrite them.
4729640	4731640	Let's just cross that out here.
4731640	4741640	So pretty much what we have to do is we have to take these two and dot product with these two.
4741640	4748640	And then once we're done that, we do the same with these and these, these and these.
4748640	4754640	So we start with the first, the first row in the A matrix.
4754640	4758640	And we iterate through all of the columns in the B matrix.
4758640	4764640	And then after we're done that, we just go to the next row in the A matrix and then et cetera, right?
4764640	4766640	So let's go ahead and do this right now.
4766640	4770640	That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here.
4770640	4780640	So we have our one times, our one times seven plus two times 10.
4780640	4787640	So one times seven plus two times 10.
4787640	4790640	And this is equal to 27.
4790640	4796640	So that's the first dot product of one and two and seven and 10.
4796640	4801640	So what this is actually going to look like in our new matrix, I'm going to write this out here.
4801640	4804640	So this is our new matrix here.
4804640	4808640	This 27 is going to go right here.
4808640	4810640	Let's continue.
4810640	4816640	So next up, we're going to do one and two and then eight and 11.
4816640	4832640	So we're going to go one, one times eight plus two, or sorry, two and 11.
4832640	4835640	So one times eight is eight and then two times 11 is 22.
4835640	4841640	So our result here is 30 and 30 is just going to go right here.
4841640	4845640	So 27, 30, and you can see how this is going to work, right?
4845640	4853640	So in our first row of A, we're going to get the first row of this resulting matrix.
4853640	4858640	So let's go ahead and do the rest here.
4858640	4863640	So we have one and two and then nine and 12.
4863640	4870640	One times nine, two times 12.
4870640	4873640	One times nine is nine, two times 12 is 24.
4873640	4877640	So if we do, that's like 33, I believe.
4877640	4882640	So 33 and we can go ahead and write that here.
4882640	4885640	So now let's move on to the next.
4885640	4894640	We have three and four, three, three and four dot product with seven and 10.
4894640	4899640	So three will multiply seven.
4899640	4908640	And then we're going to go ahead and add that to four times 10.
4908640	4914640	Three times seven, three times seven is 21, and then four times 10 is 40.
4914640	4916640	So we're going to get 47.
4916640	4922640	So I'll put there so we can go in and write 47 right there.
4922640	4929640	And our next one is going to be three and four dot product with eight and 11.
4929640	4940640	So eight plus four times 11.
4940640	4941640	Perfect.
4941640	4947640	So we get three times eight is 24 and then plus 44.
4947640	4952640	So 24 plus 44, that's 68.
4952640	4959640	So we get 68 and we can go in and write that here.
4959640	4973640	So next up, we have three and four and nine and 12.
4973640	4976640	So three times nine is 27.
4976640	4977640	And then four times 12.
4977640	4979640	So let's just, let's just do that.
4979640	4981640	I'm not doing that in my head.
4981640	4983640	27 plus was four times 12.
4983640	4984640	So that's 48.
4984640	4990640	27 plus 48 gives us 75.
4990640	4994640	Let's go ahead and write our 75 here.
4994640	4998640	Then we can go ahead and slide down to this row since we're done, since we're done that.
4998640	5016640	And then we go five, five and six dot product was seven and 10.
5016640	5022640	So our result from this five times seven is 35 and then six times 10 is 60.
5022640	5024640	So we're going to get 95.
5024640	5029640	We can go in and write our 95 here.
5029640	5046640	And then five and six dot product with eight and 11.
5046640	5050640	So five times eight is 40 and then six times 11 is 66.
5050640	5059640	So we get 104.
5059640	5075640	And then the last one, so five and six dot product with nine and 12.
5075640	5078640	So five, five times nine is 45.
5078640	5084640	And then six times 12 is what six times 12, 72, I think.
5084640	5087640	So six times 12, 72.
5087640	5088640	Yeah.
5088640	5096640	So 45 plus 72, 117.
5096640	5104640	And that is how you do a three by two matrix and a two by three matrix multiplying them together.
5104640	5112640	So the result would be C equals that.
5112640	5121640	So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far.
5121640	5130640	So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU.
5130640	5136640	So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training.
5136640	5141640	But pretty much that's how you do dot products and matrix multiplication.
5141640	5144640	So I actually realized I messed up a little bit on the math there.
5144640	5148640	So this 104, that's actually 106.
5148640	5152640	So I messed up there if you caught that.
5152640	5153640	Good job.
5153640	5158640	But pretty much this is what this looks like in three lines of code.
5158640	5165640	So all of this up here that we just covered all of this is in three lines.
5165640	5169640	So we initialize an A tensor and a B tensor.
5169640	5171640	Each one of these is a row.
5171640	5176640	Each one of these is a row and it'll pretty much multiply these together.
5176640	5183640	So this at symbol, this is a shorthand how you multiply two matrices in pytorch together.
5183640	5191640	Another way to do this is to use the torch dot matrix multiply function or math mall for short.
5191640	5194640	And then you can do A and B.
5194640	5198640	So these will print literally the same thing.
5198640	5199640	Look at that.
5199640	5203640	So I'm not too sure on the differences between them.
5203640	5207640	I use A at B for short.
5207640	5216640	But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that.
5216640	5226640	But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks.
5226640	5229640	So there's our network here if I go up.
5229640	5239640	Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number.
5239640	5245640	So if it's like a one, it would be like one dot zero or something or just like a one dot.
5245640	5247640	That's what it would look like as a floating point number.
5247640	5252640	But if it were an integer, say B is full of ones with integers, it would just be a one.
5252640	5255640	There wouldn't be any decimal zero zero center, right?
5255640	5257640	It would just be one.
5257640	5265640	So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type.
5265640	5268640	So I showcase this right here.
5268640	5270640	We have an int 64.
5270640	5276640	So type of it is an integer and a float 32, 64 and 32 don't mean anything.
5276640	5279640	All we have to know is an integer and floating point number.
5279640	5292640	So I've initialized a torch.randint, I covered above and set above here.
5292640	5299640	And maybe not.
5299640	5307640	Anyways, this pretty much does torch.randint is going the first parameter here is anything.
5307640	5309640	It's pretty much your range.
5309640	5314640	So I could do like zero to five, or I could just do like one.
5314640	5320640	So it'll do zero up to one, and then your shape of the matrix that it generates.
5320640	5323640	So I said it's a random int.
5323640	5329640	So that means it's going to generate a tensor with the data type integer 64.
5329640	5338640	So we have a three by two, and then I initialize another random key detail here.
5338640	5341640	We don't have the int suffix.
5341640	5345640	So this just generates floating point numbers.
5345640	5348640	And if we actually return the types of each of these.
5348640	5360640	So five print int 64 dot d type, and then float 32 dot d type.
5360640	5361640	Save that.
5361640	5365640	I'm going to comment this out for now.
5365640	5368640	We get a in 64 and float 32.
5368640	5377640	So if we just try to multiply these together, try to multiply these together.
5377640	5380640	Expected scalar type long above found float.
5380640	5383640	So long is pretty much when you have a sequence of integers.
5383640	5387640	And float is, of course, you have the decimal place.
5387640	5389640	So you can actually multiply this together.
5389640	5397640	So pretty much what you can do is cast the float method on this.
5397640	5404640	If you just do dot float, and then parentheses, and then run this, it'll actually work.
5404640	5407640	So you can cast integers to floats.
5407640	5411640	And then I think there's a way you can cast floats to integers, but it has some rounding in there.
5411640	5417640	So probably not the best for input and weights, matrix multiplication.
5417640	5426640	But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise.
5426640	5430640	So you want to make sure that they have sort of room to float around.
5430640	5433640	So that's pretty much how you avoid that error.
5433640	5434640	Let's move on.
5434640	5435640	So congratulations.
5435640	5438640	You probably made it further than quite a few people already.
5438640	5440640	So congratulations on that.
5440640	5444640	That was one of the most comprehensive parts of this entire course.
5444640	5448640	Understanding the math is going on behind the scenes.
5448640	5452640	For some people, it's very hard to grasp if you're not very fluent with math.
5452640	5458640	But yeah, let's continue the biogram language model and let's pump out some code here.
5458640	5463640	So to recap, we're using CUDA to accelerate the training process.
5463640	5470640	We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel.
5470640	5472640	Two hyperparameters.
5472640	5474640	We open our text.
5474640	5476640	We make a vocabulary out of it.
5476640	5479640	We initialize our encoder and decoder.
5479640	5486640	We get our data encoding all this text, and then we get our train and bow splits.
5486640	5488640	And then this next function here, get batch.
5488640	5494640	So before I jump into this, go ahead and run this here.
5494640	5502640	So this is pretty much just taking the first little, I don't know, we have eight characters.
5502640	5509640	So it's taking the first eight characters and then index one all the way to index nine.
5510640	5518640	And we can pretty much use this to show what the current input is and then what the target would be.
5518640	5526640	So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right?
5526640	5529640	So this is the premise of the biogram language model.
5529640	5531640	Given this character, we're going to predict the next.
5531640	5534640	It doesn't know anything else in the entire history.
5534640	5538640	It just knows what's before it or just knows what the current character is.
5538640	5542640	And based on that, we're going to predict the next one.
5542640	5548640	So we have this get batch function here, and this part right here is the most important piece of code.
5548640	5555640	This is going to work a little bit more later with our train and bow splits, making sure that, you know,
5555640	5558640	I'll try to explain this in a different way with our training bow splits.
5558640	5562640	So imagine you take a course, as you take a math course, okay?
5562640	5569640	And 90% of all your work is done just learning how the course works, learning all about the math.
5569640	5572640	So that's like 90% of data you get from it.
5572640	5574640	And then maybe another 10%.
5574640	5580640	Another 10% at the end is that final exam, which might have some questions you've never seen before.
5580640	5585640	So the point is in that first 90%, you're tested on based on what you know.
5585640	5589640	And then this other 10% is what you don't know.
5589640	5595640	And this pretty much means you can't memorize everything and then just start generating based on your memory.
5595640	5601640	You generate something that's alike or something that's close based on what you already know and the patterns you captured
5601640	5604640	in that 90% of the course.
5604640	5606640	So you can write your final exam successfully.
5606640	5608640	So that's pretty much what's going on here.
5608640	5616640	The training is the course, learning everything about it and then validation is validating the final exam.
5616640	5629640	So pretty much what we're doing here is initialize IX and that'll take a random manager between zero
5629640	5634640	and then length of the length of the entire text minus block size.
5634640	5644640	So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data.
5644640	5646640	So that's kind of how that works.
5646640	5650640	And if we print this out here, it'll just give us this right here.
5650640	5651640	So we get some random integers.
5651640	5660640	These are some random indices in the entire text that we can start generating from.
5660640	5664640	So print this out and then torch.stack.
5664640	5665640	We covered this before.
5665640	5668640	Pretty much what this does, it's going to stack them in batches.
5668640	5670640	This is the entire point of batches.
5670640	5673640	So that's what we do there.
5673640	5679640	We get X and then Y is just the same thing, but offset by one like this.
5679640	5682640	So that's what happens there.
5682640	5687640	And let's get into actually, I'm going to add something here.
5687640	5688640	This is going to be very important.
5688640	5696640	We're going to go X and Y is equal to model dot.
5696640	5702640	We're going to go X dot to device.
5702640	5706640	So notice how, no, we didn't do it up here.
5706640	5715640	Okay, we'll cover this later, but pretty much you're going to see what this does in a second here.
5716640	5723640	We return these and you can see that the device changed.
5723640	5724640	So now we're actually on CUDA.
5724640	5733640	And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU.
5733640	5739640	They're no longer going to be processed sequentially, but rather in our batches in parallel.
5739640	5748640	So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here.
5748640	5752640	So now we can go ahead and actually initialize our neural net.
5752640	5759640	So what I'm going to do is I'm going to go back up here and we're going to import some more stuff.
5759640	5767640	So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second.
5767640	5769640	I'm going to explain this here.
5769640	5780640	I just want to get some code out first.
5780640	5782640	And down here we can initialize this.
5782640	5783640	So it's a class.
5783640	5793640	We're going to make it a by-gram language model subclass of nn.module.
5793640	5802640	And the reason why we do nn.module here is because it's going to take an nn.module.
5802640	5817640	I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters.
5817640	5822640	So I'm going to go ahead and look at the documentation here so you can sort of understand this better.
5823640	5825640	We go to nn.
5825640	5836640	So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before.
5836640	5838640	So we have nn.linear.
5838640	5846640	So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable.
5846640	5851640	So that weight matrix will be changed through gradient descent.
5851640	5854640	And actually, I think I should probably cover gradient descent right now.
5854640	5862640	So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better.
5862640	5866640	So I'm going to go ahead and set up a little graph for that right now.
5866640	5869640	So I'm going to be using a little tool called Desmos.
5869640	5871640	Desmos is actually great.
5871640	5873640	It acts as a graphing calculator.
5873640	5876640	So you can plug in formulas and move things around.
5876640	5879640	You sort of visualize how math functions work.
5879640	5886640	So I've written some functions out here that will basically calculate the derivative of a sine wave.
5886640	5890640	So if I move A around, you'll see that changes.
5890640	5897640	So before I get into what's really going on here, I need to first tell you what the loss actually is.
5897640	5903640	If you're not familiar with the loss, let's say we have 80 characters in our vocabulary.
5903640	5908640	And we have just started our model, no training at all, completely random weights.
5908640	5914640	And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully.
5914640	5922640	So how we can measure the loss of this is by taking the negative log likelihood.
5922640	5924640	So the likelihood is one out of 80.
5924640	5927640	We take the log of that and then negative.
5927640	5931640	So if we plug this in here, we'll get 4.38.
5931640	5933640	So that's a terrible loss.
5933640	5935640	Obviously, that's one out of 80.
5935640	5939640	So it's like, you know, not even 2% chance.
5939640	5941640	So that's not great.
5941640	5948640	So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss.
5948640	5950640	And that's how we train our network.
5950640	5951640	So how does this actually work?
5951640	5953640	How does this actually work out in code, you ask?
5953640	5957640	So pretty much, let's say we have a loss here, okay?
5957640	5960640	Start off with a loss of 2, just arbitrary loss, whatever.
5960640	5964640	And what we're trying to do is decrease it.
5964640	5969640	So over time, it's going to become smaller and smaller if we move in this direction.
5969640	5972640	So how do we know if we're moving in the right direction?
5972640	5977640	Well, we take the derivative of what the current point is at right now,
5977640	5979640	and then we try moving it in a different direction.
5979640	5982640	So if we move it this way, sure, it'll go down.
5982640	5983640	That's great.
5983640	5987640	We can hit the local bottom over there, or we can move to this side.
5987640	5991640	And then we can see that the slope is increasing in a negative direction.
5991640	5996640	So we're going to keep adjusting the parameters in favor of this direction.
5996640	5999640	So that's pretty much what gradient descent is.
5999640	6003640	We're descending with the gradient.
6003640	6005640	So pretty self-explanatory.
6005640	6007640	That's what the loss function does.
6007640	6011640	And gradient descent is an optimizer.
6011640	6013640	So it's an optimizer for the network.
6013640	6017640	Optimizes our parameters, our weight, matrices, etc.
6017640	6020640	So these are some common optimizers that are used.
6020640	6025640	And this is just by going to torch.optim, short for optimizer.
6025640	6029640	And these are just a list of a bunch of optimizers that PyTorch provides.
6029640	6034640	So what we're going to be using is something called AdamW.
6034640	6040640	And what AdamW is, is it pretty much...
6040640	6042640	I'm just going to read off my little script here,
6042640	6046640	because I can't memorize every optimizer that exists.
6046640	6052640	So Adam, without Adam, just Adam, not AdamW,
6052640	6057640	Adam is a popular optimization algorithm that combines ideas of momentum.
6057640	6063640	And it uses a moving average of both the gradient and its squared value
6063640	6066640	to adapt the learning rate of each parameter.
6066640	6070640	And the learning rate is something that we should also go over.
6070640	6075640	So let's say I figure out I need to move in this direction.
6075640	6077640	I move, I take a step like that.
6077640	6080640	Okay, that's a very big step that I say,
6080640	6082640	okay, we need to keep moving in that direction.
6082640	6086640	So what happens is I go like this, and then I end up there.
6086640	6089640	And it's like, whoa, we're going up now, what happened?
6089640	6091640	So that's because you have a very high learning rate.
6091640	6095640	If you have a lower learning rate, what will happen is you'll start here.
6095640	6098640	It'll take little one-pixel steps or very, very small steps.
6098640	6101640	Okay, that's good. That's better. It's even better.
6101640	6104640	Keep going in this direction. This is great.
6104640	6107640	And you keep going down. You're like, okay, this is good.
6107640	6110640	We're descending. And it's starting to flatten out.
6110640	6113640	So we know that we're hitting a local bottom here.
6113640	6116640	And then we stop because it starts ascending again.
6116640	6123640	So that means this is our best set of parameters because of what that loss is
6123640	6128640	or what the derivative is of that particular point.
6128640	6132640	So pretty much this is what the learning rate is.
6132640	6137640	So you want to have a small learning rate so that you don't take too large steps
6137640	6141640	so that the parameters don't change dramatically and end up messing you up.
6141640	6144640	So you want to make them small enough so that you can still have efficient training.
6144640	6153640	You don't want to be moving in a millionth of one or something.
6153640	6158640	That would be ridiculous. You'd have to do so many iterations to even get this far.
6158640	6165640	So maybe you'd make it decently high but not too high that it'll go like that, right?
6165640	6170640	So that's what the learning rate is, just how fast it learns pretty much.
6170640	6179640	And yeah, so AtomW is a modification of the Atom Optimizer.
6179640	6185640	And it adds weight to K. So pretty much there's just some features that you add on to gradient descent
6185640	6189640	and then AtomW is the same thing except that has weight to K.
6189640	6193640	And what this pretty much means is it generalizes the parameters more.
6193640	6197640	So instead of having very high level of performance or very low level,
6197640	6200640	it takes a little generalized in between.
6200640	6207640	So the weight significance will actually shrink as it flans out.
6207640	6212640	So this will pretty much make sure that certain parameters in your network,
6212640	6219640	certain parameters in your weight matrices aren't affecting the output of this model drastically.
6219640	6222640	That could be in a positive or negative direction.
6222640	6227640	You can have insanely high performance from some lucky parameters in your weight matrices.
6227640	6233640	So pretty much the point is to minimize those, to decay those values.
6233640	6240640	That's what weight to K is, to prevent it from having that insane or super low performance.
6240640	6242640	That's what weight to K is.
6242640	6247640	So that's a little background on gradient descent and optimizers.
6247640	6250640	Let's go ahead and finish typing this out.
6250640	6257640	So next up, we actually, we need to initialize some things.
6257640	6267640	So we have our init self, of course, since it's a class, vocab size.
6267640	6272640	I want to make sure that's correct, vocabulary size.
6272640	6287640	I might actually shrink this just a vocab size because it sounds way easier to type out.
6287640	6289640	And vocab size, good.
6289640	6293640	So we're going to pump out some R code here.
6293640	6297640	And this is just assuming that you have some sort of a background in Python.
6297640	6301640	If not, it's all good.
6301640	6304640	Just understanding the premise of what's going on here.
6304640	6309640	So we're going to make something called an embedding table.
6309640	6319640	And I'm going to explain this to you in a second here, why the embedding table is really important.
6319640	6323640	Notice that we use the nn.
6323640	6326640	We use the nn module in this.
6326640	6330640	So that means this is going to be a learnable parameter, the init.embedding.
6330640	6336640	So we're going to make this vocab size by vocab size.
6336640	6341640	So let's say you have all eight characters here and you have all eight characters here.
6341640	6346640	I'm going to actually show you what this looks like in a second here and why this is really important.
6346640	6351640	So first off, we're going to finish typing out this background language model.
6351640	6355640	So we're going to define our forward pass here.
6355640	6362640	So the reason why we type this forward pass out, instead of just using what it offers by default,
6362640	6372640	is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task.
6372640	6378640	This is a really good practice because we want to actually know what's going on behind the scenes in our model.
6378640	6380640	We want to know exactly what's going on.
6380640	6389640	We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug.
6389640	6397640	So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch?
6397640	6402640	Well, like I said, understanding the process, what are all the transformations that are actually going on,
6402640	6410640	all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output?
6410640	6423640	Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices
6423640	6432640	If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that.
6432640	6441640	So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up.
6441640	6448640	So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here.
6448640	6454640	Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right?
6454640	6461640	So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right?
6461640	6468640	And optimization, of course. So that's pretty much why we write out the forward pass from scratch.
6468640	6475640	It's also just best practice. So it's never really a good idea to not write this.
6475640	6483640	But let's continue. So self, and it will do index and targets.
6483640	6492640	So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here.
6492640	6497640	Before we do logits, I'm going to explain to you this embedding table here.
6498640	6503640	Paste that in.
6509640	6515640	Return logits. You're going to see why we return logits in a second here.
6515640	6521640	So this an end on embedding here is pretty much just a lookup table.
6521640	6525640	So what we're going to have, I'm actually going to pull up my notebook here.
6525640	6533640	So we have a giant sort of grid of what the predictions are going to look like.
6533640	6537640	It's going to look, can I drag it in here? No.
6537640	6543640	So go ahead and download this full screen. Boom.
6543640	6546640	This is my notion here, but pretty much this is what it looks like.
6546640	6549640	And I took this picture from Andrei Karpathy's lecture.
6549640	6554640	But what this is, is it has start tokens and end tokens.
6554640	6558640	So start is at the start of the block, and end tokens are at the end of the block.
6558640	6569640	And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character.
6569640	6585640	So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.
6585640	6591640	So if we just add up all these, if we normalize them, and we get a little probability of this happening,
6591640	6595640	I don't know, if we add up all these together, I don't know what that is.
6595640	6599640	It's some crazy number, maybe 20,000 or something, something crazy.
6599640	6607640	Pretty much that percentage is the percentage of the end token coming after the character A.
6607640	6615640	And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind.
6615640	6624640	That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that.
6624640	6630640	To that entire row. So this one's pretty significant in proportion to the others.
6630640	6633640	So this one's going to be a fairly high probability of coming next.
6633640	6637640	A lot of the times you're going to have an I coming after an R.
6637640	6640640	And that's pretty much what that is. That's the embedding table.
6640640	6643640	So that's why we make it vocab size by vocab size.
6643640	6648640	So that's a little background on what we're doing here.
6648640	6653640	So let's continue with the term logits.
6653640	6657640	So what exactly are the logits? You're probably asking that.
6657640	6664640	So let's actually go back to a little notebook I had over here.
6664640	6670640	So remember our softmax function, right? Our softmax right here.
6670640	6675640	So we exponentiated each of these values and then we normalized them.
6675640	6681640	Normalized. We took its contribution to the sum of everything. That's what normalizing is.
6681640	6688640	So you can think of logits as just a bunch of floating point numbers that are normalized, right?
6688640	6693640	So you have a total, I'll write this out.
6693640	6703640	So let's say we have, that's a terrible line. Let's draw a new one.
6704640	6723640	So let's say we have 2, 4, and 6. And we want to normalize these.
6723640	6729640	So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12.
6729640	6735640	So 2 divided by 12. We take the percentage of that.
6735640	6741640	2 out of 12 is 0.16 something, okay?
6741640	6747640	So 0.16, we'll just do 1.167.
6747640	6751640	And then 4 out of 12 would be double that.
6751640	6758640	So 4 out of 12 would be 33, 33%.
6758640	6762640	And then 6 out of 12, that's 50. So 0.5.
6762640	6770640	So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution.
6770640	6784640	So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d.
6784640	6788640	We know that from this distribution, a followed by d is most likely to come next.
6788640	6795640	So this is what the logits are. They're pretty much a probability distribution of what we want to predict.
6795640	6801640	So given that, let's hop back into here. We're going to mess around with these a little bit.
6801640	6807640	So we have this embedding table, and I already showed you what that looked like.
6807640	6810640	It looked like this right here. This is our embedding table.
6810640	6818640	So let's use something called, we're going to use a function called dot view.
6818640	6823640	So this is going to help us sort of reshape what our logits look like.
6823640	6826640	And I'm going to go over an example of what this looks like in a second here.
6826640	6831640	I'm just going to pump out some code. So we have our batch by our time.
6831640	6836640	So the time is, you can think of time as that sequence of integers.
6836640	6839640	That's the time dimension, right? You start from here.
6839640	6843640	Maybe through the generating process, we don't know what's here next.
6843640	6846640	We don't know what's on the, we don't know what the next token is.
6846640	6850640	So that's why we say it's time because there's some we don't know yet and there's some that we already do know.
6850640	6852640	That's what we call the time dimension.
6852640	6858640	And then channels would just be, how many different channels are, what's the vocabulary size?
6858640	6860640	Channels is the vocabulary size.
6860640	6864640	So we can make this the logits dot shape.
6864640	6867640	This is what logits going to return here is B by T by C.
6867640	6869640	That's the shape of it.
6869640	6878640	And then our targets do, actually, no, we won't do that yet.
6878640	6886640	We'll do do logits equals logits dot view.
6886640	6892640	And then we'll, this is very important, B by T.
6892640	6902640	So because we're particularly paying attention to the channels, the vocabulary, the batch and time,
6902640	6905640	they, I mean, they're not as important here.
6905640	6907640	So we can sort of blend these together.
6907640	6915640	And as long as the logits and the targets have the same batch and time, we should be all right.
6915640	6921640	So we're going to do B, B times T by C.
6921640	6926640	And then we can go to initialize our targets.
6926640	6933640	It's going to be targets dot view.
6933640	6939640	And it's going to be just a B by T.
6939640	6943640	And then we can make our loss, remember the loss function, right?
6943640	6949640	We do the functional of cross entropy, just a way of measuring the loss.
6949640	6952640	And we basically take where there's two parameters here.
6952640	6958640	So we have the logits and the targets.
6958640	6962640	So I'm going to go over exactly what's going on here in a second.
6962640	6965640	But first, you might be asking, what does this view mean?
6965640	6966640	What exactly does this do?
6966640	6968640	So I'm going to show you that right now.
6968640	6974640	There's some code here that initializes a random tensor of shape 2 by 3 by 5.
6974640	6982640	And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape.
6982640	6987640	So shape takes the, you know, it takes the 2 by 3 by 5.
6987640	6991640	We get x equals 2, y equals 3, and z equals 5.
6991640	7002640	So then we can do dot view, and that'll pretty much make that tensor again with those dimensions.
7002640	7005640	So then we can just print that out afterwards.
7005640	7013640	We go, we could print out, I don't know, print x, y, z.
7013640	7016640	We have 2, 3, 5.
7016640	7020640	Print, print a dot shape.
7020640	7028640	And actually, I'll print out a dot shape right here first so you can see that this actually does line up.
7028640	7031640	A dot shape.
7031640	7033640	And then down here as well.
7033640	7035640	Same exact thing.
7035640	7040640	This also view does, basically allows us to unpack with the dot shape,
7040640	7044640	and then we can use view to put them back together into a tensor.
7044640	7050640	So you might be asking, why in this notebook did we, did we have to reshape these?
7050640	7052640	Why do we do that?
7052640	7058640	Well, the answer sort of falls into what the shape needs to be here with cross entropy.
7058640	7060640	What does it expect?
7060640	7063640	What does PyTorch expect the actual shape to be?
7063640	7071640	So I looked at the documentation here, and it pretty much says that we want either one dimension,
7071640	7078640	which is channels, or 2, which is n, which I believe n is also the batch.
7078640	7082640	So you have n, n different blocks or batches.
7082640	7086640	And then you have some other dimensions here.
7086640	7095640	So pretty much what it's expecting is a b by c by t instead of a b by t by c,
7095640	7099640	which is precisely what we get out of here.
7099640	7102640	It's the logits dot shape is b by t by c.
7102640	7105640	We want it in a b by c by t.
7105640	7110640	So pretty much what we're doing is we're just putting this into,
7110640	7113640	we're just making this one parameter by multiplying those.
7113640	7114640	That's what's going on here.
7114640	7117640	And then that means the second one is going to be c.
7117640	7123640	So you get like a b times t equals n, and then c, just the way that it expects it, right?
7123640	7124640	Just like that.
7124640	7127640	So that's pretty much what we're doing there.
7127640	7137640	And a lot of the times you might get errors from passing it into a functional function in PyTorch.
7137640	7142640	So it's important to pay attention to how PyTorch expects the shapes to be,
7142640	7144640	because you're going to get errors from that.
7144640	7147640	And I mean, it's not very hard to reshape them.
7147640	7152640	You just use the dot view and dot shape and you unpack them, reshape them together.
7152640	7159640	It's overall pretty simple for a beginner to intermediate level projects.
7159640	7162640	So it shouldn't really be a trouble there, but just watch out for that,
7162640	7167640	because it will come back and get you if you're not aware at some point.
7167640	7170640	So I've added a new function here called generate,
7170640	7173640	and this is pretty much going to generate tokens for us.
7173640	7179640	So we pass an index, which is the current index or the context,
7179640	7183640	and then we have max new tokens, and this is passed in through here.
7183640	7191640	So we have our context, we make it a single zero, just the next line character.
7191640	7196640	And then we generate based on that, and then our max new tokens, second parameter,
7196640	7199640	we just make it 500 second parameter.
7199640	7200640	So cool.
7200640	7202640	What do we do inside of here?
7202640	7215640	We have a little loop that pretty much it generates based on the range of the max new tokens.
7215640	7221640	So we're going to generate max new tokens, tokens, if that makes sense.
7221640	7228640	Pretty much what we do is we call forward pass based on the current state of the model parameters.
7228640	7233640	And I want to be explicit here and say self dot forward, rather than just self index,
7233640	7235640	it will call self dot forward when we do this.
7235640	7239640	But let's just be explicit and say self dot forward here.
7239640	7242640	So we get the logic and the loss from this.
7242640	7244640	We focus on the last time step.
7244640	7247640	That's the only one we care about diagram language model.
7247640	7252640	We only care about the single previous character, only one doesn't have context before.
7252640	7257640	And then we apply the softmax to get probability distribution.
7257640	7260640	And we already went over the softmax function before.
7260640	7267640	The reason why we use negative one here is because we're focusing on the last dimension.
7267640	7273640	And in case you aren't familiar with negative indexing, which is what this is here and same with here,
7273640	7276640	is imagine you have a little number line.
7276640	7281640	It starts at index zero, one, two, three, four, five, et cetera.
7281640	7289640	So if you go before zero, it's just going to loop to the very end of that array.
7289640	7294640	So when we call negative one, it's going to do the last element, negative two,
7294640	7297640	second last element, negative three, third last element, et cetera.
7297640	7299640	So that's pretty much all this is here.
7299640	7301640	And you can do this for anything in Python.
7301640	7304640	Negative indexing is quite common.
7304640	7307640	So that's what we do here.
7307640	7311640	We apply softmax to the last dimension.
7311640	7314640	And then we sample from the distribution.
7314640	7319640	So we already went over torch dot monomial, we get one sample.
7319640	7329640	And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate.
7329640	7337640	It concatenates the previous context or the previous tokens with the newly generated one.
7337640	7339640	And then we just combine them together.
7339640	7341640	So they're one thing.
7341640	7346640	And we do this on a B by T plus one.
7346640	7349640	And if that doesn't make sense, let me help you out here.
7349640	7354640	So we have this time dimension, let's say we have, you know, maybe just one element here.
7354640	7357640	So we have something in the zero position.
7357640	7363640	And then whenever we generate a token, we're going to take the information from the zero position.
7363640	7365640	And then we're going to add one to it.
7365640	7367640	So it becomes a B by T.
7367640	7371640	Since there was only one element, the length of that was one, it is now two.
7371640	7374640	Then we have this two, we make it three.
7374640	7377640	And then we have this three, we make it four.
7377640	7384640	So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it.
7384640	7388640	And then we, you know, after this loop, we just return the index.
7388640	7392640	So this is all the generated tokens for max new tokens.
7392640	7396640	And that's pretty much what that does.
7396640	7404640	Model up to device here, this is just going to push our parameters to the GPU for more efficient training.
7404640	7409640	I'm not sure if this makes a huge difference right now because we're only doing background language modeling.
7409640	7413640	But yeah, it's handy to have this here.
7413640	7417640	And then, I mean, this is, this is pretty self explanatory here.
7417640	7420640	We generate based on a context.
7420640	7424640	This is the context, which is a single zero or a next line character.
7424640	7427640	We pass in our max new tokens.
7427640	7429640	And then we pretty much just decode this.
7429640	7432640	So that's how that works.
7432640	7438640	Let's move on to the optimizer and the training loop, the actual training process.
7438640	7442640	So I actually skipped something and probably left you a little bit confused.
7442640	7452640	But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here?
7452640	7458640	Because the logits only returns two dimensions, right?
7458640	7463640	You have a B by T, or you have a B times T by C.
7463640	7466640	So how exactly does this work?
7466640	7471640	Well, when we call this forward pass, all we're passing in is the index here.
7471640	7475640	So that means targets defaults to none.
7475640	7481640	So because targets is none, the loss is none, and this code does not execute.
7481640	7486640	And it just uses this logits here, which is three dimensional.
7486640	7488640	So that's how that works.
7488640	7500640	And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there.
7500640	7504640	And that will make sure targets is not none.
7504640	7511640	So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits.
7511640	7516640	So that's just a little clarification there, if that was confusing to anybody.
7516640	7524640	Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type.
7524640	7535640	So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes.
7535640	7547640	So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference
7547640	7551640	between float and int is float has decimals, it's a floating point number.
7551640	7556640	And then integers just, just a single integer doesn't, it's not really anything more than that.
7556640	7561640	It can just be bigger based on the amount of bits that occupies.
7561640	7565640	So that's just a overview on tors dot long.
7565640	7568640	It's the exact same thing as in 64.
7568640	7571640	So that's that.
7571640	7574640	Now we have this, we have this training loop here.
7574640	7578640	So we define our optimizer.
7578640	7586640	And I already meant over optimizers previously, Adam W, which is Adam weight decay.
7586640	7588640	So we have weight decay in here.
7588640	7593640	And then all of our model parameters, and then our learning rates.
7593640	7596640	So I actually wrote to learning rate up here.
7596640	7601640	So I would add this and then just rerun this part of the code here if you're typing along.
7601640	7608640	So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop.
7608640	7615640	And the learning rate is special, because sometimes you're learning what will be too high.
7615640	7618640	And some said, sometimes it'll be too low.
7618640	7629640	So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time.
7629640	7635640	So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip.
7635640	7642640	So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly.
7642640	7652640	You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that.
7653640	7662640	So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs.
7662640	7667640	So now we have this training loop here, which is going to iterate over the max iterations.
7667640	7670640	We just give each iteration the term iter.
7670640	7677640	And I don't think we use this yet, but we will later for just reporting on the loss over time.
7678640	7686640	But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training.
7686640	7688640	This is the training loop, we don't care about validation.
7688640	7696640	So we're going to call train on this, we're going to get some x inputs and some y targets.
7696640	7704640	So we go in and do a model dot forward here, we got our logits and our loss.
7704640	7710640	And then we're going to do our optimizer dot zero grad and I'll explain this in the second here.
7710640	7712640	It's a little bit confusing.
7712640	7722640	But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops.
7722640	7724640	I know I can go by this a little bit quickly.
7724640	7730640	But this is the standard training loop architecture for basic models.
7730640	7733640	And this is what it'll usually look like.
7733640	7738640	So you'll, you know, you'll get your data, get your inputs or outputs, whatever.
7738640	7740640	You'll do a forward pass.
7740640	7743640	You'll define some thing about the optimizer here.
7743640	7744640	In our case, it's your grad.
7744640	7750640	And then you'll have a loss dot backward, which is backward pass.
7750640	7755640	And the optimizer dot step, which lets gradient descent work its magic.
7755640	7759640	So back to optimizer dot zero grad.
7759640	7767640	So by default, PyTorch will accumulate the gradients over time via adding them.
7767640	7774640	And what we do by by putting a zero grad is we make sure that they do not add over time.
7774640	7778640	So the previous gradients do not affect the current one.
7778640	7783640	And the reason we don't want this is because previous gradients are from previous data.
7783640	7788640	And the data is, you know, kind of weird sometimes, sometimes it's biased.
7788640	7795640	And we don't want that determining, you know, how much like what our error is, right?
7795640	7802640	So we only want to decide, we only want to optimize based on the current gradient of our current data.
7802640	7805640	And this little parameter in here, we go set to none.
7805640	7811640	This pretty much means we're going to set, we're going to set the gradients instead of zero,
7811640	7815640	instead of zero gradient, we're going to set it to none.
7815640	7820640	And the reason why we set it to none is because none occupies a lot less space.
7820640	7824640	It just, yeah, just occupies a lot less space when you have a zero.
7824640	7828640	That's, that's probably an int 64 or something that's going to take up space.
7828640	7834640	And because, you know, we might have a lot of these accumulating that takes up space over time.
7834640	7841640	So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to.
7841640	7846640	And that's pretty much what that does.
7846640	7856640	It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets,
7856640	7861640	which need to understand previous context because they're recurrent.
7861640	7870640	I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation.
7870640	7877640	We'll simply take an average of all the accumulation steps and just averages the gradients together.
7877640	7881640	So you get a more effective, maybe block size, right?
7881640	7883640	You get more context that way.
7883640	7885640	And you can have the same batch size.
7885640	7887640	So just little neat tricks like that.
7887640	7894640	We'll talk about gradient accumulation more later in the course, but pretty much what's going on here.
7894640	7897640	We define an optimizer, Adam W.
7897640	7899640	We iterate over max editors.
7899640	7901640	We get a batch training split.
7901640	7907640	We do a forward pass, zero grad, backward pass, and then we get a step in the right direction.
7907640	7909640	So we're gradient descent works as magic.
7909640	7912640	And at the end, we could just print out the loss here.
7912640	7914640	So I've run this a few times.
7914640	7920640	And over time, I've gotten the loss of 2.55, which is okay.
7920640	7930640	And if we generate based on that loss, we get still pretty garbage tokens.
7930640	7934640	But then again, this is a background language model.
7934640	7937640	So actually, I might need to retrain this here.
7937640	7939640	It's not trained yet.
7939640	7946640	So what I'm actually going to do is run this, run this, run this, boom.
7946640	7950640	And then what I'll do, oh, it looks like we're printing out a lot of stuff here.
7950640	7953640	So that's coming from our get batch.
7953640	7955640	So I'll just comment that.
7955640	7957640	Or we can just delete it overall.
7957640	7959640	Cool.
7959640	7968640	And now if we run this again.
7968640	7973640	Give it a second.
7973640	7976640	Perfect.
7976640	7981640	So I don't know why it's still doing that.
7981640	7986640	If we run it again, let's see.
7986640	7995640	Where are we printing stuff?
7996640	7998640	No.
7998640	8000640	Ah, yes.
8000640	8002640	We have to run this again after changing it.
8002640	8006640	Silly me.
8006640	8010640	And of course, 10,000 steps is a lot.
8010640	8012640	So it takes a little while.
8012640	8014640	It takes a few seconds, which is actually quite quick.
8014640	8018640	So after the first one, we get a loss of 3.15.
8018640	8020640	We can generate from that.
8020640	8022640	And we get something that is less garbage.
8022640	8024640	You know, it has some next line characters.
8024640	8026640	It understands a little bit more to, you know,
8026640	8028640	space things out and whatnot.
8028640	8032640	So that's like slightly less garbage than before.
8032640	8036640	But yeah, this, this is pretty good.
8036640	8038640	So I lied.
8038640	8042640	There aren't actually any lectures previously where I talked about optimizers.
8042640	8045640	So might as well talk about it now.
8045640	8047640	So a bunch of common ones.
8047640	8052640	And honestly, you don't really need to know anything more than the common ones
8052640	8056640	because most of them are just built off of these.
8056640	8059640	So you have your mean squared error,
8059640	8062640	common loss function using regression, regression problems,
8062640	8065640	where it's like, you know, you have a bunch of data points,
8065640	8067640	find the best fit line, right?
8067640	8069640	That's a common regression problem.
8069640	8071640	Goals to predict a continuous output
8071640	8073640	and measures the average squared difference
8073640	8076640	between the predicted and actual values,
8076640	8079640	often used to train neural networks for regression tasks.
8079640	8081640	So cool.
8081640	8083640	That's the most basic one.
8083640	8085640	You can look into that more if you'd like,
8085640	8088640	but that's our most basic optimizer.
8088640	8090640	Gradient descent is a step up from that.
8090640	8093640	It's used to minimize the loss function in a model,
8093640	8095640	measures how well the model,
8095640	8099640	the gradient measures how well the model is able to predict
8099640	8102640	the target variable based on the input features.
8102640	8104640	So we have some input X,
8104640	8108640	we have some weights and biases maybe, WX plus B.
8108640	8114640	And all we're trying to do is make sure that the inputs
8114640	8121640	or make sure that we make the inputs become the desired outputs
8121640	8125640	and based on how far it is away from the desired outputs,
8125640	8128640	we can change the parameters of the model.
8128640	8132640	So we went over gradient descent recently or previously,
8132640	8135640	but that's pretty much what's going on here.
8135640	8140640	And momentum is just a little extension of gradient descent
8140640	8143640	that adds the momentum term.
8143640	8146640	So it helps smooth out the training
8146640	8151640	and allows it to continue moving in the right direction,
8151640	8154640	even if the gradient changes direction or varies in magnitude.
8154640	8157640	It's particularly useful for training deep neural nets.
8157640	8160640	So momentum is when you have, you know,
8160640	8164640	you consider some of the other gradients.
8164640	8167640	So you have something that's like maybe passed on from here
8167640	8170640	and then it might include a little bit of the current one.
8170640	8173640	So like 90%, like a good momentum coefficient
8173640	8176640	would be like 90% previous gradients
8176640	8178640	and then 10% of the current one.
8178640	8183640	So it kind of like lags behind and makes it converge sort of smoothly.
8183640	8185640	That makes sense.
8185640	8187640	Arm as prop, I've never used this,
8187640	8191640	but it's an algorithm that use the moving average of the squared gradient
8191640	8193640	to adapt learning rates of each parameter,
8193640	8196640	helps to avoid oscillations in the parameter updates
8196640	8199640	and can move and can improve convergence in some cases.
8199640	8202640	So you can look more into that if you'd like.
8202640	8204640	Adam, very popular,
8204640	8208640	combines the ideas of momentum and arm as prop.
8208640	8210640	He uses a moving average,
8210640	8212640	both the gradient and its squared value
8212640	8214640	to adapt learning rate of each parameter.
8214640	8218640	So often uses the default optimizer for deep learning models.
8218640	8221640	And in our case, when we continue to build this out,
8221640	8224640	it's going to be quite a deep net.
8224640	8228640	And Adam W is just a modification of the item optimizer
8228640	8230640	that adds weight decay to the parameter updates.
8230640	8235640	So helps to regularize and improve generalization performance.
8235640	8239640	Using this optimizer as it best suits the properties of the model
8239640	8241640	we'll train in this video.
8241640	8244640	So, of course, I'm reading off the script here.
8244640	8248640	There's no really other better way to say how these optimizers work.
8248640	8251640	But, yeah, if you want to look more into, you know,
8251640	8254640	concepts like momentum or weight decay
8254640	8259640	or, you know, oscillations and just some statistic stuff, you can.
8259640	8263640	But honestly, the only thing that really matters
8263640	8267640	is just knowing which optimizers are used for certain things.
8267640	8270640	So, like, what is the momentum used for?
8270640	8273640	What is Adam W great for?
8273640	8276640	What is MSC good for, right?
8276640	8280640	Just knowing what the differences and similarities are,
8280640	8285640	as well as when is the best case to use the optimizer.
8285640	8290640	So, yeah, you can find more information about that at torch.optim.
8290640	8292640	So when we develop language models,
8292640	8295640	something really important in language modeling,
8295640	8298640	data science, machine learning, at all,
8298640	8300640	is just being able to report a loss
8300640	8303640	or get an idea of how well our model is performing
8303640	8305640	over, you know, the first 1,000 iterations
8305640	8307640	and then the first 2,000 iterations
8307640	8309640	and 4,000 iterations, right?
8309640	8311640	So we want to get a general idea
8311640	8313640	of how our model is converging over time.
8313640	8316640	But we don't want to just print every single step of this.
8316640	8317640	That wouldn't make sense.
8317640	8321640	So what we actually could do is print every, you know,
8321640	8323640	200 iterations, 500.
8323640	8325640	We could print every 10,000 iterations
8325640	8328640	if you're running a crazy big language model if you wanted to.
8329640	8331640	And that's exactly what we're going to implement right here.
8331640	8337640	So, actually, this doesn't require an insane amount of Python syntax.
8337640	8341640	This is just, I'm actually just going to add it into our for loop here.
8341640	8345640	And what this is going to do is it's going to do what I just said,
8345640	8349640	is print every, you know, every certain number of iterations.
8349640	8356640	So we can add a new hyper parameter up here called eval-itters.
8356640	8361640	And I'm going to make this 250 just for,
8361640	8364640	just to make things sort of easy here.
8364640	8368640	And we're going to go ahead and add this in here.
8368640	8373640	So I'm going to go if-iter.
8373640	8375640	And we're going to do the module operator.
8375640	8378640	You can look more into this if you want later.
8378640	8383640	And we're going to do eval-itters equals equals zero.
8383640	8390640	What this is going to do is it's going to check if the current iteration
8390640	8397640	divided by, or sorry, if the remainder of the current iteration
8397640	8400640	divided by our eval-itters parameter,
8400640	8405640	if the remainder of that is zero, then we continue with it.
8405640	8407640	So hopefully that made sense.
8407640	8412640	If you want to, you could just ask GPT4
8412640	8416640	or GPT3.5, whatever you have, just this module operator,
8416640	8419640	and you should get a good general understanding of what it does.
8419640	8421640	Cool.
8421640	8424640	So all we can do now is we'll just say,
8424640	8426640	we'll just have a filler statement here.
8426640	8431640	We'll just do print, we've been f-string,
8431640	8440640	and then we'll go losses, losses, maybe that.
8440640	8442640	Or actually, I'm going to change this here.
8442640	8449640	We can go step-iter.
8449640	8453640	Add a little colon in there.
8453640	8461640	And then I'll go split.
8461640	8467640	Actually, I'll just go loss, and then losses like that.
8467640	8471640	And then we'll have some sort of put in here.
8471640	8474640	Something soon.
8474640	8476640	I don't know.
8476640	8480640	And all I've done is I've actually added a little function in here
8480640	8481640	behind the scenes.
8481640	8483640	You guys didn't see me do this yet.
8483640	8488640	But pretty much, I'm not going to go through the actual function itself,
8488640	8492640	but what is important is that you know this decorator right here.
8492640	8494640	This probably isn't very common to you.
8494640	8496640	This is torch.nograt.
8496640	8499640	And what this is going to do is it's going to make sure that
8499640	8501640	PyTorch doesn't use gradients at all in here.
8501640	8503640	That'll reduce computation.
8503640	8505640	It'll reduce memory usage.
8505640	8507640	It's just overall better for performance.
8507640	8509640	And because we're just reporting a loss,
8509640	8513640	we don't really need to do any optimizing or gradient computation here.
8513640	8515640	We're just getting losses.
8515640	8517640	We're feeding some stuff into the model.
8517640	8521640	We're getting a loss out of it, and we're going from there.
8521640	8526640	So that's pretty much what's happening with this torch.nograt.
8526640	8531640	And, you know, for things like, I don't know,
8531640	8534640	if you have other classes or other outside functions,
8534640	8537640	like, I mean, get batched by default isn't using this
8537640	8540640	because it doesn't have the model thing passed into it.
8540640	8545640	But estimate loss does have model pass into it right here.
8545640	8550640	So we just kind of want to make sure that it's not using any gradients.
8550640	8552640	We're going to reduce computation that way.
8552640	8554640	So anyways, if you want,
8554640	8557640	you can just take a quick readover of this,
8557640	8560640	and it should overall make sense.
8560640	8564640	Terms like .item.me are pretty common.
8564640	8567640	A lot of the other things here, like model, X and Y,
8567640	8569640	we get our logits and our loss.
8569640	8571640	This stuff should make sense.
8571640	8573640	It should be pretty straightforward.
8573640	8576640	And only two other things I want to touch on
8576640	8578640	is model.eval and model.train,
8578640	8581640	because you probably have not seen these yet.
8581640	8588640	So model.train essentially puts the model in the training mode.
8588640	8590640	The model learns from the data,
8590640	8592640	meaning the weights and biases,
8592640	8594640	if we have, well, sometimes you only have weights,
8594640	8596640	sometimes you, you know,
8596640	8599640	sometimes you have weights and biases, whatever it is,
8599640	8601640	those are updated during this phase.
8601640	8603640	And then some layers of the model,
8603640	8605640	like dropout and batch normalization,
8605640	8608640	which you may not be familiar with yet,
8608640	8610640	operate differently in training mode.
8610640	8612640	For example, dropout is active,
8612640	8615640	and what dropout does is this little hyperparameter
8615640	8617640	that we add up here.
8617640	8619640	It'll look like this.
8619640	8621640	Dropout would be like 0.2.
8621640	8623640	So pretty much what dropout does
8623640	8626640	is it's going to drop out random neurons in the network
8626640	8628640	so that we don't overfit.
8628640	8632640	And this is actually disabled in validation mode,
8632640	8634640	or eval mode.
8634640	8638640	So this will just help our model sort of learn better
8638640	8640640	when it has little, like, pieces of noise
8640640	8643640	and when things aren't in quite the right place
8643640	8646640	so that you don't have, you know, certain neurons in the network
8646640	8650640	taking priority and just making a lot of the heavy decisions.
8650640	8651640	We don't want that.
8651640	8654640	So dropout will just sort of help our model train better
8654640	8658640	by taking 20% of the neurons out, 0.2, at random.
8658640	8660640	And that's all dropout does.
8660640	8663640	So I'm just going to delete that for now.
8664640	8667640	And then, yeah, model that train.
8667640	8670640	Well, dropout is active during this phase,
8670640	8673640	during training, randomly turning off,
8673640	8675640	random neurons in the network.
8675640	8677640	And this is to prevent overfitting.
8677640	8679640	We went over overfitting earlier, I believe.
8679640	8682640	And as for evaluation mode,
8682640	8686640	evaluation mode is used when the model's being evaluated
8686640	8688640	or tested just like it sounds.
8688640	8689640	It's being trained.
8689640	8692640	What the other mode is being validated or tested.
8692640	8696640	And layers like dropout and batch normalization
8696640	8698640	behave differently in this mode.
8698640	8700640	Like dropout is turned off in the evaluation, right?
8700640	8702640	Because what we're actually doing
8702640	8704640	is we're using the entire network.
8704640	8707640	We want everything to be working sort of together.
8707640	8710640	And we want to actually see how well does it perform.
8710640	8712640	Training mode is when we're just, you know,
8712640	8715640	sampling, doing weird things to try to challenge the network
8715640	8716640	as we're training it.
8716640	8719640	And then evaluating or validation would be
8719640	8722640	when we just get the network in its optimal form
8722640	8725640	and we're trying to see how good of results it produces.
8725640	8727640	So that's what a val is.
8727640	8729640	And the reason we switched into a val here
8729640	8732640	is just because, well, we are testing the model.
8732640	8734640	We want to see, you know, how well it does
8734640	8738640	with any given set of data from a get batch.
8738640	8740640	And we don't actually need to train here.
8740640	8743640	If there was no training, this would not be here
8743640	8746640	because we would not be using any gradients.
8747640	8750640	So we would be using gradients if training was on.
8750640	8753640	Anyways, that's estimate loss for you.
8753640	8757640	This function is, you know, just generally good
8757640	8759640	to have a data science.
8759640	8762640	Your train and validation splits, whatnot.
8762640	8765640	And yeah, good for reporting.
8765640	8767640	You know how it is.
8767640	8769640	And we can go ahead and add this down here.
8769640	8771640	So there's something soon.
8771640	8778640	We'll go losses is equal to estimates loss.
8778640	8784640	And then we can go ahead and put a...
8784640	8788640	Yeah, we don't actually have to put anything in here.
8788640	8789640	Cool.
8789640	8792640	So now let's go ahead and run this.
8792640	8794640	Let me run from the start here.
8794640	8798640	Boom, boom, boom, boom, boom, boom.
8802640	8804640	Perfect.
8811640	8813640	I'm running for 10,000 iterations.
8813640	8815640	That's interesting.
8815640	8817640	Okay.
8817640	8821640	So, yes.
8821640	8823640	So what I'm going to do actually here
8823640	8825640	is you can see this loss part is weird.
8825640	8828640	So I'm actually going to change this up.
8828640	8832640	And I'm just going to switch it to...
8832640	8835640	We're going to go train loss.
8835640	8837640	And we're going to go losses.
8837640	8840640	And we're going to do the train split.
8840640	8843640	And then we're going to go over here
8843640	8847640	and just do the validation loss.
8847640	8850640	We can do validation or just val for short.
8850640	8855640	And I'm going to make it consistent here.
8855640	8858640	So we have a colon there, a colon here.
8858640	8864640	And then we just go losses and do val.
8864640	8865640	Cool.
8865640	8870640	So I'm going to reduce these max editors up here to only 1,000.
8870640	8872640	Run that.
8872640	8874640	Run this.
8874640	8877640	Oh, somebody did a match.
8886640	8888640	Okay.
8888640	8890640	Okay.
8890640	8892640	Okay.
8914640	8916640	Yes.
8916640	8918640	So what actually happened here was
8918640	8921640	when we were doing these little ticks,
8921640	8924640	what was happening is these were matching up with these.
8924640	8927640	And it was telling us, oh, you can't do that.
8927640	8929640	You can't start here and then end there
8929640	8931640	and have all this weird stuff.
8931640	8933640	Like, you can't do that.
8933640	8936640	So pretty much we just need to make sure that these are different.
8936640	8938640	So I'm going to do a double quote instead of single
8938640	8940640	and then double quote to finish it off.
8940640	8943640	And as you can see, this worked out here.
8943640	8945640	So I'll just run that again
8945640	8947640	so you guys can see what this looks like.
8947640	8948640	Okay.
8948640	8951640	Because we have, you know, a lot of decimal places.
8951640	8956640	So what we can actually do here is we can add in a little format
8956640	8959640	or a little decimal place reducer, if you call it,
8959640	8963640	just for, you know, so you can read it.
8963640	8965640	So it's not like some weird decimal number
8965640	8967640	and you're like, oh, does this eight matter?
8967640	8968640	Probably not.
8968640	8970640	Just like the first three digits, maybe.
8970640	8973640	So all we can do here is just add in,
8973640	8975640	I believe this is how it goes.
8977640	8980640	I don't think it's the other way.
8980640	8982640	We'll find out.
8982640	8986640	Some stuff in Python is extremely confusing to me.
8986640	8989640	But there we go.
8989640	8990640	So I got it right.
8990640	8992640	Go on and then period.
8992640	8994640	And as you can see, we have those digits reduced.
8994640	8997640	So I can actually put this down to 3F.
9003640	9004640	Wonderful.
9004640	9009640	So we have our train loss and our validation loss.
9009640	9011640	Great job you made it this far.
9011640	9013640	This is absolutely amazing.
9013640	9014640	This is insane.
9014640	9016640	You've gotten this far in the video.
9016640	9019640	We've covered all the basics, everything you need to know
9019640	9022640	about background language models, optimizers,
9022640	9025640	training loops, reporting losses.
9025640	9028640	I can't even name everything we've done because it's so much.
9028640	9031640	So congratulations that you made it this far.
9031640	9033640	You should go take a quick break.
9033640	9037640	Give yourself a pat on the back and get ready for the next part
9037640	9039640	here because it's going to be absolutely insane.
9039640	9043640	We're going to dig into literally state of the art language
9043640	9047640	models and how we can build them from scratch,
9047640	9049640	or at least how we can pre-train them.
9049640	9052640	And some of these terms are going to seem a little bit out
9052640	9056640	there, but I can ensure you by the end of this next section
9056640	9060640	here, you're going to have a pretty good understanding
9060640	9063640	about the state of language models right now.
9063640	9067640	So go take a quick break and I'll see you back in a little bit.
9067640	9071640	So there's something I'd like to clear up and actually sort of
9071640	9075640	lied to you a little bit, a little while back in this course
9075640	9077640	about what normalizing is.
9077640	9081640	So I recall we were talking about the softmax function
9081640	9084640	and normalizing vectors.
9084640	9089640	So the softmax is definitely a form of normalization,
9089640	9091640	but there are many forms.
9091640	9095640	There are not just a few or like there's not just one or two normalizations.
9095640	9100640	There are actually many of them and I have them on my second monitor here,
9100640	9104640	but I don't want to just dump that library of information on your head
9104640	9106640	because that's not how you learn.
9106640	9109640	So what we're going to do is we're going to plug this into GPT-4.
9110640	9126640	I'm going to say, can you list all the forms of normalizing in machine learning?
9126640	9136640	And how are they different from one another?
9136640	9138640	GPT-4 is a great tool.
9138640	9142640	If you don't already use it, I highly suggest you use it,
9142640	9146640	or even GPT-3.5, which is the free version.
9146640	9150640	But yeah, it's a great tool for just quickly learning anything
9150640	9155640	and then have it give you example practice questions with answers
9155640	9158640	so you can learn topics in literally minutes
9158640	9163640	that would take you several lectures to learn in a university course.
9163640	9167640	But anyways, there's a few here.
9167640	9171640	So min-max normalization, yep.
9171640	9174640	z-score, decimal scaling, mean normalization,
9174640	9179640	unit vector, or layer 2, robust scaling, power transformations.
9179640	9180640	Okay.
9180640	9183640	So yeah, and then softmax would be another one.
9183640	9189640	What about softmax?
9189640	9191640	It is in data type normalization,
9191640	9199640	but it's not typically using from normalizing input data.
9199640	9201640	It's commonly used in the output layer.
9201640	9204640	So softmax is a type of normalization,
9204640	9209640	but it's not used for normalizing input data.
9209640	9216640	And honestly, we've proved that here by actually producing some probabilities.
9216640	9219640	So this isn't something we used in our forward pass.
9219640	9221640	This is something we used in our generate function
9221640	9224640	to get a bunch of probabilities from our logits.
9224640	9227640	So this is, yeah, interesting.
9227640	9230640	It's good to just figure little things like these out for, you know,
9230640	9234640	just to be, put you on the edge a little bit more
9234640	9237640	for the future when it comes to engineering these kind of things.
9237640	9239640	All right, great.
9239640	9242640	So the next thing I want to touch on is activation functions.
9242640	9246640	And activation functions are extremely important
9246640	9252640	in offering new ways of changing our inputs that are not linear.
9252640	9256640	So, for example, if we were to have a bunch of linear layers,
9256640	9259640	a bunch of, let me erase this,
9259640	9263640	if we were to have a bunch of, you know,
9263640	9266640	nn.linears in a row,
9266640	9270640	what would actually happen is they would all just, you know,
9270640	9272640	they would all squeeze together
9272640	9277640	and essentially apply one transformation that sums up all of them kind of.
9277640	9281640	They all sort of multiply together and it gives us one transformation
9281640	9285640	that is kind of just a waste of computation
9285640	9289640	because let's say you have 100 of these nn.linear layers
9289640	9292640	and nothing else.
9292640	9294640	You're essentially going from inputs to outputs,
9294640	9299640	but you're doing 100 times the computation for just one multiplication.
9299640	9301640	That doesn't really make sense.
9301640	9306640	So what can we do to actually make these deep neural networks important
9306640	9310640	and what can we offer that's more than just linear transformations?
9310640	9313640	Well, that's where activation functions come in
9313640	9316640	and I'm going to go over these in a quick second here.
9316640	9319640	So let's go navigate over to the PyTorch docs.
9319640	9323640	So the three activation functions I'm going to cover
9323640	9327640	in this little part of the video are the relu, the sigmoid,
9327640	9329640	and the tanh activation functions.
9329640	9335640	So let's start off with the relu or rectified linear unit.
9335640	9337640	So we're going to use functional relu
9337640	9340640	and the reason why we're not just going to use torch.n
9340640	9343640	is because we're not doing any forward passes here.
9343640	9347640	I'm just going to add these into our,
9347640	9352640	I'm going to add these, let me clear this, clear this output.
9352640	9353640	That's fine.
9353640	9356640	I'm actually going to add these into here and there's no forward pass.
9356640	9358640	We're just going to simply run them through a function
9358640	9361640	and get an output just so we can see what it looks like.
9361640	9366640	So I've actually added this up here from torch.n
9366640	9368640	and import functional as capital F.
9368640	9372640	It's just kind of a common PyTorch practice, capital F.
9372640	9377640	And let's go ahead and start off with the relu here.
9377640	9385640	So we can go, I don't know, x equals torch.tensor
9385640	9391640	and then we'll make it a negative 0.05, for example.
9391640	9398640	And then we'll go dtype equals torch.flurp32
9398640	9405640	and we can go y equals f.relu of x.
9405640	9412640	And then we'll go ahead and print y.
9412640	9414640	It has no attribute relu.
9414640	9416640	Okay, let's try nn then.
9416640	9421640	Let's try nn and see if that works.
9421640	9424640	Okay, well that didn't work and that's fine
9424640	9427640	because we can simply take a look at this
9427640	9430640	and it'll help us understand.
9430640	9432640	We don't actually need to,
9432640	9433640	we don't need to write this out in code
9433640	9435640	as long as it sort of makes sense.
9435640	9437640	We don't need to write this in the forward pass, really.
9437640	9439640	You're not going to use it anywhere else.
9439640	9442640	So yeah, I'm not going to be too discouraged
9442640	9446640	that that does not work in the functional library.
9446640	9449640	But yeah, so pretty much what this does
9449640	9451640	is if a number is below,
9451640	9454640	if a number is 0 or below 0,
9454640	9456640	it will turn that number into 0.
9456640	9460640	And then if it's above 0, it'll stay the same.
9460640	9463640	So this graph sort of helps you visualize that.
9463640	9465640	There's a little function here.
9465640	9467640	That might make sense to some people.
9467640	9469640	I don't really care about the functions too much
9469640	9472640	as long as I can sort of visualize what the function means,
9472640	9475640	what it does, what are some applications it can be used.
9475640	9479640	That usually covers enough for like any function at all.
9479640	9482640	So that's the Relu function.
9482640	9483640	Pretty cool.
9483640	9487640	It simply offers a non-linearity to our linear networks.
9487640	9489640	So if you have 100 layers deep
9489640	9491640	and every, I don't know,
9491640	9494640	every second step you put a Relu,
9494640	9496640	that network is going to learn a lot more things.
9496640	9499640	It's going to learn a lot more linearity, non-linearity.
9499640	9502640	Then if you were to just have 100 layers
9502640	9505640	multiplying all into one transformation.
9505640	9507640	So that's what that is.
9507640	9508640	That's the Relu.
9508640	9510640	Now let's go over to Sigmoid.
9510640	9514640	So here we can actually use the functional library.
9514640	9518640	And all Sigmoid does is we go 1 over 1 plus
9518640	9521640	exponentiated of negative x.
9521640	9524640	So I'm going to add that here.
9524640	9527640	We could, yeah, why not do that?
9527640	9531640	Negative 0.05 float 32.
9531640	9532640	Sure.
9532640	9536640	We'll go f dot Sigmoid.
9536640	9539640	And then we'll just go x and then we'll print y.
9539640	9540640	Cool.
9540640	9544640	So we get a tensor 0.4875.
9544640	9545640	Interesting.
9545640	9549640	So this little negative 0.05 here
9549640	9553640	is essentially being plugged into this negative x.
9553640	9561640	So 1 over 1 plus 2.71 to the power of negative 0.05.
9561640	9563640	So it's essentially,
9563640	9573640	if we do 2.71, 2.71 to the power of negative 0.05,
9573640	9575640	we're just going to get positive.
9575640	9580640	So 1.05 and then 1 plus that.
9580640	9584640	So that's 2.05.
9584640	9586640	We just do 1 over that.
9586640	9587640	2.05.
9587640	9590640	So we get about 0.487.
9590640	9593640	And what do we get here?
9593640	9594640	0.4 at 7.
9594640	9596640	Cool.
9596640	9597640	So that's interesting.
9597640	9600640	And let's actually look, is there a graph here?
9600640	9605640	Let's look at the Sigmoid activation function.
9605640	9606640	Wikipedia.
9606640	9608640	Don't get too scared by this math here.
9608640	9609640	I don't like it either,
9609640	9613640	but I like the graphs they're cool to look at.
9613640	9615640	So this is pretty much what it's doing here.
9615640	9619640	So yeah, it's just a little curve.
9619640	9623640	Kind of looks like a, it's kind of just like a wave,
9623640	9626640	but it's cool looking.
9626640	9628640	That's what the Sigmoid function does.
9628640	9631640	It's used to just generalize over this line.
9631640	9636640	And yeah, Sigmoid function is pretty cool.
9636640	9638640	So now let's move on to the tanh.
9638640	9640640	The tanh function.
9640640	9643640	Google Bing is, or Microsoft Bing is giving me
9643640	9644640	a nice description of that.
9644640	9646640	Cool.
9646640	9647640	Perfect.
9647640	9648640	E to the negative x.
9648640	9651640	I like that.
9651640	9653640	So tanh is a little bit different.
9653640	9656640	There's a lot more exponentiating going on here.
9656640	9661640	So you have, I'll just say expo or exp of x
9661640	9663640	minus exp of negative x
9663640	9666640	divided by exp of x plus exp of negative x.
9666640	9669640	There's a lot of positives and negatives in here.
9669640	9673640	Positive, positive, negative, negative, negative, positive.
9673640	9676640	So that's interesting.
9676640	9679640	Let's go ahead and put this into code here.
9679640	9683640	So I'll go torch dot examples, or torch examples.
9683640	9685640	This is our file here.
9685640	9690640	And I'll just go tanh.
9690640	9692640	Cool.
9692640	9695640	So negative 0.05.
9695640	9696640	Cool.
9696640	9697640	What if we do a one?
9697640	9701640	What will that produce?
9701640	9705640	Oh, 0.76.
9705640	9710640	What if we do a 10?
9710640	9711640	1.0.
9711640	9713640	Interesting.
9713640	9716640	So this is sort of similar to the sigmoid,
9716640	9718640	except it's, you know,
9718640	9725640	it's actually asked to attach a BT what the difference is.
9726640	9733640	When would you use tanh over sigmoid?
9733640	9736640	Let's see here.
9736640	9740640	Sigmoid function and hyperbolic tangent or tanh function
9740640	9743640	are activations functions used in neural networks.
9743640	9745640	They have a similar s-shaped curve,
9745640	9747640	but have different ranges.
9747640	9750640	So sigmoid output values between a 0 and a 1
9750640	9753640	while tanh is between a negative 1 and a 1.
9753640	9755640	So if you're, you know,
9755640	9758640	if you're rating maybe the,
9758640	9762640	maybe if you're getting a probability distribution,
9762640	9766640	for example, you want it to be between 0 and 1,
9766640	9768640	meaning percentages or decimal places.
9768640	9773640	So like a 0.5 would be 50%, 0.87 would be 87%.
9773640	9776640	And that's what the sigmoid function does.
9776640	9779640	It's quite close to the softmax function, actually.
9779640	9782640	Except the softmax just, you know,
9782640	9785640	it prioritizes the bigger values
9785640	9788640	and puts the smaller values to our priority.
9788640	9790640	That's all the softmax says.
9790640	9792640	It's kind of a sigmoid on steroids.
9792640	9796640	And the tanh outputs between negative 1 and 1.
9796640	9800640	So, yeah, you could maybe even start theorycrafting
9800640	9803640	and thinking of some ways you could use
9803640	9806640	even the tanh function and sigmoid in different use cases.
9806640	9809640	So that's kind of a general overview on those.
9809640	9811640	So biogram language models are finished.
9811640	9813640	All of this we finished here is now done.
9813640	9815640	You're back from your break.
9815640	9818640	If you took one, if you didn't, that's fine too.
9818640	9821640	But pretty much we're going to dig into
9821640	9824640	the transformer architecture now.
9824640	9827640	And we're actually going to build it from scratch.
9827640	9830640	So there was recently a paper proposed
9830640	9833640	called the transformer model.
9833640	9837640	And this uses a mechanism called self-attention.
9837640	9840640	Self-attention is used in these multi-head attention,
9840640	9842640	little bricks here.
9842640	9845640	And there's a lot that happens.
9845640	9847640	So there's something I want to clarify
9847640	9849640	before we jump right into this architecture
9849640	9852640	and just dump a bunch of information
9852640	9855640	on your poor little brain right now.
9855640	9858640	But a lot of these networks, at first,
9858640	9861640	can be extremely confusing to beginners.
9861640	9863640	So I want to make it clear.
9863640	9866640	It's perfectly okay if you don't understand this at first.
9866640	9869640	I'm going to try to explain this in the best way possible.
9869640	9871640	Believe me, I've seen tons of videos
9871640	9874640	on people explaining the transformer architecture.
9874640	9877640	And all of them have been, to some degree,
9877640	9879640	a bit confusing to me as well.
9879640	9881640	So I'm going to try to clarify
9881640	9886640	all those little pieces of confusion.
9886640	9888640	Like what does that mean?
9888640	9889640	You didn't cover that piece.
9889640	9891640	I don't know what's going on here.
9891640	9893640	I'm going to cover all those little bits
9893640	9896640	and make sure that nothing is left behind.
9896640	9898640	So you're going to want to sit tight
9898640	9901640	and pay attention for this next part here.
9901640	9904640	So yeah, let's go ahead and dive into
9904640	9907640	just the general transformer architecture
9907640	9909640	and why it's important.
9909640	9911640	So in the transformer network,
9911640	9914640	you have a lot of computation going on.
9914640	9917640	You have some adding and normalizing.
9917640	9919640	You have some multi-hat attention.
9919640	9921640	You have some feed forward networks.
9921640	9923640	There's a lot going on here.
9923640	9925640	There's a lot of computation, a lot of multiplying,
9925640	9927640	there's a lot going on.
9927640	9929640	So the question I actually had at first was,
9929640	9932640	well, if you're just multiplying these inputs
9932640	9934640	by a bunch of different things along,
9934640	9937640	you should just end up with some random value at the end
9937640	9940640	that maybe doesn't really mean that much
9940640	9942640	of the initial input.
9942640	9944640	And that's actually correct.
9944640	9947640	For the first few iterations,
9947640	9949640	the model has absolutely no context
9949640	9950640	as to what's going on.
9950640	9951640	It is clueless.
9951640	9953640	It is going in random directions
9953640	9957640	and it's just trying to find the best way to converge.
9957640	9959640	So this is what machine learning and deep learning
9959640	9961640	is actually all about,
9961640	9964640	is having all these little parameters in,
9964640	9966640	you know, the adding and normalizing,
9966640	9969640	the feed forward networks, even multi-hat attention.
9969640	9972640	We're trying to optimize the parameters
9972640	9975640	for producing an output that is meaningful
9975640	9977640	that will actually help us produce
9977640	9981640	almost perfectly like English text.
9981640	9983640	And so this is the entire process of pre-training.
9983640	9986640	You send a bunch of inputs into a transformer
9986640	9989640	and you get some output probabilities
9989640	9991640	that you used to generate from.
9991640	9994640	And what attention does
9994640	9997640	is it sets little different scores
9997640	10001640	to, you know, each little token in a sentence.
10001640	10004640	For tokens you have character, subword,
10004640	10006640	and word-level tokens.
10006640	10009640	So you're pretty much just mapping
10009640	10011640	bits of attention to each of these,
10011640	10013640	as well as, you know,
10013640	10016640	what is the position also mean as well.
10016640	10020640	So you could have two words that are right next to each other,
10020640	10022640	but then if you don't actually, you know,
10022640	10024640	positionally encode them,
10024640	10026640	it doesn't really mean much,
10026640	10029640	because it's like, oh, these could be like 4,000 characters apart.
10029640	10031640	So that's why you need both
10031640	10034640	to put attention scores on these tokens
10034640	10037640	and to positionally encode them.
10037640	10039640	And that's what's happening here.
10039640	10044640	So what we do is we get to our inputs.
10044640	10046640	We got our inputs.
10046640	10048640	So, I mean, we went over this with
10048640	10050640	diagram language models.
10050640	10053640	We feed our X and Y,
10053640	10055640	so X would be our inputs,
10055640	10058640	Y would be our targets or outputs.
10058640	10061640	And what we're going to do
10061640	10064640	is give these little embeddings.
10064640	10067640	So I believe we went over embeddings a little while ago,
10067640	10069640	and pretty much what those mean
10069640	10071640	is it's going to have a little row
10071640	10073640	for each token on that table,
10073640	10075640	and that's going to store, you know,
10075640	10078640	some vector as to what that token means.
10078640	10081640	So let's say you had, like, you know,
10081640	10084640	the character E, for example,
10084640	10089640	the sentiment or the vector of the character E
10089640	10091640	is probably going to be vastly different
10091640	10093640	than the sentiment of Z, right?
10093640	10095640	Because E is a very common vowel,
10095640	10097640	and Z is one of the most uncommon,
10097640	10101640	if not the most uncommon letter in the English language.
10101640	10104640	So these embeddings are learned.
10104640	10107640	We have these both for our inputs and our outputs.
10107640	10109640	We give them positional encodings
10109640	10111640	like I was talking about,
10111640	10113640	and there's ways we can do that.
10113640	10116640	We can actually use learnable parameters
10116640	10118640	to assign these encodings.
10118640	10121640	A lot of these are learnable parameters, by the way,
10121640	10123640	and you'll see that as you, you know,
10123640	10125640	delve more and more into transformers.
10125640	10130640	But, yeah, so after we've given these inputs,
10130640	10132640	embeddings, and positional encodings,
10132640	10134640	and same thing with the outputs,
10134640	10136640	which are essentially just shifted right,
10136640	10139640	you have, you know, I up to block size for inputs,
10139640	10144640	and then I plus one up to block size plus one, right?
10144640	10148640	Or whatever little thing we employed here
10148640	10150640	in our background language models.
10150640	10152640	Quite what it was.
10152640	10154640	Or even if we did that at all.
10158640	10160640	No.
10160640	10162640	I'm just speaking gibberish right now,
10162640	10164640	but that's fine because it's going to make sense
10164640	10166640	in a little bit here.
10168640	10171640	So what I'm going to actually do
10171640	10173640	is I'm not going to read off of this right here
10173640	10175640	because this is really confusing.
10175640	10178640	So I'm going to switch over to a little,
10179640	10182640	like a little sketch that I drew out.
10182640	10184640	And this is pretty much the entire transformer
10184640	10186640	with a lot of other things considered
10186640	10190640	that this initial image does not really put into perspective.
10190640	10193640	So let's go ahead and jump into
10193640	10197640	sort of what's going on in here from the ground up.
10197640	10199640	So like I was talking about before,
10199640	10201640	we have some inputs and we have some outputs
10201640	10203640	which are shifted right,
10203640	10207640	and we give each of them some embedding vectors
10207640	10209640	and positional encodings.
10209640	10212640	So from here, let's say we have n layers.
10212640	10214640	This is going to make sense in a second.
10214640	10216640	n layers is set to four.
10216640	10218640	So the amount of layers we have is set to four.
10218640	10221640	So you can see we have an encoder, encoder.
10221640	10224640	Like we have four of these and we have four decoders.
10224640	10227640	So four is actually the amount of encoders
10227640	10229640	and decoders we have.
10229640	10231640	We always have the same amount of each.
10231640	10234640	So if we have, you know, ten layers,
10234640	10237640	that means we have ten encoders and ten decoders.
10237640	10239640	And pretty much what would happen
10239640	10242640	is after this input,
10242640	10244640	embedding and positional embedding,
10244640	10247640	we feed that into the first encoder layer
10247640	10249640	and then the next, and then next,
10249640	10251640	and then right as soon as we hit the last one,
10251640	10256640	we feed these into each of these decoders here,
10256640	10258640	each of these decoder layers.
10258640	10263640	So only the last encoder will feed into these decoders.
10263640	10268640	And pretty much these decoders will all run.
10268640	10270640	They'll all learn different things.
10270640	10272640	And then they'll turn what they learned.
10272640	10275640	They'll do, they'll apply a linear transformation
10275640	10276640	at the end of it.
10276640	10278640	This is not in the decoder function.
10278640	10280640	This is actually after the last decoder.
10280640	10282640	It'll apply a linear transformation
10282640	10286640	to pretty much sort of simplify
10286640	10288640	or give a summary of what it learned.
10288640	10293640	And then we apply a softmax on that new, you know, tensor
10293640	10296640	to get some probabilities to sample from,
10296640	10298640	like we talked about in the generate function
10298640	10299640	in our biogram.
10299640	10302640	And then once we get these probabilities,
10302640	10307640	we can then sample from them and generate tokens.
10307640	10311640	And that's kind of like the first little step here.
10311640	10312640	That's what's going on.
10312640	10313640	We have some encoders.
10313640	10315640	We have some decoders.
10315640	10317640	We do a transformation to summarize.
10317640	10319640	We have a softmax to get probabilities.
10319640	10321640	And then we generate based on those probabilities.
10321640	10324640	Cool.
10324640	10326640	Next up, in the encoder,
10326640	10329640	in each of these encoders, this is what it's going to look like.
10329640	10331640	So we have multi-hat attention,
10331640	10334640	which I'm going to dub into a second here.
10334640	10337640	So after this multi-hat attention,
10337640	10339640	we have a residual connection.
10339640	10342640	So in case you aren't familiar with residual connections,
10342640	10344640	I might have went over this before.
10344640	10346640	But pretty much what they do is
10346640	10348640	it's a little connector.
10348640	10350640	So I don't know.
10350640	10352640	Let's say you get some inputs X,
10352640	10354640	you have some inputs X down here,
10354640	10358640	and you put them into some sort of function here,
10358640	10360640	some sort of like feedforward network, whatever it is.
10360640	10363640	A feedforward network is essentially just a linear,
10363640	10365640	a RELU, and then a linear.
10365640	10367640	That's all feedforward network is right here.
10367640	10369640	Linear, really, really linear.
10369640	10375640	And all you do is you wrap those inputs
10375640	10378640	around so you don't actually put them
10378640	10380640	into that feedforward network.
10380640	10382640	You actually wrap them around,
10382640	10385640	and then you can add them to the output.
10385640	10387640	So you had some X values here,
10387640	10390640	go through the RELU, and then you had some wrap around.
10390640	10394640	And then right here, you simply add them together
10394640	10397640	and you normalize them using some encod layer norm,
10397640	10399640	which we're going to cover in a little bit.
10399640	10403640	And the reason why residual connections
10403640	10406640	are so useful in transformers
10406640	10409640	is because when you have a really deep neural network,
10409640	10412640	a lot of the information is actually forgotten
10412640	10414640	in the first steps.
10414640	10417640	So if you have your first view encoder layers
10417640	10419640	and your first view decoder layers,
10419640	10422640	a lot of the information here is going to be forgotten
10422640	10424640	because it's not being carried through.
10424640	10428640	The first steps of it aren't explicitly being carried through
10428640	10432640	and sort of skipped through the functions.
10432640	10436640	And yeah, you can sort of see how they would just be forgotten.
10436640	10439640	So residual connections are sort of just a cheat
10439640	10441640	for getting around that,
10441640	10443640	for not having deep neural networks forget things
10443640	10445640	from the beginning,
10445640	10447640	and having them all sort of work together to the same degree.
10447640	10450640	So residual connections are great that way.
10450640	10453640	And then, you know, at the end there,
10453640	10456640	you would add them together and then normalize.
10456640	10459640	And there's two different ways that you can do this add a norm.
10459640	10462640	There's add a norm and then norm and add.
10462640	10467640	So these are two different separate architectures
10467640	10470640	that you can do in transformers.
10470640	10474640	And both of these are sort of like meta architectures.
10474640	10480640	But pretty much pre-norm is the normalize then add,
10480640	10482640	and then post-norm is add then normalize.
10482640	10486640	So in this attention is all you need paper
10486640	10491640	proposed by a bunch of research scientists was
10491640	10495640	initially you want to add these,
10495640	10499640	you want to add these together and then normalize them.
10499640	10505640	So that is what we call the post-norm architecture.
10505640	10508640	And then pre-norm is just flip them around.
10508640	10513640	So I've actually done some testing with pre-norm and post-norm
10513640	10517640	and the original transformer paper
10517640	10521640	turned out to be quite actually a lot better,
10521640	10524640	at least for training very small language models.
10524640	10526640	If you're training bigger ones, it might be different,
10526640	10531640	but essentially we're just going to go by the rules that we use in here.
10531640	10532640	So add a norm.
10532640	10533640	We're not going to do norm and add.
10533640	10537640	Add a norm in this video specifically because it works better
10537640	10541640	and we just don't want to break any of the rules and go outside of it
10541640	10542640	because then that starts to get confusing.
10542640	10545640	And actually if you watch the Andre Carpathi lecture
10545640	10547640	on building GPTs from scratch,
10547640	10553640	he actually implemented it in the pre-norm way.
10553640	10555640	So normalize then add.
10555640	10558640	So yeah, based on my experience,
10558640	10565640	what I've done on my computer here is the post-norm architecture works quite better.
10565640	10567640	So that's why we're going to use it.
10567640	10570640	We're going to do add then normalize.
10570640	10575640	So then we essentially feed this into a feedforward network
10575640	10576640	which we covered earlier.
10576640	10580640	And then how did it go?
10580640	10583640	So we're encoder.
10583640	10588640	We do a residual connection from here to here
10588640	10595640	and then another residual connection from outside of our feedforward network.
10595640	10599640	So each time we're doing some other things like some, you know,
10599640	10601640	some computation blocks in here,
10601640	10603640	we're going to have a rest connection.
10603640	10606640	Same with our feedforward rest connection.
10606640	10609640	And then of course the output from here,
10609640	10611640	just when it exits,
10611640	10613640	it's going to feed into the next encoder block
10613640	10615640	if it's not the last encoder.
10615640	10617640	So this one is going to do all this.
10617640	10618640	It's going to feed into that one.
10618640	10619640	It's going to do the same thing.
10619640	10620640	Feed into this one.
10620640	10622640	Going to feed into that one.
10622640	10626640	And then the output of this is going to feed into each of these decoders,
10626640	10629640	all the same information.
10629640	10635640	And yeah, so that's a little bit scoped in as to what these encoders look like.
10635640	10638640	So now that you know what the encoder looks like,
10638640	10639640	what the feedforward looks like,
10639640	10641640	we're going to go into multi-head attention,
10641640	10643640	sort of the premise,
10643640	10646640	sort of the highlight of the transformer architecture
10646640	10648640	and why it's so important.
10648640	10651640	So multi-head attention,
10651640	10653640	we call it multi-head attention
10653640	10655640	because there are a bunch of these different heads
10655640	10657640	learning different semantic info
10657640	10659640	from a unique perspective.
10659640	10662640	So let's say you have 10 different people
10662640	10665640	looking at the same book.
10665640	10667640	If you have 10 different people,
10667640	10672640	let's say they're all reading the same Harry Potter book.
10672640	10674640	These different people,
10674640	10677640	they might have different cognitive abilities.
10677640	10679640	They might have different IQs.
10679640	10681640	They might have been raised in different ways.
10681640	10683640	So they might interpret things differently.
10683640	10686640	They might look at little things in that book
10686640	10688640	and their mind will,
10688640	10690640	they'll imagine different scenarios,
10690640	10692640	different environments from the book.
10692640	10696640	And essentially why this is so valuable
10696640	10699640	is because we don't just want to have one person,
10699640	10701640	just one perspective on this.
10701640	10704640	We want to have a bunch of different heads in parallel
10704640	10710640	looking at this same piece of data
10710640	10713640	because they're all going to capture different things about it.
10713640	10716640	And keep in mind each of these heads,
10716640	10718640	each of these heads in parallel,
10718640	10720640	these different perspectives,
10720640	10722640	they have different learnable parameters.
10722640	10724640	So they're not all the same one
10724640	10726640	looking at this piece of data.
10726640	10729640	They're actually,
10729640	10731640	they all have different learnable parameters.
10731640	10734640	So you have a bunch of these
10734640	10736640	at the same time learning different things
10736640	10738640	and that's why it's so powerful.
10738640	10744640	So this scale.product attention runs in parallel,
10744640	10746640	which means we can scale that to the GPU,
10746640	10748640	which is very useful.
10748640	10750640	It's good to touch on that.
10750640	10752640	Anything with the GPU that you can accelerate
10752640	10754640	is just an automatic win
10754640	10759640	because parallelism is great in machine learning.
10759640	10761640	Why not have parallelism, right?
10761640	10763640	If it's just going to be running the CPU, what's the point?
10763640	10765640	That's why we love GPUs.
10765640	10767640	Anyways, yeah.
10767640	10769640	So you're going to have these different,
10769640	10771640	you're going to have these things that are called keys,
10771640	10773640	queries and values.
10773640	10775640	I'll touch on those in a second here
10775640	10777640	because keys, queries and values
10777640	10779640	sort of point to self-attention,
10779640	10781640	which is literally the entire point of the transformer.
10781640	10783640	Transformer wouldn't really mean anything
10783640	10785640	without self-attention.
10785640	10787640	So I'll touch on those in a second here
10787640	10789640	and we'll actually delve deeper
10789640	10791640	as we hit this sort of block.
10791640	10793640	But yeah, you have these keys, queries and values.
10793640	10795640	They go into scale.product attention.
10795640	10797640	So a bunch of these running in parallel
10797640	10799640	and then you concatenate the results
10799640	10801640	from all these different heads running in parallel.
10801640	10803640	You have all these different people.
10803640	10805640	You concatenate all of them,
10805640	10807640	you generalize it,
10807640	10809640	and then you apply a transformation
10809640	10811640	to a linear transformation
10811640	10813640	to pretty much summarize that
10813640	10816640	and then do your add a norm,
10816640	10818640	then pay for a network.
10818640	10820640	So that's what's going on in multi-head attention.
10820640	10822640	You're just doing a bunch of self-attentions
10822640	10824640	in parallel, concatenating,
10824640	10826640	and then continuing on with this part.
10826640	10828640	So scale.product attention.
10828640	10830640	What is that?
10830640	10832640	So let's just start from the ground up here.
10832640	10834640	We'll just go from left to right.
10834640	10836640	So you have your keys, queries and values.
10836640	10838640	What do your keys do?
10838640	10840640	Well, a key is
10840640	10842640	let's just say you have a token and a sentence.
10842640	10844640	Okay?
10844640	10846640	So if you have
10846640	10848640	let me just
10848640	10850640	roll down here to a good example.
10850640	10852640	So
10852640	10854640	self-attention
10854640	10856640	uses
10856640	10858640	keys, queries and values.
10858640	10860640	Self-attention helps
10860640	10862640	identify
10862640	10864640	which of these tokens in a sentence
10864640	10866640	in any given sentence are more important
10866640	10868640	and how much attention
10868640	10870640	you should pay
10870640	10872640	to each of those characters or words, whatever you're using.
10872640	10874640	We'll just use words
10874640	10876640	to
10876640	10878640	make it easier to understand for the purpose of this video.
10878640	10880640	But
10880640	10882640	essentially imagine you have
10884640	10886640	these two sentences here.
10886640	10888640	So you have
10888640	10890640	let me bring out my little piece of text.
10890640	10892640	So you have
10894640	10896640	that didn't work.
10898640	10900640	So imagine you have
10902640	10904640	server, can I have the check?
10904640	10906640	And then you have
10908640	10910640	and you have
10910640	10912640	looks like I crashed the server.
10912640	10914640	So
10914640	10916640	I mean, both of these have
10916640	10918640	the word server in them, but they mean different things.
10918640	10920640	Server meaning like the waiter
10920640	10922640	or the waitress or whoever
10922640	10924640	is billing
10924640	10926640	you at the end of your restaurant visit.
10926640	10928640	And then looks like I crashed the server
10928640	10930640	is like, oh, there's actually a server running
10930640	10932640	in the cloud, not like a person
10932640	10934640	that's billing me, but an actual server.
10934640	10936640	That's maybe running a video game.
10936640	10938640	And
10938640	10940640	these are two different things. So what attention can do
10940640	10942640	is it can actually identify
10942640	10944640	which words would get attention here.
10944640	10946640	So it can say
10946640	10948640	server, can I have the check?
10948640	10950640	Can I have?
10950640	10952640	So it's maybe you're looking
10952640	10954640	for something you're looking for the check
10954640	10956640	and then server
10956640	10958640	is like, oh, well in this
10958640	10960640	in this particular sequence or in this
10960640	10962640	in the sentiment of this sentence here
10962640	10964640	server
10964640	10966640	is specifically tied to
10966640	10968640	this one meaning, maybe a human
10968640	10970640	someone at a restaurant
10970640	10972640	and then crash
10972640	10974640	the server
10974640	10976640	crash is going to get a very high attention
10976640	10978640	score because
10978640	10980640	you don't normally
10980640	10982640	crash a server at a restaurant
10982640	10984640	that doesn't particularly make sense.
10984640	10986640	So
10986640	10988640	if you have different words like this
10988640	10990640	what self-attention will do
10990640	10992640	is it will learn
10992640	10994640	which words in the sentence
10994640	10996640	are actually more important
10996640	10998640	and which words should
10998640	11000640	pay more attention to.
11000640	11002640	So that's really all that's going on here
11002640	11004640	and
11004640	11006640	the key
11006640	11008640	is essentially going to emit
11008640	11010640	a different
11010640	11012640	it's going to emit
11012640	11014640	a little tensor
11014640	11016640	here saying
11016640	11018640	what do I contain
11018640	11020640	and then query
11020640	11022640	is going to say
11022640	11024640	what am I looking for?
11024640	11026640	So what's going to happen
11026640	11028640	is if these, let's say
11028640	11030640	server, it's going to look for things like
11030640	11032640	check or crashed
11032640	11034640	so if it sees crashed
11034640	11036640	then that means the key and the query
11036640	11038640	are going to multiply
11038640	11040640	and it's going to get a very high attention score
11040640	11042640	but if you have something
11042640	11044640	like
11044640	11046640	it's like
11046640	11048640	there's literally almost any sentence
11048640	11050640	so that doesn't mean much.
11050640	11052640	We're not going to pay attention to those words
11052640	11054640	so that's going to get a very low attention score
11054640	11056640	and all attention
11056640	11058640	is you're just dot-producting
11058640	11060640	these vectors together.
11060640	11062640	So you get a key
11062640	11064640	and a query, you dot-product them
11064640	11066640	we already went over dot-products
11066640	11068640	in this course before
11068640	11070640	and then
11070640	11072640	this is a little bit of a confusing part
11072640	11074640	is you just scale
11074640	11076640	by one over the
11076640	11078640	square root
11078640	11080640	of the length of a row
11080640	11082640	in the keys or queries matrix
11082640	11084640	otherwise known as
11084640	11086640	DK.
11086640	11088640	So let's say we have
11088640	11090640	our key and our query
11090640	11092640	these are all going to be the same length by the way.
11092640	11094640	Let's say our keys
11094640	11096640	is
11096640	11098640	maybe our keys is going to be like
11098640	11100640	10 characters long
11100640	11102640	our keys are going to be 10 characters long as well
11102640	11104640	so it's going to do
11104640	11106640	one over the square root of 10
11106640	11108640	if that makes sense
11108640	11110640	and so
11110640	11112640	that's just
11112640	11114640	essentially a way of preventing
11114640	11116640	these dot-products
11116640	11118640	from exploding
11118640	11120640	we want to scale them because
11120640	11122640	as we have
11122640	11124640	as the length of it increases
11124640	11126640	so will the
11126640	11128640	ending dot-product
11128640	11130640	because there's more of these to multiply
11130640	11132640	so we pretty much just want to
11132640	11134640	scale it by using
11134640	11136640	an inverse square root
11136640	11138640	and that will just help us with
11138640	11140640	scaling make sure nothing explodes
11140640	11142640	in unnecessary ways
11142640	11144640	and then
11144640	11146640	the next little important part
11146640	11148640	is using tort.trill
11148640	11150640	which I imagine we went over in our examples here
11150640	11152640	trill
11152640	11154640	yeah
11154640	11156640	so
11156640	11158640	you can see that
11158640	11160640	it's a diagonal
11160640	11162640	it's a left triangular
11162640	11164640	matrix of ones
11164640	11166640	and these aren't going to be ones
11166640	11168640	in our self-attention here
11168640	11170640	in our tort.trill or masking
11170640	11172640	what this is going to be
11172640	11174640	is
11174640	11176640	the scores at each time step
11176640	11178640	the combination of scores
11178640	11180640	at each time step
11180640	11182640	so
11182640	11184640	if we've only gone
11184640	11186640	if we're only looking at the first
11186640	11188640	time step
11188640	11190640	we should not have access to the rest of things
11190640	11192640	or else that would be cheating
11192640	11194640	we shouldn't be allowed to look ahead
11194640	11196640	because we haven't actually produced these yet
11196640	11198640	we need to produce these before we can
11198640	11200640	put them into perspective
11200640	11202640	and put a weight on them
11202640	11204640	so we're going to set all these to zero
11204640	11206640	and then we go to the next time step
11206640	11208640	so now we've just generated this
11208640	11210640	one we haven't generated these yet
11210640	11212640	so we can't look at them
11212640	11214640	and then as we go more and more
11214640	11216640	as the time step increases
11216640	11218640	we know more and more context
11218640	11220640	about all of these tokens
11220640	11222640	so
11222640	11224640	that's all that's doing
11224640	11226640	mask attention is pretty much just saying
11226640	11228640	we don't want to look into the future
11228640	11230640	we want to only guess with what we currently know
11230640	11232640	in our current time step
11232640	11234640	and everything before it
11234640	11236640	you can't jump into the future
11236640	11238640	look at what happened in the past
11238640	11240640	and do stuff based on that
11240640	11242640	same thing applies to life
11242640	11244640	you can't really skip to the future and say
11244640	11246640	hey if you do this you're going to be a billionaire
11246640	11248640	no that would be cheating
11248640	11250640	you're not allowed to do that
11250640	11252640	you can only look at the mistakes you made
11252640	11254640	and say how can I become a billionaire
11254640	11256640	based on all these other mistakes that I made
11256640	11258640	how can I become as close to perfect as possible
11258640	11260640	which no one I can ever be perfect
11260640	11262640	but that's my little analogy for the day
11262640	11264640	so that's mask attention
11264640	11266640	pretty much just not letting us skip time steps
11266640	11268640	so that's fun
11268640	11270640	let's continue
11270640	11272640	two more little things I want to touch on before I jump forward here
11272640	11274640	so
11274640	11276640	these keys, queries and values
11276640	11278640	each of these are learned through a linear transformation
11278640	11280640	just an end dot linear
11280640	11282640	is applied
11282640	11284640	and that's how we get our keys, queries and values
11284640	11286640	so that's just a little
11286640	11288640	touching there if you're wondering how do we get those
11288640	11290640	it's just an end dot linear transformation
11290640	11292640	and then as for our
11292640	11294640	masking we don't actually apply this all the time
11294640	11296640	you might have seen right here
11296640	11298640	we have
11298640	11300640	multi-head attention
11300640	11302640	multi-head attention and then mask
11302640	11304640	multi-head attention
11304640	11306640	so this masked attention isn't used all the time
11306640	11308640	it's only used
11308640	11310640	actually one out of the three attentions
11310640	11312640	we have per layer
11312640	11314640	so
11314640	11316640	I'll give you a little bit more information
11316640	11318640	about that as we
11318640	11320640	progress more and more into the architecture
11320640	11322640	as we learn more about it
11322640	11324640	I'm not going to dive into that
11324640	11326640	quite yet though
11326640	11328640	so let's just continue on with what's going on
11328640	11330640	so we have a softmax
11330640	11332640	and why softmax important
11332640	11334640	well
11334640	11336640	I actually mentioned earlier
11336640	11338640	softmax is not commonly used
11338640	11340640	as a normalization method
11340640	11342640	but here we're actually using
11342640	11344640	softmax to normalize
11344640	11346640	so when you have all of these
11346640	11348640	when you have all of these
11348640	11350640	attention scores
11350640	11352640	essentially what the softmax is doing
11352640	11354640	is it's going to
11354640	11356640	exponentiate and normalize all of these
11356640	11358640	so
11358640	11360640	all of the attention scores that have scored
11360640	11362640	high like maybe 50 to
11362640	11364640	90% or whatever it is
11364640	11366640	those are going to take a massive effect
11366640	11368640	in that entire
11368640	11370640	attention
11370640	11372640	I guess tensor if you want to call it that
11374640	11376640	and that's important
11376640	11378640	it might not seem important
11378640	11380640	but it's essentially just giving the model
11380640	11382640	more confidence
11382640	11384640	as to which tokens matter more
11384640	11386640	so for example
11386640	11388640	if we just
11388640	11390640	did a normalization
11390640	11392640	we would
11392640	11394640	have words like server and crash
11394640	11396640	and then server and check
11396640	11398640	and then
11398640	11400640	you would just know
11400640	11402640	a decent amount about those
11402640	11404640	those would pay attention to a decent amount
11404640	11406640	because they multiply together quite well
11406640	11408640	but if you softmax those
11408640	11410640	then it's like
11410640	11412640	those are almost the only characters that matter
11412640	11414640	so it's looking at the context
11414640	11416640	of those two
11416640	11418640	and then we're sort of filling in
11418640	11420640	like we're learning about the rest of the sentence
11420640	11422640	based on just the
11422640	11424640	sentiment of those attention scores
11424640	11426640	because they're so high priority
11426640	11428640	because they multiply together
11428640	11430640	to such a high degree
11430640	11432640	we want to emphasize them
11432640	11434640	basically let the model learn more
11434640	11436640	about which words matter more together
11436640	11438640	so
11438640	11440640	that's pretty much just what the softmax does
11440640	11442640	it increases our confidence in
11442640	11444640	attention
11444640	11446640	and then a matrix multiply
11446640	11448640	we go back to our V here
11448640	11450640	and this is a value
11450640	11452640	so essentially what this is
11452640	11454640	is just a linear transformation
11454640	11456640	and we apply this on our
11456640	11458640	we apply this on our inputs
11458640	11460640	and
11460640	11462640	we have some value about
11462640	11464640	you know
11464640	11466640	what exactly those tokens are
11466640	11468640	and after we've gotten all of our attention
11468640	11470640	our softmax everything done
11470640	11472640	it's just going to multiply
11472640	11474640	the original values
11474640	11476640	by everything we've gotten so far
11476640	11478640	just so that you don't have any information
11478640	11480640	that's really lost or we don't have anything scrambled
11480640	11482640	just that we have like a general idea
11482640	11484640	of okay these are actually
11484640	11486640	all the tokens we have
11486640	11488640	and then these are
11488640	11490640	we found interesting the attention scores
11492640	11494640	so
11494640	11496640	we have an output which is a blend of input
11496640	11498640	vector values and attention placed on each token
11498640	11500640	and
11500640	11502640	that's pretty much what's happening in scaled dot
11502640	11504640	product attention in parallel
11504640	11506640	so we have a bunch of these that are just happening
11506640	11508640	at the same time
11508640	11510640	many of these happening at the same time
11510640	11512640	and yeah so
11512640	11514640	that's what attention is
11514640	11516640	that's what feedforward networks are
11516640	11518640	residual connections are
11520640	11522640	and yeah
11522640	11524640	and then so after this after we've
11524640	11526640	fed these into our decoders
11526640	11528640	we get an output
11528640	11530640	we apply linear transformation to summarize
11530640	11532640	softmax probabilities
11532640	11534640	and then we generate based on that
11534640	11536640	based on everything that we learned
11536640	11538640	and
11538640	11540640	actually what I didn't quite write a lot about
11540640	11542640	was the decoder
11542640	11544640	so what I'm actually going to talk about next
11544640	11546640	is something I didn't fill in yet
11546640	11548640	which is why
11548640	11550640	why the heck do we
11550640	11552640	use mass attention here
11552640	11554640	but not in these places so why the heck
11554640	11556640	do we have a multi attention here
11556640	11558640	all that attention here but mass attention here
11558640	11560640	so why is this
11560640	11562640	well the purpose of the encoder
11562640	11564640	is to pretty much learn
11564640	11566640	the present
11566640	11568640	past and future
11568640	11570640	and put that into a vector representation
11570640	11572640	for the decoder
11572640	11574640	that's what the encoder does
11574640	11576640	so it's okay if we look into the future
11576640	11578640	and understand tokens that way
11578640	11580640	because we're technically not cheating
11580640	11582640	we're just learning the different attention scores
11582640	11584640	and yeah we're just using that
11584640	11586640	to help us predict based on
11586640	11588640	what the sentence looks like
11588640	11590640	but not explicitly giving it away
11590640	11592640	just giving it an idea of
11592640	11594640	what to look for type of thing
11594640	11596640	and then
11596640	11598640	we use mass attention here because
11598640	11600640	well we don't want to look ahead
11600640	11602640	we want to look at the present and the past
11602640	11604640	and
11604640	11606640	later on
11606640	11608640	we're not giving anything explicit
11608640	11610640	here we're not giving anything yet
11610640	11612640	so we want to make some raw guesses
11612640	11614640	they're not going to be very good guesses at first
11614640	11616640	we want to make some raw guesses
11616640	11618640	and then later on
11618640	11620640	we can feed these
11620640	11622640	the added and normalized guesses
11622640	11624640	into
11624640	11626640	this next multi attention
11626640	11628640	which isn't masked
11628640	11630640	and then we can use
11630640	11632640	this max multi head attention
11632640	11634640	with the vector representation
11634640	11636640	given by the encoder
11636640	11638640	and then we can sort of do
11638640	11640640	more useful things with that
11640640	11642640	rather than just being forced to guess
11642640	11644640	raw attention scores
11644640	11646640	and then being judged for that
11646640	11648640	we can sort of introduce more
11648640	11650640	more and more elements
11650640	11652640	in this decoder block to help us learn more meaningful things
11652640	11654640	so
11654640	11656640	we start off with
11656640	11658640	making this
11658640	11660640	mass multi head attention
11660640	11662640	and then combining that
11662640	11664640	with
11664640	11666640	our
11666640	11668640	then afterwards we do a multi head attention
11668640	11670640	with the
11670640	11672640	vector representation from the encoder
11672640	11674640	and then we can make decisions on that
11674640	11676640	so that's kind of why that works
11676640	11678640	this way
11678640	11680640	if you don't think I explain it like amazingly
11680640	11682640	well you can totally just
11682640	11684640	ask GPT4
11684640	11686640	or GPT3.5
11686640	11688640	and get a pretty decent answer
11688640	11690640	but that's how that works
11690640	11692640	and
11692640	11694640	another thing I kind of wanted to point out here
11694640	11696640	is these linear transformations
11696640	11698640	that you see
11698640	11700640	I mean there's a lot of them
11700640	11702640	in the
11702640	11704640	scaled dot project attention
11704640	11706640	so you have your linears
11706640	11708640	for your value or key value
11708640	11710640	and key query and values
11710640	11712640	so
11712640	11714640	as well as the one up here
11714640	11716640	linears are great
11716640	11718640	for just expanding or shrinking
11718640	11720640	a bunch of important info
11720640	11722640	into something easier to work with
11722640	11724640	so if you have a bunch of
11724640	11726640	if you have a large vector containing a bunch
11726640	11728640	of info learned from this
11728640	11730640	scaled dot project attention
11730640	11732640	you can
11732640	11734640	you can sort of just compress
11734640	11736640	that into something more manageable
11736640	11738640	through a linear transformation
11738640	11740640	and it's essentially what's just happening here
11740640	11742640	with Softmax as well as
11742640	11744640	in our
11744640	11746640	scaled dot project attention here
11746640	11748640	for these linear transformations
11748640	11750640	from our inputs
11750640	11752640	to
11752640	11754640	quick keys, queries and values
11754640	11756640	that's all that's happening
11756640	11758640	if you want to read more about
11758640	11760640	linear transformations the importance of them
11760640	11762640	you can totally go out of your way to do that
11762640	11764640	but that's just sort of a brief summary
11764640	11766640	as to why they're important
11766640	11768640	just shrinking or expanding
11768640	11770640	so that's sort of a brief overview on how
11770640	11772640	transformers work
11772640	11774640	however in this
11774640	11776640	course we will not be building the transformer
11776640	11778640	architecture we'll be building
11778640	11780640	something called a GPT which you're probably familiar
11780640	11782640	with and GPT stands for
11782640	11784640	Generatively Pre-Trained Transformer
11784640	11786640	or Generative Pre-Trained Transformer
11786640	11788640	one of the two
11788640	11790640	and pretty much what this is
11790640	11792640	it's pretty close to the transformer
11792640	11794640	this architecture here except
11794640	11796640	it only adopts
11796640	11798640	the decoder blocks and it takes away
11798640	11800640	this multi-head attention here
11800640	11802640	so all we're doing is we're removing
11802640	11804640	the encoder
11804640	11806640	as well as what the encoder plugs into
11806640	11808640	so all we have left
11808640	11810640	is just some inputs
11810640	11812640	our max multi-head
11812640	11814640	attention
11814640	11816640	our post-norm architecture
11816640	11818640	and then
11818640	11820640	right after this we're not going to
11820640	11822640	a non-mass multi-head attention
11822640	11824640	but rather to a feed forward network
11824640	11826640	and then a post-norm
11826640	11828640	so that's all it is, it's just 1, 2, 3, 4
11828640	11830640	that's all it's going to look like
11830640	11832640	that's all the blocks are going to be
11832640	11834640	it is still important
11834640	11836640	to understand the transformer architecture itself
11836640	11838640	because you might need that in the future
11838640	11840640	and it is sort of a good practice in language
11840640	11842640	modeling to
11842640	11844640	have a grasp on and to understand
11844640	11846640	you know why we use mass multi-head
11846640	11848640	attention in the decoder and why we don't
11848640	11850640	use it in the encoder and stuff like that
11850640	11852640	so anyways
11852640	11854640	we're going to go ahead and build this
11854640	11856640	if you need to
11856640	11858640	look back if something wasn't quite clear
11858640	11860640	definitely skip back a few seconds
11860640	11862640	or a few minutes through the video and just
11862640	11864640	make sure you clarify everything up to this point
11864640	11866640	but yeah
11866640	11868640	I'm going to go over some more
11868640	11870640	math on the side here and just some other
11870640	11872640	little
11872640	11874640	little widgets we're going to need
11874640	11876640	for building the decoder
11876640	11878640	GPT architecture
11878640	11880640	so let's go ahead and do that
11880640	11882640	we're going to jump into
11882640	11884640	building the transformer rather than
11884640	11886640	building the GPT from scratch
11886640	11888640	what I want to do is linger on
11888640	11890640	self-attention for a little bit
11890640	11892640	or rather just the attention mechanism
11892640	11894640	and the matrix multiplication behind it
11894640	11896640	and why it works
11896640	11898640	so I'm going to use
11898640	11900640	whiteboard to illustrate this
11900640	11902640	so we're going to go ahead and draw out
11902640	11904640	a
11904640	11906640	we'll just use maybe a four token
11906640	11908640	sequence here of words
11908640	11910640	okay
11910640	11912640	so
11912640	11914640	we're going to highlight which words
11914640	11916640	are probably going to end up
11916640	11918640	correlating together
11918640	11920640	or the attention mechanism
11920640	11922640	is going to multiply them together
11922640	11924640	to a high amount based on what it learns
11924640	11926640	about those tokens this is what this is
11926640	11928640	so I'm going to help us illustrate that
11928640	11930640	and what the
11930640	11932640	GPT is going to see
11932640	11934640	sort of from the inside what it looks like from the inside
11934640	11936640	so
11936640	11938640	I'm going to go ahead and draw this out here
11942640	11944640	just make a table here
11944640	11946640	we'll give it
11950640	11952640	four of these
11954640	11956640	and draw a little line through the middle
11956640	11958640	my drawing might not be
11958640	11960640	perfect but it's definitely better
11960640	11962640	than on paper
11962640	11964640	so cool we have this
11964640	11966640	we have
11966640	11968640	my
11970640	11972640	I'm going to go here
11972640	11974640	dog
11978640	11980640	has
11980640	11982640	please
11982640	11984640	and then my
11986640	11988640	my dog
11990640	11992640	so I delete that
11994640	11996640	my dog has
11996640	11998640	please
11998640	12000640	cool
12000640	12002640	so to what degree
12002640	12004640	are these going to interact well my and my
12004640	12006640	I mean it doesn't really
12006640	12008640	give away that much it's only just the start
12008640	12010640	so maybe this will interact to
12010640	12012640	a low amount
12012640	12014640	and then you have my and dog
12014640	12016640	these might interact to a medium
12016640	12018640	amount because it's like your dog
12018640	12020640	so we might go
12020640	12022640	we might go medium
12022640	12024640	like that
12024640	12026640	and then my and has well that doesn't give away too much
12026640	12028640	so maybe that'll be low
12028640	12030640	and then my and please it's like oh
12030640	12032640	that doesn't really mean much my please that doesn't
12032640	12034640	really make sense maybe we'll
12034640	12036640	have it interact to a low amount
12036640	12038640	and then
12038640	12040640	these would be the same
12040640	12042640	thing so
12042640	12044640	my and dog so be medium
12044640	12046640	and then has and has
12046640	12048640	would be low
12048640	12050640	and then my and please would also be low
12050640	12052640	and then you have dog and dog
12052640	12054640	so these might interact to a low amount they're the same word
12054640	12056640	so we'll just
12056640	12058640	forget about that and then we have
12058640	12060640	a dog has
12060640	12062640	so these might interact to a medium amount
12062640	12064640	dog has the dog has
12064640	12066640	something
12066640	12068640	and then dog and please
12068640	12070640	these might interact to a high amount
12070640	12072640	because they're associating the dog
12072640	12074640	with something else meaning please
12074640	12076640	we have has
12076640	12078640	and dog these would interact to the same amount so
12078640	12080640	medium and then has and has
12080640	12082640	be
12082640	12084640	probably
12084640	12086640	to a low amount
12086640	12088640	and then
12088640	12090640	we could do low for
12090640	12092640	we could do what was it high
12092640	12094640	for this one as well please and dog
12094640	12096640	so these will interact
12096640	12098640	to a high amount
12098640	12100640	and then we have has and please
12100640	12102640	so
12102640	12104640	these could interact maybe a medium
12104640	12106640	amount
12106640	12108640	medium and then please and please which would be low
12108640	12110640	so what you get
12110640	12112640	I'll just highlight this in
12112640	12114640	I'll just highlight this in green here
12114640	12116640	so you get
12116640	12118640	all the medium
12118640	12120640	and high attention scores
12120640	12122640	you'd have your medium here
12122640	12124640	medium here
12124640	12126640	high medium
12126640	12128640	medium high
12128640	12130640	medium and medium
12130640	12132640	so you can see that these are sort of symmetrical
12132640	12134640	and this is what the attention map
12134640	12136640	will look like of course there's going to be some
12136640	12138640	scaling going on here based on the amount
12138640	12140640	of actual attention's
12140640	12142640	heads we have running in parallel
12142640	12144640	but that's besides the point
12144640	12146640	really what's going on here
12146640	12148640	is the network
12148640	12150640	is going to learn how to place
12150640	12152640	the right
12152640	12154640	attention scores because attention is simply
12154640	12156640	being used to generate tokens
12156640	12158640	that's that's how the
12158640	12160640	that's how the GPT works it's using attention
12160640	12162640	to generate tokens
12162640	12164640	so we can make
12164640	12166640	those sort of attention
12166640	12168640	scores how they're placed
12168640	12170640	we can make those learnable
12170640	12172640	through all of the like embeddings
12172640	12174640	like everything we have in the entire
12174640	12176640	network can make sure
12176640	12178640	that we place effective attention scores
12178640	12180640	and to make sure that they're measured properly
12180640	12182640	so
12182640	12184640	obviously I didn't quantify these very well
12184640	12186640	like not with floating point numbers
12186640	12188640	but this is sort of the premise
12188640	12190640	of how it works and how we want
12190640	12192640	the model to look at different tokens
12192640	12194640	and how they relate to one another
12194640	12196640	so that's what the
12196640	12198640	attention mechanism looks like under the hood
12198640	12200640	so this is what the actual
12200640	12202640	GPT or decoder only
12202640	12204640	transformer architecture looks like
12204640	12206640	and
12206640	12208640	so I'm just going to go through this step by step here
12208640	12210640	and then we can hopefully jump into some of the math
12210640	12212640	and code behind how this works
12212640	12214640	so we have
12214640	12216640	our inputs embeddings and positional
12216640	12218640	encodings we have only decoder
12218640	12220640	blocks and then some
12220640	12222640	linear transformation
12222640	12224640	and then pretty much just
12224640	12226640	we do some softmax
12226640	12228640	probability distribution
12228640	12230640	we sample from those and then we
12230640	12232640	start just generating some output
12232640	12234640	and then we compare those to our inputs
12234640	12236640	and see how off they were, optimized from that
12236640	12238640	in each of these
12238640	12240640	decoder blocks we have our all data
12240640	12242640	attention, res connections
12242640	12244640	feedforward network consists
12244640	12246640	of a linear, real linear
12246640	12248640	border and then
12248640	12250640	another res connection
12250640	12252640	in each of these multi-attentions
12252640	12254640	we have
12254640	12256640	multiple heads running in parallel
12256640	12258640	and each of these heads is going to take a
12258640	12260640	key, query and value
12260640	12262640	these are all learnable
12262640	12264640	linear transformations
12264640	12266640	and
12266640	12268640	we're going to basically dot product the key and query together
12268640	12270640	concatenate these results
12270640	12272640	and
12272640	12274640	do a little transformation to sort of
12274640	12276640	summarize it afterwards
12276640	12278640	and then what actually goes on in the
12278640	12280640	dot product attention is just the dot
12280640	12282640	product meaning of the key and query
12282640	12284640	the scaling to prevent
12284640	12286640	these values from exploding
12286640	12288640	to prevent the vanishing gradient problem
12288640	12290640	and then we have our
12290640	12292640	masking to make sure that
12292640	12294640	these, to make sure the model
12294640	12296640	isn't looking ahead and cheating
12296640	12298640	and then softmax matrix multiply
12298640	12300640	we output that and then
12300640	12302640	kind of fill in the blank there, so cool
12302640	12304640	this is a little bit
12304640	12306640	pretty much the
12306640	12308640	transform architecture a little bit dumb
12308640	12310640	down a little smaller
12310640	12312640	in complexity to actually understand but
12312640	12314640	that's kind of the premise of what's going on here
12314640	12316640	so still
12316640	12318640	implements a self-attention mechanism
12320640	12322640	so as you can see now
12322640	12324640	I am currently
12324640	12326640	on my macbook
12326640	12328640	M2 chip, I'm not going to
12328640	12330640	go into the specs of why it's important
12330640	12332640	but really quick, I'm just going to show you
12332640	12334640	how I SSH onto my other PC
12334640	12336640	so I go
12336640	12338640	SSH
12338640	12340640	just like that and then I type in my
12340640	12342640	ipv4 address
12342640	12344640	and then
12344640	12346640	I just
12346640	12348640	get a simple password
12348640	12350640	here, password that I've never had
12350640	12352640	is cool
12352640	12354640	so now I'm on my desktop computer
12354640	12356640	and this is the command prompt that I use for it
12356640	12358640	so awesome
12358640	12360640	I'm going to go ahead and go into the
12360640	12362640	free code camp
12362640	12364640	little directory I have
12364640	12366640	so cd desktop
12366640	12368640	cd python testing
12368640	12370640	and then here I'm actually going to activate
12370640	12372640	my CUDA virtual
12372640	12374640	environment
12374640	12376640	oops, not accelerate
12376640	12378640	I'm going to go CUDA
12378640	12380640	activate
12380640	12382640	cool and then I'm going to go
12382640	12384640	cd into free code camp
12384640	12386640	gbt course, awesome
12386640	12388640	so now, if I actually do
12388640	12390640	code on here like this to open up my
12390640	12392640	VS code, it doesn't do that
12392640	12394640	so there's another little way I have to do this
12394640	12396640	and you have to go into
12396640	12398640	VS code
12398640	12400640	go into a little remote explorer here
12400640	12402640	and then you can simply connect
12402640	12404640	so I'm just going to connect
12404640	12406640	to the current window
12406640	12408640	itself
12408640	12410640	there's an extension you need for this
12410640	12412640	called open SSH server, I think it's what it's called
12412640	12414640	and
12414640	12416640	it's simply the same password I used in the command prompt
12416640	12418640	I can type it correctly
12424640	12426640	awesome
12426640	12428640	so now it's SSH into my computer
12428640	12430640	upstairs
12430640	12432640	and I'm just going to open the little editor in here
12434640	12436640	nice, so you can see
12436640	12438640	that it looks just like that, that's wonderful
12438640	12440640	so now
12440640	12442640	I'm going to open this in a Jupyter notebook
12444640	12446640	actually
12446640	12448640	cd into desktop here
12448640	12450640	cd python
12450640	12452640	cd python testing
12452640	12454640	CUDA scripts
12454640	12456640	activate
12456640	12458640	cd free code camp
12458640	12460640	gbt course and then code
12460640	12462640	like that and it will open
12462640	12464640	perfect
12464640	12466640	how wonderful is that and I've already done
12466640	12468640	a little bit of this here but
12468640	12470640	we're going to
12470640	12472640	jump into exactly
12472640	12474640	how we can build up this transformer
12474640	12476640	or gbt architecture
12476640	12478640	in the code itself
12478640	12480640	so I'm going to
12480640	12482640	pop over to my Jupyter notebook in here
12486640	12488640	cool and now this little address
12488640	12490640	I'm going to paste that
12490640	12492640	into my
12492640	12494640	browser
12494640	12496640	awesome
12496640	12498640	so we have this gbt v1
12498640	12500640	Jupyter notebook
12502640	12504640	so what I've actually done is
12504640	12506640	I've done some importations here
12506640	12508640	so I've
12508640	12510640	imported all of these
12510640	12512640	python importations
12512640	12514640	all the hyper parameters that we used from before
12516640	12518640	I've imported the data loader
12518640	12520640	I've imported the tokenizer
12520640	12522640	the train and bell splits
12522640	12524640	they get batch function
12524640	12526640	estimate loss, just everything
12526640	12528640	that we're going to need and it's all in
12528640	12530640	neatly organized little code blocks
12530640	12532640	so awesome
12532640	12534640	now what?
12534640	12536640	well let's go ahead and continue here
12536640	12538640	with the actual
12538640	12540640	upgrading
12540640	12542640	from the very
12542640	12544640	top level so I remember
12544640	12546640	I actually showed
12546640	12548640	and you can skip back to this
12548640	12550640	I actually showed
12550640	12552640	the architecture of the gbt
12552640	12554640	sort of
12554640	12556640	lined out in I guess a little sketch
12556640	12558640	a little sketch that I did
12558640	12560640	and all we're going to do
12560640	12562640	is pretty much build up from the high level
12562640	12564640	the high high level general
12564640	12566640	architecture down to the technical stuff
12566640	12568640	down to the very root
12568640	12570640	dot product attention
12570640	12572640	that we're going to be doing here
12572640	12574640	so I'm going to go ahead and start off
12574640	12576640	with this
12576640	12578640	gbt language model which I just
12578640	12580640	renamed I replaced
12580640	12582640	bygram
12582640	12584640	with gbt here
12584640	12586640	so that's all we're doing and
12586640	12588640	we're going to add some
12588640	12590640	little code bits and
12590640	12592640	just walk through step by step
12592640	12594640	what we're doing so
12594640	12596640	let's do that so great
12596640	12598640	we're going to next we're going to talk about
12598640	12600640	these positional encodings
12600640	12602640	so I go back to the paper here
12602640	12604640	rather this architecture
12604640	12606640	we initially have our tokenize inputs
12606640	12608640	and then we give
12608640	12610640	we give them embedding
12610640	12612640	so token embeddings and then a positional
12612640	12614640	encoding so this positional
12614640	12616640	encoding going back to the attention paper is right here
12616640	12618640	so all it does
12618640	12620640	is every
12620640	12622640	even token index
12622640	12624640	we apply this function
12624640	12626640	and then every odd token index
12626640	12628640	we apply this function you don't really need to know
12628640	12630640	what it's doing other than
12630640	12632640	the fact that these are the different sine
12632640	12634640	and cosine functions that it uses
12634640	12636640	to apply positional encodings
12636640	12638640	to the tokenized inputs
12638640	12640640	so every
12640640	12642640	so on our first
12642640	12644640	index or whatever let's say we have hello world
12644640	12646640	okay there's five characters here
12646640	12648640	h will be index zero
12648640	12650640	so it'll get an even
12650640	12652640	encoding function
12652640	12654640	and then e will be odd
12654640	12656640	since it's index one so it'll get this one
12656640	12658640	and then l will get this the next l will get
12658640	12660640	this and then
12660640	12662640	or I don't know if I messed up that
12662640	12664640	order but essentially it just iterates
12664640	12666640	and it goes back and forth between
12666640	12668640	those applying these fixed functions
12668640	12670640	and the thing is with fixed functions
12670640	12672640	is that they don't actually
12672640	12674640	learn about the data at all
12674640	12676640	because they're fixed so another way we could
12676640	12678640	do this would be using
12678640	12680640	nn.embedding which is what we use
12680640	12682640	for the token
12682640	12684640	embedding so I'm going to go ahead
12684640	12686640	and implement this here in our
12686640	12688640	gbtv one script so I'm going to go
12688640	12690640	ahead and add on this line
12690640	12692640	self dot positional
12692640	12694640	self dot position embedding table
12694640	12696640	nn.embedding block size
12696640	12698640	so the block size is the length
12698640	12700640	or the sequence length
12700640	12702640	which in our case
12702640	12704640	it's going to be 8 so there's going to be 8 tokens
12704640	12706640	and
12706640	12708640	this means
12708640	12710640	we're going to have 8 different indices
12710640	12712640	and each one is going to be
12712640	12714640	of size nn.embed
12714640	12716640	and this is a new parameter I actually want to add here
12716640	12718640	so
12718640	12720640	nn.embed will not only be used
12720640	12722640	in positional embedding
12722640	12724640	but it will also be used in our
12724640	12726640	token embedding because when we actually
12726640	12728640	store
12728640	12730640	information about the tokens
12730640	12732640	we want that to be in a very large
12732640	12734640	vector so not necessarily
12734640	12736640	a probability distribution
12736640	12738640	or what we were using before in the
12738640	12740640	bi-gram language model but rather
12740640	12742640	a really large vector
12742640	12744640	or a list you could think about it
12744640	12746640	as a bunch of different
12746640	12748640	attributes that
12748640	12750640	are about a character so maybe
12750640	12752640	you know
12752640	12754640	A and E would be pretty close
12754640	12756640	but both vowels versus like
12756640	12758640	E and Z
12758640	12760640	would be very different because Z is not
12760640	12762640	a very common letter and E is the most common letter
12762640	12764640	in the alphabet so
12764640	12766640	we pretty much just want to have
12766640	12768640	vectors to differentiate
12768640	12770640	these tokens to place some
12770640	12772640	semantic meaning on them
12772640	12774640	and anyways
12774640	12776640	that's a little talk about what token embedding table
12776640	12778640	is going to do when we add n.embed
12778640	12780640	and then positional embedding table
12780640	12782640	is just the same thing
12782640	12784640	but instead of each character
12784640	12786640	having its own thing
12786640	12788640	each letter
12788640	12790640	index in the input is going to have its own embedding
12790640	12792640	so I can go and add this
12792640	12794640	up here
12794640	12796640	the n.embed
12796640	12798640	and we can just make this
12798640	12800640	maybe 384
12800640	12802640	so 384 is quite huge
12802640	12804640	and it's maybe a little too big
12804640	12806640	for your PC but we'll see in a second
12806640	12808640	so
12808640	12810640	what this is going to do is it's going to have a giant vector
12810640	12812640	it's going to be like
12812640	12814640	we could say like
12814640	12816640	embedding
12816640	12818640	embedding vector
12818640	12820640	and then it would be like this
12820640	12822640	and you would have
12822640	12824640	a bunch of different attributes so like 0.1
12824640	12826640	0.2
12826640	12828640	0.8
12828640	12830640	1.1
12830640	12832640	right? except
12832640	12834640	instead of 4 this is
12834640	12836640	384 elements long
12836640	12838640	and each of these
12838640	12840640	is just going to store a tiny little attribute
12840640	12842640	about that token
12842640	12844640	so
12844640	12846640	let's say we maybe had like a
12846640	12848640	two dimensional and we were using a word
12848640	12850640	so if we had
12850640	12852640	sad versus
12852640	12854640	happy
12854640	12856640	sad might be
12856640	12858640	sad might be
12858640	12860640	0.1
12860640	12862640	and then
12862640	12864640	0.8
12864640	12866640	or 0.8
12866640	12868640	whereas happy
12868640	12870640	sad would be
12870640	12872640	maybe the positivity
12872640	12874640	of what it's saying and then 0.8 would be
12874640	12876640	is it showing some sort of emotion
12876640	12878640	which is a lot right?
12878640	12880640	it's 80% emotion
12880640	12882640	and 0.1
12882640	12884640	of maybe positive sentiment
12884640	12886640	and then if we had
12886640	12888640	0.9
12888640	12890640	would be happy because it's happy
12890640	12892640	it's very good and then 0.8
12892640	12894640	is emotional because they're sort of the same
12894640	12896640	emotional level
12896640	12898640	but yeah so this is what our embedding vectors
12898640	12900640	are pretty much describing and
12900640	12902640	all this hyperparameter
12902640	12904640	is concerned with is how long
12904640	12906640	that vector actually is
12906640	12908640	so anyways
12908640	12910640	let's continue with the GPT
12910640	12912640	language model class so the next bit I like
12912640	12914640	to talk about is how many decoder
12914640	12916640	layers we have
12916640	12918640	so in here let's just say we have
12918640	12920640	four decoder layers
12920640	12922640	so we have four of these it's going to go through this one
12922640	12924640	and then this one and then this one
12924640	12926640	then this one this is all happening
12926640	12928640	sequentially so we could
12928640	12930640	actually make a little
12930640	12932640	sequential neural network with
12932640	12934640	four decoder layers
12934640	12936640	so I'm actually going to add this in
12936640	12938640	and then a little bit of extra code which I'll explain
12938640	12940640	in a second here so this
12940640	12942640	self
12942640	12944640	dot blocks is how many
12944640	12946640	decoder blocks we have running
12946640	12948640	sequentially or layers
12948640	12950640	blocks and layers can be used interchangeably in this
12950640	12952640	context
12952640	12954640	but yeah we have an end dot sequential
12954640	12956640	and this asterisk is pretty much saying
12956640	12958640	we're going to repeat
12958640	12960640	this right here
12960640	12962640	for how many
12962640	12964640	end layer is and end layer is another hyperparameter
12964640	12966640	we're going to add
12966640	12968640	we go end underscore layer
12968640	12970640	we go equals four
12972640	12974640	so end underscore layer equals four
12974640	12976640	that means it's going to make four of these
12976640	12978640	I guess blocks
12978640	12980640	or layers sequentially
12980640	12982640	it's going to make four of them
12982640	12984640	and this little block thing
12984640	12986640	we're going to build on top of this in a second here
12986640	12988640	we're going to make an actual block
12988640	12990640	class and I'm going to explain what that does
12990640	12992640	but for now
12992640	12994640	this is going to be some temporary code
12994640	12996640	as long as you understand that this is what
12996640	12998640	this is how we create our four layers
12998640	13000640	our four decoder layers
13000640	13002640	that's all you need to know for now
13002640	13004640	I'm going to move more into this block later
13004640	13006640	as for this
13006640	13008640	self dot layer norm final
13008640	13010640	this is the final layer norm
13010640	13012640	all this is going to do
13012640	13014640	is we're just simply going to add this
13014640	13016640	to the end of our network here
13020640	13022640	just simply at the end here
13022640	13024640	and all this is going to do
13024640	13026640	is just going to help the model converge better
13026640	13028640	layer norms are super useful
13028640	13030640	and yeah
13030640	13032640	so you'll see more how that works
13032640	13034640	I'll actually remove it later on
13034640	13036640	and we'll actually
13036640	13038640	compare and see
13038640	13040640	how good it actually does
13040640	13042640	and you can totally go out of your way
13042640	13044640	to experiment
13044640	13046640	with different normalizations
13046640	13048640	and see how well the layer norm
13048640	13050640	helps the model perform
13050640	13052640	or how well the loss
13052640	13054640	sort of converges over time
13054640	13056640	when you put the layer norm in different places
13056640	13058640	so
13058640	13060640	let's go back here
13060640	13062640	and now we have this
13062640	13064640	end here
13064640	13066640	which is the language
13066640	13068640	I believe this is the language modeling
13068640	13070640	head or something
13070640	13072640	again this is what Andrey Karpathy used
13072640	13074640	I'm assuming that means language modeling head
13074640	13076640	but pretty much
13076640	13078640	all we're doing is we're just
13078640	13080640	projecting
13080640	13082640	we're doing this final
13082640	13084640	transformation here
13084640	13086640	this final little linear layer here
13086640	13088640	from all of these sequential
13088640	13090640	decoder outputs
13090640	13092640	and we're just going to transform that
13092640	13094640	to
13094640	13096640	something that the softmax can work with
13096640	13098640	so we have our layer norm afterwards
13098640	13100640	to sort of normalize help the model converge
13100640	13102640	after all these
13102640	13104640	after all this computation
13104640	13106640	we're going to feed that into a linear layer
13106640	13108640	to make it I guess
13108640	13110640	softmax
13110640	13112640	workable so the softmax can work with it
13112640	13114640	and
13114640	13116640	yeah so we're just
13116640	13118640	simply projecting it from
13118640	13120640	an embed which is the vector length that we get
13120640	13122640	from our decoder
13122640	13124640	and
13126640	13128640	and this vocab size
13128640	13130640	so the vocab size is going to
13130640	13132640	essentially give up a little
13132640	13134640	probability distribution on each token that we have
13134640	13136640	or the vocabulary
13136640	13138640	so anyways
13138640	13140640	I'm going to make this back to normal
13140640	13142640	here and we're going to just
13142640	13144640	apply this
13144640	13146640	to the forward pass
13146640	13148640	so a little thing I wanted to add on
13148640	13150640	to
13150640	13152640	this positional embedding
13152640	13154640	or rather just the idea of
13154640	13156640	embeddings versus
13156640	13158640	the fixed definite function
13158640	13160640	of the
13160640	13162640	sinusoidal functions
13162640	13164640	and the cosine functions that we used here
13164640	13166640	these are both actually
13166640	13168640	used in practice
13168640	13170640	the reason I said we're going to use embeddings
13170640	13172640	is because we just want it to be more oriented
13172640	13174640	around our data
13174640	13176640	however in practice
13176640	13178640	sinusoidal encodings are used
13178640	13180640	in base transformer models
13180640	13182640	whereas learned embeddings what we're using
13182640	13184640	are used in variants like
13184640	13186640	GBT and we are building a
13186640	13188640	GBT so we're probably
13188640	13190640	going to find out a performance from learning about embeddings
13190640	13192640	and this is just
13192640	13194640	summing up the experts do
13194640	13196640	it's a little practice that experts do
13196640	13198640	when they're building transformer models
13198640	13200640	versus variants like GBTs
13200640	13202640	so that's just a little background on
13202640	13204640	why we're using
13204640	13206640	learnable embeddings
13206640	13208640	so now let's continue
13208640	13210640	with the forward pass here
13210640	13212640	so I'm going to paste in some more code
13212640	13214640	and
13214640	13216640	let me just make sure this is
13216640	13218640	formatted properly cool
13220640	13222640	so we have this
13222640	13224640	token embedding which is our token embedding
13224640	13226640	table
13226640	13228640	we take an IDX
13228640	13230640	token embedding here
13230640	13232640	then what we do with this positional embedding table
13232640	13234640	so we have this torch.arrange
13234640	13236640	we make sure this is on the CUDA device
13236640	13238640	the GPU device
13238640	13240640	so it's in parallel
13240640	13242640	and all this is going to do
13242640	13244640	is it's going to look at how long is T
13244640	13246640	and
13246640	13248640	let's say T is our block size
13248640	13250640	so T is going to be 8
13250640	13252640	so all it's going to do is give us 8 indices
13252640	13254640	it's going to be like 0, 1, 2, 3,
13254640	13256640	4, 5, 6, 7
13256640	13258640	8 of those
13258640	13260640	and we're essentially just going to give each of those
13260640	13262640	each of those indices
13262640	13264640	a different
13266640	13268640	a different
13268640	13270640	end embedding vector
13270640	13272640	for each of those indices
13272640	13274640	just a little lookup table
13274640	13276640	and that's what that is
13276640	13278640	so all we do now
13278640	13280640	is it's actually quite simple
13280640	13282640	and this is a very efficient way to do it
13282640	13284640	is you just add these two together
13284640	13286640	broadcasting rules
13286640	13288640	which you might want to look into
13288640	13290640	I'll actually search that up right now
13290640	13292640	torch
13292640	13294640	broadcasting semantics
13296640	13298640	pie torch
13298640	13300640	broadcasting
13300640	13302640	I cannot spell
13302640	13304640	broadcasting semantics
13304640	13306640	so
13306640	13308640	these are a little bit funky
13308640	13310640	when you look at them the first time
13310640	13312640	but pretty much these are just rules
13312640	13314640	about how you can do
13314640	13316640	arithmetic operations
13316640	13318640	and just operations in general
13318640	13320640	to tensors
13320640	13322640	so tensors are like you think of matrices
13322640	13324640	where it's like a 2x2
13324640	13326640	tensors can be the same thing
13326640	13328640	but they could be like a 2x2x2
13328640	13330640	or a 2x2x2x2x2
13330640	13332640	whatever dimension you want to have
13332640	13334640	there
13334640	13336640	and pretty much it's just rules about how you can
13338640	13340640	have two of those
13340640	13342640	weirdly
13342640	13344640	shaped tensors and do things
13344640	13346640	to them
13346640	13348640	so just some rules here
13348640	13350640	I would advise you familiarize yourself with these
13350640	13352640	even play around with it if you want
13352640	13354640	just for a few minutes
13354640	13356640	and just get an idea for
13356640	13358640	which, like just try to multiply
13358640	13360640	tensors together
13360640	13362640	and see which ones throw errors and which ones don't
13362640	13364640	so it's a good idea to understand how broadcasting
13364640	13366640	rules work
13366640	13368640	obviously this term
13368640	13370640	is a little fancy and it's like
13370640	13372640	that's like a crazy advanced term
13372640	13374640	not really
13374640	13376640	it's pretty much just
13376640	13378640	some rules about how you're
13378640	13380640	multiplying these really weirdly shaped tensors
13380640	13382640	so yeah
13382640	13384640	anyways
13384640	13386640	if we go back to here
13388640	13390640	we are allowed to broadcast these
13390640	13392640	we're allowed to actually add them together
13392640	13394640	so the positional embedding and the token embedding
13394640	13396640	we get X from this
13396640	13398640	B by T by C shape
13398640	13400640	so now
13400640	13402640	what we can do
13402640	13404640	with these is we can actually feed it
13404640	13406640	into the
13406640	13408640	GPT or I guess
13408640	13410640	sort of a transformer network if you want to say that
13410640	13412640	so we have these embeddings
13412640	13414640	and positional encodings
13414640	13416640	we add these together and then we feed them
13416640	13418640	into our sequential network
13418640	13420640	so how are we doing this
13420640	13422640	well we go self dot blocks which is up here
13422640	13424640	and we essentially just feed
13424640	13426640	an X which is literally
13426640	13428640	exactly what happens here
13428640	13430640	we have our tokenized inputs
13430640	13432640	we got our embeddings and our positional encodings
13432640	13434640	through learnable embeddings we add them together
13434640	13436640	and then we feed them into the network directly
13436640	13438640	so
13438640	13440640	that's all that's happening here
13440640	13442640	and that's how we're feeding an X
13442640	13444640	which is the output of these
13444640	13446640	then after
13446640	13448640	this is like way after
13448640	13450640	we've gotten through all of these
13450640	13452640	GPT layers or blocks
13452640	13454640	we do this final layer norm
13454640	13456640	and then this linear transformation
13456640	13458640	to get it to a
13458640	13460640	softmax
13460640	13462640	to get it to essentially probabilities
13462640	13464640	that we can feed into our softmax function
13464640	13466640	and then other than that
13466640	13468640	this forward pass is exactly the same
13468640	13470640	other than this little block of code here
13470640	13472640	so if this makes sense so far
13472640	13474640	that is absolutely amazing
13474640	13476640	let's continue I'm actually going to add
13476640	13478640	a little bit of
13478640	13480640	in practice
13480640	13482640	some little
13482640	13484640	weight initializations
13484640	13486640	that we should be using
13486640	13488640	in our language model
13488640	13490640	and in module subclass
13490640	13492640	so
13492640	13494640	I'm going to go over a little bit of math here
13494640	13496640	but this is just really important for practice
13496640	13498640	and to make sure that your model
13498640	13500640	does not fail in the training process
13500640	13502640	this is very important
13502640	13504640	it's going to be a little funky
13504640	13506640	on the conceptualizing
13506640	13508640	but bring out some pen and paper
13508640	13510640	and do some math with me
13510640	13512640	we've built up some of these
13512640	13514640	initial GPT language model architecture
13514640	13516640	and before we continue building
13516640	13518640	more of it and the other functions
13518640	13520640	some of the math stuff that's going on
13520640	13522640	the parallelization that's going on in the script
13522640	13524640	I want to show you some of the math
13524640	13526640	that we're going to use to initialize the weights
13526640	13528640	of the model to help it train
13528640	13530640	and converge better
13530640	13532640	so there's this new thing
13532640	13534640	that I want to introduce called standard deviation
13534640	13536640	and this is used in intermediate level mathematics
13536640	13538640	the symbol essentially looks like this
13538640	13540640	population standard deviation
13540640	13542640	so
13542640	13544640	n
13544640	13546640	the size
13546640	13548640	so it's just going to be an array
13548640	13550640	the length of the array
13550640	13552640	and then xi
13552640	13554640	we iterate over each value
13554640	13556640	so xf position 0
13556640	13558640	xf position 1
13558640	13560640	xf position 2
13560640	13562640	and then this u here is the mean
13562640	13564640	so
13564640	13566640	we iterate over each element
13566640	13568640	we're going to
13568640	13570640	subtract it by the mean
13570640	13572640	we're going to square that and then keep adding
13572640	13574640	all these squared results together
13574640	13576640	and then once we get the sum of that
13576640	13578640	we're going to
13578640	13580640	subtract or we're going to divide
13580640	13582640	this by the number of elements there are
13582640	13584640	and then once we get this result
13584640	13586640	we're going to square root that
13586640	13588640	so this symbol here
13588640	13590640	might also look a little bit unfamiliar
13590640	13592640	and
13592640	13594640	I'll illustrate this out for you
13594640	13596640	so we go to our whiteboard
13596640	13598640	and this e
13598640	13600640	looks like
13600640	13602640	looks like that
13602640	13604640	let's just say we were to put in
13604640	13606640	x
13606640	13608640	i like that
13608640	13610640	and our array
13610640	13612640	let's just say for instance
13612640	13614640	our array
13614640	13616640	is 0.1
13616640	13618640	0.2, 0.3
13618640	13620640	so what would the result of this be
13620640	13622640	well if we look at each element
13622640	13624640	iteratively add them together
13624640	13626640	so 0.1
13626640	13628640	plus 0.2 plus 0.3
13628640	13630640	well we get 0.6 from that
13630640	13632640	so this would essentially
13632640	13634640	be equal to
13634640	13636640	0.6
13636640	13638640	that's what that equals
13638640	13640640	we just add each of these up together
13640640	13642640	or we do whatever this is iteratively
13642640	13644640	whatever this element is
13644640	13646640	we iterate over
13646640	13648640	the number of elements we have in
13648640	13650640	the arbitrary array
13650640	13652640	or
13652640	13654640	vector or list or whatever you want to call it
13654640	13656640	and then we just
13656640	13658640	sort of look at what's going on here
13658640	13660640	and we can do some basic arithmetic stuff
13660640	13662640	so
13662640	13664640	let's walk through a few examples
13664640	13666640	just to illustrate to you
13666640	13668640	what the results look like
13668640	13670640	based on the inputs here
13670640	13672640	so I'm going to go back to my whiteboard
13672640	13674640	we're going to draw a little line here
13674640	13676640	just to separate this
13676640	13678640	so
13678640	13680640	I want to calculate the standard deviation
13680640	13682640	do standard deviation
13682640	13684640	of
13686640	13688640	and then we'll just make some random array
13688640	13690640	negative
13690640	13692640	0.38
13692640	13694640	negative 0.38
13694640	13696640	0.52
13698640	13700640	and then 2.48
13700640	13702640	cool
13702640	13704640	so we have this array this is three elements
13704640	13706640	so that means n
13706640	13708640	is going to be equal to three
13708640	13710640	let me drag this over here
13710640	13712640	so n is the number of elements
13712640	13714640	so n is going to be equal to three
13714640	13716640	our mean
13716640	13718640	well
13718640	13720640	our mean is just
13720640	13722640	we add all these up together and then we average them
13722640	13724640	so our mean
13724640	13726640	is going to be equal to
13726640	13728640	let's just say
13728640	13730640	0.38
13730640	13732640	plus 0.52
13732640	13734640	plus
13734640	13736640	2.48
13736640	13738640	and then divided by three
13738640	13740640	and the answer to this
13740640	13742640	I did the math ahead of time
13742640	13744640	is literally 0.873
13744640	13746640	repeated but we're just going to put 0.87
13746640	13748640	for simplicity's sake
13748640	13750640	cool so the mean of this
13750640	13752640	is 0.87 and n is equal to three
13752640	13754640	now we can start doing
13754640	13756640	some of the other math
13756640	13758640	so
13758640	13760640	we have this
13760640	13762640	O has a cool line
13764640	13766640	and we do
13766640	13768640	square root
13768640	13770640	one over
13770640	13772640	n which is equal to three
13772640	13774640	and then we
13774640	13776640	multiply this
13776640	13778640	by sigma
13778640	13780640	that's what this symbol is
13780640	13782640	that's sigma that's the name for it
13782640	13784640	and then we go
13784640	13786640	X
13786640	13788640	I
13788640	13790640	minus
13790640	13792640	and then our mean of
13792640	13794640	0.87
13798640	13800640	apologies for the sloppy writing
13802640	13804640	and then we square that
13804640	13806640	so let me drag this out
13806640	13808640	awesome
13808640	13810640	so let's just do this
13810640	13812640	step by step here
13812640	13814640	so the first one is going to be
13814640	13816640	0.38
13816640	13818640	0.
13818640	13820640	negative
13820640	13822640	0.38
13822640	13824640	and we're going to do minus the mean here
13824640	13826640	so minus 0.87
13826640	13828640	and I'm just going to wrap all this
13828640	13830640	in brackets so that we don't miss anything
13830640	13832640	wrap it in brackets
13832640	13834640	and then just square it and see what we get after
13834640	13836640	so I'm just going to write all these out
13836640	13838640	then we can do the calculations
13838640	13840640	so next up we have 0.52
13840640	13842640	minus 0.87
13842640	13844640	we'll square that
13846640	13848640	and then next up we have
13848640	13850640	2.48
13850640	13852640	minus 0.87
13852640	13854640	and then we square that as well
13854640	13856640	so awesome
13856640	13858640	what is the result of this
13858640	13860640	the result of
13860640	13862640	negative 0.38 minus
13862640	13864640	0.87
13864640	13866640	squared is
13866640	13868640	1.57
13868640	13870640	the result of
13870640	13872640	this line
13872640	13874640	is 0.12
13874640	13876640	again these are all approximations
13876640	13878640	they're not super spot on
13878640	13880640	we're just doing this to understand
13880640	13882640	what's going on here
13882640	13884640	just to overview the function not for precision
13884640	13886640	then the next one is going to be
13886640	13888640	2.59
13888640	13890640	and you can double check all these
13890640	13892640	calculations if you'd like
13892640	13894640	I have done these preemptively so
13894640	13896640	that is that
13896640	13898640	and now from here
13898640	13900640	what we have to do is add each of these together
13900640	13902640	so
13902640	13904640	1.57
13904640	13906640	plus 0.12
13906640	13908640	plus 2.59
13908640	13910640	divided by 3
13910640	13912640	is
13916640	13918640	1.57
13918640	13920640	plus 0.12
13920640	13922640	plus 2.59
13922640	13924640	all that divided by 3
13924640	13926640	is going to be equal to 1.42
13926640	13928640	keep in mind we also have to
13928640	13930640	square root this
13930640	13932640	so the square root of that
13932640	13934640	is going to be
13934640	13936640	1.19
13936640	13938640	approximately
13938640	13940640	we'll just add
13940640	13942640	this guy ahead of it
13942640	13944640	so that's what the
13944640	13946640	standard deviation of
13946640	13948640	this array is
13948640	13950640	negative 0.38
13950640	13952640	0.52, 2.48
13952640	13954640	standard deviation is
13954640	13956640	1.19
13956640	13958640	let's do another example
13962640	13964640	so let's say
13964640	13966640	we want to do the standard deviation
13966640	13968640	of
13968640	13970640	0.48
13972640	13974640	0.5
13976640	13978640	0.50
13978640	13980640	I guess 0.52
13984640	13986640	so there's a little pattern here
13986640	13988640	just goes up by 0.02 each time
13988640	13990640	and
13990640	13992640	you're going to see why this is
13992640	13994640	vastly different than the other example
13994640	13996640	so let's walk through this
13996640	13998640	so first of all we have N
14000640	14002640	N is equal to 3
14002640	14004640	cool
14004640	14006640	what does our mean
14006640	14008640	our mean
14008640	14010640	well if you do our mean our mean is 0.5
14010640	14012640	0.48 plus this
14012640	14014640	plus that
14014640	14016640	that's going to be 0.5
14016640	14018640	and
14018640	14020640	if you're good with numbers
14020640	14022640	you'll probably already be able to do this in your head
14022640	14024640	but that's okay if not
14024640	14026640	next up
14026640	14028640	we're going to do this in the formula
14028640	14030640	so
14030640	14032640	what do these iterations look like
14032640	14034640	so
14034640	14036640	0.
14036640	14038640	let's just do these in brackets
14038640	14040640	the old way
14040640	14042640	0.5
14042640	14044640	squared
14044640	14046640	the next one is
14046640	14048640	0.5
14048640	14050640	minus 0.5
14050640	14052640	squared which we already know is 0
14054640	14056640	and this one is 0.52
14056640	14058640	minus
14058640	14060640	0.5
14060640	14062640	squared so the result of 0.48
14062640	14064640	minus 0.5 squared
14064640	14066640	and what's right equals here
14066640	14068640	is going to be
14068640	14070640	approximately 0.02
14070640	14072640	squared
14072640	14074640	so that would be 0.004
14074640	14076640	like that
14076640	14078640	so I'll make this not actually overlap
14078640	14080640	0.004
14080640	14082640	and then this one
14082640	14084640	we obviously know would be 0
14084640	14086640	because 0.5 minus 0.5
14086640	14088640	that's 0 then you square 0
14088640	14090640	still the same thing
14090640	14092640	and then this one is
14092640	14094640	0.0004 as well
14094640	14096640	so
14098640	14100640	when we add these two together
14100640	14102640	we're going to get
14102640	14104640	0.0008
14104640	14106640	just like that
14106640	14108640	and then if we divide them by 3 or whatever
14108640	14110640	n is
14110640	14112640	then we end up getting
14112640	14114640	0.00026
14114640	14116640	repeating so I'll just write
14116640	14118640	266 like that
14118640	14120640	and so
14120640	14122640	all we have to do at this point
14122640	14124640	is do the
14124640	14126640	square root of this
14126640	14128640	and
14128640	14130640	we'll do
14130640	14132640	square root of 0.00026
14132640	14134640	approximately
14134640	14136640	and
14136640	14138640	that's going to be equal to about
14138640	14140640	0.0163
14140640	14142640	so
14142640	14144640	that is our
14144640	14146640	standard deviation
14146640	14148640	of both of these arrays here
14148640	14150640	so 0.048
14150640	14152640	and then 0.52
14152640	14154640	our standard deviation is
14154640	14156640	0.0163
14156640	14158640	so very small
14158640	14160640	and then we have
14160640	14162640	negative 0.38, 0.52
14162640	14164640	and 2.48
14164640	14166640	we get a standard deviation of 1.19
14166640	14168640	so you can see that these numbers are vastly different
14168640	14170640	one is like
14170640	14172640	one is literally
14172640	14174640	100 times greater than the other
14174640	14176640	so
14176640	14178640	the reason for this is because these
14178640	14180640	numbers are super
14180640	14182640	diverse
14182640	14184640	I guess another way
14184640	14186640	you could think of them is that
14186640	14188640	they stretch out very far from the
14188640	14190640	mean
14190640	14192640	this essentially means when you're initializing
14192640	14194640	your parameters
14194640	14196640	that if you have some outliers
14196640	14198640	then your network
14198640	14200640	is going to be funky
14200640	14202640	because it's
14202640	14204640	the learning process just messed up because you have outliers
14204640	14206640	and it's not just learning the right way
14206640	14208640	it's supposed to
14208640	14210640	whereas if you had
14210640	14212640	way too small of a standard deviation
14212640	14214640	from your initial parameters
14214640	14216640	like in here but maybe even smaller
14216640	14218640	so let's say they were all
14218640	14220640	0.5
14220640	14222640	then all of your neurons
14222640	14224640	would effectively be the same
14224640	14226640	and they would all learn the same pattern
14226640	14228640	so then you would have no learning done
14228640	14230640	so one would either be
14230640	14232640	you're learning a super super unstable
14232640	14234640	and you have outliers that are
14234640	14236640	just learning
14236640	14238640	very distinct things and not really
14238640	14240640	not really
14240640	14242640	not really letting other neurons
14242640	14244640	get opportunities to learn
14244640	14246640	or rather other parameters to learn
14248640	14250640	if you have a lot of diversity
14250640	14252640	you just have outliers and then if you have
14252640	14254640	no
14254640	14256640	diversity at all then
14256640	14258640	essentially nothing is learned and your network
14258640	14260640	is useless so all we want to do
14260640	14262640	is make sure that our standard deviation
14262640	14264640	is balanced and stable
14264640	14266640	so that the training process
14266640	14268640	can learn effective things
14268640	14270640	so each neuron can learn a little bit
14270640	14272640	so you can see here
14272640	14274640	this would probably be an okay standard deviation
14274640	14276640	if these were some parameters because
14276640	14278640	they're a little bit different than each other
14278640	14280640	they're not all like super super
14280640	14282640	close to the same
14282640	14284640	and yeah
14284640	14286640	so essentially what
14286640	14288640	this looks like in
14288640	14290640	code here is the following
14290640	14292640	so you don't actually need to
14292640	14294640	memorize what this does as it's
14294640	14296640	just used in practice
14296640	14298640	by professionals
14298640	14300640	but essentially what this does
14300640	14302640	is it initializes our weights
14302640	14304640	around certain standard deviations
14304640	14306640	so here we set it to 0.02
14306640	14308640	which is pretty much the same
14308640	14310640	as what we had in here
14310640	14312640	so
14312640	14314640	point
14314640	14316640	point
14316640	14318640	this one's a little bit off in the standard deviation
14318640	14320640	set here
14320640	14322640	but essentially
14322640	14324640	we're just making sure that our weights
14324640	14326640	are initialized properly
14326640	14328640	and you don't have to memorize this at all
14328640	14330640	it's just used in practice and it's going to help our training
14330640	14332640	converge better
14332640	14334640	so as long as you understand
14334640	14336640	that we can apply some initializations
14336640	14338640	on our weights
14338640	14340640	that's all that really matters, so cool
14340640	14342640	let's move on to the next part
14342640	14344640	of our GBT architecture
14344640	14346640	so awesome, we finished this GBT language
14346640	14348640	class, everything's pretty much done here
14348640	14350640	we did our knit
14350640	14352640	we did some weight initializations
14352640	14354640	and we did our forward pass, so awesome
14354640	14356640	that's all done, now let's move on to the next
14356640	14358640	which is the
14358640	14360640	block class
14360640	14362640	so what is block?
14362640	14364640	well, if we go back to this diagram
14364640	14366640	each of these decoder blocks is a block
14366640	14368640	so
14368640	14370640	we're pretty much just going to fill in this gap here
14370640	14372640	our GBT language model has these two
14372640	14374640	where we get our tokenized inputs
14374640	14376640	and then we do some transformations
14376640	14378640	and the softmax after
14378640	14380640	and essentially we're just filling
14380640	14382640	in this gap here and then we're going to build out
14382640	14384640	and just sort of branch out until it's
14384640	14386640	completely built
14386640	14388640	so let's go ahead and build these blocks here
14388640	14390640	what does this look like?
14390640	14392640	that's what this does
14392640	14394640	so we have our knit, we have a forward pass
14394640	14396640	as per usual
14396640	14398640	and knit
14398640	14400640	and a forward pass as seen
14400640	14402640	in the GBT language model class
14402640	14404640	which is going to look like this
14404640	14406640	forward and an init
14406640	14408640	so the init
14408640	14410640	is going to just initialize some things
14410640	14412640	it's going to initialize some transformations
14412640	14414640	and some things that we're going to do in the forward pass
14414640	14416640	that's all it's doing
14416640	14418640	so what do we do first?
14418640	14420640	well we have this new head size
14420640	14422640	parameter introduced
14422640	14424640	so head size is the number of features
14424640	14426640	that each head will be capturing
14426640	14428640	in our multi-head attention
14428640	14430640	so all the heads in parallel
14430640	14432640	features are each of them capturing
14432640	14434640	so we do that by dividing
14434640	14436640	n embed by n head
14436640	14438640	so n head is the number
14438640	14440640	of heads we have
14440640	14442640	and n embed is the number of features we have
14442640	14444640	where we're capturing
14444640	14446640	so 384 features divided by 4 heads
14446640	14448640	so each head is going to be capturing
14448640	14450640	96 features
14450640	14452640	hence head size
14452640	14454640	so
14454640	14456640	next up we have self.sa
14456640	14458640	which is just short for self-attention
14458640	14460640	we do a multi-head attention
14460640	14462640	we pass in our n head
14462640	14464640	and our head size and you'll see how these
14464640	14466640	parameters fit in later
14466640	14468640	once we build up this multi-head attention
14468640	14470640	class so cool
14470640	14472640	now we have a feed forward
14472640	14474640	which is as explained
14474640	14476640	just in the diagram here
14476640	14478640	our feed forward is just this
14478640	14480640	which we're actually going to build out next
14482640	14484640	and we have two layer norms
14484640	14486640	and these are just for the
14486640	14488640	post norm
14488640	14490640	pre norm architecture that we could implement here
14490640	14492640	in this case it's going to be
14492640	14494640	post norm just because
14494640	14496640	I found that it converges better for this
14496640	14498640	for this course and the data that we're using
14498640	14500640	and just the model parameters
14500640	14502640	and what not it just works better
14502640	14504640	so
14504640	14506640	also that is the original
14506640	14508640	architecture that we use in the
14508640	14510640	attention paper
14510640	14512640	so you might have seen that they do an add a norm
14512640	14514640	rather than a norm and add
14514640	14516640	anyways
14516640	14518640	we've initialized all of these
14518640	14520640	so we have head size, self attention
14520640	14522640	feed forward and then two layer norms
14522640	14524640	so in our forward pass
14524640	14526640	we do our self attention first
14526640	14528640	let's actually go back to here
14528640	14530640	so we do our self attention
14530640	14532640	then add a norm
14532640	14534640	then a feed forward and then add a norm again
14536640	14538640	so what does this look like
14538640	14540640	self attention, add a norm
14540640	14542640	feed forward, add a norm
14542640	14544640	cool
14544640	14546640	so we're doing an add so we're going
14546640	14548640	x plus the previous
14548640	14550640	answer which is adding them together
14550640	14552640	and then we're just applying a layer norm to this
14552640	14554640	so cool
14554640	14556640	if you want to look up more into what layer norm does
14556640	14558640	and everything and why it's so useful
14558640	14560640	you can totally go out of your way to do that
14560640	14562640	but
14562640	14564640	layer norm is essentially just going to
14564640	14566640	help smoothen out our features
14566640	14568640	here
14568640	14570640	so
14570640	14572640	and honestly there's not much else to that
14572640	14574640	we just return this final value here
14574640	14576640	and that's pretty much the output of our blocks
14576640	14578640	so
14578640	14580640	next up I'm going to add
14580640	14582640	a new little code block here
14582640	14584640	which is going to be
14584640	14586640	our feed forward
14586640	14588640	so let's go ahead and do that
14588640	14590640	so feed forward, it's just going to look exactly like this
14590640	14592640	it's actually quite simple
14592640	14594640	so all we do is we make an nn dot sequential
14594640	14596640	torch dot nn
14596640	14598640	we make this a sequential network of linear
14598640	14600640	linear, relu, and then linear
14600640	14602640	so
14602640	14604640	in our linear
14604640	14606640	we have to pay attention to the shapes here
14606640	14608640	so we have n embed
14608640	14610640	and then n embed times 4
14610640	14612640	and then the relu will just
14612640	14614640	essentially
14614640	14616640	what the relu will do is it looks like this
14616640	14618640	let me illustrate this for you guys
14618640	14620640	so
14620640	14622640	essentially you have this graph here
14624640	14626640	and
14626640	14628640	let's just make this a whole plane actually
14630640	14632640	so
14632640	14634640	all of these values
14634640	14636640	that are below 0
14636640	14638640	all these values that are below 0 on the x axis
14638640	14640640	and
14640640	14642640	equal to 0 will be changed
14642640	14644640	just to 0 like that so you have all these values
14644640	14646640	that look like this
14646640	14648640	and then everything that is above 0 just stays the same
14648640	14650640	so you essentially just have this
14650640	14652640	funny looking shape it's like straight
14652640	14654640	and then diagonal that's what the relu function does
14654640	14656640	it looks at a number
14656640	14658640	sees if it's equal to or less than 0
14658640	14660640	if that's true we give that number 0
14660640	14662640	and if it's not
14662640	14664640	then we just leave the number alone
14664640	14666640	so cool very cool
14666640	14668640	non-linearity function
14668640	14670640	you can read papers on that if you like
14670640	14672640	but
14672640	14674640	essentially the shape of this
14674640	14676640	just doesn't matter all we're doing is
14676640	14678640	we're just making sure that we're just converting
14678640	14680640	some values if they're equal to
14680640	14682640	or below 0 that's all this is doing
14682640	14684640	and then
14684640	14686640	we essentially are multiplying
14686640	14688640	this we're doing this
14688640	14690640	linear transformation times this one
14690640	14692640	so we have to make sure that these inner
14692640	14694640	we have to make sure that these
14694640	14696640	inner dimensions line up so 4 times
14696640	14698640	N embed and 4 times N embed
14698640	14700640	those are equal to each other so our output shape
14700640	14702640	should be N embed
14702640	14704640	by N embed cool
14704640	14706640	so now we have our dropout
14706640	14708640	and in case you don't know what dropout is
14708640	14710640	it pretty much just
14710640	14712640	makes a certain percentage
14712640	14714640	of our neurons just
14714640	14716640	dropout and become 0
14716640	14718640	this is used to prevent overfitting
14718640	14720640	and some other little details
14720640	14722640	that I'm sure you could
14722640	14724640	you could figure out through experimenting
14724640	14726640	so
14726640	14728640	all this actually looks like in a parameter form
14728640	14730640	is just
14730640	14732640	dropout
14734640	14736640	dropout equals
14736640	14738640	we'll just say 0.2 for the same
14738640	14740640	so 0.2 means
14740640	14742640	20%
14742640	14744640	or 0.2 is going to
14744640	14746640	yeah so 0.2
14746640	14748640	in percentage form is just going to dropout
14748640	14750640	20% of our
14750640	14752640	neurons turn them to 0 to prevent overfitting
14752640	14754640	that's what that's doing
14754640	14756640	so cool
14756640	14758640	we have our feedforward network we dropout after
14758640	14760640	to prevent overfitting and then we just
14760640	14762640	call it forward on this sequential network
14762640	14764640	so cool
14764640	14766640	feedforward pretty self-explanatory
14766640	14768640	we're going to add the
14768640	14770640	multi-head attention class
14770640	14772640	so we've built all these decoder blocks
14772640	14774640	we've built
14774640	14776640	inside of the decoder blocks we've built the feedforward
14776640	14778640	and our res connections
14778640	14780640	and now
14780640	14782640	all we have to do left in this block
14782640	14784640	is the multi-head attention
14784640	14786640	so it's going to look exactly like this here
14786640	14788640	we're going to ignore the keys and queers for now
14788640	14790640	and save this for dot product attention
14790640	14792640	so we're going to
14792640	14794640	essentially just make a bunch of these
14794640	14796640	multiple
14796640	14798640	heads
14798640	14800640	and we're going to concatenate results and do a linear
14800640	14802640	transformation so what does this look like in code
14802640	14804640	well let's go ahead
14804640	14806640	and add this here
14806640	14808640	all that attention cool
14808640	14810640	so multiple heads of attention in parallel
14810640	14812640	I explained this earlier so I'm not going to jump into
14812640	14814640	too much detail on that
14814640	14816640	but we have our knit
14816640	14818640	we have our forward
14818640	14820640	and what are we doing in here
14820640	14822640	so our self dot heads is just a module list
14822640	14824640	and
14824640	14826640	module list is kind of funky I'll dive into it
14826640	14828640	a little bit later
14828640	14830640	but essentially what we're doing is we're having
14830640	14832640	a bunch of these heads
14832640	14834640	essentially in parallel
14834640	14836640	for each head
14836640	14838640	so num heads let's say our num heads is
14838640	14840640	set to
14840640	14842640	our num heads
14842640	14844640	is set to
14844640	14846640	maybe four in this
14846640	14848640	block we do multi-head attention
14848640	14850640	we do n heads and then head size
14850640	14852640	so
14852640	14854640	and heads and then head size so num heads
14854640	14856640	essentially what it is so for the number of
14856640	14858640	heads that we have which is four
14858640	14860640	we're going to pretty much make one head
14860640	14862640	running in parallel
14862640	14864640	so four heads running in parallel is what this
14864640	14866640	does here
14866640	14868640	then we have this projection
14868640	14870640	which is essentially just going to
14870640	14872640	project the
14872640	14874640	head size
14874640	14876640	times the number of
14876640	14878640	heads to an embed
14878640	14880640	and you might ask well that's
14880640	14882640	weird because
14882640	14884640	num heads times this is
14884640	14886640	literally equal to an embedding
14886640	14888640	if you go back to the math
14888640	14890640	we did here
14890640	14892640	and the purpose of this is just to be
14892640	14894640	super hackable so that if you actually do want to
14894640	14896640	change these around it won't be throwing you dimensionality
14896640	14898640	errors so that's what we're doing
14898640	14900640	just a little projection
14900640	14902640	from our
14902640	14904640	whatever these values are
14904640	14906640	up to this
14906640	14908640	constant feature
14908640	14910640	length of an embed
14910640	14912640	so then we just follow that with a drop out
14912640	14914640	dropping out 20% of the
14914640	14916640	networks neurons
14916640	14918640	now let's go into this forward here
14918640	14920640	so forward
14920640	14922640	torch dot concatenate or torch dot cat
14922640	14924640	we do four h and self dot heads
14924640	14926640	so we're going to concatenate
14926640	14928640	each head together
14928640	14930640	along the last dimension
14930640	14932640	and the last dimension in this case
14932640	14934640	is the
14934640	14936640	b batch
14936640	14938640	by time
14938640	14940640	by we just say feature dimension or channel dimension
14942640	14944640	the channel dimension here is the
14944640	14946640	last one so we're going to
14946640	14948640	concatenate along this feature dimension
14950640	14952640	and let me just help you illustrate
14952640	14954640	what exactly this looks like
14954640	14956640	so
14956640	14958640	when we concatenate along these
14958640	14960640	we have this b by t
14960640	14962640	and then we'll just say
14964640	14966640	our features are going to be
14966640	14968640	h1 like
14968640	14970640	each of our heads here
14970640	14972640	another h1
14972640	14974640	h1 h1 and these are all just features
14974640	14976640	of head one and then our next
14976640	14978640	would be h2
14978640	14980640	h2 h2 h2
14980640	14982640	and then let's just say we have
14982640	14984640	a third head go h3
14984640	14986640	h3
14986640	14988640	h3 h3
14988640	14990640	h3 like that
14990640	14992640	so we have
14992640	14994640	maybe four features per head
14994640	14996640	and there's three heads
14996640	14998640	so essentially all we're doing
14998640	15000640	when we do this concatenate
15000640	15002640	is we're just concatenating these along the last
15002640	15004640	dimension so to convert
15004640	15006640	this like ugly list format
15006640	15008640	of just each head
15008640	15010640	features sequentially in order
15010640	15012640	which is like really hard
15012640	15014640	to process we're just concatenating these
15014640	15016640	so they're easier to process
15016640	15018640	so that's what that does
15018640	15020640	and then we just follow this with a dropout
15020640	15022640	self dot projection
15022640	15024640	and then just follow that with a
15024640	15026640	dropout so cool
15026640	15028640	if that didn't totally make
15028640	15030640	sense you can totally just plug this code into chat
15030640	15032640	gbt and
15032640	15034640	get a detailed explanation on how it works
15034640	15036640	if something wasn't particularly clear
15036640	15038640	but essentially that's the premise
15038640	15040640	you have your batch by time
15040640	15042640	batch by
15042640	15044640	sequence length
15044640	15046640	or time use interchangeably
15046640	15048640	and then you have your features which are all
15048640	15050640	just in this weird list format
15050640	15052640	of each feature just listed
15052640	15054640	after another
15054640	15056640	so cool
15056640	15058640	that's what multi head attention looks like
15058640	15060640	let's go ahead and implement dot product
15060640	15062640	attention or scale dot product attention
15062640	15064640	so a little something I'd like to cover before
15064640	15066640	we go into our next scaled
15066640	15068640	dot product attention was just this linear
15068640	15070640	transformation here
15070640	15072640	and you might think well what's the point if we're just
15072640	15074640	transforming
15074640	15076640	an embed to an embed right
15076640	15078640	we're to have the match like that
15078640	15080640	and
15080640	15082640	essentially what this does is it just adds in another
15082640	15084640	learnable parameter
15084640	15086640	for us so it has a weight
15086640	15088640	and a bias if we set bias
15088640	15090640	to false
15090640	15092640	like that then it wouldn't have a bias
15092640	15094640	but it does have
15094640	15096640	a bias so another just wx
15096640	15098640	plus b if you will a weight times x
15098640	15100640	plus a bias so it just adds
15100640	15102640	more learnable parameters to help our
15102640	15104640	network
15104640	15106640	learn more about this text
15106640	15108640	so cool I'm going to go ahead and add
15108640	15110640	in this last but not least
15112640	15114640	scale dot product attention
15114640	15116640	or head class so there's going to be
15116640	15118640	a bunch of these
15118640	15120640	heads hence
15120640	15122640	class head running in parallel
15122640	15124640	and inside of here we're going to do some
15124640	15126640	scale dot product attention
15126640	15128640	so there's a lot of code in here don't get
15128640	15130640	too overwhelmed by this but I'm going to walk
15130640	15132640	through this step by step so we have our
15132640	15134640	in it we have our forward
15134640	15136640	awesome
15136640	15138640	so what do we do in our
15138640	15140640	architecture here
15140640	15142640	so we have a key
15142640	15144640	a query and a value
15144640	15146640	the keys and the queries dot
15146640	15148640	product together they get scaled
15148640	15150640	by
15150640	15152640	one over the square root of
15152640	15154640	length of a row in the keys or queries
15154640	15156640	matrix so we'll just say maybe keys
15156640	15158640	for example
15158640	15160640	the row of keys
15160640	15162640	the length of a row in keys
15162640	15164640	and then we just do our
15164640	15166640	masking to make sure the network
15166640	15168640	does not look ahead and cheat
15168640	15170640	and then we do a softmax
15170640	15172640	and a matrix
15172640	15174640	multiply to
15174640	15176640	essentially add this
15176640	15178640	value weight on top of it
15178640	15180640	so cool
15180640	15182640	we do this
15182640	15184640	keep in mind this initialization
15184640	15186640	is not actually doing any calculations
15186640	15188640	but just rather initializing
15188640	15190640	linear transformations that we will do
15190640	15192640	in the forward pass
15192640	15194640	so this self dot key
15194640	15196640	is just going to
15196640	15198640	transform and embed to head size
15198640	15200640	bias false and then
15200640	15202640	I mean the rest of these are just the same
15202640	15204640	and embed to head size because each head
15204640	15206640	will have 96 features
15206640	15208640	rather than 384
15208640	15210640	so we kind of already went over that
15210640	15212640	but that's just what that's doing
15212640	15214640	cool that's just a linear transformation
15214640	15216640	that's happening to convert
15216640	15218640	from 384 to 96 features
15218640	15220640	then we have this
15220640	15222640	self dot register buffer
15222640	15224640	well what does this do you might ask
15224640	15226640	register buffer is essentially just going
15226640	15228640	to register
15228640	15230640	this no look ahead
15230640	15232640	masking in the model state
15232640	15234640	so instead of having to re-initialize
15234640	15236640	this every single head for every
15236640	15238640	single forward and backward pass
15238640	15240640	we're just going to add this to the model
15240640	15242640	state so it's going to save us a lot of
15242640	15244640	computation that way on our training
15244640	15246640	so our training times can be reduced just because
15246640	15248640	we're registering this
15248640	15250640	yeah
15250640	15252640	so it's just going to prevent some of that
15252640	15254640	overhead computation of having to redo
15254640	15256640	this over and over again
15256640	15258640	you could still do training without
15258640	15260640	this it would just take longer
15260640	15262640	so that's what that's doing
15262640	15264640	yeah
15264640	15266640	so now we have this drop-out
15266640	15268640	of course and then in our forward pass
15268640	15270640	let's
15270640	15272640	break this down step by step here
15272640	15274640	so we have a b by t by c
15274640	15276640	so batch by time
15276640	15278640	by channel is our shape
15278640	15280640	we just unpack those numbers
15280640	15282640	and then we have a key
15282640	15284640	which is just calling this
15284640	15286640	linear transformation here on an input
15286640	15288640	x
15288640	15290640	and then a query which is also
15290640	15292640	calling the same transformation but a different
15292640	15294640	learnable transformation on x as well
15294640	15296640	so what we get
15296640	15298640	is this instead of b by t by c
15298640	15300640	we get b by t by head size
15300640	15302640	hence this transformation
15302640	15304640	from 384 to 96
15304640	15306640	so that's what that is
15306640	15308640	that's how these turn out here
15308640	15310640	so now we can actually compute
15310640	15312640	the attention scores
15312640	15314640	so what do we do
15314640	15316640	we'll just say weights is our attention
15316640	15318640	weights are
15318640	15320640	I guess you could say that
15320640	15322640	we have our queries
15322640	15324640	dot product matrix multiply
15324640	15326640	with the
15326640	15328640	keys transposed
15328640	15330640	so
15330640	15332640	what does this actually look like
15332640	15334640	and I want to help you guys
15334640	15336640	sort of understand what transposing does here
15336640	15338640	so
15338640	15340640	let's go back to here
15340640	15342640	and draw out what this is going to look like
15342640	15344640	so
15344640	15346640	essentially what transposing is going to do
15346640	15348640	is
15348640	15350640	it is just going to make sure
15350640	15352640	let me draw this out
15352640	15354640	first
15354640	15356640	so let's say you had
15356640	15358640	I don't know
15358640	15360640	maybe
15360640	15362640	a
15366640	15368640	b
15368640	15370640	c
15370640	15372640	d
15372640	15374640	and you have a
15374640	15376640	b
15376640	15378640	c
15378640	15380640	and d cool let's draw some lines
15380640	15382640	to separate these
15384640	15386640	so
15394640	15396640	awesome so essentially what this does
15396640	15398640	is the transposing
15398640	15400640	puts it into this form
15400640	15402640	so if we didn't have
15402640	15404640	transposed then this would be in a different order
15404640	15406640	it wouldn't be a b c d
15406640	15408640	in both
15408640	15410640	from like top to bottom left to right type of thing
15410640	15412640	it would be in a different order
15412640	15414640	but essentially not allow us
15414640	15416640	to multiply them the same way
15416640	15418640	so when we do a by a
15418640	15420640	a times b
15420640	15422640	it's like sort of a direct
15422640	15424640	multiply if you will
15424640	15426640	I don't know if you remember times tables at all
15426640	15428640	from elementary school
15428640	15430640	but that's pretty much what it is
15430640	15432640	we're just setting up in a times table form
15432640	15434640	and we're computing attention scores that way
15434640	15436640	so
15436640	15438640	that's what that is
15438640	15440640	that's what this transposing is doing
15440640	15442640	all this does is it just flips
15442640	15444640	the second last dimension
15444640	15446640	with the last dimension
15446640	15448640	so
15448640	15450640	in our case our second last
15450640	15452640	is t and our last is head size
15452640	15454640	so it just swaps these two
15454640	15456640	so we get b by t by head size
15456640	15458640	and then b by head size by t
15458640	15460640	we dot product these together
15460640	15462640	also keeping in mind our scaling
15462640	15464640	here
15464640	15466640	which is taking this
15466640	15468640	we're just taking this scaling
15468640	15470640	one
15470640	15472640	over the square root of length
15472640	15474640	of a row in the keys
15474640	15476640	if we look at this here
15476640	15478640	now there's little analogy
15478640	15480640	I'd like to provide for this scaling
15480640	15482640	right here
15482640	15484640	so imagine in a room
15484640	15486640	with a group of people and you're trying to understand
15486640	15488640	the overall conversation
15488640	15490640	if everyone is talking at once
15490640	15492640	it might be challenging to keep track
15492640	15494640	of what's being said
15494640	15496640	it would be more manageable if you could focus on
15496640	15498640	time right?
15498640	15500640	so that's similar to how a multi head attention
15500640	15502640	in a transformer works
15502640	15504640	so each of these heads
15504640	15506640	divides the original problem
15506640	15508640	of understanding the entire conversation
15508640	15510640	i.e. the entire input sequence
15510640	15512640	into smaller more manageable
15512640	15514640	sub problems
15514640	15516640	each of these sub problems is a head
15516640	15518640	so the head size
15518640	15520640	is the number of these sub problems
15520640	15522640	now consider what happens when each person
15522640	15524640	talks louder or quieter
15524640	15526640	if someone speaks too loudly
15526640	15528640	or
15528640	15530640	the values and the vectors are very large
15530640	15532640	it might drown out the others
15532640	15534640	this could make it difficult to understand the conversation
15534640	15536640	because you're only hearing one voice
15536640	15538640	or most of one voice
15538640	15540640	to prevent this
15540640	15542640	we want to control how loud
15542640	15544640	or how quiet each person is talking
15544640	15546640	so we can hear everyone evenly
15546640	15548640	the dot product of the
15548640	15550640	query and key vectors
15550640	15552640	in the attention mechanism
15552640	15554640	we want to check how loud each of voices
15554640	15556640	if the vectors are very large
15556640	15558640	or high dimensional
15558640	15560640	or many people are talking
15560640	15562640	the dot product can be very large
15562640	15564640	to control this volume
15564640	15566640	by scaling down the dot product
15566640	15568640	using the square root of the head size
15570640	15572640	this scaling helps ensure that no single
15572640	15574640	voice is too dominant
15574640	15576640	allowing us to hear all the voices evenly
15576640	15578640	this is why we don't scale
15578640	15580640	by the number of heads
15580640	15582640	time steps
15582640	15584640	they don't directly affect how loud each voice is
15584640	15586640	so in sum
15586640	15588640	multi head attention allows us to focus on
15588640	15590640	different parts of the conversation
15590640	15592640	and scaling helps us to hear
15592640	15594640	all parts of the conversation evenly
15594640	15596640	allowing us to understand
15596640	15598640	the overall conversation better
15598640	15600640	so hopefully that helps you understand exactly
15600640	15602640	what this scaling is doing
15602640	15604640	so now let's go into the rest of this here
15606640	15608640	so we have this scaling applied
15608640	15610640	for our head size
15610640	15612640	our head size dimension
15612640	15614640	we're doing this
15614640	15616640	dot product matrix multiplication
15616640	15618640	here we get our B by T by T
15618640	15620640	and then what is this
15620640	15622640	masked fill doing
15622640	15624640	so let me help you illustrate this here
15624640	15626640	so mask fill
15626640	15628640	is essentially
15628640	15630640	we'll say block size
15630640	15632640	is 3 here alright
15632640	15634640	so we have
15634640	15636640	initially
15636640	15638640	like a 1
15638640	15640640	a 0.6
15640640	15642640	and then like a 0.4
15642640	15644640	then our next one is
15646640	15648640	yeah we'll just say all of these are the same
15650640	15652640	so essentially
15652640	15654640	in our first one
15654640	15656640	we want to mask out everything
15656640	15658640	except for the first time step
15658640	15660640	and then when we advance one
15660640	15662640	so let's just change this here back to 0
15666640	15668640	when we go on to the next time step
15668640	15670640	we want to expose the next piece
15670640	15672640	so 0.6 I believe it was
15672640	15674640	and then a 0 again
15674640	15676640	and then when we expose the next time step after that
15676640	15678640	we want to expose all of them
15678640	15680640	so just kind of what this means is
15680640	15682640	as we
15682640	15684640	as the time step advances
15684640	15686640	in this sort of I guess vertical
15686640	15688640	part
15688640	15690640	is every time this steps 1
15690640	15692640	we just want to expose one more token
15692640	15694640	or one more
15694640	15696640	and then we'll use sort of in like a staircase format
15696640	15698640	so
15698640	15700640	essentially what this mask fill is doing
15700640	15702640	is it's making this
15702640	15704640	T by T so block size by block size
15704640	15706640	and
15706640	15708640	for each of these values we're going to set them
15708640	15710640	to negative infinity
15710640	15712640	so for each value that's 0
15712640	15714640	we're going to make that the float value negative infinity
15714640	15716640	so it's going to look like this
15716640	15718640	negative infinity
15720640	15722640	negative infinity
15724640	15726640	just like that
15726640	15728640	so essentially what happens after this
15728640	15730640	is our softmax
15730640	15732640	is going to take these values
15732640	15734640	and it's going to exponentiate normalize them
15734640	15736640	we already went over the soft
15736640	15738640	softmax previously
15738640	15740640	but
15740640	15742640	essentially what this is going to do this
15742640	15744640	this last dimension here
15744640	15746640	concatenate
15746640	15748640	or not concatenate
15748640	15750640	rather apply the softmax along the last dimension
15750640	15752640	is it's going to do that
15752640	15754640	in this sort of horizontal here
15754640	15756640	so this last
15756640	15758640	this last T
15758640	15760640	it's like blocks
15760640	15762640	it's like block size by block size
15762640	15764640	so it's like we'll say
15764640	15766640	T1 and T2
15766640	15768640	each of these being like the block size
15768640	15770640	we're just going to do it to this last T2 here
15770640	15772640	and this horizontal is T2
15772640	15774640	so
15774640	15776640	hopefully that makes sense
15776640	15778640	and essentially
15778640	15780640	what this exponentiation is going to do
15780640	15782640	is it's going to turn these values to 0
15782640	15784640	and
15784640	15786640	this one is obviously going to remain a 1
15786640	15788640	and then
15788640	15790640	it's going to turn these
15790640	15792640	into 0
15792640	15794640	and it's going to probably sharpen this 1 here
15794640	15796640	so this 1 is going to be more significant
15796640	15798640	it's going to grow more than the 0.6
15798640	15800640	because we're exponentiating
15800640	15802640	and then same here so this 1 is going to be
15802640	15804640	very, very sharp
15804640	15806640	compared to 0.6
15806640	15808640	or 0.4
15808640	15810640	that's what the softmax does
15810640	15812640	essentially the point of the softmax function
15812640	15814640	is to
15814640	15816640	make the values stand out more
15816640	15818640	it's to make the model more confident
15818640	15820640	in highlighting attention scores
15820640	15822640	so when you have one value that's like very big
15822640	15824640	but not too big, not exploding
15824640	15826640	because of our scaling, right?
15826640	15828640	we want to keep a minor scaling
15828640	15830640	but when a value is big, when a score
15830640	15832640	or attention score is very big
15832640	15834640	we want the model to put a lot of focus on that
15834640	15836640	and to say this
15836640	15838640	the entire sentence or the entire thing of tokens
15838640	15840640	and we just want it to learn the most
15840640	15842640	from that
15842640	15844640	so essentially that's what softmax is doing
15844640	15846640	instead of just a normal
15846640	15848640	normalizing mechanism
15848640	15850640	it's just doing some exponentiation
15850640	15852640	to that to make the model more confident
15852640	15854640	in its predictions
15854640	15856640	so this will help us score better
15856640	15858640	in the long run if we just
15858640	15860640	highlight what tokens
15860640	15862640	and what attention scores are more important
15862640	15864640	in the sequence
15864640	15866640	and then after this
15866640	15868640	softmax here
15868640	15870640	we just apply a simple dropout
15870640	15872640	on this way variable
15872640	15874640	this new
15874640	15876640	calculated way
15876640	15878640	scale.product.attention
15878640	15880640	masked
15880640	15882640	and then softmaxed
15882640	15884640	we apply a dropout on that
15884640	15886640	and then we perform our final
15886640	15888640	weighted aggregation
15888640	15890640	so this v
15890640	15892640	multiplied by the output of the softmax
15892640	15894640	cool
15894640	15896640	so we get this v
15896640	15898640	self.value of x so we just multiply that
15898640	15900640	a little pointer
15900640	15902640	I wanted to add
15902640	15904640	to this
15904640	15906640	module list
15906640	15908640	module list here
15908640	15910640	and then our
15910640	15912640	go
15914640	15916640	yes our sequential network here
15916640	15918640	so we have this
15918640	15920640	sequential number of blocks here
15920640	15922640	for n layers
15922640	15924640	and we have our module
15924640	15926640	list so what really is the difference here
15926640	15928640	well
15928640	15930640	module list is not the same as n and not sequential
15930640	15932640	in terms of the
15932640	15934640	asterisk usage that we see
15936640	15938640	in the language model class
15938640	15940640	module list doesn't run one layer
15940640	15942640	or head after another
15942640	15944640	but rather each is
15944640	15946640	isolated and gets its own unique perspective
15946640	15948640	sequential processing
15948640	15950640	is where one block depends on another
15950640	15952640	to synchronously complete
15952640	15954640	so that means we're waiting on one
15954640	15956640	to finish before we move on to the next
15956640	15958640	so they're not completing asynchronously
15958640	15960640	or in parallel
15960640	15962640	so the multiple heads in a transformer model
15962640	15964640	operate independently
15964640	15966640	and their computations can be processed
15966640	15968640	in parallel however this parallel
15968640	15970640	parallelism isn't due
15970640	15972640	to the module list that stores
15972640	15974640	the heads
15974640	15976640	instead
15976640	15978640	it's because of how
15978640	15980640	the computation are structured
15980640	15982640	to take advantage of the GPU's capabilities
15982640	15984640	for simultaneous
15984640	15986640	computation
15986640	15988640	and this is also how the deep learning
15988640	15990640	framework PyTorch
15990640	15992640	interfaces with the GPU
15992640	15994640	so this isn't particularly something we have to worry
15994640	15996640	about too much but
15996640	15998640	you could supposedly think
15998640	16000640	that these are sort of running in parallel
16000640	16002640	yeah
16002640	16004640	so if you want to get into hardware
16004640	16006640	then that's like your whole realm there
16006640	16008640	but this is PyTorch, this is software
16008640	16010640	not hardware at all
16010640	16012640	I don't expect you have to have any hardware
16012640	16014640	knowledge about GPU, CPU
16014640	16016640	anything like that
16016640	16018640	anyways that's just kind of a background
16018640	16020640	of what's going on there
16020640	16022640	so cool
16022640	16024640	so let's actually go over what is going on
16024640	16026640	from the ground up here
16026640	16028640	so we have this
16028640	16030640	GPT language model
16030640	16032640	we got our token embeddings, positional embeddings
16032640	16034640	we have these sequential blocks
16034640	16036640	initialize our weights
16036640	16038640	for each of these blocks
16038640	16040640	we have a
16040640	16042640	this class block
16042640	16044640	so we get a head size parameter
16044640	16046640	which is n embedded of 384
16046640	16048640	divided by n heads which is 4
16048640	16050640	so we get 96 from that
16050640	16052640	that's the number of features we're capturing
16052640	16054640	self-attention
16054640	16056640	we do a feed forward to layer norms
16056640	16058640	self-attention, layer norm
16058640	16060640	feed forward
16060640	16062640	layer norm
16062640	16064640	in the post norm architecture
16064640	16066640	then we do a feed forward
16066640	16068640	just a linear
16068640	16070640	followed by a relu followed by a linear
16070640	16072640	and then dropping that out
16072640	16074640	and then we have our multi-head attention
16074640	16076640	which just sort of structured
16076640	16078640	these attention heads
16078640	16080640	running in parallel and then concatenates the results
16080640	16082640	and then for each of these heads
16082640	16084640	we have our keys, queries
16084640	16086640	and values
16086640	16088640	we register a model state
16088640	16090640	to prevent overhead computation
16090640	16092640	excessively
16092640	16094640	then we just
16094640	16096640	do our scale dot product attention
16096640	16098640	in this line, we do our mast field
16098640	16100640	to prevent look ahead
16100640	16102640	we do our softmax to make our values
16102640	16104640	sharper and to make some of them stand out
16104640	16106640	and then
16106640	16108640	we do a drop out finally
16108640	16110640	on that and just some weighted aggregation
16110640	16112640	we do our weights
16112640	16114640	this final
16114640	16116640	weight variable
16116640	16118640	multiplied by our
16118640	16120640	weighted value
16120640	16122640	from this
16122640	16124640	initially this linear transformation
16124640	16126640	so cool, that's what's happening
16126640	16128640	step by step
16128640	16130640	in this GBT architecture
16130640	16132640	amazing
16132640	16134640	give yourself a good pat on the back
16134640	16136640	go grab some coffee, do whatever you need to do
16136640	16138640	even get some sleep
16138640	16140640	and get ready for the next section
16140640	16142640	so there's actually another hyper parameter
16142640	16144640	I forgot to add
16144640	16146640	which is n layer
16146640	16148640	and n layer
16148640	16150640	is essentially equal to 4
16150640	16152640	n layer is essentially equal to
16152640	16154640	the number
16154640	16156640	of decoder blocks
16156640	16158640	we have
16158640	16160640	so instead of n block we just say n layers
16160640	16162640	doesn't really matter what it's called
16162640	16164640	but that's what it means
16164640	16166640	and then number of heads is how many heads
16166640	16168640	we have running theoretically in parallel
16168640	16170640	and then n embed
16170640	16172640	is the number of total
16172640	16174640	dimensions we want to capture
16174640	16176640	from all the heads concatenated together
16176640	16178640	type of thing, we already went over that, so cool
16178640	16180640	hyper parameters
16180640	16182640	block size, sequence length
16182640	16184640	batch sizes, how many
16184640	16186640	of these do we want at the same time
16186640	16188640	max itters is just training
16188640	16190640	how many iterations we want to do
16190640	16192640	learning rate is
16192640	16194640	what we covered that in
16194640	16196640	actually the Desmos calculator that I showed
16196640	16198640	a little while back
16198640	16200640	just showing how
16200640	16202640	we update the model weights based on the derivative
16202640	16204640	of the loss function
16204640	16206640	and then
16206640	16208640	validators
16208640	16210640	which was just reporting the loss
16210640	16212640	and then lastly the
16212640	16214640	dropout which is dropping out
16214640	16216640	0.2 or 20%
16216640	16218640	of the total neurons
16218640	16220640	so awesome
16220640	16222640	that's pretty cool, let's go ahead and jump into
16222640	16224640	some data stuff
16224640	16226640	I'm going to pull out a paper here
16226640	16228640	so let's just make sure everything works here
16228640	16230640	and then we're actually going to download our data
16230640	16232640	so I want to try to run some iterations
16232640	16234640	and
16234640	16236640	just make sure that our, actually I made some changes
16238640	16240640	pretty much this was
16240640	16242640	weird and didn't work so I just changed
16242640	16244640	this around to
16244640	16246640	making our characters empty
16246640	16248640	opening this text file
16248640	16250640	opening it
16250640	16252640	storing it in a variable
16252640	16254640	format
16254640	16256640	and then just making our vocab
16256640	16258640	this sorted list
16258640	16260640	set of our text
16260640	16262640	and then just making the vocab
16262640	16264640	size the length of that
16264640	16266640	so let's go ahead and actually run
16266640	16268640	this through, I did change the block size
16268640	16270640	to 64 batch size 128
16270640	16272640	some other hype parameters here
16272640	16274640	so
16274640	16276640	honestly the block size and batch size will depend
16276640	16278640	on your
16278640	16280640	computational resources
16280640	16282640	so
16282640	16284640	just experiment with these
16284640	16286640	I'm just going to try these out first
16286640	16288640	just to show you guys what this looks like
16292640	16294640	okay
16294640	16296640	so it looks like we're getting idx is not defined
16296640	16298640	where could that be
16298640	16300640	okay yep
16300640	16302640	so this is
16302640	16304640	we could just change that
16304640	16306640	it's just saying idx is not defined
16306640	16308640	we're using index here idx there so that should work now
16310640	16312640	and we're getting local variable
16312640	16314640	t reference before assignment
16314640	16316640	okay so
16316640	16318640	we have some
16318640	16320640	we have t here and then we initialize
16320640	16322640	t there so let's just bring up
16324640	16326640	up to there cool
16326640	16328640	now let's try and run this
16328640	16330640	oh shape is invalid
16330640	16332640	for input size of
16332640	16334640	okay let's see what we got
16334640	16336640	it turns out we don't actually need
16336640	16338640	two token embedding tables a little bit of a selling
16338640	16340640	mistake but we don't need two of those
16340640	16342640	so I'll just delete that
16342640	16344640	and then
16344640	16346640	what I'm going to do is go ahead and run this
16346640	16348640	again let's see a new error
16348640	16350640	local variable t reference before assignment
16350640	16352640	okay so our
16352640	16354640	t is referenced here
16354640	16356640	and well how can we initialize this
16356640	16358640	what we can do is we could take this index
16358640	16360640	here of shape
16360640	16362640	b by t because it goes b by t
16362640	16364640	plus 1 etc and just keeps growing
16364640	16366640	so we could actually unpack that
16366640	16368640	so we could go b
16368640	16370640	b and
16370640	16372640	t is going to be index
16372640	16374640	that shape just unpack that
16374640	16376640	so cool
16376640	16378640	so now we're going to run this training
16378640	16380640	loop and
16380640	16382640	it looks like it's working so far
16382640	16384640	so that's amazing
16384640	16386640	super cool
16386640	16388640	step 0 train last
16388640	16390640	4.4 that's actually a pretty good training
16390640	16392640	loss overall so
16392640	16394640	we'll come back after this is done
16394640	16396640	I've set it to train
16396640	16398640	for
16398640	16400640	3,000 iterations printing every 500 iterations
16400640	16402640	so we'll just see
16402640	16404640	the loss six times over this entire
16404640	16406640	training process
16406640	16408640	or we should
16408640	16410640	I don't know why it's going to 100
16410640	16412640	eval itters
16414640	16416640	eval itters
16416640	16418640	estimate loss is
16420640	16422640	okay so we don't actually need
16422640	16424640	eval interval
16426640	16428640	we'll just make this
16428640	16430640	sure why not 100
16430640	16432640	we'll keep that
16432640	16434640	and it's just going to keep going here
16434640	16436640	we'll see our loss over time
16436640	16438640	it's going to get smaller so
16438640	16440640	I'll come back when that's done
16440640	16442640	as for the data we're going to be using
16442640	16444640	the open web text corpus
16444640	16446640	and
16446640	16448640	let's just go down here
16448640	16450640	so this is a paper called
16450640	16452640	survey
16452640	16454640	survey of large language models
16454640	16456640	so I'll just go back to open
16456640	16458640	web
16458640	16460640	text
16460640	16462640	where that is
16462640	16464640	up
16464640	16466640	it's just fine
16466640	16468640	so open web text
16468640	16470640	this is consisted of a bunch of reddit links
16470640	16472640	or just reddit upvotes
16472640	16474640	so if you go and reddit and you see
16474640	16476640	a bunch of those
16476640	16478640	posts that are highly upvoted
16478640	16480640	or downvoted
16480640	16482640	they're pretty much those
16482640	16484640	pieces of text are valuable
16484640	16486640	and they contain things that we can train them
16486640	16488640	so
16488640	16490640	pretty much web text is
16490640	16492640	just a corpus of all these upvoted links
16492640	16494640	but it's not publicly available
16494640	16496640	so somebody created an open source version
16496640	16498640	called
16498640	16500640	open web text
16500640	16502640	hence open
16502640	16504640	and it's pretty much as an open version of this
16504640	16506640	so we're going to download that
16506640	16508640	for a here like common crawl which is
16508640	16510640	really really big so like
16510640	16512640	petabyte scale data volume
16512640	16514640	you have a bunch of books
16514640	16516640	so this is a good paper to read over
16516640	16518640	it's just called
16518640	16520640	a survey
16520640	16522640	of large language models you can search this up
16522640	16524640	and it'll come up you can just download the pdf for
16524640	16526640	so this is a really nice paper
16526640	16528640	read over that if you'd like
16528640	16530640	but anyways
16530640	16532640	this is a download link for this open web text corpus
16532640	16534640	so just go to this link
16534640	16536640	I have it in the github repo
16536640	16538640	and you just go to download
16538640	16540640	and it'll bring you to this drive
16540640	16542640	so you can go in and right click this
16542640	16544640	and just hit download
16544640	16546640	it'll say 12 gigabytes exceeds
16546640	16548640	maximum file size that it can scan so it's like
16548640	16550640	this might have a virus
16550640	16552640	don't worry it doesn't have a virus this is actually
16552640	16554640	created by a researcher so
16554640	16556640	not really bad people are in charge of
16556640	16558640	creating text corpora
16558640	16560640	so go in and download anyway
16560640	16562640	I've actually already downloaded this
16562640	16564640	so
16564640	16566640	yeah I'll come back
16566640	16568640	when our training
16568640	16570640	is actually done here
16570640	16572640	so I'm actually going to stop here iteration
16572640	16574640	2000 because
16574640	16576640	we're not actually getting that much amazing progress
16576640	16578640	and the reason for this is because
16578640	16580640	our hyper parameters
16580640	16582640	so batch size and block size
16582640	16584640	I mean these are okay
16584640	16586640	but we might want to change up as our learning rate
16586640	16588640	so some combinations of learning rates that are really useful
16588640	16590640	is like
16590640	16592640	3e to the negative 3
16592640	16594640	you go 3e to the
16594640	16596640	negative 4
16596640	16598640	you go 1e to the negative 3
16598640	16600640	1e
16600640	16602640	1e to the negative 4
16602640	16604640	so these are all learning rates that I like to play around with
16604640	16606640	these are just sort of common ones
16606640	16608640	it's up to you if you want to use them or not but
16608640	16610640	what I might do actually
16610640	16612640	is just downgrade to 3e to the negative 4
16612640	16614640	and we'll retest it
16614640	16616640	as well I'm going to bump up the
16616640	16618640	the number of heads
16618640	16620640	and the number of layers
16620640	16622640	so that we can capture more
16622640	16624640	complex relationships in the text
16624640	16626640	thus having it learn more
16626640	16628640	so I'm going to change each of these
16628640	16630640	to 8
16630640	16632640	go 8
16632640	16634640	actually
16634640	16636640	kernel will go
16636640	16638640	restart
16640640	16642640	now we'll just run this from the top
16648640	16650640	and
16652640	16654640	and we'll run that
16654640	16656640	cool
16656640	16658640	so let's see
16658640	16660640	what we actually start off with and what our loss looks like over time
16670640	16672640	cool
16672640	16674640	so we got step 1 4.5 about the same as last time
16674640	16676640	it's like 0.2 off
16676640	16678640	or something so it's pretty close
16678640	16680640	let's see the next iteration here
16706640	16708640	that's wonderful
16708640	16710640	so before we were getting like 3.1 ish
16710640	16712640	or something around that range 3.15
16712640	16714640	now we're getting 2.2
16714640	16716640	so you can see that
16716640	16718640	as we change hyper parameters
16718640	16720640	we can actually see a significant change
16720640	16722640	in our loss
16722640	16724640	this is amazing
16724640	16726640	this is just to sort of prove how cool hyper
16726640	16728640	parameters are and what they do for you
16728640	16730640	so
16730640	16732640	let's start
16732640	16734640	changing around some data stuff
16734640	16736640	this right here is the Wizard of Oz text
16736640	16738640	just a simple text file
16738640	16740640	it's the size isn't
16740640	16742640	super large
16742640	16744640	so we can actually open it all into ram at once
16744640	16746640	but
16746640	16748640	if we were to use the open web text
16748640	16750640	we cannot actually read
16750640	16752640	you know 45 gigabytes of
16752640	16754640	utfa text in ram at once
16754640	16756640	just can't do that unless you have like maybe
16756640	16758640	64 or 128 gigabytes
16758640	16760640	of ram this is really just
16760640	16762640	not feasible at all
16762640	16764640	so
16764640	16766640	we're going to do some data pre-processing
16766640	16768640	here some data cleaning
16768640	16770640	and then just a way to simply load
16770640	16772640	data into the
16772640	16774640	GPT so let's go ahead and do that
16774640	16776640	so the model has actually gotten really good at predicting
16776640	16778640	the next token as you can see
16778640	16780640	the train loss here is 1.01
16780640	16782640	so let's actually
16782640	16784640	find
16784640	16786640	what the prediction accuracy of that is
16786640	16788640	so I might just go into GPT-4
16788640	16790640	here
16790640	16792640	and
16792640	16794640	just ask it
16794640	16796640	what is
16796640	16798640	the prediction
16798640	16800640	accuracy
16800640	16802640	of
16802640	16804640	loss 1.01
16806640	16808640	the loss value
16808640	16810640	comes with a loss function during the pre-process
16810640	16812640	okay so let's
16812640	16814640	see
16816640	16818640	cross entropy loss
16818640	16820640	doesn't mean the model is 99% accurate
16822640	16824640	okay
16824640	16826640	so
16826640	16828640	that pretty much means that the model is really accurate
16828640	16830640	but I want to find a value here
16830640	16832640	so
16832640	16834640	if the
16834640	16836640	we'll go to Wolfram alpha
16838640	16840640	and just we'll just guess some values here
16840640	16842640	so negative ln
16842640	16844640	of let's say
16844640	16846640	0.9
16846640	16848640	okay so probably not that
16850640	16852640	0.3
16852640	16854640	0.2
16854640	16856640	0.4
16856640	16858640	0.35
16858640	16860640	yep so the model
16860640	16862640	has about a 35% chance
16862640	16864640	of guessing the next token as of right now
16864640	16866640	so that's actually pretty good
16866640	16868640	so 1 in every 3 tokens
16868640	16870640	are spot on
16870640	16872640	so that is wonderful
16872640	16874640	this is converging even more
16874640	16876640	we're getting 0.89 so now it's getting like
16876640	16878640	every
16878640	16880640	40% are being guessed properly
16880640	16882640	our validation is not doing
16882640	16884640	amazing though
16884640	16886640	but we'll linger on that a little bit here
16886640	16888640	and you'll see sort of how this changes
16888640	16890640	as we scale our data
16890640	16892640	but
16892640	16894640	so I've installed this
16894640	16896640	webtext.tar file
16896640	16898640	tar file is interesting
16898640	16900640	so in order to actually extract these
16900640	16902640	you simply just
16902640	16904640	right click on them
16904640	16906640	you go extract to
16906640	16908640	and then it'll just make a new file here
16908640	16910640	so it'll process this
16910640	16912640	you have to make sure you have WinRAR or else this might not work
16912640	16914640	to the fullest extent
16914640	16916640	and yeah
16916640	16918640	so we'll just wait for this to finish up here
16918640	16920640	we should end up with something that looks like this
16920640	16922640	so open webtext
16922640	16924640	and inside of here
16924640	16926640	you have a bunch of xz files
16926640	16928640	cool so there's actually 20,000
16928640	16930640	of these so we're gonna have to do a lot of
16930640	16932640	there's definitely gonna be some for loops in here for sure
16932640	16934640	so
16934640	16936640	let's just handle this
16936640	16938640	step by step in this data
16938640	16940640	extract file
16940640	16942640	so first off
16942640	16944640	we're gonna need to import some python modules
16944640	16946640	we're gonna use OS for interacting with the operating system
16946640	16948640	LZMA
16948640	16950640	for handling
16950640	16952640	xz files which are a type of compressed file
16952640	16954640	like 7zip for example
16954640	16956640	and then
16956640	16958640	TQDM for displaying a progress bar
16958640	16960640	so you see a progress bar left to right
16960640	16962640	in the terminal
16962640	16964640	and that's pretty much gonna show us how quick we are
16964640	16966640	at executing the script
16966640	16968640	so next up
16968640	16970640	we're gonna define a function
16970640	16972640	called xz files in dir
16972640	16974640	it takes a directory as an input
16974640	16976640	returns a list of all of the xz file names
16976640	16978640	in that directory
16978640	16980640	it's gonna use os.listdir
16980640	16982640	to get all the file names
16982640	16984640	and os
16984640	16986640	path as file
16986640	16988640	to check if each one is a file
16988640	16990640	and not a directory or
16990640	16992640	symbolic link
16992640	16994640	if a file name ends with .xz
16994640	16996640	and it's a file
16996640	16998640	it'll be added to the list
16998640	17000640	so we just have a bunch of these files
17000640	17002640	each element
17002640	17004640	is just the title of each file in there
17004640	17006640	so that's pretty much what that does
17006640	17008640	and next up here
17008640	17010640	we'll set up some variables
17010640	17012640	folder path
17012640	17014640	it's just gonna be where our xz files are located
17014640	17016640	so I'm actually gonna change this here
17016640	17018640	because that's an incorrect file path
17018640	17020640	but
17022640	17024640	yes
17024640	17026640	just like that
17028640	17030640	you have to make sure that these
17030640	17032640	slashes are actually forward slashes
17032640	17034640	or else you might get bytecode errors
17034640	17036640	so when it actually tries to read the string
17036640	17038640	it doesn't think that
17038640	17040640	these are separated
17040640	17042640	the backward slashes do weird things
17042640	17044640	so you could either do
17044640	17046640	a one forward slash
17046640	17048640	or two backward slashes
17048640	17050640	that should work
17050640	17052640	just make sure you get forward slashes
17052640	17054640	and you should be good
17054640	17056640	so folder path is where all these files are located
17056640	17058640	all these xz files are located as you saw
17058640	17060640	output file
17060640	17062640	is the pattern for output file names
17062640	17064640	in case we want to have more than one of them
17064640	17066640	so if you want to have 200 output files
17066640	17068640	instead of one then it'll just be like
17068640	17070640	output 0, output 1, output 2 etc
17070640	17072640	and then vocab file is where we want to save
17072640	17074640	our vocabulary
17074640	17076640	keep in mind in this giant corpus
17076640	17078640	you can't push it on to ram at once
17078640	17080640	so what we're gonna do is as we're
17080640	17082640	reading these little compressed files
17082640	17084640	20,000 of them
17084640	17086640	we're gonna take all of the new characters
17086640	17088640	from them and just push them into some vocab file
17088640	17090640	containing all of the different
17090640	17092640	characters that we have
17092640	17094640	so that way we can handle this later
17094640	17096640	and just pretty much sort it into some
17096640	17098640	list containing all of our vocabulary
17098640	17100640	split files
17100640	17102640	how many files do we want to split this into
17102640	17104640	so pretty much this
17104640	17106640	it ties back to output file
17106640	17108640	and just these curly braces here
17108640	17110640	how many do we want to have
17110640	17112640	if we want to have more than one then we would
17112640	17114640	this would take effect
17114640	17116640	so cool
17116640	17118640	now we'll use our
17118640	17120640	x files in dir
17120640	17122640	to get a list of file names and store them in this variable
17122640	17124640	we'll count
17124640	17126640	the number of
17126640	17128640	total xd files
17128640	17130640	simply the length of our file names
17130640	17132640	now in here
17132640	17134640	we'll calculate the number of files
17134640	17136640	to process for each output file
17136640	17138640	if the user is requested
17138640	17140640	more than one output file
17140640	17142640	request more than one output file
17142640	17144640	this is the total number of
17144640	17146640	files divided by the number
17146640	17148640	output files rounded down
17148640	17150640	so
17150640	17152640	if the user only wants one
17152640	17154640	output file max count is the same as total files
17154640	17156640	and
17156640	17158640	that's how that works
17158640	17160640	so
17160640	17162640	next up we'll just create a
17162640	17164640	set to store a vocabulary when we
17164640	17166640	start appending these new characters into it
17166640	17168640	a set is a
17168640	17170640	collection of unique items in case you did not know
17170640	17172640	entirely what a set was
17174640	17176640	now
17176640	17178640	this is where it gets interesting
17178640	17180640	we're ready to process our
17180640	17182640	.xz files
17182640	17184640	for each output file we'll process
17184640	17186640	max count files
17186640	17188640	for each file we'll open it
17188640	17190640	read its contents
17190640	17192640	and write the contents to the current output file
17192640	17194640	and then add any unique characters to our vocabulary
17194640	17196640	set
17196640	17198640	after processing max count files
17198640	17200640	remove them from our list of files
17200640	17202640	and then finally
17208640	17210640	we'll write all of our vocabulary to this file
17210640	17212640	so
17212640	17214640	we pretty much just open
17214640	17216640	we just
17216640	17218640	write all of these characters in the vocab
17218640	17220640	to this
17220640	17222640	vocab file which is here vocab.txt
17222640	17224640	so awesome
17224640	17226640	now
17226640	17228640	honestly we could just go ahead
17228640	17230640	and run this
17230640	17232640	so let's go ahead and go in here
17232640	17234640	I'm going to go cls to clear that
17234640	17236640	we'll go python
17236640	17238640	data extract
17238640	17240640	.py
17240640	17242640	let's see this work it's magic
17244640	17246640	how many files would you like to split this into
17246640	17248640	we'll go one
17248640	17250640	then we get a progress bar
17250640	17252640	20,000 files and we'll just let that load
17252640	17254640	I'll come back to you in
17254640	17256640	about 30 minutes to check up on this
17256640	17258640	okay so there's not a little
17258640	17260640	one thing we want to consider for
17260640	17262640	and
17262640	17264640	it's actually quite important is our splits
17264640	17266640	for train and file splits
17266640	17268640	it would be really inefficient
17268640	17270640	to just get blocks and then creating
17270640	17272640	train and file splits as we go
17272640	17274640	every new batch we get
17274640	17276640	so in turn
17276640	17278640	what we might be better off doing
17278640	17280640	is just creating an
17280640	17282640	output train file and an output file file
17282640	17284640	so just two of them instead of one
17284640	17286640	train is 90% of our data
17286640	17288640	file is 10% of our data
17288640	17290640	if that makes sense
17290640	17292640	so pretty much what I did
17292640	17294640	is I got the output line for how many
17294640	17296640	files do you want
17296640	17298640	so you can see I got quite a bit of files
17298640	17300640	produced here
17300640	17302640	by not doing that correctly
17302640	17304640	so don't do that
17304640	17306640	and
17306640	17308640	yeah
17308640	17310640	essentially we're just
17310640	17312640	we're pretty much just doing that
17312640	17314640	so we're processing some training files
17314640	17316640	we're separating 90%
17316640	17318640	of the names on the left side
17318640	17320640	and then 10% of the names
17320640	17322640	we're just separating those in the two different
17322640	17324640	arrays, file names
17324640	17326640	and then we're just processing each of those
17326640	17328640	arrays based on the file names
17328640	17330640	so I took away that little bit
17330640	17332640	that was asking
17332640	17334640	how many files per
17334640	17336640	split do you want
17336640	17338640	so I took that away
17338640	17340640	and this is effectively the same code
17340640	17342640	just a little bit of tweaks
17342640	17344640	and yeah
17344640	17346640	so I'm going to go ahead and run this
17346640	17348640	data extract
17348640	17350640	cool
17350640	17352640	so we got an output train
17352640	17354640	and then after this it's going to do
17354640	17356640	the output validation set
17356640	17358640	so I'll come back after this is done
17358640	17360640	so awesome I have just downloaded
17360640	17362640	both or I've both
17362640	17364640	got both these splits
17364640	17366640	output train and val train so just to
17366640	17368640	confirm that they're
17368640	17370640	actually the right size got 38.9
17370640	17372640	and then 4.27
17372640	17374640	so if we do this divided by
17374640	17376640	9 so about 30
17376640	17378640	8.9 divided by 9
17378640	17380640	4.32 and it's
17380640	17382640	pretty close to 4.27 so
17382640	17384640	we can confirm that these are pretty much
17384640	17386640	the
17386640	17388640	length that we expect them to be
17388640	17390640	so awesome we have this vocab.txt
17390640	17392640	file wonderful so now
17392640	17394640	we have to focus on is
17394640	17396640	getting this into
17396640	17398640	our batches so when we call
17398640	17400640	our get batch function actually
17400640	17402640	cd out of this open this in
17402640	17404640	a Jupyter notebook
17406640	17408640	copy my desktop
17410640	17412640	paste it over here
17412640	17414640	and perfect
17414640	17416640	so
17416640	17418640	this open when web text folder with
17418640	17420640	these files awesome
17420640	17422640	and our GPTV
17422640	17424640	one
17424640	17426640	so
17426640	17428640	this get batch function
17428640	17430640	is going to have to
17430640	17432640	change also these
17432640	17434640	are going to have to change as well
17434640	17436640	and this one too these are probably
17436640	17438640	not going to be here
17438640	17440640	but pretty much
17440640	17442640	let's go ahead and first of all
17442640	17444640	get this vocab.txt
17444640	17446640	in so what I'm going to do
17446640	17448640	I'm just going to go
17448640	17450640	we're going to go
17450640	17452640	open web text slash
17452640	17454640	vocab.txt
17454640	17456640	cool so that's our vocab
17456640	17458640	right there text read
17458640	17460640	vocab size the length of that nice
17460640	17462640	so that's what our vocab is
17462640	17464640	and then
17464640	17466640	what we're going to do next
17466640	17468640	is change this get batch function
17468640	17470640	around
17470640	17472640	so first of all I'm going to go ahead
17472640	17474640	and get rid of this here
17474640	17476640	and then
17476640	17478640	I've actually produced
17478640	17480640	some code specifically for
17480640	17482640	this so I'm just going to go back
17482640	17484640	to my
17484640	17486640	I'm just going to find
17486640	17488640	this folder
17488640	17490640	okay so I've
17490640	17492640	actually produced some
17492640	17494640	code here
17494640	17496640	I produced this off camera
17496640	17498640	but
17498640	17500640	pretty much what this is going to do
17500640	17502640	it's going to let us call a split
17502640	17504640	okay so we have our get batch
17504640	17506640	function all of this down here is the
17506640	17508640	same as our GPTV
17508640	17510640	one file and then
17510640	17512640	this data is
17512640	17514640	just going to get a random chunk of text
17514640	17516640	with giant block of text
17516640	17518640	and the way
17518640	17520640	that we get it is actually pretty interesting
17520640	17522640	so the way that we get this text is
17522640	17524640	something called memory mapping
17524640	17526640	so memory mapping is a way
17526640	17528640	to look at disk files
17528640	17530640	or to open them and look at pieces of them
17530640	17532640	without opening the entire thing at once
17532640	17534640	so memory mapping
17534640	17536640	I'm not a hardware guy so I can't
17536640	17538640	really talk about that
17538640	17540640	memory mapping is pretty
17540640	17542640	cool and allows us to look at little
17542640	17544640	chunks at a time in very large text files
17544640	17546640	so that's essentially what we're doing here
17546640	17548640	we're passing this split
17548640	17550640	split
17550640	17552640	file name is equal to train split
17552640	17554640	this is just an example text file
17556640	17558640	if the split is equal to train then this
17558640	17560640	is our file name else
17560640	17562640	file split and then we're going to
17562640	17564640	open this file name in binary mode
17564640	17566640	this has to be in binary mode
17566640	17568640	it's also a lot more efficient in binary
17568640	17570640	mode and then
17570640	17572640	we're going to open this with a mem map
17572640	17574640	so I don't expect you to memorize all the mem map syntax
17574640	17576640	you can look at the docs if you would like
17576640	17578640	but I'm just going to explain
17578640	17580640	logically what's happening
17580640	17582640	so we're going to open this
17582640	17584640	with the mem map library
17584640	17586640	and we're going to open this as
17586640	17588640	mm so
17588640	17590640	the file size is literally
17590640	17592640	just the length of it so determining
17592640	17594640	the file size and
17594640	17596640	all we're doing from this point is we're just finding
17596640	17598640	a position so we're using the random library
17598640	17600640	and we're finding
17600640	17602640	a position between
17602640	17604640	0
17604640	17606640	and the file size
17606640	17608640	minus block size times batch size
17608640	17610640	so pretty much we have this
17610640	17612640	giant text
17612640	17614640	file we could either
17614640	17616640	what we want to do is we want to start
17616640	17618640	from 0 and go up to like
17618640	17620640	just before the end because if we
17620640	17622640	actually sample
17622640	17624640	that last piece then it's still
17624640	17626640	going to have some wiggle room to
17626640	17628640	reach further into the file
17628640	17630640	if we just made it from like
17630640	17632640	the first
17632640	17634640	the very start of the file to the very end
17634640	17636640	then it would want to do
17636640	17638640	is it would want to look past the end
17638640	17640640	because it would want to look at more tokens from that
17640640	17642640	and then we would just get errors
17642640	17644640	because you can't read more than
17644640	17646640	the file size if that makes sense
17646640	17648640	so that's why I'm just making this little threshold here
17648640	17650640	and
17650640	17652640	yeah so that's what that does
17652640	17654640	that's the starting position could be a random
17654640	17656640	number between the start and
17656640	17658640	a little bit a little margin from the end
17658640	17660640	here so
17660640	17662640	next up we have
17662640	17664640	this seek function so seek is going to
17664640	17666640	go to the start position and then
17666640	17668640	block is going to
17668640	17670640	read we're going to
17670640	17672640	go up to the start position it's going to
17672640	17674640	seek up to there that's where it's going to start
17674640	17676640	it's going to go up to it and then the read
17676640	17678640	function is going to
17678640	17680640	find a block of text that is
17680640	17682640	block size times batch size so it's
17682640	17684640	going to find a little snippet
17684640	17686640	of text in there at the starting
17686640	17688640	position and it's going to be of size
17688640	17690640	it's going to have this the same amount of
17690640	17692640	I guess bytes as
17692640	17694640	block size time times batch size
17694640	17696640	then all that minus one
17696640	17698640	just so that it fits into this start position
17698640	17700640	we don't get errors here that's why I put the minus one
17700640	17702640	but
17702640	17704640	yeah so we'll get a pretty
17704640	17706640	we'll get a pretty decent
17706640	17708640	text amount I guess you could say
17708640	17710640	it's going to be enough to work with you could
17710640	17712640	you could of course increases if you
17712640	17714640	wanted to you could do like
17714640	17716640	times eight if you wanted
17716640	17718640	times eight and then times eight up here but
17718640	17720640	we're not going to do that
17720640	17722640	based on my experience this is performed pretty well
17722640	17724640	so we're going to stick with this method here
17724640	17726640	and then
17726640	17728640	we just decode this
17728640	17730640	bit of text the reason we decode it is it's
17730640	17732640	it's because it's
17732640	17734640	we read it in binary form
17734640	17736640	so once we have this block of
17736640	17738640	text we actually have to decode this to
17738640	17740640	UFA format or UTF
17740640	17742640	format and then any like
17742640	17744640	bytecode errors we get we're just going to ignore
17744640	17746640	that this is something you learn
17746640	17748640	through practice is when you start dealing
17748640	17750640	with like really weird data or if it has
17750640	17752640	like corruptions in it you'll get errors
17752640	17754640	so all you want to do is all
17754640	17756640	this does is it pretty much says
17756640	17758640	okay we're just going to ignore this
17758640	17760640	bit of text and we're just going to sample
17760640	17762640	everything around it and not include that
17762640	17764640	part and plus since we're doing so many
17764640	17766640	iterations it won't actually interfere
17766640	17768640	that much so we should
17768640	17770640	be all right and then for this replace
17770640	17772640	go function here I was noticing
17772640	17774640	I got errors about this slash R
17774640	17776640	so all this does is it just replaces that
17776640	17778640	with an empty string and then finally
17778640	17780640	we have all this
17780640	17782640	we have all this decoded data
17782640	17784640	so all we're going to do is just encode
17784640	17786640	this into the
17786640	17788640	tokenized form so it's all in
17788640	17790640	it's all in the tokenized form
17790640	17792640	integers or torch.longs
17792640	17794640	data type
17794640	17796640	and we just that's what our data is
17796640	17798640	instead of a bunch of characters it's just a bunch
17798640	17800640	of numbers and then we
17800640	17802640	return that into our get batch
17802640	17804640	and this is what our data is
17804640	17806640	so that's pretty cool
17806640	17808640	we can get either train or a valve
17808640	17810640	split and
17810640	17812640	that's sort of what it looks like in practice
17812640	17814640	that's how we sample from
17814640	17816640	very large text files at a smaller
17816640	17818640	scale bit by bit so
17818640	17820640	let's go ahead and implement this here
17820640	17822640	and go grab this entire
17822640	17824640	thing
17824640	17826640	and pop over to here
17826640	17828640	we're just going to replace that
17828640	17830640	so
17830640	17832640	get random chunk, get batch
17832640	17834640	cool
17834640	17836640	so now we can actually go ahead and
17836640	17838640	perhaps run this
17838640	17840640	actually before we run this there's a little something we need to
17840640	17842640	add in here
17842640	17844640	so I have this
17844640	17846640	train split.txt and a valve split.txt
17846640	17848640	so I actually need to
17848640	17850640	change these
17850640	17852640	so let's go rename we'll go
17852640	17854640	train split.txt
17854640	17856640	and then
17856640	17858640	a valve split.txt
17858640	17860640	cool
17860640	17862640	and then we could just go
17862640	17864640	open web text
17864640	17866640	forward slash
17866640	17868640	and then same thing for here
17868640	17870640	cool let's go ahead
17870640	17872640	and run this now
17876640	17878640	and we're getting errors
17878640	17880640	mem map is not defined
17880640	17882640	so that's another thing we need to probably
17882640	17884640	add in then
17884640	17886640	so I'm actually just going to
17886640	17888640	stop this process from running here
17888640	17890640	we're going to go pip
17890640	17892640	install
17892640	17894640	mem map
17896640	17898640	mem map is not defined
17898640	17900640	we don't actually need to install this
17900640	17902640	by default comes with the operating system
17902640	17904640	so
17904640	17906640	what we actually need to do
17906640	17908640	is
17908640	17910640	just close this
17910640	17912640	gptv1
17912640	17914640	awesome
17914640	17916640	everything is good
17916640	17918640	nothing is broken
17918640	17920640	so what I actually need to do up here
17920640	17922640	is import this
17922640	17924640	so I need to go
17924640	17926640	import mem map
17926640	17928640	just like that
17928640	17930640	and
17930640	17932640	should be good to start running this script
17932640	17934640	name random is not defined
17934640	17936640	again another importation we have to make
17936640	17938640	import
17938640	17940640	import
17940640	17942640	random
17946640	17948640	and we should start seeing some
17948640	17950640	progress going here so once we see the first iteration
17950640	17952640	I'm going to stop it come back
17952640	17954640	at the last iteration and
17954640	17956640	then we'll start adding some little bits and pieces
17956640	17958640	onto our script here to make it better
17958640	17960640	so we're already about 600 iterations
17960640	17962640	in and you can see how the training loss
17962640	17964640	is actually done really well so far
17964640	17966640	it's gone from 10.5 drop all the way to
17966640	17968640	2.38
17968640	17970640	and
17970640	17972640	we can actually see that
17972640	17974640	we might be able to actually get a
17974640	17976640	val loss that is lower than the
17976640	17978640	train because keep in mind
17978640	17980640	in train mode
17980640	17982640	the dropout takes effect but in val
17982640	17984640	in eval mode
17984640	17986640	let me just scroll up to this here
17986640	17988640	yes
17988640	17990640	so model about eval what this does
17990640	17992640	is it turns off the dropout
17992640	17994640	so
17994640	17996640	we don't lose any of the neurons
17996640	17998640	and they're all sort of showing the same
17998640	18000640	features and giving all the information that they're supposed
18000640	18002640	to because they're all active but in train mode
18002640	18004640	20% of them are off so
18004640	18006640	once you actually see
18006640	18008640	in eval mode it does better
18008640	18010640	that means
18010640	18012640	that the network has started to
18012640	18014640	form a sense
18014640	18016640	of completeness in its learning
18016640	18018640	so it's just adjusting things a little bit
18018640	18020640	once it hits that point
18020640	18022640	and we might see this happen
18022640	18024640	momentarily but this is
18024640	18026640	really good progress so far a loss of
18026640	18028640	1.8 is amazing
18028640	18030640	so
18030640	18032640	in the meantime
18032640	18034640	I'm just going to add some little tweaks
18034640	18036640	here and there to improve this script
18036640	18038640	so I've actually stopped the iteration process
18038640	18040640	but we've gotten to 700 steps and we can already
18040640	18042640	see that val loss
18042640	18044640	is becoming a less than train loss
18044640	18046640	which is showing that the model is actually converging
18046640	18048640	and doing very well
18048640	18050640	so this architecture is amazing
18050640	18052640	we've pretty much covered
18052640	18054640	every
18054640	18056640	architectural, math, pie torch part
18056640	18058640	that this script has to offer
18058640	18060640	the only thing I want to add
18060640	18062640	actually a few things I want to add
18062640	18064640	one of them being torch.load
18064640	18066640	and torch.save
18066640	18068640	so one thing that's going to be really important
18068640	18070640	when you start to scale up
18070640	18072640	your iterations
18072640	18074640	is you don't just want to run a script
18074640	18076640	that executes a training loop
18076640	18078640	with an architecture and
18078640	18080640	that's it. You won't have some way to
18080640	18082640	store those learning parameters
18082640	18084640	so that's what torch.load and torch.save does
18086640	18088640	save some file
18088640	18090640	right and
18090640	18092640	you can pretty much
18092640	18094640	you could put it into like a serialized
18094640	18096640	format when you
18096640	18098640	save it you take your initial
18098640	18100640	architecture in our case it would actually
18100640	18102640	be the GPT language model so you would
18102640	18104640	save this because it contains
18104640	18106640	everything all these other classes
18106640	18108640	as well they're all inside of GPT
18108640	18110640	language model would save that architecture
18110640	18112640	and you essentially
18112640	18114640	serialize it into some pickled file
18114640	18116640	that would have
18116640	18118640	the file extension .pkl
18118640	18120640	so
18120640	18122640	essentially
18122640	18124640	instead of using torch we're just going to use
18124640	18126640	a library called pickle because
18126640	18128640	they're essentially the same thing
18128640	18130640	pickle is a little bit easier
18130640	18132640	to use or at least a little bit easier to understand
18132640	18134640	there's less to it
18134640	18136640	pickle will only work
18136640	18138640	on one GPU
18138640	18140640	so if you have like 8 GPUs at the same time
18140640	18142640	you're going to want to learn a little bit more
18142640	18144640	about hardware stuff and
18144640	18146640	some PyTorch docs but
18146640	18148640	pretty much
18148640	18150640	if we want to
18150640	18152640	save this after training
18152640	18154640	what we're going to do is we're going to use
18154640	18156640	a little library called pickle and this comes
18156640	18158640	pre-installed with windows
18160640	18162640	import pickle
18162640	18164640	okay so what we want to do is
18164640	18166640	implement this after the training loop
18166640	18168640	after all these parameters have been updated
18168640	18170640	and learned to the fullest extent
18170640	18172640	so after this training loop
18172640	18174640	we're simply going to open
18174640	18176640	we're going to do with open
18176640	18178640	and we could just go
18178640	18180640	model 01 like that
18180640	18182640	and then
18182640	18184640	just that .pkl is the file extension
18184640	18186640	for it
18186640	18188640	and then since we're writing to it we're going to go
18188640	18190640	write binary
18190640	18192640	F
18192640	18194640	and then in order to actually save this
18194640	18196640	we just go pickle.dump
18196640	18198640	and then we can use
18198640	18200640	model and then
18200640	18202640	just F like that
18202640	18204640	so
18204640	18206640	if I start recording this
18206640	18208640	it's going to make
18208640	18210640	if I start recording this training process
18210640	18212640	it's going to make my clip
18212640	18214640	like so
18214640	18216640	I'm going to come back to this after we've done
18216640	18218640	let's just say about
18218640	18220640	100 iterations
18220640	18222640	we're going to do 100 editors
18222640	18224640	and I'm going to come back and
18224640	18226640	show you guys
18226640	18228640	what the model file looks like
18228640	18230640	what I actually did is I changed some of the model
18230640	18232640	hyper parameters because
18232640	18234640	it was taking way too long
18234640	18236640	to perform what we wanted it to so I changed
18236640	18238640	and head to one and layer to one
18238640	18240640	and I had half batch size
18240640	18242640	all the way down from 64 to 32
18242640	18244640	so what I'm actually going to add here is just
18244640	18246640	to make sure I like to print this out at the beginning
18246640	18248640	of this
18248640	18250640	make sure that the device is CUDA
18250640	18252640	let's go back down
18252640	18254640	so it did in fact train the model
18254640	18256640	so we got all this done
18256640	18258640	and yeah
18258640	18260640	so I don't know why I did 2.54
18260640	18262640	whatever that
18262640	18264640	that was just the entire loss
18264640	18266640	so
18266640	18268640	model saved awesome
18268640	18270640	what does this actually look like here
18270640	18272640	so this model.pkl
18272640	18274640	106 megabytes isn't that wonderful
18274640	18276640	so this is our model file this is what they look like
18276640	18278640	it's just a serialized
18278640	18280640	pretty much the entire architecture
18280640	18282640	all the parameters of the model the state
18282640	18284640	everything that it contains
18284640	18286640	and we just compress that
18286640	18288640	into a little pkl file take that out
18288640	18290640	decompress it and then just use it again
18290640	18292640	with all those same parameters so
18292640	18294640	awesome
18294640	18296640	and all this really took was
18296640	18298640	we just open
18298640	18300640	as this
18300640	18302640	we do a pickle.dump
18302640	18304640	to make sure that actually save I just like to add
18304640	18306640	a little print statement there cool
18306640	18308640	so next
18308640	18310640	what I'd like to add is a little
18310640	18312640	wait for us to
18312640	18314640	instead of just doing all of our training at once
18314640	18316640	and then saving the model being able to
18316640	18318640	train multiple times
18318640	18320640	so I'm gonna go up here
18320640	18322640	to our
18322640	18324640	GPT language model here
18324640	18326640	and
18326640	18328640	let's just see
18328640	18330640	what I'm gonna do
18330640	18332640	with open
18332640	18334640	and we're gonna go
18334640	18336640	model 01
18336640	18338640	pkl
18338640	18340640	and we're gonna go read binary
18340640	18342640	so actually gonna read it we're gonna
18342640	18344640	load this into
18344640	18346640	our script here
18346640	18348640	so
18348640	18350640	we're gonna go as f
18350640	18352640	and then
18352640	18354640	I believe it's pickle.load
18356640	18358640	you just go yeah
18358640	18360640	model equals
18360640	18362640	pickle.load and then we'll just
18362640	18364640	essentially dump that
18364640	18366640	right in there
18366640	18368640	go print
18368640	18370640	loading
18370640	18372640	model
18372640	18374640	parameters
18374640	18376640	dot dot dot
18376640	18378640	and then
18378640	18380640	just put f in there
18380640	18382640	and then once it is loaded
18382640	18384640	we'll do print
18384640	18386640	loaded
18386640	18388640	successfully
18388640	18390640	cool
18390640	18392640	so I'm actually gonna try this out now
18392640	18394640	go
18394640	18396640	do that
18396640	18398640	boom
18398640	18400640	and boom
18400640	18402640	okay
18402640	18404640	so
18404640	18406640	loading model parameters loaded successfully
18406640	18408640	and we'll actually see this
18408640	18410640	start to work on its own now
18410640	18412640	so
18412640	18414640	is it going to begin or is it not going to begin
18414640	18416640	let's run that
18416640	18418640	okay perfect
18418640	18420640	so now we should take the loss
18420640	18422640	that we had before which was about
18422640	18424640	2.54 I believe
18424640	18426640	something around those, something along those lines
18426640	18428640	you can see that our training process
18428640	18430640	is greatly accelerated
18432640	18434640	so we had 100
18434640	18436640	now it's just gonna do an estimate loss
18436640	18438640	cool
18440640	18442640	and we're almost done
18445640	18447640	1.96 awesome
18447640	18449640	and the model saved
18449640	18451640	so essentially what we can do with this
18451640	18453640	is we can now
18453640	18455640	save models
18455640	18457640	and then we can load them and then iterate further
18457640	18459640	so if you wanted to
18459640	18461640	you could create a super cool
18461640	18463640	GPT language model
18463640	18465640	script here and
18465640	18467640	you could essentially give it like 10,000 or 20,000
18467640	18469640	iterations to run overnight
18469640	18471640	you'd be able to save it
18471640	18473640	and then import that into say a chat bot
18473640	18475640	if you want
18475640	18477640	so that's pretty cool and that's just kind of
18477640	18479640	a good thing
18479640	18481640	good little, it's kind of
18481640	18483640	essential for language modeling because
18483640	18485640	what's the point
18485640	18487640	in having a machine learning model if you can't
18487640	18489640	actually use it and deploy it
18489640	18491640	so you need to save for this stuff to work
18491640	18493640	alright
18493640	18495640	now let's move on to
18495640	18497640	a little something in this task manager
18497640	18499640	here which I'd like to go over
18499640	18501640	so this shared GPU memory here
18501640	18503640	and this dedicated GPU memory
18503640	18505640	so dedicated
18505640	18507640	means how much
18507640	18509640	VRAM, video RAM
18509640	18511640	does your GPU actually have
18511640	18513640	on the card
18513640	18515640	so on the card it's going to be very quick memory
18515640	18517640	because it doesn't have to
18517640	18519640	the electrons don't have to travel as quickly
18519640	18521640	that's kind of the logic of it
18521640	18523640	the electrons don't have to travel
18523640	18525640	they don't have to travel as far
18525640	18527640	because
18527640	18529640	the little RAM chip is right there
18529640	18531640	so
18531640	18533640	dedicated GPU memory is a lot faster
18533640	18535640	shared GPU memory
18535640	18537640	is essentially if this gets overloaded
18537640	18539640	it'll use some of the RAM on your
18539640	18541640	computer instead
18541640	18543640	so this will typically be about half of your
18543640	18545640	computer's RAM
18545640	18547640	I have 32 gigabytes of RAM on my computer
18547640	18549640	so 16.0 makes sense
18549640	18551640	half 32
18551640	18553640	and yeah
18553640	18555640	so you want to make sure you're only using dedicated
18555640	18557640	GPU memory
18557640	18559640	having your shared GPU memory go up
18559640	18561640	is not usually a good thing
18561640	18563640	a little bit is fine
18563640	18565640	but
18565640	18567640	dedicated GPU memory is the fastest
18567640	18569640	and you want everything to stick on there
18569640	18571640	just try to make sure all of your parameters
18571640	18573640	sort of fit around this
18573640	18575640	whatever your max capacity is
18575640	18577640	maybe it's 4, maybe it's 8
18577640	18579640	maybe it's 48
18579640	18581640	who knows
18581640	18583640	and a good way to figure out
18583640	18585640	what you can use on your GPU
18585640	18587640	without it getting memory errors
18587640	18589640	or using shared memory
18589640	18591640	is to actually play around
18591640	18593640	with
18593640	18595640	these parameters up here
18595640	18597640	so
18597640	18599640	block size and batch size
18599640	18601640	actually let me switch those around
18601640	18603640	these are not supposed to be in that order
18603640	18605640	but
18605640	18607640	all good
18607640	18609640	we'll make our batch size
18609640	18611640	64
18611640	18613640	that's 128
18613640	18615640	okay
18615640	18617640	so
18617640	18619640	batch size and block size
18619640	18621640	are very big contributors to how much memory you're going to use
18621640	18623640	learning rate is not
18623640	18625640	max iterations is not
18625640	18627640	evaluators is not
18627640	18629640	but these three will
18629640	18631640	the amount of features that you store
18631640	18633640	the amount of heads you have running in parallel
18633640	18635640	and then also
18635640	18637640	layers so
18637640	18639640	some of these will not
18639640	18641640	affect you as much because they're more
18641640	18643640	sort of restrained to computation
18643640	18645640	how quickly you can do operations if something is sequential
18647640	18649640	so N layer won't strain you
18649640	18651640	as much as something like batch and block size
18651640	18653640	but
18653640	18655640	those are just good little things to
18655640	18657640	sort of tweak and play around with
18657640	18659640	so I found the optimal
18659640	18661640	sort of set of
18661640	18663640	hyper parameters for my PC
18663640	18665640	that happens to be
18665640	18667640	8, 8, 3, 8, 4
18667640	18669640	learning rates is the same
18669640	18671640	and then 64, 128 for this
18671640	18673640	so that happened to be the optimal
18673640	18675640	hyper parameters for my computer
18675640	18677640	it'll probably be different for yours
18677640	18679640	if you don't have 8 gigabytes of RAM on your GPU
18681640	18683640	so anyways
18683640	18685640	that's a little something you have to pay attention to
18685640	18687640	to make sure you don't run out of errors
18687640	18689640	and a technique you can use
18689640	18691640	which I'm not actually going to show you in this course
18691640	18693640	but it's quite useful is something called auto tuning
18693640	18695640	and what auto tuning does
18695640	18697640	is it pretty much runs
18697640	18699640	a bunch of these
18699640	18701640	a bunch of models with different
18701640	18703640	sets of hyper parameters
18703640	18705640	so to run like batch size 64
18705640	18707640	batch size 32, batch size 16
18707640	18709640	batch size maybe 256
18709640	18711640	we'll be like okay which ones are throwing errors and which ones aren't
18711640	18713640	so what it'll do
18713640	18715640	if you properly
18715640	18717640	if you properly set up an auto tuning script
18717640	18719640	is
18719640	18721640	you will be able to find
18721640	18723640	the most optimal
18723640	18725640	set of parameters for your computer
18725640	18727640	most optimal set of hyper parameters
18727640	18729640	that is possible
18729640	18731640	so auto tuning is cool
18731640	18733640	you can definitely look more into that
18733640	18735640	there's tons of research on it
18735640	18737640	and yeah so
18737640	18739640	auto tuning is cool let's dig into the next part
18739640	18741640	the next little trick we use in practice
18741640	18743640	especially by machine learning engineers
18743640	18745640	it's a little something called arguments
18745640	18747640	so you pass an argument into
18747640	18749640	not necessarily a function but into the command line
18749640	18751640	so this is what it'll look like
18751640	18753640	this is just a basic example
18753640	18755640	of what arg parsing will look like
18755640	18757640	so just go
18757640	18759640	python, arg parsing
18759640	18761640	because that's a script's name
18761640	18763640	I go dash
18763640	18765640	llms because that's what it says
18765640	18767640	right here this is what the argument is
18767640	18769640	and then we can just pass in a string
18769640	18771640	say hello
18771640	18773640	the provided
18773640	18775640	whatever is hello
18775640	18777640	cool you can add little arguments to this
18777640	18779640	and I'm even going to change this around
18779640	18781640	I could say
18785640	18787640	batch size
18787640	18789640	and then
18789640	18791640	let's go like that
18791640	18793640	batch
18793640	18795640	batch size
18799640	18801640	please
18801640	18803640	provide
18803640	18805640	a batch size
18807640	18809640	I can do the same thing again
18811640	18813640	and see it says
18813640	18815640	following arguments required are batch size
18815640	18817640	so that obviously didn't work
18817640	18819640	and if we actually tried the correct way
18819640	18821640	our parsing.py then we go
18821640	18823640	dash, batch size
18823640	18825640	we can make it 32
18825640	18827640	oops
18831640	18833640	that's because it's not a string
18833640	18835640	so
18835640	18837640	what we need to actually do
18837640	18839640	is it's bs somewhere
18839640	18841640	okay
18841640	18843640	so
18843640	18845640	args
18845640	18847640	parse args
18847640	18849640	so we need to change this
18849640	18851640	to bs like that
18851640	18853640	let me go batch size
18853640	18855640	batch size is 32
18855640	18857640	okay
18857640	18859640	so even I'm a little bit new to arguments as well
18859640	18861640	but
18861640	18863640	this is something that comes in very handy
18863640	18865640	when you're trying to know each time
18865640	18867640	you're trying to change some parameters
18867640	18869640	if you add
18869640	18871640	new gpu or whatever and you're like oh I want to double my batch size
18871640	18873640	it's like sure you can easily do that
18873640	18875640	so a lot of the times
18875640	18877640	it won't just have one but you'll have like
18877640	18879640	many meaning like maybe a dozen
18879640	18881640	or so of these
18881640	18883640	of these little arguments
18883640	18885640	so that is what this looks like
18885640	18887640	and
18887640	18889640	we're going to go ahead and implement this
18889640	18891640	into our little script here
18891640	18893640	so
18893640	18895640	I'm just going to
18895640	18897640	pop over to gpt1
18897640	18899640	I'm going to pull this up on my
18899640	18901640	second monitor here
18903640	18905640	and
18905640	18907640	in terms of these
18907640	18909640	I'm just going to start off
18909640	18911640	making a
18911640	18913640	importation
18913640	18915640	arg
18915640	18917640	arg parser
18917640	18919640	or arg parse rather
18919640	18921640	that's what it's called
18921640	18923640	and then we go
18923640	18925640	parser is equal to
18925640	18927640	I'll just
18927640	18929640	copy and paste this entire thing
18929640	18931640	and why not
18931640	18933640	cool
18935640	18937640	okay
18937640	18939640	so
18939640	18941640	we get a batch size
18941640	18943640	or something
18943640	18945640	and then
18945640	18947640	we'll add in the second part here
18947640	18949640	so
18955640	18957640	args parse the arguments
18957640	18959640	here
18961640	18963640	and the little scope
18963640	18965640	of
18965640	18967640	batch size like that
18967640	18969640	our batch size is equal to
18969640	18971640	whatever that was
18971640	18973640	and we'll just go args
18973640	18975640	dot
18975640	18977640	args dot batch size so cool
18979640	18981640	we're going to run this
18981640	18983640	and
18983640	18985640	not defined
18985640	18987640	so I got a little not defined thing here
18987640	18989640	and pretty much
18989640	18991640	all I missed was that
18991640	18993640	we're doing this so essentially
18993640	18995640	this
18995640	18997640	should be equal to this right here
18997640	18999640	so I'm just going to go ahead and copy that
18999640	19001640	and
19003640	19005640	boot parse args
19005640	19007640	except
19007640	19009640	we don't have a parse args function
19009640	19011640	so
19011640	19013640	what do we need to do instead
19013640	19015640	well it
19015640	19017640	actually that might just work on it so let's try it out
19021640	19023640	okay so it looks like
19023640	19025640	it's actually expecting some input here
19025640	19027640	in code so
19027640	19029640	that's probably working
19029640	19031640	and if we
19031640	19033640	ported this into a script
19033640	19035640	then it would simply ask us for some input
19035640	19037640	so I believe we're doing this correctly
19037640	19039640	let's go ahead
19039640	19041640	and actually switch over
19041640	19043640	and pour all of this into some code
19043640	19045640	so I'm going to make
19045640	19047640	a training file
19047640	19049640	and a chat file
19049640	19051640	the training file is going to be all of our parameters
19051640	19053640	whatever all of our architecture
19053640	19055640	and then the actual training loop itself
19055640	19057640	we're going to have some arguments in there
19057640	19059640	and then the chat bot is going to be
19059640	19061640	pretty much just a question-answer
19061640	19063640	thing that just reproduces text
19063640	19065640	so it'll just be like prompt, completion
19065640	19067640	type of thing and
19067640	19069640	yeah so let's go ahead and implement that here
19069640	19071640	so in our
19071640	19073640	GPT course
19073640	19075640	here I'm going to go
19075640	19077640	training.py
19077640	19079640	and we're going to go
19079640	19081640	chatbot.py
19081640	19083640	just like that
19083640	19085640	so in training
19085640	19087640	let's go ahead and drag everything in here
19089640	19091640	I'm just going to
19091640	19093640	move this over to the second screen
19093640	19095640	and just copy and paste
19095640	19097640	everything in order here
19097640	19099640	so next up we have our
19099640	19101640	characters
19101640	19103640	and then we have our
19103640	19105640	tokenizer
19105640	19107640	and then our
19107640	19109640	getRandomChunk and getBatches
19113640	19115640	suite
19115640	19117640	our estimateLoss function
19121640	19123640	and then this giant piece
19123640	19125640	of code
19125640	19127640	containing
19127640	19129640	most of the architecture we built up
19131640	19133640	we're just going to add that in there
19133640	19135640	we're not getting any warnings
19137640	19139640	and then the training loop
19141640	19143640	and the optimizer
19143640	19145640	awesome
19145640	19147640	then after this
19147640	19149640	we would simply have this context
19149640	19151640	but the point of this is that we want to have this in our
19151640	19153640	chatbot script
19153640	19155640	so what I'm going to do
19155640	19157640	is in this training.py
19157640	19159640	I'm going to keep
19159640	19161640	all of these the same I'm going to keep this entire thing
19161640	19163640	the same
19163640	19165640	get rid of this little block of code
19165640	19167640	and we're going to go into
19167640	19169640	the chatbot
19169640	19171640	here so loadingMile
19171640	19173640	parameters good we want to load some in
19173640	19175640	train some more and then dump it
19175640	19177640	chatbot is not going to dump anything
19177640	19179640	it's just going to save so I'm going to take
19179640	19181640	all of our training here
19183640	19185640	and instead of dumping
19185640	19187640	take that away we'll also take
19187640	19189640	away the training
19189640	19191640	loop as well
19195640	19197640	I don't believe we have anything
19197640	19199640	else to actually bring in
19199640	19201640	we don't need our getBatch
19201640	19203640	we do not need our getRandomChunks
19203640	19205640	so awesome
19205640	19207640	we're just importing these parameters
19207640	19209640	by default like that
19209640	19211640	awesome
19211640	19213640	so from this point
19213640	19215640	we have imported
19215640	19217640	we've imported our model
19217640	19219640	cool so let's go ahead
19219640	19221640	and port in our little
19221640	19223640	chatbot here
19223640	19225640	this little end piece
19225640	19227640	which is going to allow us to
19227640	19229640	essentially chat with the model
19229640	19231640	this is what it looks like a little wild loop
19231640	19233640	we have a prompt we just input
19233640	19235640	something
19235640	19237640	prompt next line that should be fairly self explanatory
19237640	19239640	and we have this tensor
19239640	19241640	we're going to encode this prompt into a bunch
19241640	19243640	of integers or torch.long data types
19243640	19245640	on the GPU
19245640	19247640	devices CUDA
19247640	19249640	and then after
19249640	19251640	after we've actually generated these
19251640	19253640	so model.generate
19253640	19255640	we're going to unsqueeze these
19255640	19257640	remember it's a torch.tensor
19257640	19259640	so it's going to be in the matrices form
19259640	19261640	so it's going to look like this
19261640	19263640	it's going to look like this or whatever
19263640	19265640	that's essentially what the shape is
19265640	19267640	so all we're doing when we unsqueeze it
19267640	19269640	is we're just taking away this wrapping
19269640	19271640	around it
19271640	19273640	so awesome
19273640	19275640	we're just going to do some
19275640	19277640	tokens for example 150 here
19277640	19279640	and then to a list format
19279640	19281640	and then we can just print these out
19281640	19283640	as January characters
19283640	19285640	awesome so we're just going to ask this prompt
19285640	19287640	and then do some compute give us a completion
19287640	19289640	so on and so forth
19289640	19291640	so that's what this is doing here
19291640	19293640	and another thing I wanted to point out
19293640	19295640	is actually when we load these
19295640	19297640	parameters in
19297640	19299640	at least on training
19299640	19301640	it's going to initially give us errors
19301640	19303640	from we're going to get errors from that
19303640	19305640	because the model will just not be
19305640	19307640	anything and we won't be able to import stuff
19307640	19309640	so that's going to give you errors first of all
19309640	19311640	another thing you want to pay attention to
19311640	19313640	is to make sure that when you've actually trained
19313640	19315640	this initial model that it matches
19315640	19317640	all of the architectural
19317640	19319640	stuff and the hyper parameters
19319640	19321640	that you used
19321640	19323640	that when you're using to load up again
19323640	19325640	so
19325640	19327640	when you're running your forward pass and whatnot
19327640	19329640	you just want to make sure that this architecture
19329640	19331640	sort of lines up with it
19331640	19333640	just so that you don't get any architectural errors
19333640	19335640	those can be really confusing to debug
19335640	19337640	so yeah
19337640	19339640	and the way we can do this is actually just
19339640	19341640	commenting it out here
19341640	19343640	awesome, we're able to save load models
19343640	19345640	and
19345640	19347640	we're able to use a little loop
19347640	19349640	to create a sort of
19349640	19351640	chat-up that's not really helpful
19351640	19353640	because we haven't trained it
19353640	19355640	an insane amount on
19355640	19357640	data that actually is useful
19357640	19359640	so another little detail that's very important
19359640	19361640	is to actually
19361640	19363640	make sure that you have nn-module in all
19363640	19365640	of these classes and subclasses
19365640	19367640	nn.module basically works
19367640	19369640	as a tracker for all of your
19369640	19371640	parameters it makes
19371640	19373640	make sure that all of your
19373640	19375640	nn extensions run correctly
19375640	19377640	and just overall a cornerstone
19377640	19379640	for PyTorch like you need it
19379640	19381640	so make sure you have nn-module in all of these classes
19381640	19383640	I know that
19383640	19385640	block sort of comes out of GPT
19385640	19387640	language model and so on and so forth
19387640	19389640	but just all of these
19389640	19391640	classes with nn
19391640	19393640	or any learnable parameters
19393640	19395640	you will need it in it's overall just
19395640	19397640	a good practice to have nn-module in all
19397640	19399640	of your classes overall
19399640	19401640	just to sort of avoid those errors
19401640	19403640	so cool
19403640	19405640	I didn't explicitly go over that
19405640	19407640	at the beginning but that's just a heads up
19407640	19409640	you always want to make sure nn-module is inside of these
19409640	19411640	so cool
19411640	19413640	now
19413640	19415640	something I'd like to highlight
19415640	19417640	is a little error that we get
19417640	19419640	we try to generate when we have max new
19419640	19421640	tokens above block size so let me show you
19421640	19423640	that right now
19423640	19425640	you just go python, chat bot
19425640	19427640	and then batch size 32
19427640	19429640	so we could say
19429640	19431640	we could say hello
19431640	19433640	for example
19435640	19437640	okay so it's going to give us
19437640	19439640	some errors here and what exactly
19439640	19441640	does this error mean
19441640	19443640	well when we try to
19443640	19445640	generate 150 new
19445640	19447640	tokens what it's doing
19447640	19449640	is it's taking the previous
19449640	19451640	you know
19451640	19453640	H-E-L-L-O
19453640	19455640	exclamation mark 6 tokens
19455640	19457640	and it's pretty much adding up 150
19457640	19459640	on top of that so we have
19459640	19461640	156 tokens
19461640	19463640	that we're now trying to fit inside of block size
19463640	19465640	which in our case is
19465640	19467640	128
19467640	19469640	so of course
19469640	19471640	156 does not fit
19471640	19473640	into 128 and that's
19473640	19475640	why we get some errors here
19475640	19477640	so
19477640	19479640	all we have to do is make sure
19479640	19481640	that
19481640	19483640	we essentially
19483640	19485640	what we could do is make sure that max new tokens
19485640	19487640	is small enough and then be sort of
19487640	19489640	paying attention when we make prompts
19489640	19491640	or
19491640	19493640	we could actually make a little
19493640	19495640	cropping
19495640	19497640	cropping tool here so what this will do
19497640	19499640	is it will pretty much crop
19499640	19501640	through the last block size tokens
19501640	19503640	and
19503640	19505640	this is super useful because it
19505640	19507640	pretty much doesn't make us have to pay
19507640	19509640	attention to max new tokens all the time
19509640	19511640	and it just essentially
19511640	19513640	crops it around that 128 limit
19513640	19515640	so
19515640	19517640	I'm going to go ahead and replace index here
19517640	19519640	with index con or index condition
19519640	19521640	and
19521640	19523640	we go ahead and run this again
19527640	19529640	so I could say hello
19531640	19533640	and we get a successful
19533640	19535640	completion awesome
19535640	19537640	we can keep asking new prompts like this
19537640	19539640	right
19543640	19545640	and awesome so
19545640	19547640	yeah we're not really getting any of these
19547640	19549640	dimensionality like
19549640	19551640	architecture fitting type errors if you want to call them
19551640	19553640	if you want to make it super fancy that way
19553640	19555640	but yeah
19555640	19557640	not really that much else to do
19557640	19559640	yeah there's a few points I want to go over
19559640	19561640	including fine tuning
19561640	19563640	so I'm going to go over a little
19563640	19565640	illustrative example as to what
19565640	19567640	fine tuning actually looks like in practice
19567640	19569640	so in pre-training
19569640	19571640	which is what this course is based off of
19571640	19573640	in pre-training you have this
19573640	19575640	giant text corpus right you have this
19575640	19577640	giant corpus here
19579640	19581640	some text in it
19581640	19583640	and essentially
19583640	19585640	what you do is you take out little snippets
19585640	19587640	these are called
19587640	19589640	blocks or batches
19589640	19591640	or chunks you could say you take out little batches
19591640	19593640	of these you sample
19593640	19595640	random little blocks and you take multiple batches
19595640	19597640	of them and
19597640	19599640	you essentially have this
19599640	19601640	let's just say
19601640	19603640	H E L L O
19603640	19605640	and maybe the next
19605640	19607640	predict maybe the outputs
19607640	19609640	or the targets rather
19609640	19611640	or
19611640	19613640	the L L O
19613640	19615640	exclamation mark
19615640	19617640	so it's just shifted over by one
19617640	19619640	and so given this
19619640	19621640	sequence of characters
19621640	19623640	you want to predict this which is just
19623640	19625640	the input shifted by one
19625640	19627640	that's what pre-training is
19627640	19629640	and keep in mind that these are the same size
19629640	19631640	this is one, two,
19631640	19633640	three, four, and five
19633640	19635640	same thing here these are both
19635640	19637640	five characters long
19637640	19639640	fine tuning however is not completely the same
19639640	19641640	so I could have
19641640	19643640	hello
19643640	19645640	and then maybe like a question mark
19645640	19647640	and it would respond
19647640	19649640	you know
19651640	19653640	the model might respond
19653640	19655640	L R U
19655640	19657640	maybe that's just a
19657640	19659640	a response that it gives us
19659640	19661640	we can obviously see that hello does not have the same amount of characters
19661640	19663640	with the same amount of indices
19663640	19665640	as how are you
19665640	19667640	so
19667640	19669640	this is essentially the difference between
19669640	19671640	fine tuning and pre-training
19671640	19673640	with fine tuning you just have to add a little bit of
19673640	19675640	different things in your generate function
19675640	19677640	to compensate for not having
19677640	19679640	the same
19679640	19681640	amount of indices in your inputs
19681640	19683640	and targets and rather just
19683640	19685640	generate until you receive an end token
19685640	19687640	so
19687640	19689640	what they don't explicitly say here is at the
19689640	19691640	end of this question
19691640	19693640	there's actually a little end token which we usually
19693640	19695640	do
19695640	19697640	looks like this
19697640	19699640	like that
19699640	19701640	or
19701640	19703640	like this
19703640	19705640	these are end tokens and then you typically
19705640	19707640	have the same for start tokens like an s
19707640	19709640	or
19709640	19711640	start
19711640	19713640	like that, pretty simple
19713640	19715640	and essentially you would just append them
19717640	19719640	and
19719640	19721640	a start token
19721640	19723640	the start token doesn't matter as much
19723640	19725640	as we essentially just are looking at
19725640	19727640	what this does and then
19727640	19729640	we start generating the start doesn't really
19729640	19731640	matter because
19731640	19733640	we don't really need to know when to start generating
19733640	19735640	it just happens but the end token is
19735640	19737640	important because we don't want to just generate
19737640	19739640	an infinite number of tokens
19739640	19741640	because these aren't the same size
19741640	19743640	it could theoretically generate a really
19743640	19745640	really long completion
19745640	19747640	so all we want to make sure
19747640	19749640	is that it's not generating an infinite amount of tokens
19749640	19751640	consuming an infinite amount of computation
19751640	19753640	and just to prevent that loop
19753640	19755640	so that's why we append this end token
19755640	19757640	to the end here
19759640	19761640	we have this little end bit
19761640	19763640	and
19763640	19765640	essentially once this end token is sampled
19765640	19767640	you would end the generation
19767640	19769640	simple as that
19769640	19771640	and we don't actually
19771640	19773640	sample from the token itself
19773640	19775640	but rather the actual
19775640	19777640	the
19777640	19779640	I guess you could say index
19779640	19781640	or the miracle value
19781640	19783640	the encoded version of end
19783640	19785640	which
19785640	19787640	is usually just going to be the length of your vocab
19787640	19789640	size
19789640	19791640	plus one
19791640	19793640	so if your vocab size in our case
19793640	19795640	is like maybe 32,000
19795640	19797640	your end token would be at index
19797640	19799640	32,001
19799640	19801640	so that way when you sample
19801640	19803640	when you sample an end token
19803640	19805640	when you sample that
19805640	19807640	32,001 token
19809640	19811640	you actually just end the sequence
19811640	19813640	and of course when you train
19813640	19815640	your model you're always
19815640	19817640	appending this end token to the end
19817640	19819640	so you get your initial inputs
19819640	19821640	and then inside of either your
19821640	19823640	training data
19823640	19825640	or when you actually are processing it
19825640	19827640	and feeding it into that transformer
19827640	19829640	you have some sort of function that's just appending
19829640	19831640	that little
19831640	19833640	32,001 token index
19833640	19835640	to it
19835640	19837640	so that's pretty much what fine tuning is
19837640	19839640	it comes up fine tuning
19839640	19841640	and the whole process of creating
19841640	19843640	these giant language models
19843640	19845640	is to of course help people
19845640	19847640	and there's no better way to do that
19847640	19849640	than to
19849640	19851640	literally have all the information
19851640	19853640	that humans have ever known meaning like common crawl
19853640	19855640	open web text or Wikipedia
19855640	19857640	and even research papers
19857640	19859640	pre-training on all of that
19859640	19861640	so just doing again the same size
19861640	19863640	and then shift over for targets
19863640	19865640	and then after you've iterated on that
19865640	19867640	many many times you switch over to fine tuning
19867640	19869640	where you have these
19869640	19871640	specifically picked out
19871640	19873640	prompt and completion pairs
19873640	19875640	and you just train on those for a really long time
19875640	19877640	until you are satisfied
19877640	19879640	with your result
19879640	19881640	and yeah that's what language modeling is
19881640	19883640	there are a few key pointers I want to leave you with
19883640	19885640	before you head on your way to
19885640	19887640	research and development and machine learning
19887640	19889640	so first things first
19889640	19891640	there's a little something called
19891640	19893640	efficiency testing
19893640	19895640	or just finding out how quickly
19895640	19897640	certain operations takes
19897640	19899640	we'll just call this
19899640	19901640	efficiency testing and I'll show you
19901640	19903640	exactly how to do this right here
19903640	19905640	efficiency
19905640	19907640	yeah
19907640	19909640	I don't know if I spelled that correctly
19909640	19911640	I don't know what it's doing now
19911640	19913640	anyways
19913640	19915640	we'll just pop into code here
19915640	19917640	and
19917640	19919640	essentially
19919640	19921640	we'll just do
19921640	19923640	I don't know
19923640	19925640	I'm testing
19925640	19927640	import time
19927640	19929640	and
19929640	19931640	essentially
19931640	19933640	all we're going to do is just
19933640	19935640	time how long operations take
19935640	19937640	so
19937640	19939640	in here you can go
19939640	19941640	you can go start time
19941640	19943640	equals time dot time
19943640	19945640	and essentially what this function does
19945640	19947640	is it just takes a look at the current time right now
19947640	19949640	the current like millisecond
19949640	19951640	very precise
19951640	19953640	and we can do some little
19953640	19955640	operation like
19955640	19957640	I don't know 4
19957640	19959640	I in range
19961640	19963640	we'll just go
19963640	19965640	10,000
19965640	19967640	go
19967640	19969640	print
19969640	19971640	I
19971640	19973640	print I times 2
19973640	19975640	and then we can just end the time here
19975640	19977640	so go end time
19977640	19979640	equals time dot time again
19979640	19981640	calling the current time so we're doing
19981640	19983640	right now versus back then
19983640	19985640	and that little difference is how long it took to execute
19985640	19987640	so all we can do
19987640	19989640	is just do we can say total time
19989640	19991640	we can say total time equals
19991640	19993640	end time
19993640	19995640	minus start time
19995640	19997640	and we'll just go print
19997640	19999640	end time
19999640	20001640	or
20003640	20005640	I'm taking
20005640	20007640	let's go total
20009640	20011640	total time like that
20011640	20013640	just execute this
20015640	20017640	Python
20017640	20019640	time testing
20019640	20021640	cool
20021640	20023640	time taken 1.32 seconds
20023640	20025640	so you can essentially time every single operation
20025640	20027640	you do with this method
20027640	20029640	and you can see even in your
20029640	20031640	I encourage you to actually try this out
20031640	20033640	I'm not going to but I encourage you to try out
20033640	20035640	how long the model actually takes
20035640	20037640	to do certain things like how long does it take
20037640	20039640	to load a model how does it take to save a model
20039640	20041640	how long does it take to estimate the loss
20041640	20043640	right
20043640	20045640	play around with hyperparameters see how long things take
20045640	20047640	and maybe you'll figure out something new who knows
20047640	20049640	but this is a little something we use
20049640	20051640	to pretty much test how long something
20051640	20053640	takes how efficient it is
20053640	20055640	and then to also see if
20055640	20057640	it's worth investigating a new way of approaching
20057640	20059640	something in case it takes
20059640	20061640	ridiculous amount of time
20061640	20063640	so that's time testing
20063640	20065640	and efficiency testing for you
20065640	20067640	the next little bit I want to cover
20067640	20069640	is the history
20069640	20071640	I'm not going to go over the entire history
20071640	20073640	of AI and LLMs
20073640	20075640	but essentially
20075640	20077640	we originated with something called RNNs
20077640	20079640	okay RNNs are called
20079640	20081640	recurrent neural networks
20081640	20083640	and they're really inefficient
20083640	20085640	at least for scaled
20085640	20087640	AI systems so RNNs
20087640	20089640	are a little essentially think of it as a little loop
20089640	20091640	keeps learning and learning
20091640	20093640	and this is sequential right
20093640	20095640	it does this and then this and then this
20095640	20097640	has to wait for each completion
20097640	20099640	synchronous you can't have multiple of them at once
20099640	20101640	because they're complex
20101640	20103640	GPUs cannot run complex things
20103640	20105640	they're only designed for just
20105640	20107640	matrix multiplication and very simple
20107640	20109640	math like that
20109640	20111640	so RNNs are essentially
20111640	20113640	a little bit dumber than transformers
20113640	20115640	and they
20115640	20117640	are run on the CPU
20117640	20119640	so RNNs was where we last sort of stopped at
20119640	20121640	and what I encourage you to do
20121640	20123640	is look into more of the language
20123640	20125640	modeling and AI
20125640	20127640	history and research that has led up to this
20127640	20129640	point so you can have an idea
20129640	20131640	as to how researchers
20131640	20133640	have been able to quickly innovate
20133640	20135640	given
20135640	20137640	all these historical innovations
20137640	20139640	so you have like all these things leading up to the transformer
20139640	20141640	well how did they all
20141640	20143640	philosophize
20143640	20145640	up to that point
20145640	20147640	and yeah it's just
20147640	20149640	something good to sort of be confident
20149640	20151640	in is innovating
20151640	20153640	as both a researcher
20153640	20155640	and engineer and a
20155640	20157640	business person
20157640	20159640	so cool
20159640	20161640	RNNs were where we sort of
20161640	20163640	finished off and now it's transformers and GPTs
20163640	20165640	that's the current state of AI
20165640	20167640	next up I
20167640	20169640	would like to go over something called
20169640	20171640	quantization
20171640	20173640	so quantization is essentially
20173640	20175640	a way to reduce the memory
20175640	20177640	usage by your parameters
20177640	20179640	so there's actually a paper here
20179640	20181640	called QLaura Efficient Fine
20181640	20183640	Tuning of Quantized
20183640	20185640	LLMs so
20185640	20187640	all this does in simple
20187640	20189640	form is pretty much instead of
20189640	20191640	using 32 bit floating
20191640	20193640	point numbers it goes not only
20193640	20195640	to 16 bit of half precision
20195640	20197640	but all the way down to 4
20197640	20199640	so what this actually
20199640	20201640	looks like is in binary code
20201640	20203640	or in bytecode
20203640	20205640	it will look
20205640	20207640	here there's some array
20207640	20209640	of numbers
20209640	20211640	that it uses
20217640	20219640	okay I can't find it
20219640	20221640	but pretty much what it is
20221640	20223640	it is a bunch of
20223640	20225640	it's a bunch of floating point numbers
20225640	20227640	and they're all between
20227640	20229640	negative one and one
20229640	20231640	and there are 16 of them
20231640	20233640	if you have a 4 bit number
20233640	20235640	that means it can hold 16 different
20235640	20237640	values 0 through 15
20237640	20239640	which is 16 values
20239640	20241640	and all you pretty much do is you have this
20241640	20243640	array of floating point numbers
20243640	20245640	you use the bytecode of
20245640	20247640	that 4 bit
20247640	20249640	number to look up the index
20249640	20251640	in that array and that is your weight
20251640	20253640	that is the weight they use
20253640	20255640	in your model
20255640	20257640	so this way instead of using 32 bit
20257640	20259640	having these super long numbers
20259640	20261640	that are super precise
20261640	20263640	you can have super precise numbers
20263640	20265640	that are just generally good parameters
20265640	20267640	to have that just perform
20267640	20269640	decently
20269640	20271640	they're just sort of well spread out
20271640	20273640	and experimented on and they just
20273640	20275640	happen to work and you have 16 of them
20275640	20277640	instead of a lot
20277640	20279640	so that's
20279640	20281640	another cool little thing that's going on right now
20281640	20283640	is 4 bit quantizations
20283640	20285640	it's a little bit harder
20285640	20287640	to implement
20287640	20289640	I would encourage you to experiment with half precision
20289640	20291640	meaning 16 bit
20291640	20293640	floating point numbers
20293640	20295640	so that means it occupies
20295640	20297640	16 on and off switches
20297640	20299640	or capacitors on your GPU
20299640	20301640	and
20301640	20303640	so quantization is cool to
20303640	20305640	sort of scale down the memory
20305640	20307640	so that way you can scale up all of your hyper parameters
20307640	20309640	and have a more complex model
20309640	20311640	with these
20311640	20313640	yeah just essentially to have bigger models
20313640	20315640	with less space
20315640	20317640	take it up
20317640	20319640	so that is
20319640	20321640	quantization
20321640	20323640	and this is the paper for it
20323640	20325640	it's a little link you can search out if you want to get
20325640	20327640	more familiar with this see
20327640	20329640	sort of performance standards and what not
20329640	20331640	the next thing I'd like to cover
20331640	20333640	is gradient accumulation
20333640	20335640	so you might have heard of this you might not have heard of this
20335640	20337640	gradient accumulation
20337640	20339640	will
20339640	20341640	what gradient accumulation does
20341640	20343640	is it will accumulate
20343640	20345640	gradients
20345640	20347640	over say we just set a variable
20347640	20349640	x so every x iterations
20349640	20351640	it'll just accumulate those
20351640	20353640	iterations, average them
20353640	20355640	and what this allows you to do
20355640	20357640	is instead of
20357640	20359640	updating each iteration
20359640	20361640	you're updating every x iterations
20361640	20363640	so that allows you to fit
20363640	20365640	more parameters and more info
20365640	20367640	or generalization into this one piece
20367640	20369640	so that way when you
20369640	20371640	update your parameters
20371640	20373640	it's able to generalize more
20373640	20375640	over maybe a higher batch size
20375640	20377640	or a higher block size
20377640	20379640	so when you distribute this
20379640	20381640	over many
20381640	20383640	iterations and average them
20383640	20385640	you can fit more into each iteration
20385640	20387640	because it's sort of calculating
20387640	20389640	all of them combined
20389640	20391640	so yeah that's a cool little trick
20391640	20393640	you can use if
20393640	20395640	your GPU maybe isn't
20395640	20397640	as big if it doesn't have as much
20397640	20399640	VRAM on it
20399640	20401640	so gradient accumulation is wonderful
20401640	20403640	and it's used lots in practice
20403640	20405640	the final thing I'd like to leave
20405640	20407640	you guys off with is something called
20407640	20409640	hugging face and you've probably
20409640	20411640	heard a lot about this so far
20411640	20413640	but let me just guide you through
20413640	20415640	and show you how absolutely explosive
20415640	20417640	hugging face is
20417640	20419640	for machine learning so you have
20419640	20421640	a bunch of models, data sets
20421640	20423640	spaces, docs, etc
20423640	20425640	and
20425640	20427640	let's go to models for example
20427640	20429640	so let's just showcase how cool this is
20429640	20431640	you have multimodal AIs which could be
20431640	20433640	like
20433640	20435640	image and text or video
20435640	20437640	etc you have multiple different modes
20437640	20439640	so it's not just text or not just video
20439640	20441640	it's many different ones at the same time
20441640	20443640	so you have multimodal models
20443640	20445640	you have computer vision
20445640	20447640	you have natural language processing
20447640	20449640	and we're actually doing natural language
20449640	20451640	processing in this course
20451640	20453640	we have audio, a tabular
20453640	20455640	and reinforcement learning
20455640	20457640	so this is really cool
20457640	20459640	and you can actually just download these models
20459640	20461640	and host them on your own computer
20461640	20463640	that is really cool
20463640	20465640	you also have data sets which are even cooler
20465640	20467640	and these are pretty much
20467640	20469640	just really high quality data sets
20469640	20471640	of prompt and answer completions
20471640	20473640	at least for our purpose
20473640	20475640	if you want to use those
20475640	20477640	so you have
20477640	20479640	question answering
20479640	20481640	or conversational
20481640	20483640	work data set for example
20483640	20485640	as 9000 downloads
20485640	20487640	500 likes
20487640	20489640	it has a bunch of
20489640	20491640	IDs, system prompts
20491640	20493640	so you're an AI assistant or whatever
20493640	20495640	and then you have the cool stuff which is
20495640	20497640	you'll be given a definition of a task first
20497640	20499640	and some input of the task etc
20499640	20501640	and then the response it's like oh
20501640	20503640	we just gave it an input and asked it to answer
20503640	20505640	in a format and actually did that
20505640	20507640	correctly so
20507640	20509640	you could pretty much train these
20509640	20511640	on a bunch of
20511640	20513640	prompts that you would be able to feed into GPT-4
20513640	20515640	and try to make your model perform that way
20515640	20517640	and this actually has
20517640	20519640	4.23 million rows
20519640	20521640	in the training split which is amazing
20521640	20523640	so
20523640	20525640	data sets are wonderful
20525640	20527640	and you can find the best ones
20527640	20529640	at least the best fine tuning data sets on OpenORCA
20529640	20531640	really good
20531640	20533640	as for pre-training
20533640	20535640	I believe I mentioned this earlier
20535640	20537640	in this survey of large language models
20537640	20539640	that we just
20539640	20541640	put down through Reddit links
20545640	20547640	yep so you could use like OpenWebText
20547640	20549640	you could use CommonCrawl
20549640	20551640	you could use Books
20551640	20553640	you could use Wikipedia
20553640	20555640	these are all pre-training data sources
20555640	20557640	so yeah
20557640	20559640	hopefully that leaves you with a better understanding
20559640	20561640	on how to create GPTs, transformers
20561640	20563640	and
20563640	20565640	pretty good large language models from scratch
20565640	20567640	with your own data that you scraped
20567640	20569640	or that you downloaded
20569640	20571640	and yeah
20571640	20573640	that's it, thanks for watching
20573640	20575640	so you've learned a ton in this course
20575640	20577640	about language modeling
20577640	20579640	how to use data, how to create architecture
20579640	20581640	from scratch
20581640	20583640	maybe even how to look at research papers
20583640	20585640	so if you really enjoy this content
20585640	20587640	I would encourage you to maybe subscribe
20587640	20589640	and like on my YouTube channel
20589640	20591640	which is in the description
20591640	20593640	I make many videos about AI
20593640	20595640	and computer science in general
20595640	20597640	so
20597640	20599640	you could totally feel free to subscribe there
20599640	20601640	if you don't want to subscribe, that's fine
20601640	20603640	you could always unsubscribe later if you want to
20603640	20605640	it's completely free
20605640	20607640	but yeah, also have a GitHub repo in the description
20607640	20609640	for all the code that we used
20609640	20611640	not the data because it's way too big
20611640	20613640	but
20613640	20615640	all of the code and the Wizard of Oz
20615640	20617640	Text file
20617640	20619640	so that's all in the GitHub repo in the description
20619640	20621640	thanks for watching
