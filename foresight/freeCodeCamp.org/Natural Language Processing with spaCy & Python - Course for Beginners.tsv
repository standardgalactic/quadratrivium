start	end	text
0	4560	In this course, you will learn all about natural language processing and how to apply it to
4560	7920	real-world problems using the Spacey Library.
7920	12880	Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.
12880	14960	Hi, and welcome to this video.
14960	21080	My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.
21080	24520	I come to NLP from a humanities perspective.
24520	27520	I have my PhD in medieval history.
27520	32040	But I use Spacey on a regular basis to do all of my NLP needs.
32040	36120	So what you're going to get out of this video over the next few hours is a basic understanding
36120	43600	of what natural language processing is, or NLP, and also how to apply it to domain-specific
43600	48560	problems, or problems that exist within your own area of expertise.
48560	54280	I happen to use this all the time to analyze historical documents or financial documents
54280	56880	for my own personal investments.
56880	61920	Over the next few hours, you're going to learn a lot about NLP, language as a whole,
61920	65120	and most importantly, the Spacey Library.
65120	70320	I like the Spacey Library because it's easy to use and easy to also implement really kind
70320	74840	of general solutions to general problems with the off-the-shelf models that are already
74840	76160	available to you.
76160	79960	I'm going to walk you through, in part one of this video series, how to get the most
79960	83040	out of Spacey with these off-the-shelf features.
83040	86480	In part two, we're going to start tackling some of the features that don't exist in
86480	91760	off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components
91760	97960	in Spacey, to actually solve domain-specific problems in your own area, from the entity
97960	104360	ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects
104360	110440	patterns in a custom Spacey component that doesn't actually exist at the moment.
110440	114560	I'm going to be showing you all that in part two, so that in part three, we can take the
114560	119640	lessons that we learned in part one and part two, and actually apply them to solve a very
119640	126560	kind of common problem that exists in an LP, and that is information extraction from financial
126560	127560	documents.
127560	134160	So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.
134160	139200	If you join me over the next few hours, you will leave this lesson with a good understanding
139200	143680	of Spacey, and also a good understanding of kind of the off-the-shelf components that
143680	150920	are there, and a way to take the off-the-shelf components and apply them to your own domain.
150920	154360	If you also join me in this video and you like it, please let me know in the comments
154360	158960	down below, because I am interested in making a second part to this video that will explore
158960	164200	not only the rules-based aspects of Spacey, but the machine learning-based aspects of
164200	165200	Spacey.
165200	169320	So teaching you how to train your own models to do your own things, such as training a
169320	174320	dependency parser, training a named entity recognizer, things like this, which are not
174320	176400	covered in this video.
176400	180400	Nevertheless, if you join me for this one and you like it, you will find part two much
180400	182480	easier to understand.
182480	189400	So sit back, relax, and let's jump into what NLP is, what kind of things you can do with
189400	194240	NLP, such as information extraction, and what the Spacey library is, and how this course
194240	195680	will be laid out.
195680	200800	If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital
200800	204380	Humanities, which is linked in the description down below.
204380	209440	Even if you're not a digital humanist like me, you will find these Python tutorials useful
209440	216120	because they take Python and make it accessible to students of all levels, specifically those
216120	217320	who are beginners.
217320	221960	I walk you through not only the basics of Python, but also I walk you through step-by-step
221960	225240	some of the more common libraries that you need.
225240	230600	A lot of the channel deals with texts or text-based problems, but other content deals with things
230600	236920	like machine learning and image classification and OCR, all in Python.
236920	240840	So before we begin with Spacey, I think we should spend a little bit of time talking
240840	246440	about what NLP or natural language processing actually is.
246440	251600	Natural language processing is the process by which we try to get a computer system to
251600	258480	understand and parse and extract human language, often times with raw text.
258480	261760	There are a couple different areas of natural language processing.
261760	268080	There's named entity recognition, part of speech tagging, syntactic parsing, text categorization,
268080	273120	also known as text classification, co-reference resolution, machine translation.
273120	278720	Adjacent to NLP is another kind of computational linguistics field called natural language
278720	280980	understanding, or NLU.
280980	285340	This is where we train computer systems to do things like relation extraction, semantic
285340	293540	parsing, question and answering, summarization, sentiment analysis, and paraphrasing.
293540	299400	NLP and NLU are used by a wide array of industries, from finance industry all the way through
299400	306740	to law and academia, with researchers trying to do information extraction from texts.
306740	309660	Within NLP, there's a couple different applications.
309660	314620	The first and probably the most important is information extraction.
314620	319880	This is the process by which we try to get a computer system to extract information that
319880	323880	we find relevant to our own research or needs.
323880	328420	So for example, as we're going to see in part three of this video, when we apply spacey
328420	335280	to the financial sector, a person interested in finances might need NLP to go through and
335280	339180	extract things like company names, stocks, indexes.
339180	343340	Things that are referenced within maybe news articles, from Reuters to New York Times to
343340	345240	Wall Street Journal.
345240	349340	This is an example of using NLP to extract information.
349340	355060	A good way to think about NLP's application in this area is it takes in some unstructured
355060	361940	data, in this case raw text, and extracts structured data from it, or metadata.
361940	365780	So it finds the things that you want it to find and extracts them for you.
365780	371260	Now while there's ways to do this with gazetteers and list matching, using an NLP framework
371260	375620	like spacey, which I'll talk about in just a second, has certain advantages.
375620	380380	The main one being that you can use and leverage things that have been parsed syntactically
380380	381660	or semantically.
381660	385460	So things like the part of speech of a word, things like its dependencies, things like
385460	386940	its co-reference.
386940	391180	These are things that the spacey framework allow for you to do off the shelf and also
391180	396700	train into machine learning models and work into pipelines with rules.
396700	400720	So that's kind of one aspect of NLP and one way it's used.
400720	404380	Another way it's used is to read in data and classify it.
404380	409780	This is known as text categorization and we see that on the left hand side of this image.
409780	413740	Text categorization or text classification, and we conclude in this sentiment analysis
413740	419740	for the most part as well, is a way we take information into a computer system, again unstructured
419780	424300	data, a raw text, and we classify it in some way.
424300	430140	You've actually seen this at work for many decades now with spam detection.
430140	432180	Spam detection is nearly perfect.
432180	436380	It needs to be continually updated, but for the most part it is a solved problem.
436380	440380	The reason why you have emails that automatically go to your spam folder is because there's
440380	444380	a machine learning model that sits on the background of your, on the back end of your
444380	445700	email server.
445700	449780	And what it does is it actually looks at the emails, it sees if it fits the pattern for
449780	454420	what it's seen as spam before, and it assigns it a spam label.
454420	457140	This is known as classification.
457140	461500	This is also used by researchers, especially in the legal industry.
461500	466580	Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.
466580	471780	They don't necessarily have the human time to go through and analyze every single document
471780	472900	verbatim.
472900	477820	It is important to kind of get a quick umbrella sense of the documents without actually having
477820	480700	to go through and read them page by page.
480700	486100	And so what lawyers will oftentimes do is use NLP to do classification and information
486100	487280	extraction.
487280	492020	They will find keywords that are relevant to their case, or they will find documents
492020	495980	that are classified according to the relevant fields of their case.
495980	500860	And that way they can take a million documents and reduce it down to maybe only a handful,
500860	504180	maybe a thousand that they have to read verbatim.
504180	509420	This is a real world application of NLP or natural language processing, and both of these
509420	513540	tasks can be achieved through the SPACI framework.
513540	516460	SPACI is a framework for doing NLP.
516460	521500	Right now, as of 2021, it's only available, I believe, in Python.
521500	525180	I think there is a community that's working on an application with R, but I don't know
525180	526620	that for certain.
526620	531740	But SPACI is one of many NLP frameworks that Python has available.
531740	535340	If you're interested in looking at all of them, you can explore things like NLDK, the
535340	539740	Natural Language Toolkit, Stanza, which I believe is coming out of the same program
539740	541540	at Stanford.
541540	545700	There's many out there, but I find SPACI to be the best of all of them for a couple
545700	547180	different reasons.
547180	551860	Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they
551860	556700	perform very quickly, and they also have very good accuracy metrics, such as precision,
556700	557700	recall, and f-score.
557700	561500	And I'm not going to talk too much about the way we measure machine learning accuracy
561500	564500	right now, but know that they are quite good.
564500	569740	Second, SPACI has the ability to leverage current natural language processing methods,
569740	575100	specifically, transformer models, also known usually kind of collectively as BERT models,
575100	577820	even though that's not entirely accurate.
577820	581780	And it allows for you to use an off-the-shelf transformer model.
581780	587820	And third, it provides the framework for doing custom training relatively easily compared
587820	590820	to these other NLP frameworks that are out there.
590820	596340	Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales
596340	597340	well.
597340	602860	SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale.
602860	609100	By at scale, we mean working with large quantities of documents efficiently, effectively, and
609100	610620	accurately.
610620	615100	SPACI scales well because it can process hundreds of thousands of documents with relative ease
615100	620220	in a relative short period of time, especially if you stick with more rules-based pipes,
620220	624100	which we're going to talk about in part two of this video.
624100	629020	So those are the two things you really need to know about NLP and SPACI in general.
629020	633580	We're going to talk about SPACI in-depth as we explore it both through this video and
633580	641460	in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,
641460	644900	and it should be linked in the description down below.
644900	648460	This video and the textbook are meant to work in tandem.
648460	652100	Some stuff that I cover in the video might not necessarily be in the textbook because
652100	657140	it doesn't lend itself well to text representation, and the same goes for the opposite.
657140	661580	Some stuff that I don't have the time to cover verbatim in this video I cover in a
661580	664980	little bit more depth in the book.
664980	667140	I think that you should try to use both of these.
667140	671580	What I would recommend is doing one pass through this whole video, watch it in its entirety,
671580	675420	and get an umbrella sense of everything that SPACI can do and everything that we're going
675420	676580	to cover.
676580	682100	I would then go back and try to replicate each stage of this process on a separate window
682100	686380	or on a separate screen and try to kind of follow along in code, and then I would go
686380	690660	back through a third time and try to watch the first part where I talk about what we're
690660	694540	going to be doing and try to do it on your own without looking at the textbook or the
694540	695540	video.
695540	699820	If you can do that by your third pass, you'll be in very good shape to start using SPACI
699820	702980	to solve your own domain specific problems.
702980	709700	NLP is a complex field, and applying NLP is really complex, but fortunately frameworks
709700	714300	like SPACI make this project and this process a lot easier.
714300	718420	I encourage you to spend a few hours in this video, get to know SPACI, and I think you're
718420	722740	going to find that you can do things that you didn't think possible in relative short
722740	723900	order.
723900	728180	So sit back, relax, and enjoy this video series on SPACI.
728180	732820	In order to use SPACI, you're first going to have to install SPACI.
732820	737340	Now there's a few different ways to do this depending on your environment and your operating
737340	738340	system.
738340	745700	I recommend going to SPACI.io backslash usage and kind of enter in the correct framework
745700	746700	that you're working with.
746700	751620	So if you're using Mac OS versus Windows versus Linux, you can go through and in this very
751620	756360	handy kind of user interface, you can go through and select the different features that matter
756360	757980	most to you.
757980	762980	I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be
762980	767340	doing everything on the CPU and I'm going to be working with English.
767340	770940	So I've established all of those different parameters, and it goes through and it tells
770940	776780	me exactly how to go through and install it using PIP in the terminal.
776780	782020	So I encourage you to go through pause the video right now, go ahead and install Windows
782020	786620	however you want to, I'm going to be walking through how to install it within the Jupyter
786620	789700	notebook that we're going to be moving to in just a second.
789700	793660	I want you to not work with the GPU at all.
793660	797900	Working with Spacey on the GPU requires a lot more understanding about what the GPU
797900	801660	is used for, specifically in training machine learning models.
801660	804420	It requires you to have CUDA installed correctly.
804420	808860	It requires a couple other things that I don't really have the time to get into in this video,
808860	813900	but we'll be addressing in a more advanced Spacey tutorial video.
813900	819540	So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,
819540	823300	and then selecting CPU and since you're going to be working through this video with English
823340	829980	texts, I encourage you to select English right now and go ahead and just install or download
829980	832700	the Ncore Web SM model.
832700	833700	This is the small model.
833700	836020	I'll talk about that in just a second.
836020	842660	So the first thing we're going to do in our Jupyter notebook is we are going to be using
842660	847100	the exclamation mark to delineate in the cell that this is a terminal command.
847100	850180	We're going to say PIP install Spacey.
850180	854140	Your output when you execute this cell is going to look a little different than mine.
854140	857260	I already have Spacey installed in this environment.
857260	860940	And so mine kind of goes through and looks like this yours will actually go through.
860940	865820	And instead of saying requirement already satisfied, it'll be actually passing out the the different
865820	871420	things that it's actually installing to install Spacey and all of its dependencies.
871420	876060	The next thing that you're going to do is you're going to again, you follow the instructions
876140	882540	and you're going to be doing Python dash M space Spacey space download and then the model
882540	884180	that you want to download.
884180	886220	So let's go ahead and do that right now.
886220	892060	So let's go ahead and say Python M Spacey download.
892060	894540	So this is a Spacey terminal command.
894540	897420	And we're going to download the Ncore Web SM.
897420	900060	And again, I already have this model downloaded.
900060	905460	So on my end, Spacey is going to look a little differently than as it's going to look on your
905460	909100	end as it prints off on the Jupyter notebook.
909100	913340	And if we give it a just a second, everything will go through and it says that it's collected
913340	917900	it, it's downloading it and we are all very happy now.
917900	923060	And so now that we've got Spacey installed correctly, and that we've got the small model
923060	928100	downloaded correctly, we can go ahead and start actually using Spacey and make sure
928100	929380	everything's correct.
929380	933700	The first thing we're going to do is we're going to import the Spacey library as you
933820	935820	would with any other Python library.
935820	941500	If you're not familiar with this, a library is simply a set of classes and functions that
941500	946260	you can import into a Python script so that you don't have to write a whole bunch of extra
946260	947460	code.
947460	952220	Libraries are massive collections of classes and functions that you can call.
952220	957380	So when we import Spacey, we're importing the whole library of Spacey.
957380	961860	And now that we've seen something like this, we know that Spacey has imported correctly,
961860	967540	as long as you're not getting an error message, everything was imported fine.
967540	973460	The next thing that we need to do is we want to make sure that our English Core Web SM,
973460	976740	our small English model, was downloaded correctly.
976740	980580	So the next thing that we need to do is we need to create an NLP object.
980580	984940	I'm going to be talking a lot more about this as we move forward right now.
984940	989420	This is just troubleshooting to make sure that we've installed Spacey correctly and
989420	992140	we've downloaded our model correctly.
992140	995540	So we're going to use the spacey.load command.
995540	997460	This is going to take one argument.
997460	1002700	It's going to be a string that is going to correspond to the model that you've installed.
1002700	1007820	In this case, N Core Web SM.
1007820	1014100	And if you execute this cell and you have no errors, you have successfully installed
1014100	1019980	Spacey correctly and you've downloaded the English Core Web SM model correctly.
1019980	1025340	So go ahead, take time and get all this stuff set up, pause the video if you need to and
1025340	1031180	then pop back and we're going to start actually working through the basics of Spacey.
1031180	1036460	I'm now going to move into kind of an overview of kind of what's within Spacey, why it's
1036460	1040940	useful and kind of some of the basic features of it that you need to be familiar with.
1040940	1045420	And I'm going to be working from the Jupyter Notebook that I talked about in the introduction
1045420	1046420	to this video.
1046420	1051820	If we scroll down to the bottom of chapter one, the basics of Spacey and you get past
1051820	1055500	the install section, you get to this section on containers.
1055500	1056780	So what are containers?
1056780	1062780	Well, containers within Spacey are objects that contain a large quantity of data about
1062780	1064300	a text.
1064300	1067780	There are several different containers that you can work with in Spacey.
1067780	1073620	There's the doc, the doc bin, example, language, lexeme, span, span group and token.
1073620	1077420	We're going to be dealing with the lexeme a little bit in this video series and we're
1077420	1081580	going to be dealing with the language container a little bit in this video series, but really
1081580	1085500	the three big things that we're going to be talking about again and again is the doc,
1085500	1088420	the span and the token.
1088420	1092580	And I think when you first come to Spacey, there's a little bit of a learning curve about
1092580	1097260	what these things are, what they do, how they are structured hierarchically.
1097420	1103140	And for that reason, I've created this, in my opinion, kind of easy to understand image
1103140	1105420	of what different containers are.
1105420	1110580	So if you think about what Spacey is as a pyramid, so a hierarchical system, we've
1110580	1116340	got all these different containers structured around really the doc object.
1116340	1122300	Your doc container or your doc object contains a whole bunch of metadata about the text
1122300	1126980	that you pass to the Spacey pipeline, which we're going to see in practice.
1126980	1131500	In just a few minutes, the doc object contains a bunch of different things.
1131500	1133660	It contains attributes.
1133660	1137100	These attributes can be things like sentences.
1137100	1142140	So if you iterate over doc.sense, you can actually access all the different sentences
1142140	1145100	found within that doc object.
1145100	1150940	If you iterate over each individual item or index in your doc object, you can get individual
1150940	1152380	tokens.
1152380	1156540	Tokens are going to be things like words or punctuation marks.
1156580	1163020	Anything within your sentence or text that has a self contained important value, either
1163020	1165660	syntactically or semantically.
1165660	1170340	So this is going to be things like words, a comma, a period, a semi colon, a quotation
1170340	1174140	mark, things like this, these are all going to be your tokens.
1174140	1179900	And we're going to see how tokens are a little different than just splitting words up with
1179900	1183900	traditional string methods and Python.
1183900	1187580	The next thing that you should be kind of familiar with are spans.
1187580	1194380	So spans are important because they kind of exist within and without of the doc object.
1194380	1201220	So unlike the token, which is an index of the doc object, a span can be a token itself,
1201220	1204740	but it can also be a sequence of multiple tokens.
1204740	1206260	We're going to see that at play.
1206260	1215260	So imagine if you had a span in its category, maybe group one are our places.
1215260	1218740	So a single token might be like a city like Berlin.
1218740	1223220	But span group two, this could be something like full proper names.
1223220	1227500	So of a people, for example, so this could be like as we're going to see Martin Luther
1227500	1228500	King.
1228500	1233260	This would be a sequence of tokens, a sequence of three different items in the sentence that
1233260	1237180	make up one span or one self contained item.
1237180	1245420	So Martin Luther King would be a person who's a collection of a sequence of individual tokens.
1245420	1250620	If that doesn't make sense right now, this image will be reinforced as we go through
1250620	1254740	and learn more about spacey in practice.
1254740	1261500	For right now, I want you to be just understanding that the doc object is the thing around which
1261500	1263420	all of spacey sits.
1263420	1266280	This is going to be the object that you create.
1266280	1270740	This is going to be the object that contains all the metadata that you need to access.
1270740	1276740	And this is going to be the object that you try to essentially improve with different
1276740	1282260	custom components, factories and pipelines as you go through and do more advanced things
1282260	1283260	with spacey.
1283260	1289260	We're going to now see in just a few seconds how that doc object is kind of similar to the
1289260	1295860	text itself, but how it's very, very different and much more powerful.
1295860	1299420	We're now going to be moving on to chapter two of this textbook, which is going to deal
1299420	1304380	with kind of getting used to the in depth features of spacey.
1304380	1309220	If you want to pause the video or keep this notebook or this book open up kind of separate
1309220	1313980	from this video and follow along as we go through and explore it in live coding.
1313980	1317420	We're going to be talking about a few different things as we explore chapter two.
1317780	1320180	This will be a lot longer than chapter one.
1320180	1324380	We're going to be not only importing spacey, but actually going through and loading up
1324380	1329300	a model, creating a doc object around that model so that we're going to work with the
1329300	1331180	doc container and practice.
1331180	1335500	And then we're going to see how that doc container stores a lot of different features
1335500	1338660	or metadata or attributes about the text.
1338660	1343020	And while they look the same on the surface, they're actually quite different.
1343020	1348700	So let's go ahead and work within our same Jupiter notebook where we've imported spacey
1348700	1351940	and we have already created the NLP object.
1351940	1356980	The first thing that I want to do is I want to open up a text to start working with within
1356980	1359220	this repo.
1359220	1362580	We've got a data folder within this data sub folder.
1362580	1365260	I've got a couple of different Wikipedia openings.
1365260	1369420	I've got one on MLK that we're going to be using a little later in this video and I have
1369420	1371500	one on the United States.
1371500	1374180	This is wiki underscore us.
1374180	1376220	That's going to be what we work with right now.
1376220	1384260	So let's use our with operator and open up data backslash wiki underscore us dot txt.
1384260	1389300	We're going to just read that in as F and then we're going to create this text object,
1389300	1392220	which is going to be equal to F dot read.
1392220	1396260	And now that we've got our text object created, let's go ahead and see what this looks like.
1396260	1398780	So let's print text.
1398780	1402980	Then we see that it's a standard Wikipedia article kind of follows that same introductory
1402980	1408420	format and it's about four or five paragraphs long with a lot of the features left in such
1408420	1411900	as the brackets that delineate some kind of a footnote.
1411900	1415300	We're not going to worry too much about cleaning this up right now because we're interested
1415300	1421460	not with cleaning our data so much as just starting to work with the doc object in spacey.
1421460	1426220	So the first thing that you want to do is you're going to want to create a doc object.
1426220	1430260	It is oftentimes good practice if you're only ever working with one doc object in your
1430260	1434540	script to just call your only object doc.
1434540	1439620	If you're working with multiple objects, sometimes you'll say doc one doc two doc three
1439620	1445100	or give it some kind of specific name so that your variables can be unique and easily identifiable
1445100	1446600	later in your script.
1446600	1450820	Since we're just working with one doc object right now, we're going to say doc is equal
1450820	1452900	to NLP.
1452900	1458660	So this is going to call our NLP model that we imported earlier in this case the English
1458660	1461460	Core Web SM model.
1461460	1466100	And that's going to for right now just take one argument and that's going to be the text
1466100	1467100	itself.
1467100	1473960	So the text object, if you execute that cell, you should have a doc object now created.
1473960	1478000	Let's print off that doc object and see what it looks like.
1478000	1484000	And if you scroll down, you might be thinking to yourself, this looks very, very similar
1484000	1487760	if not identical to what I just saw a second ago.
1487760	1493760	And in fact, on the surface, it is very similar to that text object that we gave to the NLP
1493760	1497400	model or pipeline, but let's see how they're different.
1497400	1501040	Let's print off the length of text.
1501040	1505520	And let's print off the length of the doc object.
1505520	1508160	And what we have here are two different numbers.
1508160	1515240	Our text is 3525 and our doc object is 152.
1515240	1516600	What is going on here?
1516600	1522440	Well, let's get a sense by trying to iterate over the text object and iterating over the
1522440	1524920	doc object with a simple for loop.
1524920	1528800	So we're going to say for token and text, so we're going to iterate first over that
1528800	1532640	text object, we're going to print off the token.
1532640	1537280	So the first 10 indices, and we get individual letters as one might expect.
1537280	1541480	But when we do something the same thing with the doc object, let's go ahead and start writing
1541480	1542480	this out.
1542480	1549000	We're going to say for token and doc, we're going to iterate over the first 10.
1549000	1551120	We're going to print off the token.
1551120	1553120	We see something very different.
1553120	1556160	What we see here are tokens.
1556160	1561880	This is why the doc object is so much more valuable and this is why the doc object has
1561880	1564600	a different length than the text object.
1564600	1569560	The text object is just basically counting up every instance of a character, a white
1569560	1571700	space, a punctuation, etc.
1571700	1578280	The doc object is counting individual tokens, so any word, any punctuation, etc.
1578280	1581160	That's why they're of different length and that's why when we print them off, we see
1581160	1582160	something different.
1582160	1585920	So you might now already be seeing the power of spacey.
1585920	1590040	It allows for you to easily on the surface with nothing else being done, easily split
1590040	1593720	up your text into individual tokens without any effort at all.
1593720	1598480	Now, those of you familiar with Python and different string methods might be thinking
1598480	1600840	to yourself, but I've got the split method.
1600840	1603560	I can just use this to split up the text.
1603560	1605960	I don't need anything fancy from spacey.
1605960	1607560	Well, you'd be wrong.
1607560	1609320	Let me demonstrate this right now.
1609320	1616880	So if I were to say for token and text.split, so I'm splitting up that text into individual
1616880	1621720	and theory individual words, essentially, it's just a split method where it's splitting
1621720	1623360	by individual white spaces.
1623360	1628160	If I were to do that and iterate over the first 10 again.
1628160	1633560	And I would just say print token, it looks good until you get down here.
1633560	1638160	So until you get to USA, well, why is it a problem?
1638160	1640560	The problem is quite simple.
1640560	1643760	There is a parentheses mark right here.
1643760	1650120	And this is where we have a huge advantage with spacey.
1650120	1655920	Spacey automatically separates out these these kind of punctuation marks and removes them
1655920	1660200	from individual tokens when they're not relevant to the token itself.
1660200	1665160	Notice that USA has got a period within the middle of it.
1665160	1669520	It's not looking at that and thinking that that is some kind of unique token, a you a
1669520	1673200	period and s a period and an a in a period.
1673200	1677400	It's not seeing these as four individual tokens rather it's automatically identifying them
1677400	1684640	as one thing one tied together single token that's a string of characters and punctuation.
1684640	1689640	This is where the power of spacey really lies just on the surface level and go ahead spend
1689640	1691760	a few minutes and play around with this.
1691760	1696360	And then we're going to kind of jump back here and start talking about how the doc object
1696360	1699720	has a lot more than just tokens within it.
1699720	1702600	It's got sentences each token has attributes.
1702600	1710480	We're going to start exploring these when you pop back.
1710480	1714560	If you're following along with the textbook, we're now going to be moving on to the next
1714560	1717880	section, which is sentence boundary detection.
1717880	1725360	An NLP sentence boundary detection is the identification of sentences within a text on the surface.
1725360	1726600	This might look simple.
1726600	1731520	You might be thinking to yourself, I could simply use the split function and split up
1731520	1733800	a text with a simple period.
1733800	1735920	And that's going to give me all my sentences.
1735920	1740200	Those of you who have tried to do this might already be shaking your heads and saying no.
1740200	1744560	If you do think about it, there's a really easy explanation for why this doesn't work.
1744560	1750560	Were you to try to split up a text by period and make a presumption that anything that occurs
1750560	1754920	with between periods is going to be an individual sentence, you would have a serious mistake
1754960	1760560	when you get to things like USA, especially in Western languages, where the punctuation
1760560	1766840	of a period mark is used not only to delineate the change of its sentence, rather it's used
1766840	1769920	to also delineate abbreviations.
1769920	1776000	So United States of America, each period represents an abbreviated word.
1776000	1780880	So you could write in rules to kind of account for this, you could write in rules that could
1780880	1785240	also include in other ways that sentences are created, such as question marks, such
1785240	1786920	as exclamation marks.
1786920	1788200	But why do that?
1788200	1790200	That's a lot of effort.
1790200	1796820	When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly
1796820	1798320	how that works.
1798320	1806800	So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing
1806800	1809880	the sentence attribute of the doc object.
1809880	1813080	Let's print off scent.
1813080	1817520	And if you do that, you are now able to print off every individual sentence.
1817520	1821880	So the entire text has been tokenized at the sentence level.
1821880	1827760	In other words, spacey has used its sentence boundary detection and done all that for you
1827760	1829920	and giving you all the sentences.
1829920	1833600	If you work with different models of different sizes, you're going to notice that certain
1833600	1837840	models the larger they get tend to do better at sentence detection.
1837840	1841400	And that's because machine learning models tend to do a little bit better than heuristic
1841400	1842600	approaches.
1842600	1847960	The English core web SM model, while having some machine learning components in it, does
1847960	1850360	not save word vectors.
1850360	1855200	And so the larger you go with the models, typically the better you're going to have with regards
1855200	1857440	to sentence detection.
1857440	1860680	Let's go ahead and try to access one of these sentences.
1860680	1866200	So let's create an object called sentence one, we're going to make that equal to doc.sense
1866200	1867200	zero.
1867400	1874520	We're going to try to grab that zero index and let's print off sentence one, we do this,
1874520	1875920	we get an error.
1875920	1877600	Why have we gotten an error?
1877600	1880120	Well, it tells you why right here, it's a type air.
1880120	1886720	And this means that this is not a type that can be kind of iterated over, it's not subscriptable.
1886720	1888880	And it's because it is a generator.
1888880	1892960	Now in Python, if you're familiar with generators, you might be thinking to yourself, there's
1892960	1893960	a solution for this.
1893960	1895400	And in fact, there is.
1895440	1900040	If you want to work with generator objects, you need to convert them into a list.
1900040	1903360	So let's say sentence one is equal to list.
1903360	1908680	So using the list function to convert doc.sense into a list.
1908680	1913080	And then with outside of that, we're going to grab zero, the zero index, and then we're
1913080	1915960	going to print off sentence one.
1915960	1920320	And we grab the first sentence of that text.
1920400	1926080	This as we go deeper and deeper in spacey one by one, you're going to see the immense
1926080	1930880	power that you can do with Pacea, all the immense incredible things you can use spacey
1930880	1934240	for with very, very minimal code.
1934240	1939160	The doc object does a lot of things for you that would take hours to actually write out
1939160	1942040	and code to do with heuristic approaches.
1942040	1947280	This is now a great way to segment an entire text up by sentence.
1947280	1953520	And if you work with text a lot, you will already know that this has a lot of applications.
1953520	1957440	As we move forward, we're going to not just talk about sentences, we're also going to
1957440	1963160	be talking about token attributes, because within the doc object are individual tokens.
1963160	1967920	I encourage you to pause here and go ahead and play around with the doc.sense a little
1967920	1972680	bit and get familiar with how it works, what it contains, and try to convert it into a
1972680	1973680	list.
1974680	1978920	And we'll continue talking about tokens.
1978920	1982800	This is where I really encourage you to spend a little bit of time with the textbook.
1982800	1988600	Under token attributes in chapter two, I have all the different kind of major things that
1988600	1991280	you're going to be using with regards to token attributes.
1991280	1994680	We're going to look and see how to access them in just a second.
1994680	1998640	I've provided for you kind of the most important ones that you should probably be familiar
1998640	1999640	with.
1999640	2002880	We're going to see this in code in just a second, and I'm going to explain with a little
2002880	2007400	bit more detail than what's in the spacey documentation about what these different things
2007400	2010360	are, why they're useful, and how they're used.
2010360	2016480	So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.
2016480	2020120	If you remember, the doc object had a sequence of tokens.
2020120	2023400	So for token and doc, you could print off token.
2023400	2026480	And let's just do this with the first 10.
2026480	2029440	And we've got each individual token.
2029440	2035880	What you don't see here is that each individual token has a bunch of metadata buried within
2035880	2036880	it.
2036880	2042840	These metadata are things that we call attributes or different things about that token that
2042840	2047160	you can access through the spacey framework.
2047160	2049680	So let's go ahead and try to do that right now.
2049680	2055920	Let's just work with for right now token number two, which we're going to call sentence one,
2055920	2058920	and we're going to grab from sentence one, the second index.
2059440	2061960	Let's print off that word.
2061960	2063360	And it should be states.
2063360	2066000	And in fact, it is fantastic.
2066000	2070280	So now that we've got the word states accessed, we can start kind of going through and playing
2070280	2073920	around with some of the attributes that that word actually has.
2073920	2077240	Now when you print it off, it looks like a regular piece of text, it looks like just
2077240	2082400	a string, but it's got so much more buried within it now because it's been passed through
2082400	2086440	our NLP model or pipeline from spacey.
2086480	2090040	So let's go ahead and say token to dot text.
2090040	2092000	And I'm going to be saying token to dot text.
2092000	2096440	If you're working within an IDE like Adam, you're going to need to say print token to
2096440	2097440	dot text.
2097440	2102600	When we do this, we see we get a string that just is states.
2102600	2108960	This is telling us that the dot text of the object, the pure text corresponds to the word
2108960	2109960	states.
2109960	2114200	This is really important if you need to extract the text itself from the token and not work
2114240	2118160	with the token object, which has behind it a whole bunch of different metadata that we're
2118160	2121000	going to go through now and start accessing.
2121000	2123480	Let's use the token left edge.
2123480	2127760	So we can say token to dot left underscore edge.
2127760	2128760	And we can print that off.
2128760	2130440	Well, what's that telling us?
2130440	2135640	It's telling us that this is part of a multi word token or a token that is multiple has
2135640	2139720	multiple components to make up a larger span.
2139720	2143800	And that this is the leftmost token that corresponds to it.
2143800	2148480	So this is going to be the word the as in the United States.
2148480	2151360	Let's take a look at the right edge.
2151360	2157080	We can say token to dot right underscore edge, print that off, and we get the word America.
2157080	2162720	So we're able to see where this token fits within a larger span in this case a noun chunk,
2162720	2164760	which we're going to explore in just a few minutes.
2164760	2168600	But we also learn a lot about it, kind of the different components, so we know where
2168600	2172280	to grab it from the beginning and from the very end.
2172280	2175720	So that's how the left edge and the right edge work.
2175720	2181120	We also have within this token to dot int type.
2181120	2183360	This is going to be the type of entity.
2183360	2185900	Now what you're seeing here is a integer.
2185900	2187820	So this is 384.
2187820	2193160	In order to actually know what 384 means, I encourage you to not really use that so much
2193160	2197280	as and type with an underscore after it.
2197280	2201960	This is going to give you the string corresponding to number 384.
2201960	2207280	In this case, it is GPE or geopolitical entity.
2207280	2210640	We're going to be working with named entity a little bit in this video, but I have a whole
2210640	2213560	other book on named entity recognition.
2213560	2219360	It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning
2219360	2222160	and rules based in a lot more depth.
2222160	2226560	Let's go ahead and keep on moving on though and looking at different entity types here
2226560	2227560	as well.
2227560	2229840	Not entity types, attribute types.
2229840	2237920	So we're going to say token to dot int IOB, all lowercase and again underscore at the
2237920	2242120	end and we get the string here, I.
2242120	2247040	Now IOB is a specific kind of named entity code.
2247040	2251400	B would mean that it's the beginning of an entity and I means that it's inside of an
2251400	2256320	entity and O means that it's outside of an entity.
2256320	2263040	The fact that we're seeing I here tells us that this word states is inside of a larger
2263040	2264040	entity.
2264040	2267480	In fact, we know that because we've seen the left edge and we've seen the right edge.
2267480	2271280	It's inside of the United States of America.
2271280	2274600	So it's part of a larger entity at hand.
2274600	2282080	We can also say token to dot lima and under case again after that and we get the word
2282080	2283080	states.
2283080	2286720	Lima form or the root form of the word.
2286720	2289760	This means that this is what the word looks like with no inflection.
2289760	2294560	If we were working with a verb, in fact, let's go ahead and do that right now.
2294560	2296560	Let's grab sentence.
2296560	2302000	We're going to grab sentence one index 12, which should be the word no and we're going
2302000	2309840	to print off the lima for the word or sorry, it's a verb and we see the verb lima as no.
2309840	2319600	So if we were to print off sentence one specifically index 12, we see that its original form is
2319600	2320780	known.
2320780	2327640	So the lima form uninflected is the verb no K N O W.
2327640	2331240	Another thing that we can access and we're going to see that have the power of this later
2331240	2332240	on.
2332240	2335680	This might not seem important right now, but I promise you it will be.
2335680	2341400	Let's print off token that I call this again token to we're going to print that off, but
2341400	2344320	we're going to print off specifically the morph.
2344320	2345720	No underscore here.
2345720	2346720	Just morph.
2346720	2352320	What you get is what looks like a really weird output a string called noun type equal
2352320	2353320	to prop.
2353320	2358600	In fact, this means proper noun, a number which corresponds to sing.
2358600	2364480	We're going to talk a lot more about morphological analysis later on when we try to find an extract
2364480	2367320	information from our texts.
2367320	2372520	But for right now, understand that what you're looking at is the output of kind of what that
2372520	2374600	word is morphologically.
2374600	2377640	So in this case, it's a proper noun and it's singular.
2377640	2385360	If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb
2385360	2386360	it is.
2386360	2393040	So it's a perfect past participle known perfect past participle.
2393040	2396960	For being good at NLP is also being good with language.
2396960	2400240	So I encourage you to spend time and start getting familiar with those things that you
2400240	2404540	might have forgotten about from like fifth grade grammar, such as perfect participles
2404540	2406240	and things like that.
2406240	2410620	Because when you need to start creating rules to extract information, you're going to find
2410620	2414920	those pieces of information very important for writing rules.
2414920	2416920	We'll talk about that in a little bit though.
2416920	2420240	Let's go back to our other attributes from the token.
2420240	2425680	So again, let's go to token two, and we're going to grab the POS part of speech, not
2425680	2427120	what you might be thinking.
2427120	2433020	So part of speech underscore POS underscore, and we output PROPN.
2433020	2435760	This means that it is a proper noun.
2435760	2442640	It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological
2442640	2449320	detailed extraction, what kind of noun it might be with regards to in this case, singular.
2449320	2452040	So that's going to be how you extract the part of speech.
2452040	2457580	And the thing that you can do is you can extract the dependency relation.
2457580	2462400	So in this case, we can figure out what role it plays in the sentence.
2462400	2465600	In this case, the noun subject.
2465600	2469800	And then finally, the last thing I really want to talk about before we move into a more
2469800	2477800	detailed analysis of part of speech is going to be the token two dot lane.
2477800	2483280	And what this grabs for you is the language of the doc object in this case, we're working
2483280	2488680	with something from the English language, so in every language is going to have two
2488680	2490320	letters that correspond to it.
2490320	2492540	These are universally recognized.
2492540	2498880	So that's going to be how you access different kinds of attributes that each token has.
2498880	2502240	And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that
2502240	2503680	I haven't covered.
2503680	2508880	I gave you the ones that are the most important that I find to be used on a regular basis
2508880	2515400	to solve different problems with regards to information extraction from the text.
2515400	2519580	So that's going to be where we stop here with token attributes, and we're going to be moving
2519580	2526200	on to part 2.5 of the book, which is part of speech tagging.
2526200	2532360	I now want to move into kind of a more detailed analysis of part of speech within spacey and
2532360	2538640	the dependency parser and how to actually analyze it really nicely either in a notebook
2538640	2540280	or outside of a notebook.
2540280	2543120	So let's work with a different text for just a few minutes.
2543120	2544880	We're going to see why this is important.
2544880	2548760	It's because I'm working on a zoomed in screen, and to make this sentence a little easier
2548760	2556120	to understand, we're going to just use Mike and Joy's plain football, a very simple sentence.
2556120	2559960	And we're going to create a new doc object, and we're going to call this doc two.
2559960	2562840	That's going to be equal to NLP text.
2562840	2567440	Let's print off doc two just to make sure that it was created, and in fact that we see
2567440	2568440	that it was.
2568440	2574200	Now that we've got it created, let's iterate over the tokens within this and say for token
2574200	2579480	in text, we want to print off token dot text.
2579480	2581600	We want to see what the text actually is.
2581600	2593240	We want to see the token dot POS, and the token dot DEP helps if you actually iterate
2593240	2596120	over the correct object over the doc to object.
2596120	2600640	And we see that we've got Mike, proper noun, noun, subject, and Joy's verb.
2600640	2602640	It's the root plane.
2602640	2604440	In this case, it's a verb.
2604440	2608280	And then we've got football, the noun, the direct object, and a period, which is the
2608280	2609280	punctuation.
2609400	2613640	So we can see the basic semantics of the sentence at play.
2613640	2617200	What's really nice from spacey is we have a way to really visualize this information
2617200	2619360	and how these words relate to one another.
2619360	2627400	So we can say from spacey, import, displacey, and we're going to do displacey, displacey
2627400	2631400	dot render.
2631400	2635440	And this is going to take two arguments, it's going to be the text, and then it's going
2635440	2641320	to be the, actually, it's going to be doc two, and then it's going to be style.
2641320	2645440	In this case, we're going to be working with DEP, and we're going to print that off.
2645440	2650440	And we actually see how that sentence is structured.
2650440	2653720	Now in the textbook, I use a more complicated sentence.
2653720	2657480	But for the reasons of this video, I've kept it a little shorter, just because I think
2657480	2662320	it displays better on this screen, because you can see that this becomes a little bit
2662320	2665600	more difficult to understand when you're zoomed in.
2665600	2669440	But this is one sentence from that Wikipedia article.
2669440	2672640	So go ahead and look at the textbook and see how elaborate this is.
2672640	2676400	You can see how it's part of a compound, how it's preposition.
2676400	2682680	You can see the more fine-grained aspects of the dependency parser and the part of speech
2682680	2686800	tagger really at play with more complicated sentences.
2686800	2690960	So that's going to be how you really access part of speech and how you can start to visualize
2690960	2697240	how words in a sentence are connected to other words in a sentence with regards to their
2697240	2700480	part of speech and their dependencies.
2700480	2701960	That's going to be where we stop with that.
2701960	2707200	In the next section, we're going to be talking about named entity recognition and how to visualize
2707200	2709680	that information.
2709680	2713920	So named entity recognition is a very common NLP task.
2713920	2719080	It's part of kind of data extraction or information extraction from texts.
2719080	2723480	It's oftentimes just called NER, named entity recognition.
2723480	2728160	I have a whole book on how to do NER with Python and with Spacey, but we're not going
2728160	2730120	to be talking about all the ins and outs right now.
2730120	2735600	We're just going to be talking about how to access the pieces of information throughout
2735600	2737080	kind of our text.
2737080	2742560	And then we're going to be dealing with a lot of NER as we try to create elaborate systems
2742560	2747880	to do named entity extraction for things like financial analysis.
2747880	2752080	Let's go ahead and figure out how to iterate over a doc object.
2752080	2757520	So we're going to say for int and doc.n, so we're going to go back to that original doc,
2757520	2763800	the one that's got the text from Wikipedia on the United States.
2763800	2771880	We're going to say print off int.text, so the text from it, and int.label, label underscore
2771880	2772880	here.
2772880	2776200	That's going to tell us what label corresponds to that text.
2776320	2777320	Then we print this off.
2777320	2781800	We've got a lot of GPEs, which are geopolitical entities, North America.
2781800	2783280	This isn't a geopolitical entity.
2783280	2790080	It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in
2790080	2796920	this case, which is a national or religious political entity, quantity, the number of
2796920	2804440	miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke.
2804480	2808400	Then we have date being extracted, so at least 12,000 years ago.
2808400	2813360	This is a small model, and it's extracting for us a lot of very important structured
2813360	2814520	data.
2814520	2818160	But we can see that the small model makes mistakes.
2818160	2822120	So the Revolutionary War is being considered an organization.
2822120	2826880	Were I to use a large model right now, which I can download separately from Spacey, and
2826880	2832440	we're going to be seeing this later in this video, or were I to use the much larger transformer
2832440	2833520	model.
2833520	2839720	This would be correctly identified most likely as an event, not as an organization, but because
2839720	2844360	this is a small model that doesn't contain word vectors, which we're going to talk about
2844360	2850520	in just a little bit, it does not generalize or make predictions well on this particular
2850520	2851520	data.
2851520	2855680	Nevertheless, we do see really good extraction here.
2855680	2859060	We have the American Civil War being extracted as an event.
2859060	2863940	We have the Spanish American War, even with this encoding typographical error here.
2863940	2868500	And World War being extracted as an event, World War II event, Cold War event.
2868500	2870580	All of this is looking good.
2870580	2876020	And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd
2876020	2877020	expect to see.
2877020	2880740	We even see percentages extracted correctly here.
2880740	2887180	So this is how you access really vital information about your tokens, but more importantly about
2887180	2890660	the entities found within your text.
2890660	2897100	And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook.
2897100	2904540	We can say displacie.render, we can say doc, style, we can say int.
2904540	2910060	And we get this really nice visualization where each entity has its own particular color.
2910060	2914140	So you can see where these entities appear within the text, as you kind of just naturally
2914140	2915140	read it.
2915140	2919100	And you can do this with the text as long as you want, you can even change the max length
2919100	2921780	to be more than a million characters long.
2921780	2926780	And again, we can see right here, org is incorrectly identified as the American Revolutionary War
2926780	2931220	incorrectly identified as org, but nevertheless, we see really, really good results with a
2931220	2935300	small English model without a lot of custom fine tune training.
2935300	2936460	And there's a reason for this.
2936460	2940860	A lot of Wikipedia data gets included into machine learning models.
2940860	2945900	The machine learning models on text typically make good predictions on Wikipedia data, because
2945900	2948060	it was included in their training process.
2948060	2950620	Nevertheless, these are still good results.
2950620	2954260	If I'm right or wrong on that, I'm not entirely certain.
2954260	2958700	But that's going to be how you kind of extract important entities from your text, and most
2958700	2960580	importantly visualize it.
2960580	2964300	This is where chapter two of my book kind of ends.
2964300	2968860	After this chapter, you have a good understanding, hopefully, of kind of what the dot container
2968860	2975980	is, what tokens are, and how the doc object contains the attributes such as since and
2975980	2981100	ends, which allows for you to find sentences and entities within a text.
2981100	2987180	Hopefully you also have a good understanding of how to access the linguistic features of
2987180	2990060	each token through token attributes.
2990060	2996740	I encourage you to spend a lot of time becoming familiar with these basics, as these basics
2996740	3001180	are the building block for really robust things that we're going to be getting into
3001180	3005700	in the next few lessons.
3005700	3011860	We're now moving into chapter three of our textbook on Spacey and Python.
3011860	3016940	Now in chapter three, we're going to be continuing our theme of part one, where we're trying
3016940	3020900	to understand the larger building blocks of Spacey.
3020900	3025420	Even though this video is not going to deal with Spacey machine learning approaches, our
3025460	3030260	custom ones, that is, it's still important to be familiar with what machine learning is
3030260	3036620	and how it works, specifically with regards to language, because a lot of the Spacey models
3036620	3044060	such as the medium, large and transformer models, all are machine learning models that
3044060	3047620	have word vectors stored within them.
3047620	3054420	This means that they're going to be larger, more accurate, and do the things a bit more
3054420	3058260	slowly, depending upon its size.
3058260	3063900	We're going to be working through not only what kind of machine learning is generally,
3063900	3068540	but specifically how it works with regards to text.
3068540	3073380	I think that this is where you're going to find this textbook to be somewhat helpful.
3073380	3078820	What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did
3078820	3083860	before, but this time we're going to be installing a new model.
3083900	3092900	We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then
3092900	3097500	we're going to download the Ncore Web MD model.
3097500	3099700	This is the medium English model.
3099700	3103580	This is going to take a little longer to download, and the reason why I'm having you download
3103580	3108660	the medium model, and the reason why we're going to be using the medium model, is because
3108660	3113620	the medium model has stored within it word vectors.
3114380	3121340	Let's go ahead and talk a little bit about what word vectors are and how they're useful.
3121340	3124700	So word vectors are word embeddings.
3124700	3132820	So these are numerical representations of words in multi-dimensional space through matrices.
3132820	3135620	That's a very compacted sentence.
3135620	3137380	So let's break it down.
3137380	3139260	What are word vectors used for?
3139260	3146220	Well, they're used for a computer system to understand what a word actually means.
3146220	3149740	So computers can't really parse text all that efficiently.
3149740	3151660	They can't parse it at all.
3151660	3155660	Every word needs to be converted into some kind of a number.
3155660	3159540	Now for some old approaches, you would use something like a bag of words approach where
3159540	3163460	each individual word would have a corresponding number to it.
3163460	3167540	This would be a unique number that corresponds just to that word.
3167540	3173660	There are a lot of tasks that can work, but for something like text understanding or trying
3173660	3179780	to get a computer system to be able to understand how a word functions within a sentence in general,
3179780	3184700	in other words, how it works in the language, how it relates to all other words, that doesn't
3184700	3186860	really work for us.
3186860	3191060	So what a word vector is, is it's a multi-dimensional representation.
3191060	3195780	So instead of a number having just a single integer that corresponds to it, it instead
3195820	3202100	has what looks like to an unsuspecting eye, essentially.
3202100	3208300	It has a very complex sequence of floating numbers that are stored as an array, which
3208300	3214580	is a computationally less expensive form of a list in Python or just computing in general.
3214580	3216460	And this is what it looks like, a long sequence.
3216460	3222700	In this case, I believe it's a 300-dimensional word that corresponds to a specific word.
3222700	3227860	So this is what an array or a word vector or a word embedding looks like.
3227860	3233540	What this means to a computer system is it means syntactical and semantical meaning.
3233540	3237780	So the way word vectors are typically trained is, oh, there's a few different approaches,
3237780	3243220	but kind of the old-school word-to-vec approach is you give a computer system a whole bunch
3243220	3249260	of texts and different smaller, larger collections of texts, and what it does is it reads through
3249300	3255820	all of them and figures out how words are used in relation to other words.
3255820	3261060	And so what it's able to essentially do through this training process is figure out meaning.
3261060	3265940	And what that meaning allows for a computer system to do is understand how a word might
3265940	3271860	relate to other words within a sentence or within a language as a whole.
3271860	3276060	And in order to understand this, I think it's best if we move away from this textbook and
3276060	3280620	actually try to explore what word vectors look like in spacey.
3280620	3285300	So you can have a better sense of specifically what they do, why they're useful, and how
3285300	3291580	you, as a NLP practitioner, can go ahead and start leveraging them.
3291580	3294780	So just like before, we're going to create an NLP object.
3294780	3299820	This time, however, instead of loading in our Encore Web SM, we're going to load in
3299820	3302780	our Encore Web MD.
3302780	3307820	So the one that actually has these word vectors stored, the static vectors saved, and it's
3307820	3309060	going to be a larger model.
3309060	3311300	Let's go ahead and execute that cell.
3311300	3314940	And while that's executing, we're going to start opening up our text.
3314940	3323500	So we're going to say with open data wiki underscore us.txt, r as f, and we're going
3323500	3328460	to say text is equal to f.read, so we're going to successfully load in that text file and
3328460	3329460	open it up.
3329540	3333980	Then we're going to create our doc object, which will be equal to NLP text.
3333980	3337300	All the syntax is staying the exact same.
3337300	3340620	And just like before, let's grab the first sentence.
3340620	3347540	So we're going to convert our doc.sense generator into a list, and we're going to grab index
3347540	3348540	zero.
3348540	3353060	And let's go ahead and print off sentence one, just so you can kind of see it.
3353060	3354720	And there it is.
3354720	3360000	So now that we've got that kind of in memory, we can start kind of working with it a little
3360000	3361000	bit.
3361000	3366400	So let's go ahead and just start tackling how we can actually use word vectors with
3366400	3368080	spacey.
3368080	3372720	So let's kind of think about a general question right now.
3372720	3382800	Let's say I wanted to know how the word, let's say country is similar to other words within
3382800	3385080	our model's word embeddings.
3385080	3387840	So let's create a little way we can do this.
3387840	3395040	We're going to say your word, and this is going to be equal to the word country, country.
3395040	3396040	There we go.
3396040	3400280	And what we can do is we can say MS is equal to NLP.
3400280	3402600	So we're going to go into that NLP object.
3402600	3410440	We're going to grab the vocab.vectors, and we're going to say most similar.
3410440	3412440	And this is a little complicated way of doing it.
3412440	3415520	In fact, I'm going to go ahead and just kind of copy and paste this in.
3415520	3421520	You have the code already in your textbook that you can follow along with.
3421520	3430960	And I'm going to go ahead and just copy and paste it in right here and print off this.
3430960	3435880	And what this is going to do is it is going to go ahead and just do this entirely.
3435880	3438680	There we go.
3439680	3445520	And we have to import numpy as MP.
3445520	3448760	This lets us actually work with the data as a numpy array.
3448760	3454040	And when we execute this cell, what we get is an output that tells us all the words
3454040	3457200	that are most similar to the word country.
3457200	3461520	So in this scenario, the word country, it has these kind of all these different similar
3461520	3467440	words to it from the word country to the word country, capitalized nation, nation.
3467440	3469960	Now it's important to understand what you're seeing here.
3469960	3476440	What you're seeing is not necessarily a synonym for the word country, rather what you're seeing
3476440	3480160	is are the words that are the most similar.
3480160	3486560	Now this can be anything from a synonym to a variant spelling of that word to something
3486560	3489720	that occurs frequently alongside of it.
3489720	3494640	So for example, world, while this isn't the same, we would never consider world to be
3494640	3496720	the synonym of country.
3496720	3501640	But what happens is, is syntactically they're used in very similar situations.
3501640	3505440	So the way you describe a country is sometimes the way you would describe your world, or
3505440	3510120	maybe it's something to do with the hierarchy, so a country is found within the world.
3510120	3511520	This is a good way to understand it.
3511520	3519560	So it's always good to use this word as most similar, not to be something like synonym.
3519560	3523520	So when you're talking about word vectors similarity, you're not talking about synonym
3523520	3524520	similarity.
3525400	3527200	But this is a way you can kind of quickly get a sense.
3527200	3529440	So what does this do for you?
3529440	3531960	Why did I go through and explain all these things about word vectors?
3531960	3536240	If I'm not going to be talking about machine learning a whole bunch throughout this video.
3536240	3539560	Well, I did it so that you can do one thing that's really important.
3539560	3543040	And that's calculate document similarity in the spacey.
3543040	3545680	So we've already got our NLP model loaded up.
3545680	3547560	Let's create one object.
3547560	3551400	So we're going to make doc one, we're going to make that equal to NLP.
3551400	3554080	And we're going to create the text right here in this object.
3554080	3558120	So let's say this is coming straight from the spacey documentation.
3558120	3563600	I like salty fries and hamburgers.
3563600	3566760	And we're going to say doc two is equal to NLP.
3566760	3573040	And this is going to be the text fast food tastes very good.
3573040	3576320	And now what we can do is let's go ahead and load those into memory.
3576320	3580920	What we can do is we can actually make a calculation using spacey to find out how similar they
3580920	3583600	actually are these two different sentences.
3583640	3589000	We can say print off doc one, and we're going to say this again, this is coming straight
3589000	3593240	from the spacey documentation doc two, so you're going to be able to see what both documents
3593240	3594240	are.
3594240	3597080	And then we're going to do doc one dot similarity.
3597080	3602120	So we can go into the doc one dot similarity method and we can compare it to doc two.
3602120	3603920	We can print that off.
3603920	3609120	So what we're seeing here on the left is document one, this little divider thing that we printed
3609120	3610240	off here.
3610240	3615400	On the right, we have document two, and then we can see the degree of similarity between
3615400	3617840	document one and document two.
3617840	3618840	Let's create another doc object.
3618840	3622920	We're going to call this NLP doc three, and we're going to make this NLP.
3622920	3625240	Let's come up with a sentence that's completely different.
3625240	3632840	The Empire State Building is in New York.
3632840	3636640	So this is when I'm just making up off the top of my head right now.
3636640	3642240	I'm going to copy and paste this down, and we're going to compare this to doc one.
3642240	3646160	We're going to compare it to doc three, and we get a score of point five one.
3646160	3650400	So this is less similar to than these two.
3650400	3652880	So this is a way that you can take a whole bunch of documents.
3652880	3656760	You can create a simple for loop, and you can find and start clustering the documents
3656760	3659880	that have a lot of overlap or similarity.
3659880	3661760	How is this similarity being calculated?
3661760	3666080	Well, it's being calculated because what spacey is doing is it's going into its word
3666080	3671360	embeddings, and even though in these two situations, we're not using the word fast
3671360	3674000	food ever in this document.
3674000	3679600	It's going in and it knows that salty fries and hamburgers are probably in a close cluster
3679600	3686240	with the biogram or a token that's made up of two words, a biogram of fast food.
3686240	3691240	So what it's doing is it's assigning a prediction that these two are still somewhat similar,
3691240	3696600	more similar than these two, because of these overlapping in words.
3696600	3701140	So let's try one more example, see if we can get something that's really, really close.
3701140	3709840	So let's take doc four, and this is going to be equal to NLP, I enjoy oranges.
3709840	3715480	And then we're going to have doc five is going to be equal to NLP, I enjoy apples.
3715480	3720400	So two, I would agree, I would argue very, very syntactically similar sentences.
3720400	3725200	And we're going to do doc four here, doc five here, and we're going to look and see
3725200	3728000	a similarity between doc four and doc five.
3728000	3733000	And if we execute this, we get a similarity of 0.96.
3733000	3734600	So this is really high.
3734600	3738600	This is telling me that these two sentences are very similar, and it's not just that they're
3738600	3745160	similar because of the similar syntax here, that's definitely pushing the number up.
3745160	3749200	It's that what the individual is liking in the scenario between these two texts, they're
3749200	3750720	both fruits.
3750720	3751720	Let's try something different.
3751720	3753920	Let's make doc five.
3753920	3762240	Let's just make doc six here, and do something like this NLP, I enjoy, what's another word
3762240	3764800	we could say.
3764800	3769920	Something that's different, let's say burgers, something different from a fruit.
3769920	3774080	So we're going to make doc six like that, and we're going to again copy and paste this
3774080	3781400	down, copy and paste this down, we're going to put doc six here.
3781400	3783000	And we see this drop.
3783000	3788000	So what this demonstrates, and I'm really glad this worked because I improvised this,
3788000	3794840	what this demonstrates is that the similarity, the number that's given is not dependent on
3794840	3803080	the contextual words, rather it's dependent upon the semantic similarity of the words.
3803080	3811160	So apples and oranges are in a similar cluster around fruit because of their word embeddings.
3811160	3817720	The word burgers while still being food and still being plural is different from apples
3817720	3818960	and oranges.
3818960	3822960	So in other words, this similarity is being calculated based on something that we humans
3822960	3829000	would calculate difference in meaning based on a large understanding of a language as
3829000	3830160	a whole.
3830160	3834520	That's where word vectors really come into play.
3834520	3837240	This allows for you to calculate other things as well.
3837240	3842720	So you could even calculate the difference between salty fries and hamburgers, for example,
3842720	3847840	I've got this example ready to go in the textbook, let's go ahead and try this as well.
3847840	3853880	So we're going to grab doc one, and print off these few things right here.
3853880	3858400	So we're going to try to calculate the similarity between french fries and burgers and what
3858400	3863360	we get is a similarity of 0.73.
3863360	3868880	So if we were to maybe change this up a little bit and try to calculate the similarity between
3868880	3877560	maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity.
3877560	3882000	So my point is, is play around with the similarity calculator, play around with the structure,
3882000	3887920	the code I provided here, and get familiar with how spacey can help you kind of find
3887920	3892280	a similarity, not just between documents, but between words as well.
3892280	3895960	And we're going to be seeing how this is useful later on.
3895960	3899840	But again, it's good to be familiar with kind of generally how machine learning kind of
3899840	3905200	functions here in this context, and why these medium and large models are so much bigger.
3905200	3910000	They're so much bigger because they have more word vectors that are much deeper.
3910000	3915080	And the transformer model is much larger because it was trained in a completely different method
3915080	3917680	than the way the medium and large models were trained.
3917680	3922360	But again, that's out of the scope for this video.
3922360	3928120	I now want to turn to really the last subject of this introduction to spacey part one, which
3928120	3932120	is when we're taking this large umbrella view of the spacey.
3932120	3935480	And in the textbook, it's going to correspond to chapter four.
3935480	3941320	So what we go over in this textbook is kind of a large view of not just the dot container
3941320	3946760	and the word vectors and the linguistic annotations, but really kind of the structure of the spacey
3946880	3950240	framework, which comes around the pipeline.
3950240	3956280	So a pipeline is a very common expression in computer science and in data science.
3956280	3959680	Think of it as a traditional pipeline that you would see in a house.
3959680	3964760	Now think of a pipeline being a sequence of different pipes.
3964760	3971200	Each pipe in a computer system is going to perform some kind of permutation or some action
3971200	3976280	on a piece of data as it goes through the pipeline.
3976280	3981920	And as each pipe has a chance to act and make changes to and additions to that data,
3981920	3985440	the later pipes get to benefit from those changes.
3985440	3988320	So this is very common when you're thinking about logic of code.
3988320	3992240	I provide it like a little image here that I think maybe might help you.
3992240	3998600	So if we imagine some input sentence, right, so some input text is entering a spacey pipeline,
3998600	4001440	it's going to go through a bunch of things if you're working with the medium model or
4001480	4007280	the small model, that'll tokenize it and give it a word and vector for different words.
4007280	4013000	It'll also find the POS, the part of speech, the dependency parser will act on it.
4013000	4020040	But it might eventually get to an entity ruler, which we're going to see in just a few minutes.
4020040	4025400	The entity ruler will be a series of rules-based NER named entity recognition.
4025400	4029880	So it'll maybe assign a token to an entity.
4029880	4033320	Might be the beginning of an entity, might be the end of an entity,
4033320	4035960	might just be an individual token entity.
4035960	4041960	And then what will happen is that doc object, as it kind of goes through this pipeline,
4041960	4045360	will now receive a bunch of doc.ins.
4045360	4051600	So it'll be, this pipe will actually add to the doc object as it goes through the pipeline,
4051600	4053240	the entity component.
4053240	4058120	And then the next pipeline, the entity linker, might take all those entities and try to find out
4058120	4059400	which ones they are.
4059400	4065120	So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number
4065120	4067320	that corresponds to a specific person.
4067320	4072960	So for example, if you were seeing a bunch of things like Paul something, Paul something,
4072960	4076920	maybe that one Paul something might be Paul Hollywood from the Great British Bake Off,
4076920	4080520	and it might have to make a connection to a specific person.
4080520	4085920	So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood,
4085920	4087720	depending on the context.
4087840	4093120	That's out of the scope of this video series, but keep in mind that that pipe would do something
4093120	4097680	else that would modify the ints that would give them greater specificity.
4097680	4102200	And then what you'd be left with is the doc object on the output that not only has entities
4102200	4107200	annotated, but it's also got entities linked to some generic specific data.
4107200	4109360	So that's going to be how a pipeline works.
4109360	4111520	And this is really what spacey is.
4111520	4115640	It's a sequence of pipes that act on your data.
4115640	4119640	And that's important to understand, because it means that as you add things to a spacey
4119640	4125160	pipeline, you need to be very conscientious about where they're outed and in what order.
4125160	4128720	As we're going to see as we move over to kind of rules based spacey, when we start talking
4128720	4134560	about these different pipes, the entity ruler, the matcher custom components, regex components,
4134560	4136760	you're going to need to know which order to put them in.
4136760	4138800	It's going to be very important.
4138800	4141000	So do please keep that in mind.
4141000	4145400	Now spacey has a bunch of different attribute rulers or different pipes that you can kind
4145400	4146960	of add into it.
4146960	4151000	You've got dependency parsers that are going to come standard with all of your models.
4151000	4155040	You've got the entity linker entity, recognizer entity ruler, you're going to have to make
4155040	4157840	these yourself and add them in oftentimes.
4157840	4159000	You've got a limitizer.
4159000	4162440	This is going to be on most of your standard models, your morphologue, that's going to
4162440	4165960	be on on there as well, sentence recognizer, synthesizer.
4165960	4171520	This is what allow for you to have the doc.sense right here span categorizer.
4171520	4177320	This will help categorize different spans, be them single token spans or sequence of
4177320	4182560	token spans, your tagger, this will tag the different things in your text, which will
4182560	4185160	help with part of speech, your text categorizer.
4185160	4189200	This is when you train a machine learning model to recognize different categories of
4189200	4190200	a text.
4190200	4195760	So text classification, which is a very important machine learning task, tote to VEC.
4195760	4201440	This is going to be what assigns word embeddings to the different words in your doc object.
4201440	4206760	Organizer is what breaks that thing up and all your text into individual tokens.
4206760	4209800	And you've got things like transformer and trainable pipes.
4209800	4213160	Then within this, you've also got some other things called matchers.
4213160	4214840	So you can do some dependency matching.
4214840	4217000	We're not going to get into that in this video.
4217000	4220120	You've also got the ability to use matcher and phrase matcher.
4220120	4224720	These are a lot of the times can do some similar things, but they're executed a little differently
4224720	4225960	to make things less confusing.
4225960	4229680	I'm really only talking about the matcher of these two.
4229800	4233880	If there's a need for it, I'll add into the textbook the phrase matcher at a later date,
4233880	4235760	but I'm not going to cover it in this video.
4235760	4239880	And if I do add in the phrase matcher, it's going to be after this matcher section here.
4239880	4241440	I have it in the GitHub repo.
4241440	4245200	I just haven't included in the textbook to keep things a little bit simpler, at least
4245200	4246880	if you're just starting out.
4246880	4252280	So a big good question is, well, how do you add pipes to a spacey pipeline?
4252280	4253680	So let's go ahead and do that.
4253680	4257440	We're going to make a blank spacey pipeline right now.
4257440	4262800	Let's go ahead and just make, we'll just work with the same live coding notebook that we
4262800	4264080	have open right now.
4264080	4267520	So what we're going to do is we're going to make a blank model, and we're going to actually
4267520	4274120	add in our own sentenizer to our, to our text.
4274120	4276240	So let's go ahead and do that.
4276240	4280680	So I'm going to say NLP is equal to a spacey dot blank.
4280680	4284440	This is going to allow for me to make a blank spacey pipeline.
4284440	4288640	And I'm going to say Ian so that it knows that the tokenizer that I need to use is the
4288640	4290640	English tokenizer.
4290640	4296040	And now if I want to add a pipe to that, I can use one of the built-in spacey features.
4296040	4300880	So I can say add underscore pipe, and I can say sentenizer.
4300880	4303400	So I can add in a sentenizer.
4303400	4309160	This is going to allow for me to create a pipeline now that has a sequence of two different
4309160	4310160	pipes.
4310160	4313560	And I demonstrate in the textbook why this is important.
4313560	4318960	Sometimes what you need to do is you need to just only break down a text into individual
4318960	4319960	sentences.
4319960	4328080	So I grabbed a massive, massive corpus from the internet, which is on MIT.edu.
4328080	4330560	And it's the entire Shakespeare corpus.
4330560	4335320	And I just try to calculate the, the quantity of sentences found within it.
4335320	4342760	There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and
4342760	4346040	count those sentences with the spacey model.
4346040	4352080	Using the small model, however, it took a total amount of time of 47 minutes to actually
4352080	4355520	break down all those sentences and extract them.
4355520	4359600	Why is there a difference in time between 7 seconds and 47 minutes?
4359600	4364320	It's because that this spacey small model has a bunch of other pipes in it that are
4364320	4366520	trying to do a bunch of other things.
4366520	4374600	If you just need to do one task, it's always a good idea to just activate one pipe or maybe
4374600	4379240	make a blank model and just add that single pipe or the only pipes that you need to it.
4379240	4384120	A great example of this is needing to tokenize a whole bunch of sentences in relatively short
4384120	4385120	time.
4385120	4391280	So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.
4391280	4392840	However comes at a trade-off.
4392840	4398640	The small model is going to be more accurate in how it finds sentence boundaries.
4398640	4401080	So we have a difference in quantity here.
4401080	4406240	This difference in quantity indicates that this one messed up and made some mistakes because
4406240	4407960	it was just the sentenceizer.
4407960	4411760	The sentenceizer didn't have extra data being fed to it.
4411760	4416560	In fact, if I probably used larger models, I might even have better results.
4416560	4417760	But always think about that.
4417760	4422400	If time is of the essence and you don't care so much about accuracy, a great way to get
4422400	4426800	the quantity of sentences or at least a ballpark is to use this method where you simply add
4426800	4430120	in a sentenceizer to a blank model.
4430120	4436240	So that's how you actually add in different pipes to a spacey pipeline and we're going
4436240	4440760	to be reinforcing that skill as we go through, especially in part two, where we really kind
4440760	4443200	of work with this in a lot of detail.
4443200	4448800	Right now I'm just interested in giving you the general understanding of how this might
4448800	4449800	work.
4449800	4456240	Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes.
4456240	4460160	We can analyze what our analyze, there we go.
4460160	4462460	We can actually analyze our pipeline.
4462460	4467600	If we look at the NLP object, which is our blank model with the sentenceizer, we see
4467600	4475760	that our NLP pipeline ignore summary, ignore this bit here.
4475800	4479840	But what you're actually able to kind of go through and see right away is that we've really
4479840	4482640	just got the sentenceizer sitting in it.
4482640	4488520	If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey
4488520	4495920	dot load and core web SM, we're going to create that NLP two object around the small spacey
4495920	4499760	English model.
4499760	4506760	We can analyze the pipes again, and we see a much more elaborate pipeline.
4506760	4507760	So what are we looking at?
4507760	4512400	Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger
4512400	4518880	after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got
4518880	4523520	an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed
4523520	4526400	the doc dot ends, and we keep on going down.
4526400	4531240	We can see the limitizer, but we can see also a whole bunch of other things.
4531240	4534840	We can see what these different things actually assign.
4534840	4542000	So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually
4542000	4543120	require.
4543120	4548040	So if we look up here, we see that the NER pipe, so the name to the recognition pipe
4548040	4551280	is responsible for assigning the doc dot ends.
4551280	4556080	So that attribute of the doc object, and it's also responsible at the token level for
4556080	4562480	assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes
4562480	4570080	ago when we talked about the IOB being the opening beginning or out beginning inside
4570080	4576720	for a different entity, it also assigns the end dot end underscore type for each token
4576720	4577720	attribute.
4577720	4583240	So you can see a lot of different things about your pipeline by using NLP dot analyze underscore
4583240	4584880	pipes.
4584880	4589800	If you've gotten to this point in the video, then I think you should by now have a good
4589800	4596040	really umbrella view of what spacey is, how it works, why it's useful.
4596040	4599560	And some of the basic features that it can do and how it can solve some pretty complex
4599560	4603320	problems with some pretty simple lines of code.
4603320	4608280	What we're going to see now moving forward is how you as a practitioner of NLP cannot
4608280	4612920	just take what's given to you with spacey, but start working with it and start leveraging
4612920	4615200	it for your own uses.
4615200	4617280	So taking what is already available.
4617280	4622120	So like these models like the English model and adding to them contributing to them.
4622120	4627000	Maybe you want to make an entity ruler where you can find more entities in a text based
4627000	4630120	on some cousin tier or list that you have.
4630120	4634280	Maybe you want to make a matcher so you can find specific sequences within a text.
4634280	4636880	Maybe that's important for information extraction.
4636880	4640600	Maybe you need to add custom functions or components into your spacey pipeline.
4640600	4644920	I'm going to be going through in part two rules based spacey and giving you all the
4644920	4651160	basics of how to do some really robust custom things relatively quickly with within the
4651160	4652920	spacey framework.
4652920	4656720	All of that's going to lay the groundwork so that in part three, we can start applying
4656720	4660760	all these skills and start solving some real world problems.
4660760	4663680	In this case, we're going to look at financial analysis.
4663680	4668440	So that's going to be where we move to next is part two.
4668440	4673280	We are now moving into part two of this Jupiter book on spacey and we're going to be working
4673280	4675520	with rules based spacey.
4675520	4678440	Now this is really kind of the bread and butter of this video.
4678440	4682800	You've gotten a sense of the umbrella structure of spacey as a framework.
4682800	4685840	You've gotten a sense of what the container can contain.
4685840	4690920	You've gotten a sense of the token attributes and the linguistic annotations from part one
4690920	4693680	of this book and the earlier part of this video.
4693680	4698880	Now we're going to move into taking those skills and really developing them into custom
4698880	4703520	components and modified pipes that exist within spacey.
4703520	4707960	In other words, I'm going to show you how to take what we've learned now and start really
4707960	4712720	doing more robust and sophisticated things with that knowledge.
4712720	4716320	So we're going to be working first with the entity ruler, then with the matcher in the
4716320	4719080	next chapter, then in the components in spacey.
4719080	4723160	So a custom component is a custom function that you can put into a pipeline.
4723160	4726280	Then we're going to talk about regex or regular expressions.
4726280	4728880	And then we're going to talk about some advanced regex with spacey.
4728880	4733720	If you don't know what regex is, I'm going to cover this in chapter eight.
4733720	4739000	So let's go over to our Jupiter notebook that we're going to be using for our entity ruler
4739000	4740000	lesson.
4740000	4742680	So let's go ahead and execute some of these cells.
4742680	4745600	And then I'm going to be talking about it in just a second.
4745600	4750960	First I want to take some time to explain what the entity ruler is as a pipe in spacey,
4750960	4755200	what it's used for, why you'd find it useful and when to actually implement it.
4755200	4759840	So there are two different ways in which you can kind of add in custom features to a spacey
4759840	4761320	language pipeline.
4761320	4765600	There is a rules based approach and a machine learning based approach.
4765600	4769320	Rules based approaches should be used when you can think about how to generate a set
4769320	4775680	of rules based on either a list of known things or a set of rules that can be generated through
4775680	4779680	regex, code or linguistic features.
4779680	4784160	Machine learning is when you don't know how to actually write out the rules or the rules
4784160	4787800	that you would need to write out would be exceptionally complicated.
4787800	4791920	A great example of a rules based approach versus a machine learning based approach and when
4791920	4797080	to use them is with entity types for named entity recognition.
4797080	4801760	Imagine if you wanted to extract dates from a text.
4801760	4806200	There are a finite, very finite number of ways that a date can appear in a text.
4806200	4811960	You could have something like January 1, 2005, you could have one January 2005, you could
4811960	4818840	have one Jan 2005, you could have one slash five slash 2005, there's there's different
4818840	4821320	ways that you can do this and there's a lot of them.
4821320	4825400	But there really is a finite number that you could easily write a regex expression for
4825400	4829600	a regular expression for to capture all of those.
4829600	4833240	And in fact, those regex expressions already exist.
4833240	4837360	That's why spacey is already really good at identifying dates.
4837360	4843040	So dates are something that you would probably use a rules based approach for something that's
4843040	4847880	a good machine learning approach for or something like names.
4847880	4853880	If you wanted to capture the names of people, you would have to generate an entity ruler
4853880	4857680	with a whole bunch of robust features.
4857680	4862840	So you would have to have a list of all known possible first names, all known possible last
4862920	4869920	names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc.
4870320	4872600	And you'd have to have a list of all known suffixes.
4872600	4877320	So junior, senior, the third, the fourth, etc. on down the list.
4877320	4882520	This would be very, very difficult to write because first of all, the quantity of names
4882520	4885440	that exist in the world are massive.
4885440	4889440	The quantity of last names that exist in the world is massive.
4889440	4894040	There's not a set gazetteer or set list out there of these anywhere.
4894040	4899040	So for this reason, oftentimes things like people names will be worked into machine learning
4899040	4900040	components.
4900040	4904040	I'm going to address machine learning in another video at a later date, but right now we're
4904040	4907320	going to focus on a rules based approach.
4907320	4914320	So using the rules based features that spacey offers, a good NLP practitioner will be excellent
4915000	4920120	at both rules based approaches and machine learning based approaches and knowing when
4920120	4926920	to use which approach and when maybe maybe a task is not appropriate for machine learning
4926920	4930800	when it can be worked in with rules relatively well.
4930800	4935560	If you're taking a rules based approach, the approach that you take should have a high
4935560	4940640	degree of confidence that the rules will always return true positives.
4940640	4942400	And you need to think about that.
4942400	4947640	If you are okay with your rules, maybe catching a few false positives or missing a few true
4947640	4952800	positives, then maybe think about how you write the rules and allowing for those and
4952800	4955040	making it known in your documentation.
4955040	4959760	So that's generally what a rules based approach is in an entity ruler is a way that we can
4959760	4966760	use a list or a series of features, language features to add tokens into the entity, the
4967320	4971060	dot ints container within the dot container.
4971060	4973940	So let's go ahead and try to do this right now.
4973940	4977180	The text we're going to be working with is a kind of fun one, I think.
4977180	4981820	So if you've already gotten the reference, congratulations, it's kind of obscure.
4981820	4984540	But we're going to have a sentence right here that I just wrote out.
4984540	4988180	West Chesterton Fieldville was referenced in Mr. Deeds.
4988180	4991180	So in this context, we are going to have a few different entities.
4991180	4996980	We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.
4996980	4999020	It's a fake place that doesn't really exist.
4999020	5001380	It was made up in the movie Mr. Deeds.
5001380	5005180	And what we want is for Mr. Deeds to be grabbed as an entity as well.
5005180	5008100	And this would ideally be labeled as a film.
5008100	5010780	But in this case, that's probably not going to happen.
5010780	5012740	Let's go ahead and see what does happen.
5012740	5019580	So we're going to say for int and doc dot ints, print off int dot text, int dot label,
5019580	5024020	like we learned from our NER lesson a few moments ago.
5024020	5026100	And we see that the output looks like this.
5026100	5028700	It's gotten almost all the entities that we wanted.
5028700	5030940	Mr. was left off of Deeds.
5030940	5034620	And it's grabbed the West Chesterton Fieldville and labeled it as a person.
5034620	5036020	So what's gone wrong here?
5036020	5038180	Well, there's a few different things that have gone wrong.
5038180	5042980	The NCORE Web SM model is a machine learning model for NER.
5042980	5045140	The word vectors are not saved.
5045140	5047860	So the static vectors are not in it.
5047860	5050020	So it's making the best prediction that it can.
5050020	5054780	But even with a very robust machine learning model, unless it has seen West Chesterton
5054780	5061180	Fieldville, there is not really a good way for the model to actually know that that's
5061180	5062700	a place.
5062700	5068740	Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer
5068740	5071500	model might actually get this right.
5071500	5073540	But for the most part, this is a very challenging thing.
5073540	5074900	This would be challenging for a human.
5074900	5079620	There's not a lot of context here to tell you what this kind of entity is, unless you
5079620	5088540	knew a lot about how maybe northeastern villages and towns in North America would be called.
5088540	5094020	Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.
5094020	5099420	Now ideally, we would have an NER model that would label West Chesterton Fieldville as
5099420	5101940	a GPE and Mr. Deeds as a film.
5101940	5103660	But we've got two problems.
5103660	5108460	One, the machine learning model doesn't have film as an entity type.
5108460	5113980	And on top of that, West Chesterton Fieldville is not coming out correct as GPE.
5113980	5118820	So our goal right now is to fix both of these problems with an entity ruler.
5118820	5124300	This would be useful if I were maybe doing some text analysis on fictional places referenced
5124300	5125460	in films.
5125460	5129700	So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all
5129700	5131820	be classified as kind of fictional places.
5131820	5136660	So let's go ahead and make a ruler to correct this problem.
5136660	5142100	So what we're going to do is first we're going to make a ruler by saying ruler is equal
5142100	5146920	to NLP dot add pipe.
5146920	5150620	And this is going to take one argument here, you're going to find out when we start working
5150620	5154300	with custom components that you can have a few different arguments here, especially
5154300	5156300	if you create your own custom components.
5156300	5160300	But for right now, we're working with the components that come standard with spacey.
5160300	5161860	There's about 18 of them.
5161860	5167220	One of them is the entity underscore ruler, all lowercase.
5167220	5170980	We're going to add that ruler into our NLP model.
5170980	5177980	And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER
5177980	5185780	model and see as we go down that the NER pipe is here and the entity ruler is now the exit,
5185780	5187740	the final pipe in our pipeline.
5187740	5190660	So we see that it has been successfully added.
5190660	5195540	Let's go ahead now and try to add patterns into that pipeline.
5195540	5199780	Patterns are the things that the spacey model is going to look for in the label that it's
5199780	5203300	going to assign when it finds something that meets that pattern.
5203300	5206260	This will always be a list of lists.
5206260	5208140	So let's go ahead and do this right now.
5208140	5210100	Sorry, a list of dictionaries.
5210100	5217380	So the first pattern that we're really looking for here is going to be a dictionary.
5217380	5224340	It's going to have one key of label, which is going to be equal to GPE and another label
5224340	5230060	of pattern, which is going to be equal to, in this case, we want to find West Chesterton
5230060	5231060	Fieldville.
5231060	5239020	Let me go ahead and just copy and paste it so I don't make a mistake here.
5239020	5243620	And what we want to do is we want our entity ruler to see West Chesterton Fieldville.
5243620	5246380	And when it sees it, assign the label of GPE.
5246380	5247940	So it's a geopolitical entity.
5247940	5249740	So it's a place.
5249740	5251340	So let's go ahead and execute that.
5251340	5252340	Great.
5252340	5253340	We've got the patterns.
5253340	5255740	Now comes time to load them into the ruler.
5255740	5260020	So we can say ruler.add underscore patterns.
5260020	5261340	This is going to take one argument.
5261340	5266380	It's going to be our list of patterns added in.
5266380	5267540	Cool.
5267540	5269020	Now let's create a new doc object.
5269020	5272540	We're going to call this doc to that's going to be equal to NLP.
5272540	5275700	We're going to pass in that same text.
5275700	5283780	We're going to say for int n doc to dot ints print off int dot text and end dot label.
5283780	5288940	And you're going to notice that nothing has changed.
5288940	5290700	So why has nothing changed?
5290700	5292660	We're still getting the same results.
5292660	5294780	And we've added the correct pattern in.
5294780	5297460	The answer lies into one key thing.
5297460	5303340	If we look back up here, we see that our entity ruler comes after our NER.
5303340	5304340	What does that mean?
5304340	5308060	Well, imagine how the pipeline works that I talked about a little while ago in this
5308060	5309060	video.
5309060	5314420	A pipeline works by different components, adding things to an object and making changes
5314420	5321460	to it, in this case, adding ints to it, and then making those things isolated from later
5321460	5324940	pipes from being able to overwrite them unless specified.
5324940	5329860	What this means is that when West Chesterton field bill goes through and is identified
5329860	5337020	by the NER pipe as a person, it can no longer be identified as anything else.
5337020	5341520	What this means is that you need to do one of two things give your ruler the ability
5341520	5349160	to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline.
5349160	5353220	So let's go through and solve this common problem right now.
5353220	5358460	We're going to create a new NLP object called NLP to, which is going to be equal to spacey
5358460	5359460	dot load.
5359460	5366220	And again, we're going to load in the English core web SM's model and core web SM.
5366220	5368060	Great.
5368060	5381540	And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going
5381540	5385420	to make that an object too.
5385420	5389980	Now what we can do is we can say ruler dot add patterns, again, we're going to go through
5389980	5393060	all of these steps that we just went through, we're going to add in those patterns that
5393060	5394940	we created up above.
5394940	5398940	And now what we're going to do is we're going to actually do one thing a little different
5398940	5400380	than what we did.
5400380	5404440	What we're going to do is we're going to load this up again, and we're going to do an extra
5404440	5405820	keyword argument.
5405820	5411620	Now we can say either after or before here, we're going to say before NER, what this is
5411620	5418540	going to do is it's going to place our NER before our entity will ever for the NER component.
5418540	5424820	And now when we add our patterns in, we can now create a new doc object.
5424820	5432300	Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print
5432300	5437340	off int dot text, and dot label.
5437460	5441780	Now we notice that it is correctly labeled as a GPE.
5441780	5442780	Why is this?
5442780	5450540	Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will
5450540	5455220	notice that our entity ruler now in the pipeline sits before the NER model.
5455220	5460100	In other words, we've given primacy to our custom entity ruler, so that it's going to
5460100	5464580	have the first shot at actually correctly identifying these things, but we've got another
5464580	5466060	problem here.
5466060	5474260	This is coming out as a person, it should be Mr. Deeds as the entire collective multi
5474260	5476660	word token, and that should be a new entity.
5476660	5481660	We can use the entity ruler to add in custom types of labels here.
5481660	5484900	So let's go ahead and do this same thing.
5484900	5491220	Let's go ahead and just copy and paste our patterns, and we're going to create one more
5491380	5499300	NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM.
5499300	5502020	Great, we've got that loaded up.
5502020	5508260	We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to
5508260	5514860	NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it
5514860	5521620	before the NER pipe, NLP three, there we go.
5521620	5524820	And what we need to do now is we need to copy in these patterns, and we're going to add
5524820	5526620	in one more pattern.
5526620	5528580	Remember this can be a list here.
5528580	5535020	So this pattern, we're going to have a new label called film, and we're going to look
5535020	5540180	for the sequence Mr. Deeds.
5540180	5543940	And that's going to be our pattern that we want to add in to our ruler.
5544020	5550060	So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember
5550060	5554980	that one keyword argument, or one argument is going to be the list itself.
5554980	5559380	And now we can create a new doc object, which is going to be equal to NLP three, I think
5559380	5568260	I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text
5568260	5571060	and and dot label.
5571060	5575860	And if we execute this, we see now that not only have you gotten the entity ruler to correctly
5575860	5582940	identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly,
5582940	5584980	Mr. Deeds as a film.
5584980	5588580	Now some of you might be realizing the problem here, this is actually a problem for machine
5588580	5589580	learning models.
5589580	5594180	And the reason for this is because Mr. Deeds in some instances could be the person and
5594180	5597980	Mr. Deeds in other instances could be the movie itself.
5597980	5600220	This is what we would call a toponym.
5600220	5604020	So spelled like this, and this is a common problem in natural language processing.
5604020	5608020	And it's actually one of the few problems or one of many problems really, that remain
5608020	5615420	a little bit unsolved toponym resolution, spelled like this, or TR is the resolution
5615420	5616420	of toponym.
5616420	5621260	So things that can have multiple labels that are dependent upon context.
5621260	5626860	Another example of toponym resolution is something like this, if you were to look at this word,
5626860	5631460	let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology.
5631460	5633940	Let's say it's only going to ever be a GPE.
5633940	5639700	The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas.
5639700	5645460	Toponym resolution is also the ability to resolve problems like this, when in context
5645460	5651540	is Paris was kind of talking about Paris, France, when in context is it talking about
5651540	5655580	Kentucky, and when in context is it talking about Texas.
5655580	5659500	So that's something that you really want to think about when you're generating your
5659500	5665540	rules for an entity ruler, is this ever going to be a false positive?
5665540	5670180	And if the answer is that it's going to be a false positive half the time, or it's a
5670180	5676100	50-50 shot, then really consider incorporating that kind of an entity into a machine learning
5676100	5681740	model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as
5681740	5682740	a person.
5682740	5688460	And learn with word embeddings when that context means it's a film and when that context means
5688460	5689460	it's a person.
5689460	5692020	That's just a little toy example.
5692020	5695300	What we're going to see moving forward, though, and we're going to do this with a matcher,
5695300	5699620	not with the entity ruler, is that spacey can do a lot of things.
5699620	5705060	You might be thinking to yourself, now I could easily just come up with a list and just check
5705060	5709820	and see whenever Mr. Deeds pops up and just inject that into the doc.ins.
5709820	5712180	I could do the same thing with West Chesterton Field Bill.
5712180	5715500	Why do I need an NLP framework to do this?
5715500	5719780	And the answer is going to come up in just a few minutes when we start realizing that
5719780	5725740	spacey can do a lot more than things like regex or things like just a basic gazetteer
5725740	5727820	check or a list check.
5727820	5732380	What you can do with spacey is you can have the pattern not just take a sequence of characters
5732380	5738180	and look for a match, but a sequence of linguistic features as well, that earlier pipes have
5738180	5739180	identified.
5739180	5743420	And I think it's best if we save that for just a second when we start talking about
5743420	5749140	the matcher, which is, in my opinion, one of the more robust things that you can do
5749140	5754860	with spacey and what sets spacey apart from things like regex or other fancier string
5754860	5756860	matching approaches.
5756860	5763180	Okay, we're now moving into chapter six of this book, and this is really kind of, in
5763180	5767300	my opinion, one of the most important areas in this entire video.
5767300	5771620	If you can master the techniques I'm going to show you for the next maybe 20 minutes
5771620	5776300	or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really
5776300	5780060	going to see really kind of its true power.
5780060	5784380	A lot of the stuff that we talk about here in the matcher can also be implemented in
5784380	5788500	the entity ruler as well with a pattern.
5788500	5793660	The key difference between the entity ruler and the matcher is in how data the data is
5793660	5795080	kind of extracted.
5795080	5799280	So the matcher is going to store information a little differently.
5799280	5803120	It's going to store it as within the vocab of the NLP model.
5803120	5808920	It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk
5808920	5810920	about that more in just a second.
5810920	5813360	And it's not going to store it in the doc ends.
5813360	5817360	So matchers don't put things in your doc.ends.
5817360	5821540	So when do you want to use a matcher over an entity ruler?
5821540	5826620	You want to use the entity ruler when the thing that you're trying to extract is something
5826620	5831340	that is important to have a label that corresponds to it within the entities that are coming
5831340	5832620	out.
5832620	5837300	So in my research, I use this for anything from like, let's say stocks, if I'm working
5837300	5843660	with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm
5843660	5851500	a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside
5851620	5852620	other entities.
5852620	5858260	I'll also work in things like ships, so the names of ships, streets, things like that.
5858260	5863460	When I use the the matcher, it's when I'm looking for something that is not necessarily
5863460	5871060	an entity type, but something that is a structure within the text that will help me extract
5871060	5872060	information.
5872060	5875500	And I think that'll make more sense as we go through and I show you kind of how to improve
5875500	5881060	examples going through it, we're kind of using the matcher as you would in the real world.
5881100	5885980	But remember, all the patterns that I show you can also be implemented in the entity
5885980	5886980	ruler.
5886980	5891140	And I'm also going to talk about when we get to chapter eight, how rejects can actually
5891140	5894460	be used to do similar things, but in a different way.
5894460	5900180	Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic
5900180	5907260	components, so the lemma of a word or the identifying if the word is a specific type
5907340	5912300	of an entity, that's when you're going to want to use the matcher over rejects.
5912300	5916380	And when you're going to use rejects is when you really have a complicated pattern that
5916380	5918540	you need to extract.
5918540	5923380	And that pattern is not dependent upon specific parts of speech, you're going to see with
5923380	5927820	that how that works as we kind of go through the rest of part two, but keep that in the
5927820	5929220	back of your mind.
5929220	5934100	So let's go ahead and take our work over to our blank Jupiter notebook again.
5934100	5936780	So what we're going to do is we're going to just set up with a basic example.
5936780	5938780	We need to import spacey.
5938780	5944780	And since we're working with the matcher, we also need to say from spacey dot matcher,
5944780	5950020	import matcher with a capital M, very important capital M.
5950020	5953900	Once we have this loaded up, we can start actually working with the matcher.
5953900	5960260	And we're going to be putting the matcher in a just the small English model.
5960260	5964100	And we're going to say NLP is equal to spacey dot load.
5964100	5969580	And you should be getting familiar with this in core web SM, the small English model.
5969580	5974780	Once we've got that loaded, and we do now, we can start actually working with the matcher.
5974780	5976380	So how do you create the matcher?
5976380	5979900	Well, the Pythonic way to do this and the weights in the documentation is to call the
5979900	5984740	object a matcher, that's going to be equal to matcher with a capital M. So we're calling
5984740	5987580	this class right here.
5987580	5990980	And now what we need to do is we need to pass in one argument.
5990980	5994100	This is going to be NLP dot vocab.
5994100	5997500	We're going to see that we can add in some extra features here in just a little bit.
5997500	6000740	I'm going to show you why you want to add in extra features at this stage, but we're
6000740	6002580	going to ignore that for right now.
6002580	6007220	What we're going to try to do is we're going to try to find email addresses within a text,
6007220	6011300	a very simple task that's really not that difficult to do.
6011300	6015380	We can do it with a very simple pattern because spacey has given us that ability.
6015380	6017820	So let's create a pattern.
6017820	6025980	And that's going to be equal to a list, which is going to contain a dictionary.
6025980	6033860	The first item in the dictionary, or the first key, is going to be the thing that you're
6033860	6035060	looking for.
6035060	6038580	So in this case, we have a bunch of different things that the matcher can look for.
6038580	6040540	And I'm going to be talking about all those in just a second.
6040540	6045540	But one of them is very handily, this label of like email.
6045540	6051700	So if the if the string or the sequence of tokens or the token is looking like an email,
6051700	6058580	and that's true, then that is what we want to extract, we want to extract everything that
6058580	6059900	looks like an email.
6059900	6064540	And to make sure that this occurs, we're going to say matcher dot add.
6064540	6069180	And then here, we're going to pass in two arguments, argument one is going to be the
6069180	6073840	think of it as a label that we want to assign to it.
6073840	6078480	And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll
6078480	6080000	see in just a second.
6080000	6083520	And the next thing is a pattern.
6083520	6087200	And it's important here to note that this is a list.
6087200	6090880	The argument here takes a list of lists.
6090880	6095400	And because this is just one list right now, I'm making it into a list.
6095400	6101680	So each one of these different patterns would be a list within a list, essentially the let's
6101680	6103920	go ahead and execute that.
6103920	6109160	And now we're going to say doc is equal to nlp.
6109160	6117320	And I'm going to add in a text that I have in the textbook.
6117320	6121600	And this is my email address w mattingly at aol.com.
6121600	6122840	That might be a real email address.
6122840	6124920	I don't believe it is, it's definitely not mine.
6124920	6127360	So don't try and email it.
6127360	6131200	And then we're going to say matches is equal to matcher doc.
6131240	6133760	This is going to be how we find our matches.
6133760	6140040	We pass that doc object into our matcher class.
6140040	6143520	And now what we have is the ability to print off our matches.
6143520	6145760	And what we get is a list.
6145760	6150400	And this list is a set of tuples that will always have three indices.
6150400	6154440	So index zero is going to be this very long number.
6154440	6160600	What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook.
6160600	6164880	And the next thing is the start token and the end token.
6164880	6167600	So you might be seeing the importance here already.
6167600	6173840	What we can do with this is we can actually go into the nlp vocab where this integer lies
6173840	6176080	and find what it corresponds to.
6176080	6178000	So this is where this is pretty cool.
6178000	6179280	Check this out.
6179280	6182000	So you print off nlp dot vocab.
6182000	6184720	So we're going into that vocab object.
6184720	6188600	We're going to index it matches zero.
6188600	6193800	So this is going to be the first index, so this tuple at this point.
6193800	6195960	And then we're going to grab index zero.
6195960	6198240	So now we've gone into this list.
6198240	6204480	We've gone to index zero, this first tuple, and now we're grabbing that first item there.
6204480	6210800	Now what we need to do is we need to say dot text, you need to do it right here.
6210800	6217040	And if you print this off, we get this email address, that label that we gave it up there
6217040	6222440	was added into the nlp vocab with this unique lexeme that allows for us to understand what
6222440	6227400	that number corresponds to within the nlp framework.
6227400	6234120	So this is a very simple example of how a matcher works and how you can use it to do
6234120	6236040	some pretty cool things.
6236040	6241120	But let's take a moment, let's pause and let's see what we can do with this matcher.
6241120	6246360	So if we go up into spacey's documentation on the matcher, we'll see that you got a couple
6246360	6248000	different attributes you can work with.
6248000	6250760	Now we've, we're going to be seeing this a little bit.
6250760	6254560	The orth, this is the exact verbatim of a token.
6254560	6260160	And we're also going to see text, the exact verbatim, text of a token.
6260160	6261920	What we also have is lower.
6261920	6267800	So what you can do here is you can use lower to say when the item is lowercase and it looks
6267800	6271040	like and then give some lowercase pattern.
6271040	6275720	This is going to be very useful for capturing things that might be at the start of a sentence.
6275720	6282440	For example, if you were to look for the penguin in the text, anywhere you saw the penguin.
6282440	6287760	If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the
6287760	6289040	start of a sentence.
6289040	6291560	It would miss it because the T would be capitalized.
6291560	6295640	By using lower, you can ensure that your pattern that you're giving it is going to be looking
6295640	6300360	for any pattern that matches that when the text is lowercase.
6300360	6307760	If is going to be the, the length of your token text is alpha is ASCII is digit.
6307760	6311560	This is when your characters are either going to be alphabetical ASCII characters.
6311560	6316640	So the American standard coding initiative, I can't remember what it stands for, but it's
6316640	6322920	that, I think it's 128 bit thing that America came up with when they started in coding text.
6322920	6327200	It's now replaced with UTF eight and is digit is going to look for something if it is a
6327200	6328200	digit.
6328200	6329400	So think of each of these as a token.
6329400	6334800	So if the token is a digit, then that counts in the pattern is lower is upper is title.
6334800	6336360	These should be all self explanatory.
6336360	6340800	If it's lowercase, if it's uppercase, if it's a title, so capitalized.
6340800	6344000	And if you don't understand what all of these do right now, I'm going to be going through
6344000	6347640	and showing you in just a second, just giving you an overview of different things that can
6347640	6352640	be included within the, the matcher or the entity ruler here.
6352640	6359240	So what we can also do is find something that if the token is actually the start of a sentence,
6359240	6363400	if it's like a number, like a URL, like an email, you can extract it.
6363400	6367000	And here is the main part I want to talk about because this is where you're really going
6367000	6372520	to find spacey out shines any other string matching system out there.
6372520	6377800	So what you can do is you can use the tokens, part of speech tag, morphological analysis,
6377800	6382360	dependency label, lima and shape to actually make matches.
6382360	6387380	So not just matching a sequence of characters, but matching a sequence of linguistic features.
6387380	6388660	So think about this.
6388660	6393900	If you wanted to capture all instances of a proper noun followed by a verb, you would
6393900	6396620	not be able to do that with regex.
6396620	6397720	There's not a way to do it.
6397720	6400100	You can't give regex if this is a verb.
6400100	6402320	Regex is just a string matching framework.
6402320	6406500	It's not a framework for actually identifying linguistic features, using them and extracting
6406500	6407500	them.
6407500	6412220	So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological
6412220	6416900	analysis, the depth, the lemma, et cetera.
6417540	6421940	We can actually use all those things that have gone through the pipeline and the matcher
6421940	6427460	can leverage those linguistic features and make some really cool, allow us to make really
6427460	6431380	cool patterns that can match really robust and complicated things.
6431380	6434700	And the final thing I'm going to talk about is right here, the OP.
6434700	6439340	This is the operator or quantifier and determines how often to match a token.
6439340	6440980	So there's a few different things you can use here.
6440980	6446020	There's the exclamation mark, negate the pattern, requiring it to match zero times.
6446060	6449260	So in this scenario, the sequence would never occur.
6449260	6453980	There's the question mark, make the pattern optional, allowing it to match zero or one
6453980	6459580	times require the pattern to match one or more times with the plus and the asterisk,
6459580	6463500	the thing on the shift eight, allow the pattern to match zero or more times.
6463500	6467620	There's other things as well that you can do to make this match or a bit more robust.
6467620	6471700	But for right now, let's jump into the basics and see how we can really kind of take these
6471700	6475580	and apply them in a real world question.
6475580	6480580	So what I'm going to do is I'm going to work with another data set or another piece of
6480580	6482980	data that I've grabbed off of Wikipedia.
6482980	6487100	And this is the Wikipedia article entry on Martin Luther King, Jr.
6487100	6492820	It's the opening opening few paragraphs, let's print it off and just take a quick look.
6492820	6493820	And this is what it looks like.
6493820	6495020	You can go through and read it.
6495020	6497820	We're not too concerned about what it says right now.
6497820	6501660	We're concerned about trying to extract a very specific set of patterns.
6501660	6505060	What we're interested in grabbing are all proper nouns.
6505060	6506540	That's the task ahead of us.
6506540	6512060	Somebody has asked us to take this text in, extract all the proper nouns for me, but we're
6512060	6516740	going to do a lot more and not just the proper nouns, but we want to get multi word tokens.
6516740	6523300	So we want to have Martin Luther King, Jr. extracted as one token, so one export.
6523300	6529020	So the other things that we want to have are these kind of structured in sequential order.
6529020	6534260	So find out where they appear and extract them based on their start token.
6534260	6537660	So let's go ahead and start trying to do some of these things right now.
6537660	6539180	Let's scroll down here.
6539180	6540220	Great.
6540220	6544420	So we need to create really a new NLP object now at this point.
6544420	6545420	So let's create a new one.
6545420	6550760	We're going to start working with the Ncore Web SM model.
6550760	6555300	If you're working with a different model, like the large or the transformer, you're
6555300	6557420	going to have more accurate results.
6557420	6561460	But for right now, we're just trying to do this quickly for demonstration purposes.
6561460	6566780	So again, just like before, we're creating that with NLP dot vocab.
6566780	6569060	And then we're going to create a pattern.
6569060	6571380	So this is the pattern that we're going to work with.
6571380	6578980	We want to find any occurrence of a POS part of speech that corresponds to proper noun.
6578980	6585220	That's the way in which POS labels proper nouns is prop in.
6585220	6588340	And we should be able to with that extract all proper nouns.
6588340	6594540	So we can say matcher dot add, and we're going to say proper noun.
6594540	6598060	And that's going to be our pattern.
6598060	6600820	And then what we can do just like before, we're going to create the doc object.
6600820	6604220	This is going to be NLP text.
6604220	6608300	And then we're going to say matches is equal to matcher doc.
6608300	6614420	So we're going to create the matches by passing that doc object into our matcher class.
6614420	6617860	And then we're going to print off the length of the matches.
6617860	6622540	So how many matches were found, and then we're going to say for match in matches.
6622540	6625420	And we're just going to grab the first 10 because I've done this and there's a lot
6625420	6630940	and you'll see why let's print off.
6630940	6633380	Let's print off in this case, match.
6633380	6636260	And then we're going to print off specifically what that text is.
6636260	6641900	Remember, the output is the lexine followed by the start token and the end token, which
6641900	6644420	means we can go into the doc object.
6644420	6646260	And we can set up something like this.
6646260	6653820	We can say match one, so index one, which is the start token and match two, which is
6653820	6655020	the end token.
6655020	6658620	And that'll allow us to actually index what these words are.
6658620	6660900	And when we do this, we can see all these printed out.
6660900	6667140	So this is the match, the lexine here, which is going to be proper down all the way down.
6667140	6673140	We've got the zero here, which corresponds to the start token, the end token.
6673140	6675060	And this is the the token that we extracted.
6675060	6681020	Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right?
6681020	6683780	So the problem should be pretty obvious right now.
6683780	6691900	And the problem is that we have grabbed all proper nouns, but these proper nouns are just
6691900	6693540	individual tokens.
6693540	6696220	We haven't grabbed the multi word tokens.
6696220	6697780	So how do we go about doing that?
6697780	6702180	Well, we can solve this problem by let's go ahead and just copy and paste all this from
6702180	6703180	here.
6703180	6707540	And we're going to make one small adjustment here.
6707540	6713940	We're going to change this to OP with a plus.
6713940	6715180	So what does that mean?
6715180	6719740	Well, let's pop back into our matcher under spacey and check it out.
6719740	6725800	So OP members, the operator or quantifier, we're going to use the plus symbol.
6725800	6732560	So it's going to look for a proper noun that occurs one or more times.
6732560	6736560	So in theory, right, this should allow us to grab multi word tokens.
6736560	6739600	It's going to look for a proper noun and grab as many as there are.
6739600	6744440	So anything that occurs one or more times, if we run this, though, we see a problem.
6744440	6748560	We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther
6748560	6753000	King, Luther King, King Martin Luther King, Junior, what what is going on here?
6753000	6755560	Well, you might already have figured it out.
6755560	6758320	It has done exactly what we told it to do.
6758320	6762800	It's grabbed all sequence of tokens that were proper nouns that occurred one or more
6762800	6764440	times.
6764440	6766280	Just so happens some of these overlap.
6766280	6770960	So token that's doc zero to one, zero to two.
6770960	6776360	So you can see the problem here is it's grabbing all of these and any combination of them.
6776360	6780400	What we can do, though, is we can add an extra layer to this.
6780400	6784560	So let's again, copy what we've just done because it was, it was almost there.
6784560	6786720	It was good, but it wasn't great.
6786720	6791080	We're going to do one new thing here when we add in the patterns, we're going to pass
6791080	6797480	in the keyword argument, greedy, we're going to say longest capital, all capital letters
6797480	6798480	here.
6798480	6802560	And if we execute that, it's going to look for the longest token out of that mix, and
6802560	6806720	it's going to give that one, make that one the only token that it extracts.
6806720	6813560	We noticed that our length has changed from what was it up here, 175 to 61.
6813560	6814880	So this is much better.
6814880	6819720	However, we should have recognized right now, another problem.
6819720	6820960	What have we done wrong?
6820960	6825240	Well, what we've done wrong is these are all out of order.
6825240	6828880	In fact, what happens is when you do this, I don't have evidence to support this, but
6828880	6831360	I believe it's right.
6831360	6836200	What will always happen is the, the greedy longest will result in all of your tokens
6836200	6841360	being organized or all your matches being organized from longest to shortest.
6841360	6846320	So if we were to scroll down the list and look at maybe negative one, negative, let's
6846320	6850800	do negative 10 on, you'll see single word tokens.
6850800	6854200	And again, this is me just guessing, but I think based on what you've just seen, that's
6854200	6856080	a fairly good guess.
6856080	6859080	So let's go ahead and just kind of so we can see what the output is here.
6859080	6862720	So how would you go about organizing these sequentially?
6862720	6869840	Well, this is where really kind of a sort comes in handy when you can pass a lambda to
6869840	6870840	it.
6870840	6875240	I can copy all this again, because again, we almost had this right.
6875240	6881160	Here we're going to sort our matches though, we can say matches.sort, and this is going
6881160	6886600	to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb
6886600	6893600	is going to allow us to actually iterate over all this and find any instance where X occurs.
6893600	6896320	And we're going to say to sort by X one.
6896320	6898520	So what this is, it's a list of tuples.
6898520	6902760	And what we're using lambda for is we're going to say sort this whole list of tuples out,
6902760	6907520	but sort it by the first index, in other words, sort it by the start token.
6907520	6912520	And when we execute that, we've got everything now coming out as we would expect and nor
6912520	6914600	these typos that exist.
6914600	6917880	We've got zero to four, six to nine.
6917880	6923040	So we actually are extracting these things in sequential order as they appear in our
6923040	6924040	text.
6924040	6930000	You can actually go through and sort the appearance of the, of the matcher.
6930000	6935600	But what if our, the person who kind of gave us this job, they were happy with this, but
6935600	6937160	they came back and said, okay, that's cool.
6937160	6941440	But what we're really interested in what we really want to know is every instance where
6941440	6948520	a proper noun of any length, grab the multi word token still, but we want to know anytime
6948520	6950560	that occurs after a verb.
6950560	6953600	So anytime this proper noun is followed by a verb.
6953600	6956480	So what we can do is we can add in, okay, okay, we can do this.
6956480	6957800	We're going to have a comma here.
6957800	6960520	So the same pattern is going to be a sequence now.
6960520	6962760	It's not just going to be one thing.
6962760	6968040	We're going to say token one needs to be a proper noun and grab as many of those tokens
6968040	6971720	as you can zero or one to more times.
6971720	6977360	And then after those are done comma, this is where the next thing has to occur POS.
6977360	6980440	So the part of speech needs to be a verb.
6980440	6984000	So the next thing that comes out needs to be a verb.
6984000	6985680	And we want that to be the case.
6985680	6989840	Well, when we do this, we can kind of go through and see the results of the first instance
6989840	6998040	of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced
6998040	7002280	258 director J Edgar Hoover considered.
7002280	7009000	Now we're able to use those linguistic features that make Spacey amazing and actually extract
7009000	7011160	some vital information.
7011160	7017840	So we've been able to figure out where in this text a a proper noun is proceeded by
7017840	7018840	a verb.
7018840	7022480	So you can already start to probably see the implications here.
7022480	7026120	And we can we can create very elaborate things with this.
7026120	7029880	We can use any of these as long of a sequence as you can imagine.
7029880	7033640	We're going to work with a different text and kind of demonstrate that it's a fun toy
7033640	7034640	example.
7034640	7040240	We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file.
7040240	7043160	I'm going to load it in right now.
7043160	7049400	And then I'm going to just grab the first sentence from the first chapter.
7049400	7052000	And what we have here is the first sentence.
7052000	7053800	So here's our scenario.
7053800	7061320	Somebody has asked us to grab all the quotation marks and try to identify the person described
7061320	7065320	or the person described the person who's doing the speaking or the thinking.
7065320	7068560	In other words, we want to be able to grab Alice thought.
7068560	7074120	Now I picked Alice in Wonderland because of the complexity of the text, not complexity
7074120	7079680	in the sense of the language used children's book, but complexity and the syntax.
7079680	7085720	These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly
7085720	7090040	inconsistent in how we structured these kind of sequences of quotes.
7090040	7094680	And the other thing I chose to do as I left in one mistake here, and that is this non
7094680	7096680	standardized quotation mark.
7096680	7099820	So remember, when you need to do this, things need to match perfectly.
7099820	7104360	So we're going to replace this first things first is to create a cleaner text, or we do
7104360	7109160	text equals text dot replace, and we're going to replace the instance of I believe it's
7109160	7113400	that mark, but let's just copy and paste it in to make sure we're going to replace that
7113400	7119240	with a, with a single quotation mark, and we can print off text just to make sure that
7119400	7120400	was done correctly.
7120400	7121400	Cool.
7121400	7122400	Great.
7122400	7123400	It was it's now looking good.
7123400	7127880	Remember, whenever you're doing information extraction, standardize the texts as much as
7127880	7129080	possible.
7129080	7133720	Things like quotation marks will always throw off your data.
7133720	7140480	Now that we've got that, let's go ahead and start trying to create a fairly robust pattern
7140480	7147640	to try to grab all instances where there is a quotation mark, thought, something like
7147720	7151480	this, and then followed by another quotation mark.
7151480	7155760	So the first thing I'm going to try and do is I'm going to try to just capture all quotation
7155760	7157440	marks and a text.
7157440	7161520	So let's go through and try to figure out how to do that right now.
7161520	7165480	So we're going to copy in a lot of the same things that we used up above, but we're going
7165480	7167040	to make some modifications to it.
7167040	7172640	Let's go ahead and copy and paste all that we're going to completely change our pattern.
7172640	7174040	So let's get rid of this.
7174040	7175400	So what are we looking for?
7175400	7181240	Well, first of all, the first thing that's going to occur in this pattern is this quotation
7181240	7182080	mark.
7182080	7187320	So that's going to be a full text match, which is an or if you remember, and we're going
7187320	7191600	to have to use double quotation marks to add in that single quotation mark.
7191600	7192720	So that's what we grabbed first.
7192720	7197160	We're going to look for anything that is an or and the next thing that's going to occur
7197160	7202520	after that, I think this is good to probably do this now on a line by line basis.
7202520	7205120	So we can keep this straight.
7205120	7208400	So the next thing that's going to occur is we're looking for anything in between.
7208400	7214240	So anything that is an alpha character, we're going to just grab it all.
7214240	7222160	So is alpha and then we need to say true.
7222160	7226960	But within this, we need to specify how many times that occurs because if we say is true,
7226960	7232600	it's just going to look at the next token in this case and and then say that's the end.
7232600	7233600	That's it.
7233600	7234600	That's the pattern.
7235080	7240880	But we want to grab not just and but and what is the use of a everything.
7240880	7247280	So we need to grab not only that, but when you say OP, so our operator again.
7247280	7250280	And if you said plus, you would be right here.
7250280	7253960	We need to make sure that it's a plus sign, so it's grabbing everything.
7253960	7259720	Now in this scenario, this is a common construct is when you have a injection here in the middle
7259720	7260720	of the sentence.
7260720	7263180	So thought or said, and it's the character doing it.
7263180	7266020	This oftentimes got a comma right here.
7266020	7268580	So we need to add in that kind of a feature.
7268580	7272380	So there could be is punked.
7272380	7275220	There could be a punked here.
7275220	7278760	And we're going to say that that is equal to true.
7278760	7281300	But that might not always be the case.
7281300	7283180	There might not always be one there.
7283180	7288300	So we're going to say OP is equal to a star.
7288300	7289300	We go back.
7289300	7290300	We'll see why.
7290300	7295300	To our OP, the star allow the pattern to match zero or more time.
7295300	7300120	So in this scenario, the punctuation may or may not be there.
7300120	7301860	So that's the next thing that occurs.
7301860	7306820	Once we've got that, the last thing that we need to match is the exact same thing that
7306820	7311980	we had at the start is this or appear.
7311980	7313420	And that's our sequence.
7313420	7317020	So this is going to look for anything that starts with a quotation mark has a series
7317020	7323700	of alpha characters has a punctuation like a comma possibly, and then closes the quotation
7323700	7324700	marks.
7324700	7327580	If we execute this, we succeeded.
7327580	7328580	We got it.
7328580	7331580	We extracted both matches from that first sentence.
7331580	7333820	There are no other quotation marks in there.
7333820	7337300	But our task was not just to extract this information.
7337300	7342340	Our task was also to match who is the speaker.
7342340	7345580	Now we can do this in a few different ways and you're going to see why this is such a
7345580	7348420	complicated problem in just a second.
7348420	7350260	So let's go ahead and do this.
7350260	7352060	How can we make this better?
7352060	7356180	Well, we're going to have this occur twice.
7356180	7360380	But in the middle, we need to figure out when somebody is speaking.
7360380	7363180	So one of the things that we can do is we can make a list.
7363180	7369240	So let's make a list of limitized forms of our verbs.
7369240	7373980	So we're going to say, let's call this speak underscore limits.
7373980	7376060	This can be equal to a list.
7376060	7379500	And the first thing we're going to say is think, because we know that think is in there
7379500	7384720	and say this is the limitized form of thought and said.
7384720	7388440	So what we can do now is after that occurs, it's adding a new thing.
7388440	7392060	We're going to be able to now add in a new pattern that we're looking for.
7392060	7397700	And so not just the start of a quotation mark, not just the end of a quotation mark, but
7397700	7400980	also a sequence that'll be something like this.
7400980	7402980	So it's going to be a part of speech.
7402980	7406380	So it's going to be a verb that occurs first, right?
7406380	7408620	And that's going to be a verb.
7408620	7421180	But more importantly, it's going to be a lemma that is in what did I call you speak lemmas?
7421180	7423740	So let's break this down.
7423740	7428180	The next token needs to be a verb.
7428180	7437460	And it needs to have a limitized form that is contained within the speak lemmas list.
7437460	7441620	So if it's got that fantastic, let's execute this and see what happens.
7441620	7443620	We should only have one hit.
7443620	7444620	Cool.
7444620	7445620	We do.
7445620	7446620	So we've got that first hit.
7446620	7450020	And the second one hasn't appeared anymore because that second quotation mark wasn't
7450020	7452980	proceeded by a verb.
7452980	7456660	Let's go ahead and make some modifications that we can improve this a little bit.
7456660	7460860	Because we want to know not just what that person's doing.
7460860	7462980	We also need to know who the speaker is.
7462980	7464940	So let's grab it.
7464940	7466780	Let's figure out who that speaker is.
7466780	7468820	So we can use part of speech.
7468820	7471020	Again, another feature here.
7471020	7474780	We know that it's going to be a proper noun because oftentimes proper nouns are doing
7474780	7476300	the speaking.
7476300	7477660	Sometimes it might not be.
7477660	7481800	Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those
7481800	7484360	situations for just right now.
7484360	7486000	So we're looking for a proper noun.
7486000	7491480	But remember proper nouns, as we saw just a second ago, could be multiple tokens.
7491480	7493840	So we're going to say OP plus.
7493840	7495920	So it could be a sequence of tokens.
7495920	7497680	Let's execute this.
7497680	7499960	Now we've captured Alice here as well.
7499960	7504000	So and is the use and what is the use of a book thought Alice.
7504000	7508120	Now we know who the speaker is, but this is a partial quotation.
7508120	7509240	This is not the whole thing.
7509240	7511160	We need to grab the other quote.
7511160	7512960	Oh, how will we ever do that?
7512960	7515000	Well, we've already solved that.
7515000	7521120	We can copy and paste all of this that we already have done right down here.
7521120	7526400	And now we've successfully extracted that entire quote.
7526400	7528640	So you might be thinking to yourself, yeah, we did it.
7528640	7536600	We can now extract quotation marks and we can even extract, extract, you know, any instance
7536600	7539800	where there's a quote and somebody speaking.
7539800	7540800	Not so fast.
7540800	7542360	We're going to try to iterate over this data.
7542360	7550040	So we're going to say for text in data, zero twos, we're going to iterate over the first
7550040	7552400	chapter.
7552400	7559920	And we're going to go ahead and let's let's do all of this.
7559920	7567600	Doc is going to be equal to that sort that out.
7567600	7573040	And then again, we're going to be printing out this information, the same stuff I did
7573040	7576360	before, just now it's going to be iterating over the whole chapter.
7576360	7583080	And if we let this run, we've got a serious, serious problem.
7583080	7586680	And it doesn't actually grab us anything.
7586680	7589200	Nothing has been grabbed successfully.
7589200	7594200	What is going on?
7594200	7595840	We've got a problem.
7595840	7604520	And that problem stems from the fact that our patterns and the problem is that we don't
7604520	7611280	have our our text correctly, we're being removing the quotation mark that was the problem up
7611280	7612280	above.
7612280	7615640	So we're going to add this bit of code in.
7615640	7617640	And we're going to be able to fix it.
7617640	7620880	So now when we execute this, we see that we've only grabbed one match.
7620880	7623720	Now you might be thinking to yourself, there's an issue here and there there is, let's go
7623720	7627000	ahead and print off the length of matches.
7627000	7629720	And we see that we've only grabbed one match.
7629720	7631000	And then we haven't grabbed anything else.
7631000	7632400	Well, what's the problem here?
7632400	7636760	Are there are there no other instances of quotation marks in the rest of the first chapter?
7636760	7638480	And the answer is no, there are.
7638480	7643320	There absolutely are other quotation marks and other paragraphs from the first chapter.
7643320	7647840	The problem is, is that our pattern is singular.
7647840	7649280	It's not multivariate.
7649280	7655560	We need to add in additional ways in which a text might be structured.
7655560	7660800	So let's go ahead and try and do this with some more patterns.
7660800	7664840	I'm going to go ahead and copy and paste these in from the textbook.
7664840	7670000	So you'll be able to actually see them at work.
7670000	7674320	And so what I've did, I've done is I've added in more patterns, pattern two and pattern
7674320	7678920	three allow for instances like this, well thought Alice.
7678920	7683160	So an instance where there's a punctuation, but there's no proceeding quotation after
7683160	7687760	this, and then which certainly said before an instance where there's a comma followed
7687760	7688760	by that.
7688760	7692640	So we've been able to capture more variants and more ways in which quotation marks might
7692640	7694920	exist followed by the speaker.
7694920	7698320	Now this is where being a domain expert comes into play.
7698320	7701440	You'd have to kind of look through and see the different ways that Louis C. Carroll
7701440	7705640	structures quotation marks and write out patterns for capturing them.
7705640	7709280	I'm not going to go through and try to capture everything from Alice in Wonderland because
7709280	7711600	that would take a good deal of time.
7711600	7715760	And it's not really in the best interest because it doesn't matter to me at all.
7715760	7719400	What I encourage you to do, if this is something interesting to you is try to apply it to your
7719400	7724640	own texts, different authors, structure quotation marks a little differently than what patterns
7724640	7727560	that I've gotten written here are a good starting point.
7727560	7730480	But I would encourage you to start playing around with them a little bit more.
7730480	7735520	And what you can do is when you actually have this match extracted, you know that the
7735880	7741960	instance of a proper noun that occurs between these quotation marks or after one is probably
7741960	7748040	going to be the person or thing that is doing the speaking or the thinking.
7748040	7750080	So that's kind of how the matcher works.
7750080	7755680	It allows for you to do these things, these robust type data extractions without relying
7755680	7756920	on entity ruler.
7756920	7761000	And remember, you can use a lot of these same things with an entity ruler as well.
7761000	7765400	But we don't want this in this case, we don't want things like this to be labeled as entities.
7765440	7769320	We want them to just be separate things that we can extract outside of the of the
7769320	7770960	ints dot doc dot ints.
7771320	7776560	That's going to be where we conclude our chapter on on the on the matcher.
7776920	7780920	In the next section of this video, we're going to be talking about custom components in
7780920	7786880	spacey, which allow for us to do some pretty cool things such as add in special functions
7786880	7793840	that allow for us to kind of do different custom shapes, permutations on our data with
7794520	7798680	components that don't exist like an entity ruler would be a component components that
7798680	7800880	don't exist within the spacey framework.
7800880	7806760	So add in custom things like an entity ruler that do very specific things to your data.
7809080	7814160	Hello, we're now moving into a more advanced aspect of the textbook specifically chapter
7814160	7814760	seven.
7814760	7817000	And that's working with custom components.
7817320	7821240	A good way to think about a custom component is something that you need to do to the doc
7821240	7825080	object or the doc container that spacey can't do off the shelf.
7825080	7827880	You want to modify it at some point in the pipeline.
7828200	7832560	So I'm going to use a basic toy example that demonstrates the power of this.
7832840	7835720	Let's look at this basic example that I've already loaded into memory.
7835920	7838760	It's two sentences that are in the doc object now.
7839440	7840640	And that's Britain is a place.
7840840	7842000	Mary is a doctor.
7842360	7849080	So let's do for int and doc dot ints print off int dot text and dot label.
7850080	7851520	And we see what we'd expect.
7851520	7854520	Britain is GPE a geopolitical entity.
7854760	7856400	Mary is a person.
7857120	7858000	That's fantastic.
7858280	7864560	But I've just been told by somebody higher up that they want the model to never ever
7864560	7871480	give anything as GPE or maybe they want any instance of GPE to be flagged as LOC.
7872280	7877640	So all the different locations all have LOC as a label or we just want to remove them
7877640	7878240	entirely.
7878920	7880680	So I'm going to work with that latter example.
7881000	7887760	We need to create a custom pipe that removes all instances of GPE from the doc dot
7887800	7889040	ints container.
7889320	7890080	So how do we do that?
7890120	7892880	Well, we need to use a custom component.
7893200	7898720	We can do this very easily in spacey by saying from spacey dot language import
7899080	7901240	language capital L very important there.
7901240	7904600	Capital L now that we've got that class loaded up.
7905080	7906320	Let's start working with this.
7906320	7909280	What we need to do first is we need to use a flag.
7909280	7913520	So the symbol and we need to say at language dot component.
7914480	7916280	And we need to give that component a name.
7916720	7920160	We're going to say in this case, let's say remove GPE.
7921480	7923560	And now we need to create a function to do this.
7923920	7927200	So we're going to call this remove GPE.
7927400	7929240	I always kind of keep these as the same.
7929680	7931280	That's my personal preference.
7931760	7935040	And this is going to take one, one, one thing.
7935080	7936400	That's going to be the doc object.
7936680	7939600	So the doc object, think about how it moves through the pipeline.
7939840	7942640	This component is another pipe and that pipeline.
7942640	7947400	It needs to receive the doc object and send off the doc object.
7947640	7948760	You could do a lot of other things.
7949080	7950600	It could print off entity found.
7950600	7952800	It could do really any number of things.
7952800	7956440	It could add stuff to the data coming out of the pipeline.
7957040	7960240	All we're concerned with right now is modifying the doc dot ints.
7961080	7962400	So we can do something like this.
7962880	7968120	We can say original ends is equal to a list of the doc dot ends.
7968240	7972800	So remember, we have to convert the ends from a generator into a list.
7973040	7978000	Now what we can do is we can say for int and doc dot ends, if the end not label.
7978000	7985040	So if that label is equal to GPE, then what we want to do is we want to just
7985160	7986240	we just want to remove it.
7986360	7991360	So let's say original ints.remove and we're going to remove the int.
7991640	7992720	Remember, it's now a list.
7993000	7994560	Sorry, I executed that too soon.
7994840	7996040	Remember, it's now a list.
7996200	8000960	So what we can do is we can go ahead now and convert those original
8000960	8006320	ends back into doc dot ends by saying doc dot ends equals original ends.
8006720	8010800	And if we've done things correctly, we can return the doc object and it will
8010800	8012840	have all of those things removed.
8013040	8015280	So this is what we would call a custom component.
8015280	8019760	Something that changes the doc object along the way in the pipeline, but
8019760	8021440	we need to add it to NLP.
8021880	8024120	So we can do NLP dot add pipe.
8025040	8027240	We want to make sure that it comes after the NER.
8027480	8032880	So we're just going to say, uh, add the pipe or move GPE corresponds
8032880	8034160	to the component name.
8035880	8039400	And now let's go ahead and NLP dot analyze pipes.
8040400	8044880	And you'll be able to see that it sits at the end of our pipeline right there.
8044880	8045880	Remove GPE.
8046360	8048400	Now comes time to see if it actually works.
8048720	8051800	So we're going to copy and paste our code from earlier up here.
8057040	8061480	Let's go ahead and copy this.
8063680	8068600	And now we're going to say for int and doc dot ends print off int dot text.
8069400	8070480	And dot label.
8070920	8075200	And we should see, as we would expect, just marry coming out.
8075640	8078400	Our pipeline has successfully worked.
8078840	8082560	Now, as we're going to see when we move into red checks, you can do a lot
8082560	8086280	of really, really cool things with custom components.
8086640	8090480	I'm going to kind of save the, the advanced features for, I think I've
8090480	8094400	got it scheduled for chapter here, chapter nine in our textbook.
8094680	8099320	This is just a very, very basic example of how you can introduce a custom
8099320	8101560	component to your spacey pipeline.
8101920	8104080	If you can do this, you can do a lot more.
8104400	8106440	You can maybe change a different entity.
8106440	8107640	So they have different labels.
8107800	8110680	You can make it where GPEs and locks all agree.
8110920	8112200	You can remove certain things.
8112200	8115280	You can have it print off place found person found.
8115560	8116400	You can do a lot.
8117240	8120800	So really the sky's the limit here, but a lot of the times you're going
8120800	8122600	to need to modify that doc object.
8122920	8126240	And this is how you do it with a custom pipe so that you don't have to write
8126240	8131880	a bunch of code for a user outside of that NLP object, the NLP object.
8131880	8138560	Once you save it to disk by doing something like NLP dot to disk data,
8139200	8145640	new and core web SM, it's going to actually be able to go to the disk
8145960	8147640	and be saved with everything.
8148080	8152440	But one thing that you should note is that the component that you have
8152440	8156520	here is not automatically saved with your data.
8156920	8160320	So in order for your component to actually be saved with your data,
8160640	8166000	you need to store that outside of this entire script.
8166280	8171360	You need to save it as a library that can be given to the model
8171400	8172760	when you go to package it.
8172960	8175280	That's beyond the scope of this video for right now.
8175640	8179880	In order for this to work in a different Jupyter notebook, if you were to try
8179880	8185320	to use this, this container, this component has to actually be in the script.
8185560	8189400	When it comes time to package your model, your pipeline and distribute it,
8189680	8190800	that's a different scenario.
8190800	8194480	And that scenario, you're going to make sure that you've got a special my
8194480	8199560	component dot pie file with this bit of code in there so that, so that spacing
8199560	8202560	knows how to handle your particular data.
8203560	8206800	It's now time to move on to chapter eight of this textbook.
8206800	8208680	And this is where a spacey gets really interesting.
8208680	8213280	You can start applying regular expressions into a spacey component
8213280	8217360	like an entity ruler or a custom component, as we're going to see in just
8217360	8218840	a moment with chapter nine.
8219200	8222680	I'm not going to spend a good deal of time talking about regular expressions.
8222680	8227000	I could spend five hours talking about regex and what all it can do.
8227120	8230880	In the textbook, I go over what you really need to know, which is what regular
8230920	8235920	expressions is, which is as a way to do a really robust string pattern matching.
8236280	8239440	I talk about the strengths of it, the weaknesses of it, its drawbacks,
8239800	8243080	how to implement it in Python and how to really work with regex.
8243240	8245640	But this is a video series on spacey.
8245880	8249320	What I want to talk about is how to use regex with spacey.
8249640	8252400	And so let's move over to a Jupiter notebook where we actually have this
8252400	8254240	code to execute and play around with.
8255000	8257840	If we look here, we have the same example that we saw before.
8258160	8260800	What my goal is is not to extract the whole phone number,
8260800	8263200	rather try to grab this sequence here.
8263520	8265760	And we do this with a regular expression pattern.
8266080	8269680	What this says is it tells it to look for a sequence of tokens or sequence
8269680	8271400	of characters like this.
8271680	8276840	It's going to be three digits followed by a dash followed by four digits.
8277160	8280440	And if I were to execute this whole code, nothing is printed out.
8280840	8283400	Does that mean that I failed to write good regex?
8283400	8284760	No, it does not at all.
8285120	8287360	It's failed for one very important reason.
8287440	8291720	And this is the whole reason why I have this chapter in here is that regex,
8292000	8295320	when it comes to pattern matching, pattern matching only really works
8296160	8299840	when it comes to regex for single tokens.
8300040	8305480	You can't use regex across multi-word tokens, at least as of spacey 3.1.
8305920	8307200	So what does that mean?
8307200	8310920	Well, it means that that dash right there in our phone number is causing
8310920	8312200	all kinds of problems.
8312520	8315960	If we move down to our second example, it's going to be the exact same pattern.
8316280	8317080	A little different.
8317080	8319640	Let me go ahead and move this over so you can see it a bit better.
8320840	8324920	It's going to be regex that looks like this, where we just look for a sequence
8324920	8329040	of five digits, we execute that, we find it just fine.
8329080	8332160	And the reason for that is because this does not have a dash.
8332520	8336800	So regex, if you're familiar with it, if you've worked with it, it's very powerful.
8337000	8338880	You can do a lot of cool things.
8339320	8344440	When you're going to use this in Python, if you're using just the standard
8344440	8345960	off the shelf components.
8346200	8349680	So the entity ruler, the matcher, you're going to be using this when
8349680	8354160	you want to match regex to a single token.
8354440	8359480	So think about this, if you're looking for a word that starts off with a capital
8359480	8364320	D, and you want to just grab all words that start with a capital D, that would
8364320	8368360	be an example of when you would want to use it in a standard off the shelf component.
8368800	8371520	But that's not all you can do in spacey.
8371880	8375800	You can use regex to actually capture multi word tokens.
8376080	8378320	So capture things like Mr.
8378400	8379280	Deeds.
8379480	8381200	So any instance of Mr.
8381200	8384800	Period Space Name, a sequence of proper nouns.
8385680	8390880	You can also use it to, but yet in order to do that, you have to actually
8390880	8394520	understand how to add in a custom component for it.
8394800	8399320	And we're going to be seeing that in just a second as we move on to chapter nine,
8399560	8401120	which is advanced regex.
8401320	8405360	If you're not familiar with regex at all, take a few minutes, read chapter eight.
8405360	8410800	I encourage you to do so because I go over in detail and I talk about how to
8410800	8415080	actually engage in regex and Python and its strengths and weaknesses.
8415440	8418680	What I want you to really focus on though, and get away from, get from all this
8418920	8424160	is how to do some really complex multi word token matching with regex.
8424160	8427440	Remember, you're going to want to use regular expressions when the pattern
8427440	8433840	matching that you want to do is unindependent of the, the lima, the POS,
8433840	8436360	or any of the linguistic features that space is going to use.
8436680	8440040	If you're working with linguistic features, you have to use the
8440040	8445040	spacey pattern, pattern matching things like the morph, the orth, the lima,
8445040	8445840	things like that.
8445840	8450400	But if your sequence of strings is not dependent on that, so you're looking
8450400	8453840	for any instance of, in this case, we're going to talk about in just a second,
8454240	8459880	a, a case where Paul is followed by a capitalized letter and then a word break.
8460680	8463640	Then you're going to want to use regular expressions because in this case,
8463960	8468840	this is independent of any linguistic features and regular expressions
8468840	8472040	allows for you to write much more robust patterns, much more quickly.
8472040	8474960	If you know how to use it well, and it allows for you to do much more
8474960	8478440	quick robust things within a custom component.
8478680	8481000	And that's going to be where we move to now.
8482000	8485400	Now that we know a little bit about regex and how it can be implemented in
8485400	8490000	Python, let's go ahead and also in spacey, let's go ahead and try and see
8490200	8497400	how we can get regex to actually find multi word tokens for us within spacey
8497600	8499400	using everything in the spacey framework.
8499600	8503000	So the first thing I'm going to do to kind of demonstrate all this is I'm going
8503000	8505000	to import regex.
8505000	8509400	This comes standard with Python and you can import it as RE just that way.
8509400	8512400	Import RE and that's going to import regex.
8512800	8516800	I'm going to work from the textbook and work with this sample text.
8517000	8521800	So this is Paul Newman was an American actor, but Paul Hollywood is a British TV
8522000	8522800	TV host.
8523000	8525000	The name Paul is quite common.
8525200	8529000	So it's going to be the text that we work with throughout this entire chapter.
8529400	8533600	Now a regex pattern that I could write to capture all instances of things like
8533600	8537800	Paul Newman and Paul Hollywood, which is what my goal is, could look something
8538400	8544600	like this, I could say or make an R string here and say Paul, and then I'm going
8544600	8548200	to grab everything that starts with a capital letter and then my grab
8548400	8550200	everything until a word break.
8550400	8554200	And that's going to be a pattern that I can use in regex with this formula
8554200	8559100	means is find any instance of Paul proceeded by a in this case, a capital
8559100	8561900	letter until the actual word break.
8561900	8565300	So grab the first name Paul and then what we can make a presumption is going
8565300	8569700	to be that individual's last name in the text, a simple example, but one
8569700	8572400	that will demonstrate our kind of purpose right now.
8572700	8577100	So how we can do this is we can create an object called matches and use regex
8577100	8583200	dot find it or we can pass in the pattern and we can pass in the text.
8583400	8587100	So what this is going to do is it's going to use regex to try to find this
8587100	8589300	pattern within this text.
8589500	8591900	And then what we can do is we can iterate over those matches.
8591900	8599200	So for match and matches, we can grab and print off the match and we have
8599200	8601500	something that looks like this.
8603100	8606900	What we're looking at here is what we would call it a regex match object.
8607100	8608800	It's got a couple of different components here.
8609000	8614100	It's got a span, which tells us the start character and the end character.
8615200	8619500	And then it has a match and what this match means is the actual text itself.
8619700	8623300	So the match here is Paul Newman and the match here is Paul Hollywood.
8623500	8627900	So we've been able to extract the two entities in the text that begin with
8627900	8632500	Paul and have a proper last name structured with a capital letter.
8632700	8634500	We grabbed everything up until the word break.
8634900	8635500	That's great.
8635700	8638500	That's going to be what you need to know kind of going forward because what
8638500	8643200	we're going to do now is we're going to implement this in a custom spacey pipe.
8643400	8647100	But first let's go through and write the code so that we can then easily kind
8647100	8648600	of create the pipe afterwards.
8649400	8654000	So what we need to do is we need to import spacey and we also need to say
8654000	8659500	from spacey dot tokens import span and we're going to be importing a couple
8659500	8662500	of different things as we move forward because we're going to see that we're
8662500	8664300	going to make a couple of mistakes intentionally.
8664300	8667400	I'm going to show you how to kind of address these common mistakes that might
8667400	8669900	surface in trying to do something like this.
8670400	8673900	So once we've imported those two things, we can start actually writing out our
8673900	8674300	code.
8674600	8677300	Again, we're going to stick with the exact same text and again, we're going
8677300	8681900	to stick with the exact same pattern that we've got stored in memory up above.
8682600	8686500	So what we need to do now is we need to create a blank spacey object or sorry,
8686500	8690700	a blank spacey pipeline that we can kind of put all this information into.
8692200	8697500	And for right now what we're going to do is we're just going to kind of go
8697500	8700300	through and look at these individual entities.
8708300	8716100	So again, we're going to create the doc object, which is going to be equal to
8716100	8721500	nlp text and this is not going to be necessary for right now, but I'm
8721500	8725300	establishing a kind of a consistent workflow for us and you're going to see
8725300	8728400	how we kind of take all this and implement it inside of a pipeline.
8728700	8732200	So we're going to say original ends is equal to list doc dot ends.
8732200	8735500	Now in this scenario, there's not going to be any entities because we don't
8735500	8740200	have an any R or an entity ruler in our blank spacey pipeline.
8741100	8743800	What we're going to do next is we're going to create something called an
8743800	8749000	nwt int and that's going to stand for multi word token entity.
8750000	8751400	You can name this whatever you like.
8751400	8754800	This is just what I kind of stick to and then we're going to do and this
8754800	8756600	is straight from the spacey documentation.
8756900	8760100	We're going to say for match an RE dot find it or the same thing
8760100	8764200	that we saw above pattern doc dot text.
8764500	8767300	So what this is going to do is it's going to take that doc object.
8768200	8772000	Look at it as raw text because remember the doc object is a container
8772300	8776200	that doesn't actually have raw text in it until you actually call the dot
8776200	8779900	text attribute and then our goal is for each of these things.
8779900	8783200	We're going to look and call in this span.
8783500	8789300	So we're going to say is start and the end is equal to match dot span.
8789600	8793200	So what we're doing here is we're going in and grabbing the span attribute
8793700	8797000	and we're grabbing these two components the start and the end.
8797200	8798100	But we have a problem.
8798400	8800400	These are character spans.
8800400	8803600	Remember the doc object works on a token level.
8803800	8807300	So we've got to kind of figure out a way to reverse engineer this almost
8807500	8810200	to actually get this into a spacey form.
8810400	8815300	Fortunately the doc object also has an attribute called character span.
8816000	8821500	So what we can do is we can say the span is equal to doc dot char span
8822500	8823700	start and end.
8823700	8827100	So what this is going to do is it's going to print off essentially for us.
8827300	8828800	Let's go ahead and do that.
8829100	8832000	It would print off for us where we worry to actually have an entity here.
8832600	8836400	It would print off for us as we can see Paul Newman and Paul Hollywood.
8836700	8842700	So what we need to do now is we need to get this span into our entities.
8843400	8849100	So what we can do is instead of printing things off we can say if span is not
8849100	8852100	none because in some instance instances this will be the case.
8852500	8855900	You're going to say NWT ends dot append.
8856700	8863600	You're going to append a tuple here span dot start span dot end span dot
8863600	8864300	text.
8864400	8868600	So this is going to be the start the end and the text itself.
8869100	8873900	And once we've done that we've managed to get our multi word tokens into
8875100	8877100	a list that looks like this.
8878100	8886600	Start and Paul Newman Paul Hollywood and notice that our span dot start is
8886600	8889900	aligning not with a character span.
8889900	8894200	Now it's rather aligning with a token span.
8894400	8897800	So what we've done is we've taken this character span here and been able to
8897800	8903600	find out where they start and end within the the token sequence.
8903600	8904900	So we have zero and two.
8905300	8908300	So Paul Newman one this was the zero index.
8908300	8910200	It goes up until the second index.
8910400	8914300	So it grabs index token zero and token one and we've done the same thing
8914300	8915300	with Paul Hollywood.
8915700	8917000	Now we've got that data.
8917200	8924900	We can actually start to inject these entities into our original entities.
8924900	8926600	So let's go through and do that right now.
8927100	8930200	So we can do once we've got these things appended to this list.
8930200	8933200	We can start injecting them into our original entities.
8933200	8937300	So we can say for end in MWT ends.
8937800	8941500	What we want to do is we want to say the start the end and the name is equal
8941500	8945900	to end because this is going to correspond to the tuple the start the
8945900	8948700	end and the entity text.
8950000	8952000	Now what we can do is we can say per end.
8952000	8953600	So this is going to be the individual end.
8953600	8956800	We're going to create a span object in spacey.
8957900	8959000	It's going to look like this.
8959300	8960700	So a capital S here.
8960700	8962800	Remember we imported it right up here.
8963000	8967300	This is where we're going to be working with the span class and this is
8967300	8971300	going to create for us a span object that we can now safely inject into
8971300	8973800	the spacey doc.ins list.
8974300	8979800	So we can say doc start and label and this is going to be the label that
8979800	8983300	we want to actually assign it and this is going to be person in this
8983300	8988100	case because these are all people we can do now as we can go through and
8988100	8992900	say doc we can inject this into the original ends.
8996100	9002500	Original ins dot append and we're going to append the per end which is
9002500	9008900	going to be this span object and finally what we can say is doc.ins is
9008900	9014500	equal to original ends kind of like what we saw just a few moments ago
9015300	9017200	and let's go ahead and print off.
9023100	9026600	We've got our entities right there or we to do this up here when we first
9026600	9030900	kind of create the doc object you'll see nothing an empty list but now
9030900	9036300	what we've been able to do is inject these into the doc object the doc.ins
9036300	9040300	attribute and we can say for end and doc.ins just like everything else
9040300	9045200	and dot text and dot label and because we converted it into a span we
9045200	9050900	were able to inject it into the entity attribute from the doc object kind
9050900	9053400	of natively so that spacey can actually understand it.
9053800	9057100	So what can we do with this well one of the things that we could do is
9057100	9060200	we can use the knowledge that we just acquired about custom components
9060500	9064000	and build a custom component around all of this.
9064100	9067400	So how might we do that well let's go through and try it out.
9068400	9074600	The first thing that we need to do is we need to import our language class so
9074600	9077600	if you remember from a few moments ago whenever you need to work with a
9077600	9084600	custom component you need to say from spacey dot language import language
9084600	9088100	with a capital L what we're going to do now is we're going to take the code
9088100	9091500	that we just wrote and we're going to try to convert that into an actual
9091500	9096700	custom pipe that can fit inside of our pipeline as kind of our own custom
9096700	9097900	entity ruler if you will.
9098700	9101700	So what we're going to do now is we're going to call this language dot
9101700	9107500	component and we're going to call this let's call this Paul NER something
9107600	9111400	not too not too clever but kind of very descriptive we're going to call
9111400	9116000	this Paul NER and this is going to take that single doc object because
9116000	9119800	remember this pipe needs to receive the doc object and do stuff to it.
9120200	9123100	So what we can do is we can take all this code that we just wrote.
9126700	9134300	From here down and paste it into our function and what we have is the
9134300	9137100	ability now to implement this as a custom pipe.
9138200	9141500	We don't need to do this because we don't want to print things off but
9141500	9143900	here we're going to return the doc object.
9143900	9148500	So we have now is a custom kind of entity ruler that uses regex across
9148500	9149900	multiple tokens.
9150100	9154700	If you want to use regex in spacey across multiple tokens as of spacey
9154700	9157500	3.1 this is the only way to implement this.
9158100	9165200	So now we can take this pipe and we can actually add it to a blank custom
9165300	9165800	model.
9166100	9172600	So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're
9172600	9177900	going to create a blank English model nlp2 dot add pipe.
9179300	9181500	We're going to add in Paul NER.
9185100	9187700	And now we see that we've actually created that successfully.
9187700	9190400	So we have one pipe kind of sitting in all of this.
9190800	9194500	Now what we can do is we can go through and we need to probably add in our
9194500	9199300	pattern as well here just for good practice because this should be
9199300	9201100	stored somewhat adjacent.
9201100	9204100	I like to sometimes to keep it up here when I'm doing this but you can
9204100	9207000	also keep it kind of inside of the function itself.
9208200	9211500	Let's go ahead and just kind of save that and we're going to rerun this
9212300	9212700	cool.
9213400	9217200	Now what we can do is we can say doc to is equal to nlp2 we're going
9217200	9221200	to go over that exact same text and we're going to print off our doc
9221300	9226300	to dot ints and we've now managed to implement that as a custom
9226400	9229500	spacey pipe but we've got one big problem.
9229900	9235900	Let's say just hypothetically we wanted to also kind of work in really
9235900	9241900	a another kind of something into our actual pipeline.
9241900	9246900	We wanted this pipeline to sit on top of maybe an existing spacey model
9247300	9251800	and for whatever reason we don't want Paul Hollywood to have that title.
9251800	9253200	We wanted to have the title.
9253700	9257000	Maybe we want to just kind of keep Paul Hollywood as a person but we
9257000	9261600	also want to find maybe other cinema style entities.
9261600	9265400	So we're going to create another entity here instead of all this that's
9265400	9270100	going to be something like let's go ahead and make a new a new container
9270100	9272200	down here a new component down here.
9273100	9276600	We're going to just look for any instance of Hollywood and we're going
9276600	9279300	to call that the word the label of cinema.
9279800	9282000	So I want to demonstrate this because this is going to show you
9282000	9284800	something that you are going to encounter when you try to implement
9284800	9287200	this in the real world and I'm going to show you how to kind of
9287500	9289500	address the problem that you're going to encounter.
9289900	9293100	So if we had a component that looked like this now it's going to look
9293100	9297200	for just instance instances of Hollywood and let's call this Holly
9298900	9302900	Cinema NER and change this here as well.
9303400	9305700	What we can do now is go ahead and load that up into memories.
9305700	9309500	We've got this new component called Cinema NER and just like before
9309500	9312200	we're going to create an LP three now this is going to be spacey dot
9312200	9314400	load in core web.
9315500	9320500	SM and so what this is going to do is it's going to load up the spacey
9320500	9326700	small model and LP three dot add pipe and it's going to be the what did
9326700	9332300	I call this again the cinema NER and if we were to go through
9332300	9336900	and add that and create a new object called doc three make that
9336900	9339900	equal to an LP three text.
9341700	9345600	We're going to get this air and this is a common air and if you
9345600	9347700	Google it you'll eventually find the right answer.
9347700	9349500	I'm just going to give it to you right now.
9349800	9353800	So what this is telling you is that there are spans that overlap
9354800	9361200	that don't actually work because one of the spans for cinema is Hollywood
9361500	9367500	and the small model is extracting not only that Hollywood as a cinema
9367500	9371600	but it's also extracting Paul Hollywood as part of a longer token.
9371900	9377100	So what's happened here is we're trying to assign a span to two of
9377100	9379400	the same tokens and that doesn't work in spacey.
9379400	9381500	It'll break so what can you do?
9381700	9386300	Well a common method of solving this issue is to work with the filter
9386300	9388900	spans from the spacey dot util.
9389800	9393900	Let's go ahead and do this right now so you can say from spacey dot
9393900	9397200	util import filter spans.
9397400	9402200	What filter spans allows for you to do is to actually filter out all
9402200	9404600	of the the spans that are being identified.
9404900	9407700	So what we can do is we can say at this stage.
9410400	9415400	Before you get to the dock dot ends you can say filtered is equal
9415400	9419400	to filter spans original ends.
9419500	9420500	So what does this do?
9420500	9423200	Well what this does is it goes through and looks at all of the
9423200	9427800	different start and end sections from all of your entities.
9428200	9432400	And if there is an ever an instance where there is a an overlap
9432400	9436700	of tokens so 8 to 10 and 9 to 10.
9437300	9442000	Primacy and priority is going to be given to the longer token.
9442400	9445800	So what we can do is we can set this now to filtered and it helps
9445800	9448200	if you call it correctly filtered.
9448200	9448800	There we go.
9449800	9452800	We can set that to filtered instead of the original entities.
9453100	9454200	Go ahead and save that.
9454500	9458700	We're going to add this again and we're going to do doc 3 and
9458700	9463200	we're going to say for int and doc 3 dot ends print int dot
9463200	9465100	text and int dot label.
9466100	9470400	And if we've done this correctly we're not going to see the
9470600	9474900	cinema label come out at all because Paul Hollywood is a
9474900	9479000	longer token than just Hollywood.
9479000	9483400	So what we've done is we've set told spacey give the primacy
9483400	9487600	to the longer tokens and assign that label by filtering out
9487600	9490800	the tokens you can prevent that air from ever surfacing.
9491100	9494100	But this is a very common thing that you're going to have to
9494100	9498300	implement sometimes rejects really is the easiest way to
9498300	9500700	inject and do pattern matching in the entity.
9501300	9501600	Okay.
9501600	9504300	So here's the scenario that we have before us in order to make
9504300	9507500	this live this kind of live coding and applied spacey a
9507500	9510300	little bit more interesting imagine in this scenario we
9510300	9514100	have a client and the client is a stockbroker or somebody
9514100	9516900	who's interested in investing and what they want to be able
9516900	9519800	to do is look at news articles like those coming out of Reuters
9520200	9522200	and they want to find the news articles that are the most
9522200	9526400	relevant to what they need to actually search for and read
9526400	9526900	for the day.
9526900	9529000	So they want to find the ones that deal with their their
9529000	9532600	personal stocks their holdings or maybe their the specific
9532600	9534300	index that they're actually interested in.
9534600	9540600	So what this client wants is a way to use spacey to automatically
9540600	9545500	find all companies referenced within a text all stocks
9545500	9549500	referenced within a text and all indexes referenced within
9549500	9552600	the next text and maybe even some stock exchanges as well.
9552800	9555800	Now on the actual textbook if you go through to this chapter
9555800	9558900	which is number 10 you're going to find all the kind of
9558900	9562200	solutions laid out for you what I'm going to do throughout
9562200	9566500	the next 30 or 40 minutes is kind of walk through how I might
9566500	9568800	solve this problem at least on the surface.
9568800	9572300	This is going to be a rudimentary solution that demonstrates
9572300	9575700	the power of spacey and how you can apply it in a very short
9575700	9580100	period of time to do some pretty custom tasks such as financial
9580100	9584000	analysis with that structured data that you've extracted you
9584000	9587700	can then do any number of things what we're going to start off
9587700	9595000	with though is importing spacey and importing pandas as PD if
9595000	9597900	you're not familiar with pandas I've got a whole tutorial
9597900	9600900	series on that on my channel Python tutorials for digital
9600900	9603700	humanities even though it has digital humanities in the title
9603700	9606900	it's for kind of everyone but go through if you're not familiar
9606900	9609500	with pandas and check that out you're not really going to need
9609500	9613000	it for for this video here you're going to just need to
9613000	9617000	understand that I'm using pandas to access and grab the data
9617000	9621200	that I need from a couple CSV files or comma separated value
9621200	9622200	files that I have.
9623400	9625400	So the first thing that we need to do is we need to create
9625400	9628400	what's known as a pandas data frame and this is going to be
9628400	9632700	equal to PD dot read CSV and I actually have these stored in
9632700	9636700	the data sub folder in the repo you have free access to these
9636700	9639500	they're a little tiny data sets that I cultivated pretty
9639500	9642300	quickly they're not perfect but they're good enough for our
9642300	9645900	purposes and we're going to use the separator keyword argument
9645900	9649500	which is going to say to separate everything out by tab
9649500	9653400	because these are CSV files tab separated value files and we
9653400	9656100	have something that looks like this so what this stocks dot
9656100	9660100	CSV file is is it's all the symbols company names industry
9660100	9663300	and market caps for I think it's around five thousand seven
9663300	9665700	hundred different stocks five thousand eight hundred and seventy
9665700	9668900	nine and so what we're going to use this for is as a way to
9668900	9673300	start working into an entity ruler all these different symbols
9673300	9676400	and company names what we want to do is we want to use these
9676400	9679900	symbols to work into a model as a way to grab stocks that might
9679900	9682400	be referenced and you can already probably start to see a
9682400	9685400	problem with this capital a here we're going to get to that
9685400	9687900	in a little bit and we want to grab all the company names
9687900	9690300	so we can maybe create two different entity types from
9690300	9695900	this data set stock and company so let's go through and make
9695900	9699100	these into lists so they're a little bit more so let's go
9699100	9701500	through and make these into lists so they're a little bit
9701500	9706100	more manageable what we need to do is we need to create a list
9706100	9710500	of symbols and that's going to be equal to DF dot symbol dot
9710500	9713900	two list this is a great way to do it and pandas so you can
9713900	9717300	kind of easily convert all these different columns into
9718500	9721400	different lists that you can work with in Python so companies
9721400	9725100	is going to be equal to DF dot company and name I believe
9725100	9728800	the name was two list and just to demonstrate how this works
9728800	9732700	let's print off symbols we're going to print up to 10 and
9732700	9735400	you can kind of see we've managed to take these columns now
9735400	9739100	and kind of build them into a simple Python list so what can
9739100	9742100	we do with that well one of the things that we can do is we
9742100	9745900	can use that information to start cultivating an entity
9745900	9749700	ruler but remember we want more things than just one or two
9749700	9752700	kind of in our entity ruler we don't just want stocks and we
9752700	9755900	don't just want companies we also want things like indexes
9755900	9757800	we're going to get to that in just a second though for right
9757800	9761600	now let's try to work these two things into an entity ruler
9761600	9765300	how might we go about doing that well as you might expect
9765300	9769000	we're going to create a fairly simple entity ruler so we're
9769000	9771800	going to say is nlp is going to be equal to spacey dot blank
9771800	9774500	we don't need a lot of fancy features here we're just going
9774500	9778200	to have a blank model that's just going to hose host an
9778700	9782900	single entity ruler that's going to be equal to nlp dot add
9783500	9788100	underscore pipe and this is going to be entity ruler and now
9788100	9790700	what we need to do is we need to come up with a way to go
9790700	9794800	through all of these different symbols and add them in so we
9794800	9800200	can say for symbol and symbols we want to say patterns dot
9800200	9803700	append and we're going to make a an empty list of patterns
9803700	9809100	up here and what we're going to append is that dictionary that
9809100	9812100	you met when we talked about the entity ruler and I believe
9812100	9816100	it was chapter five yeah and what this is going to have
9816100	9819000	there are two things label which is going to correspond to
9819100	9824600	stock in this case and it's going to have a pattern and
9824600	9829000	that's going to correspond to the pattern of the symbol so
9829000	9832500	we're going to say symbol and what that lets us do is kind of
9832500	9836100	go through and easily create and add these patterns and and
9836100	9839500	we can do the same thing for company remember it's never a
9839500	9843200	good idea to copy and paste in your code I am simply doing
9843200	9846200	it for demonstration purposes right now this is not polished
9846200	9849600	code by any stretch of the imagination and what we can do
9849600	9852000	here now is we can do the same thing loop over the different
9852000	9855600	companies and add each company and so what this is doing is
9855600	9858500	it's creating a large list of different patterns that the
9858500	9862500	entity ruler will use to then go through and as we create the
9862500	9867100	a doc object over that sample Reuters text I just showed you
9867100	9870200	a second ago which we should probably just go ahead and
9870200	9873100	pull up right now I'm going to copy and paste it straight
9873100	9875100	from the textbook.
9878600	9881600	Let's go ahead and execute that cell and we're going to add
9881600	9885000	in this text here it is a little lengthy but it'll be all
9885000	9887800	right and what we're going to do now is we're going to iterate
9887800	9891000	over create a doc object to iterate over all of that.
9894000	9897800	And our goal here is going to be able to say for int and doc
9897800	9901800	dot ends we want to have extracted all of these different
9902100	9908100	entities so we can say print off and dot text and dot label
9910000	9911300	and let's see if we succeeded.
9915400	9919000	And we have to add in our patterns to our entity ruler so
9919000	9922600	remember we can do this by saying ruler dot add patterns.
9924900	9926000	Patterns there we go.
9927000	9932000	That's what this error actually means and now when we do it
9932000	9936500	we see that we've been able to extract Apple as a company
9936500	9940700	Apple as a company Nasdaq everything's looking pretty good
9940700	9943900	but I notice really quickly that I wasn't actually able to
9943900	9948800	extract Apple as a stock and I've also got another problem
9949100	9954700	I've extracted to the lowercase TWO as a stock as well why
9954700	9957900	have these two things are as a company. Well it turns out in
9957900	9963500	our data set we've got to TWO that is a company name that's
9963500	9967200	almost always going to be a false positive and we know that
9967200	9970100	that kind of thing might be better off worked into a machine
9970100	9973100	learning model for right now though we're going to work
9973100	9975700	under the presumption that anytime we encounter this kind
9975700	9979500	of obscure company TWO as a lowercase it's going to be a
9979500	9982900	false positive. I also have another problem I know for a
9982900	9988500	fact that Apple the stock is referenced within this text to
9988500	9991400	make it a little easier. Let's see it right here and notice
9991400	9993900	that it didn't find it to make this a little easier to
9993900	9997300	display. Let's go ahead and display what we're looking at
9997500	10001700	as the splacy render so what we can do is we can use that the
10001700	10005200	splacy render that we met a little bit ago in this video.
10006700	10009700	So in order to import this if you remember we need to say
10010700	10016800	from spacey import display see and that's going to allow us
10016800	10020200	to actually display our entities. Let's go ahead and put
10020200	10023400	this however on a different cell just so we don't have to
10023400	10029400	execute that every time and we're going to say at splacy render
10029600	10033500	and we're going to render the doc object with a style that's
10033500	10037800	equal to ENT and we can see that we've got our text now
10037800	10041300	popping out with our things labeled and you can see pretty
10041300	10044100	quickly where we've made some mistakes where we need to
10044100	10048300	incorporate some things into our entity ruler. So for example
10048300	10051500	if I'm scrolling through this is gray little ugly we can change
10051500	10054500	the colors that's beyond the scope of this video though but
10054500	10057900	let's keep on going down we notice that we have Apple dot
10057900	10061600	IO and yet this has been missed by our entity ruler. Why has
10061600	10066400	this been missed well. Spacey as a tokenizer is seeing this
10066400	10072200	as a single token so Apple dot Oh the letter Oh capital letter
10072200	10075900	Oh why is that well I didn't know about this but apparently
10075900	10079800	it does has to deal with kind of the way in which stock indices
10079800	10083500	are I think it's on the NASDAQ kind of structure things so
10083500	10086200	what can we do well we've got a couple different options here
10086600	10089400	I know that these go through all different letters from A to
10089400	10093000	Z so we can either work with the string library or we can do
10093000	10096500	is we can import a quick list that I've already written out
10096800	10100000	of all the different letters of the alphabet and iterate
10100000	10103800	through those with our ruler up here.
10106000	10109100	Let's go ahead and add these letters right there and we can
10109100	10111100	kind of iterate through those and whenever a stock kind of
10111100	10115800	pops out with that kind of symbol plus any occurrence where
10115800	10120400	it's got a period followed by a letter in those scenarios we
10120400	10124600	want that to be flagged as a stock as well so what we can do
10124600	10127400	is we can add in another thing right here add in another
10127400	10131800	pattern and this is now going to be symbol plus we're going
10131800	10135900	to add in F string right here a formatted string any occurrence
10135900	10145300	of L we can set up a loop to say for L and letters do this
10145700	10149400	and what this is going to allow us to do is to look for any
10149400	10154900	instance where there is a symbol followed by a period
10154900	10158400	followed by one of these capitalized letters that I just
10158400	10162400	copied and pasted in so if we do that we can execute that cell
10164400	10167600	and we can scroll down and we can now do the exact same thing
10167600	10171800	that we just did a second ago and actually display this
10172800	10180000	and now we're finding these stocks highlighted as stock so
10180000	10182800	we're successfully getting these stocks and extracting them
10182800	10185800	we've got a few different things that our client wants to
10185800	10189000	also extract though they don't want to just extract companies
10189000	10193200	and they don't want to just extract stock and they want to
10193200	10197200	also extract stock exchanges and indexes but we have one
10197200	10200500	other problem and go ahead and get rid of this as the display
10200500	10203600	mode and switch back to just our set of entities because
10203600	10206400	it's a little easier to read for this example we've got
10206400	10208400	another problem and we see we have a couple other stocks
10208400	10212600	popping out we now know that Kroger stock is here the n i
10212600	10216000	o dot n stock is in this text as well now we're starting to
10216000	10219700	see a greater degree of specificity for right now I'm
10219700	10224000	going to include two as a set of a stop technical term would
10224000	10226500	be like a stop or something that I don't want to be included
10226500	10229700	into the model so I'm going to make a list of stops and
10229700	10232400	we're just going to include two in that and we're going to
10232400	10240200	say for company and companies do all this if company not in
10240200	10244900	stops we want this to occur what this means now is that our
10244900	10247900	our pipeline while going through and having all of these
10247900	10250900	different things all these different rules it's also going
10250900	10254200	to have another rule that looks to see if there's a stop or
10254200	10258400	if this company name is this stop and if it is then we want
10258400	10263000	it to just kind of skip over and ignore it and if we go
10263000	10265800	through we notice that now we've successfully eliminated
10265800	10270300	this what we would presume to be a consistent false positive
10270300	10273100	something that's going to come up again and again as a false
10273100	10277100	positive great so we've been able to get this where it works
10277100	10280300	now pretty well what I also want to work into this model if
10280300	10283600	you remember though are things like indexes fortunately
10283600	10288200	I've also provided for us a list of all different indexes that
10288200	10291500	are available from I believe it's like everything like the
10291500	10295700	Dow Jones is about 13 or 14 of them let's go ahead and import
10295700	10301300	those up above and let's do that right here in this cell so
10301300	10303800	it kind of goes in sequential order that follows better with
10303800	10306900	the textbook to so it's a new data frame object this is going
10306900	10310800	to be equal to P a PD dot read CSV we're going to read in that
10310800	10313700	data file that I've given us and that's going to be the indexes
10313700	10319900	dot T SV with a separator that's equal to a tab let's see what
10319900	10324000	that looks like and this is what it looks like so all these
10324000	10326900	different indices now I know I'm going to have a problem right
10326900	10331100	out of the gate and that's going to be that sometimes you're
10331100	10334600	going to see things referenced as SNP 500 I don't know a lot
10334600	10337100	about finances but I know that you don't always see it as
10337100	10342200	SNP 500 index but I do think that these index symbols are
10342200	10345700	also going to be useful so like I did before I'm going to convert
10345700	10348500	these things into a list so it's a little easier for me to work
10348500	10353900	with in a for loop and I'm going to say indexes is equal to
10353900	10361800	DF2 dot index name so grabbing that column to list and index
10361800	10368400	symbols is equal to DF2 dot index symbol dot to list and
10368400	10371500	both of these are going to be different and they're both going
10371500	10376200	to have the same exact entity label which is going to be an
10376200	10380200	index and so let's go ahead and iterate over these and add them
10380200	10383300	in as well so I'm going to go ahead and do that right now
10384000	10393700	for indexes and indexes we want this label to be index we
10393700	10397700	want this to be index here so it's going to allow us to kind
10397700	10400200	of go through and grab all those and we want to do the same
10400200	10406000	thing with index symbols keep these a little separated here
10406000	10412300	index symbols and that allows for us to do that and let's go
10412300	10415200	ahead and without making any adjustments let's see let's see
10415200	10417900	how this does with these new patterns that we've added in
10418400	10421100	and because we've already got this text loaded into memory
10421100	10424500	I'm going to go ahead and put this right here doc is going to
10424500	10432900	be equal to nlp text for int and doc and print off and dot
10432900	10438400	text and dot label and we can kind of go through and we're
10438400	10442200	actually now able to extract some indexes and I believe when
10442200	10445200	I was looking at this text really quickly though I noticed
10445200	10449600	that there was one instance at least where we had not only
10449600	10455300	the index referenced but also a name like S&P 500 right here
10455300	10458700	S&P 500 notice that it isn't found because it doesn't have
10458700	10461800	the name index after it and notice also that none of our
10462300	10465000	our symbols are being found because they all seem to be
10465000	10471500	preceded by a dot so in this case a dot J a DJI and so that's
10471500	10473600	something else that I have to work into this model and the
10473600	10477000	list I gave the data set that's not there so I need to collect
10477100	10480600	a list of these different names and work those into an entity
10480600	10483500	ruler as well but for right now let's ignore that and focus
10483500	10488500	on including this S&P 500 so how can I get the S&P 500 in
10488500	10491800	there from the list I already gave it well what I can do is
10491800	10496800	I can say okay so under these indices not only do I want to
10496800	10500300	add that specific pattern let's go ahead and break these things
10500300	10503700	up into different words and so I'm going to have the words is
10503700	10507400	equal to index dot split and then I'm going to make a
10507400	10513600	presumption that the the first two words so the S&P 500 the
10513600	10518800	S&P 400 are sometimes going to be referenced by themselves so
10518800	10521400	what I want to do is I want to work that into the model as
10521400	10526600	well and I want to say we're going to say patterns dot
10526600	10534700	append copy this as well we can say something like dot join
10536300	10541800	words up until the second index and let's go ahead and work
10541800	10545700	that into our model in our patterns or pipeline and print
10545700	10550300	off our NLP again and you'll find that we've now been able to
10550300	10556300	capture things like S&P 500 that aren't proceeded by the word
10556300	10560700	index and we see that we in fact have S&P 500 is now popping
10560700	10564100	out time and again that's fantastic I'm pretty happy with
10564100	10566800	that now we're we're getting a deeper sense of what this
10566800	10569700	text is about without actually having to read it we know that
10569700	10572700	it's going to deal heavily with Apple and we know that it's
10572700	10575000	also going to tangentially deal with some of these other
10575000	10579200	things as well but I also want to include into this into this
10579200	10583200	pipeline the ability for the entity ruler to not just find
10583200	10586200	these things but I also wanted to be able to find different
10586200	10590000	stock exchanges so I've got a list I cultivated for different
10590000	10593800	stock exchanges which are things like NYSE things like that
10593800	10600500	so I can say DS3 is going to be equal to PD dot read CSV data
10600500	10607800	backslash stock exchanges dot TSV and then the separator is
10607800	10611300	going to be again a tab and let's take a look at what this
10611300	10612000	looks like.
10614700	10616000	Stanges there we go.
10620600	10623300	There we are and we have something that looks like this
10623300	10629600	a pretty a pretty large CSV file CSV file sorry that's got a
10629600	10633100	bunch of different rows the ones I'm most interested in well
10633100	10636700	there's a couple actually I'm interested in specifically the
10636700	10640900	Google Prefix and this description the description has
10640900	10644400	the actual name and the Prefix has this really nice abbreviation
10644400	10647900	that I've seen pop out a few different times such as Nasdaq
10647900	10651100	here if we keep on going down we would see different things
10651100	10655200	as well NYSE these are kind of different stock exchanges.
10656000	10660600	So let's pop back down here and let's go ahead and convert
10660600	10664600	those two things into individual lists as well so we're going
10664600	10669200	to say exchanges it's going to be equal to DF3 dot ISO Mike
10669900	10676800	dot to list and then I'm also going to grab the F3 dot sorry
10676800	10680200	Google I have to do this as a dictionary because it's the
10680200	10683700	way the data sets cultivated it's got a space in the middle
10683700	10687900	this is a common problem that you run into and then I also
10687900	10692100	want to know grab all of these exchanges as well so I'm going
10692100	10700400	to say also on top of that DF3 dot description to list so I'm
10700400	10706700	making a large list exchanges and I get this here because it
10706700	10710400	says Google Prefix isn't an actual thing and in fact it's
10710400	10714500	prefix with an I and now we actually are able to get all
10714500	10719400	these things extracted so what I want to do now is I want to
10719400	10723300	work all these different symbols and descriptions into into
10723300	10726500	the model as well or into the pipeline as well so I can say
10726500	10734500	for for E and exchanges I want to say patterns dot append
10737700	10741700	and I want to do a label that's going to be let's do stock
10742400	10747300	exchange and then the next thing I want to do is a pattern
10747600	10751100	and that's going to be equal to in this case E as we're going
10751100	10754600	to see this is not adequate enough we need to do a few
10754600	10758100	different things to really kind of work this out but it's going
10758100	10760200	to be a good enough to at least get started
10765200	10766600	and it's going to take it just a second
10772000	10774200	and the main thing that's happening right now are these
10774200	10778200	different for loops so if we keep on going down we now see
10778200	10781800	that we were able to extract the NYSE stock exchange so we've
10781800	10784600	not only been able to work into a pipeline in a very short
10784600	10787200	order maybe about 20 30 minutes we've been able to work
10787200	10790600	into a pipeline all of these different things that are coming
10790600	10794000	out we do however see a couple problems and this is where I'm
10794000	10796000	going to leave it though because you've got the basic
10796000	10799600	mechanics down now comes time for you being a domain expert
10799800	10802100	to work out and come up with rules to solve some of these
10802100	10806300	problems Nasdaq is not a company so there's a problem with
10806300	10810100	the data set or Nasdaq is listed as a company name and one of
10810100	10813500	the data sets we need to work that out where Nasdaq is never
10813500	10817100	referenced as a company we have the S&P and is now being
10817100	10820200	coming out correctly as S&P 500 there might be instances
10820200	10823600	where just S&P is referenced which I think in that context
10823600	10827100	would probably be the S&P 500 but nevertheless we've been
10827100	10833200	able to actually extract these things sometimes the Dow Jones
10833200	10837800	Industrial Average might just be referenced to Dow Jones so
10837800	10840400	this index might just be these first two words I know that's
10840400	10842900	a common occurrence we've also seen that we weren't able to
10842900	10845700	extract some of those things that were a period followed by
10845700	10850100	a symbol that referenced the actual index itself nevertheless
10850100	10852600	this is a really good starting point and you can see how just
10852600	10855200	in a few minutes you're able to generate this thing that can
10855200	10859700	extract information from unstructured text at the end of
10859700	10862100	the day like I said in the introduction to this entire
10862100	10867100	video that's one of the essential tasks of NLP designing
10867100	10870600	this and implementing it is pretty quick and easy perfecting
10870600	10874200	it is where the time really is to get this financial analysis
10874800	10879000	entity ruler working really well where it has almost no false
10879000	10884600	positives and almost never misses a true a true positive it
10884600	10887100	would take maybe a few more hours of just some kind of working
10887100	10889300	and eventually there are certain things you might find that
10889300	10892600	would work better in a machine learning model nevertheless
10892600	10896000	you can see the degree to which rules based approaches in
10896000	10899600	Spacey can really accomplish some pretty robust tasks with
10899600	10903400	minimal minimal amount of code so long as you have access to
10903400	10906400	or have already cultivated the data sets required.
10909100	10912900	Thank you so much for watching this video series on Spacey
10912900	10915700	an introduction to basic concepts of natural language
10915700	10920700	processing linguistic annotations in Spacey vectors pipelines
10920700	10923800	and kind of rules based Spacey you've enjoyed this video
10923800	10927200	please like and subscribe down below and if you've also found
10927200	10930600	this video useful consider joining me on my channel Python
10930600	10933700	tutorials for digital humanities if you have like this and
10933700	10937300	found this video useful I'm envisioning a second part to
10937300	10941500	this video where I go with the machine learning aspects of
10941500	10944000	Spacey if you're interested in that let me know in the
10944000	10946700	comments down below and I'll make a second video that
10946700	10948000	corresponds to this one.
10948700	10950600	Thank you for watching and have a great day.
